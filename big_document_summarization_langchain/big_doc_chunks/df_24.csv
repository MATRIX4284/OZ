,page,img_cnt,img_npy_lst,text,tables,entities,relationships,summary_rel,summary,highlights
240,329,0,[],"297
Using a StatefulSet
The data you sent should now be stored in that pod. Let’s see if it returns the stored
data when you perform a GET request again:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: Hey there! This greeting was submitted to kubia-0.
Okay, so far so good. Now let’s see what the other cluster node (the kubia-1 pod)
says:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/
You've hit kubia-1
Data stored on this pod: No data posted yet
As expected, each node has its own state. But is that state persisted? Let’s find out.
DELETING A STATEFUL POD TO SEE IF THE RESCHEDULED POD IS REATTACHED TO THE SAME STORAGE
You’re going to delete the kubia-0 pod and wait for it to be rescheduled. Then you’ll
see if it’s still serving the same data as before:
$ kubectl delete po kubia-0
pod ""kubia-0"" deleted
If you list the pods, you’ll see that the pod is terminating: 
$ kubectl get po
NAME      READY     STATUS        RESTARTS   AGE
kubia-0   1/1       Terminating   0          3m
kubia-1   1/1       Running       0          3m
As soon as it terminates successfully, a new pod with the same name is created by the
StatefulSet:
$ kubectl get po
NAME      READY     STATUS              RESTARTS   AGE
kubia-0   0/1       ContainerCreating   0          6s
kubia-1   1/1       Running             0          4m
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
kubia-0   1/1       Running   0          9s
kubia-1   1/1       Running   0          4m
Let me remind you again that this new pod may be scheduled to any node in the clus-
ter, not necessarily the same node that the old pod was scheduled to. The old pod’s
whole identity (the name, hostname, and the storage) is effectively moved to the new
node (as shown in figure 10.11). If you’re using Minikube, you can’t see this because it
only runs a single node, but in a multi-node cluster, you may see the pod scheduled to
a different node than before.
 
",[],"[{'entity': 'StatefulSet', 'description': 'A Kubernetes StatefulSet is used to manage stateful applications.', 'category': 'software'}, {'entity': 'pod', 'description': 'A pod is a logical host for one or more containers.', 'category': 'container'}, {'entity': 'kubectl', 'description': 'The command-line tool for running commands against Kubernetes clusters.', 'category': 'command'}, {'entity': 'curl', 'description': 'A command-line tool for transferring data to and from a web server.', 'category': 'command'}, {'entity': 'GET request', 'description': 'An HTTP request method used to retrieve data from a server.', 'category': 'protocol'}, {'entity': 'StatefulSet', 'description': 'A Kubernetes StatefulSet is used to manage stateful applications.', 'category': 'software'}, {'entity': 'kubia-0', 'description': 'The name of the first pod in the cluster.', 'category': 'container'}, {'entity': 'kubia-1', 'description': 'The name of the second pod in the cluster.', 'category': 'container'}, {'entity': 'localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/', 'description': ""The URL for accessing the kubia-0 pod's proxy service."", 'category': 'url'}, {'entity': 'DELETING A STATEFUL POD', 'description': 'A process of deleting a StatefulSet pod and verifying its behavior.', 'category': 'process'}, {'entity': 'reschedule', 'description': 'The process of scheduling a new pod to replace an existing one.', 'category': 'process'}, {'entity': 'storage', 'description': 'The persistent storage used by the StatefulSet pods.', 'category': 'hardware'}, {'entity': 'Minikube', 'description': 'A tool for running Kubernetes locally on a single node.', 'category': 'software'}]","[{'source_entity': '""kubia-0""', 'description': 'is being accessed by', 'destination_entity': '""localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/""'}, {'source_entity': '""kubectl""', 'description': 'is used to interact with', 'destination_entity': '""kubia-0""'}, {'source_entity': '""kubectl""', 'description': 'is used to interact with', 'destination_entity': '""StatefulSet""'}, {'source_entity': '""kubectl""', 'description': 'is used to reschedule', 'destination_entity': '""kubia-0""'}, {'source_entity': '""DELETING A STATEFUL POD""', 'description': 'is being performed on', 'destination_entity': '""kubia-0""'}, {'source_entity': '""kubectl""', 'description': 'is used to manage', 'destination_entity': '""pod""'}, {'source_entity': '""Minikube""', 'description': 'is being used as a platform for', 'destination_entity': '""kubia-0""'}, {'source_entity': '""curl""', 'description': 'is being used to send', 'destination_entity': '""GET request""'}]","['[\n  {\n    ""source"": ""kubia-0"",\n    ""destination"": ""localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/"",\n    ""relation_description"": ""is being accessed by"",\n    ""summary_er"": ""Kubernetes pod kubia-0 is being accessed through a proxy URL.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is used to interact with"",\n    ""summary_er"": ""kubectl is a command-line tool used to interact with Kubernetes clusters, specifically pods.""\n  },\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""kubia-0"",\n    ""relation_description"": ""interact with"",\n    ""summary_er"": ""kubectl interacts with the pod named kubia-0 in the Kubernetes cluster.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is used to interact with"",\n    ""summary_er"": ""Kubectl is a command-line tool used to interact with pods, managing their lifecycle and configuration.""\n  },\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages"",\n    ""summary_er"": ""A StatefulSet manages the deployment and scaling of pods, ensuring consistent state across replicas.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is used to reschedule"",\n    ""summary_er"": ""Kubectl is used to reschedule a pod, such as \'kubia-0\', for redeployment or restart.""\n  }\n]', '[\n  {\n    ""source"": ""DELETING A STATEFUL POD"",\n    ""destination"": ""pod kubia-0"",\n    ""relation_description"": ""is being performed on"",\n    ""summary_er"": ""A stateful pod, specifically \'kubia-0\', is being deleted.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is used to manage"",\n    ""summary_er"": ""Kubectl is a command-line tool used to manage and deploy pods in a Kubernetes cluster.""\n  }\n]', '[\n  {\n    ""source"": ""Minikube"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is being used as a platform for"",\n    ""summary_er"": ""Minikube is used as a platform to run and manage Kubernetes pods, such as kubia-0.""\n  }\n]', '[\n  {\n    ""source"": ""curl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is being used to send"",\n    ""summary_er"": ""Curl is a command-line tool used to send GET requests to pods.""\n  }\n]']","A StatefulSet was used to store data on a pod. When a GET request is made, the stored data is returned. The pod is then deleted and recreated by the StatefulSet, which still serves the same data as before. This demonstrates that a StatefulSet preserves state even when pods are rescheduled or deleted.","[{'highlight': 'The data stored in a StatefulSet is persisted across pod rescheduling.'}, {'highlight': 'When a StatefulSet pod is deleted and recreated, it retains its state and identity, including storage.'}, {'highlight': 'Each node in a cluster has its own state, which is not shared with other nodes.'}, {'highlight': 'StatefulSets use persistent storage to store data, which is retained even when pods are rescheduled or deleted.'}, {'highlight': 'In a multi-node cluster, StatefulSet pods may be scheduled to different nodes than before after deletion and recreation.'}]"
241,330,2,[],"298
CHAPTER 10
StatefulSets: deploying replicated stateful applications
With the new pod now running, let’s check to see if it has the exact same identity as in
its previous incarnation. The pod’s name is the same, but what about the hostname
and persistent data? You can ask the pod itself to confirm:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: Hey there! This greeting was submitted to kubia-0.
The pod’s response shows that both the hostname and the data are the same as before,
confirming that a StatefulSet always replaces a deleted pod with what’s effectively the
exact same pod. 
SCALING A STATEFULSET
Scaling down a StatefulSet and scaling it back up after an extended time period
should be no different than deleting a pod and having the StatefulSet recreate it
immediately. Remember that scaling down a StatefulSet only deletes the pods, but
leaves the PersistentVolumeClaims untouched. I’ll let you try scaling down the State-
fulSet yourself and confirm this behavior. 
 The key thing to remember is that scaling down (and up) is performed gradu-
ally—similar to how individual pods are created when the StatefulSet is created ini-
tially. When scaling down by more than one instance, the pod with the highest ordinal
number is deleted first. Only after the pod terminates completely is the pod with the
second highest ordinal number deleted. 
EXPOSING STATEFUL PODS THROUGH A REGULAR, NON-HEADLESS SERVICE
Before you move on to the last part of this chapter, you’re going to add a proper, non-
headless Service in front of your pods, because clients usually connect to the pods
through a Service rather than connecting directly.
Node 1
Pod: kubia-0
Pod: kubia-1
Delete kubia-0
Storage
Storage
Storage
Pod: kubia-1
Storage
Node 1
kubia-0 rescheduled
Node 1
Node 2
Node 2
Node 2
Storage
Pod: kubia-1
Storage
Pod: kubia-0
Figure 10.11
A stateful pod may be rescheduled to a different node, but it retains the name, hostname, and storage.
 
",[],"[{'entity': 'StatefulSet', 'description': 'a Kubernetes resource that manages replicated stateful applications', 'category': 'application'}, {'entity': 'pod', 'description': 'a container running an application', 'category': 'container'}, {'entity': 'hostname', 'description': 'the name of a computer or device on a network', 'category': 'hardware'}, {'entity': 'persistent data', 'description': 'data stored on a persistent volume', 'category': 'database'}, {'entity': 'StatefulSet', 'description': 'a Kubernetes resource that manages replicated stateful applications', 'category': 'application'}, {'entity': 'pod', 'description': 'a container running an application', 'category': 'container'}, {'entity': 'PersistentVolumeClaims', 'description': 'requests for storage resources', 'category': 'database'}, {'entity': 'Service', 'description': 'an abstraction which defines a logical set of network endpoints', 'category': 'application'}, {'entity': 'kubia-0', 'description': 'a pod name', 'category': 'container'}, {'entity': 'kubia-1', 'description': 'a pod name', 'category': 'container'}, {'entity': 'ordinal number', 'description': 'the order of a pod in a StatefulSet', 'category': 'process'}, {'entity': 'node', 'description': 'a machine or device on a network', 'category': 'hardware'}]","[{'source_entity': '""kubia-0""', 'description': 'is a pod', 'destination_entity': '""pod""'}, {'source_entity': '""kubia-1""', 'description': 'is a pod', 'destination_entity': '""pod""'}, {'source_entity': '""Service""', 'description': 'provides access to kubia-0 and kubia-1', 'destination_entity': '""kubia-0"", ""kubia-1""'}, {'source_entity': '""StatefulSet""', 'description': 'manages the deployment of kubia-0 and kubia-1', 'destination_entity': '""kubia-0"", ""kubia-1""'}, {'source_entity': '""ordinal number""', 'description': 'is used to identify kubia-0 and kubia-1', 'destination_entity': '""kubia-0"", ""kubia-1""'}, {'source_entity': '""persistent data""', 'description': 'is stored in PersistentVolumeClaims', 'destination_entity': '""PersistentVolumeClaims""'}, {'source_entity': '""PersistentVolumeClaims""', 'description': 'provides storage for persistent data', 'destination_entity': '""persistent data""'}, {'source_entity': '""pod""', 'description': 'is running on a node', 'destination_entity': '""node""'}, {'source_entity': '""hostname""', 'description': 'is used to identify the node', 'destination_entity': '""node""'}]","['[\n  {\n    ""source"": ""kubia-0"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is a pod"",\n    ""summary_er"": ""Kubia-0 is a Kubernetes pod, a containerized application running on a cluster.""\n  }\n]', '[\n  {\n    ""source"": ""kubia-1"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is a"",\n    ""summary_er"": ""Kubia-1 is a pod in Kubernetes.""\n  }\n]', '[\n  {\n    ""source"": ""Service"",\n    ""destination"": ""kubia-0"",\n    ""relation_description"": ""provides access to kubia-0 and kubia-1"",\n    ""summary_er"": ""A service provides access to two pods, kubia-0 and kubia-1.""\n  },\n  {\n    ""source"": ""Service"",\n    ""destination"": ""kubia-1"",\n    ""relation_description"": ""provides access to kubia-0 and kubia-1"",\n    ""summary_er"": ""A service provides access to two pods, kubia-0 and kubia-1.""\n  }\n]', '[\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages the deployment of kubia-0 and kubia-1"",\n    ""summary_er"": ""A StatefulSet manages the deployment of multiple pods, in this case kubia-0 and kubia-1.""\n  },\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""kubia-0"",\n    ""relation_description"": ""manages the deployment of"",\n    ""summary_er"": ""A StatefulSet manages the deployment of a single pod, in this case kubia-0.""\n  },\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""kubia-1"",\n    ""relation_description"": ""manages the deployment of"",\n    ""summary_er"": ""A StatefulSet manages the deployment of a single pod, in this case kubia-1.""\n  }\n]', '[\n  {\n    ""source"": ""ordinal number"",\n    ""destination"": ""kubia-0"",\n    ""relation_description"": ""is used to identify"",\n    ""summary_er"": ""Ordinal numbers are used to uniquely identify pods, such as kubia-0 and kubia-1 in a Kubernetes cluster.""\n  },\n  {\n    ""source"": ""ordinal number"",\n    ""destination"": ""kubia-1"",\n    ""relation_description"": ""is used to identify"",\n    ""summary_er"": ""Ordinal numbers are used to uniquely identify pods, such as kubia-0 and kubia-1 in a Kubernetes cluster.""\n  }\n]', '[\n  {\n    ""source"": ""persistent data"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is stored in"",\n    ""summary_er"": ""Persistent data is stored within a pod, utilizing PersistentVolumeClaims for persistent storage.""\n  }\n]', '[\n  {\n    ""source"": ""PersistentVolumeClaims"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""provides storage for persistent data"",\n    ""summary_er"": ""PersistentVolumeClaims provide storage for pods to store persistent data, ensuring data durability and availability.""\n  }\n]', '[\n  {\n    ""source"": ""pod"",\n    ""destination"": ""node"",\n    ""relation_description"": ""is running on a node"",\n    ""summary_er"": ""A Kubernetes pod is deployed and executed on a specific node within the cluster, utilizing its resources for execution.""\n  }\n]', '[\n  {\n    ""source"": ""hostname"",\n    ""destination"": ""node"",\n    ""relation_description"": ""is used to identify the node"",\n    ""summary_er"": ""A hostname is a unique identifier for a node, allowing it to be distinguished from other nodes within a Kubernetes cluster.""\n  },\n  {\n    ""source"": ""hostname"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is used to identify the pod"",\n    ""summary_er"": ""A hostname can also serve as an identifier for a pod, enabling its management and monitoring within the containerized environment.""\n  }\n]']","A StatefulSet maintains its pods' identities, including hostnames and persistent data, even when scaled down or recreated after deletion. Scaling down a StatefulSet deletes pods but leaves PersistentVolumeClaims intact, with pods deleted in descending order of ordinal numbers. A non-headless Service can be used to expose stateful pods, allowing clients to connect through the Service rather than directly.","[{'highlight': ""A StatefulSet always replaces a deleted pod with what's effectively the exact same pod.""}, {'highlight': 'Scaling down a StatefulSet only deletes the pods, but leaves the PersistentVolumeClaims untouched.'}, {'highlight': 'When scaling down by more than one instance, the pod with the highest ordinal number is deleted first.'}, {'highlight': 'A stateful pod may be rescheduled to a different node, but it retains the name, hostname, and storage.'}, {'highlight': ""The pod's response shows that both the hostname and the data are the same as before, confirming that a StatefulSet always replaces a deleted pod with what's effectively the exact same pod.""}]"
242,331,0,[],"299
Discovering peers in a StatefulSet
 You know how to create the Service by now, but in case you don’t, the following list-
ing shows the manifest.
apiVersion: v1
kind: Service
metadata:
  name: kubia-public
spec:
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
Because this isn’t an externally exposed Service (it’s a regular ClusterIP Service, not
a NodePort or a LoadBalancer-type Service), you can only access it from inside the
cluster. You’ll need a pod to access it from, right? Not necessarily.
CONNECTING TO CLUSTER-INTERNAL SERVICES THROUGH THE API SERVER
Instead of using a piggyback pod to access the service from inside the cluster, you can
use the same proxy feature provided by the API server to access the service the way
you’ve accessed individual pods.
 The URI path for proxy-ing requests to Services is formed like this:
/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>
Therefore, you can run curl on your local machine and access the service through the
kubectl proxy like this (you ran kubectl proxy earlier and it should still be running):
$ curl localhost:8001/api/v1/namespaces/default/services/kubia-
➥ public/proxy/
You've hit kubia-1
Data stored on this pod: No data posted yet
Likewise, clients (inside the cluster) can use the kubia-public service for storing to
and reading data from your clustered data store. Of course, each request lands on a
random cluster node, so you’ll get the data from a random node each time. You’ll
improve this next.
10.4
Discovering peers in a StatefulSet
We still need to cover one more important thing. An important requirement of clus-
tered apps is peer discovery—the ability to find other members of the cluster. Each
member of a StatefulSet needs to easily find all the other members. Sure, it could do
that by talking to the API server, but one of Kubernetes’ aims is to expose features that
help keep applications completely Kubernetes-agnostic. Having apps talk to the Kuber-
netes API is therefore undesirable.
Listing 10.7
A regular Service for accessing the stateful pods: kubia-service-public.yaml
 
",[],"[{'entity': 'StatefulSet', 'description': 'a Kubernetes resource that manages a set of identical, stateful pods', 'category': 'application'}, {'entity': 'Service', 'description': 'a Kubernetes resource that provides a network identity and load balancing for accessing applications', 'category': 'application'}, {'entity': 'ClusterIP Service', 'description': 'a type of Service that is only accessible from within the cluster', 'category': 'application'}, {'entity': 'NodePort Service', 'description': 'a type of Service that exposes a port on each node in the cluster', 'category': 'application'}, {'entity': 'LoadBalancer Service', 'description': 'a type of Service that provides load balancing and access to an application from outside the cluster', 'category': 'application'}, {'entity': 'kubectl proxy', 'description': 'a feature provided by the Kubernetes API server for accessing services and pods within the cluster', 'category': 'command'}, {'entity': 'curl', 'description': 'a command-line tool for transferring data to and from a web server', 'category': 'command'}, {'entity': 'API Server', 'description': 'the central component of the Kubernetes control plane that provides an interface for accessing cluster resources', 'category': 'application'}, {'entity': 'Namespace', 'description': 'a logical grouping of resources within a Kubernetes cluster', 'category': 'application'}, {'entity': 'Pod', 'description': 'the basic execution unit in a Kubernetes cluster, representing a running instance of an application', 'category': 'application'}, {'entity': 'StatefulSet peer discovery', 'description': 'the ability to find other members of a StatefulSet within the cluster', 'category': 'process'}]","[{'source_entity': '""Service""', 'description': 'provides', 'destination_entity': '""ClusterIP Service""'}, {'source_entity': '""StatefulSet peer discovery""', 'description': 'discovered by', 'destination_entity': '""StatefulSet""'}, {'source_entity': '""Namespace""', 'description': 'contains', 'destination_entity': '""Pod""'}, {'source_entity': '""API Server""', 'description': 'manages', 'destination_entity': '""ClusterIP Service""'}, {'source_entity': '""LoadBalancer Service""', 'description': 'exposes to', 'destination_entity': '""External IP Address""'}, {'source_entity': '""curl""', 'description': 'uses to', 'destination_entity': '""API Server""'}, {'source_entity': '""kubectl proxy""', 'description': 'proxies requests to', 'destination_entity': '""API Server""'}]","['[\n  {\n    ""source"": ""Service"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""provides"",\n    ""summary_er"": ""A Service provides a network interface to access a pod\'s exposed ports.""\n  }\n]', '[\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""discovered by"",\n    ""summary_er"": ""A StatefulSet discovers a pod through peer discovery, enabling them to communicate and coordinate with each other.""\n  }\n]', '[\n  {\n    ""source"": ""Namespace"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""contains"",\n    ""summary_er"": ""A Namespace in Kubernetes contains one or more Pods, which are the basic execution units of a containerized application.""\n  }\n]', '[\n  {\n    ""source"": ""API Server"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages"",\n    ""summary_er"": ""The API Server manages a pod, which is a containerized application in Kubernetes.""\n  },\n  {\n    ""source"": ""ClusterIP Service"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""exposes"",\n    ""summary_er"": ""A ClusterIP Service exposes a pod to other pods within the same cluster, enabling communication between them.""\n  }\n]', '[\n  {\n    ""source"": ""LoadBalancer Service"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""exposes to"",\n    ""summary_er"": ""A LoadBalancer Service exposes its external IP address to a pod, allowing incoming traffic to reach the pod.""\n  }\n]', '[\n  {\n    ""source"": ""curl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""uses to"",\n    ""summary_er"": ""\\""curl\\"" is a command-line tool that uses HTTP requests to interact with APIs, and it can be used to communicate with a \\""pod\\"", which is a containerized application in Kubernetes.""\n  },\n  {\n    ""source"": ""API Server"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages"",\n    ""summary_er"": ""\\""API Server\\"" manages the lifecycle of a \\""pod\\"", including creation, deletion, and scaling, ensuring efficient resource utilization in a Kubernetes cluster.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""API Server"",\n    ""relation_description"": ""proxies requests to"",\n    ""summary_er"": ""The kubectl command proxies HTTP requests from the client to the API server, allowing for communication between the two.""\n  }\n]']","To access a cluster-internal service, you can use the proxy feature provided by the API server, or use a pod to access it. The URI path is formed like /api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>. Peer discovery in a StatefulSet is also important for clustered apps to find other members.","[{'highlight': ""You can access a ClusterIP Service from inside the cluster using the API server's proxy feature, without needing a pod to access it.""}, {'highlight': 'The URI path for proxy-ing requests to Services is formed like this: /api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>'}, {'highlight': 'Clients inside the cluster can use the kubia-public service for storing and reading data from a clustered data store.'}, {'highlight': ""Each request to a ClusterIP Service lands on a random cluster node, so you'll get data from a random node each time.""}, {'highlight': 'StatefulSets require peer discovery - the ability to find other members of the cluster, which can be achieved without talking to the API server.'}]"
243,332,0,[],"300
CHAPTER 10
StatefulSets: deploying replicated stateful applications
 How can a pod discover its peers without talking to the API? Is there an existing,
well-known technology you can use that makes this possible? How about the Domain
Name System (DNS)? Depending on how much you know about DNS, you probably
understand what an A, CNAME, or MX record is used for. Other lesser-known types of
DNS records also exist. One of them is the SRV record.
INTRODUCING SRV RECORDS
SRV records are used to point to hostnames and ports of servers providing a specific
service. Kubernetes creates SRV records to point to the hostnames of the pods back-
ing a headless service. 
 You’re going to list the SRV records for your stateful pods by running the dig DNS
lookup tool inside a new temporary pod. This is the command you’ll use:
$ kubectl run -it srvlookup --image=tutum/dnsutils --rm 
➥ --restart=Never -- dig SRV kubia.default.svc.cluster.local
The command runs a one-off pod (--restart=Never) called srvlookup, which is
attached to the console (-it) and is deleted as soon as it terminates (--rm). The
pod runs a single container from the tutum/dnsutils image and runs the following
command:
dig SRV kubia.default.svc.cluster.local
The following listing shows what the command prints out.
...
;; ANSWER SECTION:
k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.
k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.
;; ADDITIONAL SECTION:
kubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4
kubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6
...
NOTE
I’ve had to shorten the actual name to get records to fit into a single
line, so kubia.d.s.c.l is actually kubia.default.svc.cluster.local.
The ANSWER SECTION shows two SRV records pointing to the two pods backing your head-
less service. Each pod also gets its own A record, as shown in ADDITIONAL SECTION.
 For a pod to get a list of all the other pods of a StatefulSet, all you need to do is
perform an SRV DNS lookup. In Node.js, for example, the lookup is performed
like this:
dns.resolveSrv(""kubia.default.svc.cluster.local"", callBackFunction);
You’ll use this command in your app to enable each pod to discover its peers.
Listing 10.8
Listing DNS SRV records of your headless Service
 
",[],"[{'entity': 'SRV record', 'description': 'Used to point to hostnames and ports of servers providing a specific service.', 'category': 'network'}, {'entity': 'DNS', 'description': 'Domain Name System, used for resolving domain names to IP addresses.', 'category': 'network'}, {'entity': 'A record', 'description': 'Used to map a hostname to an IP address.', 'category': 'network'}, {'entity': 'CNAME record', 'description': 'Used to map one domain name to another.', 'category': 'network'}, {'entity': 'MX record', 'description': 'Used to specify the mail server responsible for a domain.', 'category': 'network'}, {'entity': 'dig DNS lookup tool', 'description': 'A command-line tool used to perform DNS lookups.', 'category': 'software'}, {'entity': 'kubectl run', 'description': 'A command used to create a new pod and run a command inside it.', 'category': 'command'}, {'entity': 'tutum/dnsutils image', 'description': 'A Docker image containing the DNS utilities.', 'category': 'container'}, {'entity': 'StatefulSet', 'description': 'A Kubernetes resource used to deploy stateful applications.', 'category': 'application'}, {'entity': 'headless service', 'description': 'A type of Kubernetes service that does not have an IP address assigned to it.', 'category': 'application'}, {'entity': 'Node.js', 'description': 'A JavaScript runtime environment used for server-side programming.', 'category': 'software'}, {'entity': 'dns.resolveSrv() function', 'description': 'A Node.js function used to perform an SRV DNS lookup.', 'category': 'function'}]","[{'source_entity': '""MX record""', 'description': 'provides a list of IP addresses that can be used to resolve a domain name', 'destination_entity': '""dns.resolveSrv() function""'}, {'source_entity': '""SRV record""', 'description': 'specifies the location of services within a DNS namespace', 'destination_entity': '""headless service""'}, {'source_entity': '""StatefulSet""', 'description': 'manages persistent data for pods, ensuring that each pod has access to the same data', 'destination_entity': '""Node.js""'}, {'source_entity': '""dns.resolveSrv() function""', 'description': 'resolves SRV records and returns a list of IP addresses', 'destination_entity': '""dig DNS lookup tool""'}, {'source_entity': '""kubectl run""', 'description': 'creates a new pod with the specified image, in this case ""tutum/dnsutils image""', 'destination_entity': '""tutum/dnsutils image""'}, {'source_entity': '""A record""', 'description': 'maps a domain name to an IP address', 'destination_entity': '""dig DNS lookup tool""'}, {'source_entity': '""CNAME record""', 'description': 'maps an alias for a domain name to the actual domain name', 'destination_entity': '""A record""'}, {'source_entity': '""headless service""', 'description': 'provides a way to access services without exposing them directly to the network', 'destination_entity': '""StatefulSet""'}, {'source_entity': '""DNS""', 'description': 'resolves domain names to IP addresses using various record types, including MX and SRV records', 'destination_entity': '""dig DNS lookup tool""'}]","['[\n  {\n    ""source"": ""MX record"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""provides a list of IP addresses that can be used to resolve a domain name"",\n    ""summary_er"": ""An MX record provides a list of IP addresses for resolving a domain name, which is essential for Kubernetes pods to communicate with each other.""\n  },\n  {\n    ""source"": ""dns.resolveSrv() function"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""resolves SRV records for a given domain name"",\n    ""summary_er"": ""The dns.resolveSrv() function resolves SRV records, which is crucial for Kubernetes pods to discover and communicate with each other.""\n  }\n]', '[\n  {\n    ""source"": ""DNS"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""SRV record"",\n    ""summary_er"": ""A DNS SRV record specifies the location of services within a namespace, allowing clients to discover and connect to pods.""\n  },\n  {\n    ""source"": ""Service"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""headless service"",\n    ""summary_er"": ""A headless service in Kubernetes allows clients to access a pod without creating a DNS entry for it, enabling more flexible service discovery and communication.""\n  }\n]', '[\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages persistent data for pods"",\n    ""summary_er"": ""StatefulSet ensures each pod has access to shared data.""\n  },\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""Node.js"",\n    ""relation_description"": ""uses Node.js as a runtime environment"",\n    ""summary_er"": ""StatefulSet utilizes Node.js for server-side logic execution.""\n  }\n]', '[\n  {\n    ""source"": ""dns.resolveSrv() function"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""resolves SRV records and returns a list of IP addresses"",\n    ""summary_er"": ""The dns.resolveSrv() function resolves SRV records, returning a list of IP addresses that can be used to communicate with a pod.""\n  },\n  {\n    ""source"": ""dig DNS lookup tool"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""performs DNS lookups"",\n    ""summary_er"": ""The dig DNS lookup tool performs DNS lookups, allowing it to identify and connect to a specific pod in the Kubernetes cluster.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl run"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""creates a new pod with the specified image"",\n    ""summary_er"": ""Kubectl creates a new pod using the specified Docker image, in this case \'tutum/dnsutils\'.""\n  },\n  {\n    ""source"": ""tutum/dnsutils image"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""is used as the image for the new pod"",\n    ""summary_er"": ""The \'tutum/dnsutils\' Docker image is used to create a new pod with kubectl.""\n  }\n]', '[\n  {\n    ""source"": ""A record"",\n    ""destination"": ""dig DNS lookup tool"",\n    ""relation_description"": ""maps a domain name to an IP address"",\n    ""summary_er"": ""A record maps a domain name to an IP address, which is used by dig DNS lookup tool for resolving domain names.""\n  }\n]', '[\n  {\n    ""source"": ""CNAME record"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""maps an alias for a domain name to the actual domain name"",\n    ""summary_er"": ""A CNAME record maps an alias domain name to its canonical name, similar to how a pod in Kubernetes is an abstraction of a containerized application.""\n  },\n  {\n    ""source"": ""A record"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""maps an IP address to the actual domain name"",\n    ""summary_er"": ""An A record maps an IP address to its corresponding domain name, similar to how a pod in Kubernetes is assigned an IP address for communication.""\n  }\n]', '[\n  {\n    ""source"": ""Headless Service"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""provides a way to access services without exposing them directly to the network"",\n    ""summary_er"": ""A Headless Service allows accessing services without direct network exposure, enabling communication between pods.""\n  },\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""manages stateful applications with persistent storage and predictable networking"",\n    ""summary_er"": ""A StatefulSet manages stateful apps with persistent storage and predictable networking, ensuring consistent pod behavior.""\n  }\n]', '[\n  {\n    ""source"": ""DNS"",\n    ""destination"": ""dig DNS lookup tool"",\n    ""relation_description"": ""resolves domain names to IP addresses using various record types, including MX and SRV records"",\n    ""summary_er"": ""DNS resolves domain names to IP addresses using various record types, including MX and SRV records, which is similar to the functionality of dig DNS lookup tool.""\n  }\n]']","A StatefulSet's pods can discover their peers by performing an SRV DNS lookup, which points to the hostnames and ports of servers providing a specific service. Kubernetes creates SRV records to point to the hostnames of the pods backing a headless service. A pod can get a list of all other pods by running the dig command inside a temporary pod.","[{'highlight': 'SRV records are used to point to hostnames and ports of servers providing a specific service.'}, {'highlight': 'Kubernetes creates SRV records to point to the hostnames of the pods backing a headless service.'}, {'highlight': 'Performing an SRV DNS lookup allows a pod to get a list of all the other pods of a StatefulSet.'}, {'highlight': 'The ANSWER SECTION shows two SRV records pointing to the two pods backing your headless service.'}, {'highlight': 'You can use dns.resolveSrv() in Node.js to perform an SRV DNS lookup and enable each pod to discover its peers.'}]"
244,333,0,[],"301
Discovering peers in a StatefulSet
NOTE
The order of the returned SRV records is random, because they all have
the same priority. Don’t expect to always see kubia-0 listed before kubia-1.
10.4.1 Implementing peer discovery through DNS
Your Stone Age data store isn’t clustered yet. Each data store node runs completely
independently of all the others—no communication exists between them. You’ll get
them talking to each other next.
 Data posted by clients connecting to your data store cluster through the kubia-
public Service lands on a random cluster node. The cluster can store multiple data
entries, but clients currently have no good way to see all those entries. Because ser-
vices forward requests to pods randomly, a client would need to perform many
requests until it hit all the pods if it wanted to get the data from all the pods. 
 You can improve this by having the node respond with data from all the cluster
nodes. To do this, the node needs to find all its peers. You’re going to use what you
learned about StatefulSets and SRV records to do this.
 You’ll modify your app’s source code as shown in the following listing (the full
source is available in the book’s code archive; the listing shows only the important
parts).
...
const dns = require('dns');
const dataFile = ""/var/data/kubia.txt"";
const serviceName = ""kubia.default.svc.cluster.local"";
const port = 8080;
...
var handler = function(request, response) {
  if (request.method == 'POST') {
    ...
  } else {
    response.writeHead(200);
    if (request.url == '/data') {
      var data = fileExists(dataFile) 
        ? fs.readFileSync(dataFile, 'utf8') 
        : ""No data posted yet"";
      response.end(data);
    } else {
      response.write(""You've hit "" + os.hostname() + ""\n"");
      response.write(""Data stored in the cluster:\n"");
      dns.resolveSrv(serviceName, function (err, addresses) {    
        if (err) {
          response.end(""Could not look up DNS SRV records: "" + err);
          return;
        }
        var numResponses = 0;
        if (addresses.length == 0) {
          response.end(""No peers discovered."");
        } else {
Listing 10.9
Discovering peers in a sample app: kubia-pet-peers-image/app.js
The app 
performs a DNS 
lookup to obtain 
SRV records.
 
",[],"[{'entity': 'StatefulSet', 'description': 'A Kubernetes resource that manages stateful applications', 'category': 'application'}, {'entity': 'DNS', 'description': 'A protocol for resolving hostnames to IP addresses', 'category': 'network'}, {'entity': 'SRV records', 'description': 'A type of DNS record that specifies a service and its associated port', 'category': 'network'}, {'entity': 'kubia-0', 'description': 'A pod in a Kubernetes cluster', 'category': 'container'}, {'entity': 'kubia-1', 'description': 'A pod in a Kubernetes cluster', 'category': 'container'}, {'entity': 'Stone Age data store', 'description': 'An unclustered, independent data storage system', 'category': 'database'}, {'entity': 'Service', 'description': 'A Kubernetes resource that provides a network interface for accessing applications', 'category': 'application'}, {'entity': 'kubia-public Service', 'description': 'A Kubernetes service that exposes the kubia application to clients', 'category': 'application'}, {'entity': 'dns.resolveSrv', 'description': 'A function in Node.js that performs a DNS SRV record lookup', 'category': 'library'}, {'entity': 'fs.readFileSync', 'description': 'A function in Node.js that reads a file synchronously', 'category': 'library'}, {'entity': 'os.hostname', 'description': 'A property of the os module in Node.js that returns the hostname of the machine', 'category': 'library'}, {'entity': 'handler', 'description': 'A function in the kubia-pet-peers-image/app.js file that handles incoming requests', 'category': 'application'}, {'entity': 'request', 'description': 'An object representing an HTTP request', 'category': 'network'}, {'entity': 'response', 'description': 'An object representing an HTTP response', 'category': 'network'}, {'entity': 'fileExists', 'description': 'A function in the kubia-pet-peers-image/app.js file that checks if a file exists', 'category': 'library'}]","[{'source_entity': 'dns.resolveSrv', 'description': 'resolves SRV records for Service', 'destination_entity': 'SRV records'}, {'source_entity': 'os.hostname', 'description': 'gets hostname from operating system', 'destination_entity': 'handler'}, {'source_entity': 'fileExists', 'description': 'checks if file exists on disk', 'destination_entity': 'Stone Age data store'}, {'source_entity': 'dns.resolveSrv', 'description': 'resolves SRV records for Service', 'destination_entity': 'Service'}, {'source_entity': 'handler', 'description': 'handles incoming requests from clients', 'destination_entity': 'request'}, {'source_entity': 'fs.readFileSync', 'description': 'reads file synchronously from disk', 'destination_entity': 'Stone Age data store'}, {'source_entity': 'kubia-public Service', 'description': 'provides public API for Kubia application', 'destination_entity': 'response'}, {'source_entity': 'request', 'description': 'represents incoming HTTP request from client', 'destination_entity': 'handler'}]","['[\n  {\n    ""source"": ""dns.resolveSrv"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""resolves SRV records for Service"",\n    ""summary_er"": ""DNS resolves SRV records to find service endpoints in a Kubernetes cluster.""\n  }\n]', '[\n  {\n    ""source"": ""os.hostname"",\n    ""destination"": ""pod.handler"",\n    ""relation_description"": ""gets hostname from operating system"",\n    ""summary_er"": ""The pod handler retrieves the hostname from the operating system using os.hostname.""\n  }\n]', '[\n  {\n    ""source"": ""fileExists"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""checks if file exists on disk"",\n    ""summary_er"": ""The fileExists relation checks if a file exists on disk, ensuring data integrity for the Stone Age data store pod.""\n  }\n]', '[\n  {\n    ""source"": ""dns.resolveSrv"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""resolves SRV records for Service"",\n    ""summary_er"": ""DNS resolver service resolves SRV records to find a pod providing a specific service.""\n  }\n]', '[\n  {\n    ""source"": ""handler"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""handles incoming requests from clients"",\n    ""summary_er"": ""The handler receives client requests, which are then processed by a pod in the Kubernetes cluster.""\n  }\n]', '[\n  {\n    ""source"": ""fs.readFileSync"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""reads file synchronously from disk"",\n    ""summary_er"": ""The fs.readFileSync function reads a file synchronously from disk, which can be used to store data in a pod.""\n  }\n]', '[\n  {\n    ""source"": ""kubia-public Service"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""provides public API for Kubia application"",\n    ""summary_er"": ""The kubia-public Service provides a public API to interact with the Kubia application, which is running in a pod.""\n  }\n]', '[\n  {\n    ""source"": ""request"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""represents incoming HTTP request from client"",\n    ""summary_er"": ""The incoming HTTP request from a client is handled by a pod in Kubernetes, which processes and responds to the request.""\n  }\n]']","To discover peers in a StatefulSet, the application performs a DNS lookup using the `dns.resolveSrv` method. It queries for SRV records for a specific service name and retrieves a list of addresses. If no peers are found, it returns 'No peers discovered.' Otherwise, it displays data from all cluster nodes.","[{'highlight': 'You can improve the data store by having each node respond with data from all cluster nodes, which requires the node to find its peers.'}, {'highlight': ""To discover peers, you'll use what you learned about StatefulSets and SRV records, modifying your app's source code to perform a DNS lookup for SRV records.""}, {'highlight': ""The order of returned SRV records is random due to equal priority, so don't expect kubia-0 to always be listed before kubia-1.""}, {'highlight': ""You'll need to modify your app's source code as shown in the listing (available in the book's code archive) to implement peer discovery through DNS.""}, {'highlight': 'The modified app performs a DNS lookup for SRV records using the dns.resolveSrv function, which returns an array of addresses that can be used to discover peers.'}]"
245,334,0,[],"302
CHAPTER 10
StatefulSets: deploying replicated stateful applications
          addresses.forEach(function (item) {                   
            var requestOptions = {
              host: item.name, 
              port: port, 
              path: '/data'
            };
            httpGet(requestOptions, function (returnedData) {   
              numResponses++;
              response.write(""- "" + item.name + "": "" + returnedData);
              response.write(""\n"");
              if (numResponses == addresses.length) {
                response.end();
              }
            });
          });
        }
      });
    }
  }
};
...
Figure 10.12 shows what happens when a GET request is received by your app. The
server that receives the request first performs a lookup of SRV records for the head-
less kubia service and then sends a GET request to each of the pods backing the ser-
vice (even to itself, which obviously isn’t necessary, but I wanted to keep the code as
simple as possible). It then returns a list of all the nodes along with the data stored on
each of them.
The container image containing this new version of the app is available at docker.io/
luksa/kubia-pet-peers.
10.4.2 Updating a StatefulSet
Your StatefulSet is already running, so let’s see how to update its pod template so the
pods use the new image. You’ll also set the replica count to 3 at the same time. To
Each pod 
pointed to by 
an SRV record is 
then contacted 
to get its data.
curl
DNS
1. GET /
4. GET /data
5. GET /data
2. SRV lookup
6. Return collated data
kubia-0
kubia-1
kubia-2
3. GET /data
Figure 10.12
The operation of your simplistic distributed data store
 
",[],"[{'entity': 'StatefulSets', 'description': 'deploying replicated stateful applications', 'category': 'software'}, {'entity': 'addresses.forEach', 'description': 'iterating over a list of addresses', 'category': 'javascript'}, {'entity': 'httpGet', 'description': 'making an HTTP GET request', 'category': 'javascript'}, {'entity': 'requestOptions', 'description': 'object containing options for the HTTP request', 'category': 'javascript'}, {'entity': 'host', 'description': 'property of requestOptions object, specifying the host to connect to', 'category': 'javascript'}, {'entity': 'port', 'description': 'property of requestOptions object, specifying the port to use', 'category': 'javascript'}, {'entity': 'path', 'description': 'property of requestOptions object, specifying the path to request', 'category': 'javascript'}, {'entity': 'returnedData', 'description': 'data returned from the HTTP GET request', 'category': 'javascript'}, {'entity': 'numResponses', 'description': 'counter for tracking the number of responses received', 'category': 'javascript'}, {'entity': 'response.write', 'description': 'method for writing output to a response object', 'category': 'javascript'}, {'entity': 'response.end', 'description': 'method for ending a response object', 'category': 'javascript'}, {'entity': 'docker.io/luksa/kubia-pet-peers', 'description': 'container image URL', 'category': 'software'}, {'entity': 'StatefulSet', 'description': 'resource in Kubernetes, managing stateful applications', 'category': 'software'}, {'entity': 'pod template', 'description': 'template for creating pods in a StatefulSet', 'category': 'software'}, {'entity': 'replica count', 'description': 'number of replicas to maintain in a StatefulSet', 'category': 'software'}, {'entity': 'SRV records', 'description': 'DNS resource records, used for service discovery', 'category': 'network'}, {'entity': 'GET /data', 'description': 'HTTP request method and path', 'category': 'http'}, {'entity': 'curl', 'description': 'command-line tool for making HTTP requests', 'category': 'software'}]","[{'source_entity': '""httpGet""', 'description': 'performs a GET request', 'destination_entity': '""response.write""'}, {'source_entity': '""httpGet""', 'description': 'sends data to be written to the response', 'destination_entity': '""response.end""'}, {'source_entity': '""requestOptions""', 'description': 'specifies options for the HTTP request', 'destination_entity': '""httpGet""'}, {'source_entity': '""SRV records""', 'description': 'provides service records for DNS resolution', 'destination_entity': '""StatefulSet""'}, {'source_entity': '""response.end""', 'description': 'terminates the HTTP response', 'destination_entity': '""httpGet""'}, {'source_entity': '""path""', 'description': 'specifies the URL path for the GET request', 'destination_entity': '""httpGet""'}, {'source_entity': '""pod template""', 'description': 'defines a template for creating pods', 'destination_entity': '""StatefulSet""'}, {'source_entity': '""host""', 'description': 'specifies the hostname for the HTTP request', 'destination_entity': '""httpGet""'}, {'source_entity': '""port""', 'description': 'specifies the port number for the HTTP request', 'destination_entity': '""httpGet""'}, {'source_entity': '""numResponses""', 'description': 'specifies the number of responses to return', 'destination_entity': '""returnedData""'}, {'source_entity': '""addresses.forEach""', 'description': 'iterates over a list of addresses', 'destination_entity': '""SRV records""'}]","['[\n  {\n    ""source"": ""Kubernetes"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""HTTP Request"",\n    ""summary_er"": ""In Kubernetes, an HTTP GET request is performed to interact with a pod.""\n  },\n  {\n    ""source"": ""Docker"",\n    ""destination"": ""Container"",\n    ""relation_description"": ""Image Pulling"",\n    ""summary_er"": ""Docker pulls images from a registry to create containers for deployment.""\n  },\n  {\n    ""source"": ""Machine Learning"",\n    ""destination"": ""Model"",\n    ""relation_description"": ""Training Data"",\n    ""summary_er"": ""In Machine Learning, training data is used to train models for prediction and classification.""\n  },\n  {\n    ""source"": ""Generative AI"",\n    ""destination"": ""Text Generation"",\n    ""relation_description"": ""Language Model"",\n    ""summary_er"": ""Generative AI uses language models to generate human-like text for various applications.""\n  },\n  {\n    ""source"": ""Natural Language Understanding"",\n    ""destination"": ""Sentiment Analysis"",\n    ""relation_description"": ""Text Classification"",\n    ""summary_er"": ""NLU performs text classification to determine sentiment and intent from user input.""\n  },\n  {\n    ""source"": ""Computer Vision"",\n    ""destination"": ""Image Recognition"",\n    ""relation_description"": ""Object Detection"",\n    ""summary_er"": ""CV uses object detection algorithms to identify objects within images for various applications.""\n  }\n]', '[\n  {\n    ""source"": ""httpGet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""sends data to be written to the response"",\n    ""summary_er"": ""HTTP GET request sends data from pod to client, which is then written to response.""\n  },\n  {\n    ""source"": ""response.end"",\n    ""destination"": ""httpGet"",\n    ""relation_description"": ""terminates HTTP response"",\n    ""summary_er"": ""Response.end terminates the HTTP response sent by httpGet, ending the request cycle.""\n  }\n]', '[\n  {\n    ""source"": ""requestOptions"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""specifies options for the HTTP request"",\n    ""summary_er"": ""The requestOptions entity specifies HTTP request options, which are used to configure an HTTP GET request in a pod.""\n  }\n]', '[\n  {\n    ""source"": ""SRV records"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""provides service records for DNS resolution"",\n    ""summary_er"": ""SRV records provide service records for DNS resolution, which are used by pods to resolve domain names and access services.""\n  },\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages stateful applications"",\n    ""summary_er"": ""A StatefulSet is a Kubernetes resource that manages stateful applications, which are deployed as pods to provide persistent storage and consistent network identity.""\n  }\n]', '[\n  {\n    ""source"": ""response.end"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""terminates the HTTP response"",\n    ""summary_er"": ""The Kubernetes Pod terminates the HTTP response using the \'end\' attribute.""\n  },\n  {\n    ""source"": ""httpGet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""HTTP request to a pod"",\n    ""summary_er"": ""The \'httpGet\' command sends an HTTP request to a running Kubernetes Pod.""\n  }\n]', '[\n  {\n    ""source"": ""path"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""specifies the URL path for the GET request"",\n    ""summary_er"": ""The path attribute specifies the URL path for a GET request to be sent to a pod in a Kubernetes cluster.""\n  },\n  {\n    ""source"": ""httpGet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""executes an HTTP GET request on a pod"",\n    ""summary_er"": ""The httpGet command executes an HTTP GET request on a running pod, allowing for inspection and debugging of the pod\'s behavior.""\n  }\n]', '[\n  {\n    ""source"": ""Pod Template"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""defines a template for creating pods"",\n    ""summary_er"": ""A Pod Template defines a blueprint for creating Pods, specifying container configurations and settings.""\n  },\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""manages stateful applications"",\n    ""summary_er"": ""A StatefulSet manages a set of identical Pods, ensuring each Pod maintains its own state across restarts and updates.""\n  }\n]', '[\n  {\n    ""source"": ""host"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""specifies the hostname for the HTTP request"",\n    ""summary_er"": ""The host field specifies the hostname or IP address of a pod to be used as the target for an HTTP GET request.""\n  }\n]', '[\n  {\n    ""source"": ""port"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""specifies the port number for the HTTP request"",\n    ""summary_er"": ""The port entity specifies the port number used by a pod to receive HTTP requests.""\n  },\n  {\n    ""source"": ""httpGet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""executes an HTTP GET request on the pod"",\n    ""summary_er"": ""The httpGet action executes an HTTP GET request on a pod, allowing for inspection and debugging of the container\'s network behavior.""\n  }\n]', '[\n  {\n    ""source"": ""numResponses"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""specifies the number of responses to return"",\n    ""summary_er"": ""The numResponses parameter determines how many responses a pod will return, affecting data retrieval efficiency.""\n  },\n  {\n    ""source"": ""returnedData"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""data returned by the pod"",\n    ""summary_er"": ""ReturnedData is the output generated by a pod, containing the results of its processing and computations.""\n  }\n]', '[\n  {\n    ""source"": ""addresses"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""iterates over a list of addresses"",\n    ""summary_er"": ""The Kubernetes controller iterates over a list of addresses to manage pod resources.""\n  },\n  {\n    ""source"": ""SRV records"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""DNS service discovery"",\n    ""summary_er"": ""Kubernetes uses SRV records for DNS-based service discovery and load balancing in pods.""\n  }\n]']","StatefulSets are used to deploy replicated stateful applications, allowing for multiple pods to be created with a specified replica count. The pod template can be updated using kubectl, and the process involves creating a new image with the desired changes and then applying it to the StatefulSet, which will automatically update each pod with the new image.","[{'highlight': 'A GET request to a headless kubia service performs a lookup of SRV records for each pod backing the service.'}, {'highlight': 'Each pod pointed to by an SRV record is contacted to get its data.'}, {'highlight': 'The container image containing the new version of the app is available at docker.io/luksa/kubia-pet-peers.'}, {'highlight': 'To update a StatefulSet, you can modify its pod template and set the replica count to 3.'}, {'highlight': 'A GET request to /data returns a list of all nodes along with the data stored on each node.'}]"
246,335,0,[],"303
Discovering peers in a StatefulSet
update the StatefulSet, use the kubectl edit command (the patch command would
be another option):
$ kubectl edit statefulset kubia
This opens the StatefulSet definition in your default editor. In the definition, change
spec.replicas to 3 and modify the spec.template.spec.containers.image attri-
bute so it points to the new image (luksa/kubia-pet-peers instead of luksa/kubia-
pet). Save the file and exit the editor to update the StatefulSet. Two replicas were
running previously, so you should now see an additional replica called kubia-2 start-
ing. List the pods to confirm:
$ kubectl get po
NAME      READY     STATUS              RESTARTS   AGE
kubia-0   1/1       Running             0          25m
kubia-1   1/1       Running             0          26m
kubia-2   0/1       ContainerCreating   0          4s
The new pod instance is running the new image. But what about the existing two rep-
licas? Judging from their age, they don’t seem to have been updated. This is expected,
because initially, StatefulSets were more like ReplicaSets and not like Deployments,
so they don’t perform a rollout when the template is modified. You need to delete
the replicas manually and the StatefulSet will bring them up again based on the new
template:
$ kubectl delete po kubia-0 kubia-1
pod ""kubia-0"" deleted
pod ""kubia-1"" deleted
NOTE
Starting from Kubernetes version 1.7, StatefulSets support rolling
updates the same way Deployments and DaemonSets do. See the StatefulSet’s
spec.updateStrategy field documentation using kubectl explain for more
information.
10.4.3 Trying out your clustered data store
Once the two pods are up, you can see if your shiny new Stone Age data store works as
expected. Post a few requests to the cluster, as shown in the following listing.
$ curl -X POST -d ""The sun is shining"" \
➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
Data stored on pod kubia-1
$ curl -X POST -d ""The weather is sweet"" \
➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
Data stored on pod kubia-0
Now, read the stored data, as shown in the following listing.
Listing 10.10
Writing to the clustered data store through the service
 
",[],"[{'entity': 'StatefulSet', 'description': 'A Kubernetes resource that manages a set of replicas with persistent identities.', 'category': 'application'}, {'entity': 'kubectl edit', 'description': 'A command used to update a StatefulSet by editing its definition in the default editor.', 'category': 'command'}, {'entity': 'patch', 'description': 'An option for updating a StatefulSet using a patch file.', 'category': 'command'}, {'entity': 'spec.replicas', 'description': 'A field in the StatefulSet definition that specifies the number of replicas to run.', 'category': 'field'}, {'entity': 'spec.template.spec.containers.image', 'description': 'A field in the StatefulSet definition that specifies the image to use for each replica.', 'category': 'field'}, {'entity': 'kubectl get po', 'description': 'A command used to list all pods in a cluster.', 'category': 'command'}, {'entity': 'pod', 'description': 'A Kubernetes resource that represents a running container.', 'category': 'application'}, {'entity': 'StatefulSet updateStrategy', 'description': 'A field in the StatefulSet definition that specifies how to update the set of replicas.', 'category': 'field'}, {'entity': 'curl', 'description': 'A command used to send HTTP requests to a server.', 'category': 'command'}, {'entity': 'POST', 'description': 'An HTTP method used to create or update resources on the server.', 'category': 'protocol'}, {'entity': 'localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/', 'description': 'A URL that specifies the endpoint for sending requests to a service in a cluster.', 'category': 'url'}, {'entity': 'pod kubia-0', 'description': 'A specific pod instance running on a node in the cluster.', 'category': 'application'}, {'entity': 'pod kubia-1', 'description': 'A specific pod instance running on a node in the cluster.', 'category': 'application'}]","[{'source_entity': 'patch', 'description': 'applies updates to', 'destination_entity': 'StatefulSet'}, {'source_entity': 'kubectl get po', 'description': 'fetches information about', 'destination_entity': 'pod kubia-0'}, {'source_entity': 'spec.template.spec.containers.image', 'description': 'defines the image used by', 'destination_entity': 'pod kubia-0'}, {'source_entity': 'POST', 'description': 'sends a request to', 'destination_entity': 'localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/'}, {'source_entity': 'spec.replicas', 'description': 'defines the number of replicas for', 'destination_entity': 'StatefulSet'}, {'source_entity': 'curl', 'description': 'sends a request to', 'destination_entity': 'localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/'}, {'source_entity': 'kubectl edit', 'description': 'edits the configuration of', 'destination_entity': 'pod kubia-1'}, {'source_entity': 'StatefulSet updateStrategy', 'description': 'defines the strategy for updating', 'destination_entity': 'StatefulSet'}]","['[\n  {\n    ""source"": ""patch"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""applies updates to"",\n    ""summary_er"": ""A patch file is applied to a running pod in a Kubernetes cluster, updating its configuration.""\n  },\n  {\n    ""source"": ""patch"",\n    ""destination"": ""StatefulSet"",\n    ""relation_description"": ""applies updates to"",\n    ""summary_er"": ""A patch file is applied to a StatefulSet in a Kubernetes cluster, updating its configuration and scaling.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""fetches information about"",\n    ""summary_er"": ""Kubectl command fetches details of a specific pod, such as its status and container logs.""\n  },\n  {\n    ""source"": ""pod"",\n    ""destination"": ""kubia-0"",\n    ""relation_description"": ""information about"",\n    ""summary_er"": ""Pod kubia-0 contains the Kubia web server application, running in a Docker container.""\n  }\n]', '[\n  {\n    ""source"": ""spec.template.spec.containers.image"",\n    ""destination"": ""pod kubia-0"",\n    ""relation_description"": ""defines the image used by"",\n    ""summary_er"": ""The image for pod kubia-0 is defined in spec.template.spec.containers.""\n  }\n]', '[\n  {\n    ""source"": ""POST"",\n    ""destination"": ""localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/"",\n    ""relation_description"": ""sends a request to"",\n    ""summary_er"": ""A POST request is sent from the client to the Kubia service proxy, which handles incoming requests and directs them to the appropriate pod.""\n  }\n]', '[\n  {\n    ""source"": ""spec.replicas"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""defines the number of replicas for"",\n    ""summary_er"": ""The spec.replicas field determines the number of pod replicas to run in a StatefulSet.""\n  }\n]', '[\n  {\n    ""source"": ""curl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""sends a request to"",\n    ""summary_er"": ""A curl command sends an HTTP request to a pod\'s service proxy endpoint, allowing communication between the client and server.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl edit"",\n    ""destination"": ""pod kubia-1"",\n    ""relation_description"": ""edits the configuration of"",\n    ""summary_er"": ""Edits the pod\'s configuration using kubectl edit command, targeting the \'kubia-1\' pod.""\n  }\n]', '[\n  {\n    ""source"": ""StatefulSet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""updateStrategy"",\n    ""summary_er"": ""The update strategy of a StatefulSet defines how to update its pods, ensuring consistent and reliable state management.""\n  }\n]']","To update a StatefulSet, edit its definition using `kubectl edit` and modify the spec.replicas and image attributes. Save the file and exit to apply changes. If existing replicas are not updated, delete them manually for the StatefulSet to bring up new ones based on the new template.","[{'highlight': 'To update a StatefulSet, use the kubectl edit command or patch command and modify spec.replicas and spec.template.spec.containers.image attribute.'}, {'highlight': 'StatefulSets do not perform a rollout when the template is modified, so existing replicas need to be deleted manually for the new template to take effect.'}, {'highlight': 'Starting from Kubernetes version 1.7, StatefulSets support rolling updates like Deployments and DaemonSets.'}, {'highlight': 'To test a clustered data store, post requests to the cluster using curl commands and verify data storage on each pod.'}, {'highlight': 'StatefulSets were initially more like ReplicaSets than Deployments, but with Kubernetes version 1.7, they support rolling updates like Deployments and DaemonSets.'}]"
247,336,0,[],"304
CHAPTER 10
StatefulSets: deploying replicated stateful applications
$ curl localhost:8001/api/v1/namespaces/default/services
➥ /kubia-public/proxy/
You've hit kubia-2
Data stored on each cluster node:
- kubia-0.kubia.default.svc.cluster.local: The weather is sweet
- kubia-1.kubia.default.svc.cluster.local: The sun is shining
- kubia-2.kubia.default.svc.cluster.local: No data posted yet
Nice! When a client request reaches one of your cluster nodes, it discovers all its
peers, gathers data from them, and sends all the data back to the client. Even if you
scale the StatefulSet up or down, the pod servicing the client’s request can always find
all the peers running at that time. 
 The app itself isn’t that useful, but I hope you found it a fun way to show how
instances of a replicated stateful app can discover their peers and handle horizontal
scaling with ease.
10.5
Understanding how StatefulSets deal with node 
failures
In section 10.2.4 we stated that Kubernetes must be absolutely sure that a stateful
pod is no longer running before creating its replacement. When a node fails
abruptly, Kubernetes can’t know the state of the node or its pods. It can’t know
whether the pods are no longer running, or if they still are and are possibly even still
reachable, and it’s only the Kubelet that has stopped reporting the node’s state to
the master.
 Because a StatefulSet guarantees that there will never be two pods running with
the same identity and storage, when a node appears to have failed, the StatefulSet can-
not and should not create a replacement pod until it knows for certain that the pod is
no longer running. 
 It can only know that when the cluster administrator tells it so. To do that, the
admin needs to either delete the pod or delete the whole node (doing so then deletes
all the pods scheduled to the node).
 As your final exercise in this chapter, you’ll look at what happens to StatefulSets
and their pods when one of the cluster nodes gets disconnected from the network.
10.5.1 Simulating a node’s disconnection from the network 
As in chapter 4, you’ll simulate the node disconnecting from the network by shutting
down the node’s eth0 network interface. Because this example requires multiple
nodes, you can’t run it on Minikube. You’ll use Google Kubernetes Engine instead.
SHUTTING DOWN THE NODE’S NETWORK ADAPTER
To shut down a node’s eth0 interface, you need to ssh into one of the nodes like this:
$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1
Listing 10.11
Reading from the data store
 
",[],"[{'entity': 'StatefulSets', 'description': 'deploying replicated stateful applications', 'category': 'application'}, {'entity': 'Kubernetes', 'description': 'cluster management system', 'category': 'software'}, {'entity': 'curl', 'description': 'command-line tool for transferring data', 'category': 'command'}, {'entity': '/kubia-public/proxy/', 'description': 'service endpoint', 'category': 'endpoint'}, {'entity': 'kubia-0.kubia.default.svc.cluster.local', 'description': 'pod name', 'category': 'pod'}, {'entity': 'kubia-1.kubia.default.svc.cluster.local', 'description': 'pod name', 'category': 'pod'}, {'entity': 'kubia-2.kubia.default.svc.cluster.local', 'description': 'pod name', 'category': 'pod'}, {'entity': 'StatefulSet', 'description': 'guarantees that there will never be two pods running with the same identity and storage', 'category': 'application'}, {'entity': 'Kubelet', 'description': 'component responsible for node management', 'category': 'software'}, {'entity': 'node', 'description': 'cluster node', 'category': 'hardware'}, {'entity': 'eth0', 'description': 'network interface', 'category': 'hardware'}, {'entity': 'Google Kubernetes Engine', 'description': 'cloud-based Kubernetes service', 'category': 'software'}, {'entity': 'ssh', 'description': 'command-line tool for secure shell access', 'category': 'command'}, {'entity': 'gcloud', 'description': 'command-line tool for Google Cloud Platform management', 'category': 'command'}]","[{'source_entity': 'gcloud', 'description': 'deploys', 'destination_entity': 'StatefulSet'}, {'source_entity': 'Google Kubernetes Engine', 'description': 'manages', 'destination_entity': 'Kubernetes'}, {'source_entity': 'ssh', 'description': 'connects to', 'destination_entity': 'kubia-2.kubia.default.svc.cluster.local'}, {'source_entity': 'curl', 'description': 'requests data from', 'destination_entity': '/kubia-public/proxy/'}, {'source_entity': 'Kubelet', 'description': 'manages', 'destination_entity': 'node'}, {'source_entity': 'StatefulSets', 'description': 'orchestrates', 'destination_entity': 'kubia-0.kubia.default.svc.cluster.local'}, {'source_entity': 'Kubernetes', 'description': 'manages', 'destination_entity': 'eth0'}, {'source_entity': 'StatefulSets', 'description': 'orchestrates', 'destination_entity': 'kubia-1.kubia.default.svc.cluster.local'}]","['[\n  {\n    ""source"": ""gcloud"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""deploys"",\n    ""summary_er"": ""Google Cloud deploys pods, which are ephemeral containers that can be scaled and managed by Kubernetes.""\n  },\n  {\n    ""source"": ""gcloud"",\n    ""destination"": ""StatefulSet"",\n    ""relation_description"": ""deploys"",\n    ""summary_er"": ""Google Cloud deploys StatefulSets, which are a type of Kubernetes object that manages stateful applications.""\n  }\n]', '[\n  {\n    ""source"": ""Google Kubernetes Engine"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages"",\n    ""summary_er"": ""Google Kubernetes Engine manages pods, providing a scalable and secure environment for containerized applications.""\n  }\n]', '[\n  {\n    ""source"": ""ssh"",\n    ""destination"": ""kubia-2.kubia.default.svc.cluster.local"",\n    ""relation_description"": ""connects to"",\n    ""summary_er"": ""SSH connects kubia-2 pod to Kubia service in default namespace.""\n  }\n]', '[\n  {\n    ""source"": ""curl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""requests data from"",\n    ""summary_er"": ""Curl command sends HTTP requests to a pod\'s proxy endpoint /kubia-public/proxy/""\n  }\n]', '[\n  {\n    ""source"": ""Kubelet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages"",\n    ""summary_er"": ""The Kubelet component manages pods on a node, ensuring they are running and healthy.""\n  }\n]', '[\n  {\n    ""source"": ""StatefulSets"",\n    ""destination"": ""pod kubia-0.kubia.default.svc.cluster.local"",\n    ""relation_description"": ""orchestrates"",\n    ""summary_er"": ""StatefulSets manages and orchestrates a pod, ensuring its stable and consistent execution within the Kubernetes cluster.""\n  }\n]', '[\n  {\n    ""source"": ""Kubernetes"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages"",\n    ""summary_er"": ""Kubernetes manages a pod, which is a container that runs an application.""\n  }\n]', '[\n  {\n    ""source"": ""StatefulSets"",\n    ""destination"": ""pod kubia-1.kubia.default.svc.cluster.local"",\n    ""relation_description"": ""orchestrates"",\n    ""summary_er"": ""StatefulSets manages and orchestrates the deployment of a pod, ensuring its lifecycle is properly managed within the Kubernetes cluster.""\n  }\n]']","StatefulSets in Kubernetes allow for replicated stateful applications to be deployed, where each pod has a unique identity and storage. When scaling up or down, pods can discover peers and handle horizontal scaling with ease. However, when a node fails abruptly, StatefulSet cannot create a replacement pod until it knows the old pod is no longer running, which requires manual intervention by the cluster administrator. This can be observed by simulating a node's disconnection from the network by shutting down its eth0 interface.","[{'highlight': 'StatefulSets guarantee that there will never be two pods running with the same identity and storage, which allows them to handle node failures and horizontal scaling with ease.'}, {'highlight': ""When a node fails abruptly, Kubernetes can't know the state of the node or its pods, so StatefulSet can't create a replacement pod until it knows for certain that the pod is no longer running.""}, {'highlight': 'To handle node failures, cluster administrators need to either delete the pod or delete the whole node, which deletes all the pods scheduled to the node.'}, {'highlight': ""Simulating a node's disconnection from the network by shutting down its eth0 network interface requires multiple nodes and can't be run on Minikube; Google Kubernetes Engine is recommended instead.""}, {'highlight': 'StatefulSets allow instances of a replicated stateful app to discover their peers and handle horizontal scaling with ease, making them useful for applications that require consistent data storage across multiple pods.'}]"
248,337,0,[],"305
Understanding how StatefulSets deal with node failures
Then, inside the node, run the following command:
$ sudo ifconfig eth0 down
Your ssh session will stop working, so you’ll need to open another terminal to continue.
CHECKING THE NODE’S STATUS AS SEEN BY THE KUBERNETES MASTER
With the node’s network interface down, the Kubelet running on the node can no
longer contact the Kubernetes API server and let it know that the node and all its pods
are still running.
 After a while, the control plane will mark the node as NotReady. You can see this
when listing nodes, as the following listing shows.
$ kubectl get node
NAME                                   STATUS     AGE       VERSION
gke-kubia-default-pool-32a2cac8-596v   Ready      16m       v1.6.2
gke-kubia-default-pool-32a2cac8-m0g1   NotReady   16m       v1.6.2
gke-kubia-default-pool-32a2cac8-sgl7   Ready      16m       v1.6.2
Because the control plane is no longer getting status updates from the node, the
status of all pods on that node is Unknown. This is shown in the pod list in the follow-
ing listing.
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
kubia-0   1/1       Unknown   0          15m
kubia-1   1/1       Running   0          14m
kubia-2   1/1       Running   0          13m
As you can see, the kubia-0 pod’s status is no longer known because the pod was (and
still is) running on the node whose network interface you shut down.
UNDERSTANDING WHAT HAPPENS TO PODS WHOSE STATUS IS UNKNOWN
If the node were to come back online and report its and its pod statuses again, the pod
would again be marked as Running. But if the pod’s status remains unknown for more
than a few minutes (this time is configurable), the pod is automatically evicted from
the node. This is done by the master (the Kubernetes control plane). It evicts the pod
by deleting the pod resource. 
 When the Kubelet sees that the pod has been marked for deletion, it starts ter-
minating the pod. In your case, the Kubelet can no longer reach the master (because
you disconnected the node from the network), which means the pod will keep
running.
Listing 10.12
Observing a failed node’s status change to NotReady
Listing 10.13
Observing the pod’s status change after its node becomes NotReady
 
",[],"[{'entity': 'StatefulSets', 'description': 'A Kubernetes resource that manages stateful applications.', 'category': 'software'}, {'entity': 'node failures', 'description': 'An event where a node in the Kubernetes cluster fails or becomes unavailable.', 'category': 'hardware'}, {'entity': '$ sudo ifconfig eth0 down', 'description': 'A Linux command that shuts down a network interface.', 'category': 'command'}, {'entity': 'Kubelet', 'description': ""A Kubernetes component that runs on each node and is responsible for managing the node's pods."", 'category': 'software'}, {'entity': 'Kubernetes API server', 'description': 'The central component of a Kubernetes cluster that manages all resources and provides an interface to interact with them.', 'category': 'software'}, {'entity': 'NotReady', 'description': 'A status that the control plane assigns to a node when it is no longer receiving updates from the node.', 'category': 'status'}, {'entity': '$ kubectl get node', 'description': 'A Kubernetes command that lists all nodes in the cluster.', 'category': 'command'}, {'entity': '$ kubectl get po', 'description': 'A Kubernetes command that lists all pods in the cluster.', 'category': 'command'}, {'entity': 'pod', 'description': 'A lightweight and portable container that runs a single instance of an application.', 'category': 'software'}, {'entity': 'Unknown', 'description': 'A status that the control plane assigns to a pod when it is no longer receiving updates from the node.', 'category': 'status'}, {'entity': 'evicted', 'description': 'An event where a pod is automatically deleted by the master due to its unknown status for an extended period.', 'category': 'process'}, {'entity': 'Kubelet', 'description': ""A Kubernetes component that runs on each node and is responsible for managing the node's pods."", 'category': 'software'}]","[{'source_entity': '""evicted""', 'description': 'causes', 'destination_entity': '""node failures""'}, {'source_entity': '""Kubernetes API server""', 'description': 'is responsible for', 'destination_entity': '""pod""'}, {'source_entity': '""Kubernetes API server""', 'description': 'manages', 'destination_entity': '""StatefulSets""'}, {'source_entity': '""Unknown""', 'description': 'uses', 'destination_entity': '""$ sudo ifconfig eth0 down""'}, {'source_entity': '""Kubelet""', 'description': 'communicates with', 'destination_entity': '""Kubernetes API server""'}, {'source_entity': '""$ kubectl get po""', 'description': 'requests information from', 'destination_entity': '""Kubernetes API server""'}, {'source_entity': '""$ kubectl get node""', 'description': 'requests information from', 'destination_entity': '""Kubernetes API server""'}, {'source_entity': '""NotReady""', 'description': 'indicates the state of', 'destination_entity': '""node failures""'}]","['[\n  {\n    ""source"": ""pod"",\n    ""destination"": ""node"",\n    ""relation_description"": ""causes"",\n    ""summary_er"": ""Pod eviction can cause node failures due to resource constraints or other issues, leading to potential downtime and impact on application performance.""\n  },\n  {\n    ""source"": ""node failures"",\n    ""destination"": ""pod evictions"",\n    ""relation_description"": ""result of"",\n    ""summary_er"": ""Node failures can result in pod evictions, as the node\'s resources are no longer available to run the pods, leading to potential data loss or corruption.""\n  }\n]', '[\n  {\n    ""source"": ""Kubernetes API server"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""is responsible for"",\n    ""summary_er"": ""The Kubernetes API server manages and controls pods, ensuring they run as expected.""\n  }\n]', '[\n  {\n    ""source"": ""Kubernetes API server"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages"",\n    ""summary_er"": ""The Kubernetes API server manages a pod, ensuring its lifecycle and configuration.""\n  },\n  {\n    ""source"": ""StatefulSets"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""manages"",\n    ""summary_er"": ""A StatefulSet manages a pod, providing stable network identity and persistent storage.""\n  }\n]', '[{\n  ""source"": ""Unknown"",\n  ""destination"": ""Pod"",\n  ""relation_description"": ""uses"",\n  ""summary_er"": ""The unknown entity utilizes a pod for its operation.""\n}]', '[\n  {\n    ""source"": ""Kubelet"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""communicates with"",\n    ""summary_er"": ""The Kubelet communicates with pods to manage and monitor their lifecycle.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""requests information from"",\n    ""summary_er"": ""The Kubernetes command-line tool (kubectl) sends a request to the API server to retrieve information about a pod.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""node"",\n    ""relation_description"": ""requests information from"",\n    ""summary_er"": ""The Kubernetes command-line tool (kubectl) retrieves node information, such as its status and resources.""\n  },\n  {\n    ""source"": ""Kubernetes API server"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""requests information from"",\n    ""summary_er"": ""The Kubernetes API server receives requests for pod information, including its configuration and status.""\n  }\n]', '[\n  {\n    ""source"": ""Pod"",\n    ""destination"": ""Node"",\n    ""relation_description"": ""NotReady"",\n    ""summary_er"": ""A Pod is in a NotReady state due to node failures, indicating that the pod\'s container(s) are not yet running on a healthy node.""\n  }\n]']","StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown. If the node comes back online, the pod is marked as Running again. However, if the pod's status remains unknown for more than a few minutes, it is automatically evicted from the node and deleted.","[{'highlight': ""When a node's network interface is shut down, the Kubelet running on the node can no longer contact the Kubernetes API server and let it know that the node and all its pods are still running.""}, {'highlight': 'The control plane will mark the node as NotReady after a while, and the status of all pods on that node will be Unknown.'}, {'highlight': ""If a pod's status remains unknown for more than a few minutes, the pod is automatically evicted from the node by the master (the Kubernetes control plane).""}, {'highlight': 'The Kubelet starts terminating the pod when it sees that the pod has been marked for deletion, but in this case, the pod will keep running because the Kubelet can no longer reach the master.'}, {'highlight': 'The status of a node and its pods can be observed using commands like kubectl get node and kubectl get po.'}]"
249,338,0,[],"306
CHAPTER 10
StatefulSets: deploying replicated stateful applications
 Let’s examine the current situation. Use kubectl describe to display details about
the kubia-0 pod, as shown in the following listing.
$ kubectl describe po kubia-0
Name:        kubia-0
Namespace:   default
Node:        gke-kubia-default-pool-32a2cac8-m0g1/10.132.0.2
...
Status:      Terminating (expires Tue, 23 May 2017 15:06:09 +0200)
Reason:      NodeLost
Message:     Node gke-kubia-default-pool-32a2cac8-m0g1 which was 
             running pod kubia-0 is unresponsive
The pod is shown as Terminating, with NodeLost listed as the reason for the termina-
tion. The message says the node is considered lost because it’s unresponsive.
NOTE
What’s shown here is the control plane’s view of the world. In reality,
the pod’s container is still running perfectly fine. It isn’t terminating at all.
10.5.2 Deleting the pod manually
You know the node isn’t coming back, but you need all three pods running to handle
clients properly. You need to get the kubia-0 pod rescheduled to a healthy node. As
mentioned earlier, you need to delete the node or the pod manually. 
DELETING THE POD IN THE USUAL WAY
Delete the pod the way you’ve always deleted pods:
$ kubectl delete po kubia-0
pod ""kubia-0"" deleted
All done, right? By deleting the pod, the StatefulSet should immediately create a
replacement pod, which will get scheduled to one of the remaining nodes. List the
pods again to confirm: 
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
kubia-0   1/1       Unknown   0          15m
kubia-1   1/1       Running   0          14m
kubia-2   1/1       Running   0          13m
That’s strange. You deleted the pod a moment ago and kubectl said it had deleted it.
Why is the same pod still there? 
NOTE
The kubia-0 pod in the listing isn’t a new pod with the same name—
this is clear by looking at the AGE column. If it were new, its age would be
merely a few seconds.
Listing 10.14
Displaying details of the pod with the unknown status
 
",[],"[{'entity': 'kubectl', 'description': 'command-line tool for interacting with Kubernetes clusters', 'category': 'software'}, {'entity': 'describe', 'description': 'kubectl command to display detailed information about a pod, service, or other Kubernetes resource', 'category': 'software'}, {'entity': 'po', 'description': ""short form of 'pod' in kubectl commands"", 'category': 'software'}, {'entity': 'kubia-0', 'description': 'name of the pod being examined', 'category': 'application'}, {'entity': 'StatefulSets', 'description': 'Kubernetes feature for deploying replicated stateful applications', 'category': 'software'}, {'entity': 'pod', 'description': 'basic execution unit in Kubernetes, similar to a container', 'category': 'container'}, {'entity': 'node', 'description': 'physical or virtual machine running a Kubernetes cluster', 'category': 'hardware'}, {'entity': 'gke-kubia-default-pool-32a2cac8-m0g1', 'description': 'name of the node being examined', 'category': 'hardware'}, {'entity': 'NodeLost', 'description': 'reason for pod termination due to unresponsive node', 'category': 'error'}, {'entity': 'Terminating', 'description': 'status of the pod being deleted', 'category': 'process'}, {'entity': 'kubectl delete', 'description': 'command to delete a pod or other Kubernetes resource', 'category': 'software'}, {'entity': 'pod ""kubia-0"" deleted', 'description': 'output of kubectl delete command indicating successful deletion', 'category': 'output'}, {'entity': 'kubectl get po', 'description': 'command to list all pods in a Kubernetes cluster', 'category': 'software'}, {'entity': 'NAME', 'description': 'column header for pod names in kubectl output', 'category': 'output'}, {'entity': 'READY', 'description': 'column header for pod readiness status in kubectl output', 'category': 'output'}, {'entity': 'STATUS', 'description': 'column header for pod status in kubectl output', 'category': 'output'}, {'entity': 'RESTARTS', 'description': 'column header for number of restarts in kubectl output', 'category': 'output'}, {'entity': 'AGE', 'description': 'column header for age of pods in kubectl output', 'category': 'output'}]","[{'source_entity': '""kubectl""', 'description': 'executes a command to get information about pods', 'destination_entity': '""kubectl get po""'}, {'source_entity': '""kubectl""', 'description': 'uses to delete a pod', 'destination_entity': '""pod \'kubia-0\'""'}, {'source_entity': '""kubectl""', 'description': 'displays the status of a pod', 'destination_entity': '""STATUS""'}, {'source_entity': '""kubectl""', 'description': 'gets information about a pod', 'destination_entity': '""po""'}, {'source_entity': '""kubectl delete""', 'description': 'terminates the operation of deleting a pod', 'destination_entity': '""pod \'kubia-0\' deleted""'}, {'source_entity': '""kubectl delete""', 'description': 'uses to terminate a pod', 'destination_entity': '""Terminating""'}, {'source_entity': '""kubectl get po""', 'description': 'displays the name of a node', 'destination_entity': '""NAME""'}, {'source_entity': '""kubectl get po""', 'description': 'displays the status of a pod', 'destination_entity': '""STATUS""'}, {'source_entity': '""kubectl get po""', 'description': 'displays the node on which a pod is running', 'destination_entity': '""NodeLost""'}, {'source_entity': '""kubectl get po""', 'description': 'displays the number of restarts for a pod', 'destination_entity': '""RESTARTS""'}, {'source_entity': '""kubectl get po""', 'description': 'displays the age of a pod', 'destination_entity': '""AGE""'}, {'source_entity': '""kubectl get po""', 'description': 'displays the readiness status of a pod', 'destination_entity': '""READY""'}, {'source_entity': '""kubectl get po""', 'description': 'displays the state of a pod', 'destination_entity': '""po""'}, {'source_entity': '""kubectl get po""', 'description': 'displays the name of a pod', 'destination_entity': '""pod \'kubia-0\'""'}]","['[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""executes a command to get information about"",\n    ""summary_er"": ""Kubectl uses \'get\' command to retrieve pod details, providing real-time info on running pods.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod \'kubia-0\'"",\n    ""relation_description"": ""uses to delete a pod"",\n    ""summary_er"": ""Kubectl command deletes a specific pod in the Kubernetes cluster, identified by its name \'kubia-0\'.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""displays the status of a pod"",\n    ""summary_er"": ""Kubectl displays the current status of a running pod, providing information on its execution state and any potential issues.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""gets information about a pod"",\n    ""summary_er"": ""Kubectl retrieves pod details, providing insights into its status, logs, and configuration.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl delete"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""terminates the operation of deleting a pod"",\n    ""summary_er"": ""Kubernetes command \'kubectl delete\' terminates the operation of deleting a pod, resulting in the deletion of the specified pod.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl delete"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""uses to terminate a pod"",\n    ""summary_er"": ""Kubernetes command used to delete a running pod, terminating its execution and releasing resources.""\n  }\n]', '[\n  {\n    ""source"": ""node"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""displays the name of a node"",\n    ""summary_er"": ""The \'kubectl get po\' command displays the name of a node associated with each pod.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""displays the status of a pod"",\n    ""summary_er"": ""Kubectl command displays the current status of a pod, including its running state and any potential issues.""\n  }\n]', '[\n  {\n    ""source"": ""Pod"",\n    ""destination"": ""Node"",\n    ""relation_description"": ""displays the node on which a pod is running"",\n    ""summary_er"": ""A Pod\'s status displays the Node it is running on, providing information about its execution environment.""\n  },\n  {\n    ""source"": ""Node"",\n    ""destination"": ""Pod"",\n    ""relation_description"": ""NodeLost"",\n    ""summary_er"": ""When a Node experiences a failure or becomes unavailable, it can result in Pod failures or crashes, leading to potential data loss and service disruptions.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""displays the number of restarts for a pod"",\n    ""summary_er"": ""Kubectl command displays restart count for a specific pod, providing insights into its reliability and performance.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""displays the age of a pod"",\n    ""summary_er"": ""Kubectl command displays the age (creation time) of a pod, providing information on its existence duration.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""displays the readiness status"",\n    ""summary_er"": ""The \'kubectl get po\' command displays the readiness status of a pod, providing real-time information on its operational state.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""displays the state of"",\n    ""summary_er"": ""Using kubectl to view the current status of a pod, including its running state.""\n  }\n]', '[\n  {\n    ""source"": ""kubectl"",\n    ""destination"": ""pod"",\n    ""relation_description"": ""displays the name of a pod"",\n    ""summary_er"": ""Kubectl command displays the name of a pod, which is an instance of a containerized application.""\n  },\n  {\n    ""source"": ""pod"",\n    ""destination"": ""\'kubia-0\'"",\n    ""relation_description"": ""name"",\n    ""summary_er"": ""A pod named \'kubia-0\' represents a running instance of the Kubia web server application.""\n  }\n]']","A StatefulSet deployment is experiencing issues due to an unresponsive node. The kubia-0 pod is shown as Terminating, but in reality, its container is still running fine. To resolve this, the pod needs to be deleted manually, and a replacement pod should be created by the StatefulSet. However, upon deletion, the kubia-0 pod remains, with an unknown status, despite being deleted.","[{'highlight': ""The kubia-0 pod's container is still running perfectly fine despite being shown as Terminating in the control plane's view.""}, {'highlight': 'Deleting a pod manually using kubectl delete po kubia-0 does not immediately create a replacement pod, and the StatefulSet does not reschedule it to a healthy node.'}, {'highlight': 'The kubia-0 pod still exists after being deleted with kubectl delete po kubia-0, and its status is shown as Unknown in the listing.'}, {'highlight': 'The AGE column in the kubectl get po output shows that the kubia-0 pod was not newly created, but rather it has been running for 15 minutes.'}, {'highlight': 'The StatefulSet does not automatically reschedule a deleted pod to a healthy node; manual intervention is required to delete the pod and have it rescheduled.'}]"
