{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40be6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import ollama\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "#rag_test_input_path='/home/matrix4284/MY_GEN_AI_PROJECTS/RAG/GraphRAG/graphrag-local-ollama/ragtest/input/'+file_name\n",
    "import os\n",
    "# importing shutil module\n",
    "import shutil\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "#embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import XMLOutputParser\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a9c5ace",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-16k\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mprompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatOpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "pdf_enrichment_output_dir = '../pdf_enrichment/pdf_enriched_output/'  \n",
    "pdf_enrichment_output_file = 'pdf_enriched_content_dict_phase5_extract_highligts_478_final.pickle'\n",
    "\n",
    "with open(pdf_enrichment_output_dir+pdf_enrichment_output_file, 'rb') as handle:\n",
    "    document_dict_deserialized = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3da9a8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated. Containers within a pod share certain resources like network interfaces and IPC namespaces, but have fully isolated filesystems unless shared using a Kubernetes Volume concept.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict_deserialized[0][\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf32b7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 89,\n",
       " 'img_cnt': 0,\n",
       " 'img_npy_lst': [],\n",
       " 'text': '57\\nIntroducing pods\\n Therefore, you need to run each process in its own container. That’s how Docker\\nand Kubernetes are meant to be used. \\n3.1.2\\nUnderstanding pods\\nBecause you’re not supposed to group multiple processes into a single container, it’s\\nobvious you need another higher-level construct that will allow you to bind containers\\ntogether and manage them as a single unit. This is the reasoning behind pods. \\n A pod of containers allows you to run closely related processes together and pro-\\nvide them with (almost) the same environment as if they were all running in a single\\ncontainer, while keeping them somewhat isolated. This way, you get the best of both\\nworlds. You can take advantage of all the features containers provide, while at the\\nsame time giving the processes the illusion of running together. \\nUNDERSTANDING THE PARTIAL ISOLATION BETWEEN CONTAINERS OF THE SAME POD\\nIn the previous chapter, you learned that containers are completely isolated from\\neach other, but now you see that you want to isolate groups of containers instead of\\nindividual ones. You want containers inside each group to share certain resources,\\nalthough not all, so that they’re not fully isolated. Kubernetes achieves this by config-\\nuring Docker to have all containers of a pod share the same set of Linux namespaces\\ninstead of each container having its own set. \\n Because all containers of a pod run under the same Network and UTS namespaces\\n(we’re talking about Linux namespaces here), they all share the same hostname and\\nnetwork interfaces. Similarly, all containers of a pod run under the same IPC namespace\\nand can communicate through IPC. In the latest Kubernetes and Docker versions, they\\ncan also share the same PID namespace, but that feature isn’t enabled by default. \\nNOTE\\nWhen containers of the same pod use separate PID namespaces, you\\nonly see the container’s own processes when running ps aux in the container.\\nBut when it comes to the filesystem, things are a little different. Because most of the\\ncontainer’s filesystem comes from the container image, by default, the filesystem of\\neach container is fully isolated from other containers. However, it’s possible to have\\nthem share file directories using a Kubernetes concept called a Volume, which we’ll\\ntalk about in chapter 6.\\nUNDERSTANDING HOW CONTAINERS SHARE THE SAME IP AND PORT SPACE\\nOne thing to stress here is that because containers in a pod run in the same Network\\nnamespace, they share the same IP address and port space. This means processes run-\\nning in containers of the same pod need to take care not to bind to the same port\\nnumbers or they’ll run into port conflicts. But this only concerns containers in the\\nsame pod. Containers of different pods can never run into port conflicts, because\\neach pod has a separate port space. All the containers in a pod also have the same\\nloopback network interface, so a container can communicate with other containers in\\nthe same pod through localhost.\\n \\n',\n",
       " 'tables': [],\n",
       " 'entities': [{'entity': 'Docker',\n",
       "   'description': 'Containerization platform',\n",
       "   'category': 'software'},\n",
       "  {'entity': 'Kubernetes',\n",
       "   'description': 'Container orchestration system',\n",
       "   'category': 'software'},\n",
       "  {'entity': 'Pods',\n",
       "   'description': 'Group of containers that share resources and run as a single unit',\n",
       "   'category': 'application'},\n",
       "  {'entity': 'Containers',\n",
       "   'description': 'Isolated execution environment for processes',\n",
       "   'category': 'container'},\n",
       "  {'entity': 'Linux namespaces',\n",
       "   'description': 'Resource isolation mechanism in Linux',\n",
       "   'category': 'process'},\n",
       "  {'entity': 'Network namespace',\n",
       "   'description': 'Shared network resources among containers in a pod',\n",
       "   'category': 'network'},\n",
       "  {'entity': 'UTS namespace',\n",
       "   'description': 'Shared hostname and network interfaces among containers in a pod',\n",
       "   'category': 'network'},\n",
       "  {'entity': 'IPC namespace',\n",
       "   'description': 'Shared inter-process communication resources among containers in a pod',\n",
       "   'category': 'process'},\n",
       "  {'entity': 'PID namespace',\n",
       "   'description': 'Shared process ID space among containers in a pod',\n",
       "   'category': 'process'},\n",
       "  {'entity': 'Volume',\n",
       "   'description': 'Kubernetes concept for sharing file directories among containers',\n",
       "   'category': 'database'},\n",
       "  {'entity': 'IP address',\n",
       "   'description': 'Shared IP address among containers in a pod',\n",
       "   'category': 'network'},\n",
       "  {'entity': 'Port space',\n",
       "   'description': 'Shared port numbers among containers in a pod',\n",
       "   'category': 'network'}],\n",
       " 'relationships': [{'source_entity': '\"Network namespace\"',\n",
       "   'description': 'manages',\n",
       "   'destination_entity': '\"Linux namespaces\"'},\n",
       "  {'source_entity': '\"Network namespace\"',\n",
       "   'description': 'allocates',\n",
       "   'destination_entity': '\"Port space\"'},\n",
       "  {'source_entity': '\"UTS namespace\"',\n",
       "   'description': 'manages',\n",
       "   'destination_entity': '\"Linux namespaces\"'},\n",
       "  {'source_entity': '\"Kubernetes\"',\n",
       "   'description': 'orchestrates',\n",
       "   'destination_entity': '\"Pods\"'},\n",
       "  {'source_entity': '\"Docker\"',\n",
       "   'description': 'runs',\n",
       "   'destination_entity': '\"Containers\"'},\n",
       "  {'source_entity': '\"Kubernetes\"',\n",
       "   'description': 'manages',\n",
       "   'destination_entity': '\"Volume\"'},\n",
       "  {'source_entity': '\"PID namespace\"',\n",
       "   'description': 'manages',\n",
       "   'destination_entity': '\"Linux namespaces\"'},\n",
       "  {'source_entity': '\"IPC namespace\"',\n",
       "   'description': 'manages',\n",
       "   'destination_entity': '\"Linux namespaces\"'}],\n",
       " 'summary_rel': ['[\\n  {\\n    \"source\": \"Network namespace\",\\n    \"destination\": \"pod\",\\n    \"relation_description\": \"manages\",\\n    \"summary_er\": \"A network namespace manages a pod by providing isolation and resource management for its network stack.\"\\n  }\\n]',\n",
       "  '[\\n  {\\n    \"source\": \"Network namespace\",\\n    \"destination\": \"pod\",\\n    \"relation_description\": \"allocates\",\\n    \"summary_er\": \"The network namespace allocates resources to a pod, enabling communication and resource sharing between them.\"\\n  },\\n  {\\n    \"source\": \"Port space\",\\n    \"destination\": \"pod\",\\n    \"relation_description\": \"\",\\n    \"summary_er\": \"\"\\n  }\\n]',\n",
       "  '[\\n  {\\n    \"source\": \"UTS namespace\",\\n    \"destination\": \"pod\",\\n    \"relation_description\": \"manages\",\\n    \"summary_er\": \"The UTS namespace manages a Linux pod, providing isolation and resource control.\"\\n  }\\n]',\n",
       "  '[\\n  {\\n    \"source\": \"Kubernetes\",\\n    \"destination\": \"pod\",\\n    \"relation_description\": \"orchestrates\",\\n    \"summary_er\": \"Kubernetes manages and coordinates the execution of pods, ensuring efficient resource utilization and scalability.\"\\n  }\\n]',\n",
       "  '[\\n  {\\n    \"source\": \"Docker\",\\n    \"destination\": \"pod\",\\n    \"relation_description\": \"runs\",\\n    \"summary_er\": \"Docker runs containers, which are lightweight and portable packages of software that include everything needed to run an application.\"\\n  }\\n]',\n",
       "  '[\\n  {\\n    \"source\": \"Kubernetes\",\\n    \"destination\": \"pod\",\\n    \"relation_description\": \"manages\",\\n    \"summary_er\": \"Kubernetes manages pods, ensuring efficient resource allocation and scaling.\"\\n  },\\n  {\\n    \"source\": \"Kubernetes\",\\n    \"destination\": \"Volume\",\\n    \"relation_description\": \"uses\",\\n    \"summary_er\": \"Kubernetes utilizes volumes to persist data across pod restarts or deletion.\"\\n  }\\n]',\n",
       "  '[\\n  {\\n    \"source\": \"PID namespace\",\\n    \"destination\": \"pod\",\\n    \"relation_description\": \"manages\",\\n    \"summary_er\": \"The PID namespace in Linux manages a pod, controlling its process ID space and ensuring isolation between processes.\"\\n  }\\n]',\n",
       "  '[\\n  {\\n    \"source\": \"IPC namespace\",\\n    \"destination\": \"pod\",\\n    \"relation_description\": \"manages\",\\n    \"summary_er\": \"The IPC namespace manages a pod, controlling how processes within it interact with each other.\"\\n  },\\n  {\\n    \"source\": \"Linux namespaces\",\\n    \"destination\": \"pod\",\\n    \"relation_description\": \"manages\",\\n    \"summary_er\": \"A Linux namespace manages a pod, providing isolation and resource control for its contained processes.\"\\n  }\\n]'],\n",
       " 'summary': 'A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated. Containers within a pod share certain resources like network interfaces and IPC namespaces, but have fully isolated filesystems unless shared using a Kubernetes Volume concept.',\n",
       " 'highlights': [{'highlight': 'A pod of containers allows you to run closely related processes together and provide them with (almost) the same environment as if they were all running in a single container, while keeping them somewhat isolated.'},\n",
       "  {'highlight': 'Kubernetes achieves partial isolation between containers of the same pod by configuring Docker to have all containers share the same set of Linux namespaces instead of each container having its own set.'},\n",
       "  {'highlight': \"Containers in a pod run in the same Network namespace, sharing the same IP address and port space, which means processes running in containers of the same pod need to take care not to bind to the same port numbers or they'll run into port conflicts.\"},\n",
       "  {'highlight': 'Each pod has a separate port space from other pods, so containers of different pods can never run into port conflicts.'},\n",
       "  {'highlight': 'Containers in a pod share the same hostname and network interfaces, as well as the ability to communicate through IPC, but have fully isolated filesystems by default.'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict_deserialized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021fc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_doc_sumary=[]\n",
    "doc_len=len(document_dict_deserialized)\n",
    "for i in range(0,doc_len):\n",
    "    summary=document_dict_deserialized[i][\"summary\"]\n",
    "    full_doc_sumary.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff35134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated. Containers within a pod share certain resources like network interfaces and IPC namespaces, but have fully isolated filesystems unless shared using a Kubernetes Volume concept.',\n",
       " 'Kubernetes pods are logical hosts that behave like physical hosts or VMs, with processes running in the same pod behaving like processes on the same machine. Each pod has its own IP address and can communicate directly with other pods through a flat network, without NAT gateways. Pods should be organized by app, with each containing tightly related components or processes, allowing for as many pods as needed without significant overhead.',\n",
       " 'A multi-tier application consisting of frontend and backend components should not be configured as a single pod but rather split into multiple pods to enable individual scaling and utilize computational resources on multiple nodes. This approach allows for separate scaling requirements for frontend and backend components, making it more efficient and suitable for applications with diverse resource needs.',\n",
       " 'Pods in Kubernetes are groups of containers that can be run together, like a web server and a sidecar container for downloading content. To decide when to use multiple containers in a pod, ask yourself: do they need to run together, represent a single whole, or must they be scaled together? Typically, containers should be run in separate pods unless a specific reason requires them to be part of the same pod.',\n",
       " 'You can create pods by posting a JSON or YAML manifest to the Kubernetes REST API endpoint. This method allows configuration of all properties, but requires knowledge of the Kubernetes API object definitions. Alternatively, you can use commands like kubectl run, but they limit the configurable properties. The YAML descriptor for an existing pod can be obtained using kubectl get with the -o yaml option, showing metadata and specification details.',\n",
       " 'A Kubernetes Pod is a logical host for one or more application containers. It consists of metadata (name, namespace, labels) and spec (containers, volumes), with optional detailed status information. Key elements include terminationMessagePath, dnsPolicy, restartPolicy, serviceAccount, and volumeMounts.',\n",
       " \"A Kubernetes pod can be created using a YAML or JSON descriptor, which typically consists of three parts: metadata, spec, and status. The spec section defines the container's image, name, and ports, with specifying ports being informational only. A simple example is shown in kubia-manual.yaml, where a single container based on luksa/kubia image listens on port 8080.\",\n",
       " 'A Kubernetes pod is a collection of containers that run on a host, and can be described using a manifest. The pod spec contains attributes such as hostname, IP addresses, ports, and volumes that can be mounted by containers. Using kubectl explain, one can discover possible API object fields and drill deeper to learn more about each attribute.',\n",
       " \"To create a pod from a YAML file, use kubectl create -f command. After creating the pod, you can retrieve its full YAML or JSON definition using kubectl get po <pod_name> -o yaml/json commands. You can also view application logs by tailing the container's standard output and error streams.\",\n",
       " 'This chapter discusses pods in Kubernetes, focusing on retrieving logs from containers running within a pod. The container runtime redirects streams to files, allowing users to view logs by running `docker logs <container id>`. However, Kubernetes provides an easier way using `kubectl logs`, which can be used to retrieve logs from a pod without the need for SSH access. Additionally, if a pod contains multiple containers, the user must specify the container name when retrieving logs. The chapter also touches on centralized logging and port forwarding as methods to connect to a pod for testing and debugging purposes.',\n",
       " 'Kubernetes allows port forwarding to a specific pod through the `kubectl port-forward` command, enabling direct access for debugging or testing purposes. This can be achieved by running `$ kubectl port-forward kubia-manual 8888:8080` and sending an HTTP request using `curl localhost:8888`. This method is effective for testing individual pods, especially in microservices architectures where many pods need to be categorized and managed.',\n",
       " 'Kubernetes allows running multiple copies of the same component and different versions concurrently, which can lead to hundreds of pods without organization. To manage this, labels are used to organize pods and other Kubernetes resources into smaller groups based on arbitrary criteria, allowing developers and administrators to easily identify and operate on specific pods or groups with a single action.',\n",
       " \"Adding labels to pods in a Kubernetes system allows for easy organization and understanding of the system's structure. Labels can specify which app or microservice a pod belongs to, as well as whether it's a stable, beta, or canary release. By using these labels, developers and ops personnel can easily see where each pod fits in, making it easier to manage complex microservices architectures.\",\n",
       " 'This chapter is about pods in Kubernetes, specifically running containers and adding/removing labels. Labels can be added to or modified on existing pods using kubectl label command. The --overwrite option is required when changing existing labels. Examples of labeling a new pod, viewing labels with kubectl get po --show-labels, and modifying labels on an existing pod are shown.',\n",
       " 'Label selectors allow selecting subsets of pods based on labels. A label selector can filter resources by key, value, or not equal to a specified value. Examples include listing pods with creation_method=manual, env label, or no env label.',\n",
       " 'This chapter discusses Kubernetes Pods, specifically focusing on running containers in a cluster. Label selectors are used to identify and select pods based on labels, with examples including creation_method!=manual, env in (prod,devel), and app=pc for selecting the product catalog microservice pods. Multiple conditions can be combined using comma-separated criteria, as shown in the selector app=pc,rel=beta. Label selectors are not only used for listing pods but also for performing actions on a subset of all pods.',\n",
       " 'In Kubernetes, using labels and selectors is a way to constrain pod scheduling without specifying exact node placement. This allows for flexible scheduling based on node requirements, such as hardware infrastructure or GPU acceleration. Labels can be applied to nodes, and selectors can be used to match those labels, ensuring that pods are scheduled to nodes that meet specific criteria, while maintaining the decoupling of applications from infrastructure.',\n",
       " 'Labels can be attached to Kubernetes objects like pods and nodes. Using labels, the ops team categorizes new nodes by hardware type or features like GPU availability. To schedule a pod that requires a GPU, create a YAML file with a node selector set to gpu=true and use kubectl create -f to deploy the pod.',\n",
       " 'Pods can be annotated with labels and annotations. Labels are key-value pairs used for identification and grouping, while annotations hold larger pieces of information primarily meant for tools. Annotations are automatically added by Kubernetes or manually by users and are useful for adding descriptions, specifying creator names, and introducing new features. The importance of label selectors will become evident in future chapters on Replication-Controllers and Services.',\n",
       " \"Kubernetes pods can have labels and annotations, where labels are short and used for organization, while annotations can contain large blobs of data up to 256KB. Annotations like kubernetes.io/created-by were deprecated in version 1.8 and removed in 1.9. Annotations can be added or modified using the kubectl annotate command, and it's recommended to use unique prefixes to prevent key collisions.\",\n",
       " 'Kubernetes groups objects into namespaces, which provide a scope for object names and allow for separate, non-overlapping groups. Namespaces enable operating within one group at a time and using the same resource names multiple times across different namespaces. They can be used to split complex systems, separate resources in multi-tenant environments, or divide resources into production, development, and QA environments.',\n",
       " \"Namespaces in Kubernetes enable separation of resources into non-overlapping groups, isolating them from other users' resources. They can be created by posting a YAML file to the Kubernetes API server using kubectl create -f or kubectl create namespace command.\",\n",
       " 'Namespaces allow grouping resources and can be created using kubectl create command or by posting YAML manifest to API server. Resources in other namespaces can be managed by adding namespace entry to metadata section or specifying namespace with kubectl create command. Namespaces provide isolation for objects, but do not guarantee network isolation between pods across different namespaces.',\n",
       " \"Pods in Kubernetes can communicate with each other within a namespace. To stop and remove pods, use kubectl delete command. Pods can be deleted by name, label selector, or even deleting the whole namespace. When deleting a pod, Kubernetes sends a SIGTERM signal to shut down containers, and if they don't respond, a SIGKILL signal is sent. It's essential for processes to handle the SIGTERM signal properly.\",\n",
       " 'To delete all pods in a namespace, use the command $ kubectl delete po --all. This will delete all running and terminating pods in the current namespace. Alternatively, you can delete a specific pod by its name or delete all pods with a certain label using the label selector.',\n",
       " 'Kubernetes pods run multiple containers as one entity, with kubectl commands like run and delete creating ReplicationControllers that manage pods. Deleting all resources in a namespace can be done with kubectl delete all --all, but note that some resources like Secrets are preserved and need to be deleted explicitly.',\n",
       " 'Pods can run multiple processes similar to physical hosts. They have YAML/JSON descriptors that define their specification and current state. Labels and selectors help organize and perform operations on multiple pods. Annotations attach data to pods, while namespaces allow different teams to use the same cluster as separate Kubernetes clusters. The kubectl explain command provides information on resources.',\n",
       " 'Kubernetes manages pods automatically, using resources like Replication-Controllers and Deployments to create and manage pods. This chapter focuses on keeping pods healthy, running multiple instances of the same pod, automating rescheduling after a node fails, scaling pods horizontally, running system-level pods, and batch jobs, as well as scheduling periodic or future tasks.',\n",
       " \"Kubernetes checks if a container is alive through liveness probes and restarts it if it fails. Liveness probes can be specified for each container in a pod's specification. Kubernetes periodically executes the probe and restarts the container if it fails. This ensures that applications are restarted even if they stop working without crashing, such as due to memory leaks or infinite loops.\",\n",
       " 'A liveness probe checks if a container is running correctly. A successful probe returns a 2xx or 3xx HTTP response code, while a failed probe returns an error code or no response at all. The chapter demonstrates creating a new pod with an HTTP GET liveness probe for a Node.js app that intentionally fails after five requests.',\n",
       " 'The document explains how Kubernetes uses liveness probes to keep pods healthy. An httpGet liveness probe sends HTTP GET requests to a path on port 8080, and if the status code becomes 500, Kubernetes restarts the container. The document also demonstrates this by creating a pod with a liveness probe, showing it gets restarted after about a minute and a half, and describes how to obtain the application log of a crashed container using kubectl logs --previous.',\n",
       " 'This page discusses replication and other controllers in Kubernetes, specifically the liveness probe which checks if a container is running correctly. If the container fails the probe, it will be killed and re-created. The page also explains how to configure additional properties of the liveness probe, such as delay, timeout, period, and initial delay. An example YAML file is provided to demonstrate how to set an initial delay for the liveness probe.',\n",
       " \"To keep pods healthy, it's essential to set an initial delay for liveness probes. This prevents probes from failing as soon as the app starts, leading to unnecessary restarts. A liveness probe should check if the server is responding and ideally perform internal status checks on vital components. It's crucial to ensure the /health endpoint doesn't require authentication and only checks internals of the app, not external factors.\",\n",
       " \"A ReplicationController in Kubernetes ensures its pods are always kept running by creating replacement pods if one disappears. Liveness probes shouldn't use too many computational resources and should be executed relatively often to keep containers running. Kubernetes will retry a probe several times before considering it a failed attempt, so implementing a retry loop is unnecessary.\",\n",
       " 'A ReplicationController monitors running pods and ensures the desired number of replicas matches the actual number. If too few, it creates new replicas; if too many, it removes excess replicas. It recreates lost pods when a node fails, but not manually created or changed pods.',\n",
       " \"A ReplicationController's job is to maintain an exact number of pods that match its label selector by creating or deleting pods as needed, with three essential parts: a label selector, replica count, and pod template.\",\n",
       " \"A ReplicationController's replica count, label selector, and pod template can be modified at any time, but only changes to the replica count affect existing pods. Changes to the label selector or pod template have no effect on existing pods and are used as a 'cookie cutter' for new pods created by this ReplicationController. The controller ensures a pod is always running, creates replacement replicas when a cluster node fails, and enables easy horizontal scaling of pods.\",\n",
       " \"Kubernetes creates a Replication-Controller named kubia that ensures three pod instances match the label selector app=kubia. When there aren't enough pods, it creates new ones from the provided pod template. The API server verifies the ReplicationController definition and will not accept it if misconfigured. To prevent such scenarios, let Kubernetes extract the selector from the pod template.\",\n",
       " \"A ReplicationController is introduced, managing three pods and automatically spinning up new ones if any are deleted. The kubectl get command shows information about ReplicationControllers, including desired and actual pod numbers. Additional details can be obtained with the kubectl describe command, displaying the ReplicationController's name, namespace, selector, labels, annotations, replicas, and pod status.\",\n",
       " 'A ReplicationController in Kubernetes creates a new pod to replace one that has been deleted when it detects an inadequate number of running pods, triggered by events such as pod deletion or termination.',\n",
       " 'A ReplicationController in Kubernetes automatically spins up new pods to replace those that are down when a node fails, as demonstrated by simulating a node failure on a three-node cluster. After shutting down the network interface of one node, the status is shown as NotReady, and the pods remain unchanged for several minutes before the ReplicationController creates a new pod to replace the downed ones.',\n",
       " \"ReplicationController automatically manages pods based on a label selector and can spin up new pods if one fails or is removed from scope. A pod's labels can be changed to move it in or out of the ReplicationController's scope, but changing its labels does not delete it. The replication controller will notice if a managed pod is missing and spin up a new one to replace it.\",\n",
       " \"A ReplicationController doesn't care if labels are added to its managed pods. Changing a label on a managed pod makes it no longer match the controller's label selector, prompting the controller to start a new pod to bring the number back to three.\",\n",
       " \"A ReplicationController can spin up new pods to bring the number back up after one is removed, allowing for independent pod management and changing label selectors to control which pods are included in the controller's scope.\",\n",
       " 'ReplicationControllers can modify their pod template at any time, but changes only affect new pods created after the modification. To change an existing pod, it must be deleted and a new one will be created based on the updated template.',\n",
       " \"ReplicationControllers ensure a specific number of pod instances is always running. Scaling pods horizontally is trivial and can be done by changing the replicas field, either by using the command `kubectl scale` or by editing the ReplicationController's definition directly with `kubectl edit`. This allows scaling up or down with ease.\",\n",
       " 'A ReplicationController is updated when scaled up or down, and it immediately scales the number of pods to the desired state. Scaling is a matter of stating the desired state, not telling Kubernetes how to do it. Declarative approach makes interacting with a Kubernetes cluster easy. When deleting a ReplicationController through kubectl delete, the pods are also deleted unless managed by another controller.',\n",
       " 'ReplicationControllers manage pods and keep them running without interruption, but can be deleted while keeping the pods running using the --cascade=false option. ReplicaSets are a newer resource that replaces ReplicationControllers completely and should be used instead. Deleting a ReplicationController leaves its pods unmanaged, but a new one can be created to manage them again.',\n",
       " 'ReplicaSets are used instead of ReplicationControllers to manage replicas. A ReplicaSet behaves exactly like a ReplicationController but with more expressive pod selectors, allowing matching pods based on label presence or absence, and not just specific values. This enables a single ReplicaSet to match multiple sets of pods and treat them as a single group. The process of creating a ReplicaSet involves defining its YAML configuration, including the API version, kind, metadata, selector, replicas, template, and containers, which can be used to adopt orphaned pods created by a ReplicationController.',\n",
       " \"ReplicaSets aren't part of the v1 API, so specify the proper apiVersion when creating a resource. To create a ReplicaSet, use kubectl create command with YAML file, then examine it with kubectl get and describe commands. The apiVersion property specifies the API group (apps) and actual API version (v1beta2), which categorizes Kubernetes resources into core and other groups.\",\n",
       " 'ReplicaSets are similar to ReplicationControllers, but with more expressive label selectors. The matchExpressions property allows for complex matching rules, such as requiring a pod to have a specific label or not having a certain label. This provides flexibility in selecting pods, making ReplicaSets more powerful than ReplicationControllers.',\n",
       " 'ReplicaSets and ReplicationControllers are used for running a specific number of pods in a Kubernetes cluster, but DaemonSets are used to run one pod on each node, with exactly one instance per node, suitable for infrastructure-related pods like log collectors and resource monitors.',\n",
       " 'A DaemonSet is used to run a pod on every node in a Kubernetes cluster, or on a subset of nodes specified by a node selector. It ensures a desired number of pods exist and creates a new pod instance if a new node is added or an existing node is deleted. Unlike ReplicaSets, DaemonSets do not need a replica count and will deploy pods even to unschedulable nodes.',\n",
       " \"A DaemonSet is created for deploying managed pods. A YAML definition is written for the DaemonSet to run a mock ssd-monitor process on nodes with the 'disk=ssd' label. The DaemonSet will create an instance of the pod on each node that meets this condition.\",\n",
       " 'Creating a DaemonSet to run one pod on each node requires labeling nodes with the required label. Initially, the DaemonSet appears not to deploy pods due to missing labels. Adding the label to one or more nodes triggers the DaemonSet to create pods for matching nodes.',\n",
       " 'A chapter about deploying managed pods using Replication and other controllers. It discusses running pods that perform a single completable task, which is different from continuous tasks like DaemonSets. The Job resource is introduced as a solution for this type of task, allowing pods to be rescheduled in case of node failure. An example is given of running a container image built on top of the busybox image, invoking the sleep command for two minutes.',\n",
       " 'A Job resource in Kubernetes runs a single completable task. A YAML definition of a Job resource is provided, which defines a pod that will run an image invoking a process for 120 seconds and then exit. The restartPolicy specifies what to do when the processes finish. Jobs are part of the batch API group, version v1, and cannot use the default restart policy. Pods managed by Jobs are rescheduled until they finish successfully.',\n",
       " 'A Job resource in Kubernetes can be used to deploy a single, managed pod with a restart policy of OnFailure or Never. Once created, the Job will run the pod until completion, at which point it will be marked as successful and the pod deleted. Jobs can also be configured to create multiple pods that run in parallel or sequentially by setting the completions and parallelism properties.',\n",
       " \"A Job can be configured to run multiple pods sequentially or in parallel. To run pods sequentially, set completions to the number of times you want the Job's pod to run. For example, setting completions to 5 will create one pod at a time until five pods complete successfully. To run pods in parallel, specify how many pods are allowed to run with the parallelism Job spec property. This allows up to that many pods to be created and running at the same time.\",\n",
       " \"You can scale a Job's parallelism property while it's running using kubectl scale command. Additionally, you can limit a pod's time to complete by setting activeDeadlineSeconds in the pod spec. A Job can also be configured to retry failed pods up to 6 times before being marked as failed. Furthermore, Kubernetes supports scheduling Jobs periodically or once in the future through CronJobs, which create a Job resource at the specified time and run it according to the Job template.\",\n",
       " 'A CronJob resource creates Job objects based on a specified schedule. The schedule is set using the cron format (minute, hour, day of month, month, day of week), and can be configured to run jobs at specific intervals. The jobTemplate property defines the template for creating Job resources, which are created from the CronJob resource at approximately the scheduled time.',\n",
       " 'A CronJob creates a single Job for each execution configured in the schedule, but can create two Jobs if run concurrently or none at all. To combat this, jobs should be idempotent and next job runs should perform work missed by previous runs. A startingDeadlineSeconds field can also be specified to ensure pods start running within a certain timeframe.',\n",
       " 'ReplicationControllers are being replaced with ReplicaSets and Deployments which provide additional features, DaemonSets ensure every node runs a pod instance, Jobs schedule batch tasks while CronJobs handle future executions.',\n",
       " 'Services enable clients to discover and communicate with pods, allowing them to respond to external requests. This chapter covers creating Service resources to expose a group of pods at a single address, discovering services in the cluster, exposing services to external clients, connecting to external services from inside the cluster, controlling pod readiness for service participation, and troubleshooting services.',\n",
       " 'A Kubernetes Service is a resource that provides a single, constant point of entry to a group of pods providing the same service. Each service has an IP address and port that never change while the service exists, allowing clients to connect without needing to know individual pod locations. This enables external clients to connect to frontend pods without worrying about IP changes and allows frontend pods to connect to backend database services with a stable address.',\n",
       " \"A service enables clients to discover and talk to pods, even if the pod's IP address changes. Services are created using label selectors, which specify which pods belong to the same set. A service can be backed by more than one pod, with connections load-balanced across all backing pods.\",\n",
       " 'A Kubernetes service called kubia is created manually by posting a YAML descriptor, which exposes all pods matching the app=kubia label selector on port 80 and routes connections to port 8080 of each pod. The service accepts connections on port 80 and forwards them to port 8080 of one of the matching pods, allowing clients to access the service through a single IP address and port.',\n",
       " \"The chapter explains services in Kubernetes, enabling clients to discover and talk to pods. A service is exposed through an internal cluster IP that's only accessible from inside the cluster. The primary purpose of services is exposing groups of pods to other pods in the cluster. To test a service, one can send requests to it from within the cluster using various methods such as creating a pod, ssh-ing into a node, or executing a command in an existing pod using kubectl exec.\",\n",
       " 'When running curl inside a pod using kubectl exec, Kubernetes proxies the connection to a random available pod among those backing the service. The double dash (--), signals the end of command options for kubectl and everything after it is executed within the pod. Without the double dash, the -s option would be interpreted as an option for kubectl, resulting in misleading errors.',\n",
       " 'This chapter discusses services in Kubernetes, enabling clients to discover and talk to pods. Session affinity can be set to either None or ClientIP, redirecting requests from the same client IP to the same pod. Services can also support multiple ports, exposing all ports through a single cluster IP, with each port requiring a specified name.',\n",
       " 'A Kubernetes Service can be defined with named ports in both the pod and service specifications. The label selector applies to the whole service, not individual ports. Ports can be referred to by name or number in the service spec.',\n",
       " \"Services enable clients to discover and talk to pods through a single and stable IP address and port, which remains unchanged throughout its lifetime. Client pods can discover the service's IP and port through environment variables or by manually looking up its IP address.\",\n",
       " 'Services in Kubernetes are exposed through environment variables, but can also be discovered using DNS. Each service gets a DNS entry and client pods can access them through their fully qualified domain name (FQDN). This allows for a more flexible way of accessing services without relying on environment variables.',\n",
       " \"Services enable clients to discover and talk to pods. Clients can connect to a service by opening a connection to its FQDN, which includes the service name, namespace, and cluster domain suffix. If in the same namespace as the database pod, the client can refer to the service simply by its name. To access a service inside a pod's container, run bash using kubectl exec command with the -it option, and then use curl to access the service.\",\n",
       " \"You can connect to services living outside the cluster by using its name as the hostname in the requested URL. Omitting namespace and svc.cluster.local suffix is also allowed due to how DNS resolver inside each pod's container is configured. However, trying to ping service IP will not work because it's a virtual IP that only has meaning when combined with the service port.\",\n",
       " 'Kubernetes services enable clients to discover and talk to pods. The Endpoints resource is a list of IP addresses and ports exposing a service. The pod selector in the service spec is used to build a list of IPs and ports, which are stored in the Endpoints resource. Clients connect to a service, and the service proxy selects one of those IP and port pairs and redirects the incoming connection to the server listening at that location.',\n",
       " 'A Kubernetes service called `external-service` is created without a pod selector, requiring a separate Endpoints resource to be manually created with the same name and containing target IP addresses and ports for the service.',\n",
       " 'Kubernetes services enable clients to discover and talk to pods, and can be exposed externally using an ExternalName service which creates a DNS record pointing to a fully qualified domain name. This allows clients to connect directly to the external service without going through the service proxy, and does not require a cluster IP address. ExternalName services are implemented solely at the DNS level and can be modified by changing the externalName attribute or switching to a ClusterIP service with an Endpoints object.',\n",
       " \"A service can be made accessible externally by setting its type to NodePort, LoadBalancer, or creating an Ingress resource. A NodePort service makes a port on all nodes reserve and forward incoming connections to the pods that are part of the service, allowing access through any node's IP and reserved node port.\",\n",
       " 'A Kubernetes Service named kubia-nodeport is created with type NodePort, specifying node port 30123 and exposing internal port 80. The service is accessible through the IP address of any cluster node on port 30123, redirecting incoming connections to a randomly selected pod.',\n",
       " \"To expose services to external clients, configure Google Cloud Platform's firewalls to allow connections on the desired port, e.g., $ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123. This enables access through one of the node's IPs on that port, which can be found in a separate step.\",\n",
       " 'Services in Kubernetes allow clients to discover and talk to pods. With NodePort services, pods are accessible through port 30123 on any node. However, this can lead to issues if a node fails. A load balancer can be used to distribute traffic across healthy nodes, and Kubernetes clusters running on cloud providers often support automatic load balancer provisioning. Using JSONPath with kubectl allows for efficient retrieval of node IPs.',\n",
       " \"Creating a Kubernetes Service with a LoadBalancer allows external access through a unique, publicly accessible IP address. The service type is set to LoadBalancer, and ports are specified for external connection. Once created, the load balancer's IP address is listed in the Service object, enabling direct access via curl or other tools.\",\n",
       " 'Services in Kubernetes allow clients to discover and talk to pods, using the load balancer to route HTTP requests to a random pod for each connection. Even with session affinity set to None, users will hit the same pod every time due to keep-alive connections from web browsers, whereas tools like curl open new connections each time.',\n",
       " \"Exposing services to external clients can be done through NodePort or LoadBalancer-type services. However, when using a node port, externally originating connections may not always go directly to the pod running on the same node, requiring an additional network hop. This can be prevented by configuring the service's externalTrafficPolicy field to 'Local', but this has its own drawbacks such as uneven distribution of connections across pods.\",\n",
       " \"Services in Kubernetes allow clients to discover and communicate with pods, but can't preserve client IP when using node ports due to Source Network Address Translation (SNAT). The Local external traffic policy affects this, but creating an Ingress resource is another way to expose services externally, allowing multiple services to share one public IP address and load balancer, improving load distribution and scalability.\",\n",
       " 'Ingresses in Kubernetes operate at the application layer, providing features like cookie-based session affinity. An Ingress controller is required to make Ingress resources work, and different environments use different implementations. To enable the Ingress add-on in Minikube, run $ minikube addons enable ingress, which allows exposing multiple services through a single Ingress.',\n",
       " 'Creating an Ingress resource in Kubernetes enables clients to discover and talk to pods. An example YAML manifest is provided, which defines an Ingress with a single rule sending all HTTP requests from the host kubia.example.com to the kubia-nodeport service on port 80. The Ingress controller pod can be listed using kubectl get po --all-namespaces.',\n",
       " \"Exposing services externally through an Ingress resource requires configuring DNS or /etc/hosts to point to the Ingress controller's IP address. The Ingress controller then selects a pod based on the Host header and forwards the request to it, allowing access to the service at http://kubia.example.com.\",\n",
       " 'An Ingress can expose multiple services on the same host by mapping different paths to different services, allowing clients to reach two or more services through a single IP address. This is achieved by specifying multiple paths in the Ingress spec and mapping each path to a specific service, as shown in Listing 5.14. Requests are routed to the corresponding service based on the path in the requested URL.',\n",
       " 'An Ingress resource can map different services to different hosts based on the Host header in the request, and can also handle TLS traffic by attaching a certificate and private key to the Ingress as a Secret. This allows for secure communication between clients and the controller without requiring the application pod to support TLS.',\n",
       " \"Services allow clients to discover and communicate with pods. A Secret was created using two files, and an Ingress object was updated to accept HTTPS requests for kubia.example.com. Alternatively, 'kubectl apply' can be used to update the Ingress resource. CertificateSigningRequest resources enable certificates to be signed by a human operator or automated process, retrieving a signed certificate from the CSR's status field.\",\n",
       " \"Kubernetes allows you to define a readiness probe for your pod, which periodically determines whether the pod should receive client requests or not. When a container's readiness probe returns success, it signals that the container is ready to accept requests, allowing traffic to be directed to it only when it's fully ready to serve.\",\n",
       " \"Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths. Readiness probes check if a container is ready to serve requests, with three types: Exec, HTTP GET, and TCP Socket. If a pod fails the readiness check, it's removed from the service until it becomes ready again. This ensures only healthy containers receive requests, distinguishing from liveness probes which keep pods running.\",\n",
       " \"Readiness probes ensure clients only talk to healthy pods by signaling when a pod is ready to accept connections. A readiness probe can be added to a pod by modifying the ReplicationController's pod template using kubectl edit, adding the probe definition under spec.template.spec.containers. The probe periodically checks if a file exists, and if it does, the pod is considered ready.\",\n",
       " \"Services: enabling clients to discover and talk to pods. ReplicationController's pod template changes have no effect on existing pods. Existing pods report not being ready until they're re-created by the Replication-Controller, which will fail the readiness check unless a /var/ready file is created in each of them.\",\n",
       " 'A readiness probe in Kubernetes determines if a pod is ready to accept connections. In real-world scenarios, it should return success or failure depending on whether the app can receive client requests. If no readiness probe is defined, pods become service endpoints immediately and clients may experience connection errors when the app takes too long to start listening for incoming connections.',\n",
       " 'Kubernetes allows clients to discover pod IPs through DNS lookups, enabling connection to all pods or individual pods using a headless service with clusterIP set to None. This method is ideal for Kubernetes-agnostic apps, providing a stable IP address for clients to connect to all backing pods.',\n",
       " 'A headless service is used to discover individual pods based on a pod selector. The service will list only ready pods as endpoints. To confirm readiness, create the /var/ready file in each pod. A DNS lookup can be performed from inside a pod using the tutum/dnsutils container image or by running a new pod without writing a YAML manifest using kubectl run with the --generator=run-pod/v1 option. This allows understanding of how DNS A records are returned for a headless service, which returns IPs of all ready pods.',\n",
       " \"Headless services allow clients to connect directly to pods by DNS name. Kubernetes provides load balancing across pods through DNS round-robin mechanism instead of service proxy. To discover all pods, including unready ones, add annotation 'service.alpha.kubernetes.io/tolerate-unready-endpoints: true' or use the publishNotReadyAddresses field in service spec.\",\n",
       " 'Make sure to access Kubernetes service from within the cluster and not outside. Check if readiness probe is succeeding, examine Endpoints object, and try accessing service using its cluster IP or FQDN. Ensure correct port is exposed and target port is not used. Connect directly to pod IP to confirm connections are being accepted. If still issues persist, check if app is binding only to localhost.',\n",
       " \"Services enable clients to discover and talk to pods using a pod's readiness probe, enabling discovery of pod IPs through DNS for headless services. Additionally, troubleshooting and modifying firewall rules in Google Kubernetes/Compute Engine, executing commands in pod containers, running bash shells in existing pods, and modifying resources with kubectl apply can be performed.\",\n",
       " 'This chapter explores how containers in a pod can access external disk storage and share storage between them. Containers have isolated file systems, but volumes allow sharing disk space. Topics include creating multi-container pods, using Git repositories inside pods, attaching persistent storage, and dynamic provisioning of persistent storage.',\n",
       " \"Kubernetes provides storage volumes that allow new containers to continue where the last one finished, preserving directories with actual data across container restarts. Volumes are defined in a pod's specification and must be mounted in each container that needs to access it, allowing multiple containers to share disk storage and enabling them to work together effectively.\",\n",
       " 'The document introduces the concept of volumes in Kubernetes, where multiple containers within a pod can share storage without relying on shared filesystems. Three containers are used as examples: WebServer, ContentAgent, and LogRotator, each with its own filesystem but sharing two volumes, publicHtml and logVol, mounted at different paths to illustrate this concept.',\n",
       " \"Volumes in Kubernetes allow attaching disk storage to containers, enabling them to operate on the same files. A volume is bound to a pod's lifecycle and can be mounted at arbitrary locations within the file tree. Various types of volumes are available, including emptyDir, hostPath, gitRepo, nfs, gcePersistentDisk, awsElasticBlockStore, and azureDisk, each with its own purpose and use case. To access a volume from within a container, a VolumeMount must be defined in the container's spec.\",\n",
       " 'Volumes in Kubernetes can be used to share data between containers or for exposing Kubernetes resources and cluster information. Special types of volumes like secret, downwardAPI, and configMap are used to expose metadata to apps running in a pod. A single pod can use multiple volumes of different types at the same time, with each container having the option to mount or not. An emptyDir volume is useful for sharing files between containers or for temporary data storage by a single container.',\n",
       " 'To create a pod that uses a shared volume, you need to build a Docker image with the required binary (fortune) and script (fortuneloop.sh). The image is based on ubuntu:latest, installs fortune, adds the script to /bin folder, and sets it as the ENTRYPOINT. You then create a pod manifest (fortune-pod.yaml) that specifies two containers sharing the same volume. Finally, you can run the pod using kubectl apply -f fortune-pod.yaml',\n",
       " \"A pod contains two containers and a shared volume between them. The html-generator container writes to the volume every 10 seconds, while the web-server container serves files from it. By forwarding port 80 on the local machine to the pod's port, users can access the Nginx server through localhost:8080 and receive a different fortune message with each request.\",\n",
       " 'An emptyDir volume can be created on tmpfs filesystem for better performance, while a gitRepo volume clones and checks out a Git repository at pod startup. The files in a gitRepo volume are not kept in sync with the referenced repo, but are updated when a new pod is created. This type of volume is useful for storing static HTML files or serving the latest version of a website.',\n",
       " 'Using volumes in Kubernetes, specifically gitRepo volumes, allows sharing data between containers. This is demonstrated by running a web server pod serving files from a cloned Git repository, where the pod is created with a single Nginx container and a single gitRepo volume that clones the repository into the root directory of the volume.',\n",
       " \"To keep files in sync with a Git repository, you can create a sidecar container that runs a Git sync process. This process can be run in an existing container image from Docker Hub, such as 'git sync'. The sidecar container should mount the gitRepo volume and configure the Git sync process to keep the files in sync with the Git repo. This method is recommended instead of using a gitRepo volume for private Git repositories, which are not supported by Kubernetes.\",\n",
       " \"A gitRepo volume is created for and used exclusively by a pod, but its contents can survive multiple pod instantiations if the volume type is different. hostPath volumes allow pods to access files on the node's filesystem, making it possible for system-level pods to read or use the node's devices through the filesystem.\",\n",
       " \"HostPath volumes are not suitable for storing a database's data directory as they store contents on a specific node's filesystem, making it sensitive to scheduling. Instead, use them to access the node's log files, kubeconfig, or CA certificates. System-wide pods like fluentd-kubia use hostPath volumes to access node's data.\",\n",
       " 'To persist data across pods, a network-attached storage (NAS) is needed. A GCE Persistent Disk can be used as underlying storage mechanism on Google Kubernetes Engine. The disk must be created in the same zone as the Kubernetes cluster and its size should be at least 200GB for optimal I/O performance.',\n",
       " \"This chapter explains how to attach disk storage to containers using Kubernetes volumes. It provides an example of creating a 1 GiB GCE persistent disk called 'mongodb' and configuring a pod to use it as a volume, mounting it at '/data/db'. The YAML for the pod is provided, specifying the gcePersistentDisk type, fsType as ext4, and mountPath as /data/db. A note is also given for using Minikube, where you can't use a GCE Persistent Disk, but instead deploy mongodb-pod-hostpath.yaml using a hostPath volume.\",\n",
       " 'To use persistent storage, write data to the MongoDB database by running the MongoDB shell inside the container and inserting JSON documents. The data will be stored on a GCE persistent disk. After deleting and re-creating the pod, the new pod can read the persisted data from the previous pod, using the same GCE persistent disk.',\n",
       " 'You can attach disk storage to containers using Kubernetes volumes, such as GCE Persistent Disk, awsElasticBlockStore, azureFile, or azureDisk. These volumes provide persistent storage for pods, allowing data to be retained across pod instances. To use a different volume type, create the underlying storage and set properties in the volume definition.',\n",
       " \"Kubernetes supports various storage technologies, including NFS, ISCSI, GlusterFS, and others. However, it's recommended to use volumes in a way that decouples pod definitions from specific clusters, avoiding infrastructure-related details in pod specifications.\",\n",
       " 'Kubernetes aims to hide infrastructure from developers, allowing them to request persistent storage without knowing specific details. Cluster admins configure the cluster to provide what apps request, using PersistentVolumes and PersistentVolumeClaims to decouple pods from underlying storage technology.',\n",
       " 'A cluster administrator creates a PersistentVolume resource through the Kubernetes API server, specifying its size and access modes. A user then creates a PersistentVolumeClaim manifest, specifying their required size and access mode, which is bound to an existing PersistentVolume. The volume can be used in a pod, but other users cannot use it until the claim is released.',\n",
       " \"A PersistentVolume is created by specifying its capacity, access modes, and storage type. The administrator can then claim the PV with a PersistentVolumeClaim, which allows a container to read from or write to it. A PV is cluster-level resource like nodes and doesn't belong to any namespace. It's created with kubectl create command and shown as Available until claimed.\",\n",
       " 'To use a PersistentVolume in a Kubernetes pod that requires persistent storage, you need to create a PersistentVolumeClaim (PVC) first. This is done by preparing a PVC manifest and posting it to the Kubernetes API through kubectl create. The PVC claims the PersistentVolume for exclusive use within a namespace, allowing the same PVC to stay available even if the pod is rescheduled.',\n",
       " \"A Kubernetes PersistentVolumeClaim is created with a requested 1Gi of storage and ReadWriteOnce access mode. The claim is bound to a matching PersistentVolume, which is shown as Bound in kubectl get pvc and pv commands. The PersistentVolume's capacity and access modes match the claim's requirements.\",\n",
       " \"To use a PersistentVolume in a pod, reference the PersistentVolumeClaim by name inside the pod's volume. A Pod can claim and use the same PersistentVolume until it is released, allowing decoupling from underlying storage technology.\",\n",
       " 'The chapter discusses the benefits of using PersistentVolumes (PVs) and claims to attach disk storage to containers, making it simpler for application developers by abstracting away infrastructure-specific details. A pod can use a GCE Persistent Disk either directly or through a PV and claim, allowing for greater flexibility and portability across different Kubernetes clusters.',\n",
       " \"When a PersistentVolumeClaim is deleted, its status becomes Pending and it's no longer bound to a PersistentVolume, which can be reused by other pods after being manually recycled or reclaimed automatically using Retain, Recycle, or Delete policies, allowing the reuse of volumes across different namespaces.\",\n",
       " 'A PersistentVolume only supports Retain or Delete policies. The reclaim policy can be changed on an existing PersistentVolume. Kubernetes also performs dynamic provisioning of PersistentVolumes through persistent-volume provisioners and StorageClass objects, allowing users to choose the type of PersistentVolume they want.',\n",
       " 'Dynamic provisioning of PersistentVolumes allows administrators to define one or two StorageClasses, enabling the system to create new PersistentVolumes each time a PersistentVolumeClaim is requested. This eliminates the possibility of running out of PersistentVolumes. The StorageClass resource specifies the provisioner and parameters for provisioning, which can be specific to cloud providers like GCE. Users can refer to the storage class by name in their PersistentVolumeClaims, enabling dynamic provisioning of PersistentVolumes.',\n",
       " \"A PersistentVolumeClaim (PVC) can specify a custom storage class, such as 'fast', which is referenced by a provisioner to create a PersistentVolume. The provisioner is used even if an existing manually provisioned PV matches the PVC. If the storage class does not exist, provisioning will fail. The dynamically created PV has the requested capacity and access modes, with a reclaim policy of Delete, meaning it will be deleted when the PVC is deleted.\",\n",
       " 'Dynamic provisioning of PersistentVolumes allows cluster admins to create multiple storage classes with different performance characteristics. Developers can then choose which one is most appropriate for each claim they create. This makes PVC definitions portable across different clusters as long as StorageClass names are the same, demonstrating flexibility and consistency in Kubernetes environments.',\n",
       " 'The default storage class in a GKE cluster is defined by an annotation, which makes it the default storage class. A PersistentVolumeClaim can be created without specifying a storage class and a GCE Persistent Disk of type pd-standard will be provisioned for you.',\n",
       " 'Dynamic provisioning of PersistentVolumes uses the default storage class when creating a PVC. To bind a PVC to a manually pre-provisioned PV, explicitly set storageClassName to an empty string. This prevents the dynamic provisioner from provisioning a new PV and allows the PVC to use the existing one.',\n",
       " 'This chapter explains how volumes provide temporary or persistent storage to containers in a pod. Key concepts include creating multi-container pods with shared files using volumes, mounting external storage for persistence across restarts, and dynamically provisioning PersistentVolumes through PersistentVolumeClaims and StorageClasses.',\n",
       " 'ConfigMaps and Secrets allow passing configuration data to Kubernetes applications, configuring containerized applications by changing the main process, passing command-line options, setting environment variables, or using ConfigMaps for non-sensitive settings and Secrets for sensitive info like credentials.',\n",
       " 'ConfigMaps and Secrets allow storing configuration data and sensitive information separately from container images. ConfigMaps store config data as a top-level Kubernetes resource, while Secrets handle sensitive info like credentials or encryption keys with special care. This allows for easier management of config changes and keeping sensitive data secure.',\n",
       " \"You can pass command-line arguments to Docker containers by specifying them in the docker run command, overriding any default arguments set in the image's Dockerfile. This is achieved through the ENTRYPOINT instruction, which defines the executable, and CMD, which specifies the default arguments. The ENTRYPOINT instruction supports two forms: shell and exec, where shell form invokes the command inside a shell and exec form runs it directly.\",\n",
       " 'This chapter discusses ConfigMaps and Secrets in Kubernetes, specifically how to make an interval configurable in a Docker image using the exec form of the ENTRYPOINT instruction and setting a default value with the CMD instruction. A script is modified to accept an INTERVAL variable from the command line, and the Dockerfile is updated to use this new script and set the default interval to 10 seconds.',\n",
       " \"Kubernetes allows overriding command and arguments in a container by setting 'command' and 'args' fields in the container specification. This can be done when creating a pod, but not updated after it's created. The equivalent Dockerfile instructions are ENTRYPOINT and CMD.\",\n",
       " 'The chapter discusses ConfigMaps and Secrets for configuring applications in Kubernetes. It explains how to pass configuration options through command-line arguments using the args array, and also through environment variables. The example uses the fortune:args image to generate a new fortune every two seconds.',\n",
       " 'To make the interval in your fortuneloop.sh script configurable through an environment variable, remove the row where the INTERVAL variable is initialized. This allows the script to be configured from an environment variable, and can be used with Docker containers. To specify environment variables in a container definition, set them inside the container definition, not at the pod level. This can be done using YAML files like fortune-pod-env.yaml.',\n",
       " 'You can reference previously defined environment variables by using the $(VAR) syntax, decoupling configuration from pod descriptors using ConfigMaps, and passing values as environment variables or files in a volume.',\n",
       " 'ConfigMaps allow you to keep configuration separate from your app, making it easy to switch between environments by using different config values in each environment without changing the pod specification.',\n",
       " 'A ConfigMap is used to store and manage configuration data for applications. It can be created using the kubectl create configmap command, which allows defining entries by passing literals or creating from files. A ConfigMap named fortune-config was created with a single entry sleep-interval=25, and its YAML descriptor was inspected using kubectl get command.',\n",
       " 'ConfigMaps in Kubernetes can store configuration data, including complete config files, which can be created using the `kubectl create configmap` command. Files can be added individually or from a directory, and keys can be specified manually. ConfigMaps can also combine different options, such as literal values, files, and directories, to create a single map entry.',\n",
       " 'You can pass ConfigMap entries to a container as environment variables using the valueFrom field in the pod descriptor. The pod descriptor should have an apiVersion of v1 and kind of Pod, with a ConfigMap named my-config. You can specify the key-value pairs from the ConfigMap as environment variables using --from-file or --from-literal flags, allowing you to pass JSON data and literal values to the container.',\n",
       " \"This section shows how to decouple configuration from pod specification using a ConfigMap. A ConfigMap is referenced in the pod definition, allowing configuration options to be kept together and avoiding duplication across multiple pod manifests. If a referenced ConfigMap doesn't exist, the container referencing it will fail to start, but other containers will start normally. Optional references can also be marked, enabling containers to start even if the ConfigMap doesn't exist.\",\n",
       " 'Kubernetes version 1.6 allows exposing all entries of a ConfigMap as environment variables using the envFrom attribute, instead of individual env variables. This can be done by specifying a prefix for the environment variables, which will result in environment variables with the same name as the keys. However, if a ConfigMap key is not in the proper format (e.g., contains a dash), it will skip the entry and record an event. Additionally, ConfigMap entries cannot be referenced directly in the pod.spec.containers.args field, but can be passed as command-line arguments by first initializing an environment variable from the ConfigMap entry.',\n",
       " 'A ConfigMap can be used to expose entries as files or to pass configuration options as environment variables. A special volume type, configMap volume, can expose each entry of a ConfigMap as a file, allowing the container process to obtain the value by reading the contents of the file. This approach is suitable for exposing whole config files contained in a ConfigMap.',\n",
       " \"A ConfigMap is created to pass a config file to an Nginx web server running inside a pod's web-server container, enabling gzip compression for plain text and XML files. A new directory is created on the local disk with two files: my-nginx-config.conf containing the Nginx config and sleep-interval with the number 25. The ConfigMap is then created from these files using kubectl.\",\n",
       " \"A ConfigMap is used to decouple configuration with Kubernetes, allowing for easy management and updates of configurations. It can contain multiple entries, each with its own key-value pair, and can be referenced in a pod's container using a volume populated with the ConfigMap's contents. This allows for the use of default configuration files while still adding custom configurations, as shown in Listing 7.14.\",\n",
       " \"This chapter discusses ConfigMaps and Secrets for configuring applications in Kubernetes. A Pod is defined with a volume referencing a fortune-config ConfigMap, which is mounted into the /etc/nginx/conf.d directory to make Nginx use it. The configuration can be verified by port-forwarding and checking the server's response with curl, demonstrating that the mounted ConfigMap entries are being used by the web server.\",\n",
       " \"A ConfigMap can be decoupled from a pod's configuration using a ConfigMap volume. This allows certain entries to be exposed as files in a directory, while others remain hidden. The `items` attribute of the volume can be used to specify which ConfigMap entries should be included as files, and the `key` and `path` fields define how the entry is stored. This approach enables fine-grained control over the configuration of containers within a pod.\",\n",
       " 'When mounting a ConfigMap or Secret as a directory, it hides existing files in that directory. To add individual files without hiding others, use the `subPath` property on the volumeMount to mount either a single file or directory from the volume, preserving the original files.',\n",
       " 'A ConfigMap can be used to decouple configuration from an application, allowing for easy updates without restarting the app. A subPath can be used to mount individual files from a volume instead of the whole volume, but this method has limitations with file updating. File permissions in a ConfigMap volume default to 644 and can be changed using the defaultMode property. Updating a ConfigMap updates all referencing volumes, allowing for configuration changes without app restarts.',\n",
       " \"ConfigMaps and Secrets can be edited using kubectl edit, which updates the files exposed in the configMap volume atomically using symbolic links. Changes to the ConfigMap are reflected in the actual file, but Nginx doesn't reload its config automatically. To signal Nginx to reload its config, use 'nginx -s reload' command within a pod. This allows changing an app's config without restarting the container or recreating the pod.\",\n",
       " \"Kubernetes uses symbolic links to update ConfigMap volumes when a new directory is created. However, updating individual files in an existing directory does not trigger an update. Modifying an existing ConfigMap while pods are using it may not be ideal if the app doesn't reload its config automatically, as different running instances will have different configs.\",\n",
       " 'Kubernetes provides Secrets to store and distribute sensitive information, which can be used like ConfigMaps. They are stored in memory on nodes and encrypted in etcd from v1.7. Choose between Secret and ConfigMap based on sensitivity: use ConfigMap for non-sensitive data and Secret for sensitive data. A default token Secret is automatically mounted into every container, accessible with kubectl get secrets.',\n",
       " 'Secrets are used to pass sensitive data to containers, and contain entries like ca.crt, namespace, and token which provide secure access to the Kubernetes API server from within pods. The default-token Secret is mounted into every container by default, but can be disabled in each pod or service account.',\n",
       " 'Creating a Secret involves generating certificate and private key files, then using kubectl to create a generic Secret called fortune-https from these files and an additional dummy file containing the string bar. This process is similar to creating ConfigMaps, but with the added security of keeping sensitive information like private keys secure within the Secret.',\n",
       " 'Secrets in Kubernetes can hold sensitive or non-sensitive binary data, which are encoded as Base64 strings. This contrasts with ConfigMaps that store plain-text data. Secrets have a maximum size limit of 1MB and can be used even for non-sensitive binary data.',\n",
       " 'Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved. Secrets are decoded and written to files or environment variables in their actual form, allowing apps to read them directly without decoding.',\n",
       " 'This document explains how to pass sensitive data (SSL certificates) to a container using Kubernetes secrets. It shows an example of mounting a secret volume in a pod, specifically for a fortune-https pod that uses Nginx and mounts the certificate and key files from a secret volume at /etc/nginx/certs.',\n",
       " 'A Kubernetes pod is configured using a ConfigMap and a Secret, with the ConfigMap providing application configuration and the Secret providing sensitive data such as SSL certificates. The ConfigMap and Secret are referenced in the pod descriptor through their respective names, with file permissions specified for the Secret files.',\n",
       " \"Using Kubernetes secrets to pass sensitive data to containers, a pod's HTTPS traffic can be tested by opening a port-forward tunnel and using curl. The server's certificate can also be checked with curl. Secrets are stored in memory (tmpfs) and do not write to disk, making them secure. Alternatively, secret entries can be exposed as environment variables.\",\n",
       " 'ConfigMaps and Secrets in Kubernetes allow applications to access configuration data and secrets. However, exposing secrets through environment variables is not recommended due to security risks. Instead, use secret volumes or pass credentials to Kubernetes itself using image pull secrets for private container registries.',\n",
       " \"To pass sensitive data to containers, create a Secret holding Docker registry credentials using kubectl create secret docker-registry command. Specify the Secret's name in the pod spec as an imagePullSecrets. This enables pulling images from a private image registry. Alternatively, add Secrets to a ServiceAccount to automatically include them in all pods.\",\n",
       " 'This chapter summarizes how to pass configuration data to containers using ConfigMaps and Secrets, including overriding commands, passing arguments, setting environment variables, decoupling config from pods, storing sensitive data in Secrets, and creating a docker-registry Secret.',\n",
       " 'This chapter explores how applications can access pod metadata, resources, and interact with the Kubernetes API server. It covers using the Downward API to pass information into containers, exploring the Kubernetes REST API, accessing the API server from within a container, and understanding the ambassador container pattern.',\n",
       " \"The Kubernetes Downward API allows passing metadata about a pod and its environment through environment variables or files, solving problems of repeating information in multiple places, and exposing pod metadata to processes running inside the pod, with currently available metadata being the pod's name, IP address, and labels/annotations.\",\n",
       " \"The Downward API allows passing metadata such as namespace, node name, service account, CPU and memory requests/limits, labels, and annotations to containers through environment variables or a volume. This can be useful for providing containerized processes with information about their environment. An example is provided in the form of a simple single-container pod manifest that passes the pod's and container's metadata to the container using environment variables.\",\n",
       " 'The process can access environment variables defined in the pod spec. Environment variables expose pod metadata such as name, IP, namespace, and node name. Additionally, variables are created for CPU requests and memory limits with a divisor to convert values into desired units.',\n",
       " \"The Downward API allows passing metadata from a pod's or container's attributes as environment variables to the running application. This is demonstrated by creating a pod with specified CPU and memory limits, then using kubectl exec to show the resulting environment variables in the container. The divisor for CPU limits can be 1 (one whole core) or 1m (one millicore), while memory limits can use units such as 1K or 1Mi.\",\n",
       " \"Pods can expose metadata through environment variables or files in a downwardAPI volume. Environment variables can only pass single-value metadata, while a downwardAPI volume allows exposing labels and annotations. A Pod's name and namespace can be exposed through a downwardAPI volume by specifying the fieldRef path to the metadata.name and metadata.namespace fields.\",\n",
       " 'The Downward API allows passing metadata through a volume, mounting it in the container under /etc/downward. Each item specifies the path where metadata should be written and references either a pod-level field or a container resource field.',\n",
       " \"This chapter discusses accessing pod metadata and other resources from applications using Kubernetes. It explains how to mount a downwardAPI volume, which makes available various metadata fields such as labels, annotations, and container resource requests as files within the pod's filesystem. The contents of these files can be accessed using kubectl exec commands. Additionally, it highlights that labels and annotations can be modified while a pod is running, and Kubernetes updates the corresponding files in the downwardAPI volume.\",\n",
       " \"When exposing container-level metadata using the Downward API, you need to specify the name of the container whose resource field you're referencing. This is because volumes are defined at the pod level, not at the container level. Using the Downward API allows you to keep your application Kubernetes-agnostic by passing data from the pod and containers to the process running inside them. However, it only exposes a limited subset of metadata, so if your app needs more information about other pods or resources in the cluster, you'll need to obtain that directly from the Kubernetes API server.\",\n",
       " \"The Kubernetes API server provides REST endpoints for accessing pod metadata and other resources, but requires authentication. To access it directly, use kubectl proxy to run a proxy server that handles authentication and verifies the server's certificate on each request, allowing apps within pods to talk to the API server and get information about other resources or the most up-to-date data possible.\",\n",
       " 'To explore the Kubernetes API through kubectl proxy, run $ kubectl proxy to start serving on 127.0.0.1:8001, then use curl or a web browser to navigate to http://localhost:8001 to list available API paths and resource types.',\n",
       " 'The Kubernetes batch API group has two versions (v1 and v2alpha1), with v1 being the preferred version. The /apis/batch path displays the available versions, while /apis/batch/v1 shows a list of resource types in this group, including jobs which are namespaced.',\n",
       " 'The Kubernetes API server returns a list of resource types and REST endpoints in the batch/v1 API group. The Job resource is exposed with verbs to retrieve, update, delete, create, watch, patch and get. Additional API endpoints are also available for modifying job status.',\n",
       " \"Accessing pod metadata and other resources from applications involves using Kubernetes REST API. You can retrieve information about pods, jobs, and namespaces using curl commands. To talk to the API server from within a pod, you need to find its location, authenticate with it, and ensure you're not talking to an impersonator. This allows applications running in pods to interact with Kubernetes services.\",\n",
       " \"To communicate with the Kubernetes API server, create a pod using the tutum/curl image and run a shell inside it. Find the API server's address by looking up environment variables KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT within the container.\",\n",
       " \"To access pod metadata and other resources from applications, use the environment variables to get the service's port number. However, always verify the server's identity by checking its certificate. Use curl with the --cacert option to specify the CA certificate, which is stored in a Secret called default-token-xyz. This verifies that the server's certificate is signed by the CA and prevents man-in-the-middle attacks.\",\n",
       " 'To access the Kubernetes API server, set the CURL_CA_BUNDLE environment variable to trust its certificate. Then, authenticate with the server by loading an authentication token into an environment variable using TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token). Finally, send requests to the API server using curl -H \"Authorization: Bearer $TOKEN\" https://kubernetes',\n",
       " 'You can access pod metadata and other resources from applications by passing a token inside the Authorization HTTP header. You can also retrieve the namespace of your pod by reading the contents of the /var/run/secrets/kubernetes.io/serviceaccount/namespace file into the NS environment variable. With this information, you can list all pods running in the same namespace as your pod using a GET request to https://kubernetes/api/v1/namespaces/$NS/pods. Additionally, you can disable role-based access control (RBAC) by creating a clusterrolebinding with cluster-admin privileges for service accounts.',\n",
       " \"An app running in a pod can access the Kubernetes API by verifying the API server's certificate, authenticating with a bearer token, and using a namespace file to pass the namespace for CRUD operations. This process can be simplified using an ambassador container, which makes communication with the API server more straightforward while keeping it secure.\",\n",
       " 'The chapter discusses how to access pod metadata and resources from applications using the kubectl proxy command, which can be run inside pods as an ambassador container pattern. This allows applications to query the API server securely through the ambassador without direct communication with the API server.',\n",
       " 'This page discusses connecting to a Kubernetes API server using an ambassador container. The ambassador container runs `kubectl proxy` and handles authentication with the API server, allowing the main container to send plain HTTP requests to localhost:8001. This simplifies communication between containers and can be reused across different apps, but adds an additional process consuming resources.',\n",
       " 'Kubernetes API client libraries are available for various programming languages, including Golang, Python, Java, Node.js, and PHP, allowing applications to access pod metadata and other resources from the API server.',\n",
       " 'The page discusses interacting with the Kubernetes API server using various client libraries such as Ruby, Clojure, Scala, Perl, and Java (Fabric8 client). It provides examples of how to list services, create, edit, and delete pods in a Java app using the Fabric8 client, highlighting the simplicity and efficiency of these interactions.',\n",
       " \"The chapter discusses accessing pod metadata and other resources from applications using the Fabric8 client's fluent DSL API. If no client is available, you can use Swagger to generate a client library and documentation. The Kubernetes API server exposes Swagger definitions at /swaggerapi and OpenAPI spec at /swagger.json. You can explore the API with Swagger UI, which provides a web interface for interacting with REST APIs.\",\n",
       " \"You now know how your app running inside a pod can get data about itself, other pods, and components deployed in the cluster through environment variables or downwardAPI volumes. You've learned to access CPU and memory requests, browse the Kubernetes REST API, find the API server's location, authenticate yourself, and use client libraries to interact with Kubernetes.\",\n",
       " 'This chapter covers updating apps running in a Kubernetes cluster, focusing on using Deployments to perform zero-downtime updates. Key topics include replacing pods with newer versions, updating managed pods, and performing rolling updates, as well as automatically blocking rollouts of bad versions and controlling the rollout rate.',\n",
       " 'Updating applications running in pods involves replacing old pods with new ones, either by deleting existing pods first and then starting the new ones or by adding new pods while gradually removing old ones, requiring app to handle two versions simultaneously.',\n",
       " 'In Kubernetes, you can update applications declaratively using Deployments, which can be updated automatically or manually. The manual method involves updating the pod template of a ReplicationController to refer to a new image version, then deleting the old pods. Alternatively, you can spin up new pods and delete the old ones without downtime, if your app supports running multiple versions at once.',\n",
       " \"You can update applications running in pods by combining replication controllers and services, switching from the old to the new version at once using a blue-green deployment or performing a rolling update where you replace pods step by step. This requires updating the service's pod selector and scaling down the previous replication controller while scaling up the new one.\",\n",
       " 'You can perform an automatic rolling update with a ReplicationController by having kubectl do it, but this is now an outdated way of updating apps. The process involves running the initial version of the app, creating a modified version that returns its version number in the response, and using two ReplicationControllers to roll out the new version.',\n",
       " 'A single YAML file is used to create both a ReplicationController and a LoadBalancer Service, enabling external access to the app. The YAML defines 3 replicas of a pod running the luksa/kubia:v1 image, and exposes it on port 80 with targetPort 8080. After posting the YAML to Kubernetes, the three v1 pods and load balancer run, allowing curl requests to be made to the external IP.',\n",
       " \"This chapter discusses deploying and updating applications declaratively using Kubernetes Deployments. It explains how to perform a rolling update with kubectl by creating a new version of an app without disrupting existing traffic. The importance of setting the container's imagePullPolicy property to Always when pushing updates to the same image tag is also highlighted, especially when using tags other than latest. This ensures that all nodes run the updated image.\",\n",
       " 'To perform an automatic rolling update with a ReplicationController in Kubernetes, run the kubectl rolling-update command and specify the old RC, new RC name, and new image. A new RC will be created immediately, referencing the new image and initially having a desired replica count of 0. The system will then scale up the new RC while scaling down the old one, keeping the total number of pods at 3.',\n",
       " \"Before performing a rolling update, kubectl modifies the ReplicationController's selector and adds an additional deployment label to its pod template, ensuring that only pods managed by the new controller are selected. The old controller is also modified with a new selector, allowing it to see zero matching pods, but the live pods' labels have been updated to include the new deployment label, preventing them from being seen by the old controller.\",\n",
       " 'Performing a rolling update with a ReplicationController using kubectl involves scaling up a new controller while scaling down an old one, replacing old pods with new ones. This process deletes v1 pods and replaces them with v2 pods, eventually directing all requests to the new version. The Service redirects requests to both old and new pods during the rolling update, progressively increasing the percentage of requests hitting v2 pods.',\n",
       " \"Kubernetes' ReplicationController can be updated declaratively using `kubectl` commands, allowing for zero-downtime updates. However, the deprecated `kubectl rolling-update` command modifies existing objects and is not recommended. Instead, use explicit `kubectl` commands to scale and update resources, which provides greater control and avoids unexpected modifications.\",\n",
       " 'Deployments in Kubernetes provide a declarative way to update applications by introducing a ReplicaSet that creates and manages pods, providing a more scalable and efficient way of updating applications compared to using ReplicationControllers or ReplicaSets directly.',\n",
       " \"A Deployment resource in Kubernetes is used to update applications declaratively by defining the desired state and letting Kubernetes handle the rest. A Deployment can have multiple pod versions running under its wing, so its name shouldn't reference the app version. Creating a Deployment requires specifying a deployment strategy and only three trivial changes are needed to modify a ReplicationController YAML file to describe a Deployment.\",\n",
       " 'Creating a Deployment in Kubernetes involves deleting any existing ReplicationControllers and pods, then running `kubectl create -f kubia-deployment-v1.yaml --record` to create a new Deployment. The `--record` option records the command in the revision history. To check the status of the Deployment rollout, use `kubectl rollout status deployment kubia`. A Deployment creates ReplicaSets, which then create pods with unique names that include a numeric value corresponding to the hashed value of the pod template and ReplicaSet managing them.',\n",
       " 'Deployments provide an easy way to update applications declaratively, using a template that can always use the same ReplicaSet for a given version of the pod template. Updating a Deployment only requires modifying its pod template and Kubernetes takes care of replacing all original pods with new ones, achieving the desired state through a configured deployment strategy, either rolling update or recreate.',\n",
       " 'The RollingUpdate strategy in Kubernetes allows for updating applications declaratively by removing old pods one by one and adding new ones at the same time, keeping the application available throughout the process. To slow down the update process, set the minReadySeconds attribute on the Deployment. Triggering the actual rollout is done by changing the image used in the single pod container to a new version using the kubectl set image command.',\n",
       " \"Kubernetes deployments can be updated using various methods such as kubectl edit, patch, apply, replace and set image. These methods change the Deployment's specification, triggering a rollout process. The deployment can also be modified by updating its pod template or container image. Examples of these methods are provided in the text.\",\n",
       " 'Using Deployments allows for declarative updates to apps by changing the pod template in a single field, which is then performed by Kubernetes controllers. This process is simpler than running special commands with kubectl. Note that modifying ConfigMaps will not trigger an update unless referencing a new ConfigMap.',\n",
       " 'This chapter discusses Deployments in Kubernetes, which allows for declarative updates to applications. A Deployment resource manages ReplicaSets, making it easier to manage compared to ReplicationControllers. The chapter simulates a problem during a rollout process by introducing a bug in version 3 of an app that returns a 500 error after the fifth request.',\n",
       " 'To update an app declaratively, change the image in the Deployment specification using $ kubectl set image deployment kubia nodejs=luksa/kubia:v3. Follow rollout progress with kubectl rollout status and roll back to previous revision with kubectl rollout undo deployment kubia.',\n",
       " 'Deployments in Kubernetes keep a revision history of rollouts, which can be displayed with kubectl rollout history command. This history allows rolling back to any revision by specifying the revision number in the undo command. The length of the revision history is limited by the revisionHistoryLimit property and older ReplicaSets are deleted automatically.',\n",
       " 'Deployments in Kubernetes allow for declarative updates to apps. The rollout process can be controlled using the `maxSurge` and `maxUnavailable` properties of the rolling update strategy. These properties determine how many pod instances are allowed above the desired replica count (maxSurge) and how many can be unavailable during the update (maxUnavailable). Both default to 25% but can also be specified as absolute values.',\n",
       " 'Deployments can be updated declaratively using `maxSurge` and `maxUnavailable` properties, which control the number of unavailable pods during a rollout. The `extensions/v1beta1` version sets both to 1 instead of 25%, affecting the rollout process as shown in figures 9.12 and 9.13.',\n",
       " 'A Deployment in Kubernetes allows for declarative updates to an application, ensuring that a minimum number of replicas are always available during the rollout process. The `maxUnavailable` property is relative to the desired replica count, meaning that at least one pod must be available when updating from three replicas. Additionally, Deployments can be paused during the rollout process, enabling the creation of a single new pod alongside existing ones for verification before proceeding with the full update.',\n",
       " 'Deployments in Kubernetes can be paused or resumed using the `rollout pause` and `rollout resume` commands, allowing for a controlled rollout process. The `minReadySeconds` property can also be used to block rollouts of malfunctioning versions by specifying how long a new pod must be ready before being treated as available.',\n",
       " 'Deployments can be used for updating apps declaratively, with a properly configured readiness probe and minReadySeconds setting, Kubernetes can prevent deploying buggy versions. A Deployment is updated using kubectl apply command with YAML that includes apiVersion, kind, metadata, spec, replicas, minReadySeconds, strategy, rollingUpdate, maxSurge, maxUnavailable, type, template, metadata, name, labels, app, and containers with image set to luksa/kubia:v3.',\n",
       " \"To update a Deployment with kubectl apply, use the command $ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml. This updates the Deployment with everything defined in the YAML file, including the image and readiness probe definition. To keep the desired replica count unchanged, don't include the replicas field in the YAML. The rollout status command can be used to follow the update process, which will create new pods but doesn't guarantee that they'll be used.\",\n",
       " 'A deployment is prevented from updating an app declaratively due to a failing readiness probe in the new pod, which returns HTTP status code 500 and gets removed as an endpoint from the service, until it becomes available for at least 10 seconds.',\n",
       " 'Deployments allow updating applications declaratively, and the rollout process can be configured with deadlines and minReadySeconds. If a rollout fails, it can be aborted using `kubectl rollout undo` command. The `progressDeadlineSeconds` property in Deployment spec is configurable to set a deadline for the rollout.',\n",
       " 'This chapter taught you a declarative approach to deploying and updating applications in Kubernetes, including rolling updates, Deployments, and controlling rollout rates using maxSurge and maxUnavailable properties.',\n",
       " 'This chapter focuses on deploying stateful clustered applications using StatefulSets, providing separate storage for each replicated pod instance, guaranteeing stable names and hostnames for pod replicas, controlling start/stop sequences, and peer discovery via DNS SRV records.',\n",
       " \"Stateful pods can't be replicated using ReplicaSets as they require separate storage for each instance, which is not possible with a single ReplicaSet. Options include creating pods manually or using one ReplicaSet per pod instance, but these are cumbersome and not viable for scaling.\",\n",
       " 'StatefulSets allow deploying replicated stateful applications, but using multiple ReplicaSets is not ideal. A workaround is to have a single ReplicaSet with pods using the same PersistentVolume, but each instance selecting and creating its own separate file directory, requiring coordination between instances.',\n",
       " \"Certain apps require a stable network identity, but Kubernetes assigns a new hostname and IP every time a pod is rescheduled. To work around this, create a dedicated service for each individual member, providing a stable network address, similar to creating a ReplicaSet for individual storage. This setup can be seen in Figure 10.4, but it's still not a complete solution as pods can't self-register using the stable IP.\",\n",
       " 'StatefulSets in Kubernetes deploy replicated stateful applications with stable names and states, treating instances as non-fungible individuals like pets, requiring replacement with new instances having same name, network identity, and state as the old one when it fails or is replaced.',\n",
       " 'A StatefulSet ensures pods are rescheduled to retain their identity and state, allowing easy scaling. Pods created by StatefulSets have unique volumes and stable identities, with each pod assigned an ordinal index used for naming and hostname. A governing headless Service is required to provide a network identity, enabling addressability by hostname.',\n",
       " 'StatefulSets allow deploying replicated stateful applications, enabling access to pods through fully qualified domain names and DNS lookups. They also ensure replacement of lost pods with new instances having the same name and hostname, unlike ReplicaSets which replace them with unrelated pods.',\n",
       " 'StatefulSets ensure stateful pods have stable identities, even when scaled up or down. Scaling up creates new instances with unused ordinal indexes, while scaling down removes instances with the highest index first, making effects predictable. StatefulSets also prevent scale-down operations if any instance is unhealthy and provide stable dedicated storage to each instance.',\n",
       " 'In Kubernetes, StatefulSets deploy replicated stateful applications by creating separate PersistentVolumeClaims for each pod instance. The StatefulSet stamps out these claims along with the pod instances, allowing for persistent storage to be attached to each pod. Scaling up a StatefulSet creates new API objects, including one or more PersistentVolumeClaims, while scaling down deletes only the pod, leaving the claims intact. Manual deletion of PersistentVolumeClaims is required to release the underlying PersistentVolumes and prevent data loss.',\n",
       " \"StatefulSets in Kubernetes allow for stable identity and storage of pods, guaranteeing that a pod's replacement has the same name, hostname, and persistent storage as the original. This means that if a StatefulSet is scaled down and then scaled back up, the new pod instance will have the same persisted state and be reattached to the same PersistentVolumeClaim, preventing data loss and ensuring consistency in the system.\",\n",
       " 'StatefulSets in Kubernetes must ensure two stateful pod instances are never running with the same identity and are bound to the same PersistentVolumeClaim. A StatefulSet must guarantee at-most-one semantics for stateful pod instances, ensuring a pod is no longer running before creating a replacement pod. This affects node failure handling, as demonstrated later in the chapter. A simple clustered data store is built using the kubia app, allowing data storage and retrieval on each pod instance.',\n",
       " 'A simple app is created using Node.js and a Docker image, which writes POST requests to a file and returns stored data on GET requests. To deploy this app through a StatefulSet, PersistentVolumes must be created for storing data files, along with a governing Service required by the StatefulSet, and the StatefulSet itself. For Minikube users, PersistentVolumes can be deployed from a YAML file, while Google Kubernetes Engine users need to create actual GCE Persistent Disks before proceeding.',\n",
       " 'This chapter explains how to deploy replicated stateful applications using StatefulSets. A list of three PersistentVolumes is created from the persistent-volumes-gcepd.yaml file, with each volume having a capacity of 1 Mebibyte and recycling when released. A headless Service called kubia is also created as the governing Service for the StatefulSet, which must be used to provide network identity for stateful pods.',\n",
       " 'A stateless Service is created with a clusterIP field set to None, enabling peer discovery between pods. A StatefulSet manifest is then created with a serviceName and replicas of 2, using a volumeClaimTemplates list to define a Persistent-VolumeClaim for each pod, referencing a persistentVolumeClaim volume in the manifest.',\n",
       " \"StatefulSets in Kubernetes create pods one at a time, ensuring safety for clustered apps sensitive to race conditions. The first pod is brought up fully before continuing to bring up the rest. A closer look at the first pod's spec shows how the StatefulSet constructs the pod from templates and PersistentVolume-Claim template, adding volumes automatically.\",\n",
       " 'A StatefulSet was used to create a PersistentVolumeClaim and volume inside a pod. The PersistentVolumeClaims were listed using kubectl get pvc, showing two claims bound to volumes pv-c and pv-a. Communication with individual pods can be done by proxying through the API server or using port-forwarding.',\n",
       " 'The chapter explains how to deploy replicated stateful applications using StatefulSets in Kubernetes. It demonstrates how to use the kubectl proxy to communicate with a pod, send GET and POST requests to the pod, and store data on the pod. The example uses curl commands to interact with the pod through the API server, showing how to retrieve data from the pod and update it with new information.',\n",
       " 'A StatefulSet was used to store data on a pod. When a GET request is made, the stored data is returned. The pod is then deleted and recreated by the StatefulSet, which still serves the same data as before. This demonstrates that a StatefulSet preserves state even when pods are rescheduled or deleted.',\n",
       " \"A StatefulSet maintains its pods' identities, including hostnames and persistent data, even when scaled down or recreated after deletion. Scaling down a StatefulSet deletes pods but leaves PersistentVolumeClaims intact, with pods deleted in descending order of ordinal numbers. A non-headless Service can be used to expose stateful pods, allowing clients to connect through the Service rather than directly.\",\n",
       " 'To access a cluster-internal service, you can use the proxy feature provided by the API server, or use a pod to access it. The URI path is formed like /api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>. Peer discovery in a StatefulSet is also important for clustered apps to find other members.',\n",
       " \"A StatefulSet's pods can discover their peers by performing an SRV DNS lookup, which points to the hostnames and ports of servers providing a specific service. Kubernetes creates SRV records to point to the hostnames of the pods backing a headless service. A pod can get a list of all other pods by running the dig command inside a temporary pod.\",\n",
       " \"To discover peers in a StatefulSet, the application performs a DNS lookup using the `dns.resolveSrv` method. It queries for SRV records for a specific service name and retrieves a list of addresses. If no peers are found, it returns 'No peers discovered.' Otherwise, it displays data from all cluster nodes.\",\n",
       " 'StatefulSets are used to deploy replicated stateful applications, allowing for multiple pods to be created with a specified replica count. The pod template can be updated using kubectl, and the process involves creating a new image with the desired changes and then applying it to the StatefulSet, which will automatically update each pod with the new image.',\n",
       " 'To update a StatefulSet, edit its definition using `kubectl edit` and modify the spec.replicas and image attributes. Save the file and exit to apply changes. If existing replicas are not updated, delete them manually for the StatefulSet to bring up new ones based on the new template.',\n",
       " \"StatefulSets in Kubernetes allow for replicated stateful applications to be deployed, where each pod has a unique identity and storage. When scaling up or down, pods can discover peers and handle horizontal scaling with ease. However, when a node fails abruptly, StatefulSet cannot create a replacement pod until it knows the old pod is no longer running, which requires manual intervention by the cluster administrator. This can be observed by simulating a node's disconnection from the network by shutting down its eth0 interface.\",\n",
       " \"StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown. If the node comes back online, the pod is marked as Running again. However, if the pod's status remains unknown for more than a few minutes, it is automatically evicted from the node and deleted.\",\n",
       " 'A StatefulSet deployment is experiencing issues due to an unresponsive node. The kubia-0 pod is shown as Terminating, but in reality, its container is still running fine. To resolve this, the pod needs to be deleted manually, and a replacement pod should be created by the StatefulSet. However, upon deletion, the kubia-0 pod remains, with an unknown status, despite being deleted.',\n",
       " \"A pod marked for deletion can't be deleted immediately due to node's network being down. Use --force and --grace-period 0 options with kubectl delete command to forcefully delete the pod. This approach is risky, especially for stateful pods. It's recommended to bring back a disconnected node before continuing.\",\n",
       " 'StatefulSets allow replicated stateful applications to connect with each other through host names and enable forcible deletion of stateful pods, a crucial feature for managing Kubernetes-managed apps.',\n",
       " \"This chapter delves into Kubernetes internals, explaining how resources are implemented. It covers what makes up a cluster, each component's role and functionality, pod scheduling, controllers in the Controller Manager, and resource deployment. Specific topics include Deployments, pods, networking between pods, Services, and achieving high-availability.\",\n",
       " 'A Kubernetes cluster consists of two parts: the Control Plane, which controls the cluster and its components include etcd, API server, Scheduler, Controller Manager, and add-on components such as DNS server, Dashboard, Ingress controller, Heapster, and Container Network Interface plugin. The worker nodes run the Kubelet, Service Proxy, and Container Runtime, with Docker being a common choice. The distributed nature of these components allows for scalability and flexibility in managing containerized applications.',\n",
       " 'Kubernetes system components communicate through the API server, which connects to etcd. Components on the worker node run on the same node, but Control Plane components can be split across multiple servers. The API server exposes a ComponentStatus resource showing health status of each component. The kubectl get command displays statuses of all components.',\n",
       " 'Kubernetes Control Plane components can have multiple instances of etcd and API server running for high availability. However, only one instance of Scheduler and Controller Manager may be active at a given time. These components, along with kube-proxy, can run on the system directly or as pods, with the Kubelet deploying them as pods. The Control Plane components are currently running as pods on the master node in the provided cluster, while worker nodes run kube-proxy and Flannel networking pods.',\n",
       " 'Kubernetes stores cluster state and metadata in etcd, a fast, distributed key-value store. etcd v3 is recommended due to improved performance. Resources are stored under /registry with version numbers for optimistic concurrency control, preventing simultaneous updates.',\n",
       " \"Kubernetes stores resource types like pods, secrets, and services as key-value entries in etcd. These entries are JSON representations of the resources, with each entry corresponding to an individual pod or other resource. Prior to v1.7, secrets were stored unencrypted, but from then on they're encrypted for security.\",\n",
       " \"Kubernetes ensures consistency by requiring all Control Plane components to go through the API server, implementing optimistic locking in a single place and validating data written to the store. In a distributed etcd cluster, RAFT consensus algorithm is used to reach a consensus on the actual state, ensuring that each node's state is either the current or previously agreed-upon state. This prevents split-brain scenarios where the cluster splits into two disconnected groups of nodes, allowing only the group with majority (quorum) to accept state changes.\",\n",
       " 'Kubernetes etcd clusters should have an odd number of instances, ideally 5 or 7, to handle failures and maintain a majority for state transitions. The API server provides a CRUD interface over RESTful API for querying and modifying cluster state, storing it in etcd, validating objects, and handling optimistic locking to prevent concurrent updates. Clients like kubectl post requests to the API server through HTTP POST.',\n",
       " 'The API server authenticates clients using authentication plugins, which extract client data from HTTP requests or certificates. Authorization plugins then determine if the authenticated user can perform requested actions on resources. Admission Control plugins validate and/or modify resource requests, modifying fields, overriding values, or rejecting requests. Examples of Admission Control plugins include AlwaysPullImages, ServiceAccount, NamespaceLifecycle, and ResourceQuota.',\n",
       " 'The Kubernetes API server validates and stores objects in etcd, then notifies clients of resource changes by sending updates to watchers. Clients can watch for changes using an HTTP connection, receiving a stream of modifications to watched objects. The kubectl tool also supports watching resources, allowing users to be notified of creation, modification, or deletion events without needing to constantly poll lists of resources.',\n",
       " \"The document discusses Kubernetes' scheduling process, where the Scheduler waits for newly created pods through the API server's watch mechanism and assigns a node to each new pod. The Scheduler updates the pod definition, and the API server notifies the Kubelet to create and run the pod's containers. The default Scheduler uses an algorithm that filters nodes to obtain acceptable ones and prioritizes them to choose the best one, with round-robin used if multiple nodes have the highest score.\",\n",
       " 'To determine which nodes are acceptable for a pod, Kubernetes Scheduler passes each node through a list of configured predicate functions that check things such as hardware resource availability, memory or disk pressure conditions, and volume usage. After passing all checks, the Scheduler ends up with a subset of eligible nodes from which it selects the best one based on factors like available resources, pod requirements, and affinity rules.',\n",
       " 'The Kubernetes Scheduler can be configured to suit specific needs or infrastructure specifics, or even replaced with a custom implementation. Multiple Schedulers can be run in the cluster, and pods can be scheduled using a specified Scheduler by setting the schedulerName property. The Controller Manager also runs controllers that make sure the actual state of the system converges toward the desired state, as specified in resources deployed through the API server, such as ReplicationController, ReplicaSet, DaemonSet, and Job controllers.',\n",
       " \"Kubernetes controllers are active components that perform work as a result of deployed resources. They watch the API server for changes and perform operations such as creation, update or deletion of resources. Controllers run a reconciliation loop to reconcile actual state with desired state and use the watch mechanism to be notified of changes, also performing periodic re-list operations to ensure they haven't missed anything. Each controller connects to the API server and asks to be notified when a change occurs in its responsible resource list.\",\n",
       " \"The Replication Manager is a controller that makes ReplicationController resources come to life, and it's not the actual work but the manager that does it. It uses the watch mechanism to be notified of changes affecting the desired replica count or number of matched pods, triggering rechecks and actions accordingly. The worker() method contains the magic where all the actual function calls are made.\",\n",
       " 'Kubernetes controllers manage Pod resources through the API server, with different controllers like ReplicaSet, DaemonSet, Job, Deployment, StatefulSet, Node, and Service controlling various aspects of cluster management, such as pod creation, scaling, and load balancing.',\n",
       " \"Services aren't linked directly to pods but contain a list of endpoints which is updated by the Endpoints controller based on pod selector and pod IPs/ports. The Namespace controller deletes all resources in a namespace when it's deleted, while the PersistentVolume controller binds PVs to PVCs matching access mode and capacity requirements.\",\n",
       " \"Kubernetes controllers operate on API objects through the API server without communicating with Kubelets. The Control Plane handles part of the system's operation, while the Kubelet and Service Proxy run on worker nodes, responsible for starting and monitoring containers, reporting status and events to the API server, and terminating containers when their Pod is deleted.\",\n",
       " 'The Kubernetes Service Proxy, also known as kube-proxy, is responsible for making sure clients can connect to services defined through the Kubernetes API. It performs load balancing across pods backing a service and ensures connections end up at one of the pods or non-pod service endpoints. The proxy runs on every worker node and uses iptables rules to intercept connections destined to service IPs, redirecting them to the proxy server.',\n",
       " 'Kubernetes kube-proxy uses iptables rules to redirect packets to a randomly selected backend pod, without passing them through an actual proxy server. This is called the iptables proxy mode and has performance benefits over user-space proxying, which also balances connections across pods in a true round-robin fashion. Add-ons like DNS lookup and web dashboard are deployed as pods using YAML manifests and can be managed with resources such as Deployments and DaemonSets.',\n",
       " \"Kubernetes cluster's DNS add-on is a Deployment that provides a DNS server for pods to look up services by name or IP addresses. The DNS server pod uses the API server's watch mechanism to update its records with Service and Endpoints changes. Ingress controllers run reverse proxy servers like Nginx, observing resources through the watch mechanism and configuring the proxy server accordingly. Unlike Services, Ingress controllers forward traffic directly to service pods, preserving client IPs when external clients connect.\",\n",
       " 'Kubernetes system is composed of small, loosely coupled components that work together to synchronize actual and desired state. The API server triggers a coordinated dance of components when submitting a pod manifest or Deployment resource, resulting in containers running. Controllers, Scheduler, Kubelet, and other components watch the API server for changes and cooperate to create and manage resources such as Pods, Deployments, and ReplicaSets.',\n",
       " 'When a Deployment manifest is submitted to Kubernetes, the API server validates it and returns a response. The Deployment controller creates a ReplicaSet, which in turn creates pods. A chain of notifications through watch mechanisms triggers this process, involving clients such as kubectl, Scheduler, and Kubelet.',\n",
       " 'The ReplicaSet controller creates Pod resources based on a pod template, which are then scheduled by the Scheduler to a specific node. The Kubelet runs the containers on the assigned node, and both the Control Plane components and Kubelet emit events to the API server as they perform these actions.',\n",
       " \"A running pod is a logical host that can contain one or more containers and has its own IP address. It's created by the Kubelet which runs the container(s) specified in the pod spec. The Kubelet creates a network namespace for each pod, allowing for isolated networking between pods.\",\n",
       " \"Kubernetes uses an additional 'pause' container to hold all containers of a pod together, sharing network and other Linux namespaces. This infrastructure container runs from pod scheduling until deletion, allowing application containers to reuse these namespaces if restarted.\",\n",
       " 'Kubernetes achieves inter-pod networking by not setting up the network itself, but rather relying on system administrators or CNI plugins to do so. The network must allow pods to communicate with each other without NAT and with the same IP addresses visible to all pods. This enables simple networking for applications running inside pods as if they were connected to the same network switch.',\n",
       " \"A Kubernetes cluster's inter-pod networking works by creating a virtual Ethernet interface pair (veth pair) for each pod, connecting it to the same bridge as other pods on the same node. The pod's containers use its network namespace and IP address, which is set up and held by the infrastructure container (pause container). This allows communication between pods on the same node without needing NAT.\",\n",
       " \"Pods in a Kubernetes cluster can communicate with each other using the container runtime's network bridge. To enable communication between pods on different nodes, bridges must use non-overlapping IP address ranges and can be connected through overlay or underlay networks, regular layer 3 routing, or by configuring node physical network interfaces and routing tables to route packets between nodes.\",\n",
       " 'Kubernetes uses a veth pair to connect containers on the same network switch without routers in between. To make it easier, a Software Defined Network (SDN) can be used, or a Container Network Interface (CNI) plugin such as Calico, Flannel, Romana, Weave Net can be installed by deploying a YAML containing a DaemonSet and supporting resources. Services are implemented to expose a set of pods at a long-lived, stable IP address and port.',\n",
       " \"Kubernetes' kube-proxy handles Services by assigning a stable IP address and port, which clients connect to. The virtual IP address is assigned immediately upon service creation, and kube-proxy sets up iptables rules on worker nodes to redirect packets destined for the service IP/port pair to one of the backing pods. Kube-proxy also watches Endpoints objects for changes in backing pods.\",\n",
       " \"A packet sent to a Kubernetes service's virtual IP is modified by the kernel on the node it's received on, according to iptables rules. If the packet matches a rule, its destination IP and port are changed to point to a randomly selected backend pod. This process is handled by kube-proxy, which watches for changes to services and endpoints.\",\n",
       " 'To achieve high availability in Kubernetes, apps can be run through a Deployment resource with an appropriate number of replicas. If a replica becomes unavailable, it will be replaced quickly, although there may be a short period of downtime. For non-horizontally scalable apps, leader-election mechanisms can be used to ensure only one instance is active at a time, avoiding downtime. Kubernetes itself requires high availability, and its Control Plane components can be made highly available using techniques such as load balancing and multiple masters.',\n",
       " 'To make Kubernetes highly available, run multiple master nodes with etcd, API server, Controller Manager, and Scheduler components. Each component can be made highly available by running multiple instances and replicating data across them, ensuring the cluster can handle failures and maintain read/write operations.',\n",
       " 'Running multiple instances of etcd, API servers, controllers, and schedulers can provide high availability in Kubernetes clusters. However, careful consideration is needed for components like the Controller Manager and Scheduler to avoid racing conditions and undesired effects. The use of leader election mechanisms can help ensure that only one instance is active at a time, providing a stable and reliable system.',\n",
       " \"Kubernetes components such as Controller Manager and Scheduler can run collocated or on separate machines. Leader election is achieved by creating a resource in the API server using an Endpoints object, which has no side effects unless a Service with the same name exists. The first instance to successfully write its name into the resource becomes the leader, and periodic updates from the leader ensure that other instances know it's still alive.\",\n",
       " 'Kubernetes components such as API server, Scheduler, controllers, Kubelet, and kube-proxy work together to bring a pod to life. Each component has a specific role: API server receives requests, Scheduler assigns resources, controllers manage pods, Kubelet runs containers on nodes, and kube-proxy performs load balancing. High availability is achieved by running multiple instances of each component.',\n",
       " 'This chapter covers authentication, ServiceAccounts, and permissions configuration in a Kubernetes cluster. It explains how the API server handles requests using authentication plugins and introduces the concept of ServiceAccounts for authenticating applications running in pods.',\n",
       " \"Kubernetes uses authentication plugins to determine who's sending a request by examining the request and returning the username, user ID, and groups to the API server core. The API server stops invoking remaining plugins and continues onto authorization. Authentication plugins can obtain client identity using methods such as client certificate, authentication token, or basic HTTP authentication. Kubernetes distinguishes between users (actual humans) and pods (applications running inside them), with users managed by external systems like SSO and pods using service accounts created in the cluster.\",\n",
       " 'ServiceAccounts are identities of apps running in pods, allowing them to authenticate with the API server using a token. The API server passes the username to authorization plugins, which determine if actions can be performed. ServiceAccounts are resources scoped to individual namespaces and can be listed like other Kubernetes resources.',\n",
       " 'Kubernetes authentication works by assigning ServiceAccounts to pods, which determine resource access. Each namespace has a default ServiceAccount, but additional ones can be created for cluster security reasons, such as running pods under constrained accounts or granting permissions to retrieve or modify resources.',\n",
       " \"A ServiceAccount is created with `kubectl create serviceaccount` and can be inspected with `kubectl describe sa`. A custom token Secret is associated with the ServiceAccount, containing a CA certificate, namespace, and token. The token is a JSON Web Token (JWT) that can be mounted inside a pod if 'mountable Secrets' are enforced.\",\n",
       " \"A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories. A pod's ServiceAccount must be set when creating the pod, and it cannot be changed later. Image pull Secrets are added automatically to all pods using a ServiceAccount, saving the need to add them individually.\",\n",
       " \"A Kubernetes Pod is created using a non-default ServiceAccount named foo, which allows it to list pods when talking to the API server. The Pod's containers can access the token from the ServiceAccount and use it to authenticate with the API server, as shown by the successful response received from listing pods.\",\n",
       " \"Kubernetes' Role-Based Access Control (RBAC) authorization plugin prevents unauthorized users from viewing or modifying the cluster state. The default ServiceAccount isn't allowed to view or modify the cluster state, unless granted additional privileges. Additional authorization plugins like Attribute-based access control (ABAC), Web- Hook, and custom implementations are also available, but RBAC is the standard.\",\n",
       " \"The Kubernetes API server's security is ensured through the RBAC authorization plugin, which uses user roles to determine permissions. Roles are associated with subjects and allow certain verbs on resources or non-resource URL paths. Managing authorization is done by creating four RBAC-specific Kubernetes resources, including RoleBindings and ClusterRoleBindings.\",\n",
       " 'RBAC (Role-Based Access Control) in Kubernetes is configured through four resources: Roles and ClusterRoles that define what can be done on resources, and RoleBindings and ClusterRoleBindings that bind roles to users or groups. Roles are namespaced while ClusterRoles are cluster-level, allowing multiple bindings within a namespace or across the cluster.',\n",
       " 'To secure the Kubernetes API server, RBAC must be enabled in the cluster by setting version 1.6 or higher and disabling legacy authorization if using GKE 1.6 or 1.7. Minikube requires enabling RBAC with --extra-config. The permissive-binding clusterrolebinding should be deleted to re-enable RBAC.',\n",
       " \"The document explains how to secure a Kubernetes cluster with role-based access control (RBAC). It demonstrates creating two pods in separate namespaces using kubectl commands and attempting to list services from within each pod using curl. The example shows that RBAC prevents the default ServiceAccount from listing services, even though it's running in the same namespace, and guides the reader on how to create a Role resource to allow the ServiceAccount to perform such actions.\",\n",
       " 'A Role resource defines what actions can be taken on which resources, allowing users to get and list Services in a specific namespace (foo) via a Role named service-reader.',\n",
       " 'To secure a Kubernetes cluster with role-based access control, create a Role (e.g. service-reader) in a namespace using kubectl create or -f service-reader.yaml. Bind the Role to a ServiceAccount in the same namespace using kubectl create rolebinding, specifying the Role and ServiceAccount names. This grants permissions for the ServiceAccount to perform actions defined by the Role.',\n",
       " 'A RoleBinding references a single Role and can bind it to multiple subjects, such as ServiceAccounts, users, or groups. In this case, the test RoleBinding binds the default ServiceAccount with the service-reader Role in the foo namespace, allowing the pod running under that account to list Services.',\n",
       " \"You can add a pod's ServiceAccount from another namespace to a RoleBinding by editing the RoleBinding and adding the subject, allowing the pod to list Services in its own and other namespaces.\",\n",
       " 'Regular Roles allow access to resources in the same namespace but not across different namespaces. ClusterRoles, on the other hand, are cluster-level resources that can grant access to non-namespaced resources, non-resource URLs, or used as a common role inside individual namespaces. A ClusterRole is created using kubectl create clusterrole with the desired verb and resource, and can be bound to a ServiceAccount in a specific namespace using ClusterRoleBinding.',\n",
       " 'To secure a Kubernetes cluster with role-based access control, create a ClusterRole that specifies API groups, resources, and verbs. Bind this ClusterRole to a ServiceAccount using a RoleBinding, then verify if the ServiceAccount can list PersistentVolumes using curl.',\n",
       " 'To secure the Kubernetes API server, you must use a ClusterRoleBinding to grant access to cluster-level (non-namespaced) resources, unlike with namespaced resources where a RoleBinding can be used. The command to create a ClusterRoleBinding is similar to that of a RoleBinding, but without specifying the namespace and replacing rolebinding with clusterrolebinding.',\n",
       " 'To secure a Kubernetes cluster with role-based access control, use a ClusterRole and a ClusterRoleBinding to grant access to cluster-level resources. Non-resource URLs must also be granted explicitly, usually done through the system:discovery ClusterRole and its binding, which allow access to URLs like /api, /apis, /healthz, etc.',\n",
       " \"The system:discovery ClusterRole allows access to non-resource URLs with only GET HTTP method, and can be bound to all users through a ClusterRoleBinding. This binding grants access to the API server's /api URL path to anyone who accesses it from within a pod.\",\n",
       " \"ClusterRoles can be used with namespaced RoleBindings to grant access to specific namespaces and their resources. The 'view' ClusterRole allows reading (get, list, watch) but not writing resources in a namespace, demonstrating how ClusterRoles can control access to resources within a specific scope.\",\n",
       " \"The Kubernetes API server's permissions are determined by a ClusterRoleBinding or RoleBinding. A ClusterRoleBinding allows subjects to view resources across all namespaces, while a RoleBinding only allows viewing resources in the namespace of the binding. The example demonstrates listing pods using curl commands before and after creating a ClusterRoleBinding, showing that it applies across all namespaces.\",\n",
       " \"A pod can access namespaced resources in any namespace by combining a ClusterRoleBinding with a ClusterRole referring to namespaced resources. Replacing the ClusterRoleBinding with a RoleBinding limits the pod's access to only the specified namespace, as demonstrated with the creation of a RoleBinding in the foo namespace.\",\n",
       " 'Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles. The document explains various combinations of these concepts for specific use cases, such as accessing cluster-level resources, non-resource URLs, namespaced resources in any or specific namespaces. It also highlights how a ServiceAccount can only view pods within its own namespace, despite using a ClusterRole.',\n",
       " 'Kubernetes has a default set of ClusterRoles and ClusterRoleBindings that are updated every time the API server starts, allowing for automatic recreation if deleted or changed in newer versions. The default roles include cluster-admin, system:basic-user, and various controller roles.',\n",
       " \"The most important ClusterRoles in Kubernetes are view, edit, admin, and cluster-admin, designed to prevent privilege escalation. The view role allows read-only access, while the edit role grants modifying resources within a namespace. The admin role provides complete control of a namespace's resources, but not ResourceQuotas or Namespace itself. The cluster-admin role offers complete control over the entire Kubernetes cluster.\",\n",
       " \"The Controller Manager runs as a single pod, but each controller can use a separate ClusterRole and ClusterRoleBinding. By default, ServiceAccounts have no permissions, so pods can't view cluster state. It's best to grant only the necessary permissions (principle of least privilege). Create specific ServiceAccounts for each pod and associate them with tailor-made Roles through RoleBindings. Constrain ServiceAccounts to prevent damage if compromised.\",\n",
       " 'Kubernetes API server security is discussed. ServiceAccounts are used to run pods, with default accounts created for each namespace. Additional accounts can be created manually and configured to allow mounting specific Secrets. Roles and ClusterRoles define allowed actions on resources, and RoleBindings bind these to users, groups, and ServiceAccounts. This sets the stage for securing cluster nodes and isolating pods in the next chapter.',\n",
       " 'This chapter focuses on securing cluster nodes and the network, allowing pods to access node resources while limiting user actions. Key topics include using default Linux namespaces in pods, running containers as different users, privileged containers, modifying kernel capabilities, defining security policies, and securing the pod network.',\n",
       " \"In Kubernetes, containers in a pod run under separate namespaces, isolating their processes from other containers or the node's default namespace. Certain system pods can use the node's network namespace by setting hostNetwork to true in the pod spec, allowing them to see and manipulate node-level resources and devices.\",\n",
       " \"A Kubernetes pod can use the host's network namespace by setting `hostNetwork: true` in its spec. This allows it to see all the host's network adapters and bind to a port in the node's default namespace using `hostPort`. Note that this is different from a NodePort service, which binds the port on all nodes even if no pod is running on them.\",\n",
       " \"When using a specific host port in a pod, only one instance of the pod can be scheduled to each node due to multiple processes cannot bind to the same host port. The Scheduler takes this into account and doesn't schedule multiple pods to the same node, allowing only three pods to be scheduled out of four replicas when three nodes are available.\",\n",
       " \"Using the host node's namespaces in a pod, you can define hostPort in a pod's YAML definition. This allows access to the pod through the node's port, but not on other nodes. The hostPID and hostIPC pod spec properties allow containers to use the node's PID and IPC namespaces, respectively.\",\n",
       " 'This chapter discusses securing cluster nodes and networks by configuring the security context of pods and containers. This includes setting hostIPC to true for processes to communicate, configuring container security through user ID, preventing root access, running in privileged mode, adding or dropping capabilities, setting SELinux options, and preventing process writing to the filesystem.',\n",
       " \"To run a pod under a different user ID than the one baked into the container image, set the pod's securityContext.runAsUser property. This was shown by running a pod as user 'guest' with UID 405 and verifying the result using the id command inside the container.\",\n",
       " \"The chapter discusses securing cluster nodes and the network by preventing containers from running as root. It explains how to prevent an attacker from pushing a malicious image under the same tag as a trusted image, and how to specify that a pod's container needs to run as a non-root user using `runAsNonRoot: true`. It also touches on running pods in privileged mode for specific use cases.\",\n",
       " \"A Kubernetes pod's container can run in privileged mode by setting the `privileged` property to true in its security context, allowing access to the node's kernel and device files. This is demonstrated by comparing the devices visible in a non-privileged container with those in a privileged container.\",\n",
       " \"In Kubernetes, instead of making a container privileged and giving it unlimited permissions, you can give it access only to the kernel features it really requires by adding individual kernel capabilities. This allows fine-tuning of the container's permissions and limiting the impact of a potential intrusion. For example, you can add CAP_SYS_TIME capability to allow the container to change the system time.\",\n",
       " \"Configuring a container's security context by adding or dropping Linux kernel capabilities, such as SYS_TIME. This can be done under the securityContext property in a pod spec, and is a more controlled way than giving full privileges with privileged: true. Capabilities can be added or dropped to allow specific actions, but requires knowledge of what each capability does.\",\n",
       " 'To prevent containers from modifying the owner of files or writing to their own filesystem, Kubernetes capabilities can be dropped and the readonlyRootFilesystem property set to true. This prevents malicious code injection in case of vulnerabilities.',\n",
       " \"When configuring a pod's security context, setting the container's readOnlyRootFilesystem property to true makes the filesystem read-only, preventing write access to the / directory. However, writing to a mounted volume is allowed. To increase security in production environments, set this property to true at the pod level or override it at the container level.\",\n",
       " \"Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs. The fsGroup and supplementalGroups properties are used in a pod's security context to achieve this. An example is provided where two containers with different user IDs share the same volume, and the container running as user ID 1111 can read or write files of the container running as user ID 2222 due to the shared group permissions.\",\n",
       " 'The document explains how to restrict the use of security-related features in pods. It discusses the fsGroup and supplementalGroups properties, which are used to set group IDs for users running containers. The fsGroup property sets the ownership of a mounted volume, while the supplementalGroups property defines additional group IDs associated with a user. A cluster administrator can restrict the use of these features by creating PodSecurityPolicy resources, which define what security-related features users can or cannot use in their pods.',\n",
       " 'A PodSecurityPolicy admission control plugin validates pod definitions against configured policies before storing them in etcd. A PodSecurityPolicy resource defines settings such as IPC and network namespace usage, host ports, user IDs, and privileged container creation. To enable RBAC and PodSecurityPolicy admission control in Minikube, use the command: $ minikube start --extra-config apiserver.Authentication.PasswordFile=/etc/kubernetes/passwd --extra-config=apiserver.Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRunOptions.AdmissionControl=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,PodSecurityPolicy. Create a password file with the command: $ cat <<EOF | minikube ssh sudo tee /etc/kubernetes/passwd password,alice,1000,basic-user password,bob,2000,privileged-user EOF',\n",
       " \"This document discusses restricting security-related features in pods, specifically kernel capabilities, SELinux labels, writable root filesystems, and volume types. A sample PodSecurityPolicy is provided that prevents pods from using the host's IPC, PID, and Network namespaces, and restricts privileged containers and host ports. The policy allows containers to run as any user or group, and use any SELinux groups.\",\n",
       " 'A cluster node and network security chapter that explains the restriction of deploying privileged pods due to a pod security policy. It also discusses how to constrain container user IDs using the MustRunAs rule, with examples showing how to specify allowed ID ranges for runAsUser, fsGroup, and supplementalGroups fields.',\n",
       " \"A PodSecurityPolicy can restrict the use of security-related features in pods, enforcing only when creating or updating pods. If a pod spec tries to set fields outside allowed ranges, it's rejected by the API server. However, if a container image has an out-of-range user ID, but the runAsUser property is not set, the API server may still accept the pod and run the container with the specified ID in the PodSecurityPolicy.\",\n",
       " 'Securing cluster nodes and network involves using the MustRunAsNonRoot rule in the runAsUser field, preventing users from deploying containers that run as root. Configuring allowed, default, and disallowed capabilities includes specifying which capabilities can be added or dropped in a container using fields like allowedCapabilities, defaultAddCapabilities, and requiredDropCapabilities. This helps control what operations containers can perform by adding or dropping Linux kernel capabilities.',\n",
       " 'PodSecurityPolicy resources allow restricting security-related features in pods, adding capabilities to containers, and constraining volume types. Capabilities can be added or dropped from containers using defaultAddCapabilities and requiredDropCapabilities fields respectively. Volume types can also be restricted, with a minimum of emptyDir, configMap, secret, downwardAPI, and persistentVolumeClaim allowed.',\n",
       " \"PodSecurityPolicies (PSPs) are cluster-level resources that can't be stored in a specific namespace. Different PSBs can be assigned to different users and groups using the RBAC mechanism by creating ClusterRole resources, pointing them to individual policies, and binding them to users or groups with ClusterRoleBindings. A new PSP is created to allow privileged containers to be deployed, allowing for more flexibility in managing system pods and user pods.\",\n",
       " 'To restrict security-related features in pods, you can use PodSecurityPolicies. You can create two ClusterRoles (psp-default and psp-privileged) to allow different users to use specific policies. Bind these roles to users using a ClusterRoleBinding, referencing the policies by name (e.g., default or privileged). This way, Alice can only deploy non-privileged pods while Bob can deploy both. Authenticated users will have access to the default policy.',\n",
       " \"You'll bind the psp-privileged ClusterRole only to Bob by creating a clusterrolebinding. Alice will have access to the default PodSecurity-Policy, while Bob has access to both. To authenticate as Alice or Bob, create new users in kubectl's config with set-credentials commands. You can then use the --user option to create pods with different user credentials, demonstrating that Bob can create privileged pods while Alice cannot.\",\n",
       " 'Isolating the pod network by configuring NetworkPolicy resources, which apply to pods matching a label selector and specify sources or destinations that can access the matched pods. This is configurable if the container networking plugin supports it. A default-deny NetworkPolicy prevents all clients from connecting to any pod in a namespace, and can be enabled by creating a NetworkPolicy with an empty pod selector.',\n",
       " 'To secure cluster nodes and network, a CNI plugin or networking solution must support NetworkPolicy. A NetworkPolicy resource can be created in the same namespace as a database pod to allow only specific pods (e.g. webserver) to connect on port 5432, while blocking other pods from connecting to the database.',\n",
       " \"To secure a microservice for a specific tenant, create a NetworkPolicy that allows only pods from the same tenant's namespaces to access the microservice on a specific port. The policy applies to pods labeled with 'microservice=shopping-cart' and allows access from namespaces labeled as 'tenant=manning'. This example demonstrates how to isolate network traffic between Kubernetes namespaces for multiple tenants using the same cluster.\",\n",
       " \"In a multi-tenant Kubernetes cluster, tenants can't add labels to their namespaces themselves. NetworkPolicies ensure only pods running in specific namespaces or IP blocks can access targeted pods. An example ingress rule allows traffic from the 192.168.1.0/24 IP block to access shopping-cart pods.\",\n",
       " \"In this chapter, Network Policies are used to limit a pod's inbound and outbound traffic. Egress rules allow limiting outbound traffic of a set of pods by specifying which pods they can connect to. PodSecurityPolicy resources can be created to prevent users from creating pods that could compromise a node. Cluster-level policies can be associated with specific users using RBAC's ClusterRoles and ClusterRoleBindings.\",\n",
       " 'This chapter covers requesting CPU and memory computational resources for containers, setting hard limits, understanding Quality of Service guarantees for pods, and limiting total resources in a namespace.',\n",
       " \"When creating a pod, you can specify resource requests and limits for each container individually. The pod's total resources are the sum of all containers' requests and limits. Requests define the minimum amount of CPU and memory a container needs, while limits set a hard limit on what it may consume.\",\n",
       " \"The Kubernetes Scheduler determines if a pod can fit on a node based on the sum of resources requested by existing pods, not actual resource consumption. A pod's resource requests specify its minimum requirements, and scheduling is denied if unallocated resources are insufficient to meet these requirements. Requests don't limit CPU usage, but specifying a CPU limit does.\",\n",
       " 'The Kubernetes Scheduler prioritizes nodes based on requested resources, using functions like LeastRequestedPriority and MostRequestedPriority to select the best node for a pod. The MostRequestedPriority function is useful in cloud infrastructure where adding or removing nodes is possible, as it allows for tight packing of pods and potential removal of unused nodes, saving costs.',\n",
       " \"This chapter discusses managing pods' computational resources, specifically the Node resource. It explains that a node's capacity represents its total resources, but not all are available to pods due to reserved resources for Kubernetes and system components. The Scheduler bases its decisions on allocatable resource amounts. A pod with CPU requests of 800 millicores was successfully scheduled, but attempting to deploy another pod with 1 core of CPU request did not fit on any node.\",\n",
       " \"A Kubernetes pod's container request for 1 whole core CPU instead of millicores causes scheduling failure due to insufficient CPU on a single node. The issue is resolved by inspecting the node resource with `kubectl describe node` and examining the output, which shows that the node has allocated resources that are not associated with the pod, resulting in failed scheduling.\",\n",
       " 'The Kubernetes Scheduler allocates computational resources based on pod requests and limits. A total of 1,275 millicores have been requested by running pods, exceeding initial requests. The culprit behind additional CPU resources usage is identified in the kube-system namespace. To free up resources for a third pod to be scheduled, one of the first two pods can be deleted. This triggers the Scheduler to schedule the third pod as soon as the deleted pod terminates. Both CPU and memory requests are treated equally by the Scheduler.',\n",
       " 'Kubernetes distributes unused CPU time among pods in a ratio based on their CPU requests, allowing one pod to consume all available CPU if the other is idle. Custom resources can also be added and requested by pods, initially as Opaque Integer Resources, then replaced with Extended Resources in Kubernetes 1.8.',\n",
       " \"To manage pods' computational resources in Kubernetes, a custom resource can be added to the Node object's capacity field. This involves performing a PATCH HTTP request and specifying the resource name and quantity. When creating pods, the same resource name and requested quantity must be specified under the resources.requests field. Resource limits for containers can also be set to prevent them from using up excessive CPU or memory. Limits can be set on both CPU and memory usage, preventing malfunctioning or malicious pods from affecting other nodes.\",\n",
       " \"A Kubernetes pod's container has configured resource limits for CPU and memory, limiting its consumption to 1 CPU core and 20Mi of memory. Resource limits are not constrained by the node's allocatable resources and can be overcommitted, potentially leading to containers being killed if resources are fully utilized.\",\n",
       " \"When a process running in a container tries to use more resources than allowed, the process is throttled for CPU usage. However, for memory, if the process allocates more memory than its limit, it's killed (OOMKilled) and restarted by Kubernetes with increasing delays between restarts, eventually resulting in a CrashLoopBackOff status.\",\n",
       " \"Containers may get OOMKilled even if they aren't over their memory limit due to the way apps in containers see limits. The top command shows memory amounts of the whole node, not the container's memory limit, which can be misleading. Containers always see the node's memory, not their own, and this can lead to unexpected behavior.\",\n",
       " \"When running containers in a Kubernetes cluster, it's essential to manage pods' computational resources properly. Containers can see all node CPUs and may exceed memory limits if not configured correctly. The JVM may be OOMKilled when the heap size exceeds container memory limits. Setting -Xmx options doesn't solve issues with off-heap memory. New Java versions consider container limits, but certain applications that rely on CPU count for worker threads may spin up too many threads and exceed resources.\",\n",
       " \"Kubernetes categorizes pods into three Quality of Service (QoS) classes: BestEffort (lowest priority), Burstable, and Guaranteed (highest). The QoS class is derived from a pod's resource requests and limits. A Guaranteed class is assigned to pods with equal request and limit settings for CPU and memory, while a BestEffort class is given to pods with no requests or limits set. This allows Kubernetes to make decisions on which containers to kill in case of resource shortages.\",\n",
       " \"A pod's Quality of Service (QoS) class is determined by the relationship between its resource requests and limits. The three QoS classes are BestEffort, Burstable, and Guaranteed. A pod with a best effort QoS can consume any available resources. A burstable pod gets the requested amount of resources, but can use additional ones up to their limit if needed. A guaranteed pod gets the exact amount of resources it requests.\",\n",
       " \"A pod's QoS (Quality of Service) class is determined by the classes of its containers, and can be BestEffort, Burstable, or Guaranteed. For single-container pods, requests and limits are used to determine the class, while for multi-container pods, the highest container class determines the pod's class. If all containers have the same QoS class, it's also the pod's class, but if at least one container has a different class, the pod's class is Burstable.\",\n",
       " 'When system memory is overcommitted, QoS classes determine which container gets killed first. BestEffort class gets killed first, followed by Burstable, and finally Guaranteed. If containers have the same QoS class, the process with the highest OutOfMemory (OOM) score gets killed, calculated from available memory consumption and fixed OOM score adjustment based on QoS class and requested memory.',\n",
       " \"To avoid containers being at the mercy of others that specify resource requests and limits, it's recommended to set these values on every container or use a LimitRange resource per namespace to specify minimum/max limit values and default resource requests.\",\n",
       " 'A LimitRange resource is used by the LimitRanger Admission Control plugin to validate pod specs. It prevents users from creating pods bigger than any node in the cluster, specifying limits for individual containers or objects in the same namespace, but not total resources across all pods.',\n",
       " \"A LimitRange object is used to set default requests and limits for pods per namespace, applying to containers' requests and limits. It allows setting min/max values, default resource requests/limits, and max ratio of limits vs requests. This validation is performed by the API server when receiving a new pod manifest, and does not affect existing pods or PVCs created before modifying the limits.\",\n",
       " 'A Kubernetes LimitRange object is used to set maximum CPU and memory usage limits for pods and containers. When creating a pod with a container requesting more than the allowed limit, the pod is rejected due to the Forbidden error message from the server listing all reasons why the pod was rejected. Default resource requests and limits can be applied automatically when creating a pod by setting them in a LimitRange object, allowing admins to configure default, min, and max resources for pods per namespace.',\n",
       " 'Resource quotas limit the total amount of resources available in a namespace, including computational resources, storage, number of pods, claims, and API objects. They are enforced at pod creation time and do not affect existing pods. A ResourceQuota object can be created to specify quotas for CPU and memory, as shown in Listing 14.13.',\n",
       " \"A ResourceQuota object sets separate totals for requests and limits of CPU and memory resources in a namespace. It can be inspected using kubectl describe quota, showing used amounts for each resource. The quota applies to all pods' resource requests and limits in total, unlike LimitRange which applies to individual pods or containers separately.\",\n",
       " 'A ResourceQuota object can limit resources available in a namespace, including CPU, memory, persistent storage, and the number of objects that can be created. A LimitRange object is required alongside ResourceQuota to specify resource requests or limits for pods.',\n",
       " 'A ResourceQuota in Kubernetes limits the number of objects that can be created in a namespace, such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services. The quota specifies hard limits for each object type, e.g., 10 pods, 5 replication controllers, 10 secrets, etc. It also allows limiting specific types of services like load balancers and node ports.',\n",
       " 'You can limit resources in a namespace by specifying quotas for specific pod states and/or QoS classes, using quota scopes like BestEffort, NotBestEffort, Terminating, and NotTerminating. These scopes apply to pods with certain QoS classes or active deadline seconds, and can be used to limit the number of pods, CPU/memory requests, and limits. For example, a ResourceQuota for BestEffort/NotTerminating pods can ensure that at most four such pods exist.',\n",
       " 'Properly setting resource requests and limits in Kubernetes is crucial for efficient cluster usage. Monitoring actual resource usage under expected load levels helps find the optimal spot. The Kubelet contains an agent called cAdvisor that collects basic resource consumption data, while Heapster collects and exposes metrics from all nodes in a single location.',\n",
       " \"Heapster is a component that collects container and node usage data without needing to connect to the processes running inside pods' containers. It can be enabled in Minikube using `minikube addons enable heapster`. Once enabled, you can use `kubectl top` commands to see actual CPU and memory usage for cluster nodes and individual pods.\",\n",
       " 'The kubectl top command shows current pod metrics, but may not show metrics immediately due to Heapster aggregation. Historical resource consumption can be analyzed using tools like InfluxDB and Grafana for storing and visualizing data.',\n",
       " \"InfluxDB and Grafana can be run as pods in a Kubernetes cluster, providing monitoring capabilities for resource usage. Deploying them is straightforward using manifests available in the Heapster Git repository or enabled with Minikube's Heapster add-on. To analyze pod resource usage, open the Grafana web console to explore predefined dashboards and discover CPU usage across the cluster.\",\n",
       " \"When using Minikube, Grafana's web console can be opened in a browser to view resource usage statistics for nodes and pods. The Cluster dashboard shows overall cluster usage, while the Pods dashboard displays individual pod resource usages. By examining these charts, users can determine if resource requests or limits need to be adjusted to accommodate more pods on a node.\",\n",
       " \"This chapter emphasizes considering a pod's resource usage, configuring requests and limits to keep everything running smoothly. Key takeaways include specifying resource requests to schedule pods across the cluster, setting resource limits to prevent pods from starving other resources, and managing CPU and memory usage to optimize application performance.\",\n",
       " \"To manage pods' computational resources, use LimitRange objects for individual resource requests and limits or ResourceQuota objects to limit namespace-wide resources. Monitor pod usage over time to determine optimal resource settings, with Kubernetes using these metrics for automatic scaling in the next chapter.\",\n",
       " 'Applications can be scaled out manually or automatically by increasing replicas or resource requests. However, manual scaling is not ideal for sudden traffic increases. This chapter covers configuring automatic horizontal scaling of pods and cluster nodes based on CPU utilization and custom metrics, as well as understanding vertical scaling limitations.',\n",
       " 'Kubernetes can automatically scale pods and cluster nodes based on CPU usage or other metrics, spinning up additional nodes if necessary. The autoscaling feature was rewritten between Kubernetes 1.6 and 1.7, so outdated information may exist online. Horizontal pod autoscaling adjusts the number of replicas by periodically checking pod metrics, calculating the required number of replicas, and updating the replicas field on the target resource.',\n",
       " 'Horizontal pod autoscaling should already be enabled in your cluster, and once enabled, the Autoscaler can use metrics from Heapster or the aggregated resource metrics API to calculate the required number of pods. The calculation is based on a set of pod metrics and the target value, taking into account factors such as metric instability and multiple metrics per pod.',\n",
       " 'The final step of autoscaling is updating the desired replica count field on the scaled resource object and letting the Replica-Set controller manage additional pods or excess ones. The Autoscaler controller modifies the replicas field through the Scale sub-resource, allowing it to operate on scalable resources like Deployments, ReplicaSets, ReplicationControllers, and StatefulSets.',\n",
       " \"The horizontal pod autoscaler obtains metrics from cAdvisor, Heapster, and Kubelet to adjust replicas based on CPU utilization, taking into account a delay in propagating metrics data and performing scaling actions. It's essential to consider this delay when observing the Autoscaler in action.\",\n",
       " 'The chapter discusses automatic scaling of pods and cluster nodes by focusing on scaling out (increasing the number of pods). The average CPU usage should come down, but setting a target CPU usage well below 100% is recommended to leave room for sudden load spikes. A HorizontalPodAutoscaler can be created to scale pods based on their CPU utilization, requiring CPU resource requests to be set in the pod template.',\n",
       " 'To enable horizontal autoscaling for a Deployment, create a HorizontalPodAutoscaler (HPA) object and point it to the Deployment. The HPA will adjust the number of replicas to keep CPU utilization around 30% while ensuring a minimum of one replica and a maximum of five. This can be done using the kubectl autoscale command or by preparing and posting a YAML manifest for the HPA. Always make sure to autoscale Deployments instead of ReplicaSets to preserve the desired replica count across application updates.',\n",
       " 'Automatic scaling of pods and cluster nodes is achieved using Horizontal Pod Autoscalers (HPA). The HPA adjusts the desired replica count on the Deployment based on CPU metrics. In a scenario where three pods have zero CPU usage, the autoscaler scales them down to a single pod, ensuring CPU utilization remains below the 30% target.',\n",
       " 'The horizontal pod autoscaler successfully rescaled to one replica because all metrics were below target. To trigger a scale-up, expose the pods through a Service and start sending requests to your pod, thereby increasing its CPU usage. You can watch the HorizontalPodAutoscaler and Deployment with kubectl get --watch.',\n",
       " 'This chapter covers automatic scaling of pods and cluster nodes in Kubernetes. It explains how to run a pod that repeatedly hits the kubia Service using the kubectl run command with options such as -it, --rm, and --restart=Never. The autoscaler increases the number of replicas based on CPU utilization, and events can be inspected with kubectl describe. The chapter also discusses how the autoscaler concludes the need for multiple replicas based on target CPU utilization percentages.',\n",
       " 'The autoscaler in Kubernetes has a maximum rate of scaling, doubling the number of replicas in a single operation if more than two exist, or scaling up to four replicas. It also has a limit on how soon a subsequent scale-up operation can occur after the previous one, which is currently every three minutes for scale-up and five minutes for scale-down. The target metric value for CPU utilization can be modified by editing the HPA resource with kubectl edit command, increasing it from 30 to 60 in this case.',\n",
       " 'Memory-based autoscaling in Kubernetes is more problematic than CPU-based due to the need to force old pods to release memory. This requires the app itself to manage, and can lead to infinite scaling if not implemented correctly. Custom metrics can also be used for autoscaling, but this was complicated in earlier versions of Kubernetes. Newer versions have simplified this process, allowing for more flexible scaling options.',\n",
       " \"Kubernetes Horizontal Pod Autoscaling (HPA) allows defining multiple metrics for autoscaling decisions. There are three types of metrics: Resource, Pods, and Object. The Resource type uses a resource metric like CPU or memory requests. The Pods type refers to custom metrics related to the pod, such as Queries-Per-Second (QPS). The Object metric type scales pods based on a metric not directly pertaining to those pods, like an Ingress object's QPS.\",\n",
       " 'Horizontal pod autoscalers (HPAs) use metrics to monitor resources and scale pods accordingly. Appropriate metrics include those with a linear decrease in average value as replicas increase, such as Queries per Second (QPS). However, not all metrics are suitable, like memory consumption, which can lead to non-linear behavior. HPAs currently do not allow scaling down to zero replicas.',\n",
       " \"Kubernetes does not currently support idling and un-idling of pods or vertical pod autoscaling, but an experimental feature called InitialResources sets CPU and memory requests for newly created pods based on historical resource usage data, and a new proposal is being finalized to modify existing pod's resource requests vertically.\",\n",
       " 'The Cluster Autoscaler in Kubernetes automatically requests additional nodes from the cloud provider when a new pod cannot be scheduled due to lack of resources on existing nodes. It also de-provisions underutilized nodes for longer periods. The Autoscaler examines available node groups, selects the best one that can fit the unscheduled pod, and increases its size or adds another node to it.',\n",
       " \"When scaling a Kubernetes cluster, the Cluster Autoscaler monitors node utilization and CPU/Memory requests of running pods. If a node is underutilized (CPU/memory < 50%), it's considered unnecessary unless system or unmanaged pods are running on it. The Autoscaler marks the node as unschedulable and evicts its pods before shutting it down. Scaling up involves identifying an available node type, selecting one, and scaling that group to fit the pod.\",\n",
       " 'Automatic scaling of pods and cluster nodes can be enabled on GKE, GCE, AWS, and Azure through Cluster Autoscaler. On GKE, use gcloud command with --enable-autoscaling flag. On GCE, set environment variables KUBE_ENABLE_CLUSTER_AUTOSCALER=true, KUBE_AUTOSCALER_MIN_NODES=3, and KUBE_AUTOSCALER_MAX_NODES=5 before running kube-up.sh. The Cluster Autoscaler publishes its status to the cluster-autoscaler-status ConfigMap in the kube-system namespace. To limit service disruption during scale-down, manually cordon and drain nodes using kubectl commands.',\n",
       " 'Kubernetes provides a way to specify the minimum number of pods that must always be available through the PodDisruptionBudget resource, especially for quorum-based clustered applications. The PDB resource contains a pod label selector and a number specifying the minimum or maximum number of pods that can be unavailable. It can also use percentages instead of absolute numbers. The Cluster Autoscaler and kubectl drain command will adhere to this resource, ensuring that evictions do not bring the number of such pods below the specified threshold.',\n",
       " 'Kubernetes can scale not only pods but also cluster nodes automatically. HorizontalPodAutoscaler configures scaling based on CPU utilization or custom metrics. Vertical pod autoscaling is not possible yet. Cluster node auto-scaling is supported on cloud providers. Additionally, pods can be run one-off and deleted with kubectl run options.',\n",
       " 'Kubernetes allows for advanced scheduling by specifying a node selector in the pod specification, or using taints and tolerations to keep pods away from certain nodes. Additional features include defining node affinity rules, co-locating pods, and keeping pods away from each other using pod anti-affinity.',\n",
       " \"Node taints allow rejecting deployment of pods to certain nodes by adding taints without modifying existing pods, while tolerations enable pods to opt-in and use tainted nodes. A node's taints can be displayed using kubectl describe node, showing a key-value pair with an effect, such as NoSchedule preventing pod scheduling unless tolerated.\",\n",
       " \"In Kubernetes, a pod can be scheduled to a tainted node by adding a toleration that matches the node's taint. This allows system pods like kube-proxy to run on master nodes. A pod with no tolerations can only be scheduled to nodes without taints, and tolerations define how long a pod is allowed to run on nodes that aren't ready or are unreachable.\",\n",
       " \"Kubernetes taints and tolerations allow you to label nodes with effects that can be tolerated by pods. Taints have three possible effects: NoSchedule, PreferNoSchedule, and NoExecute. Adding a NoExecute taint to a node will evict running pods that don't tolerate it. To deploy production pods to tainted nodes, they must include tolerations in their manifests matching the key, value, and effect of the taint.\",\n",
       " 'Taints and tolerations can be used to control pod scheduling in Kubernetes. A taint is added to a node, and a matching toleration is added to a pod to allow it to run on that node. Tolerations can tolerate specific values or any value for a specific taint key. Taints can prevent new pods from running (NoSchedule), define unpreferred nodes (PreferNoSchedule), or evict existing pods (NoExecute). This allows for partitioning a cluster into separate partitions, controlling pod scheduling based on node type.',\n",
       " \"Kubernetes can wait up to 300 seconds (5 minutes) after a node failure before rescheduling a pod. This delay can be adjusted by adding tolerations to the pod's spec, and is currently an alpha feature. Node affinity allows scheduling pods only to specific subsets of nodes, replacing the initial node-selector mechanism which was simpler but didn't offer everything needed.\",\n",
       " 'Node affinity allows specifying hard requirements or preferences for pods to run on certain nodes, based on their labels. Kubernetes uses these labels to select nodes, and by understanding default node labels, you can create rules that attract pods to specific nodes.',\n",
       " 'The document discusses advanced scheduling in Kubernetes, specifically the use of node selectors and affinity rules to deploy pods on nodes with specific labels. The nodeSelector field specifies a simple rule for deployment, while the nodeAffinity field provides more expressive and detailed rules, including requiredDuringSchedulingIgnoredDuringExecution, which ensures the pod is scheduled only on nodes meeting specified criteria during scheduling but ignores execution.',\n",
       " \"Node affinity in Kubernetes allows pods to be scheduled to nodes with specific labels, such as gpu=true. The nodeSelectorTerms field defines expressions that a node's labels must match for the pod to be scheduled. Node affinity also enables prioritizing nodes during scheduling through the preferredDuringSchedulingIgnoredDuringExecution field, allowing for preference of certain zones or machines over others.\",\n",
       " 'Node affinity allows scheduling of pods to machines reserved for deployments, and can be specified by labeling nodes with availability zone and share type labels. This can be demonstrated using kubectl label command to label nodes as dedicated or shared within specific zones. A Deployment can then be created that prefers dedicated nodes in a particular zone.',\n",
       " \"You're defining a node affinity preference for pods to be scheduled on nodes with specific labels (availability-zone=zone1 and share-type=dedicated) with the first preference having a weight of 80 and the second one having a weight of 20, indicating that zone preference is more important than dedicated node preference in case of scheduling conflicts.\",\n",
       " 'In a two-node Kubernetes cluster, deploying a Deployment shows most pods deployed to one node due to prioritization functions like Selector-SpreadPriority. Scaling the Deployment up spreads pods evenly between nodes without node affinity preferences. Pod affinity allows specifying the affinity between pods themselves, such as keeping frontend and backend pods close together by configuring them to deploy on the same node.',\n",
       " 'A Deployment is created with a podAffinity rule that requires frontend pods to be deployed on the same node as backend pods, which have an app=backend label. This ensures that all frontend pods will be scheduled only to the node where the backend pod was scheduled to, creating a co-locating requirement for the two types of pods.',\n",
       " \"This chapter discusses advanced scheduling in Kubernetes using pod affinity rules. The Scheduler first finds all pods that match a labelSelector defined in a pod's configuration and schedules it to the same node. If a pod with affinity rules is deleted, the Scheduler will reschedule it to the same node to maintain consistency, even if no rules are defined on the deleted pod.\",\n",
       " 'Pods can be co-located using pod affinity and anti-affinity, prioritizing scheduling to a node based on shared labels or topology keys such as zone or region. For example, setting topologyKey to failure-domain.beta.kubernetes.io/zone allows pods to be deployed in the same availability zone, while setting it to failure-domain.beta.kubernetes.io/region schedules them in the same geographical region.',\n",
       " 'In Kubernetes, scheduling preferences can be expressed using label selectors and node or pod affinities. The Scheduler matches pods based on these rules, but if a match is not found, it will schedule the pod elsewhere. Pod affinity can also specify preferred nodes while allowing for flexibility if those nodes are unavailable.',\n",
       " 'Co-locating pods with pod affinity and anti-affinity can be achieved by defining a weight for each rule, specifying the topologyKey and labelSelector. This allows the Scheduler to prefer nodes where pods with a certain label are running. For example, in a deployment of 5 replicas, the Scheduler may deploy four pods on the same node as the backend pod, and one pod on another node.',\n",
       " \"This chapter discusses advanced scheduling in Kubernetes, specifically how to schedule pods away from each other using pod anti-affinity. This is useful when two sets of pods interfere with each other's performance if they run on the same node. Pod anti-affinity can be used to spread pods across different availability zones or regions to prevent a whole zone failure from bringing down the service completely.\",\n",
       " \"To force frontend pods to be scheduled on different nodes, you can use pod anti-affinity. This is achieved by configuring the podAntiAffinity property in the deployment's spec section and making it match the same pods that the deployment creates. A soft requirement can also be used with preferredDuringSchedulingIgnoredDuringExecution property if scheduling two frontend pods on the same node is not a problem, otherwise requiredDuringSchedulingIgnoredDuringExecution should be used for hard requirements.\",\n",
       " \"This chapter explores advanced scheduling techniques in Kubernetes, including pod affinity, topologyKey, taints, node affinity, pod affinity/anti-affinity, and their use cases. It discusses how to ensure pods aren't scheduled to certain nodes or are only scheduled to specific nodes based on node labels or pod requirements.\",\n",
       " 'This chapter covers best practices for developing apps on Kubernetes, including understanding typical application resources, adding lifecycle hooks, properly terminating apps without breaking client requests, making apps easy to manage, using init containers, and developing locally with Minikube.',\n",
       " 'A typical application manifest contains one or more Deployment and/or StatefulSet objects, including a pod template with containers, liveliness probes, readiness probes, and Services for exposing pods to others. The pod templates reference Secrets for pulling container images and those used directly by the process running inside the pods. Other resources like ReplicaSets, Endpoints, Horizontal Pod Autoscalers, and Ingress are also defined in the app manifest.',\n",
       " \"A pod's lifecycle is crucial to understand, as it can be killed and relocated by Kubernetes at any time due to scale-down requests or node relocations. This differs from traditional VMs where apps are rarely moved, giving operators more control over the app in its new location. Kubernetes controllers automatically create objects such as Endpoints, ReplicaSets, and pods, which are often labeled and annotated for organization and metadata purposes.\",\n",
       " 'Kubernetes application developers should ensure their apps can be moved and restarted without issues, considering IP and hostname changes. Stateful apps should use StatefulSets for persistence. Apps writing data to disk may lose it when restarted or rescheduled unless using persistent storage. Volumes can preserve data across container restarts, making them useful for caching results or other sensitive data.',\n",
       " \"A pod's lifecycle involves a container and its process, which writes to a filesystem with a writable layer on top of read-only layers and image layers. When the container crashes or is killed, a new container starts with a new writable layer, losing all previous files. Using a volume mounts allows data persistence across container restarts, enabling a new process to use preserved data in the volume.\",\n",
       " \"Using volumes to preserve files across container restarts can be a double-edged sword, as it may lead to continuous crash loops if data gets corrupted. Similarly, dead or partially dead pods are not automatically removed and rescheduled by ReplicaSet controllers, even if they're part of a desired replica count, resulting in a lower actual replica count.\",\n",
       " \"A Kubernetes ReplicaSet with pods that keep crashing due to a container issue is created. The pod's status shows the Kubelet delaying the restart, but no action is taken by the controller since the current replicas match the desired ones, showing three running replicas.\",\n",
       " 'Kubernetes pods can include init containers to initialize the pod and delay the start of main containers. Init containers are executed sequentially and only after completion do main containers start. An example shows an init container checking if a service is responding before allowing the main container to start, using a busybox image and a while loop to continuously check until the service is up.',\n",
       " \"When deploying a pod, its init container is started first, shown by kubectl get. The main container won't run until dependencies are met, such as services being ready. It's best to write apps that handle internal dependencies and use readiness probes to signal unavailability. Lifecycle hooks can also be defined per container for post-start and pre-stop execution, similar to liveness and readiness probes.\",\n",
       " \"A post-start hook in Kubernetes is executed immediately after a container's main process is started. It runs in parallel with the main process and can perform additional operations without modifying the application source code. The hook affects the container by keeping it in the Waiting state until completion, and if it fails or returns a non-zero exit code, the main container will be killed. A pod manifest containing a post-start hook is shown, executing a shell script as part of the container lifecycle.\",\n",
       " \"A pod's lifecycle includes post-start and pre-stop hooks, which can be used to execute commands or initiate a graceful shutdown. Post-start hooks are executed immediately after a container is started, while pre-stop hooks are executed before a container is terminated. If a hook fails, it will display an error message in the pod's events. To troubleshoot failed hooks, one can use kubectl describe pod and exec into the container to examine log files or mount an emptyDir volume for logging purposes.\",\n",
       " \"Kubernetes pre-stop hooks can be used to perform actions before a pod is terminated, such as sending an HTTP GET request. However, if the hook fails or returns an error, it will not prevent the container from being terminated. It's also important to note that using a pre-stop hook solely to send a SIGTERM signal to an app is not necessary, and instead the shell should be configured to pass the signal to the app process. A pre-stop hook YAML snippet example is provided for performing an HTTP GET request in a pod.\",\n",
       " \"A pod's lifecycle involves container termination, triggered by the deletion of the Pod object through the API server. The Kubelet terminates each container, running pre-stop hooks and sending SIGTERM signals to main processes. If containers don't shut down cleanly within the configured termination grace period, they are forcibly killed with a SIGKILL signal.\",\n",
       " \"The termination grace period for Kubernetes pods can be configured in the pod spec or overridden when deleting the pod. It's essential to set a sufficient time for processes to finish cleaning up before being killed. Applications should react to SIGTERM signals by starting their shut-down procedure and terminating within a fixed amount of time, using pre-stop hooks if necessary.\",\n",
       " 'When a Kubernetes pod receives a termination signal, it should not start migrating its data immediately. Instead, a dedicated pod or CronJob resource can be used to periodically check for orphaned data and migrate it to remaining pods, ensuring data is not lost in case of node failure or application upgrade.',\n",
       " 'To handle client requests properly, Kubernetes apps need to follow rules to prevent broken connections when pods are starting up or shutting down. This involves ensuring each connection is handled properly at pod startup by adding an HTTP GET readiness probe that returns success only when the app is ready to accept incoming requests.',\n",
       " \"When a pod is deleted, the API server modifies etcd and notifies watchers, including Kubelet and Endpoints controller. Two parallel sequences of events occur: A) containers on the worker node stop, kube-proxy removes the pod as an endpoint, and iptables updates; B) the Pod's deletion notification is sent to the client, the endpoints controller removes the pod from its list, and the kubelet removes the pod from iptables.\",\n",
       " \"When a pod is deleted in Kubernetes, two sequences of events occur in parallel: the Kubelet shuts down the app's process and removes it from iptables rules. The shutdown sequence is relatively short, while updating iptables rules involves a longer chain of events, including notification to the Endpoints controller, API server, and kube-proxy.\",\n",
       " 'The pod needs to keep accepting connections after receiving the termination signal until all kube-proxies and other components have finished updating their rules. A long-enough delay, such as 5-10 seconds, should be added before shutting down the pod to ensure a smooth user experience.',\n",
       " 'To properly shut down an application, wait for a few seconds, then stop accepting new connections, close inactive keep-alive connections, wait for active requests to finish, and finally shut down completely. A pre-stop hook can be added to wait a few seconds before shutting down, preventing broken connections and frustrating the user with lingering pod listings.',\n",
       " 'To make apps easy to run and manage in Kubernetes, focus on creating small, minimal container images without unnecessary cruft. This includes using the FROM scratch directive in Dockerfiles and avoiding the latest image tag, which can cause versioning issues. Proper tagging of images and using imagePullPolicy wisely is also crucial to ensure smooth deployment and scaling.',\n",
       " 'Use tags with version designators, label resources with multiple dimensions, add annotations for resource descriptions and dependencies. Set imagePullPolicy to Always only in development. Use labels and annotations to manage resources and show dependencies between pods, and include contact information and build/version metadata.',\n",
       " \"In Kubernetes, you can make triage easier by having a container write a termination message to a specific file before exiting. This message is then shown in the output of kubectl describe pod without needing to inspect container logs. The default path for this message is /dev/termination-log but can be overridden with the terminationMessagePath field in the container definition. An example of this is provided, where a busybox container writes a message to /var/termination-reason and dies immediately, causing the pod's status to show CrashLoopBackOff, which can then be seen using kubectl describe.\",\n",
       " \"The chapter discusses best practices for developing apps in Kubernetes. It highlights the importance of writing app-specific status messages to a file or using the standard output, which can be easily viewed with the `kubectl logs` command. If an application crashes and is replaced, the new container's log is displayed, but using the `--previous` option shows the previous container's logs. The chapter also covers copying files from/to containers using `kubectl cp`, including logging files.\",\n",
       " 'Kubernetes provides no centralized logging by itself, requiring additional components to store and analyze container logs. Deploying a centralized logging solution like the EFK stack (FluentD, ElasticSearch, Kibana) is easy through YAML/JSON manifests or Helm charts, allowing for historical log examination and trend analysis.',\n",
       " 'The chapter discusses best practices for developing apps in Kubernetes. It highlights the importance of handling multi-line log statements by outputting JSON logs instead of plain text, which can be stored and shown as a single entry in Kibana. The solution is to keep outputting human-readable logs while writing JSON logs to a file and having them processed by FluentD. Additionally, it suggests running apps outside of Kubernetes during development on local machines or IDEs without the need for containerization. It also advises connecting to backend services manually or temporarily making them accessible externally using NodePort or LoadBalancer-type Services.',\n",
       " \"When developing an app that requires access to the Kubernetes API server, you can easily talk to the API server from outside the cluster during development using ServiceAccount's token or ambassador container. Alternatively, you can run your app inside a Kubernetes cluster using Minikube, which provides a single worker node but is valuable for trying out your app in Kubernetes and developing resource manifests. You can also mount local files into the Minikube VM and containers using hostPath volume or use the Docker daemon inside the Minikube VM to build container images.\",\n",
       " 'Minikube allows developers to build apps locally without pushing images to a registry. Environment variables can be set using \"eval $(minikube docker-env)\" to use the Minikube VM\\'s Docker daemon. Images can also be built locally and copied over to the Minikube VM, or combined with a proper Kubernetes cluster for development and deployment.',\n",
       " 'The document discusses best practices for development and testing in Kubernetes, including using tools like kube-applier to manage running apps through version control systems. It also introduces Ksonnet as an alternative to writing YAML/JSON manifests, allowing users to define parameterized JSON fragments and build full manifests with much less code.',\n",
       " \"The chapter emphasizes the importance of using Ksonnet and Jsonnet for consistent manifests, employing Continuous Integration and Continuous Delivery (CI/CD) pipelines with tools like Fabric8 or Google Cloud Platform's online labs to automate deployment, and understanding Kubernetes' distributed nature and eventual consistency model. It also highlights the need for apps to shut down properly without breaking client connections.\",\n",
       " 'The document provides small tips for app management by keeping image sizes small, adding annotations and labels, and making termination reasons clear. It also teaches how to develop Kubernetes apps locally or in Mini-kube before deploying them on a multi-node cluster. Finally, it explains how to extend Kubernetes with custom API objects and controllers, enabling the creation of Platform-as-a-Service solutions.',\n",
       " \"This chapter covers extending Kubernetes by defining custom API objects, creating controllers for those objects, and adding custom API servers. It also explores how others have built Platform-as-a-Service solutions on top of Kubernetes, including Red Hat's OpenShift Container Platform and Deis Workflow.\",\n",
       " 'Kubernetes allows defining custom API objects through CustomResourceDefinitions (CRD) which is a description of the custom resource type. A CRD can be posted to the Kubernetes API server, enabling users to create instances of the custom resource. Each CRD typically has an associated controller that makes something tangible happen in the cluster, such as spinning up a new web server pod and exposing it through a Service when creating an instance of the Website resource.',\n",
       " \"Kubernetes custom resource is created by posting a CustomResourceDefinition to the API server. A custom resource definition object has apiVersion, kind, metadata name and spec with scope as Namespaced. It's used to make Kubernetes accept instances of custom Website resources which will result in creation of Service and Pod for each instance.\",\n",
       " 'A custom API object called Website is defined with group: extensions.example.com, version: v1, and names: kind: Website. After posting the descriptor to Kubernetes, instances of the custom Website resource can be created. A YAML manifest for a Website resource instance is shown, specifying apiVersion: extensions.example.com/v1, kind: Website, metadata: name: kubia, and spec: gitRepo: https://github.com/luksa/kubia-website-example.git.',\n",
       " \"You can now store, retrieve and delete custom resources through the Kubernetes API server after creating a CustomResourceDefinition object. These objects don't do anything yet and you'll need to create a controller to make them functional.\",\n",
       " 'A custom API object, such as a Website object, can be created to trigger the spinning up of a web server serving Git repository contents. A custom controller is needed to automate this process by watching the API server for Website object creation and creating a Deployment and Service for each one. This allows the Pod to be managed and survive node failures.',\n",
       " 'The Website controller connects to the kubectl proxy process, which forwards requests to the API server, allowing the API server to send watch events for every change to any Website object. When a new Website object is created, the API server sends an ADDED event, triggering the controller to create a Deployment and Service object with a template for a pod containing an nginx server and a git-sync process, exposing the web server through a random port on each node.',\n",
       " 'A custom API controller is created to manage Website resources, which are deleted by the API server and watched through periodic re-listing. The controller runs as a pod in Kubernetes for development and deployment, using a Deployment resource to ensure proper execution.',\n",
       " \"A Kubernetes Deployment is created with two containers, main and proxy, running under a special ServiceAccount. The controller watches for events and creates resources as needed. A ClusterRoleBinding is required to enable access control. The deployment can be tested by creating a kubia Website resource and checking the controller's logs for watch event and resource creation.\",\n",
       " 'A custom API object was successfully created by the controller, which received an ADDED event and created a Service and a Deployment for the kubia-website Website. The API server responded with a 201 Created response, and the resulting Pod was also created. However, users can create invalid Website objects without validation schema in the CustomResourceDefinition. The controller can only validate the object when it receives it in a watch event, and if invalid, write an error message to the Website object.',\n",
       " 'To extend Kubernetes, validation of custom objects was introduced in version 1.8, enabling the API server to validate custom objects immediately. Alternatively, implementing a custom API server and integrating it with the main Kubernetes API server through API server aggregation allows for more control over custom object handling. This approach eliminates the need for a CRD and enables direct implementation of custom object types within the custom API server.',\n",
       " \"Kubernetes can be extended by creating Custom Resource Definitions (CRDs) in the core API server's etcd store. A custom API server can be added to a cluster by deploying it as a pod and exposing it through a Service, then integrating it into the main API server using an APIService resource. This allows client requests to be forwarded to the custom API server for specific resources. Additionally, custom clients can be built to create custom objects, making deployment easier. The Kubernetes Service Catalog API server will also be added through API server aggregation, enabling pods to consume services.\",\n",
       " \"Kubernetes' Service Catalog is a feature that allows users to provision services without dealing with underlying components like Pods and Services. It uses four generic API resources: ClusterServiceBroker, ClusterServiceClass, ServiceInstance, and ServiceBinding. A cluster admin creates a ClusterServiceBroker resource for each service broker, which lists available services. Users create a ServiceInstance resource for the required service, and a ServiceBinding to bind it to client pods.\",\n",
       " 'The Kubernetes Service Catalog is a distributed system composed of three components: API Server, etcd, and Controller Manager. The Service Catalog API server stores resources created by posting YAML/JSON manifests to itself or uses CustomResourceDefinitions in the main API server. Controllers running in the Controller Manager talk to the Service Catalog API server and provision services using external service brokers registered with ServiceBroker resources.',\n",
       " 'A cluster administrator can register external ServiceBrokers in the Service Catalog through the OpenServiceBroker API, which provides operations for provisioning, updating, and deprovisioning services. A ClusterServiceBroker resource manifest is posted to the Service Catalog API to register a broker, and a controller connects to the specified URL to retrieve the list of services this broker can provision, creating ClusterServiceClass resources for each service type.',\n",
       " 'The Kubernetes Service Catalog allows users to retrieve a list of available services in a cluster using kubectl get serviceclasses. ClusterServiceClasses are similar to StorageClasses, but allow users to select the type of service they want to use. An example is shown for a PostgreSQL database, with two plans: free and premium. The ClusterServiceClass is provided by the database-broker broker.',\n",
       " 'To provision a service instance, create a Service-Instance resource with ClusterServiceClass and plan specified. The Service Catalog will contact the broker, passing on chosen class, plan, and parameters. The broker then provisions the service according to its configuration, potentially spinning up a new instance of a database or running it in a VM. Successful provisioning can be checked by inspecting the status section of the created ServiceInstance.',\n",
       " 'A Kubernetes Service Catalog is extended by binding a provisioned ServiceInstance to pods using a ServiceBinding resource. A Secret is created with necessary credentials, which can be manually mounted into pods for access to the ServiceInstance.',\n",
       " 'The Service Catalog allows service providers to expose services in any Kubernetes cluster by registering a broker. A Secret can be created with credentials for connecting to a service instance, and can be used by multiple pods. Once no longer needed, the ServiceBinding can be deleted, which will delete the Secret and perform an unbinding operation on the service broker. Additionally, if not needed, the ServiceInstance resource should also be deleted to deprovision the service.',\n",
       " \"Kubernetes is becoming a widely accepted foundation for Platform-as-a-Service (PaaS) offerings. Platforms built on top of Kubernetes, such as Deis Workflow and Red Hat's OpenShift, provide features like easy provisioning, automated rollouts and scaling, user and group management, and additional API objects. Red Hat OpenShift automates application image building and deployment without requiring a Continuous Integration solution.\",\n",
       " 'OpenShift provides powerful user management features, allowing users to access certain Projects (Kubernetes Namespaces with additional annotations) and granting access by a cluster administrator. Application Templates in OpenShift are parameterizable JSON or YAML manifests that can be instantiated with placeholder values replaced with parameter values.',\n",
       " 'OpenShift provides pre-fabricated templates for complex applications, allowing users to quickly run them with minimal arguments. It also enables automatic deployment of newly built images by creating a DeploymentConfig object and pointing it to an ImageStream. BuildConfigs can trigger builds immediately after changes are committed to the source Git repository, building container images without manual intervention.',\n",
       " 'Kubernetes can be extended with features from OpenShift, such as DeploymentConfig, which provides pre- and post-deployment hooks and creates ReplicationControllers instead of ReplicaSets. Routes are used to expose Services externally, providing additional configuration for TLS termination and traffic splitting. Minishift is available for trying out OpenShift, along with OpenShift Online Starter. Deis Workflow, also built on Kubernetes, provides a PaaS with features like BuildConfigs and DeploymentConfigs.',\n",
       " \"Deis Workflow is a tool built on top of Kubernetes that creates services and replication controllers, providing developers with a simple environment. Deploying new versions of an app can be triggered by pushing changes with 'git push deis master'. The Helm tool is a package manager for Kubernetes, allowing the deployment and management of application packages called Charts.\",\n",
       " 'When extending Kubernetes, instead of writing manifests for apps like PostgreSQL or MySQL, check if someone has prepared a Helm chart for it. Once installed, running the app takes a single command and creates necessary Deployments, Services, Secrets, and PersistentVolumeClaims.',\n",
       " \"This final chapter shows how to extend Kubernetes' functionalities by registering custom resources, implementing custom controllers, and using API aggregation, Service Catalog, and platforms-as-a-service built on top of Kubernetes. A package manager called Helm is also introduced for deploying existing apps without requiring resource manifests.\",\n",
       " \"To switch between Minikube and Google Kubernetes Engine (GKE) clusters using kubectl, simply run 'minikube start' to configure kubectl for Minikube or use 'gcloud container clusters get-credentials my-gke-cluster' to set up GKE. Minikube reconfigures kubectl every time you start the cluster, making it easy to switch between the two.\",\n",
       " \"To switch between different Kubernetes clusters or namespaces without specifying the --namespace option every time, configure the kubeconfig file's location using the KUBECONFIG environment variable. This file contains four sections: clusters (list of available clusters), users (list of user credentials), contexts (defined by a cluster and a user), and current-context (the currently used context). By listing multiple config files in KUBECONFIG, kubectl can use them all at once.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_doc_sumary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a240b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entity_list_formatter(entities):\n",
    "    ent_lst=[]\n",
    "    #for entity in document_dict_deserialized[idx]['entities']:\n",
    "    for entity in entities:\n",
    "        ent_lst.append(entity['entity'])\n",
    "    return ent_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23af86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>img_cnt</th>\n",
       "      <th>img_npy_lst</th>\n",
       "      <th>text</th>\n",
       "      <th>tables</th>\n",
       "      <th>entities</th>\n",
       "      <th>relationships</th>\n",
       "      <th>summary_rel</th>\n",
       "      <th>summary</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>57\\nIntroducing pods\\n Therefore, you need to ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Docker', 'description': 'Containe...</td>\n",
       "      <td>[{'source_entity': '\"Network namespace\"', 'des...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Network namespace\",\\n ...</td>\n",
       "      <td>A pod of containers allows you to run closely ...</td>\n",
       "      <td>[{'highlight': 'A pod of containers allows you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>58\\nCHAPTER 3\\nPods: running containers in Kub...</td>\n",
       "      <td>[[Container 1 Container 1\\nContainer 2 Contain...</td>\n",
       "      <td>[{'entity': 'Pods', 'description': 'logical ho...</td>\n",
       "      <td>[{'source_entity': '\"Kubernetes\"', 'descriptio...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Kubernetes\",\\n    \"des...</td>\n",
       "      <td>Kubernetes pods are logical hosts that behave ...</td>\n",
       "      <td>[{'highlight': 'All pods in a Kubernetes clust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>59\\nIntroducing pods\\n Having said that, do yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>A multi-tier application consisting of fronten...</td>\n",
       "      <td>[{'highlight': 'Splitting multi-tier applicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>60\\nCHAPTER 3\\nPods: running containers in Kub...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Pod', 'description': 'a group of ...</td>\n",
       "      <td>[{'source_entity': '\"Frontend container\"', 'de...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Frontend container\",\\n...</td>\n",
       "      <td>Pods in Kubernetes are groups of containers th...</td>\n",
       "      <td>[{'highlight': 'Pods can contain multiple cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>61\\nCreating pods from YAML or JSON descriptor...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Kubernetes', 'description': 'Cont...</td>\n",
       "      <td>[{'source_entity': '\"volumes\"', 'description':...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"volumes\",\\n    \"destin...</td>\n",
       "      <td>You can create pods by posting a JSON or YAML ...</td>\n",
       "      <td>[{'highlight': 'You can create pods from YAML ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>563</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>531\\nPlatforms built on top of Kubernetes\\nthe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Kubernetes', 'description': 'Cont...</td>\n",
       "      <td>[{'source_entity': '\"Helm\"', 'description': 'm...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Helm\",\\n    \"destinati...</td>\n",
       "      <td>Deis Workflow is a tool built on top of Kubern...</td>\n",
       "      <td>[{'highlight': 'Deis Workflow is a tool that c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>564</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>532\\nCHAPTER 18\\nExtending Kubernetes\\nWhen yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Kubernetes', 'description': 'Cont...</td>\n",
       "      <td>[{'source_entity': '\"Helm chart\"', 'descriptio...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Helm chart\",\\n    \"des...</td>\n",
       "      <td>When extending Kubernetes, instead of writing ...</td>\n",
       "      <td>[{'highlight': 'You can run a PostgreSQL or My...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>565</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>533\\nSummary\\n18.4\\nSummary\\nThis final chapte...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Kubernetes', 'description': 'Cont...</td>\n",
       "      <td>[{'source_entity': '\"Helm\"', 'description': 'p...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Helm\",\\n    \"destinati...</td>\n",
       "      <td>This final chapter shows how to extend Kuberne...</td>\n",
       "      <td>[{'highlight': 'Custom resources can be regist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>566</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>534\\nappendix A\\nUsing kubectl\\nwith multiple ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'kubectl', 'description': 'Command...</td>\n",
       "      <td>[{'source_entity': 'kubeconfig', 'description'...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"kubeconfig\",\\n    \"des...</td>\n",
       "      <td>To switch between Minikube and Google Kubernet...</td>\n",
       "      <td>[{'highlight': 'You can run examples in this b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>567</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>535\\nUsing kubectl with multiple clusters or n...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'kubectl', 'description': 'command...</td>\n",
       "      <td>[{'source_entity': 'kubectl', 'description': '...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"kubectl\",\\n    \"destin...</td>\n",
       "      <td>To switch between different Kubernetes cluster...</td>\n",
       "      <td>[{'highlight': 'You can use multiple config fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page  img_cnt img_npy_lst  \\\n",
       "0      89        0          []   \n",
       "1      90        0          []   \n",
       "2      91        0          []   \n",
       "3      92        0          []   \n",
       "4      93        0          []   \n",
       "..    ...      ...         ...   \n",
       "474   563        0          []   \n",
       "475   564        0          []   \n",
       "476   565        0          []   \n",
       "477   566        0          []   \n",
       "478   567        0          []   \n",
       "\n",
       "                                                  text  \\\n",
       "0    57\\nIntroducing pods\\n Therefore, you need to ...   \n",
       "1    58\\nCHAPTER 3\\nPods: running containers in Kub...   \n",
       "2    59\\nIntroducing pods\\n Having said that, do yo...   \n",
       "3    60\\nCHAPTER 3\\nPods: running containers in Kub...   \n",
       "4    61\\nCreating pods from YAML or JSON descriptor...   \n",
       "..                                                 ...   \n",
       "474  531\\nPlatforms built on top of Kubernetes\\nthe...   \n",
       "475  532\\nCHAPTER 18\\nExtending Kubernetes\\nWhen yo...   \n",
       "476  533\\nSummary\\n18.4\\nSummary\\nThis final chapte...   \n",
       "477  534\\nappendix A\\nUsing kubectl\\nwith multiple ...   \n",
       "478  535\\nUsing kubectl with multiple clusters or n...   \n",
       "\n",
       "                                                tables  \\\n",
       "0                                                   []   \n",
       "1    [[Container 1 Container 1\\nContainer 2 Contain...   \n",
       "2                                                   []   \n",
       "3                                                   []   \n",
       "4                                                   []   \n",
       "..                                                 ...   \n",
       "474                                                 []   \n",
       "475                                                 []   \n",
       "476                                                 []   \n",
       "477                                                 []   \n",
       "478                                                 []   \n",
       "\n",
       "                                              entities  \\\n",
       "0    [{'entity': 'Docker', 'description': 'Containe...   \n",
       "1    [{'entity': 'Pods', 'description': 'logical ho...   \n",
       "2                                                   []   \n",
       "3    [{'entity': 'Pod', 'description': 'a group of ...   \n",
       "4    [{'entity': 'Kubernetes', 'description': 'Cont...   \n",
       "..                                                 ...   \n",
       "474  [{'entity': 'Kubernetes', 'description': 'Cont...   \n",
       "475  [{'entity': 'Kubernetes', 'description': 'Cont...   \n",
       "476  [{'entity': 'Kubernetes', 'description': 'Cont...   \n",
       "477  [{'entity': 'kubectl', 'description': 'Command...   \n",
       "478  [{'entity': 'kubectl', 'description': 'command...   \n",
       "\n",
       "                                         relationships  \\\n",
       "0    [{'source_entity': '\"Network namespace\"', 'des...   \n",
       "1    [{'source_entity': '\"Kubernetes\"', 'descriptio...   \n",
       "2                                                  NaN   \n",
       "3    [{'source_entity': '\"Frontend container\"', 'de...   \n",
       "4    [{'source_entity': '\"volumes\"', 'description':...   \n",
       "..                                                 ...   \n",
       "474  [{'source_entity': '\"Helm\"', 'description': 'm...   \n",
       "475  [{'source_entity': '\"Helm chart\"', 'descriptio...   \n",
       "476  [{'source_entity': '\"Helm\"', 'description': 'p...   \n",
       "477  [{'source_entity': 'kubeconfig', 'description'...   \n",
       "478  [{'source_entity': 'kubectl', 'description': '...   \n",
       "\n",
       "                                           summary_rel  \\\n",
       "0    [[\\n  {\\n    \"source\": \"Network namespace\",\\n ...   \n",
       "1    [[\\n  {\\n    \"source\": \"Kubernetes\",\\n    \"des...   \n",
       "2                                                   []   \n",
       "3    [[\\n  {\\n    \"source\": \"Frontend container\",\\n...   \n",
       "4    [[\\n  {\\n    \"source\": \"volumes\",\\n    \"destin...   \n",
       "..                                                 ...   \n",
       "474  [[\\n  {\\n    \"source\": \"Helm\",\\n    \"destinati...   \n",
       "475  [[\\n  {\\n    \"source\": \"Helm chart\",\\n    \"des...   \n",
       "476  [[\\n  {\\n    \"source\": \"Helm\",\\n    \"destinati...   \n",
       "477  [[\\n  {\\n    \"source\": \"kubeconfig\",\\n    \"des...   \n",
       "478  [[\\n  {\\n    \"source\": \"kubectl\",\\n    \"destin...   \n",
       "\n",
       "                                               summary  \\\n",
       "0    A pod of containers allows you to run closely ...   \n",
       "1    Kubernetes pods are logical hosts that behave ...   \n",
       "2    A multi-tier application consisting of fronten...   \n",
       "3    Pods in Kubernetes are groups of containers th...   \n",
       "4    You can create pods by posting a JSON or YAML ...   \n",
       "..                                                 ...   \n",
       "474  Deis Workflow is a tool built on top of Kubern...   \n",
       "475  When extending Kubernetes, instead of writing ...   \n",
       "476  This final chapter shows how to extend Kuberne...   \n",
       "477  To switch between Minikube and Google Kubernet...   \n",
       "478  To switch between different Kubernetes cluster...   \n",
       "\n",
       "                                            highlights  \n",
       "0    [{'highlight': 'A pod of containers allows you...  \n",
       "1    [{'highlight': 'All pods in a Kubernetes clust...  \n",
       "2    [{'highlight': 'Splitting multi-tier applicati...  \n",
       "3    [{'highlight': 'Pods can contain multiple cont...  \n",
       "4    [{'highlight': 'You can create pods from YAML ...  \n",
       "..                                                 ...  \n",
       "474  [{'highlight': 'Deis Workflow is a tool that c...  \n",
       "475  [{'highlight': 'You can run a PostgreSQL or My...  \n",
       "476  [{'highlight': 'Custom resources can be regist...  \n",
       "477  [{'highlight': 'You can run examples in this b...  \n",
       "478  [{'highlight': 'You can use multiple config fi...  \n",
       "\n",
       "[479 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(document_dict_deserialized)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c28849e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entity_list']=df['entities'].apply(lambda x: entity_list_formatter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebc300f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [Docker, Kubernetes, Pods, Containers, Linux n...\n",
       "1      [Pods, Kubernetes, Flat inter-pod network, Pod...\n",
       "2                                                     []\n",
       "3      [Pod, Container, Kubernetes, Volume, Sidecar c...\n",
       "4      [Kubernetes, REST API, kubectl, YAML, JSON, Po...\n",
       "                             ...                        \n",
       "474    [Kubernetes, Helm, Deis Workflow, Services, Re...\n",
       "475    [Kubernetes, PostgreSQL, MySQL, Helm chart, De...\n",
       "476    [Kubernetes, Custom-ResourceDefinition, API se...\n",
       "477    [kubectl, Minikube, Google Kubernetes Engine (...\n",
       "478    [kubectl, multiple clusters, namespaces, kubec...\n",
       "Name: entity_list, Length: 479, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entity_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fb3ae93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>img_cnt</th>\n",
       "      <th>img_npy_lst</th>\n",
       "      <th>text</th>\n",
       "      <th>tables</th>\n",
       "      <th>entities</th>\n",
       "      <th>relationships</th>\n",
       "      <th>summary_rel</th>\n",
       "      <th>summary</th>\n",
       "      <th>highlights</th>\n",
       "      <th>entity_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>57\\nIntroducing pods\\n Therefore, you need to ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Docker', 'description': 'Containe...</td>\n",
       "      <td>[{'source_entity': '\"Network namespace\"', 'des...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Network namespace\",\\n ...</td>\n",
       "      <td>A pod of containers allows you to run closely ...</td>\n",
       "      <td>[{'highlight': 'A pod of containers allows you...</td>\n",
       "      <td>[Docker, Kubernetes, Pods, Containers, Linux n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>58\\nCHAPTER 3\\nPods: running containers in Kub...</td>\n",
       "      <td>[[Container 1 Container 1\\nContainer 2 Contain...</td>\n",
       "      <td>[{'entity': 'Pods', 'description': 'logical ho...</td>\n",
       "      <td>[{'source_entity': '\"Kubernetes\"', 'descriptio...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Kubernetes\",\\n    \"des...</td>\n",
       "      <td>Kubernetes pods are logical hosts that behave ...</td>\n",
       "      <td>[{'highlight': 'All pods in a Kubernetes clust...</td>\n",
       "      <td>[Pods, Kubernetes, Flat inter-pod network, Pod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>59\\nIntroducing pods\\n Having said that, do yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>A multi-tier application consisting of fronten...</td>\n",
       "      <td>[{'highlight': 'Splitting multi-tier applicati...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>60\\nCHAPTER 3\\nPods: running containers in Kub...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Pod', 'description': 'a group of ...</td>\n",
       "      <td>[{'source_entity': '\"Frontend container\"', 'de...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Frontend container\",\\n...</td>\n",
       "      <td>Pods in Kubernetes are groups of containers th...</td>\n",
       "      <td>[{'highlight': 'Pods can contain multiple cont...</td>\n",
       "      <td>[Pod, Container, Kubernetes, Volume, Sidecar c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>61\\nCreating pods from YAML or JSON descriptor...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Kubernetes', 'description': 'Cont...</td>\n",
       "      <td>[{'source_entity': '\"volumes\"', 'description':...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"volumes\",\\n    \"destin...</td>\n",
       "      <td>You can create pods by posting a JSON or YAML ...</td>\n",
       "      <td>[{'highlight': 'You can create pods from YAML ...</td>\n",
       "      <td>[Kubernetes, REST API, kubectl, YAML, JSON, Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>563</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>531\\nPlatforms built on top of Kubernetes\\nthe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Kubernetes', 'description': 'Cont...</td>\n",
       "      <td>[{'source_entity': '\"Helm\"', 'description': 'm...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Helm\",\\n    \"destinati...</td>\n",
       "      <td>Deis Workflow is a tool built on top of Kubern...</td>\n",
       "      <td>[{'highlight': 'Deis Workflow is a tool that c...</td>\n",
       "      <td>[Kubernetes, Helm, Deis Workflow, Services, Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>564</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>532\\nCHAPTER 18\\nExtending Kubernetes\\nWhen yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Kubernetes', 'description': 'Cont...</td>\n",
       "      <td>[{'source_entity': '\"Helm chart\"', 'descriptio...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Helm chart\",\\n    \"des...</td>\n",
       "      <td>When extending Kubernetes, instead of writing ...</td>\n",
       "      <td>[{'highlight': 'You can run a PostgreSQL or My...</td>\n",
       "      <td>[Kubernetes, PostgreSQL, MySQL, Helm chart, De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>565</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>533\\nSummary\\n18.4\\nSummary\\nThis final chapte...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'Kubernetes', 'description': 'Cont...</td>\n",
       "      <td>[{'source_entity': '\"Helm\"', 'description': 'p...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"Helm\",\\n    \"destinati...</td>\n",
       "      <td>This final chapter shows how to extend Kuberne...</td>\n",
       "      <td>[{'highlight': 'Custom resources can be regist...</td>\n",
       "      <td>[Kubernetes, Custom-ResourceDefinition, API se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>566</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>534\\nappendix A\\nUsing kubectl\\nwith multiple ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'kubectl', 'description': 'Command...</td>\n",
       "      <td>[{'source_entity': 'kubeconfig', 'description'...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"kubeconfig\",\\n    \"des...</td>\n",
       "      <td>To switch between Minikube and Google Kubernet...</td>\n",
       "      <td>[{'highlight': 'You can run examples in this b...</td>\n",
       "      <td>[kubectl, Minikube, Google Kubernetes Engine (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>567</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>535\\nUsing kubectl with multiple clusters or n...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'entity': 'kubectl', 'description': 'command...</td>\n",
       "      <td>[{'source_entity': 'kubectl', 'description': '...</td>\n",
       "      <td>[[\\n  {\\n    \"source\": \"kubectl\",\\n    \"destin...</td>\n",
       "      <td>To switch between different Kubernetes cluster...</td>\n",
       "      <td>[{'highlight': 'You can use multiple config fi...</td>\n",
       "      <td>[kubectl, multiple clusters, namespaces, kubec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page  img_cnt img_npy_lst  \\\n",
       "0      89        0          []   \n",
       "1      90        0          []   \n",
       "2      91        0          []   \n",
       "3      92        0          []   \n",
       "4      93        0          []   \n",
       "..    ...      ...         ...   \n",
       "474   563        0          []   \n",
       "475   564        0          []   \n",
       "476   565        0          []   \n",
       "477   566        0          []   \n",
       "478   567        0          []   \n",
       "\n",
       "                                                  text  \\\n",
       "0    57\\nIntroducing pods\\n Therefore, you need to ...   \n",
       "1    58\\nCHAPTER 3\\nPods: running containers in Kub...   \n",
       "2    59\\nIntroducing pods\\n Having said that, do yo...   \n",
       "3    60\\nCHAPTER 3\\nPods: running containers in Kub...   \n",
       "4    61\\nCreating pods from YAML or JSON descriptor...   \n",
       "..                                                 ...   \n",
       "474  531\\nPlatforms built on top of Kubernetes\\nthe...   \n",
       "475  532\\nCHAPTER 18\\nExtending Kubernetes\\nWhen yo...   \n",
       "476  533\\nSummary\\n18.4\\nSummary\\nThis final chapte...   \n",
       "477  534\\nappendix A\\nUsing kubectl\\nwith multiple ...   \n",
       "478  535\\nUsing kubectl with multiple clusters or n...   \n",
       "\n",
       "                                                tables  \\\n",
       "0                                                   []   \n",
       "1    [[Container 1 Container 1\\nContainer 2 Contain...   \n",
       "2                                                   []   \n",
       "3                                                   []   \n",
       "4                                                   []   \n",
       "..                                                 ...   \n",
       "474                                                 []   \n",
       "475                                                 []   \n",
       "476                                                 []   \n",
       "477                                                 []   \n",
       "478                                                 []   \n",
       "\n",
       "                                              entities  \\\n",
       "0    [{'entity': 'Docker', 'description': 'Containe...   \n",
       "1    [{'entity': 'Pods', 'description': 'logical ho...   \n",
       "2                                                   []   \n",
       "3    [{'entity': 'Pod', 'description': 'a group of ...   \n",
       "4    [{'entity': 'Kubernetes', 'description': 'Cont...   \n",
       "..                                                 ...   \n",
       "474  [{'entity': 'Kubernetes', 'description': 'Cont...   \n",
       "475  [{'entity': 'Kubernetes', 'description': 'Cont...   \n",
       "476  [{'entity': 'Kubernetes', 'description': 'Cont...   \n",
       "477  [{'entity': 'kubectl', 'description': 'Command...   \n",
       "478  [{'entity': 'kubectl', 'description': 'command...   \n",
       "\n",
       "                                         relationships  \\\n",
       "0    [{'source_entity': '\"Network namespace\"', 'des...   \n",
       "1    [{'source_entity': '\"Kubernetes\"', 'descriptio...   \n",
       "2                                                  NaN   \n",
       "3    [{'source_entity': '\"Frontend container\"', 'de...   \n",
       "4    [{'source_entity': '\"volumes\"', 'description':...   \n",
       "..                                                 ...   \n",
       "474  [{'source_entity': '\"Helm\"', 'description': 'm...   \n",
       "475  [{'source_entity': '\"Helm chart\"', 'descriptio...   \n",
       "476  [{'source_entity': '\"Helm\"', 'description': 'p...   \n",
       "477  [{'source_entity': 'kubeconfig', 'description'...   \n",
       "478  [{'source_entity': 'kubectl', 'description': '...   \n",
       "\n",
       "                                           summary_rel  \\\n",
       "0    [[\\n  {\\n    \"source\": \"Network namespace\",\\n ...   \n",
       "1    [[\\n  {\\n    \"source\": \"Kubernetes\",\\n    \"des...   \n",
       "2                                                   []   \n",
       "3    [[\\n  {\\n    \"source\": \"Frontend container\",\\n...   \n",
       "4    [[\\n  {\\n    \"source\": \"volumes\",\\n    \"destin...   \n",
       "..                                                 ...   \n",
       "474  [[\\n  {\\n    \"source\": \"Helm\",\\n    \"destinati...   \n",
       "475  [[\\n  {\\n    \"source\": \"Helm chart\",\\n    \"des...   \n",
       "476  [[\\n  {\\n    \"source\": \"Helm\",\\n    \"destinati...   \n",
       "477  [[\\n  {\\n    \"source\": \"kubeconfig\",\\n    \"des...   \n",
       "478  [[\\n  {\\n    \"source\": \"kubectl\",\\n    \"destin...   \n",
       "\n",
       "                                               summary  \\\n",
       "0    A pod of containers allows you to run closely ...   \n",
       "1    Kubernetes pods are logical hosts that behave ...   \n",
       "2    A multi-tier application consisting of fronten...   \n",
       "3    Pods in Kubernetes are groups of containers th...   \n",
       "4    You can create pods by posting a JSON or YAML ...   \n",
       "..                                                 ...   \n",
       "474  Deis Workflow is a tool built on top of Kubern...   \n",
       "475  When extending Kubernetes, instead of writing ...   \n",
       "476  This final chapter shows how to extend Kuberne...   \n",
       "477  To switch between Minikube and Google Kubernet...   \n",
       "478  To switch between different Kubernetes cluster...   \n",
       "\n",
       "                                            highlights  \\\n",
       "0    [{'highlight': 'A pod of containers allows you...   \n",
       "1    [{'highlight': 'All pods in a Kubernetes clust...   \n",
       "2    [{'highlight': 'Splitting multi-tier applicati...   \n",
       "3    [{'highlight': 'Pods can contain multiple cont...   \n",
       "4    [{'highlight': 'You can create pods from YAML ...   \n",
       "..                                                 ...   \n",
       "474  [{'highlight': 'Deis Workflow is a tool that c...   \n",
       "475  [{'highlight': 'You can run a PostgreSQL or My...   \n",
       "476  [{'highlight': 'Custom resources can be regist...   \n",
       "477  [{'highlight': 'You can run examples in this b...   \n",
       "478  [{'highlight': 'You can use multiple config fi...   \n",
       "\n",
       "                                           entity_list  \n",
       "0    [Docker, Kubernetes, Pods, Containers, Linux n...  \n",
       "1    [Pods, Kubernetes, Flat inter-pod network, Pod...  \n",
       "2                                                   []  \n",
       "3    [Pod, Container, Kubernetes, Volume, Sidecar c...  \n",
       "4    [Kubernetes, REST API, kubectl, YAML, JSON, Po...  \n",
       "..                                                 ...  \n",
       "474  [Kubernetes, Helm, Deis Workflow, Services, Re...  \n",
       "475  [Kubernetes, PostgreSQL, MySQL, Helm chart, De...  \n",
       "476  [Kubernetes, Custom-ResourceDefinition, API se...  \n",
       "477  [kubectl, Minikube, Google Kubernetes Engine (...  \n",
       "478  [kubectl, multiple clusters, namespaces, kubec...  \n",
       "\n",
       "[479 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49269efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"llama3.1\"\n",
    "book_text=''\n",
    "page_text=''\n",
    "file_name='Kubernetes_in_action_text_only'\n",
    "extension='.txt'\n",
    "#start_page_idx=0\n",
    "#end_page_index=479\n",
    "#full_filename=file_name+'_'+str(page_idx)+extension\n",
    "#full_filename\n",
    "pdf_enrichment_output_dir='../pdf_enrichment/pdf_enriched_output/'\n",
    "\n",
    "#LLM Model for Prompt Tuning\n",
    "llm = ChatOllama(base_url=\"http://192.168.50.100:11434\",model=model_name,temperature=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37a25d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_collector(page_idx):\n",
    "    \n",
    "    #print(\"Page Number\")\n",
    "    #print(page_idx)\n",
    "    \n",
    "    entities_jsonl=document_dict_deserialized[page_idx]['entities']\n",
    "    \n",
    "    entity_lst=entity_collector_per_page(entities_jsonl)\n",
    "    \n",
    "    \n",
    "    return entity_lst\n",
    "\n",
    "####Function to aggregrate all the descritption found for a node for different references.#####\n",
    "def desc_aggr_str(desc_lst):\n",
    "    desc_str=\"\"\n",
    "    for desc in desc_lst:\n",
    "        #print(desc)\n",
    "        desc_str=desc_str+desc\n",
    "        #print(desc)\n",
    "    return desc_str\n",
    "\n",
    "# Function to reverse a string\n",
    "def reverse(string):\n",
    "    string = string[::-1]\n",
    "    return string\n",
    "\n",
    "def detailed_summary(page_text,entities):\n",
    "    \n",
    "    \n",
    "    #parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker,\\n\n",
    "            Machine Learning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a document content  and create a informative and detailed summary\\n\n",
    "            within 2000 words which must include the main points from the document content.\\n\n",
    "            It must bring detailed information about the various entities and how they relate or carry out \\n\n",
    "            actions on the related onjects.\\n\n",
    "            The summary should include each and every important points from document.\\n\n",
    "            Compile the summary format the following using the rules mentioned rules below:\\n\n",
    "            1.For each summary wrap it up in json with the key named summary.\\n\n",
    "            2.After all the summaries have been extracted collate them into a list of json.\\n\n",
    "            3.Check whether the most important infomation from th text in isncluded in the summary.\\n\n",
    "            Output should only contain the list of json and no other words or character or sentences.\\n\n",
    "            You must output the collated json and nothing else.\\n\n",
    "            Here is the document content: \\n\\n {content} \\n\\n and the entities: \\n\\n {entities} \\n\\n\"\"\",\n",
    "            input_variables=[\"content\",\"entities\"],\n",
    "            ###Introduced by Kaustav while experimenting with XMLParsers\n",
    "            #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"content\": page_text,\n",
    "                \"entities\":entities\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    json_output = output\n",
    "    \n",
    "    if '[' in json_output:\n",
    "        json_output=json_output.split('[')[1]\n",
    "        json_output='['+json_output\n",
    "    elif '{' in json_output:\n",
    "        json_output=json_output.split('{')[1]\n",
    "        json_output='{'+json_output\n",
    "        json_output='['+json_output\n",
    "            \n",
    "        #try:\n",
    "    if ']' in json_output:\n",
    "        json_output=reverse(json_output)\n",
    "        #print('Reversed JSON OUTPUT:')\n",
    "        #print(json_output)\n",
    "        json_output=json_output.split(']')[1]\n",
    "        json_output=reverse(json_output)\n",
    "        #json_output=json_output.rsplit(']')[-1]\n",
    "        #page_output_json=json.loads(output)\n",
    "        #print('JSON OUTPUT:')\n",
    "        #print(json_output)\n",
    "        json_output= json_output + ']'\n",
    "        #print('JSON OUTPUT:')\n",
    "        #print(json_output) \n",
    "    elif '}' in json_output:\n",
    "        json_output=json_output.split('}')[0]\n",
    "        json_output=json_output+'}'\n",
    "        json_output=json_output+']'\n",
    "    \n",
    "    try:\n",
    "        json_output=json.loads(json_output)\n",
    "        formatted_description=json_output[0]['summary']\n",
    "        #print(\"Formatted Description:\")\n",
    "        #print(formatted_description)\n",
    "        return formatted_description\n",
    "    except:\n",
    "        print(\"Cannot load json for this raw description:\"+str(json_output))\n",
    "        #return \"Cannot load json for this raw description:\"+str(json_output)\n",
    "    \n",
    "    \n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0ac63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concise_summary(page_text,entities):\n",
    "    \n",
    "    \n",
    "    #parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker,\\n\n",
    "            Machine Learning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a detailed summary and create a concise yet informative summary,\\n\n",
    "            by picking up the main information related to the important entities and their functions.\\n\n",
    "            the length of the summary should be one fourth the of input  which must include the main points from the detailed.\\n\n",
    "            It must bring detailed information about the various entities and how they relate or carry out \\n\n",
    "            actions on the related onjects.\\n\n",
    "            The summary should include each and every important points from document.\\n\n",
    "            Compile the summary format the following using the rules mentioned rules below:\\n\n",
    "            1.For each summary wrap it up in json with the key named summary.\\n\n",
    "            2.After all the summaries have been extracted collate them into a list of json.\\n\n",
    "            3.Check whether the most important infomation from th text in isncluded in the summary.\\n\n",
    "            Output should only contain the list of json and no other words or character or sentences.\\n\n",
    "            You must output the collated json and nothing else.\\n\n",
    "            Here is the detailed summary: \\n\\n {content} \\n\\n and the entities: \\n\\n {entities} \\n\\n\"\"\",\n",
    "            input_variables=[\"content\"],\n",
    "            #input_variables=[\"content\",\"entities\"],\n",
    "            ###Introduced by Kaustav while experimenting with XMLParsers\n",
    "            #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"content\": page_text,\n",
    "                #\"entities\":entities\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    json_output = output\n",
    "    \n",
    "    if '[' in json_output:\n",
    "        json_output=json_output.split('[')[1]\n",
    "        json_output='['+json_output\n",
    "    elif '{' in json_output:\n",
    "        json_output=json_output.split('{')[1]\n",
    "        json_output='{'+json_output\n",
    "        json_output='['+json_output\n",
    "            \n",
    "        #try:\n",
    "    if ']' in json_output:\n",
    "        json_output=reverse(json_output)\n",
    "        #print('Reversed JSON OUTPUT:')\n",
    "        #print(json_output)\n",
    "        json_output=json_output.split(']')[1]\n",
    "        json_output=reverse(json_output)\n",
    "        #json_output=json_output.rsplit(']')[-1]\n",
    "        #page_output_json=json.loads(output)\n",
    "        #print('JSON OUTPUT:')\n",
    "        #print(json_output)\n",
    "        json_output= json_output + ']'\n",
    "        #print('JSON OUTPUT:')\n",
    "        #print(json_output) \n",
    "    elif '}' in json_output:\n",
    "        json_output=json_output.split('}')[0]\n",
    "        json_output=json_output+'}'\n",
    "        json_output=json_output+']'\n",
    "    \n",
    "    try:\n",
    "        json_output=json.loads(json_output)\n",
    "        formatted_description=json_output[0]['summary']\n",
    "        print(\"Formatted Description:\")\n",
    "        print(formatted_description)\n",
    "        return formatted_description\n",
    "    except:\n",
    "        print(\"Cannot load json for this raw description:\"+str(json_output))\n",
    "        #return \"Cannot load json for this raw description:\"+str(json_output)\n",
    "    \n",
    "    \n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b68eb296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"key\": \"Introducing pods\",\n",
      "      \"value\": \"You need to run each process in its own container. That’s how Docker and Kubernetes are meant to be used.\"\n",
      "    }]\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods are logical hosts that behave much like physical hosts or VMs in the non-container world. Processes running in the same pod are like processes running on the same physical or virtual machine, except that each process is encapsulated in a container.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"All pods in a Kubernetes cluster reside in a single flat, shared network-address space, which means every pod can access every other pod at the other pod's IP address. No NAT gateways exist between them.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Communication between pods is always simple and doesn't matter if two pods are scheduled onto a single or onto different worker nodes; in both cases, the containers inside those pods can communicate with each other across the flat NAT-less network.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Each pod gets its own IP address and is accessible from all other pods through this network established specifically for pods. This is usually achieved through an additional software-defined network layered on top of the actual network.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods should be organized as separate machines, but where each one hosts only a certain app. Instead of stuffing everything into a single pod, you should organize apps into multiple pods, where each one contains only tightly related components or processes.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Each pod gets a routable IP address and all other pods see the pod under that IP address.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods are relatively lightweight, so you can have as many as you need without incurring almost any overhead. This allows for efficient organization of apps into multiple pods.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The flat inter-pod network is a shared network-address space where all pods reside, allowing for simple communication between pods.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Kubernetes provides a platform for running containers in pods, which can be organized into multiple pods to efficiently manage apps and their components.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Containers inside pods can communicate with each other across the flat NAT-less network, similar to computers on a local area network (LAN).\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods are logical hosts that encapsulate processes in containers, allowing for efficient organization and communication between apps.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The flat inter-pod network is established through an additional software-defined network layered on top of the actual network, providing a shared space for all pods to communicate with each other.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Each pod gets its own IP address and can be accessed from all other pods through this network, allowing for efficient communication between apps.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods are organized into multiple pods to efficiently manage apps and their components, with each pod containing only tightly related components or processes.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The flat inter-pod network provides a shared space for all pods to communicate with each other, allowing for efficient communication between apps.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Kubernetes provides a platform for running containers in pods, which can be organized into multiple pods to efficiently manage apps and their components.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Containers inside pods can communicate with each other across the flat NAT-less network, similar to computers on a local area network (LAN).\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods are logical hosts that encapsulate processes in containers, allowing for efficient organization and communication between apps.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The flat inter-pod network is established through an additional software-defined network layered on top of the actual network, providing a shared space for all pods to communicate with each other.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Each pod gets its own IP address and can be accessed from all other pods through this network, allowing for efficient communication between apps.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods are organized into multiple pods to efficiently manage apps and their components, with each pod containing only tightly related components or processes.\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"key\": \"Introducing pods\",\n",
      "      \"value\": \"Having said that, do you think a multi-tier application consisting of a frontend application server and a backend database should be configured as a single pod or as two pods?\"\n",
      "    }]\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Creating pods from YAML or JSON descriptors\": {\n",
      "        \"description\": \"Pods and other Kubernetes resources are usually created by posting a JSON or YAML manifest to the Kubernetes REST API endpoint.\",\n",
      "        \"entities\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"entities\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods are running containers in Kubernetes. A pod can have multiple containers, and each container can expose a port to be accessed by other pods or external services. The ports exposed by a pod should be explicitly defined in the pod specification so that it's clear what ports are available for use. This allows users of the cluster to quickly see what ports each pod exposes.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The kubectl explain command can be used to discover possible API object fields when preparing a manifest. For example, when creating a pod manifest from scratch, you can ask kubectl to explain pods and it will list the attributes that the pod object can contain. This includes the kind, metadata, spec, and status of the pod.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The spec attribute of a pod is used to specify the desired behavior of the pod. It contains fields such as hostPID, volumes, and containers. The hostPID field determines whether to use the host's pid namespace, while the volumes field lists the volumes that can be mounted by containers belonging to the pod.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The status attribute of a pod is used to report the most recently observed status of the pod. This data may not be up-to-date and should not be relied upon for making decisions. Instead, it's meant to provide a general idea of the pod's state.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The volumes attribute of a pod is used to list the volumes that can be mounted by containers belonging to the pod. This allows containers to access shared data and storage resources.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The hostPID field determines whether to use the host's pid namespace when running a container in a pod. If set to true, the container will run with the same pid namespace as the host, allowing it to see all processes on the system.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The kubectl explain command is a powerful tool for discovering possible API object fields and understanding how they relate to each other. By using this command, users can gain a deeper understanding of the Kubernetes API and how to work with it effectively.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"A manifest is a YAML or JSON file that defines a set of resources to be created in a Kubernetes cluster. When preparing a manifest, users should use the kubectl explain command to discover possible API object fields and ensure that all necessary attributes are included.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"API objects in Kubernetes can contain various attributes, including kind, metadata, spec, and status. Each attribute has its own set of fields and behaviors, and users should carefully consider which attributes to include when creating a manifest or working with API objects.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The ports exposed by a pod are an important aspect of container networking in Kubernetes. By explicitly defining the ports exposed by each pod, users can ensure that services are properly configured and accessible from within the cluster.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Kubernetes provides a range of tools and features for working with containers, including the kubectl command-line tool and the API server. Users should take advantage of these resources to gain a deeper understanding of how Kubernetes works and how to use it effectively in their own projects.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The kind attribute of an API object is used to specify the type of resource being created or updated. This can be a string value representing the REST resource, such as 'Pod' or 'Service'.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The metadata attribute of an API object contains standard metadata fields, including name, namespace, and labels. These fields provide important context for understanding the resource being created or updated.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The spec attribute of an API object is used to specify the desired behavior of the resource being created or updated. This can include fields such as containers, volumes, and ports, depending on the type of resource being created.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The status attribute of an API object contains information about the most recently observed state of the resource being created or updated. This data may not be up-to-date and should not be relied upon for making decisions.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Containers in Kubernetes can expose ports to be accessed by other pods or external services. The ports exposed by a container are an important aspect of container networking and should be carefully considered when designing a pod or service.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The volumes attribute of a pod is used to list the volumes that can be mounted by containers belonging to the pod. This allows containers to access shared data and storage resources, which is essential for many applications.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The hostPID field determines whether to use the host's pid namespace when running a container in a pod. If set to true, the container will run with the same pid namespace as the host, allowing it to see all processes on the system.\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"key_points\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Organizing pods with labels\": {\n",
      "        \"description\": \"Adding labels to pods for better organization\",\n",
      "        \"labels\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Listing subsets of pods through label selectors\": \"List the pods again to see the updated labels: $ kubectl get po -L creation_method,env. Attaching labels to resources so you can see the labels next to each resource when listing them isn't that interesting. But labels go hand in hand with label selectors.\",\n",
      "      \"Label selectors allow you to select a subset of pods tagged with certain labels and perform an operation on those pods\": \"A label selector is a criterion, which filters resources based on whether they include a certain label with a certain value.\",\n",
      "      \"Listing pods using a label selector\": \"$ kubectl get po -l creation_method=manual. To list all pods that include the env label, whatever its value is: $ kubectl get po -l env. And those that don't have the env label: $ kubectl get po -l '!env'\",\n",
      "      \"Entities involved\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Using labels and selectors to constrain pod scheduling\": {\n",
      "        \"description\": \"All pods created so far have been scheduled randomly across worker nodes, but certain cases exist where you'll want to have at least a little say in where a pod should be scheduled.\",\n",
      "        \"entities\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Annotating pods\": {\n",
      "        \"You've added a nodeSelector field under the spec section. When you create the pod, the scheduler will only choose among the nodes that contain the gpu=true label (which is only a single node in your case).\",\n",
      "        \"Scheduling to one specific node: Similarly, you could also schedule a pod to an exact node, because each node also has a unique label with the key kubernetes.io/hostname and value set to the actual hostname of the node.\",\n",
      "        \"Labels and label selectors work and how they can be used to influence the operation of Kubernetes. The importance and usefulness of label selectors will become even more evident when we talk about Replication-Controllers and Services in the next two chapters.\"\n",
      "      },\n",
      "      \"Additional ways of influencing which node a pod is scheduled to are covered in chapter 16.\",\n",
      "      {\n",
      "        \"Annotating pods\": {\n",
      "          \"In addition to labels, pods and other objects can also contain annotations. Annotations are also key-value pairs, so in essence, they're similar to labels, but they aren't meant to hold identifying information.\",\n",
      "          \"Annotations can hold much larger pieces of information and are primarily meant to be used by tools. Certain annotations are automatically added to objects by Kubernetes, but others are added by users manually.\"\n",
      "        },\n",
      "        {\n",
      "          \"Looking up an object's annotations\": {\n",
      "            \"Let's see an example of an annotation that Kubernetes added automatically to the pod you created in the previous chapter. To see the annotations, you'll need to\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"nodeSelector\",\n",
      "      \"gpu=true label\",\n",
      "      \"kubernetes.io/hostname\",\n",
      "      \"label selectors\",\n",
      "      \"labels\",\n",
      "      \"annotations\",\n",
      "      \"pod\",\n",
      "      \"node\",\n",
      "      \"scheduler\",\n",
      "      \"Replication-Controllers\",\n",
      "      \"Services\",\n",
      "      \"chapter 16\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Kubernetes\": {\n",
      "        \"namespaces\": {\n",
      "          \"description\": \"groups objects into namespaces to provide a scope for object names\",\n",
      "          \"purpose\": \"split complex systems with numerous components into smaller distinct groups\"\n",
      "        }]\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"key_points\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Using namespaces to group resources has a corresponding API object that you can create, read, update, and delete by posting a YAML manifest to the API server. You could have created the namespace like this: $ kubectl create namespace custom-namespace namespace \\\"custom-namespace\\\" created NOTE Although most objects' names must conform to the naming conventions specified in RFC 1035 (Domain names), which means they may contain only letters, digits, dashes, and dots, namespaces (and a few others) aren't allowed to contain dots.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"To create resources in the namespace you've created, either add a namespace: custom-namespace entry to the metadata section, or specify the namespace when creating the resource with the kubectl create command: $ kubectl create -f kubia-manual.yaml -n custom-namespace pod \\\"kubia-manual\\\" created You now have two pods with the same name (kubia-manual). One is in the default namespace, and the other is in your custom-namespace.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"When listing, describing, modifying, or deleting objects in other namespaces, you need to pass the --namespace (or -n) flag to kubectl. If you don't specify the namespace, kubectl performs the action in the default namespace configured in the current kubectl context.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"To quickly switch to a different namespace, you can set up the following alias: alias kcd='kubectl config set-context $(kubectl config current-context) --namespace '. You can then switch between namespaces using kcd some-namespace.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Namespaces allow you to isolate objects into distinct groups, which allows you to operate only on those belonging to the specified namespace. However, they don't provide any kind of isolation of running objects. For example, different users deploying pods across different namespaces may not be isolated from each other and can communicate.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Whether namespaces provide network isolation depends on which networking solution is deployed with Kubernetes. When the solution doesn't provide inter-namespace network isolation, if a pod in namespace foo knows the IP address of a pod in namespace bar, it may be able to communicate with it.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"The entities involved are: namespaces, API object, kubectl, create namespace, custom-namespace, metadata section, kubia-manual.yaml, pod, default namespace, --namespace flag, kubectl config commands, alias kcd, networking solution, inter-namespace network isolation.\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods are running containers in Kubernetes. A pod can send traffic to another pod, such as HTTP requests. You've created a number of pods and now want to stop all of them because you don't need them anymore.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"To delete a pod by name, use the command $ kubectl delete po <pod_name>. This will terminate all containers in that pod and send a SIGTERM signal. If it doesn't shut down in time, the process is then killed through SIGKILL.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"To delete multiple pods by name, specify multiple space-separated names (for example, $ kubectl delete po <pod1> <pod2>).\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"You can also delete pods using label selectors. For example, to delete both the kubia-manual and kubia-manual-v2 pod, use the command $ kubectl delete po -l creation_method=manual.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Label selectors allow you to delete multiple pods at once based on a specific label. For example, to delete all canary pods, use the command $ kubectl delete po -l rel=canary.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"To delete a pod in a custom namespace, first delete the pod and then delete the entire namespace using the command $ kubectl delete ns <namespace_name>.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"When deleting pods, Kubernetes sends a SIGTERM signal to the process and waits for it to shut down gracefully. If it doesn't shut down in time, the process is then killed through SIGKILL.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods can be deleted by name, label selector, or entire namespace. It's essential to handle the SIGTERM signal properly to ensure processes are always shut down gracefully.\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \"Pods: running containers in Kubernetes\",\n",
      "    \"main_points\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Pods\": {\n",
      "        \"description\": \"can run multiple processes and are similar to physical hosts in the non-container world\",\n",
      "        \"actions\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Kubernetes\": {\n",
      "        \"description\": \"a container orchestration system for automating the deployment, scaling, and management of containers\",\n",
      "        \"key_points\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Kubernetes\": {\n",
      "        \"description\": \"A container orchestration system for automating the deployment, scaling, and management of containers.\",\n",
      "        \"functionality\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Replication and other controllers: deploying managed pods\": {\n",
      "        \"description\": \"In Kubernetes, a liveness probe checks if a container is running correctly. If the HTTP response code is 2xx or 3xx, the probe is successful. Otherwise, it's considered a failure and the container will be restarted.\",\n",
      "        \"entities\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"Kubernetes\": {\n",
      "        \"action\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"entities\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"entities\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"ReplicationController\": {\n",
      "        \"description\": \"Kubernetes creates a new Replication-Controller named kubia, which makes sure three pod instances always match the label selector app=kubia.\",\n",
      "        \"functionality\": \"Creates and manages pods based on a template\",\n",
      "        \"key_entities\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"entities\": \n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": {\n",
      "      \"entities\": \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_detailed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentity_list\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetailed_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentity_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pandas/core/frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10022\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10024\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10026\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10032\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10033\u001b[0m )\n\u001b[0;32m> 10034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pandas/core/apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pandas/core/apply.py:965\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 965\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pandas/core/apply.py:981\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    983\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    984\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    985\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_detailed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_list\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mdetailed_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentity_list\u001b[49m\u001b[43m)\u001b[49m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 70\u001b[0m, in \u001b[0;36mdetailed_summary\u001b[0;34m(page_text, entities)\u001b[0m\n\u001b[1;32m     55\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#Trying out XML Output parser by Kautsva added 20/08/2024####\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#chain = prompt | llm | parser\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Score\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#filtered_docs = []\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mentities\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m json_output \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_output:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_core/runnables/base.py:2878\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2876\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2877\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2878\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:756\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    750\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    754\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    755\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:613\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    612\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    614\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    615\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    617\u001b[0m ]\n\u001b[1;32m    618\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:603\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 603\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m         )\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:825\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 825\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:286\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    264\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    294\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    295\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    296\u001b[0m     )\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:217\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    210\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    216\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    219\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _chat_stream_response_to_chat_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:189\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    182\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    185\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[1;32m    188\u001b[0m     }\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    190\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload, stop\u001b[38;5;241m=\u001b[39mstop, api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    191\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/requests/models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    870\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    871\u001b[0m ):\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/requests/utils.py:572\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    571\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    573\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/urllib3/response.py:931\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/urllib3/response.py:1071\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1071\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/urllib3/response.py:999\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#df[\"summary_detailed\"]=df[['text','entity_list']].apply(lambda x : detailed_summary(x.text,x.entity_list),axis=1)\n",
    "#df[\"summary_detailed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bee5759",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'summary_detailed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'summary_detailed'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummary_detailed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'summary_detailed'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17afd1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_txt=[]\n",
    "for idx in range(0,len(document_dict_deserialized)):\n",
    "    document_txt.append(document_dict_deserialized[idx]['summary'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07cdebbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated. Containers within a pod share certain resources like network interfaces and IPC namespaces, but have fully isolated filesystems unless shared using a Kubernetes Volume concept.Kubernetes pods are logical hosts that behave like physical hosts or VMs, with processes running in the same pod behaving like processes on the same machine. Each pod has its own IP address and can communicate directly with other pods through a flat network, without NAT gateways. Pods should be organized by app, with each containing tightly related components or processes, allowing for as many pods as needed without significant overhead.A multi-tier application consisting of frontend and backend components should not be configured as a single pod but rather split into multiple pods to enable individual scaling and utilize computational resources on multiple nodes. This approach allows for separate scaling requirements for frontend and backend components, making it more efficient and suitable for applications with diverse resource needs.Pods in Kubernetes are groups of containers that can be run together, like a web server and a sidecar container for downloading content. To decide when to use multiple containers in a pod, ask yourself: do they need to run together, represent a single whole, or must they be scaled together? Typically, containers should be run in separate pods unless a specific reason requires them to be part of the same pod.You can create pods by posting a JSON or YAML manifest to the Kubernetes REST API endpoint. This method allows configuration of all properties, but requires knowledge of the Kubernetes API object definitions. Alternatively, you can use commands like kubectl run, but they limit the configurable properties. The YAML descriptor for an existing pod can be obtained using kubectl get with the -o yaml option, showing metadata and specification details.A Kubernetes Pod is a logical host for one or more application containers. It consists of metadata (name, namespace, labels) and spec (containers, volumes), with optional detailed status information. Key elements include terminationMessagePath, dnsPolicy, restartPolicy, serviceAccount, and volumeMounts.A Kubernetes pod can be created using a YAML or JSON descriptor, which typically consists of three parts: metadata, spec, and status. The spec section defines the container\\'s image, name, and ports, with specifying ports being informational only. A simple example is shown in kubia-manual.yaml, where a single container based on luksa/kubia image listens on port 8080.A Kubernetes pod is a collection of containers that run on a host, and can be described using a manifest. The pod spec contains attributes such as hostname, IP addresses, ports, and volumes that can be mounted by containers. Using kubectl explain, one can discover possible API object fields and drill deeper to learn more about each attribute.To create a pod from a YAML file, use kubectl create -f command. After creating the pod, you can retrieve its full YAML or JSON definition using kubectl get po <pod_name> -o yaml/json commands. You can also view application logs by tailing the container\\'s standard output and error streams.This chapter discusses pods in Kubernetes, focusing on retrieving logs from containers running within a pod. The container runtime redirects streams to files, allowing users to view logs by running `docker logs <container id>`. However, Kubernetes provides an easier way using `kubectl logs`, which can be used to retrieve logs from a pod without the need for SSH access. Additionally, if a pod contains multiple containers, the user must specify the container name when retrieving logs. The chapter also touches on centralized logging and port forwarding as methods to connect to a pod for testing and debugging purposes.Kubernetes allows port forwarding to a specific pod through the `kubectl port-forward` command, enabling direct access for debugging or testing purposes. This can be achieved by running `$ kubectl port-forward kubia-manual 8888:8080` and sending an HTTP request using `curl localhost:8888`. This method is effective for testing individual pods, especially in microservices architectures where many pods need to be categorized and managed.Kubernetes allows running multiple copies of the same component and different versions concurrently, which can lead to hundreds of pods without organization. To manage this, labels are used to organize pods and other Kubernetes resources into smaller groups based on arbitrary criteria, allowing developers and administrators to easily identify and operate on specific pods or groups with a single action.Adding labels to pods in a Kubernetes system allows for easy organization and understanding of the system\\'s structure. Labels can specify which app or microservice a pod belongs to, as well as whether it\\'s a stable, beta, or canary release. By using these labels, developers and ops personnel can easily see where each pod fits in, making it easier to manage complex microservices architectures.This chapter is about pods in Kubernetes, specifically running containers and adding/removing labels. Labels can be added to or modified on existing pods using kubectl label command. The --overwrite option is required when changing existing labels. Examples of labeling a new pod, viewing labels with kubectl get po --show-labels, and modifying labels on an existing pod are shown.Label selectors allow selecting subsets of pods based on labels. A label selector can filter resources by key, value, or not equal to a specified value. Examples include listing pods with creation_method=manual, env label, or no env label.This chapter discusses Kubernetes Pods, specifically focusing on running containers in a cluster. Label selectors are used to identify and select pods based on labels, with examples including creation_method!=manual, env in (prod,devel), and app=pc for selecting the product catalog microservice pods. Multiple conditions can be combined using comma-separated criteria, as shown in the selector app=pc,rel=beta. Label selectors are not only used for listing pods but also for performing actions on a subset of all pods.In Kubernetes, using labels and selectors is a way to constrain pod scheduling without specifying exact node placement. This allows for flexible scheduling based on node requirements, such as hardware infrastructure or GPU acceleration. Labels can be applied to nodes, and selectors can be used to match those labels, ensuring that pods are scheduled to nodes that meet specific criteria, while maintaining the decoupling of applications from infrastructure.Labels can be attached to Kubernetes objects like pods and nodes. Using labels, the ops team categorizes new nodes by hardware type or features like GPU availability. To schedule a pod that requires a GPU, create a YAML file with a node selector set to gpu=true and use kubectl create -f to deploy the pod.Pods can be annotated with labels and annotations. Labels are key-value pairs used for identification and grouping, while annotations hold larger pieces of information primarily meant for tools. Annotations are automatically added by Kubernetes or manually by users and are useful for adding descriptions, specifying creator names, and introducing new features. The importance of label selectors will become evident in future chapters on Replication-Controllers and Services.Kubernetes pods can have labels and annotations, where labels are short and used for organization, while annotations can contain large blobs of data up to 256KB. Annotations like kubernetes.io/created-by were deprecated in version 1.8 and removed in 1.9. Annotations can be added or modified using the kubectl annotate command, and it\\'s recommended to use unique prefixes to prevent key collisions.Kubernetes groups objects into namespaces, which provide a scope for object names and allow for separate, non-overlapping groups. Namespaces enable operating within one group at a time and using the same resource names multiple times across different namespaces. They can be used to split complex systems, separate resources in multi-tenant environments, or divide resources into production, development, and QA environments.Namespaces in Kubernetes enable separation of resources into non-overlapping groups, isolating them from other users\\' resources. They can be created by posting a YAML file to the Kubernetes API server using kubectl create -f or kubectl create namespace command.Namespaces allow grouping resources and can be created using kubectl create command or by posting YAML manifest to API server. Resources in other namespaces can be managed by adding namespace entry to metadata section or specifying namespace with kubectl create command. Namespaces provide isolation for objects, but do not guarantee network isolation between pods across different namespaces.Pods in Kubernetes can communicate with each other within a namespace. To stop and remove pods, use kubectl delete command. Pods can be deleted by name, label selector, or even deleting the whole namespace. When deleting a pod, Kubernetes sends a SIGTERM signal to shut down containers, and if they don\\'t respond, a SIGKILL signal is sent. It\\'s essential for processes to handle the SIGTERM signal properly.To delete all pods in a namespace, use the command $ kubectl delete po --all. This will delete all running and terminating pods in the current namespace. Alternatively, you can delete a specific pod by its name or delete all pods with a certain label using the label selector.Kubernetes pods run multiple containers as one entity, with kubectl commands like run and delete creating ReplicationControllers that manage pods. Deleting all resources in a namespace can be done with kubectl delete all --all, but note that some resources like Secrets are preserved and need to be deleted explicitly.Pods can run multiple processes similar to physical hosts. They have YAML/JSON descriptors that define their specification and current state. Labels and selectors help organize and perform operations on multiple pods. Annotations attach data to pods, while namespaces allow different teams to use the same cluster as separate Kubernetes clusters. The kubectl explain command provides information on resources.Kubernetes manages pods automatically, using resources like Replication-Controllers and Deployments to create and manage pods. This chapter focuses on keeping pods healthy, running multiple instances of the same pod, automating rescheduling after a node fails, scaling pods horizontally, running system-level pods, and batch jobs, as well as scheduling periodic or future tasks.Kubernetes checks if a container is alive through liveness probes and restarts it if it fails. Liveness probes can be specified for each container in a pod\\'s specification. Kubernetes periodically executes the probe and restarts the container if it fails. This ensures that applications are restarted even if they stop working without crashing, such as due to memory leaks or infinite loops.A liveness probe checks if a container is running correctly. A successful probe returns a 2xx or 3xx HTTP response code, while a failed probe returns an error code or no response at all. The chapter demonstrates creating a new pod with an HTTP GET liveness probe for a Node.js app that intentionally fails after five requests.The document explains how Kubernetes uses liveness probes to keep pods healthy. An httpGet liveness probe sends HTTP GET requests to a path on port 8080, and if the status code becomes 500, Kubernetes restarts the container. The document also demonstrates this by creating a pod with a liveness probe, showing it gets restarted after about a minute and a half, and describes how to obtain the application log of a crashed container using kubectl logs --previous.This page discusses replication and other controllers in Kubernetes, specifically the liveness probe which checks if a container is running correctly. If the container fails the probe, it will be killed and re-created. The page also explains how to configure additional properties of the liveness probe, such as delay, timeout, period, and initial delay. An example YAML file is provided to demonstrate how to set an initial delay for the liveness probe.To keep pods healthy, it\\'s essential to set an initial delay for liveness probes. This prevents probes from failing as soon as the app starts, leading to unnecessary restarts. A liveness probe should check if the server is responding and ideally perform internal status checks on vital components. It\\'s crucial to ensure the /health endpoint doesn\\'t require authentication and only checks internals of the app, not external factors.A ReplicationController in Kubernetes ensures its pods are always kept running by creating replacement pods if one disappears. Liveness probes shouldn\\'t use too many computational resources and should be executed relatively often to keep containers running. Kubernetes will retry a probe several times before considering it a failed attempt, so implementing a retry loop is unnecessary.A ReplicationController monitors running pods and ensures the desired number of replicas matches the actual number. If too few, it creates new replicas; if too many, it removes excess replicas. It recreates lost pods when a node fails, but not manually created or changed pods.A ReplicationController\\'s job is to maintain an exact number of pods that match its label selector by creating or deleting pods as needed, with three essential parts: a label selector, replica count, and pod template.A ReplicationController\\'s replica count, label selector, and pod template can be modified at any time, but only changes to the replica count affect existing pods. Changes to the label selector or pod template have no effect on existing pods and are used as a \\'cookie cutter\\' for new pods created by this ReplicationController. The controller ensures a pod is always running, creates replacement replicas when a cluster node fails, and enables easy horizontal scaling of pods.Kubernetes creates a Replication-Controller named kubia that ensures three pod instances match the label selector app=kubia. When there aren\\'t enough pods, it creates new ones from the provided pod template. The API server verifies the ReplicationController definition and will not accept it if misconfigured. To prevent such scenarios, let Kubernetes extract the selector from the pod template.A ReplicationController is introduced, managing three pods and automatically spinning up new ones if any are deleted. The kubectl get command shows information about ReplicationControllers, including desired and actual pod numbers. Additional details can be obtained with the kubectl describe command, displaying the ReplicationController\\'s name, namespace, selector, labels, annotations, replicas, and pod status.A ReplicationController in Kubernetes creates a new pod to replace one that has been deleted when it detects an inadequate number of running pods, triggered by events such as pod deletion or termination.A ReplicationController in Kubernetes automatically spins up new pods to replace those that are down when a node fails, as demonstrated by simulating a node failure on a three-node cluster. After shutting down the network interface of one node, the status is shown as NotReady, and the pods remain unchanged for several minutes before the ReplicationController creates a new pod to replace the downed ones.ReplicationController automatically manages pods based on a label selector and can spin up new pods if one fails or is removed from scope. A pod\\'s labels can be changed to move it in or out of the ReplicationController\\'s scope, but changing its labels does not delete it. The replication controller will notice if a managed pod is missing and spin up a new one to replace it.A ReplicationController doesn\\'t care if labels are added to its managed pods. Changing a label on a managed pod makes it no longer match the controller\\'s label selector, prompting the controller to start a new pod to bring the number back to three.A ReplicationController can spin up new pods to bring the number back up after one is removed, allowing for independent pod management and changing label selectors to control which pods are included in the controller\\'s scope.ReplicationControllers can modify their pod template at any time, but changes only affect new pods created after the modification. To change an existing pod, it must be deleted and a new one will be created based on the updated template.ReplicationControllers ensure a specific number of pod instances is always running. Scaling pods horizontally is trivial and can be done by changing the replicas field, either by using the command `kubectl scale` or by editing the ReplicationController\\'s definition directly with `kubectl edit`. This allows scaling up or down with ease.A ReplicationController is updated when scaled up or down, and it immediately scales the number of pods to the desired state. Scaling is a matter of stating the desired state, not telling Kubernetes how to do it. Declarative approach makes interacting with a Kubernetes cluster easy. When deleting a ReplicationController through kubectl delete, the pods are also deleted unless managed by another controller.ReplicationControllers manage pods and keep them running without interruption, but can be deleted while keeping the pods running using the --cascade=false option. ReplicaSets are a newer resource that replaces ReplicationControllers completely and should be used instead. Deleting a ReplicationController leaves its pods unmanaged, but a new one can be created to manage them again.ReplicaSets are used instead of ReplicationControllers to manage replicas. A ReplicaSet behaves exactly like a ReplicationController but with more expressive pod selectors, allowing matching pods based on label presence or absence, and not just specific values. This enables a single ReplicaSet to match multiple sets of pods and treat them as a single group. The process of creating a ReplicaSet involves defining its YAML configuration, including the API version, kind, metadata, selector, replicas, template, and containers, which can be used to adopt orphaned pods created by a ReplicationController.ReplicaSets aren\\'t part of the v1 API, so specify the proper apiVersion when creating a resource. To create a ReplicaSet, use kubectl create command with YAML file, then examine it with kubectl get and describe commands. The apiVersion property specifies the API group (apps) and actual API version (v1beta2), which categorizes Kubernetes resources into core and other groups.ReplicaSets are similar to ReplicationControllers, but with more expressive label selectors. The matchExpressions property allows for complex matching rules, such as requiring a pod to have a specific label or not having a certain label. This provides flexibility in selecting pods, making ReplicaSets more powerful than ReplicationControllers.ReplicaSets and ReplicationControllers are used for running a specific number of pods in a Kubernetes cluster, but DaemonSets are used to run one pod on each node, with exactly one instance per node, suitable for infrastructure-related pods like log collectors and resource monitors.A DaemonSet is used to run a pod on every node in a Kubernetes cluster, or on a subset of nodes specified by a node selector. It ensures a desired number of pods exist and creates a new pod instance if a new node is added or an existing node is deleted. Unlike ReplicaSets, DaemonSets do not need a replica count and will deploy pods even to unschedulable nodes.A DaemonSet is created for deploying managed pods. A YAML definition is written for the DaemonSet to run a mock ssd-monitor process on nodes with the \\'disk=ssd\\' label. The DaemonSet will create an instance of the pod on each node that meets this condition.Creating a DaemonSet to run one pod on each node requires labeling nodes with the required label. Initially, the DaemonSet appears not to deploy pods due to missing labels. Adding the label to one or more nodes triggers the DaemonSet to create pods for matching nodes.A chapter about deploying managed pods using Replication and other controllers. It discusses running pods that perform a single completable task, which is different from continuous tasks like DaemonSets. The Job resource is introduced as a solution for this type of task, allowing pods to be rescheduled in case of node failure. An example is given of running a container image built on top of the busybox image, invoking the sleep command for two minutes.A Job resource in Kubernetes runs a single completable task. A YAML definition of a Job resource is provided, which defines a pod that will run an image invoking a process for 120 seconds and then exit. The restartPolicy specifies what to do when the processes finish. Jobs are part of the batch API group, version v1, and cannot use the default restart policy. Pods managed by Jobs are rescheduled until they finish successfully.A Job resource in Kubernetes can be used to deploy a single, managed pod with a restart policy of OnFailure or Never. Once created, the Job will run the pod until completion, at which point it will be marked as successful and the pod deleted. Jobs can also be configured to create multiple pods that run in parallel or sequentially by setting the completions and parallelism properties.A Job can be configured to run multiple pods sequentially or in parallel. To run pods sequentially, set completions to the number of times you want the Job\\'s pod to run. For example, setting completions to 5 will create one pod at a time until five pods complete successfully. To run pods in parallel, specify how many pods are allowed to run with the parallelism Job spec property. This allows up to that many pods to be created and running at the same time.You can scale a Job\\'s parallelism property while it\\'s running using kubectl scale command. Additionally, you can limit a pod\\'s time to complete by setting activeDeadlineSeconds in the pod spec. A Job can also be configured to retry failed pods up to 6 times before being marked as failed. Furthermore, Kubernetes supports scheduling Jobs periodically or once in the future through CronJobs, which create a Job resource at the specified time and run it according to the Job template.A CronJob resource creates Job objects based on a specified schedule. The schedule is set using the cron format (minute, hour, day of month, month, day of week), and can be configured to run jobs at specific intervals. The jobTemplate property defines the template for creating Job resources, which are created from the CronJob resource at approximately the scheduled time.A CronJob creates a single Job for each execution configured in the schedule, but can create two Jobs if run concurrently or none at all. To combat this, jobs should be idempotent and next job runs should perform work missed by previous runs. A startingDeadlineSeconds field can also be specified to ensure pods start running within a certain timeframe.ReplicationControllers are being replaced with ReplicaSets and Deployments which provide additional features, DaemonSets ensure every node runs a pod instance, Jobs schedule batch tasks while CronJobs handle future executions.Services enable clients to discover and communicate with pods, allowing them to respond to external requests. This chapter covers creating Service resources to expose a group of pods at a single address, discovering services in the cluster, exposing services to external clients, connecting to external services from inside the cluster, controlling pod readiness for service participation, and troubleshooting services.A Kubernetes Service is a resource that provides a single, constant point of entry to a group of pods providing the same service. Each service has an IP address and port that never change while the service exists, allowing clients to connect without needing to know individual pod locations. This enables external clients to connect to frontend pods without worrying about IP changes and allows frontend pods to connect to backend database services with a stable address.A service enables clients to discover and talk to pods, even if the pod\\'s IP address changes. Services are created using label selectors, which specify which pods belong to the same set. A service can be backed by more than one pod, with connections load-balanced across all backing pods.A Kubernetes service called kubia is created manually by posting a YAML descriptor, which exposes all pods matching the app=kubia label selector on port 80 and routes connections to port 8080 of each pod. The service accepts connections on port 80 and forwards them to port 8080 of one of the matching pods, allowing clients to access the service through a single IP address and port.The chapter explains services in Kubernetes, enabling clients to discover and talk to pods. A service is exposed through an internal cluster IP that\\'s only accessible from inside the cluster. The primary purpose of services is exposing groups of pods to other pods in the cluster. To test a service, one can send requests to it from within the cluster using various methods such as creating a pod, ssh-ing into a node, or executing a command in an existing pod using kubectl exec.When running curl inside a pod using kubectl exec, Kubernetes proxies the connection to a random available pod among those backing the service. The double dash (--), signals the end of command options for kubectl and everything after it is executed within the pod. Without the double dash, the -s option would be interpreted as an option for kubectl, resulting in misleading errors.This chapter discusses services in Kubernetes, enabling clients to discover and talk to pods. Session affinity can be set to either None or ClientIP, redirecting requests from the same client IP to the same pod. Services can also support multiple ports, exposing all ports through a single cluster IP, with each port requiring a specified name.A Kubernetes Service can be defined with named ports in both the pod and service specifications. The label selector applies to the whole service, not individual ports. Ports can be referred to by name or number in the service spec.Services enable clients to discover and talk to pods through a single and stable IP address and port, which remains unchanged throughout its lifetime. Client pods can discover the service\\'s IP and port through environment variables or by manually looking up its IP address.Services in Kubernetes are exposed through environment variables, but can also be discovered using DNS. Each service gets a DNS entry and client pods can access them through their fully qualified domain name (FQDN). This allows for a more flexible way of accessing services without relying on environment variables.Services enable clients to discover and talk to pods. Clients can connect to a service by opening a connection to its FQDN, which includes the service name, namespace, and cluster domain suffix. If in the same namespace as the database pod, the client can refer to the service simply by its name. To access a service inside a pod\\'s container, run bash using kubectl exec command with the -it option, and then use curl to access the service.You can connect to services living outside the cluster by using its name as the hostname in the requested URL. Omitting namespace and svc.cluster.local suffix is also allowed due to how DNS resolver inside each pod\\'s container is configured. However, trying to ping service IP will not work because it\\'s a virtual IP that only has meaning when combined with the service port.Kubernetes services enable clients to discover and talk to pods. The Endpoints resource is a list of IP addresses and ports exposing a service. The pod selector in the service spec is used to build a list of IPs and ports, which are stored in the Endpoints resource. Clients connect to a service, and the service proxy selects one of those IP and port pairs and redirects the incoming connection to the server listening at that location.A Kubernetes service called `external-service` is created without a pod selector, requiring a separate Endpoints resource to be manually created with the same name and containing target IP addresses and ports for the service.Kubernetes services enable clients to discover and talk to pods, and can be exposed externally using an ExternalName service which creates a DNS record pointing to a fully qualified domain name. This allows clients to connect directly to the external service without going through the service proxy, and does not require a cluster IP address. ExternalName services are implemented solely at the DNS level and can be modified by changing the externalName attribute or switching to a ClusterIP service with an Endpoints object.A service can be made accessible externally by setting its type to NodePort, LoadBalancer, or creating an Ingress resource. A NodePort service makes a port on all nodes reserve and forward incoming connections to the pods that are part of the service, allowing access through any node\\'s IP and reserved node port.A Kubernetes Service named kubia-nodeport is created with type NodePort, specifying node port 30123 and exposing internal port 80. The service is accessible through the IP address of any cluster node on port 30123, redirecting incoming connections to a randomly selected pod.To expose services to external clients, configure Google Cloud Platform\\'s firewalls to allow connections on the desired port, e.g., $ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123. This enables access through one of the node\\'s IPs on that port, which can be found in a separate step.Services in Kubernetes allow clients to discover and talk to pods. With NodePort services, pods are accessible through port 30123 on any node. However, this can lead to issues if a node fails. A load balancer can be used to distribute traffic across healthy nodes, and Kubernetes clusters running on cloud providers often support automatic load balancer provisioning. Using JSONPath with kubectl allows for efficient retrieval of node IPs.Creating a Kubernetes Service with a LoadBalancer allows external access through a unique, publicly accessible IP address. The service type is set to LoadBalancer, and ports are specified for external connection. Once created, the load balancer\\'s IP address is listed in the Service object, enabling direct access via curl or other tools.Services in Kubernetes allow clients to discover and talk to pods, using the load balancer to route HTTP requests to a random pod for each connection. Even with session affinity set to None, users will hit the same pod every time due to keep-alive connections from web browsers, whereas tools like curl open new connections each time.Exposing services to external clients can be done through NodePort or LoadBalancer-type services. However, when using a node port, externally originating connections may not always go directly to the pod running on the same node, requiring an additional network hop. This can be prevented by configuring the service\\'s externalTrafficPolicy field to \\'Local\\', but this has its own drawbacks such as uneven distribution of connections across pods.Services in Kubernetes allow clients to discover and communicate with pods, but can\\'t preserve client IP when using node ports due to Source Network Address Translation (SNAT). The Local external traffic policy affects this, but creating an Ingress resource is another way to expose services externally, allowing multiple services to share one public IP address and load balancer, improving load distribution and scalability.Ingresses in Kubernetes operate at the application layer, providing features like cookie-based session affinity. An Ingress controller is required to make Ingress resources work, and different environments use different implementations. To enable the Ingress add-on in Minikube, run $ minikube addons enable ingress, which allows exposing multiple services through a single Ingress.Creating an Ingress resource in Kubernetes enables clients to discover and talk to pods. An example YAML manifest is provided, which defines an Ingress with a single rule sending all HTTP requests from the host kubia.example.com to the kubia-nodeport service on port 80. The Ingress controller pod can be listed using kubectl get po --all-namespaces.Exposing services externally through an Ingress resource requires configuring DNS or /etc/hosts to point to the Ingress controller\\'s IP address. The Ingress controller then selects a pod based on the Host header and forwards the request to it, allowing access to the service at http://kubia.example.com.An Ingress can expose multiple services on the same host by mapping different paths to different services, allowing clients to reach two or more services through a single IP address. This is achieved by specifying multiple paths in the Ingress spec and mapping each path to a specific service, as shown in Listing 5.14. Requests are routed to the corresponding service based on the path in the requested URL.An Ingress resource can map different services to different hosts based on the Host header in the request, and can also handle TLS traffic by attaching a certificate and private key to the Ingress as a Secret. This allows for secure communication between clients and the controller without requiring the application pod to support TLS.Services allow clients to discover and communicate with pods. A Secret was created using two files, and an Ingress object was updated to accept HTTPS requests for kubia.example.com. Alternatively, \\'kubectl apply\\' can be used to update the Ingress resource. CertificateSigningRequest resources enable certificates to be signed by a human operator or automated process, retrieving a signed certificate from the CSR\\'s status field.Kubernetes allows you to define a readiness probe for your pod, which periodically determines whether the pod should receive client requests or not. When a container\\'s readiness probe returns success, it signals that the container is ready to accept requests, allowing traffic to be directed to it only when it\\'s fully ready to serve.Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths. Readiness probes check if a container is ready to serve requests, with three types: Exec, HTTP GET, and TCP Socket. If a pod fails the readiness check, it\\'s removed from the service until it becomes ready again. This ensures only healthy containers receive requests, distinguishing from liveness probes which keep pods running.Readiness probes ensure clients only talk to healthy pods by signaling when a pod is ready to accept connections. A readiness probe can be added to a pod by modifying the ReplicationController\\'s pod template using kubectl edit, adding the probe definition under spec.template.spec.containers. The probe periodically checks if a file exists, and if it does, the pod is considered ready.Services: enabling clients to discover and talk to pods. ReplicationController\\'s pod template changes have no effect on existing pods. Existing pods report not being ready until they\\'re re-created by the Replication-Controller, which will fail the readiness check unless a /var/ready file is created in each of them.A readiness probe in Kubernetes determines if a pod is ready to accept connections. In real-world scenarios, it should return success or failure depending on whether the app can receive client requests. If no readiness probe is defined, pods become service endpoints immediately and clients may experience connection errors when the app takes too long to start listening for incoming connections.Kubernetes allows clients to discover pod IPs through DNS lookups, enabling connection to all pods or individual pods using a headless service with clusterIP set to None. This method is ideal for Kubernetes-agnostic apps, providing a stable IP address for clients to connect to all backing pods.A headless service is used to discover individual pods based on a pod selector. The service will list only ready pods as endpoints. To confirm readiness, create the /var/ready file in each pod. A DNS lookup can be performed from inside a pod using the tutum/dnsutils container image or by running a new pod without writing a YAML manifest using kubectl run with the --generator=run-pod/v1 option. This allows understanding of how DNS A records are returned for a headless service, which returns IPs of all ready pods.Headless services allow clients to connect directly to pods by DNS name. Kubernetes provides load balancing across pods through DNS round-robin mechanism instead of service proxy. To discover all pods, including unready ones, add annotation \\'service.alpha.kubernetes.io/tolerate-unready-endpoints: true\\' or use the publishNotReadyAddresses field in service spec.Make sure to access Kubernetes service from within the cluster and not outside. Check if readiness probe is succeeding, examine Endpoints object, and try accessing service using its cluster IP or FQDN. Ensure correct port is exposed and target port is not used. Connect directly to pod IP to confirm connections are being accepted. If still issues persist, check if app is binding only to localhost.Services enable clients to discover and talk to pods using a pod\\'s readiness probe, enabling discovery of pod IPs through DNS for headless services. Additionally, troubleshooting and modifying firewall rules in Google Kubernetes/Compute Engine, executing commands in pod containers, running bash shells in existing pods, and modifying resources with kubectl apply can be performed.This chapter explores how containers in a pod can access external disk storage and share storage between them. Containers have isolated file systems, but volumes allow sharing disk space. Topics include creating multi-container pods, using Git repositories inside pods, attaching persistent storage, and dynamic provisioning of persistent storage.Kubernetes provides storage volumes that allow new containers to continue where the last one finished, preserving directories with actual data across container restarts. Volumes are defined in a pod\\'s specification and must be mounted in each container that needs to access it, allowing multiple containers to share disk storage and enabling them to work together effectively.The document introduces the concept of volumes in Kubernetes, where multiple containers within a pod can share storage without relying on shared filesystems. Three containers are used as examples: WebServer, ContentAgent, and LogRotator, each with its own filesystem but sharing two volumes, publicHtml and logVol, mounted at different paths to illustrate this concept.Volumes in Kubernetes allow attaching disk storage to containers, enabling them to operate on the same files. A volume is bound to a pod\\'s lifecycle and can be mounted at arbitrary locations within the file tree. Various types of volumes are available, including emptyDir, hostPath, gitRepo, nfs, gcePersistentDisk, awsElasticBlockStore, and azureDisk, each with its own purpose and use case. To access a volume from within a container, a VolumeMount must be defined in the container\\'s spec.Volumes in Kubernetes can be used to share data between containers or for exposing Kubernetes resources and cluster information. Special types of volumes like secret, downwardAPI, and configMap are used to expose metadata to apps running in a pod. A single pod can use multiple volumes of different types at the same time, with each container having the option to mount or not. An emptyDir volume is useful for sharing files between containers or for temporary data storage by a single container.To create a pod that uses a shared volume, you need to build a Docker image with the required binary (fortune) and script (fortuneloop.sh). The image is based on ubuntu:latest, installs fortune, adds the script to /bin folder, and sets it as the ENTRYPOINT. You then create a pod manifest (fortune-pod.yaml) that specifies two containers sharing the same volume. Finally, you can run the pod using kubectl apply -f fortune-pod.yamlA pod contains two containers and a shared volume between them. The html-generator container writes to the volume every 10 seconds, while the web-server container serves files from it. By forwarding port 80 on the local machine to the pod\\'s port, users can access the Nginx server through localhost:8080 and receive a different fortune message with each request.An emptyDir volume can be created on tmpfs filesystem for better performance, while a gitRepo volume clones and checks out a Git repository at pod startup. The files in a gitRepo volume are not kept in sync with the referenced repo, but are updated when a new pod is created. This type of volume is useful for storing static HTML files or serving the latest version of a website.Using volumes in Kubernetes, specifically gitRepo volumes, allows sharing data between containers. This is demonstrated by running a web server pod serving files from a cloned Git repository, where the pod is created with a single Nginx container and a single gitRepo volume that clones the repository into the root directory of the volume.To keep files in sync with a Git repository, you can create a sidecar container that runs a Git sync process. This process can be run in an existing container image from Docker Hub, such as \\'git sync\\'. The sidecar container should mount the gitRepo volume and configure the Git sync process to keep the files in sync with the Git repo. This method is recommended instead of using a gitRepo volume for private Git repositories, which are not supported by Kubernetes.A gitRepo volume is created for and used exclusively by a pod, but its contents can survive multiple pod instantiations if the volume type is different. hostPath volumes allow pods to access files on the node\\'s filesystem, making it possible for system-level pods to read or use the node\\'s devices through the filesystem.HostPath volumes are not suitable for storing a database\\'s data directory as they store contents on a specific node\\'s filesystem, making it sensitive to scheduling. Instead, use them to access the node\\'s log files, kubeconfig, or CA certificates. System-wide pods like fluentd-kubia use hostPath volumes to access node\\'s data.To persist data across pods, a network-attached storage (NAS) is needed. A GCE Persistent Disk can be used as underlying storage mechanism on Google Kubernetes Engine. The disk must be created in the same zone as the Kubernetes cluster and its size should be at least 200GB for optimal I/O performance.This chapter explains how to attach disk storage to containers using Kubernetes volumes. It provides an example of creating a 1 GiB GCE persistent disk called \\'mongodb\\' and configuring a pod to use it as a volume, mounting it at \\'/data/db\\'. The YAML for the pod is provided, specifying the gcePersistentDisk type, fsType as ext4, and mountPath as /data/db. A note is also given for using Minikube, where you can\\'t use a GCE Persistent Disk, but instead deploy mongodb-pod-hostpath.yaml using a hostPath volume.To use persistent storage, write data to the MongoDB database by running the MongoDB shell inside the container and inserting JSON documents. The data will be stored on a GCE persistent disk. After deleting and re-creating the pod, the new pod can read the persisted data from the previous pod, using the same GCE persistent disk.You can attach disk storage to containers using Kubernetes volumes, such as GCE Persistent Disk, awsElasticBlockStore, azureFile, or azureDisk. These volumes provide persistent storage for pods, allowing data to be retained across pod instances. To use a different volume type, create the underlying storage and set properties in the volume definition.Kubernetes supports various storage technologies, including NFS, ISCSI, GlusterFS, and others. However, it\\'s recommended to use volumes in a way that decouples pod definitions from specific clusters, avoiding infrastructure-related details in pod specifications.Kubernetes aims to hide infrastructure from developers, allowing them to request persistent storage without knowing specific details. Cluster admins configure the cluster to provide what apps request, using PersistentVolumes and PersistentVolumeClaims to decouple pods from underlying storage technology.A cluster administrator creates a PersistentVolume resource through the Kubernetes API server, specifying its size and access modes. A user then creates a PersistentVolumeClaim manifest, specifying their required size and access mode, which is bound to an existing PersistentVolume. The volume can be used in a pod, but other users cannot use it until the claim is released.A PersistentVolume is created by specifying its capacity, access modes, and storage type. The administrator can then claim the PV with a PersistentVolumeClaim, which allows a container to read from or write to it. A PV is cluster-level resource like nodes and doesn\\'t belong to any namespace. It\\'s created with kubectl create command and shown as Available until claimed.To use a PersistentVolume in a Kubernetes pod that requires persistent storage, you need to create a PersistentVolumeClaim (PVC) first. This is done by preparing a PVC manifest and posting it to the Kubernetes API through kubectl create. The PVC claims the PersistentVolume for exclusive use within a namespace, allowing the same PVC to stay available even if the pod is rescheduled.A Kubernetes PersistentVolumeClaim is created with a requested 1Gi of storage and ReadWriteOnce access mode. The claim is bound to a matching PersistentVolume, which is shown as Bound in kubectl get pvc and pv commands. The PersistentVolume\\'s capacity and access modes match the claim\\'s requirements.To use a PersistentVolume in a pod, reference the PersistentVolumeClaim by name inside the pod\\'s volume. A Pod can claim and use the same PersistentVolume until it is released, allowing decoupling from underlying storage technology.The chapter discusses the benefits of using PersistentVolumes (PVs) and claims to attach disk storage to containers, making it simpler for application developers by abstracting away infrastructure-specific details. A pod can use a GCE Persistent Disk either directly or through a PV and claim, allowing for greater flexibility and portability across different Kubernetes clusters.When a PersistentVolumeClaim is deleted, its status becomes Pending and it\\'s no longer bound to a PersistentVolume, which can be reused by other pods after being manually recycled or reclaimed automatically using Retain, Recycle, or Delete policies, allowing the reuse of volumes across different namespaces.A PersistentVolume only supports Retain or Delete policies. The reclaim policy can be changed on an existing PersistentVolume. Kubernetes also performs dynamic provisioning of PersistentVolumes through persistent-volume provisioners and StorageClass objects, allowing users to choose the type of PersistentVolume they want.Dynamic provisioning of PersistentVolumes allows administrators to define one or two StorageClasses, enabling the system to create new PersistentVolumes each time a PersistentVolumeClaim is requested. This eliminates the possibility of running out of PersistentVolumes. The StorageClass resource specifies the provisioner and parameters for provisioning, which can be specific to cloud providers like GCE. Users can refer to the storage class by name in their PersistentVolumeClaims, enabling dynamic provisioning of PersistentVolumes.A PersistentVolumeClaim (PVC) can specify a custom storage class, such as \\'fast\\', which is referenced by a provisioner to create a PersistentVolume. The provisioner is used even if an existing manually provisioned PV matches the PVC. If the storage class does not exist, provisioning will fail. The dynamically created PV has the requested capacity and access modes, with a reclaim policy of Delete, meaning it will be deleted when the PVC is deleted.Dynamic provisioning of PersistentVolumes allows cluster admins to create multiple storage classes with different performance characteristics. Developers can then choose which one is most appropriate for each claim they create. This makes PVC definitions portable across different clusters as long as StorageClass names are the same, demonstrating flexibility and consistency in Kubernetes environments.The default storage class in a GKE cluster is defined by an annotation, which makes it the default storage class. A PersistentVolumeClaim can be created without specifying a storage class and a GCE Persistent Disk of type pd-standard will be provisioned for you.Dynamic provisioning of PersistentVolumes uses the default storage class when creating a PVC. To bind a PVC to a manually pre-provisioned PV, explicitly set storageClassName to an empty string. This prevents the dynamic provisioner from provisioning a new PV and allows the PVC to use the existing one.This chapter explains how volumes provide temporary or persistent storage to containers in a pod. Key concepts include creating multi-container pods with shared files using volumes, mounting external storage for persistence across restarts, and dynamically provisioning PersistentVolumes through PersistentVolumeClaims and StorageClasses.ConfigMaps and Secrets allow passing configuration data to Kubernetes applications, configuring containerized applications by changing the main process, passing command-line options, setting environment variables, or using ConfigMaps for non-sensitive settings and Secrets for sensitive info like credentials.ConfigMaps and Secrets allow storing configuration data and sensitive information separately from container images. ConfigMaps store config data as a top-level Kubernetes resource, while Secrets handle sensitive info like credentials or encryption keys with special care. This allows for easier management of config changes and keeping sensitive data secure.You can pass command-line arguments to Docker containers by specifying them in the docker run command, overriding any default arguments set in the image\\'s Dockerfile. This is achieved through the ENTRYPOINT instruction, which defines the executable, and CMD, which specifies the default arguments. The ENTRYPOINT instruction supports two forms: shell and exec, where shell form invokes the command inside a shell and exec form runs it directly.This chapter discusses ConfigMaps and Secrets in Kubernetes, specifically how to make an interval configurable in a Docker image using the exec form of the ENTRYPOINT instruction and setting a default value with the CMD instruction. A script is modified to accept an INTERVAL variable from the command line, and the Dockerfile is updated to use this new script and set the default interval to 10 seconds.Kubernetes allows overriding command and arguments in a container by setting \\'command\\' and \\'args\\' fields in the container specification. This can be done when creating a pod, but not updated after it\\'s created. The equivalent Dockerfile instructions are ENTRYPOINT and CMD.The chapter discusses ConfigMaps and Secrets for configuring applications in Kubernetes. It explains how to pass configuration options through command-line arguments using the args array, and also through environment variables. The example uses the fortune:args image to generate a new fortune every two seconds.To make the interval in your fortuneloop.sh script configurable through an environment variable, remove the row where the INTERVAL variable is initialized. This allows the script to be configured from an environment variable, and can be used with Docker containers. To specify environment variables in a container definition, set them inside the container definition, not at the pod level. This can be done using YAML files like fortune-pod-env.yaml.You can reference previously defined environment variables by using the $(VAR) syntax, decoupling configuration from pod descriptors using ConfigMaps, and passing values as environment variables or files in a volume.ConfigMaps allow you to keep configuration separate from your app, making it easy to switch between environments by using different config values in each environment without changing the pod specification.A ConfigMap is used to store and manage configuration data for applications. It can be created using the kubectl create configmap command, which allows defining entries by passing literals or creating from files. A ConfigMap named fortune-config was created with a single entry sleep-interval=25, and its YAML descriptor was inspected using kubectl get command.ConfigMaps in Kubernetes can store configuration data, including complete config files, which can be created using the `kubectl create configmap` command. Files can be added individually or from a directory, and keys can be specified manually. ConfigMaps can also combine different options, such as literal values, files, and directories, to create a single map entry.You can pass ConfigMap entries to a container as environment variables using the valueFrom field in the pod descriptor. The pod descriptor should have an apiVersion of v1 and kind of Pod, with a ConfigMap named my-config. You can specify the key-value pairs from the ConfigMap as environment variables using --from-file or --from-literal flags, allowing you to pass JSON data and literal values to the container.This section shows how to decouple configuration from pod specification using a ConfigMap. A ConfigMap is referenced in the pod definition, allowing configuration options to be kept together and avoiding duplication across multiple pod manifests. If a referenced ConfigMap doesn\\'t exist, the container referencing it will fail to start, but other containers will start normally. Optional references can also be marked, enabling containers to start even if the ConfigMap doesn\\'t exist.Kubernetes version 1.6 allows exposing all entries of a ConfigMap as environment variables using the envFrom attribute, instead of individual env variables. This can be done by specifying a prefix for the environment variables, which will result in environment variables with the same name as the keys. However, if a ConfigMap key is not in the proper format (e.g., contains a dash), it will skip the entry and record an event. Additionally, ConfigMap entries cannot be referenced directly in the pod.spec.containers.args field, but can be passed as command-line arguments by first initializing an environment variable from the ConfigMap entry.A ConfigMap can be used to expose entries as files or to pass configuration options as environment variables. A special volume type, configMap volume, can expose each entry of a ConfigMap as a file, allowing the container process to obtain the value by reading the contents of the file. This approach is suitable for exposing whole config files contained in a ConfigMap.A ConfigMap is created to pass a config file to an Nginx web server running inside a pod\\'s web-server container, enabling gzip compression for plain text and XML files. A new directory is created on the local disk with two files: my-nginx-config.conf containing the Nginx config and sleep-interval with the number 25. The ConfigMap is then created from these files using kubectl.A ConfigMap is used to decouple configuration with Kubernetes, allowing for easy management and updates of configurations. It can contain multiple entries, each with its own key-value pair, and can be referenced in a pod\\'s container using a volume populated with the ConfigMap\\'s contents. This allows for the use of default configuration files while still adding custom configurations, as shown in Listing 7.14.This chapter discusses ConfigMaps and Secrets for configuring applications in Kubernetes. A Pod is defined with a volume referencing a fortune-config ConfigMap, which is mounted into the /etc/nginx/conf.d directory to make Nginx use it. The configuration can be verified by port-forwarding and checking the server\\'s response with curl, demonstrating that the mounted ConfigMap entries are being used by the web server.A ConfigMap can be decoupled from a pod\\'s configuration using a ConfigMap volume. This allows certain entries to be exposed as files in a directory, while others remain hidden. The `items` attribute of the volume can be used to specify which ConfigMap entries should be included as files, and the `key` and `path` fields define how the entry is stored. This approach enables fine-grained control over the configuration of containers within a pod.When mounting a ConfigMap or Secret as a directory, it hides existing files in that directory. To add individual files without hiding others, use the `subPath` property on the volumeMount to mount either a single file or directory from the volume, preserving the original files.A ConfigMap can be used to decouple configuration from an application, allowing for easy updates without restarting the app. A subPath can be used to mount individual files from a volume instead of the whole volume, but this method has limitations with file updating. File permissions in a ConfigMap volume default to 644 and can be changed using the defaultMode property. Updating a ConfigMap updates all referencing volumes, allowing for configuration changes without app restarts.ConfigMaps and Secrets can be edited using kubectl edit, which updates the files exposed in the configMap volume atomically using symbolic links. Changes to the ConfigMap are reflected in the actual file, but Nginx doesn\\'t reload its config automatically. To signal Nginx to reload its config, use \\'nginx -s reload\\' command within a pod. This allows changing an app\\'s config without restarting the container or recreating the pod.Kubernetes uses symbolic links to update ConfigMap volumes when a new directory is created. However, updating individual files in an existing directory does not trigger an update. Modifying an existing ConfigMap while pods are using it may not be ideal if the app doesn\\'t reload its config automatically, as different running instances will have different configs.Kubernetes provides Secrets to store and distribute sensitive information, which can be used like ConfigMaps. They are stored in memory on nodes and encrypted in etcd from v1.7. Choose between Secret and ConfigMap based on sensitivity: use ConfigMap for non-sensitive data and Secret for sensitive data. A default token Secret is automatically mounted into every container, accessible with kubectl get secrets.Secrets are used to pass sensitive data to containers, and contain entries like ca.crt, namespace, and token which provide secure access to the Kubernetes API server from within pods. The default-token Secret is mounted into every container by default, but can be disabled in each pod or service account.Creating a Secret involves generating certificate and private key files, then using kubectl to create a generic Secret called fortune-https from these files and an additional dummy file containing the string bar. This process is similar to creating ConfigMaps, but with the added security of keeping sensitive information like private keys secure within the Secret.Secrets in Kubernetes can hold sensitive or non-sensitive binary data, which are encoded as Base64 strings. This contrasts with ConfigMaps that store plain-text data. Secrets have a maximum size limit of 1MB and can be used even for non-sensitive binary data.Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved. Secrets are decoded and written to files or environment variables in their actual form, allowing apps to read them directly without decoding.This document explains how to pass sensitive data (SSL certificates) to a container using Kubernetes secrets. It shows an example of mounting a secret volume in a pod, specifically for a fortune-https pod that uses Nginx and mounts the certificate and key files from a secret volume at /etc/nginx/certs.A Kubernetes pod is configured using a ConfigMap and a Secret, with the ConfigMap providing application configuration and the Secret providing sensitive data such as SSL certificates. The ConfigMap and Secret are referenced in the pod descriptor through their respective names, with file permissions specified for the Secret files.Using Kubernetes secrets to pass sensitive data to containers, a pod\\'s HTTPS traffic can be tested by opening a port-forward tunnel and using curl. The server\\'s certificate can also be checked with curl. Secrets are stored in memory (tmpfs) and do not write to disk, making them secure. Alternatively, secret entries can be exposed as environment variables.ConfigMaps and Secrets in Kubernetes allow applications to access configuration data and secrets. However, exposing secrets through environment variables is not recommended due to security risks. Instead, use secret volumes or pass credentials to Kubernetes itself using image pull secrets for private container registries.To pass sensitive data to containers, create a Secret holding Docker registry credentials using kubectl create secret docker-registry command. Specify the Secret\\'s name in the pod spec as an imagePullSecrets. This enables pulling images from a private image registry. Alternatively, add Secrets to a ServiceAccount to automatically include them in all pods.This chapter summarizes how to pass configuration data to containers using ConfigMaps and Secrets, including overriding commands, passing arguments, setting environment variables, decoupling config from pods, storing sensitive data in Secrets, and creating a docker-registry Secret.This chapter explores how applications can access pod metadata, resources, and interact with the Kubernetes API server. It covers using the Downward API to pass information into containers, exploring the Kubernetes REST API, accessing the API server from within a container, and understanding the ambassador container pattern.The Kubernetes Downward API allows passing metadata about a pod and its environment through environment variables or files, solving problems of repeating information in multiple places, and exposing pod metadata to processes running inside the pod, with currently available metadata being the pod\\'s name, IP address, and labels/annotations.The Downward API allows passing metadata such as namespace, node name, service account, CPU and memory requests/limits, labels, and annotations to containers through environment variables or a volume. This can be useful for providing containerized processes with information about their environment. An example is provided in the form of a simple single-container pod manifest that passes the pod\\'s and container\\'s metadata to the container using environment variables.The process can access environment variables defined in the pod spec. Environment variables expose pod metadata such as name, IP, namespace, and node name. Additionally, variables are created for CPU requests and memory limits with a divisor to convert values into desired units.The Downward API allows passing metadata from a pod\\'s or container\\'s attributes as environment variables to the running application. This is demonstrated by creating a pod with specified CPU and memory limits, then using kubectl exec to show the resulting environment variables in the container. The divisor for CPU limits can be 1 (one whole core) or 1m (one millicore), while memory limits can use units such as 1K or 1Mi.Pods can expose metadata through environment variables or files in a downwardAPI volume. Environment variables can only pass single-value metadata, while a downwardAPI volume allows exposing labels and annotations. A Pod\\'s name and namespace can be exposed through a downwardAPI volume by specifying the fieldRef path to the metadata.name and metadata.namespace fields.The Downward API allows passing metadata through a volume, mounting it in the container under /etc/downward. Each item specifies the path where metadata should be written and references either a pod-level field or a container resource field.This chapter discusses accessing pod metadata and other resources from applications using Kubernetes. It explains how to mount a downwardAPI volume, which makes available various metadata fields such as labels, annotations, and container resource requests as files within the pod\\'s filesystem. The contents of these files can be accessed using kubectl exec commands. Additionally, it highlights that labels and annotations can be modified while a pod is running, and Kubernetes updates the corresponding files in the downwardAPI volume.When exposing container-level metadata using the Downward API, you need to specify the name of the container whose resource field you\\'re referencing. This is because volumes are defined at the pod level, not at the container level. Using the Downward API allows you to keep your application Kubernetes-agnostic by passing data from the pod and containers to the process running inside them. However, it only exposes a limited subset of metadata, so if your app needs more information about other pods or resources in the cluster, you\\'ll need to obtain that directly from the Kubernetes API server.The Kubernetes API server provides REST endpoints for accessing pod metadata and other resources, but requires authentication. To access it directly, use kubectl proxy to run a proxy server that handles authentication and verifies the server\\'s certificate on each request, allowing apps within pods to talk to the API server and get information about other resources or the most up-to-date data possible.To explore the Kubernetes API through kubectl proxy, run $ kubectl proxy to start serving on 127.0.0.1:8001, then use curl or a web browser to navigate to http://localhost:8001 to list available API paths and resource types.The Kubernetes batch API group has two versions (v1 and v2alpha1), with v1 being the preferred version. The /apis/batch path displays the available versions, while /apis/batch/v1 shows a list of resource types in this group, including jobs which are namespaced.The Kubernetes API server returns a list of resource types and REST endpoints in the batch/v1 API group. The Job resource is exposed with verbs to retrieve, update, delete, create, watch, patch and get. Additional API endpoints are also available for modifying job status.Accessing pod metadata and other resources from applications involves using Kubernetes REST API. You can retrieve information about pods, jobs, and namespaces using curl commands. To talk to the API server from within a pod, you need to find its location, authenticate with it, and ensure you\\'re not talking to an impersonator. This allows applications running in pods to interact with Kubernetes services.To communicate with the Kubernetes API server, create a pod using the tutum/curl image and run a shell inside it. Find the API server\\'s address by looking up environment variables KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT within the container.To access pod metadata and other resources from applications, use the environment variables to get the service\\'s port number. However, always verify the server\\'s identity by checking its certificate. Use curl with the --cacert option to specify the CA certificate, which is stored in a Secret called default-token-xyz. This verifies that the server\\'s certificate is signed by the CA and prevents man-in-the-middle attacks.To access the Kubernetes API server, set the CURL_CA_BUNDLE environment variable to trust its certificate. Then, authenticate with the server by loading an authentication token into an environment variable using TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token). Finally, send requests to the API server using curl -H \"Authorization: Bearer $TOKEN\" https://kubernetesYou can access pod metadata and other resources from applications by passing a token inside the Authorization HTTP header. You can also retrieve the namespace of your pod by reading the contents of the /var/run/secrets/kubernetes.io/serviceaccount/namespace file into the NS environment variable. With this information, you can list all pods running in the same namespace as your pod using a GET request to https://kubernetes/api/v1/namespaces/$NS/pods. Additionally, you can disable role-based access control (RBAC) by creating a clusterrolebinding with cluster-admin privileges for service accounts.An app running in a pod can access the Kubernetes API by verifying the API server\\'s certificate, authenticating with a bearer token, and using a namespace file to pass the namespace for CRUD operations. This process can be simplified using an ambassador container, which makes communication with the API server more straightforward while keeping it secure.The chapter discusses how to access pod metadata and resources from applications using the kubectl proxy command, which can be run inside pods as an ambassador container pattern. This allows applications to query the API server securely through the ambassador without direct communication with the API server.This page discusses connecting to a Kubernetes API server using an ambassador container. The ambassador container runs `kubectl proxy` and handles authentication with the API server, allowing the main container to send plain HTTP requests to localhost:8001. This simplifies communication between containers and can be reused across different apps, but adds an additional process consuming resources.Kubernetes API client libraries are available for various programming languages, including Golang, Python, Java, Node.js, and PHP, allowing applications to access pod metadata and other resources from the API server.The page discusses interacting with the Kubernetes API server using various client libraries such as Ruby, Clojure, Scala, Perl, and Java (Fabric8 client). It provides examples of how to list services, create, edit, and delete pods in a Java app using the Fabric8 client, highlighting the simplicity and efficiency of these interactions.The chapter discusses accessing pod metadata and other resources from applications using the Fabric8 client\\'s fluent DSL API. If no client is available, you can use Swagger to generate a client library and documentation. The Kubernetes API server exposes Swagger definitions at /swaggerapi and OpenAPI spec at /swagger.json. You can explore the API with Swagger UI, which provides a web interface for interacting with REST APIs.You now know how your app running inside a pod can get data about itself, other pods, and components deployed in the cluster through environment variables or downwardAPI volumes. You\\'ve learned to access CPU and memory requests, browse the Kubernetes REST API, find the API server\\'s location, authenticate yourself, and use client libraries to interact with Kubernetes.This chapter covers updating apps running in a Kubernetes cluster, focusing on using Deployments to perform zero-downtime updates. Key topics include replacing pods with newer versions, updating managed pods, and performing rolling updates, as well as automatically blocking rollouts of bad versions and controlling the rollout rate.Updating applications running in pods involves replacing old pods with new ones, either by deleting existing pods first and then starting the new ones or by adding new pods while gradually removing old ones, requiring app to handle two versions simultaneously.In Kubernetes, you can update applications declaratively using Deployments, which can be updated automatically or manually. The manual method involves updating the pod template of a ReplicationController to refer to a new image version, then deleting the old pods. Alternatively, you can spin up new pods and delete the old ones without downtime, if your app supports running multiple versions at once.You can update applications running in pods by combining replication controllers and services, switching from the old to the new version at once using a blue-green deployment or performing a rolling update where you replace pods step by step. This requires updating the service\\'s pod selector and scaling down the previous replication controller while scaling up the new one.You can perform an automatic rolling update with a ReplicationController by having kubectl do it, but this is now an outdated way of updating apps. The process involves running the initial version of the app, creating a modified version that returns its version number in the response, and using two ReplicationControllers to roll out the new version.A single YAML file is used to create both a ReplicationController and a LoadBalancer Service, enabling external access to the app. The YAML defines 3 replicas of a pod running the luksa/kubia:v1 image, and exposes it on port 80 with targetPort 8080. After posting the YAML to Kubernetes, the three v1 pods and load balancer run, allowing curl requests to be made to the external IP.This chapter discusses deploying and updating applications declaratively using Kubernetes Deployments. It explains how to perform a rolling update with kubectl by creating a new version of an app without disrupting existing traffic. The importance of setting the container\\'s imagePullPolicy property to Always when pushing updates to the same image tag is also highlighted, especially when using tags other than latest. This ensures that all nodes run the updated image.To perform an automatic rolling update with a ReplicationController in Kubernetes, run the kubectl rolling-update command and specify the old RC, new RC name, and new image. A new RC will be created immediately, referencing the new image and initially having a desired replica count of 0. The system will then scale up the new RC while scaling down the old one, keeping the total number of pods at 3.Before performing a rolling update, kubectl modifies the ReplicationController\\'s selector and adds an additional deployment label to its pod template, ensuring that only pods managed by the new controller are selected. The old controller is also modified with a new selector, allowing it to see zero matching pods, but the live pods\\' labels have been updated to include the new deployment label, preventing them from being seen by the old controller.Performing a rolling update with a ReplicationController using kubectl involves scaling up a new controller while scaling down an old one, replacing old pods with new ones. This process deletes v1 pods and replaces them with v2 pods, eventually directing all requests to the new version. The Service redirects requests to both old and new pods during the rolling update, progressively increasing the percentage of requests hitting v2 pods.Kubernetes\\' ReplicationController can be updated declaratively using `kubectl` commands, allowing for zero-downtime updates. However, the deprecated `kubectl rolling-update` command modifies existing objects and is not recommended. Instead, use explicit `kubectl` commands to scale and update resources, which provides greater control and avoids unexpected modifications.Deployments in Kubernetes provide a declarative way to update applications by introducing a ReplicaSet that creates and manages pods, providing a more scalable and efficient way of updating applications compared to using ReplicationControllers or ReplicaSets directly.A Deployment resource in Kubernetes is used to update applications declaratively by defining the desired state and letting Kubernetes handle the rest. A Deployment can have multiple pod versions running under its wing, so its name shouldn\\'t reference the app version. Creating a Deployment requires specifying a deployment strategy and only three trivial changes are needed to modify a ReplicationController YAML file to describe a Deployment.Creating a Deployment in Kubernetes involves deleting any existing ReplicationControllers and pods, then running `kubectl create -f kubia-deployment-v1.yaml --record` to create a new Deployment. The `--record` option records the command in the revision history. To check the status of the Deployment rollout, use `kubectl rollout status deployment kubia`. A Deployment creates ReplicaSets, which then create pods with unique names that include a numeric value corresponding to the hashed value of the pod template and ReplicaSet managing them.Deployments provide an easy way to update applications declaratively, using a template that can always use the same ReplicaSet for a given version of the pod template. Updating a Deployment only requires modifying its pod template and Kubernetes takes care of replacing all original pods with new ones, achieving the desired state through a configured deployment strategy, either rolling update or recreate.The RollingUpdate strategy in Kubernetes allows for updating applications declaratively by removing old pods one by one and adding new ones at the same time, keeping the application available throughout the process. To slow down the update process, set the minReadySeconds attribute on the Deployment. Triggering the actual rollout is done by changing the image used in the single pod container to a new version using the kubectl set image command.Kubernetes deployments can be updated using various methods such as kubectl edit, patch, apply, replace and set image. These methods change the Deployment\\'s specification, triggering a rollout process. The deployment can also be modified by updating its pod template or container image. Examples of these methods are provided in the text.Using Deployments allows for declarative updates to apps by changing the pod template in a single field, which is then performed by Kubernetes controllers. This process is simpler than running special commands with kubectl. Note that modifying ConfigMaps will not trigger an update unless referencing a new ConfigMap.This chapter discusses Deployments in Kubernetes, which allows for declarative updates to applications. A Deployment resource manages ReplicaSets, making it easier to manage compared to ReplicationControllers. The chapter simulates a problem during a rollout process by introducing a bug in version 3 of an app that returns a 500 error after the fifth request.To update an app declaratively, change the image in the Deployment specification using $ kubectl set image deployment kubia nodejs=luksa/kubia:v3. Follow rollout progress with kubectl rollout status and roll back to previous revision with kubectl rollout undo deployment kubia.Deployments in Kubernetes keep a revision history of rollouts, which can be displayed with kubectl rollout history command. This history allows rolling back to any revision by specifying the revision number in the undo command. The length of the revision history is limited by the revisionHistoryLimit property and older ReplicaSets are deleted automatically.Deployments in Kubernetes allow for declarative updates to apps. The rollout process can be controlled using the `maxSurge` and `maxUnavailable` properties of the rolling update strategy. These properties determine how many pod instances are allowed above the desired replica count (maxSurge) and how many can be unavailable during the update (maxUnavailable). Both default to 25% but can also be specified as absolute values.Deployments can be updated declaratively using `maxSurge` and `maxUnavailable` properties, which control the number of unavailable pods during a rollout. The `extensions/v1beta1` version sets both to 1 instead of 25%, affecting the rollout process as shown in figures 9.12 and 9.13.A Deployment in Kubernetes allows for declarative updates to an application, ensuring that a minimum number of replicas are always available during the rollout process. The `maxUnavailable` property is relative to the desired replica count, meaning that at least one pod must be available when updating from three replicas. Additionally, Deployments can be paused during the rollout process, enabling the creation of a single new pod alongside existing ones for verification before proceeding with the full update.Deployments in Kubernetes can be paused or resumed using the `rollout pause` and `rollout resume` commands, allowing for a controlled rollout process. The `minReadySeconds` property can also be used to block rollouts of malfunctioning versions by specifying how long a new pod must be ready before being treated as available.Deployments can be used for updating apps declaratively, with a properly configured readiness probe and minReadySeconds setting, Kubernetes can prevent deploying buggy versions. A Deployment is updated using kubectl apply command with YAML that includes apiVersion, kind, metadata, spec, replicas, minReadySeconds, strategy, rollingUpdate, maxSurge, maxUnavailable, type, template, metadata, name, labels, app, and containers with image set to luksa/kubia:v3.To update a Deployment with kubectl apply, use the command $ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml. This updates the Deployment with everything defined in the YAML file, including the image and readiness probe definition. To keep the desired replica count unchanged, don\\'t include the replicas field in the YAML. The rollout status command can be used to follow the update process, which will create new pods but doesn\\'t guarantee that they\\'ll be used.A deployment is prevented from updating an app declaratively due to a failing readiness probe in the new pod, which returns HTTP status code 500 and gets removed as an endpoint from the service, until it becomes available for at least 10 seconds.Deployments allow updating applications declaratively, and the rollout process can be configured with deadlines and minReadySeconds. If a rollout fails, it can be aborted using `kubectl rollout undo` command. The `progressDeadlineSeconds` property in Deployment spec is configurable to set a deadline for the rollout.This chapter taught you a declarative approach to deploying and updating applications in Kubernetes, including rolling updates, Deployments, and controlling rollout rates using maxSurge and maxUnavailable properties.This chapter focuses on deploying stateful clustered applications using StatefulSets, providing separate storage for each replicated pod instance, guaranteeing stable names and hostnames for pod replicas, controlling start/stop sequences, and peer discovery via DNS SRV records.Stateful pods can\\'t be replicated using ReplicaSets as they require separate storage for each instance, which is not possible with a single ReplicaSet. Options include creating pods manually or using one ReplicaSet per pod instance, but these are cumbersome and not viable for scaling.StatefulSets allow deploying replicated stateful applications, but using multiple ReplicaSets is not ideal. A workaround is to have a single ReplicaSet with pods using the same PersistentVolume, but each instance selecting and creating its own separate file directory, requiring coordination between instances.Certain apps require a stable network identity, but Kubernetes assigns a new hostname and IP every time a pod is rescheduled. To work around this, create a dedicated service for each individual member, providing a stable network address, similar to creating a ReplicaSet for individual storage. This setup can be seen in Figure 10.4, but it\\'s still not a complete solution as pods can\\'t self-register using the stable IP.StatefulSets in Kubernetes deploy replicated stateful applications with stable names and states, treating instances as non-fungible individuals like pets, requiring replacement with new instances having same name, network identity, and state as the old one when it fails or is replaced.A StatefulSet ensures pods are rescheduled to retain their identity and state, allowing easy scaling. Pods created by StatefulSets have unique volumes and stable identities, with each pod assigned an ordinal index used for naming and hostname. A governing headless Service is required to provide a network identity, enabling addressability by hostname.StatefulSets allow deploying replicated stateful applications, enabling access to pods through fully qualified domain names and DNS lookups. They also ensure replacement of lost pods with new instances having the same name and hostname, unlike ReplicaSets which replace them with unrelated pods.StatefulSets ensure stateful pods have stable identities, even when scaled up or down. Scaling up creates new instances with unused ordinal indexes, while scaling down removes instances with the highest index first, making effects predictable. StatefulSets also prevent scale-down operations if any instance is unhealthy and provide stable dedicated storage to each instance.In Kubernetes, StatefulSets deploy replicated stateful applications by creating separate PersistentVolumeClaims for each pod instance. The StatefulSet stamps out these claims along with the pod instances, allowing for persistent storage to be attached to each pod. Scaling up a StatefulSet creates new API objects, including one or more PersistentVolumeClaims, while scaling down deletes only the pod, leaving the claims intact. Manual deletion of PersistentVolumeClaims is required to release the underlying PersistentVolumes and prevent data loss.StatefulSets in Kubernetes allow for stable identity and storage of pods, guaranteeing that a pod\\'s replacement has the same name, hostname, and persistent storage as the original. This means that if a StatefulSet is scaled down and then scaled back up, the new pod instance will have the same persisted state and be reattached to the same PersistentVolumeClaim, preventing data loss and ensuring consistency in the system.StatefulSets in Kubernetes must ensure two stateful pod instances are never running with the same identity and are bound to the same PersistentVolumeClaim. A StatefulSet must guarantee at-most-one semantics for stateful pod instances, ensuring a pod is no longer running before creating a replacement pod. This affects node failure handling, as demonstrated later in the chapter. A simple clustered data store is built using the kubia app, allowing data storage and retrieval on each pod instance.A simple app is created using Node.js and a Docker image, which writes POST requests to a file and returns stored data on GET requests. To deploy this app through a StatefulSet, PersistentVolumes must be created for storing data files, along with a governing Service required by the StatefulSet, and the StatefulSet itself. For Minikube users, PersistentVolumes can be deployed from a YAML file, while Google Kubernetes Engine users need to create actual GCE Persistent Disks before proceeding.This chapter explains how to deploy replicated stateful applications using StatefulSets. A list of three PersistentVolumes is created from the persistent-volumes-gcepd.yaml file, with each volume having a capacity of 1 Mebibyte and recycling when released. A headless Service called kubia is also created as the governing Service for the StatefulSet, which must be used to provide network identity for stateful pods.A stateless Service is created with a clusterIP field set to None, enabling peer discovery between pods. A StatefulSet manifest is then created with a serviceName and replicas of 2, using a volumeClaimTemplates list to define a Persistent-VolumeClaim for each pod, referencing a persistentVolumeClaim volume in the manifest.StatefulSets in Kubernetes create pods one at a time, ensuring safety for clustered apps sensitive to race conditions. The first pod is brought up fully before continuing to bring up the rest. A closer look at the first pod\\'s spec shows how the StatefulSet constructs the pod from templates and PersistentVolume-Claim template, adding volumes automatically.A StatefulSet was used to create a PersistentVolumeClaim and volume inside a pod. The PersistentVolumeClaims were listed using kubectl get pvc, showing two claims bound to volumes pv-c and pv-a. Communication with individual pods can be done by proxying through the API server or using port-forwarding.The chapter explains how to deploy replicated stateful applications using StatefulSets in Kubernetes. It demonstrates how to use the kubectl proxy to communicate with a pod, send GET and POST requests to the pod, and store data on the pod. The example uses curl commands to interact with the pod through the API server, showing how to retrieve data from the pod and update it with new information.A StatefulSet was used to store data on a pod. When a GET request is made, the stored data is returned. The pod is then deleted and recreated by the StatefulSet, which still serves the same data as before. This demonstrates that a StatefulSet preserves state even when pods are rescheduled or deleted.A StatefulSet maintains its pods\\' identities, including hostnames and persistent data, even when scaled down or recreated after deletion. Scaling down a StatefulSet deletes pods but leaves PersistentVolumeClaims intact, with pods deleted in descending order of ordinal numbers. A non-headless Service can be used to expose stateful pods, allowing clients to connect through the Service rather than directly.To access a cluster-internal service, you can use the proxy feature provided by the API server, or use a pod to access it. The URI path is formed like /api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>. Peer discovery in a StatefulSet is also important for clustered apps to find other members.A StatefulSet\\'s pods can discover their peers by performing an SRV DNS lookup, which points to the hostnames and ports of servers providing a specific service. Kubernetes creates SRV records to point to the hostnames of the pods backing a headless service. A pod can get a list of all other pods by running the dig command inside a temporary pod.To discover peers in a StatefulSet, the application performs a DNS lookup using the `dns.resolveSrv` method. It queries for SRV records for a specific service name and retrieves a list of addresses. If no peers are found, it returns \\'No peers discovered.\\' Otherwise, it displays data from all cluster nodes.StatefulSets are used to deploy replicated stateful applications, allowing for multiple pods to be created with a specified replica count. The pod template can be updated using kubectl, and the process involves creating a new image with the desired changes and then applying it to the StatefulSet, which will automatically update each pod with the new image.To update a StatefulSet, edit its definition using `kubectl edit` and modify the spec.replicas and image attributes. Save the file and exit to apply changes. If existing replicas are not updated, delete them manually for the StatefulSet to bring up new ones based on the new template.StatefulSets in Kubernetes allow for replicated stateful applications to be deployed, where each pod has a unique identity and storage. When scaling up or down, pods can discover peers and handle horizontal scaling with ease. However, when a node fails abruptly, StatefulSet cannot create a replacement pod until it knows the old pod is no longer running, which requires manual intervention by the cluster administrator. This can be observed by simulating a node\\'s disconnection from the network by shutting down its eth0 interface.StatefulSets handle node failures by marking the node as NotReady and pods on that node\\'s status as Unknown. If the node comes back online, the pod is marked as Running again. However, if the pod\\'s status remains unknown for more than a few minutes, it is automatically evicted from the node and deleted.A StatefulSet deployment is experiencing issues due to an unresponsive node. The kubia-0 pod is shown as Terminating, but in reality, its container is still running fine. To resolve this, the pod needs to be deleted manually, and a replacement pod should be created by the StatefulSet. However, upon deletion, the kubia-0 pod remains, with an unknown status, despite being deleted.A pod marked for deletion can\\'t be deleted immediately due to node\\'s network being down. Use --force and --grace-period 0 options with kubectl delete command to forcefully delete the pod. This approach is risky, especially for stateful pods. It\\'s recommended to bring back a disconnected node before continuing.StatefulSets allow replicated stateful applications to connect with each other through host names and enable forcible deletion of stateful pods, a crucial feature for managing Kubernetes-managed apps.This chapter delves into Kubernetes internals, explaining how resources are implemented. It covers what makes up a cluster, each component\\'s role and functionality, pod scheduling, controllers in the Controller Manager, and resource deployment. Specific topics include Deployments, pods, networking between pods, Services, and achieving high-availability.A Kubernetes cluster consists of two parts: the Control Plane, which controls the cluster and its components include etcd, API server, Scheduler, Controller Manager, and add-on components such as DNS server, Dashboard, Ingress controller, Heapster, and Container Network Interface plugin. The worker nodes run the Kubelet, Service Proxy, and Container Runtime, with Docker being a common choice. The distributed nature of these components allows for scalability and flexibility in managing containerized applications.Kubernetes system components communicate through the API server, which connects to etcd. Components on the worker node run on the same node, but Control Plane components can be split across multiple servers. The API server exposes a ComponentStatus resource showing health status of each component. The kubectl get command displays statuses of all components.Kubernetes Control Plane components can have multiple instances of etcd and API server running for high availability. However, only one instance of Scheduler and Controller Manager may be active at a given time. These components, along with kube-proxy, can run on the system directly or as pods, with the Kubelet deploying them as pods. The Control Plane components are currently running as pods on the master node in the provided cluster, while worker nodes run kube-proxy and Flannel networking pods.Kubernetes stores cluster state and metadata in etcd, a fast, distributed key-value store. etcd v3 is recommended due to improved performance. Resources are stored under /registry with version numbers for optimistic concurrency control, preventing simultaneous updates.Kubernetes stores resource types like pods, secrets, and services as key-value entries in etcd. These entries are JSON representations of the resources, with each entry corresponding to an individual pod or other resource. Prior to v1.7, secrets were stored unencrypted, but from then on they\\'re encrypted for security.Kubernetes ensures consistency by requiring all Control Plane components to go through the API server, implementing optimistic locking in a single place and validating data written to the store. In a distributed etcd cluster, RAFT consensus algorithm is used to reach a consensus on the actual state, ensuring that each node\\'s state is either the current or previously agreed-upon state. This prevents split-brain scenarios where the cluster splits into two disconnected groups of nodes, allowing only the group with majority (quorum) to accept state changes.Kubernetes etcd clusters should have an odd number of instances, ideally 5 or 7, to handle failures and maintain a majority for state transitions. The API server provides a CRUD interface over RESTful API for querying and modifying cluster state, storing it in etcd, validating objects, and handling optimistic locking to prevent concurrent updates. Clients like kubectl post requests to the API server through HTTP POST.The API server authenticates clients using authentication plugins, which extract client data from HTTP requests or certificates. Authorization plugins then determine if the authenticated user can perform requested actions on resources. Admission Control plugins validate and/or modify resource requests, modifying fields, overriding values, or rejecting requests. Examples of Admission Control plugins include AlwaysPullImages, ServiceAccount, NamespaceLifecycle, and ResourceQuota.The Kubernetes API server validates and stores objects in etcd, then notifies clients of resource changes by sending updates to watchers. Clients can watch for changes using an HTTP connection, receiving a stream of modifications to watched objects. The kubectl tool also supports watching resources, allowing users to be notified of creation, modification, or deletion events without needing to constantly poll lists of resources.The document discusses Kubernetes\\' scheduling process, where the Scheduler waits for newly created pods through the API server\\'s watch mechanism and assigns a node to each new pod. The Scheduler updates the pod definition, and the API server notifies the Kubelet to create and run the pod\\'s containers. The default Scheduler uses an algorithm that filters nodes to obtain acceptable ones and prioritizes them to choose the best one, with round-robin used if multiple nodes have the highest score.To determine which nodes are acceptable for a pod, Kubernetes Scheduler passes each node through a list of configured predicate functions that check things such as hardware resource availability, memory or disk pressure conditions, and volume usage. After passing all checks, the Scheduler ends up with a subset of eligible nodes from which it selects the best one based on factors like available resources, pod requirements, and affinity rules.The Kubernetes Scheduler can be configured to suit specific needs or infrastructure specifics, or even replaced with a custom implementation. Multiple Schedulers can be run in the cluster, and pods can be scheduled using a specified Scheduler by setting the schedulerName property. The Controller Manager also runs controllers that make sure the actual state of the system converges toward the desired state, as specified in resources deployed through the API server, such as ReplicationController, ReplicaSet, DaemonSet, and Job controllers.Kubernetes controllers are active components that perform work as a result of deployed resources. They watch the API server for changes and perform operations such as creation, update or deletion of resources. Controllers run a reconciliation loop to reconcile actual state with desired state and use the watch mechanism to be notified of changes, also performing periodic re-list operations to ensure they haven\\'t missed anything. Each controller connects to the API server and asks to be notified when a change occurs in its responsible resource list.The Replication Manager is a controller that makes ReplicationController resources come to life, and it\\'s not the actual work but the manager that does it. It uses the watch mechanism to be notified of changes affecting the desired replica count or number of matched pods, triggering rechecks and actions accordingly. The worker() method contains the magic where all the actual function calls are made.Kubernetes controllers manage Pod resources through the API server, with different controllers like ReplicaSet, DaemonSet, Job, Deployment, StatefulSet, Node, and Service controlling various aspects of cluster management, such as pod creation, scaling, and load balancing.Services aren\\'t linked directly to pods but contain a list of endpoints which is updated by the Endpoints controller based on pod selector and pod IPs/ports. The Namespace controller deletes all resources in a namespace when it\\'s deleted, while the PersistentVolume controller binds PVs to PVCs matching access mode and capacity requirements.Kubernetes controllers operate on API objects through the API server without communicating with Kubelets. The Control Plane handles part of the system\\'s operation, while the Kubelet and Service Proxy run on worker nodes, responsible for starting and monitoring containers, reporting status and events to the API server, and terminating containers when their Pod is deleted.The Kubernetes Service Proxy, also known as kube-proxy, is responsible for making sure clients can connect to services defined through the Kubernetes API. It performs load balancing across pods backing a service and ensures connections end up at one of the pods or non-pod service endpoints. The proxy runs on every worker node and uses iptables rules to intercept connections destined to service IPs, redirecting them to the proxy server.Kubernetes kube-proxy uses iptables rules to redirect packets to a randomly selected backend pod, without passing them through an actual proxy server. This is called the iptables proxy mode and has performance benefits over user-space proxying, which also balances connections across pods in a true round-robin fashion. Add-ons like DNS lookup and web dashboard are deployed as pods using YAML manifests and can be managed with resources such as Deployments and DaemonSets.Kubernetes cluster\\'s DNS add-on is a Deployment that provides a DNS server for pods to look up services by name or IP addresses. The DNS server pod uses the API server\\'s watch mechanism to update its records with Service and Endpoints changes. Ingress controllers run reverse proxy servers like Nginx, observing resources through the watch mechanism and configuring the proxy server accordingly. Unlike Services, Ingress controllers forward traffic directly to service pods, preserving client IPs when external clients connect.Kubernetes system is composed of small, loosely coupled components that work together to synchronize actual and desired state. The API server triggers a coordinated dance of components when submitting a pod manifest or Deployment resource, resulting in containers running. Controllers, Scheduler, Kubelet, and other components watch the API server for changes and cooperate to create and manage resources such as Pods, Deployments, and ReplicaSets.When a Deployment manifest is submitted to Kubernetes, the API server validates it and returns a response. The Deployment controller creates a ReplicaSet, which in turn creates pods. A chain of notifications through watch mechanisms triggers this process, involving clients such as kubectl, Scheduler, and Kubelet.The ReplicaSet controller creates Pod resources based on a pod template, which are then scheduled by the Scheduler to a specific node. The Kubelet runs the containers on the assigned node, and both the Control Plane components and Kubelet emit events to the API server as they perform these actions.A running pod is a logical host that can contain one or more containers and has its own IP address. It\\'s created by the Kubelet which runs the container(s) specified in the pod spec. The Kubelet creates a network namespace for each pod, allowing for isolated networking between pods.Kubernetes uses an additional \\'pause\\' container to hold all containers of a pod together, sharing network and other Linux namespaces. This infrastructure container runs from pod scheduling until deletion, allowing application containers to reuse these namespaces if restarted.Kubernetes achieves inter-pod networking by not setting up the network itself, but rather relying on system administrators or CNI plugins to do so. The network must allow pods to communicate with each other without NAT and with the same IP addresses visible to all pods. This enables simple networking for applications running inside pods as if they were connected to the same network switch.A Kubernetes cluster\\'s inter-pod networking works by creating a virtual Ethernet interface pair (veth pair) for each pod, connecting it to the same bridge as other pods on the same node. The pod\\'s containers use its network namespace and IP address, which is set up and held by the infrastructure container (pause container). This allows communication between pods on the same node without needing NAT.Pods in a Kubernetes cluster can communicate with each other using the container runtime\\'s network bridge. To enable communication between pods on different nodes, bridges must use non-overlapping IP address ranges and can be connected through overlay or underlay networks, regular layer 3 routing, or by configuring node physical network interfaces and routing tables to route packets between nodes.Kubernetes uses a veth pair to connect containers on the same network switch without routers in between. To make it easier, a Software Defined Network (SDN) can be used, or a Container Network Interface (CNI) plugin such as Calico, Flannel, Romana, Weave Net can be installed by deploying a YAML containing a DaemonSet and supporting resources. Services are implemented to expose a set of pods at a long-lived, stable IP address and port.Kubernetes\\' kube-proxy handles Services by assigning a stable IP address and port, which clients connect to. The virtual IP address is assigned immediately upon service creation, and kube-proxy sets up iptables rules on worker nodes to redirect packets destined for the service IP/port pair to one of the backing pods. Kube-proxy also watches Endpoints objects for changes in backing pods.A packet sent to a Kubernetes service\\'s virtual IP is modified by the kernel on the node it\\'s received on, according to iptables rules. If the packet matches a rule, its destination IP and port are changed to point to a randomly selected backend pod. This process is handled by kube-proxy, which watches for changes to services and endpoints.To achieve high availability in Kubernetes, apps can be run through a Deployment resource with an appropriate number of replicas. If a replica becomes unavailable, it will be replaced quickly, although there may be a short period of downtime. For non-horizontally scalable apps, leader-election mechanisms can be used to ensure only one instance is active at a time, avoiding downtime. Kubernetes itself requires high availability, and its Control Plane components can be made highly available using techniques such as load balancing and multiple masters.To make Kubernetes highly available, run multiple master nodes with etcd, API server, Controller Manager, and Scheduler components. Each component can be made highly available by running multiple instances and replicating data across them, ensuring the cluster can handle failures and maintain read/write operations.Running multiple instances of etcd, API servers, controllers, and schedulers can provide high availability in Kubernetes clusters. However, careful consideration is needed for components like the Controller Manager and Scheduler to avoid racing conditions and undesired effects. The use of leader election mechanisms can help ensure that only one instance is active at a time, providing a stable and reliable system.Kubernetes components such as Controller Manager and Scheduler can run collocated or on separate machines. Leader election is achieved by creating a resource in the API server using an Endpoints object, which has no side effects unless a Service with the same name exists. The first instance to successfully write its name into the resource becomes the leader, and periodic updates from the leader ensure that other instances know it\\'s still alive.Kubernetes components such as API server, Scheduler, controllers, Kubelet, and kube-proxy work together to bring a pod to life. Each component has a specific role: API server receives requests, Scheduler assigns resources, controllers manage pods, Kubelet runs containers on nodes, and kube-proxy performs load balancing. High availability is achieved by running multiple instances of each component.This chapter covers authentication, ServiceAccounts, and permissions configuration in a Kubernetes cluster. It explains how the API server handles requests using authentication plugins and introduces the concept of ServiceAccounts for authenticating applications running in pods.Kubernetes uses authentication plugins to determine who\\'s sending a request by examining the request and returning the username, user ID, and groups to the API server core. The API server stops invoking remaining plugins and continues onto authorization. Authentication plugins can obtain client identity using methods such as client certificate, authentication token, or basic HTTP authentication. Kubernetes distinguishes between users (actual humans) and pods (applications running inside them), with users managed by external systems like SSO and pods using service accounts created in the cluster.ServiceAccounts are identities of apps running in pods, allowing them to authenticate with the API server using a token. The API server passes the username to authorization plugins, which determine if actions can be performed. ServiceAccounts are resources scoped to individual namespaces and can be listed like other Kubernetes resources.Kubernetes authentication works by assigning ServiceAccounts to pods, which determine resource access. Each namespace has a default ServiceAccount, but additional ones can be created for cluster security reasons, such as running pods under constrained accounts or granting permissions to retrieve or modify resources.A ServiceAccount is created with `kubectl create serviceaccount` and can be inspected with `kubectl describe sa`. A custom token Secret is associated with the ServiceAccount, containing a CA certificate, namespace, and token. The token is a JSON Web Token (JWT) that can be mounted inside a pod if \\'mountable Secrets\\' are enforced.A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories. A pod\\'s ServiceAccount must be set when creating the pod, and it cannot be changed later. Image pull Secrets are added automatically to all pods using a ServiceAccount, saving the need to add them individually.A Kubernetes Pod is created using a non-default ServiceAccount named foo, which allows it to list pods when talking to the API server. The Pod\\'s containers can access the token from the ServiceAccount and use it to authenticate with the API server, as shown by the successful response received from listing pods.Kubernetes\\' Role-Based Access Control (RBAC) authorization plugin prevents unauthorized users from viewing or modifying the cluster state. The default ServiceAccount isn\\'t allowed to view or modify the cluster state, unless granted additional privileges. Additional authorization plugins like Attribute-based access control (ABAC), Web- Hook, and custom implementations are also available, but RBAC is the standard.The Kubernetes API server\\'s security is ensured through the RBAC authorization plugin, which uses user roles to determine permissions. Roles are associated with subjects and allow certain verbs on resources or non-resource URL paths. Managing authorization is done by creating four RBAC-specific Kubernetes resources, including RoleBindings and ClusterRoleBindings.RBAC (Role-Based Access Control) in Kubernetes is configured through four resources: Roles and ClusterRoles that define what can be done on resources, and RoleBindings and ClusterRoleBindings that bind roles to users or groups. Roles are namespaced while ClusterRoles are cluster-level, allowing multiple bindings within a namespace or across the cluster.To secure the Kubernetes API server, RBAC must be enabled in the cluster by setting version 1.6 or higher and disabling legacy authorization if using GKE 1.6 or 1.7. Minikube requires enabling RBAC with --extra-config. The permissive-binding clusterrolebinding should be deleted to re-enable RBAC.The document explains how to secure a Kubernetes cluster with role-based access control (RBAC). It demonstrates creating two pods in separate namespaces using kubectl commands and attempting to list services from within each pod using curl. The example shows that RBAC prevents the default ServiceAccount from listing services, even though it\\'s running in the same namespace, and guides the reader on how to create a Role resource to allow the ServiceAccount to perform such actions.A Role resource defines what actions can be taken on which resources, allowing users to get and list Services in a specific namespace (foo) via a Role named service-reader.To secure a Kubernetes cluster with role-based access control, create a Role (e.g. service-reader) in a namespace using kubectl create or -f service-reader.yaml. Bind the Role to a ServiceAccount in the same namespace using kubectl create rolebinding, specifying the Role and ServiceAccount names. This grants permissions for the ServiceAccount to perform actions defined by the Role.A RoleBinding references a single Role and can bind it to multiple subjects, such as ServiceAccounts, users, or groups. In this case, the test RoleBinding binds the default ServiceAccount with the service-reader Role in the foo namespace, allowing the pod running under that account to list Services.You can add a pod\\'s ServiceAccount from another namespace to a RoleBinding by editing the RoleBinding and adding the subject, allowing the pod to list Services in its own and other namespaces.Regular Roles allow access to resources in the same namespace but not across different namespaces. ClusterRoles, on the other hand, are cluster-level resources that can grant access to non-namespaced resources, non-resource URLs, or used as a common role inside individual namespaces. A ClusterRole is created using kubectl create clusterrole with the desired verb and resource, and can be bound to a ServiceAccount in a specific namespace using ClusterRoleBinding.To secure a Kubernetes cluster with role-based access control, create a ClusterRole that specifies API groups, resources, and verbs. Bind this ClusterRole to a ServiceAccount using a RoleBinding, then verify if the ServiceAccount can list PersistentVolumes using curl.To secure the Kubernetes API server, you must use a ClusterRoleBinding to grant access to cluster-level (non-namespaced) resources, unlike with namespaced resources where a RoleBinding can be used. The command to create a ClusterRoleBinding is similar to that of a RoleBinding, but without specifying the namespace and replacing rolebinding with clusterrolebinding.To secure a Kubernetes cluster with role-based access control, use a ClusterRole and a ClusterRoleBinding to grant access to cluster-level resources. Non-resource URLs must also be granted explicitly, usually done through the system:discovery ClusterRole and its binding, which allow access to URLs like /api, /apis, /healthz, etc.The system:discovery ClusterRole allows access to non-resource URLs with only GET HTTP method, and can be bound to all users through a ClusterRoleBinding. This binding grants access to the API server\\'s /api URL path to anyone who accesses it from within a pod.ClusterRoles can be used with namespaced RoleBindings to grant access to specific namespaces and their resources. The \\'view\\' ClusterRole allows reading (get, list, watch) but not writing resources in a namespace, demonstrating how ClusterRoles can control access to resources within a specific scope.The Kubernetes API server\\'s permissions are determined by a ClusterRoleBinding or RoleBinding. A ClusterRoleBinding allows subjects to view resources across all namespaces, while a RoleBinding only allows viewing resources in the namespace of the binding. The example demonstrates listing pods using curl commands before and after creating a ClusterRoleBinding, showing that it applies across all namespaces.A pod can access namespaced resources in any namespace by combining a ClusterRoleBinding with a ClusterRole referring to namespaced resources. Replacing the ClusterRoleBinding with a RoleBinding limits the pod\\'s access to only the specified namespace, as demonstrated with the creation of a RoleBinding in the foo namespace.Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles. The document explains various combinations of these concepts for specific use cases, such as accessing cluster-level resources, non-resource URLs, namespaced resources in any or specific namespaces. It also highlights how a ServiceAccount can only view pods within its own namespace, despite using a ClusterRole.Kubernetes has a default set of ClusterRoles and ClusterRoleBindings that are updated every time the API server starts, allowing for automatic recreation if deleted or changed in newer versions. The default roles include cluster-admin, system:basic-user, and various controller roles.The most important ClusterRoles in Kubernetes are view, edit, admin, and cluster-admin, designed to prevent privilege escalation. The view role allows read-only access, while the edit role grants modifying resources within a namespace. The admin role provides complete control of a namespace\\'s resources, but not ResourceQuotas or Namespace itself. The cluster-admin role offers complete control over the entire Kubernetes cluster.The Controller Manager runs as a single pod, but each controller can use a separate ClusterRole and ClusterRoleBinding. By default, ServiceAccounts have no permissions, so pods can\\'t view cluster state. It\\'s best to grant only the necessary permissions (principle of least privilege). Create specific ServiceAccounts for each pod and associate them with tailor-made Roles through RoleBindings. Constrain ServiceAccounts to prevent damage if compromised.Kubernetes API server security is discussed. ServiceAccounts are used to run pods, with default accounts created for each namespace. Additional accounts can be created manually and configured to allow mounting specific Secrets. Roles and ClusterRoles define allowed actions on resources, and RoleBindings bind these to users, groups, and ServiceAccounts. This sets the stage for securing cluster nodes and isolating pods in the next chapter.This chapter focuses on securing cluster nodes and the network, allowing pods to access node resources while limiting user actions. Key topics include using default Linux namespaces in pods, running containers as different users, privileged containers, modifying kernel capabilities, defining security policies, and securing the pod network.In Kubernetes, containers in a pod run under separate namespaces, isolating their processes from other containers or the node\\'s default namespace. Certain system pods can use the node\\'s network namespace by setting hostNetwork to true in the pod spec, allowing them to see and manipulate node-level resources and devices.A Kubernetes pod can use the host\\'s network namespace by setting `hostNetwork: true` in its spec. This allows it to see all the host\\'s network adapters and bind to a port in the node\\'s default namespace using `hostPort`. Note that this is different from a NodePort service, which binds the port on all nodes even if no pod is running on them.When using a specific host port in a pod, only one instance of the pod can be scheduled to each node due to multiple processes cannot bind to the same host port. The Scheduler takes this into account and doesn\\'t schedule multiple pods to the same node, allowing only three pods to be scheduled out of four replicas when three nodes are available.Using the host node\\'s namespaces in a pod, you can define hostPort in a pod\\'s YAML definition. This allows access to the pod through the node\\'s port, but not on other nodes. The hostPID and hostIPC pod spec properties allow containers to use the node\\'s PID and IPC namespaces, respectively.This chapter discusses securing cluster nodes and networks by configuring the security context of pods and containers. This includes setting hostIPC to true for processes to communicate, configuring container security through user ID, preventing root access, running in privileged mode, adding or dropping capabilities, setting SELinux options, and preventing process writing to the filesystem.To run a pod under a different user ID than the one baked into the container image, set the pod\\'s securityContext.runAsUser property. This was shown by running a pod as user \\'guest\\' with UID 405 and verifying the result using the id command inside the container.The chapter discusses securing cluster nodes and the network by preventing containers from running as root. It explains how to prevent an attacker from pushing a malicious image under the same tag as a trusted image, and how to specify that a pod\\'s container needs to run as a non-root user using `runAsNonRoot: true`. It also touches on running pods in privileged mode for specific use cases.A Kubernetes pod\\'s container can run in privileged mode by setting the `privileged` property to true in its security context, allowing access to the node\\'s kernel and device files. This is demonstrated by comparing the devices visible in a non-privileged container with those in a privileged container.In Kubernetes, instead of making a container privileged and giving it unlimited permissions, you can give it access only to the kernel features it really requires by adding individual kernel capabilities. This allows fine-tuning of the container\\'s permissions and limiting the impact of a potential intrusion. For example, you can add CAP_SYS_TIME capability to allow the container to change the system time.Configuring a container\\'s security context by adding or dropping Linux kernel capabilities, such as SYS_TIME. This can be done under the securityContext property in a pod spec, and is a more controlled way than giving full privileges with privileged: true. Capabilities can be added or dropped to allow specific actions, but requires knowledge of what each capability does.To prevent containers from modifying the owner of files or writing to their own filesystem, Kubernetes capabilities can be dropped and the readonlyRootFilesystem property set to true. This prevents malicious code injection in case of vulnerabilities.When configuring a pod\\'s security context, setting the container\\'s readOnlyRootFilesystem property to true makes the filesystem read-only, preventing write access to the / directory. However, writing to a mounted volume is allowed. To increase security in production environments, set this property to true at the pod level or override it at the container level.Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs. The fsGroup and supplementalGroups properties are used in a pod\\'s security context to achieve this. An example is provided where two containers with different user IDs share the same volume, and the container running as user ID 1111 can read or write files of the container running as user ID 2222 due to the shared group permissions.The document explains how to restrict the use of security-related features in pods. It discusses the fsGroup and supplementalGroups properties, which are used to set group IDs for users running containers. The fsGroup property sets the ownership of a mounted volume, while the supplementalGroups property defines additional group IDs associated with a user. A cluster administrator can restrict the use of these features by creating PodSecurityPolicy resources, which define what security-related features users can or cannot use in their pods.A PodSecurityPolicy admission control plugin validates pod definitions against configured policies before storing them in etcd. A PodSecurityPolicy resource defines settings such as IPC and network namespace usage, host ports, user IDs, and privileged container creation. To enable RBAC and PodSecurityPolicy admission control in Minikube, use the command: $ minikube start --extra-config apiserver.Authentication.PasswordFile=/etc/kubernetes/passwd --extra-config=apiserver.Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRunOptions.AdmissionControl=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,PodSecurityPolicy. Create a password file with the command: $ cat <<EOF | minikube ssh sudo tee /etc/kubernetes/passwd password,alice,1000,basic-user password,bob,2000,privileged-user EOFThis document discusses restricting security-related features in pods, specifically kernel capabilities, SELinux labels, writable root filesystems, and volume types. A sample PodSecurityPolicy is provided that prevents pods from using the host\\'s IPC, PID, and Network namespaces, and restricts privileged containers and host ports. The policy allows containers to run as any user or group, and use any SELinux groups.A cluster node and network security chapter that explains the restriction of deploying privileged pods due to a pod security policy. It also discusses how to constrain container user IDs using the MustRunAs rule, with examples showing how to specify allowed ID ranges for runAsUser, fsGroup, and supplementalGroups fields.A PodSecurityPolicy can restrict the use of security-related features in pods, enforcing only when creating or updating pods. If a pod spec tries to set fields outside allowed ranges, it\\'s rejected by the API server. However, if a container image has an out-of-range user ID, but the runAsUser property is not set, the API server may still accept the pod and run the container with the specified ID in the PodSecurityPolicy.Securing cluster nodes and network involves using the MustRunAsNonRoot rule in the runAsUser field, preventing users from deploying containers that run as root. Configuring allowed, default, and disallowed capabilities includes specifying which capabilities can be added or dropped in a container using fields like allowedCapabilities, defaultAddCapabilities, and requiredDropCapabilities. This helps control what operations containers can perform by adding or dropping Linux kernel capabilities.PodSecurityPolicy resources allow restricting security-related features in pods, adding capabilities to containers, and constraining volume types. Capabilities can be added or dropped from containers using defaultAddCapabilities and requiredDropCapabilities fields respectively. Volume types can also be restricted, with a minimum of emptyDir, configMap, secret, downwardAPI, and persistentVolumeClaim allowed.PodSecurityPolicies (PSPs) are cluster-level resources that can\\'t be stored in a specific namespace. Different PSBs can be assigned to different users and groups using the RBAC mechanism by creating ClusterRole resources, pointing them to individual policies, and binding them to users or groups with ClusterRoleBindings. A new PSP is created to allow privileged containers to be deployed, allowing for more flexibility in managing system pods and user pods.To restrict security-related features in pods, you can use PodSecurityPolicies. You can create two ClusterRoles (psp-default and psp-privileged) to allow different users to use specific policies. Bind these roles to users using a ClusterRoleBinding, referencing the policies by name (e.g., default or privileged). This way, Alice can only deploy non-privileged pods while Bob can deploy both. Authenticated users will have access to the default policy.You\\'ll bind the psp-privileged ClusterRole only to Bob by creating a clusterrolebinding. Alice will have access to the default PodSecurity-Policy, while Bob has access to both. To authenticate as Alice or Bob, create new users in kubectl\\'s config with set-credentials commands. You can then use the --user option to create pods with different user credentials, demonstrating that Bob can create privileged pods while Alice cannot.Isolating the pod network by configuring NetworkPolicy resources, which apply to pods matching a label selector and specify sources or destinations that can access the matched pods. This is configurable if the container networking plugin supports it. A default-deny NetworkPolicy prevents all clients from connecting to any pod in a namespace, and can be enabled by creating a NetworkPolicy with an empty pod selector.To secure cluster nodes and network, a CNI plugin or networking solution must support NetworkPolicy. A NetworkPolicy resource can be created in the same namespace as a database pod to allow only specific pods (e.g. webserver) to connect on port 5432, while blocking other pods from connecting to the database.To secure a microservice for a specific tenant, create a NetworkPolicy that allows only pods from the same tenant\\'s namespaces to access the microservice on a specific port. The policy applies to pods labeled with \\'microservice=shopping-cart\\' and allows access from namespaces labeled as \\'tenant=manning\\'. This example demonstrates how to isolate network traffic between Kubernetes namespaces for multiple tenants using the same cluster.In a multi-tenant Kubernetes cluster, tenants can\\'t add labels to their namespaces themselves. NetworkPolicies ensure only pods running in specific namespaces or IP blocks can access targeted pods. An example ingress rule allows traffic from the 192.168.1.0/24 IP block to access shopping-cart pods.In this chapter, Network Policies are used to limit a pod\\'s inbound and outbound traffic. Egress rules allow limiting outbound traffic of a set of pods by specifying which pods they can connect to. PodSecurityPolicy resources can be created to prevent users from creating pods that could compromise a node. Cluster-level policies can be associated with specific users using RBAC\\'s ClusterRoles and ClusterRoleBindings.This chapter covers requesting CPU and memory computational resources for containers, setting hard limits, understanding Quality of Service guarantees for pods, and limiting total resources in a namespace.When creating a pod, you can specify resource requests and limits for each container individually. The pod\\'s total resources are the sum of all containers\\' requests and limits. Requests define the minimum amount of CPU and memory a container needs, while limits set a hard limit on what it may consume.The Kubernetes Scheduler determines if a pod can fit on a node based on the sum of resources requested by existing pods, not actual resource consumption. A pod\\'s resource requests specify its minimum requirements, and scheduling is denied if unallocated resources are insufficient to meet these requirements. Requests don\\'t limit CPU usage, but specifying a CPU limit does.The Kubernetes Scheduler prioritizes nodes based on requested resources, using functions like LeastRequestedPriority and MostRequestedPriority to select the best node for a pod. The MostRequestedPriority function is useful in cloud infrastructure where adding or removing nodes is possible, as it allows for tight packing of pods and potential removal of unused nodes, saving costs.This chapter discusses managing pods\\' computational resources, specifically the Node resource. It explains that a node\\'s capacity represents its total resources, but not all are available to pods due to reserved resources for Kubernetes and system components. The Scheduler bases its decisions on allocatable resource amounts. A pod with CPU requests of 800 millicores was successfully scheduled, but attempting to deploy another pod with 1 core of CPU request did not fit on any node.A Kubernetes pod\\'s container request for 1 whole core CPU instead of millicores causes scheduling failure due to insufficient CPU on a single node. The issue is resolved by inspecting the node resource with `kubectl describe node` and examining the output, which shows that the node has allocated resources that are not associated with the pod, resulting in failed scheduling.The Kubernetes Scheduler allocates computational resources based on pod requests and limits. A total of 1,275 millicores have been requested by running pods, exceeding initial requests. The culprit behind additional CPU resources usage is identified in the kube-system namespace. To free up resources for a third pod to be scheduled, one of the first two pods can be deleted. This triggers the Scheduler to schedule the third pod as soon as the deleted pod terminates. Both CPU and memory requests are treated equally by the Scheduler.Kubernetes distributes unused CPU time among pods in a ratio based on their CPU requests, allowing one pod to consume all available CPU if the other is idle. Custom resources can also be added and requested by pods, initially as Opaque Integer Resources, then replaced with Extended Resources in Kubernetes 1.8.To manage pods\\' computational resources in Kubernetes, a custom resource can be added to the Node object\\'s capacity field. This involves performing a PATCH HTTP request and specifying the resource name and quantity. When creating pods, the same resource name and requested quantity must be specified under the resources.requests field. Resource limits for containers can also be set to prevent them from using up excessive CPU or memory. Limits can be set on both CPU and memory usage, preventing malfunctioning or malicious pods from affecting other nodes.A Kubernetes pod\\'s container has configured resource limits for CPU and memory, limiting its consumption to 1 CPU core and 20Mi of memory. Resource limits are not constrained by the node\\'s allocatable resources and can be overcommitted, potentially leading to containers being killed if resources are fully utilized.When a process running in a container tries to use more resources than allowed, the process is throttled for CPU usage. However, for memory, if the process allocates more memory than its limit, it\\'s killed (OOMKilled) and restarted by Kubernetes with increasing delays between restarts, eventually resulting in a CrashLoopBackOff status.Containers may get OOMKilled even if they aren\\'t over their memory limit due to the way apps in containers see limits. The top command shows memory amounts of the whole node, not the container\\'s memory limit, which can be misleading. Containers always see the node\\'s memory, not their own, and this can lead to unexpected behavior.When running containers in a Kubernetes cluster, it\\'s essential to manage pods\\' computational resources properly. Containers can see all node CPUs and may exceed memory limits if not configured correctly. The JVM may be OOMKilled when the heap size exceeds container memory limits. Setting -Xmx options doesn\\'t solve issues with off-heap memory. New Java versions consider container limits, but certain applications that rely on CPU count for worker threads may spin up too many threads and exceed resources.Kubernetes categorizes pods into three Quality of Service (QoS) classes: BestEffort (lowest priority), Burstable, and Guaranteed (highest). The QoS class is derived from a pod\\'s resource requests and limits. A Guaranteed class is assigned to pods with equal request and limit settings for CPU and memory, while a BestEffort class is given to pods with no requests or limits set. This allows Kubernetes to make decisions on which containers to kill in case of resource shortages.A pod\\'s Quality of Service (QoS) class is determined by the relationship between its resource requests and limits. The three QoS classes are BestEffort, Burstable, and Guaranteed. A pod with a best effort QoS can consume any available resources. A burstable pod gets the requested amount of resources, but can use additional ones up to their limit if needed. A guaranteed pod gets the exact amount of resources it requests.A pod\\'s QoS (Quality of Service) class is determined by the classes of its containers, and can be BestEffort, Burstable, or Guaranteed. For single-container pods, requests and limits are used to determine the class, while for multi-container pods, the highest container class determines the pod\\'s class. If all containers have the same QoS class, it\\'s also the pod\\'s class, but if at least one container has a different class, the pod\\'s class is Burstable.When system memory is overcommitted, QoS classes determine which container gets killed first. BestEffort class gets killed first, followed by Burstable, and finally Guaranteed. If containers have the same QoS class, the process with the highest OutOfMemory (OOM) score gets killed, calculated from available memory consumption and fixed OOM score adjustment based on QoS class and requested memory.To avoid containers being at the mercy of others that specify resource requests and limits, it\\'s recommended to set these values on every container or use a LimitRange resource per namespace to specify minimum/max limit values and default resource requests.A LimitRange resource is used by the LimitRanger Admission Control plugin to validate pod specs. It prevents users from creating pods bigger than any node in the cluster, specifying limits for individual containers or objects in the same namespace, but not total resources across all pods.A LimitRange object is used to set default requests and limits for pods per namespace, applying to containers\\' requests and limits. It allows setting min/max values, default resource requests/limits, and max ratio of limits vs requests. This validation is performed by the API server when receiving a new pod manifest, and does not affect existing pods or PVCs created before modifying the limits.A Kubernetes LimitRange object is used to set maximum CPU and memory usage limits for pods and containers. When creating a pod with a container requesting more than the allowed limit, the pod is rejected due to the Forbidden error message from the server listing all reasons why the pod was rejected. Default resource requests and limits can be applied automatically when creating a pod by setting them in a LimitRange object, allowing admins to configure default, min, and max resources for pods per namespace.Resource quotas limit the total amount of resources available in a namespace, including computational resources, storage, number of pods, claims, and API objects. They are enforced at pod creation time and do not affect existing pods. A ResourceQuota object can be created to specify quotas for CPU and memory, as shown in Listing 14.13.A ResourceQuota object sets separate totals for requests and limits of CPU and memory resources in a namespace. It can be inspected using kubectl describe quota, showing used amounts for each resource. The quota applies to all pods\\' resource requests and limits in total, unlike LimitRange which applies to individual pods or containers separately.A ResourceQuota object can limit resources available in a namespace, including CPU, memory, persistent storage, and the number of objects that can be created. A LimitRange object is required alongside ResourceQuota to specify resource requests or limits for pods.A ResourceQuota in Kubernetes limits the number of objects that can be created in a namespace, such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services. The quota specifies hard limits for each object type, e.g., 10 pods, 5 replication controllers, 10 secrets, etc. It also allows limiting specific types of services like load balancers and node ports.You can limit resources in a namespace by specifying quotas for specific pod states and/or QoS classes, using quota scopes like BestEffort, NotBestEffort, Terminating, and NotTerminating. These scopes apply to pods with certain QoS classes or active deadline seconds, and can be used to limit the number of pods, CPU/memory requests, and limits. For example, a ResourceQuota for BestEffort/NotTerminating pods can ensure that at most four such pods exist.Properly setting resource requests and limits in Kubernetes is crucial for efficient cluster usage. Monitoring actual resource usage under expected load levels helps find the optimal spot. The Kubelet contains an agent called cAdvisor that collects basic resource consumption data, while Heapster collects and exposes metrics from all nodes in a single location.Heapster is a component that collects container and node usage data without needing to connect to the processes running inside pods\\' containers. It can be enabled in Minikube using `minikube addons enable heapster`. Once enabled, you can use `kubectl top` commands to see actual CPU and memory usage for cluster nodes and individual pods.The kubectl top command shows current pod metrics, but may not show metrics immediately due to Heapster aggregation. Historical resource consumption can be analyzed using tools like InfluxDB and Grafana for storing and visualizing data.InfluxDB and Grafana can be run as pods in a Kubernetes cluster, providing monitoring capabilities for resource usage. Deploying them is straightforward using manifests available in the Heapster Git repository or enabled with Minikube\\'s Heapster add-on. To analyze pod resource usage, open the Grafana web console to explore predefined dashboards and discover CPU usage across the cluster.When using Minikube, Grafana\\'s web console can be opened in a browser to view resource usage statistics for nodes and pods. The Cluster dashboard shows overall cluster usage, while the Pods dashboard displays individual pod resource usages. By examining these charts, users can determine if resource requests or limits need to be adjusted to accommodate more pods on a node.This chapter emphasizes considering a pod\\'s resource usage, configuring requests and limits to keep everything running smoothly. Key takeaways include specifying resource requests to schedule pods across the cluster, setting resource limits to prevent pods from starving other resources, and managing CPU and memory usage to optimize application performance.To manage pods\\' computational resources, use LimitRange objects for individual resource requests and limits or ResourceQuota objects to limit namespace-wide resources. Monitor pod usage over time to determine optimal resource settings, with Kubernetes using these metrics for automatic scaling in the next chapter.Applications can be scaled out manually or automatically by increasing replicas or resource requests. However, manual scaling is not ideal for sudden traffic increases. This chapter covers configuring automatic horizontal scaling of pods and cluster nodes based on CPU utilization and custom metrics, as well as understanding vertical scaling limitations.Kubernetes can automatically scale pods and cluster nodes based on CPU usage or other metrics, spinning up additional nodes if necessary. The autoscaling feature was rewritten between Kubernetes 1.6 and 1.7, so outdated information may exist online. Horizontal pod autoscaling adjusts the number of replicas by periodically checking pod metrics, calculating the required number of replicas, and updating the replicas field on the target resource.Horizontal pod autoscaling should already be enabled in your cluster, and once enabled, the Autoscaler can use metrics from Heapster or the aggregated resource metrics API to calculate the required number of pods. The calculation is based on a set of pod metrics and the target value, taking into account factors such as metric instability and multiple metrics per pod.The final step of autoscaling is updating the desired replica count field on the scaled resource object and letting the Replica-Set controller manage additional pods or excess ones. The Autoscaler controller modifies the replicas field through the Scale sub-resource, allowing it to operate on scalable resources like Deployments, ReplicaSets, ReplicationControllers, and StatefulSets.The horizontal pod autoscaler obtains metrics from cAdvisor, Heapster, and Kubelet to adjust replicas based on CPU utilization, taking into account a delay in propagating metrics data and performing scaling actions. It\\'s essential to consider this delay when observing the Autoscaler in action.The chapter discusses automatic scaling of pods and cluster nodes by focusing on scaling out (increasing the number of pods). The average CPU usage should come down, but setting a target CPU usage well below 100% is recommended to leave room for sudden load spikes. A HorizontalPodAutoscaler can be created to scale pods based on their CPU utilization, requiring CPU resource requests to be set in the pod template.To enable horizontal autoscaling for a Deployment, create a HorizontalPodAutoscaler (HPA) object and point it to the Deployment. The HPA will adjust the number of replicas to keep CPU utilization around 30% while ensuring a minimum of one replica and a maximum of five. This can be done using the kubectl autoscale command or by preparing and posting a YAML manifest for the HPA. Always make sure to autoscale Deployments instead of ReplicaSets to preserve the desired replica count across application updates.Automatic scaling of pods and cluster nodes is achieved using Horizontal Pod Autoscalers (HPA). The HPA adjusts the desired replica count on the Deployment based on CPU metrics. In a scenario where three pods have zero CPU usage, the autoscaler scales them down to a single pod, ensuring CPU utilization remains below the 30% target.The horizontal pod autoscaler successfully rescaled to one replica because all metrics were below target. To trigger a scale-up, expose the pods through a Service and start sending requests to your pod, thereby increasing its CPU usage. You can watch the HorizontalPodAutoscaler and Deployment with kubectl get --watch.This chapter covers automatic scaling of pods and cluster nodes in Kubernetes. It explains how to run a pod that repeatedly hits the kubia Service using the kubectl run command with options such as -it, --rm, and --restart=Never. The autoscaler increases the number of replicas based on CPU utilization, and events can be inspected with kubectl describe. The chapter also discusses how the autoscaler concludes the need for multiple replicas based on target CPU utilization percentages.The autoscaler in Kubernetes has a maximum rate of scaling, doubling the number of replicas in a single operation if more than two exist, or scaling up to four replicas. It also has a limit on how soon a subsequent scale-up operation can occur after the previous one, which is currently every three minutes for scale-up and five minutes for scale-down. The target metric value for CPU utilization can be modified by editing the HPA resource with kubectl edit command, increasing it from 30 to 60 in this case.Memory-based autoscaling in Kubernetes is more problematic than CPU-based due to the need to force old pods to release memory. This requires the app itself to manage, and can lead to infinite scaling if not implemented correctly. Custom metrics can also be used for autoscaling, but this was complicated in earlier versions of Kubernetes. Newer versions have simplified this process, allowing for more flexible scaling options.Kubernetes Horizontal Pod Autoscaling (HPA) allows defining multiple metrics for autoscaling decisions. There are three types of metrics: Resource, Pods, and Object. The Resource type uses a resource metric like CPU or memory requests. The Pods type refers to custom metrics related to the pod, such as Queries-Per-Second (QPS). The Object metric type scales pods based on a metric not directly pertaining to those pods, like an Ingress object\\'s QPS.Horizontal pod autoscalers (HPAs) use metrics to monitor resources and scale pods accordingly. Appropriate metrics include those with a linear decrease in average value as replicas increase, such as Queries per Second (QPS). However, not all metrics are suitable, like memory consumption, which can lead to non-linear behavior. HPAs currently do not allow scaling down to zero replicas.Kubernetes does not currently support idling and un-idling of pods or vertical pod autoscaling, but an experimental feature called InitialResources sets CPU and memory requests for newly created pods based on historical resource usage data, and a new proposal is being finalized to modify existing pod\\'s resource requests vertically.The Cluster Autoscaler in Kubernetes automatically requests additional nodes from the cloud provider when a new pod cannot be scheduled due to lack of resources on existing nodes. It also de-provisions underutilized nodes for longer periods. The Autoscaler examines available node groups, selects the best one that can fit the unscheduled pod, and increases its size or adds another node to it.When scaling a Kubernetes cluster, the Cluster Autoscaler monitors node utilization and CPU/Memory requests of running pods. If a node is underutilized (CPU/memory < 50%), it\\'s considered unnecessary unless system or unmanaged pods are running on it. The Autoscaler marks the node as unschedulable and evicts its pods before shutting it down. Scaling up involves identifying an available node type, selecting one, and scaling that group to fit the pod.Automatic scaling of pods and cluster nodes can be enabled on GKE, GCE, AWS, and Azure through Cluster Autoscaler. On GKE, use gcloud command with --enable-autoscaling flag. On GCE, set environment variables KUBE_ENABLE_CLUSTER_AUTOSCALER=true, KUBE_AUTOSCALER_MIN_NODES=3, and KUBE_AUTOSCALER_MAX_NODES=5 before running kube-up.sh. The Cluster Autoscaler publishes its status to the cluster-autoscaler-status ConfigMap in the kube-system namespace. To limit service disruption during scale-down, manually cordon and drain nodes using kubectl commands.Kubernetes provides a way to specify the minimum number of pods that must always be available through the PodDisruptionBudget resource, especially for quorum-based clustered applications. The PDB resource contains a pod label selector and a number specifying the minimum or maximum number of pods that can be unavailable. It can also use percentages instead of absolute numbers. The Cluster Autoscaler and kubectl drain command will adhere to this resource, ensuring that evictions do not bring the number of such pods below the specified threshold.Kubernetes can scale not only pods but also cluster nodes automatically. HorizontalPodAutoscaler configures scaling based on CPU utilization or custom metrics. Vertical pod autoscaling is not possible yet. Cluster node auto-scaling is supported on cloud providers. Additionally, pods can be run one-off and deleted with kubectl run options.Kubernetes allows for advanced scheduling by specifying a node selector in the pod specification, or using taints and tolerations to keep pods away from certain nodes. Additional features include defining node affinity rules, co-locating pods, and keeping pods away from each other using pod anti-affinity.Node taints allow rejecting deployment of pods to certain nodes by adding taints without modifying existing pods, while tolerations enable pods to opt-in and use tainted nodes. A node\\'s taints can be displayed using kubectl describe node, showing a key-value pair with an effect, such as NoSchedule preventing pod scheduling unless tolerated.In Kubernetes, a pod can be scheduled to a tainted node by adding a toleration that matches the node\\'s taint. This allows system pods like kube-proxy to run on master nodes. A pod with no tolerations can only be scheduled to nodes without taints, and tolerations define how long a pod is allowed to run on nodes that aren\\'t ready or are unreachable.Kubernetes taints and tolerations allow you to label nodes with effects that can be tolerated by pods. Taints have three possible effects: NoSchedule, PreferNoSchedule, and NoExecute. Adding a NoExecute taint to a node will evict running pods that don\\'t tolerate it. To deploy production pods to tainted nodes, they must include tolerations in their manifests matching the key, value, and effect of the taint.Taints and tolerations can be used to control pod scheduling in Kubernetes. A taint is added to a node, and a matching toleration is added to a pod to allow it to run on that node. Tolerations can tolerate specific values or any value for a specific taint key. Taints can prevent new pods from running (NoSchedule), define unpreferred nodes (PreferNoSchedule), or evict existing pods (NoExecute). This allows for partitioning a cluster into separate partitions, controlling pod scheduling based on node type.Kubernetes can wait up to 300 seconds (5 minutes) after a node failure before rescheduling a pod. This delay can be adjusted by adding tolerations to the pod\\'s spec, and is currently an alpha feature. Node affinity allows scheduling pods only to specific subsets of nodes, replacing the initial node-selector mechanism which was simpler but didn\\'t offer everything needed.Node affinity allows specifying hard requirements or preferences for pods to run on certain nodes, based on their labels. Kubernetes uses these labels to select nodes, and by understanding default node labels, you can create rules that attract pods to specific nodes.The document discusses advanced scheduling in Kubernetes, specifically the use of node selectors and affinity rules to deploy pods on nodes with specific labels. The nodeSelector field specifies a simple rule for deployment, while the nodeAffinity field provides more expressive and detailed rules, including requiredDuringSchedulingIgnoredDuringExecution, which ensures the pod is scheduled only on nodes meeting specified criteria during scheduling but ignores execution.Node affinity in Kubernetes allows pods to be scheduled to nodes with specific labels, such as gpu=true. The nodeSelectorTerms field defines expressions that a node\\'s labels must match for the pod to be scheduled. Node affinity also enables prioritizing nodes during scheduling through the preferredDuringSchedulingIgnoredDuringExecution field, allowing for preference of certain zones or machines over others.Node affinity allows scheduling of pods to machines reserved for deployments, and can be specified by labeling nodes with availability zone and share type labels. This can be demonstrated using kubectl label command to label nodes as dedicated or shared within specific zones. A Deployment can then be created that prefers dedicated nodes in a particular zone.You\\'re defining a node affinity preference for pods to be scheduled on nodes with specific labels (availability-zone=zone1 and share-type=dedicated) with the first preference having a weight of 80 and the second one having a weight of 20, indicating that zone preference is more important than dedicated node preference in case of scheduling conflicts.In a two-node Kubernetes cluster, deploying a Deployment shows most pods deployed to one node due to prioritization functions like Selector-SpreadPriority. Scaling the Deployment up spreads pods evenly between nodes without node affinity preferences. Pod affinity allows specifying the affinity between pods themselves, such as keeping frontend and backend pods close together by configuring them to deploy on the same node.A Deployment is created with a podAffinity rule that requires frontend pods to be deployed on the same node as backend pods, which have an app=backend label. This ensures that all frontend pods will be scheduled only to the node where the backend pod was scheduled to, creating a co-locating requirement for the two types of pods.This chapter discusses advanced scheduling in Kubernetes using pod affinity rules. The Scheduler first finds all pods that match a labelSelector defined in a pod\\'s configuration and schedules it to the same node. If a pod with affinity rules is deleted, the Scheduler will reschedule it to the same node to maintain consistency, even if no rules are defined on the deleted pod.Pods can be co-located using pod affinity and anti-affinity, prioritizing scheduling to a node based on shared labels or topology keys such as zone or region. For example, setting topologyKey to failure-domain.beta.kubernetes.io/zone allows pods to be deployed in the same availability zone, while setting it to failure-domain.beta.kubernetes.io/region schedules them in the same geographical region.In Kubernetes, scheduling preferences can be expressed using label selectors and node or pod affinities. The Scheduler matches pods based on these rules, but if a match is not found, it will schedule the pod elsewhere. Pod affinity can also specify preferred nodes while allowing for flexibility if those nodes are unavailable.Co-locating pods with pod affinity and anti-affinity can be achieved by defining a weight for each rule, specifying the topologyKey and labelSelector. This allows the Scheduler to prefer nodes where pods with a certain label are running. For example, in a deployment of 5 replicas, the Scheduler may deploy four pods on the same node as the backend pod, and one pod on another node.This chapter discusses advanced scheduling in Kubernetes, specifically how to schedule pods away from each other using pod anti-affinity. This is useful when two sets of pods interfere with each other\\'s performance if they run on the same node. Pod anti-affinity can be used to spread pods across different availability zones or regions to prevent a whole zone failure from bringing down the service completely.To force frontend pods to be scheduled on different nodes, you can use pod anti-affinity. This is achieved by configuring the podAntiAffinity property in the deployment\\'s spec section and making it match the same pods that the deployment creates. A soft requirement can also be used with preferredDuringSchedulingIgnoredDuringExecution property if scheduling two frontend pods on the same node is not a problem, otherwise requiredDuringSchedulingIgnoredDuringExecution should be used for hard requirements.This chapter explores advanced scheduling techniques in Kubernetes, including pod affinity, topologyKey, taints, node affinity, pod affinity/anti-affinity, and their use cases. It discusses how to ensure pods aren\\'t scheduled to certain nodes or are only scheduled to specific nodes based on node labels or pod requirements.This chapter covers best practices for developing apps on Kubernetes, including understanding typical application resources, adding lifecycle hooks, properly terminating apps without breaking client requests, making apps easy to manage, using init containers, and developing locally with Minikube.A typical application manifest contains one or more Deployment and/or StatefulSet objects, including a pod template with containers, liveliness probes, readiness probes, and Services for exposing pods to others. The pod templates reference Secrets for pulling container images and those used directly by the process running inside the pods. Other resources like ReplicaSets, Endpoints, Horizontal Pod Autoscalers, and Ingress are also defined in the app manifest.A pod\\'s lifecycle is crucial to understand, as it can be killed and relocated by Kubernetes at any time due to scale-down requests or node relocations. This differs from traditional VMs where apps are rarely moved, giving operators more control over the app in its new location. Kubernetes controllers automatically create objects such as Endpoints, ReplicaSets, and pods, which are often labeled and annotated for organization and metadata purposes.Kubernetes application developers should ensure their apps can be moved and restarted without issues, considering IP and hostname changes. Stateful apps should use StatefulSets for persistence. Apps writing data to disk may lose it when restarted or rescheduled unless using persistent storage. Volumes can preserve data across container restarts, making them useful for caching results or other sensitive data.A pod\\'s lifecycle involves a container and its process, which writes to a filesystem with a writable layer on top of read-only layers and image layers. When the container crashes or is killed, a new container starts with a new writable layer, losing all previous files. Using a volume mounts allows data persistence across container restarts, enabling a new process to use preserved data in the volume.Using volumes to preserve files across container restarts can be a double-edged sword, as it may lead to continuous crash loops if data gets corrupted. Similarly, dead or partially dead pods are not automatically removed and rescheduled by ReplicaSet controllers, even if they\\'re part of a desired replica count, resulting in a lower actual replica count.A Kubernetes ReplicaSet with pods that keep crashing due to a container issue is created. The pod\\'s status shows the Kubelet delaying the restart, but no action is taken by the controller since the current replicas match the desired ones, showing three running replicas.Kubernetes pods can include init containers to initialize the pod and delay the start of main containers. Init containers are executed sequentially and only after completion do main containers start. An example shows an init container checking if a service is responding before allowing the main container to start, using a busybox image and a while loop to continuously check until the service is up.When deploying a pod, its init container is started first, shown by kubectl get. The main container won\\'t run until dependencies are met, such as services being ready. It\\'s best to write apps that handle internal dependencies and use readiness probes to signal unavailability. Lifecycle hooks can also be defined per container for post-start and pre-stop execution, similar to liveness and readiness probes.A post-start hook in Kubernetes is executed immediately after a container\\'s main process is started. It runs in parallel with the main process and can perform additional operations without modifying the application source code. The hook affects the container by keeping it in the Waiting state until completion, and if it fails or returns a non-zero exit code, the main container will be killed. A pod manifest containing a post-start hook is shown, executing a shell script as part of the container lifecycle.A pod\\'s lifecycle includes post-start and pre-stop hooks, which can be used to execute commands or initiate a graceful shutdown. Post-start hooks are executed immediately after a container is started, while pre-stop hooks are executed before a container is terminated. If a hook fails, it will display an error message in the pod\\'s events. To troubleshoot failed hooks, one can use kubectl describe pod and exec into the container to examine log files or mount an emptyDir volume for logging purposes.Kubernetes pre-stop hooks can be used to perform actions before a pod is terminated, such as sending an HTTP GET request. However, if the hook fails or returns an error, it will not prevent the container from being terminated. It\\'s also important to note that using a pre-stop hook solely to send a SIGTERM signal to an app is not necessary, and instead the shell should be configured to pass the signal to the app process. A pre-stop hook YAML snippet example is provided for performing an HTTP GET request in a pod.A pod\\'s lifecycle involves container termination, triggered by the deletion of the Pod object through the API server. The Kubelet terminates each container, running pre-stop hooks and sending SIGTERM signals to main processes. If containers don\\'t shut down cleanly within the configured termination grace period, they are forcibly killed with a SIGKILL signal.The termination grace period for Kubernetes pods can be configured in the pod spec or overridden when deleting the pod. It\\'s essential to set a sufficient time for processes to finish cleaning up before being killed. Applications should react to SIGTERM signals by starting their shut-down procedure and terminating within a fixed amount of time, using pre-stop hooks if necessary.When a Kubernetes pod receives a termination signal, it should not start migrating its data immediately. Instead, a dedicated pod or CronJob resource can be used to periodically check for orphaned data and migrate it to remaining pods, ensuring data is not lost in case of node failure or application upgrade.To handle client requests properly, Kubernetes apps need to follow rules to prevent broken connections when pods are starting up or shutting down. This involves ensuring each connection is handled properly at pod startup by adding an HTTP GET readiness probe that returns success only when the app is ready to accept incoming requests.When a pod is deleted, the API server modifies etcd and notifies watchers, including Kubelet and Endpoints controller. Two parallel sequences of events occur: A) containers on the worker node stop, kube-proxy removes the pod as an endpoint, and iptables updates; B) the Pod\\'s deletion notification is sent to the client, the endpoints controller removes the pod from its list, and the kubelet removes the pod from iptables.When a pod is deleted in Kubernetes, two sequences of events occur in parallel: the Kubelet shuts down the app\\'s process and removes it from iptables rules. The shutdown sequence is relatively short, while updating iptables rules involves a longer chain of events, including notification to the Endpoints controller, API server, and kube-proxy.The pod needs to keep accepting connections after receiving the termination signal until all kube-proxies and other components have finished updating their rules. A long-enough delay, such as 5-10 seconds, should be added before shutting down the pod to ensure a smooth user experience.To properly shut down an application, wait for a few seconds, then stop accepting new connections, close inactive keep-alive connections, wait for active requests to finish, and finally shut down completely. A pre-stop hook can be added to wait a few seconds before shutting down, preventing broken connections and frustrating the user with lingering pod listings.To make apps easy to run and manage in Kubernetes, focus on creating small, minimal container images without unnecessary cruft. This includes using the FROM scratch directive in Dockerfiles and avoiding the latest image tag, which can cause versioning issues. Proper tagging of images and using imagePullPolicy wisely is also crucial to ensure smooth deployment and scaling.Use tags with version designators, label resources with multiple dimensions, add annotations for resource descriptions and dependencies. Set imagePullPolicy to Always only in development. Use labels and annotations to manage resources and show dependencies between pods, and include contact information and build/version metadata.In Kubernetes, you can make triage easier by having a container write a termination message to a specific file before exiting. This message is then shown in the output of kubectl describe pod without needing to inspect container logs. The default path for this message is /dev/termination-log but can be overridden with the terminationMessagePath field in the container definition. An example of this is provided, where a busybox container writes a message to /var/termination-reason and dies immediately, causing the pod\\'s status to show CrashLoopBackOff, which can then be seen using kubectl describe.The chapter discusses best practices for developing apps in Kubernetes. It highlights the importance of writing app-specific status messages to a file or using the standard output, which can be easily viewed with the `kubectl logs` command. If an application crashes and is replaced, the new container\\'s log is displayed, but using the `--previous` option shows the previous container\\'s logs. The chapter also covers copying files from/to containers using `kubectl cp`, including logging files.Kubernetes provides no centralized logging by itself, requiring additional components to store and analyze container logs. Deploying a centralized logging solution like the EFK stack (FluentD, ElasticSearch, Kibana) is easy through YAML/JSON manifests or Helm charts, allowing for historical log examination and trend analysis.The chapter discusses best practices for developing apps in Kubernetes. It highlights the importance of handling multi-line log statements by outputting JSON logs instead of plain text, which can be stored and shown as a single entry in Kibana. The solution is to keep outputting human-readable logs while writing JSON logs to a file and having them processed by FluentD. Additionally, it suggests running apps outside of Kubernetes during development on local machines or IDEs without the need for containerization. It also advises connecting to backend services manually or temporarily making them accessible externally using NodePort or LoadBalancer-type Services.When developing an app that requires access to the Kubernetes API server, you can easily talk to the API server from outside the cluster during development using ServiceAccount\\'s token or ambassador container. Alternatively, you can run your app inside a Kubernetes cluster using Minikube, which provides a single worker node but is valuable for trying out your app in Kubernetes and developing resource manifests. You can also mount local files into the Minikube VM and containers using hostPath volume or use the Docker daemon inside the Minikube VM to build container images.Minikube allows developers to build apps locally without pushing images to a registry. Environment variables can be set using \"eval $(minikube docker-env)\" to use the Minikube VM\\'s Docker daemon. Images can also be built locally and copied over to the Minikube VM, or combined with a proper Kubernetes cluster for development and deployment.The document discusses best practices for development and testing in Kubernetes, including using tools like kube-applier to manage running apps through version control systems. It also introduces Ksonnet as an alternative to writing YAML/JSON manifests, allowing users to define parameterized JSON fragments and build full manifests with much less code.The chapter emphasizes the importance of using Ksonnet and Jsonnet for consistent manifests, employing Continuous Integration and Continuous Delivery (CI/CD) pipelines with tools like Fabric8 or Google Cloud Platform\\'s online labs to automate deployment, and understanding Kubernetes\\' distributed nature and eventual consistency model. It also highlights the need for apps to shut down properly without breaking client connections.The document provides small tips for app management by keeping image sizes small, adding annotations and labels, and making termination reasons clear. It also teaches how to develop Kubernetes apps locally or in Mini-kube before deploying them on a multi-node cluster. Finally, it explains how to extend Kubernetes with custom API objects and controllers, enabling the creation of Platform-as-a-Service solutions.This chapter covers extending Kubernetes by defining custom API objects, creating controllers for those objects, and adding custom API servers. It also explores how others have built Platform-as-a-Service solutions on top of Kubernetes, including Red Hat\\'s OpenShift Container Platform and Deis Workflow.Kubernetes allows defining custom API objects through CustomResourceDefinitions (CRD) which is a description of the custom resource type. A CRD can be posted to the Kubernetes API server, enabling users to create instances of the custom resource. Each CRD typically has an associated controller that makes something tangible happen in the cluster, such as spinning up a new web server pod and exposing it through a Service when creating an instance of the Website resource.Kubernetes custom resource is created by posting a CustomResourceDefinition to the API server. A custom resource definition object has apiVersion, kind, metadata name and spec with scope as Namespaced. It\\'s used to make Kubernetes accept instances of custom Website resources which will result in creation of Service and Pod for each instance.A custom API object called Website is defined with group: extensions.example.com, version: v1, and names: kind: Website. After posting the descriptor to Kubernetes, instances of the custom Website resource can be created. A YAML manifest for a Website resource instance is shown, specifying apiVersion: extensions.example.com/v1, kind: Website, metadata: name: kubia, and spec: gitRepo: https://github.com/luksa/kubia-website-example.git.You can now store, retrieve and delete custom resources through the Kubernetes API server after creating a CustomResourceDefinition object. These objects don\\'t do anything yet and you\\'ll need to create a controller to make them functional.A custom API object, such as a Website object, can be created to trigger the spinning up of a web server serving Git repository contents. A custom controller is needed to automate this process by watching the API server for Website object creation and creating a Deployment and Service for each one. This allows the Pod to be managed and survive node failures.The Website controller connects to the kubectl proxy process, which forwards requests to the API server, allowing the API server to send watch events for every change to any Website object. When a new Website object is created, the API server sends an ADDED event, triggering the controller to create a Deployment and Service object with a template for a pod containing an nginx server and a git-sync process, exposing the web server through a random port on each node.A custom API controller is created to manage Website resources, which are deleted by the API server and watched through periodic re-listing. The controller runs as a pod in Kubernetes for development and deployment, using a Deployment resource to ensure proper execution.A Kubernetes Deployment is created with two containers, main and proxy, running under a special ServiceAccount. The controller watches for events and creates resources as needed. A ClusterRoleBinding is required to enable access control. The deployment can be tested by creating a kubia Website resource and checking the controller\\'s logs for watch event and resource creation.A custom API object was successfully created by the controller, which received an ADDED event and created a Service and a Deployment for the kubia-website Website. The API server responded with a 201 Created response, and the resulting Pod was also created. However, users can create invalid Website objects without validation schema in the CustomResourceDefinition. The controller can only validate the object when it receives it in a watch event, and if invalid, write an error message to the Website object.To extend Kubernetes, validation of custom objects was introduced in version 1.8, enabling the API server to validate custom objects immediately. Alternatively, implementing a custom API server and integrating it with the main Kubernetes API server through API server aggregation allows for more control over custom object handling. This approach eliminates the need for a CRD and enables direct implementation of custom object types within the custom API server.Kubernetes can be extended by creating Custom Resource Definitions (CRDs) in the core API server\\'s etcd store. A custom API server can be added to a cluster by deploying it as a pod and exposing it through a Service, then integrating it into the main API server using an APIService resource. This allows client requests to be forwarded to the custom API server for specific resources. Additionally, custom clients can be built to create custom objects, making deployment easier. The Kubernetes Service Catalog API server will also be added through API server aggregation, enabling pods to consume services.Kubernetes\\' Service Catalog is a feature that allows users to provision services without dealing with underlying components like Pods and Services. It uses four generic API resources: ClusterServiceBroker, ClusterServiceClass, ServiceInstance, and ServiceBinding. A cluster admin creates a ClusterServiceBroker resource for each service broker, which lists available services. Users create a ServiceInstance resource for the required service, and a ServiceBinding to bind it to client pods.The Kubernetes Service Catalog is a distributed system composed of three components: API Server, etcd, and Controller Manager. The Service Catalog API server stores resources created by posting YAML/JSON manifests to itself or uses CustomResourceDefinitions in the main API server. Controllers running in the Controller Manager talk to the Service Catalog API server and provision services using external service brokers registered with ServiceBroker resources.A cluster administrator can register external ServiceBrokers in the Service Catalog through the OpenServiceBroker API, which provides operations for provisioning, updating, and deprovisioning services. A ClusterServiceBroker resource manifest is posted to the Service Catalog API to register a broker, and a controller connects to the specified URL to retrieve the list of services this broker can provision, creating ClusterServiceClass resources for each service type.The Kubernetes Service Catalog allows users to retrieve a list of available services in a cluster using kubectl get serviceclasses. ClusterServiceClasses are similar to StorageClasses, but allow users to select the type of service they want to use. An example is shown for a PostgreSQL database, with two plans: free and premium. The ClusterServiceClass is provided by the database-broker broker.To provision a service instance, create a Service-Instance resource with ClusterServiceClass and plan specified. The Service Catalog will contact the broker, passing on chosen class, plan, and parameters. The broker then provisions the service according to its configuration, potentially spinning up a new instance of a database or running it in a VM. Successful provisioning can be checked by inspecting the status section of the created ServiceInstance.A Kubernetes Service Catalog is extended by binding a provisioned ServiceInstance to pods using a ServiceBinding resource. A Secret is created with necessary credentials, which can be manually mounted into pods for access to the ServiceInstance.The Service Catalog allows service providers to expose services in any Kubernetes cluster by registering a broker. A Secret can be created with credentials for connecting to a service instance, and can be used by multiple pods. Once no longer needed, the ServiceBinding can be deleted, which will delete the Secret and perform an unbinding operation on the service broker. Additionally, if not needed, the ServiceInstance resource should also be deleted to deprovision the service.Kubernetes is becoming a widely accepted foundation for Platform-as-a-Service (PaaS) offerings. Platforms built on top of Kubernetes, such as Deis Workflow and Red Hat\\'s OpenShift, provide features like easy provisioning, automated rollouts and scaling, user and group management, and additional API objects. Red Hat OpenShift automates application image building and deployment without requiring a Continuous Integration solution.OpenShift provides powerful user management features, allowing users to access certain Projects (Kubernetes Namespaces with additional annotations) and granting access by a cluster administrator. Application Templates in OpenShift are parameterizable JSON or YAML manifests that can be instantiated with placeholder values replaced with parameter values.OpenShift provides pre-fabricated templates for complex applications, allowing users to quickly run them with minimal arguments. It also enables automatic deployment of newly built images by creating a DeploymentConfig object and pointing it to an ImageStream. BuildConfigs can trigger builds immediately after changes are committed to the source Git repository, building container images without manual intervention.Kubernetes can be extended with features from OpenShift, such as DeploymentConfig, which provides pre- and post-deployment hooks and creates ReplicationControllers instead of ReplicaSets. Routes are used to expose Services externally, providing additional configuration for TLS termination and traffic splitting. Minishift is available for trying out OpenShift, along with OpenShift Online Starter. Deis Workflow, also built on Kubernetes, provides a PaaS with features like BuildConfigs and DeploymentConfigs.Deis Workflow is a tool built on top of Kubernetes that creates services and replication controllers, providing developers with a simple environment. Deploying new versions of an app can be triggered by pushing changes with \\'git push deis master\\'. The Helm tool is a package manager for Kubernetes, allowing the deployment and management of application packages called Charts.When extending Kubernetes, instead of writing manifests for apps like PostgreSQL or MySQL, check if someone has prepared a Helm chart for it. Once installed, running the app takes a single command and creates necessary Deployments, Services, Secrets, and PersistentVolumeClaims.This final chapter shows how to extend Kubernetes\\' functionalities by registering custom resources, implementing custom controllers, and using API aggregation, Service Catalog, and platforms-as-a-service built on top of Kubernetes. A package manager called Helm is also introduced for deploying existing apps without requiring resource manifests.To switch between Minikube and Google Kubernetes Engine (GKE) clusters using kubectl, simply run \\'minikube start\\' to configure kubectl for Minikube or use \\'gcloud container clusters get-credentials my-gke-cluster\\' to set up GKE. Minikube reconfigures kubectl every time you start the cluster, making it easy to switch between the two.To switch between different Kubernetes clusters or namespaces without specifying the --namespace option every time, configure the kubeconfig file\\'s location using the KUBECONFIG environment variable. This file contains four sections: clusters (list of available clusters), users (list of user credentials), contexts (defined by a cluster and a user), and current-context (the currently used context). By listing multiple config files in KUBECONFIG, kubectl can use them all at once.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_doc=\"\"\n",
    "for doc in document_txt:\n",
    "    full_doc=full_doc+doc\n",
    "\n",
    "full_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fec4588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot load json for this raw description:It seems like you've provided a comprehensive text on how to work with Kubernetes Service Catalog API server and provision services using external service brokers registered with ServiceBroker resources.\n",
      "\n",
      "To summarize the key points:\n",
      "\n",
      "1. **Registering External Service Brokers**: A cluster administrator can register external ServiceBrokers in the Service Catalog through the OpenServiceBroker API.\n",
      "2. **Provisioning Services**: To provision a service instance, create a Service-Instance resource with ClusterServiceClass and plan specified.\n",
      "3. **Binding to Pods**: Bind a provisioned ServiceInstance to pods using a ServiceBinding resource.\n",
      "4. **Deprovisioning Services**: Delete the ServiceInstance resource to deprovision the service.\n",
      "5. **Using Kubernetes as a PaaS Foundation**: Kubernetes is becoming a widely accepted foundation for Platform-as-a-Service (PaaS) offerings, such as Deis Workflow and Red Hat's OpenShift.\n",
      "6. **Extending Kubernetes with Features from OpenShift**: Features like DeploymentConfig, Routes, and BuildConfigs can be extended to Kubernetes.\n",
      "7. **Using Helm Package Manager**: Instead of writing manifests for apps, use Helm charts to deploy existing apps without requiring resource manifests.\n",
      "\n",
      "Additionally, the text provides information on how to switch between different Kubernetes clusters or namespaces using the KUBECONFIG environment variable.\n",
      "\n",
      "If you'd like me to clarify any specific points or provide further assistance, please let me know!\n"
     ]
    }
   ],
   "source": [
    "final_summary=concise_summary(full_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ef80381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated. Containers within a pod share certain resources like network interfaces and IPC namespaces, but have fully isolated filesystems unless shared using a Kubernetes Volume concept.\n",
      "Summary for page:0\n",
      "A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated.\n",
      "Summary\n",
      "Kubernetes pods are logical hosts that behave like physical hosts or VMs, with processes running in the same pod behaving like processes on the same machine. Each pod has its own IP address and can communicate directly with other pods through a flat network, without NAT gateways. Pods should be organized by app, with each containing tightly related components or processes, allowing for as many pods as needed without significant overhead.\n",
      "Summary for page:1\n",
      "A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated.\n",
      "Summary\n",
      "A multi-tier application consisting of frontend and backend components should not be configured as a single pod but rather split into multiple pods to enable individual scaling and utilize computational resources on multiple nodes. This approach allows for separate scaling requirements for frontend and backend components, making it more efficient and suitable for applications with diverse resource needs.\n",
      "Summary for page:2\n",
      "A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated.\n",
      "Summary\n",
      "Pods in Kubernetes are groups of containers that can be run together, like a web server and a sidecar container for downloading content. To decide when to use multiple containers in a pod, ask yourself: do they need to run together, represent a single whole, or must they be scaled together? Typically, containers should be run in separate pods unless a specific reason requires them to be part of the same pod.\n",
      "Summary for page:3\n",
      "A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated.\n",
      "Summary\n",
      "You can create pods by posting a JSON or YAML manifest to the Kubernetes REST API endpoint. This method allows configuration of all properties, but requires knowledge of the Kubernetes API object definitions. Alternatively, you can use commands like kubectl run, but they limit the configurable properties. The YAML descriptor for an existing pod can be obtained using kubectl get with the -o yaml option, showing metadata and specification details.\n",
      "Summary for page:4\n",
      "A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated.\n",
      "Summary\n",
      "A Kubernetes Pod is a logical host for one or more application containers. It consists of metadata (name, namespace, labels) and spec (containers, volumes), with optional detailed status information. Key elements include terminationMessagePath, dnsPolicy, restartPolicy, serviceAccount, and volumeMounts.\n",
      "Summary for page:5\n",
      "A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated.\n",
      "Summary\n",
      "A Kubernetes pod can be created using a YAML or JSON descriptor, which typically consists of three parts: metadata, spec, and status. The spec section defines the container's image, name, and ports, with specifying ports being informational only. A simple example is shown in kubia-manual.yaml, where a single container based on luksa/kubia image listens on port 8080.\n",
      "Summary for page:6\n",
      "A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated.\n",
      "Summary\n",
      "A Kubernetes pod is a collection of containers that run on a host, and can be described using a manifest. The pod spec contains attributes such as hostname, IP addresses, ports, and volumes that can be mounted by containers. Using kubectl explain, one can discover possible API object fields and drill deeper to learn more about each attribute.\n",
      "Summary for page:7\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "To create a pod from a YAML file, use kubectl create -f command. After creating the pod, you can retrieve its full YAML or JSON definition using kubectl get po <pod_name> -o yaml/json commands. You can also view application logs by tailing the container's standard output and error streams.\n",
      "Summary for page:8\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "This chapter discusses pods in Kubernetes, focusing on retrieving logs from containers running within a pod. The container runtime redirects streams to files, allowing users to view logs by running `docker logs <container id>`. However, Kubernetes provides an easier way using `kubectl logs`, which can be used to retrieve logs from a pod without the need for SSH access. Additionally, if a pod contains multiple containers, the user must specify the container name when retrieving logs. The chapter also touches on centralized logging and port forwarding as methods to connect to a pod for testing and debugging purposes.\n",
      "Summary for page:9\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Kubernetes allows port forwarding to a specific pod through the `kubectl port-forward` command, enabling direct access for debugging or testing purposes. This can be achieved by running `$ kubectl port-forward kubia-manual 8888:8080` and sending an HTTP request using `curl localhost:8888`. This method is effective for testing individual pods, especially in microservices architectures where many pods need to be categorized and managed.\n",
      "Summary for page:10\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Kubernetes allows running multiple copies of the same component and different versions concurrently, which can lead to hundreds of pods without organization. To manage this, labels are used to organize pods and other Kubernetes resources into smaller groups based on arbitrary criteria, allowing developers and administrators to easily identify and operate on specific pods or groups with a single action.\n",
      "Summary for page:11\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Adding labels to pods in a Kubernetes system allows for easy organization and understanding of the system's structure. Labels can specify which app or microservice a pod belongs to, as well as whether it's a stable, beta, or canary release. By using these labels, developers and ops personnel can easily see where each pod fits in, making it easier to manage complex microservices architectures.\n",
      "Summary for page:12\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "This chapter is about pods in Kubernetes, specifically running containers and adding/removing labels. Labels can be added to or modified on existing pods using kubectl label command. The --overwrite option is required when changing existing labels. Examples of labeling a new pod, viewing labels with kubectl get po --show-labels, and modifying labels on an existing pod are shown.\n",
      "Summary for page:13\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Label selectors allow selecting subsets of pods based on labels. A label selector can filter resources by key, value, or not equal to a specified value. Examples include listing pods with creation_method=manual, env label, or no env label.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:14\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "This chapter discusses Kubernetes Pods, specifically focusing on running containers in a cluster. Label selectors are used to identify and select pods based on labels, with examples including creation_method!=manual, env in (prod,devel), and app=pc for selecting the product catalog microservice pods. Multiple conditions can be combined using comma-separated criteria, as shown in the selector app=pc,rel=beta. Label selectors are not only used for listing pods but also for performing actions on a subset of all pods.\n",
      "Summary for page:15\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "In Kubernetes, using labels and selectors is a way to constrain pod scheduling without specifying exact node placement. This allows for flexible scheduling based on node requirements, such as hardware infrastructure or GPU acceleration. Labels can be applied to nodes, and selectors can be used to match those labels, ensuring that pods are scheduled to nodes that meet specific criteria, while maintaining the decoupling of applications from infrastructure.\n",
      "Summary for page:16\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Labels can be attached to Kubernetes objects like pods and nodes. Using labels, the ops team categorizes new nodes by hardware type or features like GPU availability. To schedule a pod that requires a GPU, create a YAML file with a node selector set to gpu=true and use kubectl create -f to deploy the pod.\n",
      "Summary for page:17\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Pods can be annotated with labels and annotations. Labels are key-value pairs used for identification and grouping, while annotations hold larger pieces of information primarily meant for tools. Annotations are automatically added by Kubernetes or manually by users and are useful for adding descriptions, specifying creator names, and introducing new features. The importance of label selectors will become evident in future chapters on Replication-Controllers and Services.\n",
      "Summary for page:18\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Kubernetes pods can have labels and annotations, where labels are short and used for organization, while annotations can contain large blobs of data up to 256KB. Annotations like kubernetes.io/created-by were deprecated in version 1.8 and removed in 1.9. Annotations can be added or modified using the kubectl annotate command, and it's recommended to use unique prefixes to prevent key collisions.\n",
      "Summary for page:19\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Kubernetes groups objects into namespaces, which provide a scope for object names and allow for separate, non-overlapping groups. Namespaces enable operating within one group at a time and using the same resource names multiple times across different namespaces. They can be used to split complex systems, separate resources in multi-tenant environments, or divide resources into production, development, and QA environments.\n",
      "Summary for page:20\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Namespaces in Kubernetes enable separation of resources into non-overlapping groups, isolating them from other users' resources. They can be created by posting a YAML file to the Kubernetes API server using kubectl create -f or kubectl create namespace command.\n",
      "Summary for page:21\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Namespaces allow grouping resources and can be created using kubectl create command or by posting YAML manifest to API server. Resources in other namespaces can be managed by adding namespace entry to metadata section or specifying namespace with kubectl create command. Namespaces provide isolation for objects, but do not guarantee network isolation between pods across different namespaces.\n",
      "Summary for page:22\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Pods in Kubernetes can communicate with each other within a namespace. To stop and remove pods, use kubectl delete command. Pods can be deleted by name, label selector, or even deleting the whole namespace. When deleting a pod, Kubernetes sends a SIGTERM signal to shut down containers, and if they don't respond, a SIGKILL signal is sent. It's essential for processes to handle the SIGTERM signal properly.\n",
      "Summary for page:23\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "To delete all pods in a namespace, use the command $ kubectl delete po --all. This will delete all running and terminating pods in the current namespace. Alternatively, you can delete a specific pod by its name or delete all pods with a certain label using the label selector.\n",
      "Summary for page:24\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Kubernetes pods run multiple containers as one entity, with kubectl commands like run and delete creating ReplicationControllers that manage pods. Deleting all resources in a namespace can be done with kubectl delete all --all, but note that some resources like Secrets are preserved and need to be deleted explicitly.\n",
      "Summary for page:25\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Pods can run multiple processes similar to physical hosts. They have YAML/JSON descriptors that define their specification and current state. Labels and selectors help organize and perform operations on multiple pods. Annotations attach data to pods, while namespaces allow different teams to use the same cluster as separate Kubernetes clusters. The kubectl explain command provides information on resources.\n",
      "Summary for page:26\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Kubernetes manages pods automatically, using resources like Replication-Controllers and Deployments to create and manage pods. This chapter focuses on keeping pods healthy, running multiple instances of the same pod, automating rescheduling after a node fails, scaling pods horizontally, running system-level pods, and batch jobs, as well as scheduling periodic or future tasks.\n",
      "Summary for page:27\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Kubernetes checks if a container is alive through liveness probes and restarts it if it fails. Liveness probes can be specified for each container in a pod's specification. Kubernetes periodically executes the probe and restarts the container if it fails. This ensures that applications are restarted even if they stop working without crashing, such as due to memory leaks or infinite loops.\n",
      "Summary for page:28\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "A liveness probe checks if a container is running correctly. A successful probe returns a 2xx or 3xx HTTP response code, while a failed probe returns an error code or no response at all. The chapter demonstrates creating a new pod with an HTTP GET liveness probe for a Node.js app that intentionally fails after five requests.\n",
      "Summary for page:29\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "The document explains how Kubernetes uses liveness probes to keep pods healthy. An httpGet liveness probe sends HTTP GET requests to a path on port 8080, and if the status code becomes 500, Kubernetes restarts the container. The document also demonstrates this by creating a pod with a liveness probe, showing it gets restarted after about a minute and a half, and describes how to obtain the application log of a crashed container using kubectl logs --previous.\n",
      "Summary for page:30\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "This page discusses replication and other controllers in Kubernetes, specifically the liveness probe which checks if a container is running correctly. If the container fails the probe, it will be killed and re-created. The page also explains how to configure additional properties of the liveness probe, such as delay, timeout, period, and initial delay. An example YAML file is provided to demonstrate how to set an initial delay for the liveness probe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:31\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "To keep pods healthy, it's essential to set an initial delay for liveness probes. This prevents probes from failing as soon as the app starts, leading to unnecessary restarts. A liveness probe should check if the server is responding and ideally perform internal status checks on vital components. It's crucial to ensure the /health endpoint doesn't require authentication and only checks internals of the app, not external factors.\n",
      "Summary for page:32\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "A ReplicationController in Kubernetes ensures its pods are always kept running by creating replacement pods if one disappears. Liveness probes shouldn't use too many computational resources and should be executed relatively often to keep containers running. Kubernetes will retry a probe several times before considering it a failed attempt, so implementing a retry loop is unnecessary.\n",
      "Summary for page:33\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "A ReplicationController monitors running pods and ensures the desired number of replicas matches the actual number. If too few, it creates new replicas; if too many, it removes excess replicas. It recreates lost pods when a node fails, but not manually created or changed pods.\n",
      "Summary for page:34\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "A ReplicationController's job is to maintain an exact number of pods that match its label selector by creating or deleting pods as needed, with three essential parts: a label selector, replica count, and pod template.\n",
      "Summary for page:35\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "A ReplicationController's replica count, label selector, and pod template can be modified at any time, but only changes to the replica count affect existing pods. Changes to the label selector or pod template have no effect on existing pods and are used as a 'cookie cutter' for new pods created by this ReplicationController. The controller ensures a pod is always running, creates replacement replicas when a cluster node fails, and enables easy horizontal scaling of pods.\n",
      "Summary for page:36\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "Kubernetes creates a Replication-Controller named kubia that ensures three pod instances match the label selector app=kubia. When there aren't enough pods, it creates new ones from the provided pod template. The API server verifies the ReplicationController definition and will not accept it if misconfigured. To prevent such scenarios, let Kubernetes extract the selector from the pod template.\n",
      "Summary for page:37\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "A ReplicationController is introduced, managing three pods and automatically spinning up new ones if any are deleted. The kubectl get command shows information about ReplicationControllers, including desired and actual pod numbers. Additional details can be obtained with the kubectl describe command, displaying the ReplicationController's name, namespace, selector, labels, annotations, replicas, and pod status.\n",
      "Summary for page:38\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "A ReplicationController in Kubernetes creates a new pod to replace one that has been deleted when it detects an inadequate number of running pods, triggered by events such as pod deletion or termination.\n",
      "Summary for page:39\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "A ReplicationController in Kubernetes automatically spins up new pods to replace those that are down when a node fails, as demonstrated by simulating a node failure on a three-node cluster. After shutting down the network interface of one node, the status is shown as NotReady, and the pods remain unchanged for several minutes before the ReplicationController creates a new pod to replace the downed ones.\n",
      "Summary for page:40\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "ReplicationController automatically manages pods based on a label selector and can spin up new pods if one fails or is removed from scope. A pod's labels can be changed to move it in or out of the ReplicationController's scope, but changing its labels does not delete it. The replication controller will notice if a managed pod is missing and spin up a new one to replace it.\n",
      "Summary for page:41\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "A ReplicationController doesn't care if labels are added to its managed pods. Changing a label on a managed pod makes it no longer match the controller's label selector, prompting the controller to start a new pod to bring the number back to three.\n",
      "Summary for page:42\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "A ReplicationController can spin up new pods to bring the number back up after one is removed, allowing for independent pod management and changing label selectors to control which pods are included in the controller's scope.\n",
      "Summary for page:43\n",
      "A Kubernetes pod is a collection of containers that run on a host, described using a manifest.\n",
      "Summary\n",
      "ReplicationControllers can modify their pod template at any time, but changes only affect new pods created after the modification. To change an existing pod, it must be deleted and a new one will be created based on the updated template.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:44\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "ReplicationControllers ensure a specific number of pod instances is always running. Scaling pods horizontally is trivial and can be done by changing the replicas field, either by using the command `kubectl scale` or by editing the ReplicationController's definition directly with `kubectl edit`. This allows scaling up or down with ease.\n",
      "Summary for page:45\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "A ReplicationController is updated when scaled up or down, and it immediately scales the number of pods to the desired state. Scaling is a matter of stating the desired state, not telling Kubernetes how to do it. Declarative approach makes interacting with a Kubernetes cluster easy. When deleting a ReplicationController through kubectl delete, the pods are also deleted unless managed by another controller.\n",
      "Summary for page:46\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "ReplicationControllers manage pods and keep them running without interruption, but can be deleted while keeping the pods running using the --cascade=false option. ReplicaSets are a newer resource that replaces ReplicationControllers completely and should be used instead. Deleting a ReplicationController leaves its pods unmanaged, but a new one can be created to manage them again.\n",
      "Summary for page:47\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "ReplicaSets are used instead of ReplicationControllers to manage replicas. A ReplicaSet behaves exactly like a ReplicationController but with more expressive pod selectors, allowing matching pods based on label presence or absence, and not just specific values. This enables a single ReplicaSet to match multiple sets of pods and treat them as a single group. The process of creating a ReplicaSet involves defining its YAML configuration, including the API version, kind, metadata, selector, replicas, template, and containers, which can be used to adopt orphaned pods created by a ReplicationController.\n",
      "Summary for page:48\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "ReplicaSets aren't part of the v1 API, so specify the proper apiVersion when creating a resource. To create a ReplicaSet, use kubectl create command with YAML file, then examine it with kubectl get and describe commands. The apiVersion property specifies the API group (apps) and actual API version (v1beta2), which categorizes Kubernetes resources into core and other groups.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:49\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "ReplicaSets are similar to ReplicationControllers, but with more expressive label selectors. The matchExpressions property allows for complex matching rules, such as requiring a pod to have a specific label or not having a certain label. This provides flexibility in selecting pods, making ReplicaSets more powerful than ReplicationControllers.\n",
      "Summary for page:50\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "ReplicaSets and ReplicationControllers are used for running a specific number of pods in a Kubernetes cluster, but DaemonSets are used to run one pod on each node, with exactly one instance per node, suitable for infrastructure-related pods like log collectors and resource monitors.\n",
      "Summary for page:51\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "A DaemonSet is used to run a pod on every node in a Kubernetes cluster, or on a subset of nodes specified by a node selector. It ensures a desired number of pods exist and creates a new pod instance if a new node is added or an existing node is deleted. Unlike ReplicaSets, DaemonSets do not need a replica count and will deploy pods even to unschedulable nodes.\n",
      "Summary for page:52\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "A DaemonSet is created for deploying managed pods. A YAML definition is written for the DaemonSet to run a mock ssd-monitor process on nodes with the 'disk=ssd' label. The DaemonSet will create an instance of the pod on each node that meets this condition.\n",
      "Summary for page:53\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "Creating a DaemonSet to run one pod on each node requires labeling nodes with the required label. Initially, the DaemonSet appears not to deploy pods due to missing labels. Adding the label to one or more nodes triggers the DaemonSet to create pods for matching nodes.\n",
      "Summary for page:54\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "A chapter about deploying managed pods using Replication and other controllers. It discusses running pods that perform a single completable task, which is different from continuous tasks like DaemonSets. The Job resource is introduced as a solution for this type of task, allowing pods to be rescheduled in case of node failure. An example is given of running a container image built on top of the busybox image, invoking the sleep command for two minutes.\n",
      "Summary for page:55\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "A Job resource in Kubernetes runs a single completable task. A YAML definition of a Job resource is provided, which defines a pod that will run an image invoking a process for 120 seconds and then exit. The restartPolicy specifies what to do when the processes finish. Jobs are part of the batch API group, version v1, and cannot use the default restart policy. Pods managed by Jobs are rescheduled until they finish successfully.\n",
      "Summary for page:56\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "A Job resource in Kubernetes can be used to deploy a single, managed pod with a restart policy of OnFailure or Never. Once created, the Job will run the pod until completion, at which point it will be marked as successful and the pod deleted. Jobs can also be configured to create multiple pods that run in parallel or sequentially by setting the completions and parallelism properties.\n",
      "Summary for page:57\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "A Job can be configured to run multiple pods sequentially or in parallel. To run pods sequentially, set completions to the number of times you want the Job's pod to run. For example, setting completions to 5 will create one pod at a time until five pods complete successfully. To run pods in parallel, specify how many pods are allowed to run with the parallelism Job spec property. This allows up to that many pods to be created and running at the same time.\n",
      "Summary for page:58\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "You can scale a Job's parallelism property while it's running using kubectl scale command. Additionally, you can limit a pod's time to complete by setting activeDeadlineSeconds in the pod spec. A Job can also be configured to retry failed pods up to 6 times before being marked as failed. Furthermore, Kubernetes supports scheduling Jobs periodically or once in the future through CronJobs, which create a Job resource at the specified time and run it according to the Job template.\n",
      "Summary for page:59\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "A CronJob resource creates Job objects based on a specified schedule. The schedule is set using the cron format (minute, hour, day of month, month, day of week), and can be configured to run jobs at specific intervals. The jobTemplate property defines the template for creating Job resources, which are created from the CronJob resource at approximately the scheduled time.\n",
      "Summary for page:60\n",
      "ReplicationControllers ensure a specific number of pod instances is always running.\n",
      "Summary\n",
      "A CronJob creates a single Job for each execution configured in the schedule, but can create two Jobs if run concurrently or none at all. To combat this, jobs should be idempotent and next job runs should perform work missed by previous runs. A startingDeadlineSeconds field can also be specified to ensure pods start running within a certain timeframe.\n",
      "Summary for page:61\n",
      "ReplicationControllers maintain a fixed number of pod instances.\n",
      "Summary\n",
      "ReplicationControllers are being replaced with ReplicaSets and Deployments which provide additional features, DaemonSets ensure every node runs a pod instance, Jobs schedule batch tasks while CronJobs handle future executions.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:62\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "Services enable clients to discover and communicate with pods, allowing them to respond to external requests. This chapter covers creating Service resources to expose a group of pods at a single address, discovering services in the cluster, exposing services to external clients, connecting to external services from inside the cluster, controlling pod readiness for service participation, and troubleshooting services.\n",
      "Summary for page:63\n",
      "Services enable clients to discover and communicate with pods, allowing them to respond to external requests.\n",
      "Summary\n",
      "A Kubernetes Service is a resource that provides a single, constant point of entry to a group of pods providing the same service. Each service has an IP address and port that never change while the service exists, allowing clients to connect without needing to know individual pod locations. This enables external clients to connect to frontend pods without worrying about IP changes and allows frontend pods to connect to backend database services with a stable address.\n",
      "Summary for page:64\n",
      "Services enable clients to discover and communicate with pods, allowing them to respond to external requests.\n",
      "Summary\n",
      "A service enables clients to discover and talk to pods, even if the pod's IP address changes. Services are created using label selectors, which specify which pods belong to the same set. A service can be backed by more than one pod, with connections load-balanced across all backing pods.\n",
      "Summary for page:65\n",
      "Services enable clients to discover and communicate with pods, allowing them to respond to external requests.\n",
      "Summary\n",
      "A Kubernetes service called kubia is created manually by posting a YAML descriptor, which exposes all pods matching the app=kubia label selector on port 80 and routes connections to port 8080 of each pod. The service accepts connections on port 80 and forwards them to port 8080 of one of the matching pods, allowing clients to access the service through a single IP address and port.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:66\n",
      "Services enable clients to discover and communicate with pods, allowing them to respond to external requests.\n",
      "Summary\n",
      "The chapter explains services in Kubernetes, enabling clients to discover and talk to pods. A service is exposed through an internal cluster IP that's only accessible from inside the cluster. The primary purpose of services is exposing groups of pods to other pods in the cluster. To test a service, one can send requests to it from within the cluster using various methods such as creating a pod, ssh-ing into a node, or executing a command in an existing pod using kubectl exec.\n",
      "Summary for page:67\n",
      "Services enable clients to discover and communicate with pods, allowing them to respond to external requests.\n",
      "Summary\n",
      "When running curl inside a pod using kubectl exec, Kubernetes proxies the connection to a random available pod among those backing the service. The double dash (--), signals the end of command options for kubectl and everything after it is executed within the pod. Without the double dash, the -s option would be interpreted as an option for kubectl, resulting in misleading errors.\n",
      "Summary for page:68\n",
      "Services enable clients to discover and communicate with pods, allowing them to respond to external requests.\n",
      "Summary\n",
      "This chapter discusses services in Kubernetes, enabling clients to discover and talk to pods. Session affinity can be set to either None or ClientIP, redirecting requests from the same client IP to the same pod. Services can also support multiple ports, exposing all ports through a single cluster IP, with each port requiring a specified name.\n",
      "Summary for page:69\n",
      "Services enable clients to discover and communicate with pods, allowing them to respond to external requests.\n",
      "Summary\n",
      "A Kubernetes Service can be defined with named ports in both the pod and service specifications. The label selector applies to the whole service, not individual ports. Ports can be referred to by name or number in the service spec.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:70\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "Services enable clients to discover and talk to pods through a single and stable IP address and port, which remains unchanged throughout its lifetime. Client pods can discover the service's IP and port through environment variables or by manually looking up its IP address.\n",
      "Summary for page:71\n",
      "Services enable clients to discover and talk to pods through a single and stable IP address and port, which remains unchanged throughout its lifetime. Client pods can discover the service's IP and port through environment variables or by manually looking up its IP address.\n",
      "Summary\n",
      "Services in Kubernetes are exposed through environment variables, but can also be discovered using DNS. Each service gets a DNS entry and client pods can access them through their fully qualified domain name (FQDN). This allows for a more flexible way of accessing services without relying on environment variables.\n",
      "Summary for page:72\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Services enable clients to discover and talk to pods. Clients can connect to a service by opening a connection to its FQDN, which includes the service name, namespace, and cluster domain suffix. If in the same namespace as the database pod, the client can refer to the service simply by its name. To access a service inside a pod's container, run bash using kubectl exec command with the -it option, and then use curl to access the service.\n",
      "Summary for page:73\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "You can connect to services living outside the cluster by using its name as the hostname in the requested URL. Omitting namespace and svc.cluster.local suffix is also allowed due to how DNS resolver inside each pod's container is configured. However, trying to ping service IP will not work because it's a virtual IP that only has meaning when combined with the service port.\n",
      "Summary for page:74\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Kubernetes services enable clients to discover and talk to pods. The Endpoints resource is a list of IP addresses and ports exposing a service. The pod selector in the service spec is used to build a list of IPs and ports, which are stored in the Endpoints resource. Clients connect to a service, and the service proxy selects one of those IP and port pairs and redirects the incoming connection to the server listening at that location.\n",
      "Summary for page:75\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "A Kubernetes service called `external-service` is created without a pod selector, requiring a separate Endpoints resource to be manually created with the same name and containing target IP addresses and ports for the service.\n",
      "Summary for page:76\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Kubernetes services enable clients to discover and talk to pods, and can be exposed externally using an ExternalName service which creates a DNS record pointing to a fully qualified domain name. This allows clients to connect directly to the external service without going through the service proxy, and does not require a cluster IP address. ExternalName services are implemented solely at the DNS level and can be modified by changing the externalName attribute or switching to a ClusterIP service with an Endpoints object.\n",
      "Summary for page:77\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "A service can be made accessible externally by setting its type to NodePort, LoadBalancer, or creating an Ingress resource. A NodePort service makes a port on all nodes reserve and forward incoming connections to the pods that are part of the service, allowing access through any node's IP and reserved node port.\n",
      "Summary for page:78\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "A Kubernetes Service named kubia-nodeport is created with type NodePort, specifying node port 30123 and exposing internal port 80. The service is accessible through the IP address of any cluster node on port 30123, redirecting incoming connections to a randomly selected pod.\n",
      "Summary for page:79\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "To expose services to external clients, configure Google Cloud Platform's firewalls to allow connections on the desired port, e.g., $ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123. This enables access through one of the node's IPs on that port, which can be found in a separate step.\n",
      "Summary for page:80\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Services in Kubernetes allow clients to discover and talk to pods. With NodePort services, pods are accessible through port 30123 on any node. However, this can lead to issues if a node fails. A load balancer can be used to distribute traffic across healthy nodes, and Kubernetes clusters running on cloud providers often support automatic load balancer provisioning. Using JSONPath with kubectl allows for efficient retrieval of node IPs.\n",
      "Summary for page:81\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Creating a Kubernetes Service with a LoadBalancer allows external access through a unique, publicly accessible IP address. The service type is set to LoadBalancer, and ports are specified for external connection. Once created, the load balancer's IP address is listed in the Service object, enabling direct access via curl or other tools.\n",
      "Summary for page:82\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Services in Kubernetes allow clients to discover and talk to pods, using the load balancer to route HTTP requests to a random pod for each connection. Even with session affinity set to None, users will hit the same pod every time due to keep-alive connections from web browsers, whereas tools like curl open new connections each time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:83\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Exposing services to external clients can be done through NodePort or LoadBalancer-type services. However, when using a node port, externally originating connections may not always go directly to the pod running on the same node, requiring an additional network hop. This can be prevented by configuring the service's externalTrafficPolicy field to 'Local', but this has its own drawbacks such as uneven distribution of connections across pods.\n",
      "Summary for page:84\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Services in Kubernetes allow clients to discover and communicate with pods, but can't preserve client IP when using node ports due to Source Network Address Translation (SNAT). The Local external traffic policy affects this, but creating an Ingress resource is another way to expose services externally, allowing multiple services to share one public IP address and load balancer, improving load distribution and scalability.\n",
      "Summary for page:85\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Ingresses in Kubernetes operate at the application layer, providing features like cookie-based session affinity. An Ingress controller is required to make Ingress resources work, and different environments use different implementations. To enable the Ingress add-on in Minikube, run $ minikube addons enable ingress, which allows exposing multiple services through a single Ingress.\n",
      "Summary for page:86\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Creating an Ingress resource in Kubernetes enables clients to discover and talk to pods. An example YAML manifest is provided, which defines an Ingress with a single rule sending all HTTP requests from the host kubia.example.com to the kubia-nodeport service on port 80. The Ingress controller pod can be listed using kubectl get po --all-namespaces.\n",
      "Summary for page:87\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "Exposing services externally through an Ingress resource requires configuring DNS or /etc/hosts to point to the Ingress controller's IP address. The Ingress controller then selects a pod based on the Host header and forwards the request to it, allowing access to the service at http://kubia.example.com.\n",
      "Summary for page:88\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "An Ingress can expose multiple services on the same host by mapping different paths to different services, allowing clients to reach two or more services through a single IP address. This is achieved by specifying multiple paths in the Ingress spec and mapping each path to a specific service, as shown in Listing 5.14. Requests are routed to the corresponding service based on the path in the requested URL.\n",
      "Summary for page:89\n",
      "Services in Kubernetes provide a single, stable IP address and port that remains unchanged throughout its lifetime.\n",
      "Summary\n",
      "An Ingress resource can map different services to different hosts based on the Host header in the request, and can also handle TLS traffic by attaching a certificate and private key to the Ingress as a Secret. This allows for secure communication between clients and the controller without requiring the application pod to support TLS.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:90\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "Services allow clients to discover and communicate with pods. A Secret was created using two files, and an Ingress object was updated to accept HTTPS requests for kubia.example.com. Alternatively, 'kubectl apply' can be used to update the Ingress resource. CertificateSigningRequest resources enable certificates to be signed by a human operator or automated process, retrieving a signed certificate from the CSR's status field.\n",
      "Summary for page:91\n",
      "Services allow clients to discover and communicate with pods.\n",
      "Summary\n",
      "Kubernetes allows you to define a readiness probe for your pod, which periodically determines whether the pod should receive client requests or not. When a container's readiness probe returns success, it signals that the container is ready to accept requests, allowing traffic to be directed to it only when it's fully ready to serve.\n",
      "Summary for page:92\n",
      "Services allow clients to discover and communicate with pods.\n",
      "Summary\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths. Readiness probes check if a container is ready to serve requests, with three types: Exec, HTTP GET, and TCP Socket. If a pod fails the readiness check, it's removed from the service until it becomes ready again. This ensures only healthy containers receive requests, distinguishing from liveness probes which keep pods running.\n",
      "Summary for page:93\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "Readiness probes ensure clients only talk to healthy pods by signaling when a pod is ready to accept connections. A readiness probe can be added to a pod by modifying the ReplicationController's pod template using kubectl edit, adding the probe definition under spec.template.spec.containers. The probe periodically checks if a file exists, and if it does, the pod is considered ready.\n",
      "Summary for page:94\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "Services: enabling clients to discover and talk to pods. ReplicationController's pod template changes have no effect on existing pods. Existing pods report not being ready until they're re-created by the Replication-Controller, which will fail the readiness check unless a /var/ready file is created in each of them.\n",
      "Summary for page:95\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "A readiness probe in Kubernetes determines if a pod is ready to accept connections. In real-world scenarios, it should return success or failure depending on whether the app can receive client requests. If no readiness probe is defined, pods become service endpoints immediately and clients may experience connection errors when the app takes too long to start listening for incoming connections.\n",
      "Summary for page:96\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "Kubernetes allows clients to discover pod IPs through DNS lookups, enabling connection to all pods or individual pods using a headless service with clusterIP set to None. This method is ideal for Kubernetes-agnostic apps, providing a stable IP address for clients to connect to all backing pods.\n",
      "Summary for page:97\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "A headless service is used to discover individual pods based on a pod selector. The service will list only ready pods as endpoints. To confirm readiness, create the /var/ready file in each pod. A DNS lookup can be performed from inside a pod using the tutum/dnsutils container image or by running a new pod without writing a YAML manifest using kubectl run with the --generator=run-pod/v1 option. This allows understanding of how DNS A records are returned for a headless service, which returns IPs of all ready pods.\n",
      "Summary for page:98\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "Headless services allow clients to connect directly to pods by DNS name. Kubernetes provides load balancing across pods through DNS round-robin mechanism instead of service proxy. To discover all pods, including unready ones, add annotation 'service.alpha.kubernetes.io/tolerate-unready-endpoints: true' or use the publishNotReadyAddresses field in service spec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:99\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "Make sure to access Kubernetes service from within the cluster and not outside. Check if readiness probe is succeeding, examine Endpoints object, and try accessing service using its cluster IP or FQDN. Ensure correct port is exposed and target port is not used. Connect directly to pod IP to confirm connections are being accepted. If still issues persist, check if app is binding only to localhost.\n",
      "Summary for page:100\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "Services enable clients to discover and talk to pods using a pod's readiness probe, enabling discovery of pod IPs through DNS for headless services. Additionally, troubleshooting and modifying firewall rules in Google Kubernetes/Compute Engine, executing commands in pod containers, running bash shells in existing pods, and modifying resources with kubectl apply can be performed.\n",
      "Summary for page:101\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "This chapter explores how containers in a pod can access external disk storage and share storage between them. Containers have isolated file systems, but volumes allow sharing disk space. Topics include creating multi-container pods, using Git repositories inside pods, attaching persistent storage, and dynamic provisioning of persistent storage.\n",
      "Summary for page:102\n",
      "Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "Kubernetes provides storage volumes that allow new containers to continue where the last one finished, preserving directories with actual data across container restarts. Volumes are defined in a pod's specification and must be mounted in each container that needs to access it, allowing multiple containers to share disk storage and enabling them to work together effectively.\n",
      "Summary for page:103\n",
      "Kubernetes provides services for clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "The document introduces the concept of volumes in Kubernetes, where multiple containers within a pod can share storage without relying on shared filesystems. Three containers are used as examples: WebServer, ContentAgent, and LogRotator, each with its own filesystem but sharing two volumes, publicHtml and logVol, mounted at different paths to illustrate this concept.\n",
      "Summary for page:104\n",
      "Kubernetes provides services for clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "Volumes in Kubernetes allow attaching disk storage to containers, enabling them to operate on the same files. A volume is bound to a pod's lifecycle and can be mounted at arbitrary locations within the file tree. Various types of volumes are available, including emptyDir, hostPath, gitRepo, nfs, gcePersistentDisk, awsElasticBlockStore, and azureDisk, each with its own purpose and use case. To access a volume from within a container, a VolumeMount must be defined in the container's spec.\n",
      "Summary for page:105\n",
      "Kubernetes provides services for clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "Volumes in Kubernetes can be used to share data between containers or for exposing Kubernetes resources and cluster information. Special types of volumes like secret, downwardAPI, and configMap are used to expose metadata to apps running in a pod. A single pod can use multiple volumes of different types at the same time, with each container having the option to mount or not. An emptyDir volume is useful for sharing files between containers or for temporary data storage by a single container.\n",
      "Summary for page:106\n",
      "Kubernetes provides services for clients to discover and talk to pods through GET requests or specific URL paths.\n",
      "Summary\n",
      "To create a pod that uses a shared volume, you need to build a Docker image with the required binary (fortune) and script (fortuneloop.sh). The image is based on ubuntu:latest, installs fortune, adds the script to /bin folder, and sets it as the ENTRYPOINT. You then create a pod manifest (fortune-pod.yaml) that specifies two containers sharing the same volume. Finally, you can run the pod using kubectl apply -f fortune-pod.yaml\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:107\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "A pod contains two containers and a shared volume between them. The html-generator container writes to the volume every 10 seconds, while the web-server container serves files from it. By forwarding port 80 on the local machine to the pod's port, users can access the Nginx server through localhost:8080 and receive a different fortune message with each request.\n",
      "Summary for page:108\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "An emptyDir volume can be created on tmpfs filesystem for better performance, while a gitRepo volume clones and checks out a Git repository at pod startup. The files in a gitRepo volume are not kept in sync with the referenced repo, but are updated when a new pod is created. This type of volume is useful for storing static HTML files or serving the latest version of a website.\n",
      "Summary for page:109\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "Using volumes in Kubernetes, specifically gitRepo volumes, allows sharing data between containers. This is demonstrated by running a web server pod serving files from a cloned Git repository, where the pod is created with a single Nginx container and a single gitRepo volume that clones the repository into the root directory of the volume.\n",
      "Summary for page:110\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "To keep files in sync with a Git repository, you can create a sidecar container that runs a Git sync process. This process can be run in an existing container image from Docker Hub, such as 'git sync'. The sidecar container should mount the gitRepo volume and configure the Git sync process to keep the files in sync with the Git repo. This method is recommended instead of using a gitRepo volume for private Git repositories, which are not supported by Kubernetes.\n",
      "Summary for page:111\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "A gitRepo volume is created for and used exclusively by a pod, but its contents can survive multiple pod instantiations if the volume type is different. hostPath volumes allow pods to access files on the node's filesystem, making it possible for system-level pods to read or use the node's devices through the filesystem.\n",
      "Summary for page:112\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "HostPath volumes are not suitable for storing a database's data directory as they store contents on a specific node's filesystem, making it sensitive to scheduling. Instead, use them to access the node's log files, kubeconfig, or CA certificates. System-wide pods like fluentd-kubia use hostPath volumes to access node's data.\n",
      "Summary for page:113\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "To persist data across pods, a network-attached storage (NAS) is needed. A GCE Persistent Disk can be used as underlying storage mechanism on Google Kubernetes Engine. The disk must be created in the same zone as the Kubernetes cluster and its size should be at least 200GB for optimal I/O performance.\n",
      "Summary for page:114\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "This chapter explains how to attach disk storage to containers using Kubernetes volumes. It provides an example of creating a 1 GiB GCE persistent disk called 'mongodb' and configuring a pod to use it as a volume, mounting it at '/data/db'. The YAML for the pod is provided, specifying the gcePersistentDisk type, fsType as ext4, and mountPath as /data/db. A note is also given for using Minikube, where you can't use a GCE Persistent Disk, but instead deploy mongodb-pod-hostpath.yaml using a hostPath volume.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:115\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "To use persistent storage, write data to the MongoDB database by running the MongoDB shell inside the container and inserting JSON documents. The data will be stored on a GCE persistent disk. After deleting and re-creating the pod, the new pod can read the persisted data from the previous pod, using the same GCE persistent disk.\n",
      "Summary for page:116\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "You can attach disk storage to containers using Kubernetes volumes, such as GCE Persistent Disk, awsElasticBlockStore, azureFile, or azureDisk. These volumes provide persistent storage for pods, allowing data to be retained across pod instances. To use a different volume type, create the underlying storage and set properties in the volume definition.\n",
      "Summary for page:117\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "Kubernetes supports various storage technologies, including NFS, ISCSI, GlusterFS, and others. However, it's recommended to use volumes in a way that decouples pod definitions from specific clusters, avoiding infrastructure-related details in pod specifications.\n",
      "Summary for page:118\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "Kubernetes aims to hide infrastructure from developers, allowing them to request persistent storage without knowing specific details. Cluster admins configure the cluster to provide what apps request, using PersistentVolumes and PersistentVolumeClaims to decouple pods from underlying storage technology.\n",
      "Summary for page:119\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "A cluster administrator creates a PersistentVolume resource through the Kubernetes API server, specifying its size and access modes. A user then creates a PersistentVolumeClaim manifest, specifying their required size and access mode, which is bound to an existing PersistentVolume. The volume can be used in a pod, but other users cannot use it until the claim is released.\n",
      "Summary for page:120\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "A PersistentVolume is created by specifying its capacity, access modes, and storage type. The administrator can then claim the PV with a PersistentVolumeClaim, which allows a container to read from or write to it. A PV is cluster-level resource like nodes and doesn't belong to any namespace. It's created with kubectl create command and shown as Available until claimed.\n",
      "Summary for page:121\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "To use a PersistentVolume in a Kubernetes pod that requires persistent storage, you need to create a PersistentVolumeClaim (PVC) first. This is done by preparing a PVC manifest and posting it to the Kubernetes API through kubectl create. The PVC claims the PersistentVolume for exclusive use within a namespace, allowing the same PVC to stay available even if the pod is rescheduled.\n",
      "Summary for page:122\n",
      "A pod contains two containers and a shared volume between them.\n",
      "Summary\n",
      "A Kubernetes PersistentVolumeClaim is created with a requested 1Gi of storage and ReadWriteOnce access mode. The claim is bound to a matching PersistentVolume, which is shown as Bound in kubectl get pvc and pv commands. The PersistentVolume's capacity and access modes match the claim's requirements.\n",
      "Summary for page:123\n",
      "A pod contains two containers with a shared volume between them.\n",
      "Summary\n",
      "To use a PersistentVolume in a pod, reference the PersistentVolumeClaim by name inside the pod's volume. A Pod can claim and use the same PersistentVolume until it is released, allowing decoupling from underlying storage technology.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:124\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) and claims to attach disk storage to containers, making it simpler for application developers by abstracting away infrastructure-specific details. A pod can use a GCE Persistent Disk either directly or through a PV and claim, allowing for greater flexibility and portability across different Kubernetes clusters.\n",
      "Summary for page:125\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) and claims to attach disk storage to containers, making it simpler for application developers by abstracting away infrastructure-specific details. A pod can use a GCE Persistent Disk either directly or through a PV and claim, allowing for greater flexibility and portability across different Kubernetes clusters.\n",
      "Summary\n",
      "When a PersistentVolumeClaim is deleted, its status becomes Pending and it's no longer bound to a PersistentVolume, which can be reused by other pods after being manually recycled or reclaimed automatically using Retain, Recycle, or Delete policies, allowing the reuse of volumes across different namespaces.\n",
      "Summary for page:126\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) and claims to attach disk storage to containers, making it simpler for application developers by abstracting away infrastructure-specific details.\n",
      "Summary\n",
      "A PersistentVolume only supports Retain or Delete policies. The reclaim policy can be changed on an existing PersistentVolume. Kubernetes also performs dynamic provisioning of PersistentVolumes through persistent-volume provisioners and StorageClass objects, allowing users to choose the type of PersistentVolume they want.\n",
      "Summary for page:127\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) and claims to attach disk storage to containers, making it simpler for application developers by abstracting away infrastructure-specific details.\n",
      "Summary\n",
      "Dynamic provisioning of PersistentVolumes allows administrators to define one or two StorageClasses, enabling the system to create new PersistentVolumes each time a PersistentVolumeClaim is requested. This eliminates the possibility of running out of PersistentVolumes. The StorageClass resource specifies the provisioner and parameters for provisioning, which can be specific to cloud providers like GCE. Users can refer to the storage class by name in their PersistentVolumeClaims, enabling dynamic provisioning of PersistentVolumes.\n",
      "Summary for page:128\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) and claims to attach disk storage to containers, making it simpler for application developers by abstracting away infrastructure-specific details.\n",
      "Summary\n",
      "A PersistentVolumeClaim (PVC) can specify a custom storage class, such as 'fast', which is referenced by a provisioner to create a PersistentVolume. The provisioner is used even if an existing manually provisioned PV matches the PVC. If the storage class does not exist, provisioning will fail. The dynamically created PV has the requested capacity and access modes, with a reclaim policy of Delete, meaning it will be deleted when the PVC is deleted.\n",
      "Summary for page:129\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "Dynamic provisioning of PersistentVolumes allows cluster admins to create multiple storage classes with different performance characteristics. Developers can then choose which one is most appropriate for each claim they create. This makes PVC definitions portable across different clusters as long as StorageClass names are the same, demonstrating flexibility and consistency in Kubernetes environments.\n",
      "Summary for page:130\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "The default storage class in a GKE cluster is defined by an annotation, which makes it the default storage class. A PersistentVolumeClaim can be created without specifying a storage class and a GCE Persistent Disk of type pd-standard will be provisioned for you.\n",
      "Summary for page:131\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "Dynamic provisioning of PersistentVolumes uses the default storage class when creating a PVC. To bind a PVC to a manually pre-provisioned PV, explicitly set storageClassName to an empty string. This prevents the dynamic provisioner from provisioning a new PV and allows the PVC to use the existing one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:132\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "This chapter explains how volumes provide temporary or persistent storage to containers in a pod. Key concepts include creating multi-container pods with shared files using volumes, mounting external storage for persistence across restarts, and dynamically provisioning PersistentVolumes through PersistentVolumeClaims and StorageClasses.\n",
      "Summary for page:133\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "ConfigMaps and Secrets allow passing configuration data to Kubernetes applications, configuring containerized applications by changing the main process, passing command-line options, setting environment variables, or using ConfigMaps for non-sensitive settings and Secrets for sensitive info like credentials.\n",
      "Summary for page:134\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "ConfigMaps and Secrets allow storing configuration data and sensitive information separately from container images. ConfigMaps store config data as a top-level Kubernetes resource, while Secrets handle sensitive info like credentials or encryption keys with special care. This allows for easier management of config changes and keeping sensitive data secure.\n",
      "Summary for page:135\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "You can pass command-line arguments to Docker containers by specifying them in the docker run command, overriding any default arguments set in the image's Dockerfile. This is achieved through the ENTRYPOINT instruction, which defines the executable, and CMD, which specifies the default arguments. The ENTRYPOINT instruction supports two forms: shell and exec, where shell form invokes the command inside a shell and exec form runs it directly.\n",
      "Summary for page:136\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "This chapter discusses ConfigMaps and Secrets in Kubernetes, specifically how to make an interval configurable in a Docker image using the exec form of the ENTRYPOINT instruction and setting a default value with the CMD instruction. A script is modified to accept an INTERVAL variable from the command line, and the Dockerfile is updated to use this new script and set the default interval to 10 seconds.\n",
      "Summary for page:137\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "Kubernetes allows overriding command and arguments in a container by setting 'command' and 'args' fields in the container specification. This can be done when creating a pod, but not updated after it's created. The equivalent Dockerfile instructions are ENTRYPOINT and CMD.\n",
      "Summary for page:138\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "The chapter discusses ConfigMaps and Secrets for configuring applications in Kubernetes. It explains how to pass configuration options through command-line arguments using the args array, and also through environment variables. The example uses the fortune:args image to generate a new fortune every two seconds.\n",
      "Summary for page:139\n",
      "The chapter highlights the benefits of using PersistentVolumes (PVs) in Kubernetes to abstract away infrastructure-specific details for application developers.\n",
      "Summary\n",
      "To make the interval in your fortuneloop.sh script configurable through an environment variable, remove the row where the INTERVAL variable is initialized. This allows the script to be configured from an environment variable, and can be used with Docker containers. To specify environment variables in a container definition, set them inside the container definition, not at the pod level. This can be done using YAML files like fortune-pod-env.yaml.\n",
      "Summary for page:140\n",
      "The chapter emphasizes the advantages of utilizing PersistentVolumes (PVs) in Kubernetes to conceal infrastructure-specific details for application developers.\n",
      "Summary\n",
      "You can reference previously defined environment variables by using the $(VAR) syntax, decoupling configuration from pod descriptors using ConfigMaps, and passing values as environment variables or files in a volume.\n",
      "Summary for page:141\n",
      "The chapter highlights the benefits of utilizing PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "ConfigMaps allow you to keep configuration separate from your app, making it easy to switch between environments by using different config values in each environment without changing the pod specification.\n",
      "Summary for page:142\n",
      "The chapter highlights the benefits of utilizing PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "A ConfigMap is used to store and manage configuration data for applications. It can be created using the kubectl create configmap command, which allows defining entries by passing literals or creating from files. A ConfigMap named fortune-config was created with a single entry sleep-interval=25, and its YAML descriptor was inspected using kubectl get command.\n",
      "Summary for page:143\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "ConfigMaps in Kubernetes can store configuration data, including complete config files, which can be created using the `kubectl create configmap` command. Files can be added individually or from a directory, and keys can be specified manually. ConfigMaps can also combine different options, such as literal values, files, and directories, to create a single map entry.\n",
      "Summary for page:144\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "You can pass ConfigMap entries to a container as environment variables using the valueFrom field in the pod descriptor. The pod descriptor should have an apiVersion of v1 and kind of Pod, with a ConfigMap named my-config. You can specify the key-value pairs from the ConfigMap as environment variables using --from-file or --from-literal flags, allowing you to pass JSON data and literal values to the container.\n",
      "Summary for page:145\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "This section shows how to decouple configuration from pod specification using a ConfigMap. A ConfigMap is referenced in the pod definition, allowing configuration options to be kept together and avoiding duplication across multiple pod manifests. If a referenced ConfigMap doesn't exist, the container referencing it will fail to start, but other containers will start normally. Optional references can also be marked, enabling containers to start even if the ConfigMap doesn't exist.\n",
      "Summary for page:146\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "Kubernetes version 1.6 allows exposing all entries of a ConfigMap as environment variables using the envFrom attribute, instead of individual env variables. This can be done by specifying a prefix for the environment variables, which will result in environment variables with the same name as the keys. However, if a ConfigMap key is not in the proper format (e.g., contains a dash), it will skip the entry and record an event. Additionally, ConfigMap entries cannot be referenced directly in the pod.spec.containers.args field, but can be passed as command-line arguments by first initializing an environment variable from the ConfigMap entry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:147\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "A ConfigMap can be used to expose entries as files or to pass configuration options as environment variables. A special volume type, configMap volume, can expose each entry of a ConfigMap as a file, allowing the container process to obtain the value by reading the contents of the file. This approach is suitable for exposing whole config files contained in a ConfigMap.\n",
      "Summary for page:148\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "A ConfigMap is created to pass a config file to an Nginx web server running inside a pod's web-server container, enabling gzip compression for plain text and XML files. A new directory is created on the local disk with two files: my-nginx-config.conf containing the Nginx config and sleep-interval with the number 25. The ConfigMap is then created from these files using kubectl.\n",
      "Summary for page:149\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "A ConfigMap is used to decouple configuration with Kubernetes, allowing for easy management and updates of configurations. It can contain multiple entries, each with its own key-value pair, and can be referenced in a pod's container using a volume populated with the ConfigMap's contents. This allows for the use of default configuration files while still adding custom configurations, as shown in Listing 7.14.\n",
      "Summary for page:150\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "This chapter discusses ConfigMaps and Secrets for configuring applications in Kubernetes. A Pod is defined with a volume referencing a fortune-config ConfigMap, which is mounted into the /etc/nginx/conf.d directory to make Nginx use it. The configuration can be verified by port-forwarding and checking the server's response with curl, demonstrating that the mounted ConfigMap entries are being used by the web server.\n",
      "Summary for page:151\n",
      "The chapter discusses the benefits of using PersistentVolumes (PVs) in Kubernetes to hide infrastructure-specific details for application developers.\n",
      "Summary\n",
      "A ConfigMap can be decoupled from a pod's configuration using a ConfigMap volume. This allows certain entries to be exposed as files in a directory, while others remain hidden. The `items` attribute of the volume can be used to specify which ConfigMap entries should be included as files, and the `key` and `path` fields define how the entry is stored. This approach enables fine-grained control over the configuration of containers within a pod.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:152\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "When mounting a ConfigMap or Secret as a directory, it hides existing files in that directory. To add individual files without hiding others, use the `subPath` property on the volumeMount to mount either a single file or directory from the volume, preserving the original files.\n",
      "Summary for page:153\n",
      "When mounting a ConfigMap or Secret as a directory, it hides existing files in that directory. To add individual files without hiding others, use the `subPath` property on the volumeMount to mount either a single file or directory from the volume, preserving the original files.\n",
      "Summary\n",
      "A ConfigMap can be used to decouple configuration from an application, allowing for easy updates without restarting the app. A subPath can be used to mount individual files from a volume instead of the whole volume, but this method has limitations with file updating. File permissions in a ConfigMap volume default to 644 and can be changed using the defaultMode property. Updating a ConfigMap updates all referencing volumes, allowing for configuration changes without app restarts.\n",
      "Summary for page:154\n",
      "When mounting a ConfigMap or Secret as a directory, it hides existing files in that directory.\n",
      "Summary\n",
      "ConfigMaps and Secrets can be edited using kubectl edit, which updates the files exposed in the configMap volume atomically using symbolic links. Changes to the ConfigMap are reflected in the actual file, but Nginx doesn't reload its config automatically. To signal Nginx to reload its config, use 'nginx -s reload' command within a pod. This allows changing an app's config without restarting the container or recreating the pod.\n",
      "Summary for page:155\n",
      "When mounting a ConfigMap or Secret as a directory, it hides existing files in that directory.\n",
      "Summary\n",
      "Kubernetes uses symbolic links to update ConfigMap volumes when a new directory is created. However, updating individual files in an existing directory does not trigger an update. Modifying an existing ConfigMap while pods are using it may not be ideal if the app doesn't reload its config automatically, as different running instances will have different configs.\n",
      "Summary for page:156\n",
      "When mounting a ConfigMap or Secret as a directory, it hides existing files in that directory.\n",
      "Summary\n",
      "Kubernetes provides Secrets to store and distribute sensitive information, which can be used like ConfigMaps. They are stored in memory on nodes and encrypted in etcd from v1.7. Choose between Secret and ConfigMap based on sensitivity: use ConfigMap for non-sensitive data and Secret for sensitive data. A default token Secret is automatically mounted into every container, accessible with kubectl get secrets.\n",
      "Summary for page:157\n",
      "When mounting a ConfigMap or Secret as a directory, it hides existing files in that directory.\n",
      "Summary\n",
      "Secrets are used to pass sensitive data to containers, and contain entries like ca.crt, namespace, and token which provide secure access to the Kubernetes API server from within pods. The default-token Secret is mounted into every container by default, but can be disabled in each pod or service account.\n",
      "Summary for page:158\n",
      "When mounting a ConfigMap or Secret as a directory, it hides existing files in that directory.\n",
      "Summary\n",
      "Creating a Secret involves generating certificate and private key files, then using kubectl to create a generic Secret called fortune-https from these files and an additional dummy file containing the string bar. This process is similar to creating ConfigMaps, but with the added security of keeping sensitive information like private keys secure within the Secret.\n",
      "Summary for page:159\n",
      "When mounting a ConfigMap or Secret as a directory, it hides existing files in that directory.\n",
      "Summary\n",
      "Secrets in Kubernetes can hold sensitive or non-sensitive binary data, which are encoded as Base64 strings. This contrasts with ConfigMaps that store plain-text data. Secrets have a maximum size limit of 1MB and can be used even for non-sensitive binary data.\n",
      "Cannot load json for this raw description:[\n",
      "    {\n",
      "        \"summary\": \n",
      "Summary for page:160\n",
      "[\n",
      "    {\n",
      "        \"summary\": \n",
      "Summary\n",
      "Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved. Secrets are decoded and written to files or environment variables in their actual form, allowing apps to read them directly without decoding.\n",
      "Summary for page:161\n",
      "Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved.\n",
      "Summary\n",
      "This document explains how to pass sensitive data (SSL certificates) to a container using Kubernetes secrets. It shows an example of mounting a secret volume in a pod, specifically for a fortune-https pod that uses Nginx and mounts the certificate and key files from a secret volume at /etc/nginx/certs.\n",
      "Summary for page:162\n",
      "Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved.\n",
      "Summary\n",
      "A Kubernetes pod is configured using a ConfigMap and a Secret, with the ConfigMap providing application configuration and the Secret providing sensitive data such as SSL certificates. The ConfigMap and Secret are referenced in the pod descriptor through their respective names, with file permissions specified for the Secret files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:163\n",
      "Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved.\n",
      "Summary\n",
      "Using Kubernetes secrets to pass sensitive data to containers, a pod's HTTPS traffic can be tested by opening a port-forward tunnel and using curl. The server's certificate can also be checked with curl. Secrets are stored in memory (tmpfs) and do not write to disk, making them secure. Alternatively, secret entries can be exposed as environment variables.\n",
      "Summary for page:164\n",
      "Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved.\n",
      "Summary\n",
      "ConfigMaps and Secrets in Kubernetes allow applications to access configuration data and secrets. However, exposing secrets through environment variables is not recommended due to security risks. Instead, use secret volumes or pass credentials to Kubernetes itself using image pull secrets for private container registries.\n",
      "Summary for page:165\n",
      "Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved.\n",
      "Summary\n",
      "To pass sensitive data to containers, create a Secret holding Docker registry credentials using kubectl create secret docker-registry command. Specify the Secret's name in the pod spec as an imagePullSecrets. This enables pulling images from a private image registry. Alternatively, add Secrets to a ServiceAccount to automatically include them in all pods.\n",
      "Summary for page:166\n",
      "Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved.\n",
      "Summary\n",
      "This chapter summarizes how to pass configuration data to containers using ConfigMaps and Secrets, including overriding commands, passing arguments, setting environment variables, decoupling config from pods, storing sensitive data in Secrets, and creating a docker-registry Secret.\n",
      "Summary for page:167\n",
      "Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved.\n",
      "Summary\n",
      "This chapter explores how applications can access pod metadata, resources, and interact with the Kubernetes API server. It covers using the Downward API to pass information into containers, exploring the Kubernetes REST API, accessing the API server from within a container, and understanding the ambassador container pattern.\n",
      "Summary for page:168\n",
      "Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved.\n",
      "Summary\n",
      "The Kubernetes Downward API allows passing metadata about a pod and its environment through environment variables or files, solving problems of repeating information in multiple places, and exposing pod metadata to processes running inside the pod, with currently available metadata being the pod's name, IP address, and labels/annotations.\n",
      "Summary for page:169\n",
      "Kubernetes allows setting Secret values through the stringData field, which is write-only and will be shown under data when retrieved.\n",
      "Summary\n",
      "The Downward API allows passing metadata such as namespace, node name, service account, CPU and memory requests/limits, labels, and annotations to containers through environment variables or a volume. This can be useful for providing containerized processes with information about their environment. An example is provided in the form of a simple single-container pod manifest that passes the pod's and container's metadata to the container using environment variables.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:170\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "The process can access environment variables defined in the pod spec. Environment variables expose pod metadata such as name, IP, namespace, and node name. Additionally, variables are created for CPU requests and memory limits with a divisor to convert values into desired units.\n",
      "Summary for page:171\n",
      "The process can access environment variables defined in the pod spec. Environment variables expose pod metadata such as name, IP, namespace, and node name. Additionally, variables are created for CPU requests and memory limits with a divisor to convert values into desired units.\n",
      "Summary\n",
      "The Downward API allows passing metadata from a pod's or container's attributes as environment variables to the running application. This is demonstrated by creating a pod with specified CPU and memory limits, then using kubectl exec to show the resulting environment variables in the container. The divisor for CPU limits can be 1 (one whole core) or 1m (one millicore), while memory limits can use units such as 1K or 1Mi.\n",
      "Summary for page:172\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "Pods can expose metadata through environment variables or files in a downwardAPI volume. Environment variables can only pass single-value metadata, while a downwardAPI volume allows exposing labels and annotations. A Pod's name and namespace can be exposed through a downwardAPI volume by specifying the fieldRef path to the metadata.name and metadata.namespace fields.\n",
      "Summary for page:173\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "The Downward API allows passing metadata through a volume, mounting it in the container under /etc/downward. Each item specifies the path where metadata should be written and references either a pod-level field or a container resource field.\n",
      "Summary for page:174\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "This chapter discusses accessing pod metadata and other resources from applications using Kubernetes. It explains how to mount a downwardAPI volume, which makes available various metadata fields such as labels, annotations, and container resource requests as files within the pod's filesystem. The contents of these files can be accessed using kubectl exec commands. Additionally, it highlights that labels and annotations can be modified while a pod is running, and Kubernetes updates the corresponding files in the downwardAPI volume.\n",
      "Summary for page:175\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "When exposing container-level metadata using the Downward API, you need to specify the name of the container whose resource field you're referencing. This is because volumes are defined at the pod level, not at the container level. Using the Downward API allows you to keep your application Kubernetes-agnostic by passing data from the pod and containers to the process running inside them. However, it only exposes a limited subset of metadata, so if your app needs more information about other pods or resources in the cluster, you'll need to obtain that directly from the Kubernetes API server.\n",
      "Summary for page:176\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "The Kubernetes API server provides REST endpoints for accessing pod metadata and other resources, but requires authentication. To access it directly, use kubectl proxy to run a proxy server that handles authentication and verifies the server's certificate on each request, allowing apps within pods to talk to the API server and get information about other resources or the most up-to-date data possible.\n",
      "Summary for page:177\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "To explore the Kubernetes API through kubectl proxy, run $ kubectl proxy to start serving on 127.0.0.1:8001, then use curl or a web browser to navigate to http://localhost:8001 to list available API paths and resource types.\n",
      "Summary for page:178\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "The Kubernetes batch API group has two versions (v1 and v2alpha1), with v1 being the preferred version. The /apis/batch path displays the available versions, while /apis/batch/v1 shows a list of resource types in this group, including jobs which are namespaced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:179\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "The Kubernetes API server returns a list of resource types and REST endpoints in the batch/v1 API group. The Job resource is exposed with verbs to retrieve, update, delete, create, watch, patch and get. Additional API endpoints are also available for modifying job status.\n",
      "Summary for page:180\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "Accessing pod metadata and other resources from applications involves using Kubernetes REST API. You can retrieve information about pods, jobs, and namespaces using curl commands. To talk to the API server from within a pod, you need to find its location, authenticate with it, and ensure you're not talking to an impersonator. This allows applications running in pods to interact with Kubernetes services.\n",
      "Summary for page:181\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "To communicate with the Kubernetes API server, create a pod using the tutum/curl image and run a shell inside it. Find the API server's address by looking up environment variables KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT within the container.\n",
      "Summary for page:182\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "To access pod metadata and other resources from applications, use the environment variables to get the service's port number. However, always verify the server's identity by checking its certificate. Use curl with the --cacert option to specify the CA certificate, which is stored in a Secret called default-token-xyz. This verifies that the server's certificate is signed by the CA and prevents man-in-the-middle attacks.\n",
      "Summary for page:183\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "To access the Kubernetes API server, set the CURL_CA_BUNDLE environment variable to trust its certificate. Then, authenticate with the server by loading an authentication token into an environment variable using TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token). Finally, send requests to the API server using curl -H \"Authorization: Bearer $TOKEN\" https://kubernetes\n",
      "Summary for page:184\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "You can access pod metadata and other resources from applications by passing a token inside the Authorization HTTP header. You can also retrieve the namespace of your pod by reading the contents of the /var/run/secrets/kubernetes.io/serviceaccount/namespace file into the NS environment variable. With this information, you can list all pods running in the same namespace as your pod using a GET request to https://kubernetes/api/v1/namespaces/$NS/pods. Additionally, you can disable role-based access control (RBAC) by creating a clusterrolebinding with cluster-admin privileges for service accounts.\n",
      "Summary for page:185\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "An app running in a pod can access the Kubernetes API by verifying the API server's certificate, authenticating with a bearer token, and using a namespace file to pass the namespace for CRUD operations. This process can be simplified using an ambassador container, which makes communication with the API server more straightforward while keeping it secure.\n",
      "Summary for page:186\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "The chapter discusses how to access pod metadata and resources from applications using the kubectl proxy command, which can be run inside pods as an ambassador container pattern. This allows applications to query the API server securely through the ambassador without direct communication with the API server.\n",
      "Summary for page:187\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "This page discusses connecting to a Kubernetes API server using an ambassador container. The ambassador container runs `kubectl proxy` and handles authentication with the API server, allowing the main container to send plain HTTP requests to localhost:8001. This simplifies communication between containers and can be reused across different apps, but adds an additional process consuming resources.\n",
      "Summary for page:188\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "Kubernetes API client libraries are available for various programming languages, including Golang, Python, Java, Node.js, and PHP, allowing applications to access pod metadata and other resources from the API server.\n",
      "Summary for page:189\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "The page discusses interacting with the Kubernetes API server using various client libraries such as Ruby, Clojure, Scala, Perl, and Java (Fabric8 client). It provides examples of how to list services, create, edit, and delete pods in a Java app using the Fabric8 client, highlighting the simplicity and efficiency of these interactions.\n",
      "Summary for page:190\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "The chapter discusses accessing pod metadata and other resources from applications using the Fabric8 client's fluent DSL API. If no client is available, you can use Swagger to generate a client library and documentation. The Kubernetes API server exposes Swagger definitions at /swaggerapi and OpenAPI spec at /swagger.json. You can explore the API with Swagger UI, which provides a web interface for interacting with REST APIs.\n",
      "Summary for page:191\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "You now know how your app running inside a pod can get data about itself, other pods, and components deployed in the cluster through environment variables or downwardAPI volumes. You've learned to access CPU and memory requests, browse the Kubernetes REST API, find the API server's location, authenticate yourself, and use client libraries to interact with Kubernetes.\n",
      "Summary for page:192\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "This chapter covers updating apps running in a Kubernetes cluster, focusing on using Deployments to perform zero-downtime updates. Key topics include replacing pods with newer versions, updating managed pods, and performing rolling updates, as well as automatically blocking rollouts of bad versions and controlling the rollout rate.\n",
      "Summary for page:193\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "Updating applications running in pods involves replacing old pods with new ones, either by deleting existing pods first and then starting the new ones or by adding new pods while gradually removing old ones, requiring app to handle two versions simultaneously.\n",
      "Summary for page:194\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "In Kubernetes, you can update applications declaratively using Deployments, which can be updated automatically or manually. The manual method involves updating the pod template of a ReplicationController to refer to a new image version, then deleting the old pods. Alternatively, you can spin up new pods and delete the old ones without downtime, if your app supports running multiple versions at once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:195\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "You can update applications running in pods by combining replication controllers and services, switching from the old to the new version at once using a blue-green deployment or performing a rolling update where you replace pods step by step. This requires updating the service's pod selector and scaling down the previous replication controller while scaling up the new one.\n",
      "Summary for page:196\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "You can perform an automatic rolling update with a ReplicationController by having kubectl do it, but this is now an outdated way of updating apps. The process involves running the initial version of the app, creating a modified version that returns its version number in the response, and using two ReplicationControllers to roll out the new version.\n",
      "Summary for page:197\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "A single YAML file is used to create both a ReplicationController and a LoadBalancer Service, enabling external access to the app. The YAML defines 3 replicas of a pod running the luksa/kubia:v1 image, and exposes it on port 80 with targetPort 8080. After posting the YAML to Kubernetes, the three v1 pods and load balancer run, allowing curl requests to be made to the external IP.\n",
      "Summary for page:198\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "This chapter discusses deploying and updating applications declaratively using Kubernetes Deployments. It explains how to perform a rolling update with kubectl by creating a new version of an app without disrupting existing traffic. The importance of setting the container's imagePullPolicy property to Always when pushing updates to the same image tag is also highlighted, especially when using tags other than latest. This ensures that all nodes run the updated image.\n",
      "Summary for page:199\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "To perform an automatic rolling update with a ReplicationController in Kubernetes, run the kubectl rolling-update command and specify the old RC, new RC name, and new image. A new RC will be created immediately, referencing the new image and initially having a desired replica count of 0. The system will then scale up the new RC while scaling down the old one, keeping the total number of pods at 3.\n",
      "Summary for page:200\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "Before performing a rolling update, kubectl modifies the ReplicationController's selector and adds an additional deployment label to its pod template, ensuring that only pods managed by the new controller are selected. The old controller is also modified with a new selector, allowing it to see zero matching pods, but the live pods' labels have been updated to include the new deployment label, preventing them from being seen by the old controller.\n",
      "Summary for page:201\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "Performing a rolling update with a ReplicationController using kubectl involves scaling up a new controller while scaling down an old one, replacing old pods with new ones. This process deletes v1 pods and replaces them with v2 pods, eventually directing all requests to the new version. The Service redirects requests to both old and new pods during the rolling update, progressively increasing the percentage of requests hitting v2 pods.\n",
      "Summary for page:202\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "Kubernetes' ReplicationController can be updated declaratively using `kubectl` commands, allowing for zero-downtime updates. However, the deprecated `kubectl rolling-update` command modifies existing objects and is not recommended. Instead, use explicit `kubectl` commands to scale and update resources, which provides greater control and avoids unexpected modifications.\n",
      "Summary for page:203\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "Deployments in Kubernetes provide a declarative way to update applications by introducing a ReplicaSet that creates and manages pods, providing a more scalable and efficient way of updating applications compared to using ReplicationControllers or ReplicaSets directly.\n",
      "Summary for page:204\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "A Deployment resource in Kubernetes is used to update applications declaratively by defining the desired state and letting Kubernetes handle the rest. A Deployment can have multiple pod versions running under its wing, so its name shouldn't reference the app version. Creating a Deployment requires specifying a deployment strategy and only three trivial changes are needed to modify a ReplicationController YAML file to describe a Deployment.\n",
      "Summary for page:205\n",
      "The process can access environment variables defined in the pod spec, exposing pod metadata such as name, IP, namespace, and node name.\n",
      "Summary\n",
      "Creating a Deployment in Kubernetes involves deleting any existing ReplicationControllers and pods, then running `kubectl create -f kubia-deployment-v1.yaml --record` to create a new Deployment. The `--record` option records the command in the revision history. To check the status of the Deployment rollout, use `kubectl rollout status deployment kubia`. A Deployment creates ReplicaSets, which then create pods with unique names that include a numeric value corresponding to the hashed value of the pod template and ReplicaSet managing them.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:206\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "Deployments provide an easy way to update applications declaratively, using a template that can always use the same ReplicaSet for a given version of the pod template. Updating a Deployment only requires modifying its pod template and Kubernetes takes care of replacing all original pods with new ones, achieving the desired state through a configured deployment strategy, either rolling update or recreate.\n",
      "Summary for page:207\n",
      "Deployments provide an easy way to update applications declaratively, using a template that can always use the same ReplicaSet for a given version of the pod template. Updating a Deployment only requires modifying its pod template and Kubernetes takes care of replacing all original pods with new ones, achieving the desired state through a configured deployment strategy, either rolling update or recreate.\n",
      "Summary\n",
      "The RollingUpdate strategy in Kubernetes allows for updating applications declaratively by removing old pods one by one and adding new ones at the same time, keeping the application available throughout the process. To slow down the update process, set the minReadySeconds attribute on the Deployment. Triggering the actual rollout is done by changing the image used in the single pod container to a new version using the kubectl set image command.\n",
      "Summary for page:208\n",
      "Deployments provide an easy way to update applications declaratively, using a template that can always use the same ReplicaSet for a given version of the pod template.\n",
      "Summary\n",
      "Kubernetes deployments can be updated using various methods such as kubectl edit, patch, apply, replace and set image. These methods change the Deployment's specification, triggering a rollout process. The deployment can also be modified by updating its pod template or container image. Examples of these methods are provided in the text.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:209\n",
      "Deployments provide an easy way to update applications declaratively using a template that can always use the same ReplicaSet for a given version of the pod template.\n",
      "Summary\n",
      "Using Deployments allows for declarative updates to apps by changing the pod template in a single field, which is then performed by Kubernetes controllers. This process is simpler than running special commands with kubectl. Note that modifying ConfigMaps will not trigger an update unless referencing a new ConfigMap.\n",
      "Summary for page:210\n",
      "Deployments provide an easy way to update applications declaratively using a template that can always use the same ReplicaSet for a given version of the pod template.\n",
      "Summary\n",
      "This chapter discusses Deployments in Kubernetes, which allows for declarative updates to applications. A Deployment resource manages ReplicaSets, making it easier to manage compared to ReplicationControllers. The chapter simulates a problem during a rollout process by introducing a bug in version 3 of an app that returns a 500 error after the fifth request.\n",
      "Summary for page:211\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "To update an app declaratively, change the image in the Deployment specification using $ kubectl set image deployment kubia nodejs=luksa/kubia:v3. Follow rollout progress with kubectl rollout status and roll back to previous revision with kubectl rollout undo deployment kubia.\n",
      "Summary for page:212\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "Deployments in Kubernetes keep a revision history of rollouts, which can be displayed with kubectl rollout history command. This history allows rolling back to any revision by specifying the revision number in the undo command. The length of the revision history is limited by the revisionHistoryLimit property and older ReplicaSets are deleted automatically.\n",
      "Summary for page:213\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "Deployments in Kubernetes allow for declarative updates to apps. The rollout process can be controlled using the `maxSurge` and `maxUnavailable` properties of the rolling update strategy. These properties determine how many pod instances are allowed above the desired replica count (maxSurge) and how many can be unavailable during the update (maxUnavailable). Both default to 25% but can also be specified as absolute values.\n",
      "Summary for page:214\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "Deployments can be updated declaratively using `maxSurge` and `maxUnavailable` properties, which control the number of unavailable pods during a rollout. The `extensions/v1beta1` version sets both to 1 instead of 25%, affecting the rollout process as shown in figures 9.12 and 9.13.\n",
      "Summary for page:215\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "A Deployment in Kubernetes allows for declarative updates to an application, ensuring that a minimum number of replicas are always available during the rollout process. The `maxUnavailable` property is relative to the desired replica count, meaning that at least one pod must be available when updating from three replicas. Additionally, Deployments can be paused during the rollout process, enabling the creation of a single new pod alongside existing ones for verification before proceeding with the full update.\n",
      "Summary for page:216\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "Deployments in Kubernetes can be paused or resumed using the `rollout pause` and `rollout resume` commands, allowing for a controlled rollout process. The `minReadySeconds` property can also be used to block rollouts of malfunctioning versions by specifying how long a new pod must be ready before being treated as available.\n",
      "Summary for page:217\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "Deployments can be used for updating apps declaratively, with a properly configured readiness probe and minReadySeconds setting, Kubernetes can prevent deploying buggy versions. A Deployment is updated using kubectl apply command with YAML that includes apiVersion, kind, metadata, spec, replicas, minReadySeconds, strategy, rollingUpdate, maxSurge, maxUnavailable, type, template, metadata, name, labels, app, and containers with image set to luksa/kubia:v3.\n",
      "Summary for page:218\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "To update a Deployment with kubectl apply, use the command $ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml. This updates the Deployment with everything defined in the YAML file, including the image and readiness probe definition. To keep the desired replica count unchanged, don't include the replicas field in the YAML. The rollout status command can be used to follow the update process, which will create new pods but doesn't guarantee that they'll be used.\n",
      "Summary for page:219\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "A deployment is prevented from updating an app declaratively due to a failing readiness probe in the new pod, which returns HTTP status code 500 and gets removed as an endpoint from the service, until it becomes available for at least 10 seconds.\n",
      "Summary for page:220\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "Deployments allow updating applications declaratively, and the rollout process can be configured with deadlines and minReadySeconds. If a rollout fails, it can be aborted using `kubectl rollout undo` command. The `progressDeadlineSeconds` property in Deployment spec is configurable to set a deadline for the rollout.\n",
      "Summary for page:221\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "This chapter taught you a declarative approach to deploying and updating applications in Kubernetes, including rolling updates, Deployments, and controlling rollout rates using maxSurge and maxUnavailable properties.\n",
      "Summary for page:222\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "This chapter focuses on deploying stateful clustered applications using StatefulSets, providing separate storage for each replicated pod instance, guaranteeing stable names and hostnames for pod replicas, controlling start/stop sequences, and peer discovery via DNS SRV records.\n",
      "Summary for page:223\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "Stateful pods can't be replicated using ReplicaSets as they require separate storage for each instance, which is not possible with a single ReplicaSet. Options include creating pods manually or using one ReplicaSet per pod instance, but these are cumbersome and not viable for scaling.\n",
      "Summary for page:224\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "StatefulSets allow deploying replicated stateful applications, but using multiple ReplicaSets is not ideal. A workaround is to have a single ReplicaSet with pods using the same PersistentVolume, but each instance selecting and creating its own separate file directory, requiring coordination between instances.\n",
      "Summary for page:225\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "Certain apps require a stable network identity, but Kubernetes assigns a new hostname and IP every time a pod is rescheduled. To work around this, create a dedicated service for each individual member, providing a stable network address, similar to creating a ReplicaSet for individual storage. This setup can be seen in Figure 10.4, but it's still not a complete solution as pods can't self-register using the stable IP.\n",
      "Summary for page:226\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "StatefulSets in Kubernetes deploy replicated stateful applications with stable names and states, treating instances as non-fungible individuals like pets, requiring replacement with new instances having same name, network identity, and state as the old one when it fails or is replaced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:227\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "A StatefulSet ensures pods are rescheduled to retain their identity and state, allowing easy scaling. Pods created by StatefulSets have unique volumes and stable identities, with each pod assigned an ordinal index used for naming and hostname. A governing headless Service is required to provide a network identity, enabling addressability by hostname.\n",
      "Summary for page:228\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "StatefulSets allow deploying replicated stateful applications, enabling access to pods through fully qualified domain names and DNS lookups. They also ensure replacement of lost pods with new instances having the same name and hostname, unlike ReplicaSets which replace them with unrelated pods.\n",
      "Summary for page:229\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "StatefulSets ensure stateful pods have stable identities, even when scaled up or down. Scaling up creates new instances with unused ordinal indexes, while scaling down removes instances with the highest index first, making effects predictable. StatefulSets also prevent scale-down operations if any instance is unhealthy and provide stable dedicated storage to each instance.\n",
      "Summary for page:230\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "In Kubernetes, StatefulSets deploy replicated stateful applications by creating separate PersistentVolumeClaims for each pod instance. The StatefulSet stamps out these claims along with the pod instances, allowing for persistent storage to be attached to each pod. Scaling up a StatefulSet creates new API objects, including one or more PersistentVolumeClaims, while scaling down deletes only the pod, leaving the claims intact. Manual deletion of PersistentVolumeClaims is required to release the underlying PersistentVolumes and prevent data loss.\n",
      "Summary for page:231\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "StatefulSets in Kubernetes allow for stable identity and storage of pods, guaranteeing that a pod's replacement has the same name, hostname, and persistent storage as the original. This means that if a StatefulSet is scaled down and then scaled back up, the new pod instance will have the same persisted state and be reattached to the same PersistentVolumeClaim, preventing data loss and ensuring consistency in the system.\n",
      "Summary for page:232\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "StatefulSets in Kubernetes must ensure two stateful pod instances are never running with the same identity and are bound to the same PersistentVolumeClaim. A StatefulSet must guarantee at-most-one semantics for stateful pod instances, ensuring a pod is no longer running before creating a replacement pod. This affects node failure handling, as demonstrated later in the chapter. A simple clustered data store is built using the kubia app, allowing data storage and retrieval on each pod instance.\n",
      "Summary for page:233\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "A simple app is created using Node.js and a Docker image, which writes POST requests to a file and returns stored data on GET requests. To deploy this app through a StatefulSet, PersistentVolumes must be created for storing data files, along with a governing Service required by the StatefulSet, and the StatefulSet itself. For Minikube users, PersistentVolumes can be deployed from a YAML file, while Google Kubernetes Engine users need to create actual GCE Persistent Disks before proceeding.\n",
      "Summary for page:234\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "This chapter explains how to deploy replicated stateful applications using StatefulSets. A list of three PersistentVolumes is created from the persistent-volumes-gcepd.yaml file, with each volume having a capacity of 1 Mebibyte and recycling when released. A headless Service called kubia is also created as the governing Service for the StatefulSet, which must be used to provide network identity for stateful pods.\n",
      "Summary for page:235\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "A stateless Service is created with a clusterIP field set to None, enabling peer discovery between pods. A StatefulSet manifest is then created with a serviceName and replicas of 2, using a volumeClaimTemplates list to define a Persistent-VolumeClaim for each pod, referencing a persistentVolumeClaim volume in the manifest.\n",
      "Summary for page:236\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "StatefulSets in Kubernetes create pods one at a time, ensuring safety for clustered apps sensitive to race conditions. The first pod is brought up fully before continuing to bring up the rest. A closer look at the first pod's spec shows how the StatefulSet constructs the pod from templates and PersistentVolume-Claim template, adding volumes automatically.\n",
      "Summary for page:237\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "A StatefulSet was used to create a PersistentVolumeClaim and volume inside a pod. The PersistentVolumeClaims were listed using kubectl get pvc, showing two claims bound to volumes pv-c and pv-a. Communication with individual pods can be done by proxying through the API server or using port-forwarding.\n",
      "Summary for page:238\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "The chapter explains how to deploy replicated stateful applications using StatefulSets in Kubernetes. It demonstrates how to use the kubectl proxy to communicate with a pod, send GET and POST requests to the pod, and store data on the pod. The example uses curl commands to interact with the pod through the API server, showing how to retrieve data from the pod and update it with new information.\n",
      "Summary for page:239\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "A StatefulSet was used to store data on a pod. When a GET request is made, the stored data is returned. The pod is then deleted and recreated by the StatefulSet, which still serves the same data as before. This demonstrates that a StatefulSet preserves state even when pods are rescheduled or deleted.\n",
      "Summary for page:240\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "A StatefulSet maintains its pods' identities, including hostnames and persistent data, even when scaled down or recreated after deletion. Scaling down a StatefulSet deletes pods but leaves PersistentVolumeClaims intact, with pods deleted in descending order of ordinal numbers. A non-headless Service can be used to expose stateful pods, allowing clients to connect through the Service rather than directly.\n",
      "Summary for page:241\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "To access a cluster-internal service, you can use the proxy feature provided by the API server, or use a pod to access it. The URI path is formed like /api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>. Peer discovery in a StatefulSet is also important for clustered apps to find other members.\n",
      "Summary for page:242\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "A StatefulSet's pods can discover their peers by performing an SRV DNS lookup, which points to the hostnames and ports of servers providing a specific service. Kubernetes creates SRV records to point to the hostnames of the pods backing a headless service. A pod can get a list of all other pods by running the dig command inside a temporary pod.\n",
      "Summary for page:243\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "To discover peers in a StatefulSet, the application performs a DNS lookup using the `dns.resolveSrv` method. It queries for SRV records for a specific service name and retrieves a list of addresses. If no peers are found, it returns 'No peers discovered.' Otherwise, it displays data from all cluster nodes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:244\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "StatefulSets are used to deploy replicated stateful applications, allowing for multiple pods to be created with a specified replica count. The pod template can be updated using kubectl, and the process involves creating a new image with the desired changes and then applying it to the StatefulSet, which will automatically update each pod with the new image.\n",
      "Summary for page:245\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "To update a StatefulSet, edit its definition using `kubectl edit` and modify the spec.replicas and image attributes. Save the file and exit to apply changes. If existing replicas are not updated, delete them manually for the StatefulSet to bring up new ones based on the new template.\n",
      "Summary for page:246\n",
      "Deployments provide an easy way to update applications declaratively using a template.\n",
      "Summary\n",
      "StatefulSets in Kubernetes allow for replicated stateful applications to be deployed, where each pod has a unique identity and storage. When scaling up or down, pods can discover peers and handle horizontal scaling with ease. However, when a node fails abruptly, StatefulSet cannot create a replacement pod until it knows the old pod is no longer running, which requires manual intervention by the cluster administrator. This can be observed by simulating a node's disconnection from the network by shutting down its eth0 interface.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:247\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown. If the node comes back online, the pod is marked as Running again. However, if the pod's status remains unknown for more than a few minutes, it is automatically evicted from the node and deleted.\n",
      "Summary for page:248\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown. If the node comes back online, the pod is marked as Running again. However, if the pod's status remains unknown for more than a few minutes, it is automatically evicted from the node and deleted.\n",
      "Summary\n",
      "A StatefulSet deployment is experiencing issues due to an unresponsive node. The kubia-0 pod is shown as Terminating, but in reality, its container is still running fine. To resolve this, the pod needs to be deleted manually, and a replacement pod should be created by the StatefulSet. However, upon deletion, the kubia-0 pod remains, with an unknown status, despite being deleted.\n",
      "Summary for page:249\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "A pod marked for deletion can't be deleted immediately due to node's network being down. Use --force and --grace-period 0 options with kubectl delete command to forcefully delete the pod. This approach is risky, especially for stateful pods. It's recommended to bring back a disconnected node before continuing.\n",
      "Summary for page:250\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "StatefulSets allow replicated stateful applications to connect with each other through host names and enable forcible deletion of stateful pods, a crucial feature for managing Kubernetes-managed apps.\n",
      "Summary for page:251\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "This chapter delves into Kubernetes internals, explaining how resources are implemented. It covers what makes up a cluster, each component's role and functionality, pod scheduling, controllers in the Controller Manager, and resource deployment. Specific topics include Deployments, pods, networking between pods, Services, and achieving high-availability.\n",
      "Summary for page:252\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "A Kubernetes cluster consists of two parts: the Control Plane, which controls the cluster and its components include etcd, API server, Scheduler, Controller Manager, and add-on components such as DNS server, Dashboard, Ingress controller, Heapster, and Container Network Interface plugin. The worker nodes run the Kubelet, Service Proxy, and Container Runtime, with Docker being a common choice. The distributed nature of these components allows for scalability and flexibility in managing containerized applications.\n",
      "Summary for page:253\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes system components communicate through the API server, which connects to etcd. Components on the worker node run on the same node, but Control Plane components can be split across multiple servers. The API server exposes a ComponentStatus resource showing health status of each component. The kubectl get command displays statuses of all components.\n",
      "Summary for page:254\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes Control Plane components can have multiple instances of etcd and API server running for high availability. However, only one instance of Scheduler and Controller Manager may be active at a given time. These components, along with kube-proxy, can run on the system directly or as pods, with the Kubelet deploying them as pods. The Control Plane components are currently running as pods on the master node in the provided cluster, while worker nodes run kube-proxy and Flannel networking pods.\n",
      "Summary for page:255\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes stores cluster state and metadata in etcd, a fast, distributed key-value store. etcd v3 is recommended due to improved performance. Resources are stored under /registry with version numbers for optimistic concurrency control, preventing simultaneous updates.\n",
      "Summary for page:256\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes stores resource types like pods, secrets, and services as key-value entries in etcd. These entries are JSON representations of the resources, with each entry corresponding to an individual pod or other resource. Prior to v1.7, secrets were stored unencrypted, but from then on they're encrypted for security.\n",
      "Summary for page:257\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes ensures consistency by requiring all Control Plane components to go through the API server, implementing optimistic locking in a single place and validating data written to the store. In a distributed etcd cluster, RAFT consensus algorithm is used to reach a consensus on the actual state, ensuring that each node's state is either the current or previously agreed-upon state. This prevents split-brain scenarios where the cluster splits into two disconnected groups of nodes, allowing only the group with majority (quorum) to accept state changes.\n",
      "Summary for page:258\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes etcd clusters should have an odd number of instances, ideally 5 or 7, to handle failures and maintain a majority for state transitions. The API server provides a CRUD interface over RESTful API for querying and modifying cluster state, storing it in etcd, validating objects, and handling optimistic locking to prevent concurrent updates. Clients like kubectl post requests to the API server through HTTP POST.\n",
      "Summary for page:259\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "The API server authenticates clients using authentication plugins, which extract client data from HTTP requests or certificates. Authorization plugins then determine if the authenticated user can perform requested actions on resources. Admission Control plugins validate and/or modify resource requests, modifying fields, overriding values, or rejecting requests. Examples of Admission Control plugins include AlwaysPullImages, ServiceAccount, NamespaceLifecycle, and ResourceQuota.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:260\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "The Kubernetes API server validates and stores objects in etcd, then notifies clients of resource changes by sending updates to watchers. Clients can watch for changes using an HTTP connection, receiving a stream of modifications to watched objects. The kubectl tool also supports watching resources, allowing users to be notified of creation, modification, or deletion events without needing to constantly poll lists of resources.\n",
      "Summary for page:261\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "The document discusses Kubernetes' scheduling process, where the Scheduler waits for newly created pods through the API server's watch mechanism and assigns a node to each new pod. The Scheduler updates the pod definition, and the API server notifies the Kubelet to create and run the pod's containers. The default Scheduler uses an algorithm that filters nodes to obtain acceptable ones and prioritizes them to choose the best one, with round-robin used if multiple nodes have the highest score.\n",
      "Summary for page:262\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "To determine which nodes are acceptable for a pod, Kubernetes Scheduler passes each node through a list of configured predicate functions that check things such as hardware resource availability, memory or disk pressure conditions, and volume usage. After passing all checks, the Scheduler ends up with a subset of eligible nodes from which it selects the best one based on factors like available resources, pod requirements, and affinity rules.\n",
      "Summary for page:263\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "The Kubernetes Scheduler can be configured to suit specific needs or infrastructure specifics, or even replaced with a custom implementation. Multiple Schedulers can be run in the cluster, and pods can be scheduled using a specified Scheduler by setting the schedulerName property. The Controller Manager also runs controllers that make sure the actual state of the system converges toward the desired state, as specified in resources deployed through the API server, such as ReplicationController, ReplicaSet, DaemonSet, and Job controllers.\n",
      "Summary for page:264\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes controllers are active components that perform work as a result of deployed resources. They watch the API server for changes and perform operations such as creation, update or deletion of resources. Controllers run a reconciliation loop to reconcile actual state with desired state and use the watch mechanism to be notified of changes, also performing periodic re-list operations to ensure they haven't missed anything. Each controller connects to the API server and asks to be notified when a change occurs in its responsible resource list.\n",
      "Summary for page:265\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "The Replication Manager is a controller that makes ReplicationController resources come to life, and it's not the actual work but the manager that does it. It uses the watch mechanism to be notified of changes affecting the desired replica count or number of matched pods, triggering rechecks and actions accordingly. The worker() method contains the magic where all the actual function calls are made.\n",
      "Summary for page:266\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes controllers manage Pod resources through the API server, with different controllers like ReplicaSet, DaemonSet, Job, Deployment, StatefulSet, Node, and Service controlling various aspects of cluster management, such as pod creation, scaling, and load balancing.\n",
      "Summary for page:267\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Services aren't linked directly to pods but contain a list of endpoints which is updated by the Endpoints controller based on pod selector and pod IPs/ports. The Namespace controller deletes all resources in a namespace when it's deleted, while the PersistentVolume controller binds PVs to PVCs matching access mode and capacity requirements.\n",
      "Summary for page:268\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes controllers operate on API objects through the API server without communicating with Kubelets. The Control Plane handles part of the system's operation, while the Kubelet and Service Proxy run on worker nodes, responsible for starting and monitoring containers, reporting status and events to the API server, and terminating containers when their Pod is deleted.\n",
      "Summary for page:269\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "The Kubernetes Service Proxy, also known as kube-proxy, is responsible for making sure clients can connect to services defined through the Kubernetes API. It performs load balancing across pods backing a service and ensures connections end up at one of the pods or non-pod service endpoints. The proxy runs on every worker node and uses iptables rules to intercept connections destined to service IPs, redirecting them to the proxy server.\n",
      "Summary for page:270\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes kube-proxy uses iptables rules to redirect packets to a randomly selected backend pod, without passing them through an actual proxy server. This is called the iptables proxy mode and has performance benefits over user-space proxying, which also balances connections across pods in a true round-robin fashion. Add-ons like DNS lookup and web dashboard are deployed as pods using YAML manifests and can be managed with resources such as Deployments and DaemonSets.\n",
      "Summary for page:271\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes cluster's DNS add-on is a Deployment that provides a DNS server for pods to look up services by name or IP addresses. The DNS server pod uses the API server's watch mechanism to update its records with Service and Endpoints changes. Ingress controllers run reverse proxy servers like Nginx, observing resources through the watch mechanism and configuring the proxy server accordingly. Unlike Services, Ingress controllers forward traffic directly to service pods, preserving client IPs when external clients connect.\n",
      "Summary for page:272\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes system is composed of small, loosely coupled components that work together to synchronize actual and desired state. The API server triggers a coordinated dance of components when submitting a pod manifest or Deployment resource, resulting in containers running. Controllers, Scheduler, Kubelet, and other components watch the API server for changes and cooperate to create and manage resources such as Pods, Deployments, and ReplicaSets.\n",
      "Summary for page:273\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "When a Deployment manifest is submitted to Kubernetes, the API server validates it and returns a response. The Deployment controller creates a ReplicaSet, which in turn creates pods. A chain of notifications through watch mechanisms triggers this process, involving clients such as kubectl, Scheduler, and Kubelet.\n",
      "Summary for page:274\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "The ReplicaSet controller creates Pod resources based on a pod template, which are then scheduled by the Scheduler to a specific node. The Kubelet runs the containers on the assigned node, and both the Control Plane components and Kubelet emit events to the API server as they perform these actions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:275\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "A running pod is a logical host that can contain one or more containers and has its own IP address. It's created by the Kubelet which runs the container(s) specified in the pod spec. The Kubelet creates a network namespace for each pod, allowing for isolated networking between pods.\n",
      "Summary for page:276\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes uses an additional 'pause' container to hold all containers of a pod together, sharing network and other Linux namespaces. This infrastructure container runs from pod scheduling until deletion, allowing application containers to reuse these namespaces if restarted.\n",
      "Summary for page:277\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes achieves inter-pod networking by not setting up the network itself, but rather relying on system administrators or CNI plugins to do so. The network must allow pods to communicate with each other without NAT and with the same IP addresses visible to all pods. This enables simple networking for applications running inside pods as if they were connected to the same network switch.\n",
      "Summary for page:278\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "A Kubernetes cluster's inter-pod networking works by creating a virtual Ethernet interface pair (veth pair) for each pod, connecting it to the same bridge as other pods on the same node. The pod's containers use its network namespace and IP address, which is set up and held by the infrastructure container (pause container). This allows communication between pods on the same node without needing NAT.\n",
      "Summary for page:279\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Pods in a Kubernetes cluster can communicate with each other using the container runtime's network bridge. To enable communication between pods on different nodes, bridges must use non-overlapping IP address ranges and can be connected through overlay or underlay networks, regular layer 3 routing, or by configuring node physical network interfaces and routing tables to route packets between nodes.\n",
      "Summary for page:280\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes uses a veth pair to connect containers on the same network switch without routers in between. To make it easier, a Software Defined Network (SDN) can be used, or a Container Network Interface (CNI) plugin such as Calico, Flannel, Romana, Weave Net can be installed by deploying a YAML containing a DaemonSet and supporting resources. Services are implemented to expose a set of pods at a long-lived, stable IP address and port.\n",
      "Summary for page:281\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes' kube-proxy handles Services by assigning a stable IP address and port, which clients connect to. The virtual IP address is assigned immediately upon service creation, and kube-proxy sets up iptables rules on worker nodes to redirect packets destined for the service IP/port pair to one of the backing pods. Kube-proxy also watches Endpoints objects for changes in backing pods.\n",
      "Summary for page:282\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "A packet sent to a Kubernetes service's virtual IP is modified by the kernel on the node it's received on, according to iptables rules. If the packet matches a rule, its destination IP and port are changed to point to a randomly selected backend pod. This process is handled by kube-proxy, which watches for changes to services and endpoints.\n",
      "Summary for page:283\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "To achieve high availability in Kubernetes, apps can be run through a Deployment resource with an appropriate number of replicas. If a replica becomes unavailable, it will be replaced quickly, although there may be a short period of downtime. For non-horizontally scalable apps, leader-election mechanisms can be used to ensure only one instance is active at a time, avoiding downtime. Kubernetes itself requires high availability, and its Control Plane components can be made highly available using techniques such as load balancing and multiple masters.\n",
      "Summary for page:284\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "To make Kubernetes highly available, run multiple master nodes with etcd, API server, Controller Manager, and Scheduler components. Each component can be made highly available by running multiple instances and replicating data across them, ensuring the cluster can handle failures and maintain read/write operations.\n",
      "Summary for page:285\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Running multiple instances of etcd, API servers, controllers, and schedulers can provide high availability in Kubernetes clusters. However, careful consideration is needed for components like the Controller Manager and Scheduler to avoid racing conditions and undesired effects. The use of leader election mechanisms can help ensure that only one instance is active at a time, providing a stable and reliable system.\n",
      "Summary for page:286\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes components such as Controller Manager and Scheduler can run collocated or on separate machines. Leader election is achieved by creating a resource in the API server using an Endpoints object, which has no side effects unless a Service with the same name exists. The first instance to successfully write its name into the resource becomes the leader, and periodic updates from the leader ensure that other instances know it's still alive.\n",
      "Summary for page:287\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes components such as API server, Scheduler, controllers, Kubelet, and kube-proxy work together to bring a pod to life. Each component has a specific role: API server receives requests, Scheduler assigns resources, controllers manage pods, Kubelet runs containers on nodes, and kube-proxy performs load balancing. High availability is achieved by running multiple instances of each component.\n",
      "Summary for page:288\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "This chapter covers authentication, ServiceAccounts, and permissions configuration in a Kubernetes cluster. It explains how the API server handles requests using authentication plugins and introduces the concept of ServiceAccounts for authenticating applications running in pods.\n",
      "Summary for page:289\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes uses authentication plugins to determine who's sending a request by examining the request and returning the username, user ID, and groups to the API server core. The API server stops invoking remaining plugins and continues onto authorization. Authentication plugins can obtain client identity using methods such as client certificate, authentication token, or basic HTTP authentication. Kubernetes distinguishes between users (actual humans) and pods (applications running inside them), with users managed by external systems like SSO and pods using service accounts created in the cluster.\n",
      "Summary for page:290\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "ServiceAccounts are identities of apps running in pods, allowing them to authenticate with the API server using a token. The API server passes the username to authorization plugins, which determine if actions can be performed. ServiceAccounts are resources scoped to individual namespaces and can be listed like other Kubernetes resources.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:291\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "Kubernetes authentication works by assigning ServiceAccounts to pods, which determine resource access. Each namespace has a default ServiceAccount, but additional ones can be created for cluster security reasons, such as running pods under constrained accounts or granting permissions to retrieve or modify resources.\n",
      "Summary for page:292\n",
      "StatefulSets handle node failures by marking the node as NotReady and pods on that node's status as Unknown.\n",
      "Summary\n",
      "A ServiceAccount is created with `kubectl create serviceaccount` and can be inspected with `kubectl describe sa`. A custom token Secret is associated with the ServiceAccount, containing a CA certificate, namespace, and token. The token is a JSON Web Token (JWT) that can be mounted inside a pod if 'mountable Secrets' are enforced.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:293\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories. A pod's ServiceAccount must be set when creating the pod, and it cannot be changed later. Image pull Secrets are added automatically to all pods using a ServiceAccount, saving the need to add them individually.\n",
      "Summary for page:294\n",
      "A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories.\n",
      "Summary\n",
      "A Kubernetes Pod is created using a non-default ServiceAccount named foo, which allows it to list pods when talking to the API server. The Pod's containers can access the token from the ServiceAccount and use it to authenticate with the API server, as shown by the successful response received from listing pods.\n",
      "Summary for page:295\n",
      "A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories.\n",
      "Summary\n",
      "Kubernetes' Role-Based Access Control (RBAC) authorization plugin prevents unauthorized users from viewing or modifying the cluster state. The default ServiceAccount isn't allowed to view or modify the cluster state, unless granted additional privileges. Additional authorization plugins like Attribute-based access control (ABAC), Web- Hook, and custom implementations are also available, but RBAC is the standard.\n",
      "Summary for page:296\n",
      "A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories.\n",
      "Summary\n",
      "The Kubernetes API server's security is ensured through the RBAC authorization plugin, which uses user roles to determine permissions. Roles are associated with subjects and allow certain verbs on resources or non-resource URL paths. Managing authorization is done by creating four RBAC-specific Kubernetes resources, including RoleBindings and ClusterRoleBindings.\n",
      "Summary for page:297\n",
      "A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories.\n",
      "Summary\n",
      "RBAC (Role-Based Access Control) in Kubernetes is configured through four resources: Roles and ClusterRoles that define what can be done on resources, and RoleBindings and ClusterRoleBindings that bind roles to users or groups. Roles are namespaced while ClusterRoles are cluster-level, allowing multiple bindings within a namespace or across the cluster.\n",
      "Summary for page:298\n",
      "A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories.\n",
      "Summary\n",
      "To secure the Kubernetes API server, RBAC must be enabled in the cluster by setting version 1.6 or higher and disabling legacy authorization if using GKE 1.6 or 1.7. Minikube requires enabling RBAC with --extra-config. The permissive-binding clusterrolebinding should be deleted to re-enable RBAC.\n",
      "Summary for page:299\n",
      "A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories.\n",
      "Summary\n",
      "The document explains how to secure a Kubernetes cluster with role-based access control (RBAC). It demonstrates creating two pods in separate namespaces using kubectl commands and attempting to list services from within each pod using curl. The example shows that RBAC prevents the default ServiceAccount from listing services, even though it's running in the same namespace, and guides the reader on how to create a Role resource to allow the ServiceAccount to perform such actions.\n",
      "Summary for page:300\n",
      "A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories.\n",
      "Summary\n",
      "A Role resource defines what actions can be taken on which resources, allowing users to get and list Services in a specific namespace (foo) via a Role named service-reader.\n",
      "Summary for page:301\n",
      "A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories.\n",
      "Summary\n",
      "To secure a Kubernetes cluster with role-based access control, create a Role (e.g. service-reader) in a namespace using kubectl create or -f service-reader.yaml. Bind the Role to a ServiceAccount in the same namespace using kubectl create rolebinding, specifying the Role and ServiceAccount names. This grants permissions for the ServiceAccount to perform actions defined by the Role.\n",
      "Summary for page:302\n",
      "A ServiceAccount in Kubernetes allows pods to mount Secrets and can contain image pull Secrets for private image repositories.\n",
      "Summary\n",
      "A RoleBinding references a single Role and can bind it to multiple subjects, such as ServiceAccounts, users, or groups. In this case, the test RoleBinding binds the default ServiceAccount with the service-reader Role in the foo namespace, allowing the pod running under that account to list Services.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:303\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "You can add a pod's ServiceAccount from another namespace to a RoleBinding by editing the RoleBinding and adding the subject, allowing the pod to list Services in its own and other namespaces.\n",
      "Summary for page:304\n",
      "You can add a pod's ServiceAccount from another namespace to a RoleBinding by editing the RoleBinding and adding the subject, allowing the pod to list Services in its own and other namespaces.\n",
      "Summary\n",
      "Regular Roles allow access to resources in the same namespace but not across different namespaces. ClusterRoles, on the other hand, are cluster-level resources that can grant access to non-namespaced resources, non-resource URLs, or used as a common role inside individual namespaces. A ClusterRole is created using kubectl create clusterrole with the desired verb and resource, and can be bound to a ServiceAccount in a specific namespace using ClusterRoleBinding.\n",
      "Summary for page:305\n",
      "You can add a pod's ServiceAccount from another namespace to a RoleBinding by editing the RoleBinding and adding the subject, allowing the pod to list Services in its own and other namespaces.\n",
      "Summary\n",
      "To secure a Kubernetes cluster with role-based access control, create a ClusterRole that specifies API groups, resources, and verbs. Bind this ClusterRole to a ServiceAccount using a RoleBinding, then verify if the ServiceAccount can list PersistentVolumes using curl.\n",
      "Summary for page:306\n",
      "You can add a pod's ServiceAccount from another namespace to a RoleBinding by editing the RoleBinding and adding the subject, allowing the pod to list Services in its own and other namespaces.\n",
      "Summary\n",
      "To secure the Kubernetes API server, you must use a ClusterRoleBinding to grant access to cluster-level (non-namespaced) resources, unlike with namespaced resources where a RoleBinding can be used. The command to create a ClusterRoleBinding is similar to that of a RoleBinding, but without specifying the namespace and replacing rolebinding with clusterrolebinding.\n",
      "Summary for page:307\n",
      "You can add a pod's ServiceAccount from another namespace to a RoleBinding by editing the RoleBinding and adding the subject, allowing the pod to list Services in its own and other namespaces.\n",
      "Summary\n",
      "To secure a Kubernetes cluster with role-based access control, use a ClusterRole and a ClusterRoleBinding to grant access to cluster-level resources. Non-resource URLs must also be granted explicitly, usually done through the system:discovery ClusterRole and its binding, which allow access to URLs like /api, /apis, /healthz, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:308\n",
      "To add a pod's ServiceAccount from another namespace to a RoleBinding, edit the RoleBinding and add the subject, allowing the pod to list Services in its own and other namespaces.\n",
      "Summary\n",
      "The system:discovery ClusterRole allows access to non-resource URLs with only GET HTTP method, and can be bound to all users through a ClusterRoleBinding. This binding grants access to the API server's /api URL path to anyone who accesses it from within a pod.\n",
      "Summary for page:309\n",
      "To add a pod's ServiceAccount from another namespace to a RoleBinding, edit the RoleBinding and add the subject, allowing the pod to list Services in its own and other namespaces.\n",
      "Summary\n",
      "ClusterRoles can be used with namespaced RoleBindings to grant access to specific namespaces and their resources. The 'view' ClusterRole allows reading (get, list, watch) but not writing resources in a namespace, demonstrating how ClusterRoles can control access to resources within a specific scope.\n",
      "Summary for page:310\n",
      "To add a pod's ServiceAccount from another namespace to a RoleBinding, edit the RoleBinding and add the subject, allowing the pod to list Services in its own and other namespaces.\n",
      "Summary\n",
      "The Kubernetes API server's permissions are determined by a ClusterRoleBinding or RoleBinding. A ClusterRoleBinding allows subjects to view resources across all namespaces, while a RoleBinding only allows viewing resources in the namespace of the binding. The example demonstrates listing pods using curl commands before and after creating a ClusterRoleBinding, showing that it applies across all namespaces.\n",
      "Summary for page:311\n",
      "To add a pod's ServiceAccount from another namespace to a RoleBinding, edit the RoleBinding and add the subject, allowing the pod to list Services in its own and other namespaces.\n",
      "Summary\n",
      "A pod can access namespaced resources in any namespace by combining a ClusterRoleBinding with a ClusterRole referring to namespaced resources. Replacing the ClusterRoleBinding with a RoleBinding limits the pod's access to only the specified namespace, as demonstrated with the creation of a RoleBinding in the foo namespace.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:312\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles. The document explains various combinations of these concepts for specific use cases, such as accessing cluster-level resources, non-resource URLs, namespaced resources in any or specific namespaces. It also highlights how a ServiceAccount can only view pods within its own namespace, despite using a ClusterRole.\n",
      "Summary for page:313\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "Kubernetes has a default set of ClusterRoles and ClusterRoleBindings that are updated every time the API server starts, allowing for automatic recreation if deleted or changed in newer versions. The default roles include cluster-admin, system:basic-user, and various controller roles.\n",
      "Summary for page:314\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "The most important ClusterRoles in Kubernetes are view, edit, admin, and cluster-admin, designed to prevent privilege escalation. The view role allows read-only access, while the edit role grants modifying resources within a namespace. The admin role provides complete control of a namespace's resources, but not ResourceQuotas or Namespace itself. The cluster-admin role offers complete control over the entire Kubernetes cluster.\n",
      "Summary for page:315\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "The Controller Manager runs as a single pod, but each controller can use a separate ClusterRole and ClusterRoleBinding. By default, ServiceAccounts have no permissions, so pods can't view cluster state. It's best to grant only the necessary permissions (principle of least privilege). Create specific ServiceAccounts for each pod and associate them with tailor-made Roles through RoleBindings. Constrain ServiceAccounts to prevent damage if compromised.\n",
      "Summary for page:316\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "Kubernetes API server security is discussed. ServiceAccounts are used to run pods, with default accounts created for each namespace. Additional accounts can be created manually and configured to allow mounting specific Secrets. Roles and ClusterRoles define allowed actions on resources, and RoleBindings bind these to users, groups, and ServiceAccounts. This sets the stage for securing cluster nodes and isolating pods in the next chapter.\n",
      "Summary for page:317\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "This chapter focuses on securing cluster nodes and the network, allowing pods to access node resources while limiting user actions. Key topics include using default Linux namespaces in pods, running containers as different users, privileged containers, modifying kernel capabilities, defining security policies, and securing the pod network.\n",
      "Summary for page:318\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "In Kubernetes, containers in a pod run under separate namespaces, isolating their processes from other containers or the node's default namespace. Certain system pods can use the node's network namespace by setting hostNetwork to true in the pod spec, allowing them to see and manipulate node-level resources and devices.\n",
      "Summary for page:319\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "A Kubernetes pod can use the host's network namespace by setting `hostNetwork: true` in its spec. This allows it to see all the host's network adapters and bind to a port in the node's default namespace using `hostPort`. Note that this is different from a NodePort service, which binds the port on all nodes even if no pod is running on them.\n",
      "Summary for page:320\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "When using a specific host port in a pod, only one instance of the pod can be scheduled to each node due to multiple processes cannot bind to the same host port. The Scheduler takes this into account and doesn't schedule multiple pods to the same node, allowing only three pods to be scheduled out of four replicas when three nodes are available.\n",
      "Summary for page:321\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "Using the host node's namespaces in a pod, you can define hostPort in a pod's YAML definition. This allows access to the pod through the node's port, but not on other nodes. The hostPID and hostIPC pod spec properties allow containers to use the node's PID and IPC namespaces, respectively.\n",
      "Summary for page:322\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "This chapter discusses securing cluster nodes and networks by configuring the security context of pods and containers. This includes setting hostIPC to true for processes to communicate, configuring container security through user ID, preventing root access, running in privileged mode, adding or dropping capabilities, setting SELinux options, and preventing process writing to the filesystem.\n",
      "Summary for page:323\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "To run a pod under a different user ID than the one baked into the container image, set the pod's securityContext.runAsUser property. This was shown by running a pod as user 'guest' with UID 405 and verifying the result using the id command inside the container.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:324\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "The chapter discusses securing cluster nodes and the network by preventing containers from running as root. It explains how to prevent an attacker from pushing a malicious image under the same tag as a trusted image, and how to specify that a pod's container needs to run as a non-root user using `runAsNonRoot: true`. It also touches on running pods in privileged mode for specific use cases.\n",
      "Summary for page:325\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "A Kubernetes pod's container can run in privileged mode by setting the `privileged` property to true in its security context, allowing access to the node's kernel and device files. This is demonstrated by comparing the devices visible in a non-privileged container with those in a privileged container.\n",
      "Summary for page:326\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "In Kubernetes, instead of making a container privileged and giving it unlimited permissions, you can give it access only to the kernel features it really requires by adding individual kernel capabilities. This allows fine-tuning of the container's permissions and limiting the impact of a potential intrusion. For example, you can add CAP_SYS_TIME capability to allow the container to change the system time.\n",
      "Summary for page:327\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "Configuring a container's security context by adding or dropping Linux kernel capabilities, such as SYS_TIME. This can be done under the securityContext property in a pod spec, and is a more controlled way than giving full privileges with privileged: true. Capabilities can be added or dropped to allow specific actions, but requires knowledge of what each capability does.\n",
      "Summary for page:328\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "To prevent containers from modifying the owner of files or writing to their own filesystem, Kubernetes capabilities can be dropped and the readonlyRootFilesystem property set to true. This prevents malicious code injection in case of vulnerabilities.\n",
      "Summary for page:329\n",
      "Securing the Kubernetes API server involves understanding the difference between ClusterRoles, ClusterRoleBindings, RoleBindings, and Roles.\n",
      "Summary\n",
      "When configuring a pod's security context, setting the container's readOnlyRootFilesystem property to true makes the filesystem read-only, preventing write access to the / directory. However, writing to a mounted volume is allowed. To increase security in production environments, set this property to true at the pod level or override it at the container level.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:330\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs. The fsGroup and supplementalGroups properties are used in a pod's security context to achieve this. An example is provided where two containers with different user IDs share the same volume, and the container running as user ID 1111 can read or write files of the container running as user ID 2222 due to the shared group permissions.\n",
      "Summary for page:331\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "The document explains how to restrict the use of security-related features in pods. It discusses the fsGroup and supplementalGroups properties, which are used to set group IDs for users running containers. The fsGroup property sets the ownership of a mounted volume, while the supplementalGroups property defines additional group IDs associated with a user. A cluster administrator can restrict the use of these features by creating PodSecurityPolicy resources, which define what security-related features users can or cannot use in their pods.\n",
      "Summary for page:332\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "A PodSecurityPolicy admission control plugin validates pod definitions against configured policies before storing them in etcd. A PodSecurityPolicy resource defines settings such as IPC and network namespace usage, host ports, user IDs, and privileged container creation. To enable RBAC and PodSecurityPolicy admission control in Minikube, use the command: $ minikube start --extra-config apiserver.Authentication.PasswordFile=/etc/kubernetes/passwd --extra-config=apiserver.Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRunOptions.AdmissionControl=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,PodSecurityPolicy. Create a password file with the command: $ cat <<EOF | minikube ssh sudo tee /etc/kubernetes/passwd password,alice,1000,basic-user password,bob,2000,privileged-user EOF\n",
      "Summary for page:333\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "This document discusses restricting security-related features in pods, specifically kernel capabilities, SELinux labels, writable root filesystems, and volume types. A sample PodSecurityPolicy is provided that prevents pods from using the host's IPC, PID, and Network namespaces, and restricts privileged containers and host ports. The policy allows containers to run as any user or group, and use any SELinux groups.\n",
      "Summary for page:334\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "A cluster node and network security chapter that explains the restriction of deploying privileged pods due to a pod security policy. It also discusses how to constrain container user IDs using the MustRunAs rule, with examples showing how to specify allowed ID ranges for runAsUser, fsGroup, and supplementalGroups fields.\n",
      "Summary for page:335\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "A PodSecurityPolicy can restrict the use of security-related features in pods, enforcing only when creating or updating pods. If a pod spec tries to set fields outside allowed ranges, it's rejected by the API server. However, if a container image has an out-of-range user ID, but the runAsUser property is not set, the API server may still accept the pod and run the container with the specified ID in the PodSecurityPolicy.\n",
      "Summary for page:336\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "Securing cluster nodes and network involves using the MustRunAsNonRoot rule in the runAsUser field, preventing users from deploying containers that run as root. Configuring allowed, default, and disallowed capabilities includes specifying which capabilities can be added or dropped in a container using fields like allowedCapabilities, defaultAddCapabilities, and requiredDropCapabilities. This helps control what operations containers can perform by adding or dropping Linux kernel capabilities.\n",
      "Summary for page:337\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "PodSecurityPolicy resources allow restricting security-related features in pods, adding capabilities to containers, and constraining volume types. Capabilities can be added or dropped from containers using defaultAddCapabilities and requiredDropCapabilities fields respectively. Volume types can also be restricted, with a minimum of emptyDir, configMap, secret, downwardAPI, and persistentVolumeClaim allowed.\n",
      "Summary for page:338\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "PodSecurityPolicies (PSPs) are cluster-level resources that can't be stored in a specific namespace. Different PSBs can be assigned to different users and groups using the RBAC mechanism by creating ClusterRole resources, pointing them to individual policies, and binding them to users or groups with ClusterRoleBindings. A new PSP is created to allow privileged containers to be deployed, allowing for more flexibility in managing system pods and user pods.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:339\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "To restrict security-related features in pods, you can use PodSecurityPolicies. You can create two ClusterRoles (psp-default and psp-privileged) to allow different users to use specific policies. Bind these roles to users using a ClusterRoleBinding, referencing the policies by name (e.g., default or privileged). This way, Alice can only deploy non-privileged pods while Bob can deploy both. Authenticated users will have access to the default policy.\n",
      "Summary for page:340\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "You'll bind the psp-privileged ClusterRole only to Bob by creating a clusterrolebinding. Alice will have access to the default PodSecurity-Policy, while Bob has access to both. To authenticate as Alice or Bob, create new users in kubectl's config with set-credentials commands. You can then use the --user option to create pods with different user credentials, demonstrating that Bob can create privileged pods while Alice cannot.\n",
      "Summary for page:341\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "Isolating the pod network by configuring NetworkPolicy resources, which apply to pods matching a label selector and specify sources or destinations that can access the matched pods. This is configurable if the container networking plugin supports it. A default-deny NetworkPolicy prevents all clients from connecting to any pod in a namespace, and can be enabled by creating a NetworkPolicy with an empty pod selector.\n",
      "Summary for page:342\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "To secure cluster nodes and network, a CNI plugin or networking solution must support NetworkPolicy. A NetworkPolicy resource can be created in the same namespace as a database pod to allow only specific pods (e.g. webserver) to connect on port 5432, while blocking other pods from connecting to the database.\n",
      "Summary for page:343\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "To secure a microservice for a specific tenant, create a NetworkPolicy that allows only pods from the same tenant's namespaces to access the microservice on a specific port. The policy applies to pods labeled with 'microservice=shopping-cart' and allows access from namespaces labeled as 'tenant=manning'. This example demonstrates how to isolate network traffic between Kubernetes namespaces for multiple tenants using the same cluster.\n",
      "Summary for page:344\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "In a multi-tenant Kubernetes cluster, tenants can't add labels to their namespaces themselves. NetworkPolicies ensure only pods running in specific namespaces or IP blocks can access targeted pods. An example ingress rule allows traffic from the 192.168.1.0/24 IP block to access shopping-cart pods.\n",
      "Summary for page:345\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "In this chapter, Network Policies are used to limit a pod's inbound and outbound traffic. Egress rules allow limiting outbound traffic of a set of pods by specifying which pods they can connect to. PodSecurityPolicy resources can be created to prevent users from creating pods that could compromise a node. Cluster-level policies can be associated with specific users using RBAC's ClusterRoles and ClusterRoleBindings.\n",
      "Summary for page:346\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "This chapter covers requesting CPU and memory computational resources for containers, setting hard limits, understanding Quality of Service guarantees for pods, and limiting total resources in a namespace.\n",
      "Summary for page:347\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "When creating a pod, you can specify resource requests and limits for each container individually. The pod's total resources are the sum of all containers' requests and limits. Requests define the minimum amount of CPU and memory a container needs, while limits set a hard limit on what it may consume.\n",
      "Summary for page:348\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "The Kubernetes Scheduler determines if a pod can fit on a node based on the sum of resources requested by existing pods, not actual resource consumption. A pod's resource requests specify its minimum requirements, and scheduling is denied if unallocated resources are insufficient to meet these requirements. Requests don't limit CPU usage, but specifying a CPU limit does.\n",
      "Summary for page:349\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "The Kubernetes Scheduler prioritizes nodes based on requested resources, using functions like LeastRequestedPriority and MostRequestedPriority to select the best node for a pod. The MostRequestedPriority function is useful in cloud infrastructure where adding or removing nodes is possible, as it allows for tight packing of pods and potential removal of unused nodes, saving costs.\n",
      "Summary for page:350\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "This chapter discusses managing pods' computational resources, specifically the Node resource. It explains that a node's capacity represents its total resources, but not all are available to pods due to reserved resources for Kubernetes and system components. The Scheduler bases its decisions on allocatable resource amounts. A pod with CPU requests of 800 millicores was successfully scheduled, but attempting to deploy another pod with 1 core of CPU request did not fit on any node.\n",
      "Summary for page:351\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "A Kubernetes pod's container request for 1 whole core CPU instead of millicores causes scheduling failure due to insufficient CPU on a single node. The issue is resolved by inspecting the node resource with `kubectl describe node` and examining the output, which shows that the node has allocated resources that are not associated with the pod, resulting in failed scheduling.\n",
      "Summary for page:352\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "The Kubernetes Scheduler allocates computational resources based on pod requests and limits. A total of 1,275 millicores have been requested by running pods, exceeding initial requests. The culprit behind additional CPU resources usage is identified in the kube-system namespace. To free up resources for a third pod to be scheduled, one of the first two pods can be deleted. This triggers the Scheduler to schedule the third pod as soon as the deleted pod terminates. Both CPU and memory requests are treated equally by the Scheduler.\n",
      "Summary for page:353\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "Kubernetes distributes unused CPU time among pods in a ratio based on their CPU requests, allowing one pod to consume all available CPU if the other is idle. Custom resources can also be added and requested by pods, initially as Opaque Integer Resources, then replaced with Extended Resources in Kubernetes 1.8.\n",
      "Summary for page:354\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "To manage pods' computational resources in Kubernetes, a custom resource can be added to the Node object's capacity field. This involves performing a PATCH HTTP request and specifying the resource name and quantity. When creating pods, the same resource name and requested quantity must be specified under the resources.requests field. Resource limits for containers can also be set to prevent them from using up excessive CPU or memory. Limits can be set on both CPU and memory usage, preventing malfunctioning or malicious pods from affecting other nodes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:355\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "A Kubernetes pod's container has configured resource limits for CPU and memory, limiting its consumption to 1 CPU core and 20Mi of memory. Resource limits are not constrained by the node's allocatable resources and can be overcommitted, potentially leading to containers being killed if resources are fully utilized.\n",
      "Summary for page:356\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "When a process running in a container tries to use more resources than allowed, the process is throttled for CPU usage. However, for memory, if the process allocates more memory than its limit, it's killed (OOMKilled) and restarted by Kubernetes with increasing delays between restarts, eventually resulting in a CrashLoopBackOff status.\n",
      "Summary for page:357\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "Containers may get OOMKilled even if they aren't over their memory limit due to the way apps in containers see limits. The top command shows memory amounts of the whole node, not the container's memory limit, which can be misleading. Containers always see the node's memory, not their own, and this can lead to unexpected behavior.\n",
      "Summary for page:358\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "When running containers in a Kubernetes cluster, it's essential to manage pods' computational resources properly. Containers can see all node CPUs and may exceed memory limits if not configured correctly. The JVM may be OOMKilled when the heap size exceeds container memory limits. Setting -Xmx options doesn't solve issues with off-heap memory. New Java versions consider container limits, but certain applications that rely on CPU count for worker threads may spin up too many threads and exceed resources.\n",
      "Summary for page:359\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "Kubernetes categorizes pods into three Quality of Service (QoS) classes: BestEffort (lowest priority), Burstable, and Guaranteed (highest). The QoS class is derived from a pod's resource requests and limits. A Guaranteed class is assigned to pods with equal request and limit settings for CPU and memory, while a BestEffort class is given to pods with no requests or limits set. This allows Kubernetes to make decisions on which containers to kill in case of resource shortages.\n",
      "Summary for page:360\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "A pod's Quality of Service (QoS) class is determined by the relationship between its resource requests and limits. The three QoS classes are BestEffort, Burstable, and Guaranteed. A pod with a best effort QoS can consume any available resources. A burstable pod gets the requested amount of resources, but can use additional ones up to their limit if needed. A guaranteed pod gets the exact amount of resources it requests.\n",
      "Summary for page:361\n",
      "Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs.\n",
      "Summary\n",
      "A pod's QoS (Quality of Service) class is determined by the classes of its containers, and can be BestEffort, Burstable, or Guaranteed. For single-container pods, requests and limits are used to determine the class, while for multi-container pods, the highest container class determines the pod's class. If all containers have the same QoS class, it's also the pod's class, but if at least one container has a different class, the pod's class is Burstable.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:362\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "When system memory is overcommitted, QoS classes determine which container gets killed first. BestEffort class gets killed first, followed by Burstable, and finally Guaranteed. If containers have the same QoS class, the process with the highest OutOfMemory (OOM) score gets killed, calculated from available memory consumption and fixed OOM score adjustment based on QoS class and requested memory.\n",
      "Summary for page:363\n",
      "When system memory is overcommitted, QoS classes determine which container gets killed first. BestEffort class gets killed first, followed by Burstable, and finally Guaranteed. If containers have the same QoS class, the process with the highest OutOfMemory (OOM) score gets killed, calculated from available memory consumption and fixed OOM score adjustment based on QoS class and requested memory.\n",
      "Summary\n",
      "To avoid containers being at the mercy of others that specify resource requests and limits, it's recommended to set these values on every container or use a LimitRange resource per namespace to specify minimum/max limit values and default resource requests.\n",
      "Summary for page:364\n",
      "When system memory is overcommitted, QoS classes determine which container gets killed first.\n",
      "Summary\n",
      "A LimitRange resource is used by the LimitRanger Admission Control plugin to validate pod specs. It prevents users from creating pods bigger than any node in the cluster, specifying limits for individual containers or objects in the same namespace, but not total resources across all pods.\n",
      "Summary for page:365\n",
      "When system memory is overcommitted, QoS classes determine which container gets killed first.\n",
      "Summary\n",
      "A LimitRange object is used to set default requests and limits for pods per namespace, applying to containers' requests and limits. It allows setting min/max values, default resource requests/limits, and max ratio of limits vs requests. This validation is performed by the API server when receiving a new pod manifest, and does not affect existing pods or PVCs created before modifying the limits.\n",
      "Summary for page:366\n",
      "When system memory is overcommitted, QoS classes determine which container gets killed first.\n",
      "Summary\n",
      "A Kubernetes LimitRange object is used to set maximum CPU and memory usage limits for pods and containers. When creating a pod with a container requesting more than the allowed limit, the pod is rejected due to the Forbidden error message from the server listing all reasons why the pod was rejected. Default resource requests and limits can be applied automatically when creating a pod by setting them in a LimitRange object, allowing admins to configure default, min, and max resources for pods per namespace.\n",
      "Summary for page:367\n",
      "When system memory is overcommitted, QoS classes determine which container gets killed first.\n",
      "Summary\n",
      "Resource quotas limit the total amount of resources available in a namespace, including computational resources, storage, number of pods, claims, and API objects. They are enforced at pod creation time and do not affect existing pods. A ResourceQuota object can be created to specify quotas for CPU and memory, as shown in Listing 14.13.\n",
      "Summary for page:368\n",
      "When system memory is overcommitted, QoS classes determine which container gets killed first.\n",
      "Summary\n",
      "A ResourceQuota object sets separate totals for requests and limits of CPU and memory resources in a namespace. It can be inspected using kubectl describe quota, showing used amounts for each resource. The quota applies to all pods' resource requests and limits in total, unlike LimitRange which applies to individual pods or containers separately.\n",
      "Summary for page:369\n",
      "When system memory is overcommitted, QoS classes determine which container gets killed first.\n",
      "Summary\n",
      "A ResourceQuota object can limit resources available in a namespace, including CPU, memory, persistent storage, and the number of objects that can be created. A LimitRange object is required alongside ResourceQuota to specify resource requests or limits for pods.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:370\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "A ResourceQuota in Kubernetes limits the number of objects that can be created in a namespace, such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services. The quota specifies hard limits for each object type, e.g., 10 pods, 5 replication controllers, 10 secrets, etc. It also allows limiting specific types of services like load balancers and node ports.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:371\n",
      "A ResourceQuota in Kubernetes limits the number of objects that can be created in a namespace, such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "You can limit resources in a namespace by specifying quotas for specific pod states and/or QoS classes, using quota scopes like BestEffort, NotBestEffort, Terminating, and NotTerminating. These scopes apply to pods with certain QoS classes or active deadline seconds, and can be used to limit the number of pods, CPU/memory requests, and limits. For example, a ResourceQuota for BestEffort/NotTerminating pods can ensure that at most four such pods exist.\n",
      "Summary for page:372\n",
      "A Kubernetes ResourceQuota limits objects in a namespace like pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "Properly setting resource requests and limits in Kubernetes is crucial for efficient cluster usage. Monitoring actual resource usage under expected load levels helps find the optimal spot. The Kubelet contains an agent called cAdvisor that collects basic resource consumption data, while Heapster collects and exposes metrics from all nodes in a single location.\n",
      "Summary for page:373\n",
      "A Kubernetes ResourceQuota limits objects in a namespace like pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "Heapster is a component that collects container and node usage data without needing to connect to the processes running inside pods' containers. It can be enabled in Minikube using `minikube addons enable heapster`. Once enabled, you can use `kubectl top` commands to see actual CPU and memory usage for cluster nodes and individual pods.\n",
      "Summary for page:374\n",
      "A Kubernetes ResourceQuota limits objects in a namespace like pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "The kubectl top command shows current pod metrics, but may not show metrics immediately due to Heapster aggregation. Historical resource consumption can be analyzed using tools like InfluxDB and Grafana for storing and visualizing data.\n",
      "Summary for page:375\n",
      "A Kubernetes ResourceQuota limits objects in a namespace such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "InfluxDB and Grafana can be run as pods in a Kubernetes cluster, providing monitoring capabilities for resource usage. Deploying them is straightforward using manifests available in the Heapster Git repository or enabled with Minikube's Heapster add-on. To analyze pod resource usage, open the Grafana web console to explore predefined dashboards and discover CPU usage across the cluster.\n",
      "Summary for page:376\n",
      "A Kubernetes ResourceQuota limits objects in a namespace such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "When using Minikube, Grafana's web console can be opened in a browser to view resource usage statistics for nodes and pods. The Cluster dashboard shows overall cluster usage, while the Pods dashboard displays individual pod resource usages. By examining these charts, users can determine if resource requests or limits need to be adjusted to accommodate more pods on a node.\n",
      "Summary for page:377\n",
      "A Kubernetes ResourceQuota limits objects in a namespace such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "This chapter emphasizes considering a pod's resource usage, configuring requests and limits to keep everything running smoothly. Key takeaways include specifying resource requests to schedule pods across the cluster, setting resource limits to prevent pods from starving other resources, and managing CPU and memory usage to optimize application performance.\n",
      "Summary for page:378\n",
      "A Kubernetes ResourceQuota limits objects in a namespace such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "To manage pods' computational resources, use LimitRange objects for individual resource requests and limits or ResourceQuota objects to limit namespace-wide resources. Monitor pod usage over time to determine optimal resource settings, with Kubernetes using these metrics for automatic scaling in the next chapter.\n",
      "Summary for page:379\n",
      "A Kubernetes ResourceQuota limits objects in a namespace such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "Applications can be scaled out manually or automatically by increasing replicas or resource requests. However, manual scaling is not ideal for sudden traffic increases. This chapter covers configuring automatic horizontal scaling of pods and cluster nodes based on CPU utilization and custom metrics, as well as understanding vertical scaling limitations.\n",
      "Summary for page:380\n",
      "A Kubernetes ResourceQuota limits objects in a namespace such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "Kubernetes can automatically scale pods and cluster nodes based on CPU usage or other metrics, spinning up additional nodes if necessary. The autoscaling feature was rewritten between Kubernetes 1.6 and 1.7, so outdated information may exist online. Horizontal pod autoscaling adjusts the number of replicas by periodically checking pod metrics, calculating the required number of replicas, and updating the replicas field on the target resource.\n",
      "Summary for page:381\n",
      "A Kubernetes ResourceQuota limits objects in a namespace such as pods, replication controllers, secrets, configmaps, persistent volume claims, and services.\n",
      "Summary\n",
      "Horizontal pod autoscaling should already be enabled in your cluster, and once enabled, the Autoscaler can use metrics from Heapster or the aggregated resource metrics API to calculate the required number of pods. The calculation is based on a set of pod metrics and the target value, taking into account factors such as metric instability and multiple metrics per pod.\n",
      "Summary for page:382\n",
      "A Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "The final step of autoscaling is updating the desired replica count field on the scaled resource object and letting the Replica-Set controller manage additional pods or excess ones. The Autoscaler controller modifies the replicas field through the Scale sub-resource, allowing it to operate on scalable resources like Deployments, ReplicaSets, ReplicationControllers, and StatefulSets.\n",
      "Summary for page:383\n",
      "A Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "The horizontal pod autoscaler obtains metrics from cAdvisor, Heapster, and Kubelet to adjust replicas based on CPU utilization, taking into account a delay in propagating metrics data and performing scaling actions. It's essential to consider this delay when observing the Autoscaler in action.\n",
      "Summary for page:384\n",
      "A Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "The chapter discusses automatic scaling of pods and cluster nodes by focusing on scaling out (increasing the number of pods). The average CPU usage should come down, but setting a target CPU usage well below 100% is recommended to leave room for sudden load spikes. A HorizontalPodAutoscaler can be created to scale pods based on their CPU utilization, requiring CPU resource requests to be set in the pod template.\n",
      "Summary for page:385\n",
      "A Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "To enable horizontal autoscaling for a Deployment, create a HorizontalPodAutoscaler (HPA) object and point it to the Deployment. The HPA will adjust the number of replicas to keep CPU utilization around 30% while ensuring a minimum of one replica and a maximum of five. This can be done using the kubectl autoscale command or by preparing and posting a YAML manifest for the HPA. Always make sure to autoscale Deployments instead of ReplicaSets to preserve the desired replica count across application updates.\n",
      "Summary for page:386\n",
      "A Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Automatic scaling of pods and cluster nodes is achieved using Horizontal Pod Autoscalers (HPA). The HPA adjusts the desired replica count on the Deployment based on CPU metrics. In a scenario where three pods have zero CPU usage, the autoscaler scales them down to a single pod, ensuring CPU utilization remains below the 30% target.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:387\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "The horizontal pod autoscaler successfully rescaled to one replica because all metrics were below target. To trigger a scale-up, expose the pods through a Service and start sending requests to your pod, thereby increasing its CPU usage. You can watch the HorizontalPodAutoscaler and Deployment with kubectl get --watch.\n",
      "Summary for page:388\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "This chapter covers automatic scaling of pods and cluster nodes in Kubernetes. It explains how to run a pod that repeatedly hits the kubia Service using the kubectl run command with options such as -it, --rm, and --restart=Never. The autoscaler increases the number of replicas based on CPU utilization, and events can be inspected with kubectl describe. The chapter also discusses how the autoscaler concludes the need for multiple replicas based on target CPU utilization percentages.\n",
      "Summary for page:389\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "The autoscaler in Kubernetes has a maximum rate of scaling, doubling the number of replicas in a single operation if more than two exist, or scaling up to four replicas. It also has a limit on how soon a subsequent scale-up operation can occur after the previous one, which is currently every three minutes for scale-up and five minutes for scale-down. The target metric value for CPU utilization can be modified by editing the HPA resource with kubectl edit command, increasing it from 30 to 60 in this case.\n",
      "Summary for page:390\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Memory-based autoscaling in Kubernetes is more problematic than CPU-based due to the need to force old pods to release memory. This requires the app itself to manage, and can lead to infinite scaling if not implemented correctly. Custom metrics can also be used for autoscaling, but this was complicated in earlier versions of Kubernetes. Newer versions have simplified this process, allowing for more flexible scaling options.\n",
      "Summary for page:391\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Kubernetes Horizontal Pod Autoscaling (HPA) allows defining multiple metrics for autoscaling decisions. There are three types of metrics: Resource, Pods, and Object. The Resource type uses a resource metric like CPU or memory requests. The Pods type refers to custom metrics related to the pod, such as Queries-Per-Second (QPS). The Object metric type scales pods based on a metric not directly pertaining to those pods, like an Ingress object's QPS.\n",
      "Summary for page:392\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Horizontal pod autoscalers (HPAs) use metrics to monitor resources and scale pods accordingly. Appropriate metrics include those with a linear decrease in average value as replicas increase, such as Queries per Second (QPS). However, not all metrics are suitable, like memory consumption, which can lead to non-linear behavior. HPAs currently do not allow scaling down to zero replicas.\n",
      "Summary for page:393\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Kubernetes does not currently support idling and un-idling of pods or vertical pod autoscaling, but an experimental feature called InitialResources sets CPU and memory requests for newly created pods based on historical resource usage data, and a new proposal is being finalized to modify existing pod's resource requests vertically.\n",
      "Summary for page:394\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "The Cluster Autoscaler in Kubernetes automatically requests additional nodes from the cloud provider when a new pod cannot be scheduled due to lack of resources on existing nodes. It also de-provisions underutilized nodes for longer periods. The Autoscaler examines available node groups, selects the best one that can fit the unscheduled pod, and increases its size or adds another node to it.\n",
      "Summary for page:395\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "When scaling a Kubernetes cluster, the Cluster Autoscaler monitors node utilization and CPU/Memory requests of running pods. If a node is underutilized (CPU/memory < 50%), it's considered unnecessary unless system or unmanaged pods are running on it. The Autoscaler marks the node as unschedulable and evicts its pods before shutting it down. Scaling up involves identifying an available node type, selecting one, and scaling that group to fit the pod.\n",
      "Summary for page:396\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Automatic scaling of pods and cluster nodes can be enabled on GKE, GCE, AWS, and Azure through Cluster Autoscaler. On GKE, use gcloud command with --enable-autoscaling flag. On GCE, set environment variables KUBE_ENABLE_CLUSTER_AUTOSCALER=true, KUBE_AUTOSCALER_MIN_NODES=3, and KUBE_AUTOSCALER_MAX_NODES=5 before running kube-up.sh. The Cluster Autoscaler publishes its status to the cluster-autoscaler-status ConfigMap in the kube-system namespace. To limit service disruption during scale-down, manually cordon and drain nodes using kubectl commands.\n",
      "Summary for page:397\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Kubernetes provides a way to specify the minimum number of pods that must always be available through the PodDisruptionBudget resource, especially for quorum-based clustered applications. The PDB resource contains a pod label selector and a number specifying the minimum or maximum number of pods that can be unavailable. It can also use percentages instead of absolute numbers. The Cluster Autoscaler and kubectl drain command will adhere to this resource, ensuring that evictions do not bring the number of such pods below the specified threshold.\n",
      "Summary for page:398\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Kubernetes can scale not only pods but also cluster nodes automatically. HorizontalPodAutoscaler configures scaling based on CPU utilization or custom metrics. Vertical pod autoscaling is not possible yet. Cluster node auto-scaling is supported on cloud providers. Additionally, pods can be run one-off and deleted with kubectl run options.\n",
      "Summary for page:399\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Kubernetes allows for advanced scheduling by specifying a node selector in the pod specification, or using taints and tolerations to keep pods away from certain nodes. Additional features include defining node affinity rules, co-locating pods, and keeping pods away from each other using pod anti-affinity.\n",
      "Summary for page:400\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Node taints allow rejecting deployment of pods to certain nodes by adding taints without modifying existing pods, while tolerations enable pods to opt-in and use tainted nodes. A node's taints can be displayed using kubectl describe node, showing a key-value pair with an effect, such as NoSchedule preventing pod scheduling unless tolerated.\n",
      "Summary for page:401\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "In Kubernetes, a pod can be scheduled to a tainted node by adding a toleration that matches the node's taint. This allows system pods like kube-proxy to run on master nodes. A pod with no tolerations can only be scheduled to nodes without taints, and tolerations define how long a pod is allowed to run on nodes that aren't ready or are unreachable.\n",
      "Summary for page:402\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Kubernetes taints and tolerations allow you to label nodes with effects that can be tolerated by pods. Taints have three possible effects: NoSchedule, PreferNoSchedule, and NoExecute. Adding a NoExecute taint to a node will evict running pods that don't tolerate it. To deploy production pods to tainted nodes, they must include tolerations in their manifests matching the key, value, and effect of the taint.\n",
      "Summary for page:403\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Taints and tolerations can be used to control pod scheduling in Kubernetes. A taint is added to a node, and a matching toleration is added to a pod to allow it to run on that node. Tolerations can tolerate specific values or any value for a specific taint key. Taints can prevent new pods from running (NoSchedule), define unpreferred nodes (PreferNoSchedule), or evict existing pods (NoExecute). This allows for partitioning a cluster into separate partitions, controlling pod scheduling based on node type.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:404\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Kubernetes can wait up to 300 seconds (5 minutes) after a node failure before rescheduling a pod. This delay can be adjusted by adding tolerations to the pod's spec, and is currently an alpha feature. Node affinity allows scheduling pods only to specific subsets of nodes, replacing the initial node-selector mechanism which was simpler but didn't offer everything needed.\n",
      "Summary for page:405\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Node affinity allows specifying hard requirements or preferences for pods to run on certain nodes, based on their labels. Kubernetes uses these labels to select nodes, and by understanding default node labels, you can create rules that attract pods to specific nodes.\n",
      "Summary for page:406\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "The document discusses advanced scheduling in Kubernetes, specifically the use of node selectors and affinity rules to deploy pods on nodes with specific labels. The nodeSelector field specifies a simple rule for deployment, while the nodeAffinity field provides more expressive and detailed rules, including requiredDuringSchedulingIgnoredDuringExecution, which ensures the pod is scheduled only on nodes meeting specified criteria during scheduling but ignores execution.\n",
      "Summary for page:407\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Node affinity in Kubernetes allows pods to be scheduled to nodes with specific labels, such as gpu=true. The nodeSelectorTerms field defines expressions that a node's labels must match for the pod to be scheduled. Node affinity also enables prioritizing nodes during scheduling through the preferredDuringSchedulingIgnoredDuringExecution field, allowing for preference of certain zones or machines over others.\n",
      "Summary for page:408\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Node affinity allows scheduling of pods to machines reserved for deployments, and can be specified by labeling nodes with availability zone and share type labels. This can be demonstrated using kubectl label command to label nodes as dedicated or shared within specific zones. A Deployment can then be created that prefers dedicated nodes in a particular zone.\n",
      "Summary for page:409\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "You're defining a node affinity preference for pods to be scheduled on nodes with specific labels (availability-zone=zone1 and share-type=dedicated) with the first preference having a weight of 80 and the second one having a weight of 20, indicating that zone preference is more important than dedicated node preference in case of scheduling conflicts.\n",
      "Summary for page:410\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "In a two-node Kubernetes cluster, deploying a Deployment shows most pods deployed to one node due to prioritization functions like Selector-SpreadPriority. Scaling the Deployment up spreads pods evenly between nodes without node affinity preferences. Pod affinity allows specifying the affinity between pods themselves, such as keeping frontend and backend pods close together by configuring them to deploy on the same node.\n",
      "Summary for page:411\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "A Deployment is created with a podAffinity rule that requires frontend pods to be deployed on the same node as backend pods, which have an app=backend label. This ensures that all frontend pods will be scheduled only to the node where the backend pod was scheduled to, creating a co-locating requirement for the two types of pods.\n",
      "Summary for page:412\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "This chapter discusses advanced scheduling in Kubernetes using pod affinity rules. The Scheduler first finds all pods that match a labelSelector defined in a pod's configuration and schedules it to the same node. If a pod with affinity rules is deleted, the Scheduler will reschedule it to the same node to maintain consistency, even if no rules are defined on the deleted pod.\n",
      "Summary for page:413\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Pods can be co-located using pod affinity and anti-affinity, prioritizing scheduling to a node based on shared labels or topology keys such as zone or region. For example, setting topologyKey to failure-domain.beta.kubernetes.io/zone allows pods to be deployed in the same availability zone, while setting it to failure-domain.beta.kubernetes.io/region schedules them in the same geographical region.\n",
      "Summary for page:414\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "In Kubernetes, scheduling preferences can be expressed using label selectors and node or pod affinities. The Scheduler matches pods based on these rules, but if a match is not found, it will schedule the pod elsewhere. Pod affinity can also specify preferred nodes while allowing for flexibility if those nodes are unavailable.\n",
      "Summary for page:415\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Co-locating pods with pod affinity and anti-affinity can be achieved by defining a weight for each rule, specifying the topologyKey and labelSelector. This allows the Scheduler to prefer nodes where pods with a certain label are running. For example, in a deployment of 5 replicas, the Scheduler may deploy four pods on the same node as the backend pod, and one pod on another node.\n",
      "Summary for page:416\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "This chapter discusses advanced scheduling in Kubernetes, specifically how to schedule pods away from each other using pod anti-affinity. This is useful when two sets of pods interfere with each other's performance if they run on the same node. Pod anti-affinity can be used to spread pods across different availability zones or regions to prevent a whole zone failure from bringing down the service completely.\n",
      "Summary for page:417\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "To force frontend pods to be scheduled on different nodes, you can use pod anti-affinity. This is achieved by configuring the podAntiAffinity property in the deployment's spec section and making it match the same pods that the deployment creates. A soft requirement can also be used with preferredDuringSchedulingIgnoredDuringExecution property if scheduling two frontend pods on the same node is not a problem, otherwise requiredDuringSchedulingIgnoredDuringExecution should be used for hard requirements.\n",
      "Summary for page:418\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "This chapter explores advanced scheduling techniques in Kubernetes, including pod affinity, topologyKey, taints, node affinity, pod affinity/anti-affinity, and their use cases. It discusses how to ensure pods aren't scheduled to certain nodes or are only scheduled to specific nodes based on node labels or pod requirements.\n",
      "Summary for page:419\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "This chapter covers best practices for developing apps on Kubernetes, including understanding typical application resources, adding lifecycle hooks, properly terminating apps without breaking client requests, making apps easy to manage, using init containers, and developing locally with Minikube.\n",
      "Summary for page:420\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "A typical application manifest contains one or more Deployment and/or StatefulSet objects, including a pod template with containers, liveliness probes, readiness probes, and Services for exposing pods to others. The pod templates reference Secrets for pulling container images and those used directly by the process running inside the pods. Other resources like ReplicaSets, Endpoints, Horizontal Pod Autoscalers, and Ingress are also defined in the app manifest.\n",
      "Summary for page:421\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "A pod's lifecycle is crucial to understand, as it can be killed and relocated by Kubernetes at any time due to scale-down requests or node relocations. This differs from traditional VMs where apps are rarely moved, giving operators more control over the app in its new location. Kubernetes controllers automatically create objects such as Endpoints, ReplicaSets, and pods, which are often labeled and annotated for organization and metadata purposes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:422\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Kubernetes application developers should ensure their apps can be moved and restarted without issues, considering IP and hostname changes. Stateful apps should use StatefulSets for persistence. Apps writing data to disk may lose it when restarted or rescheduled unless using persistent storage. Volumes can preserve data across container restarts, making them useful for caching results or other sensitive data.\n",
      "Summary for page:423\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "A pod's lifecycle involves a container and its process, which writes to a filesystem with a writable layer on top of read-only layers and image layers. When the container crashes or is killed, a new container starts with a new writable layer, losing all previous files. Using a volume mounts allows data persistence across container restarts, enabling a new process to use preserved data in the volume.\n",
      "Summary for page:424\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Using volumes to preserve files across container restarts can be a double-edged sword, as it may lead to continuous crash loops if data gets corrupted. Similarly, dead or partially dead pods are not automatically removed and rescheduled by ReplicaSet controllers, even if they're part of a desired replica count, resulting in a lower actual replica count.\n",
      "Summary for page:425\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "A Kubernetes ReplicaSet with pods that keep crashing due to a container issue is created. The pod's status shows the Kubelet delaying the restart, but no action is taken by the controller since the current replicas match the desired ones, showing three running replicas.\n",
      "Summary for page:426\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "Kubernetes pods can include init containers to initialize the pod and delay the start of main containers. Init containers are executed sequentially and only after completion do main containers start. An example shows an init container checking if a service is responding before allowing the main container to start, using a busybox image and a while loop to continuously check until the service is up.\n",
      "Summary for page:427\n",
      "Kubernetes ResourceQuota limits objects in a namespace.\n",
      "Summary\n",
      "When deploying a pod, its init container is started first, shown by kubectl get. The main container won't run until dependencies are met, such as services being ready. It's best to write apps that handle internal dependencies and use readiness probes to signal unavailability. Lifecycle hooks can also be defined per container for post-start and pre-stop execution, similar to liveness and readiness probes.\n",
      "Cannot load json for this raw description:[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary for page:428\n",
      "[\n",
      "  {\n",
      "    \"summary\": \n",
      "Summary\n",
      "A post-start hook in Kubernetes is executed immediately after a container's main process is started. It runs in parallel with the main process and can perform additional operations without modifying the application source code. The hook affects the container by keeping it in the Waiting state until completion, and if it fails or returns a non-zero exit code, the main container will be killed. A pod manifest containing a post-start hook is shown, executing a shell script as part of the container lifecycle.\n",
      "Summary for page:429\n",
      "A post-start hook in Kubernetes is executed immediately after a container's main process is started.\n",
      "Summary\n",
      "A pod's lifecycle includes post-start and pre-stop hooks, which can be used to execute commands or initiate a graceful shutdown. Post-start hooks are executed immediately after a container is started, while pre-stop hooks are executed before a container is terminated. If a hook fails, it will display an error message in the pod's events. To troubleshoot failed hooks, one can use kubectl describe pod and exec into the container to examine log files or mount an emptyDir volume for logging purposes.\n",
      "Summary for page:430\n",
      "A post-start hook in Kubernetes is executed immediately after a container's main process is started.\n",
      "Summary\n",
      "Kubernetes pre-stop hooks can be used to perform actions before a pod is terminated, such as sending an HTTP GET request. However, if the hook fails or returns an error, it will not prevent the container from being terminated. It's also important to note that using a pre-stop hook solely to send a SIGTERM signal to an app is not necessary, and instead the shell should be configured to pass the signal to the app process. A pre-stop hook YAML snippet example is provided for performing an HTTP GET request in a pod.\n",
      "Summary for page:431\n",
      "A post-start hook in Kubernetes is executed immediately after a container's main process is started.\n",
      "Summary\n",
      "A pod's lifecycle involves container termination, triggered by the deletion of the Pod object through the API server. The Kubelet terminates each container, running pre-stop hooks and sending SIGTERM signals to main processes. If containers don't shut down cleanly within the configured termination grace period, they are forcibly killed with a SIGKILL signal.\n",
      "Summary for page:432\n",
      "A post-start hook in Kubernetes is executed immediately after a container's main process is started.\n",
      "Summary\n",
      "The termination grace period for Kubernetes pods can be configured in the pod spec or overridden when deleting the pod. It's essential to set a sufficient time for processes to finish cleaning up before being killed. Applications should react to SIGTERM signals by starting their shut-down procedure and terminating within a fixed amount of time, using pre-stop hooks if necessary.\n",
      "Summary for page:433\n",
      "A post-start hook in Kubernetes is executed immediately after a container's main process is started.\n",
      "Summary\n",
      "When a Kubernetes pod receives a termination signal, it should not start migrating its data immediately. Instead, a dedicated pod or CronJob resource can be used to periodically check for orphaned data and migrate it to remaining pods, ensuring data is not lost in case of node failure or application upgrade.\n",
      "Summary for page:434\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "To handle client requests properly, Kubernetes apps need to follow rules to prevent broken connections when pods are starting up or shutting down. This involves ensuring each connection is handled properly at pod startup by adding an HTTP GET readiness probe that returns success only when the app is ready to accept incoming requests.\n",
      "Summary for page:435\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "When a pod is deleted, the API server modifies etcd and notifies watchers, including Kubelet and Endpoints controller. Two parallel sequences of events occur: A) containers on the worker node stop, kube-proxy removes the pod as an endpoint, and iptables updates; B) the Pod's deletion notification is sent to the client, the endpoints controller removes the pod from its list, and the kubelet removes the pod from iptables.\n",
      "Summary for page:436\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "When a pod is deleted in Kubernetes, two sequences of events occur in parallel: the Kubelet shuts down the app's process and removes it from iptables rules. The shutdown sequence is relatively short, while updating iptables rules involves a longer chain of events, including notification to the Endpoints controller, API server, and kube-proxy.\n",
      "Summary for page:437\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "The pod needs to keep accepting connections after receiving the termination signal until all kube-proxies and other components have finished updating their rules. A long-enough delay, such as 5-10 seconds, should be added before shutting down the pod to ensure a smooth user experience.\n",
      "Summary for page:438\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "To properly shut down an application, wait for a few seconds, then stop accepting new connections, close inactive keep-alive connections, wait for active requests to finish, and finally shut down completely. A pre-stop hook can be added to wait a few seconds before shutting down, preventing broken connections and frustrating the user with lingering pod listings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:439\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "To make apps easy to run and manage in Kubernetes, focus on creating small, minimal container images without unnecessary cruft. This includes using the FROM scratch directive in Dockerfiles and avoiding the latest image tag, which can cause versioning issues. Proper tagging of images and using imagePullPolicy wisely is also crucial to ensure smooth deployment and scaling.\n",
      "Summary for page:440\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "Use tags with version designators, label resources with multiple dimensions, add annotations for resource descriptions and dependencies. Set imagePullPolicy to Always only in development. Use labels and annotations to manage resources and show dependencies between pods, and include contact information and build/version metadata.\n",
      "Summary for page:441\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "In Kubernetes, you can make triage easier by having a container write a termination message to a specific file before exiting. This message is then shown in the output of kubectl describe pod without needing to inspect container logs. The default path for this message is /dev/termination-log but can be overridden with the terminationMessagePath field in the container definition. An example of this is provided, where a busybox container writes a message to /var/termination-reason and dies immediately, causing the pod's status to show CrashLoopBackOff, which can then be seen using kubectl describe.\n",
      "Summary for page:442\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "The chapter discusses best practices for developing apps in Kubernetes. It highlights the importance of writing app-specific status messages to a file or using the standard output, which can be easily viewed with the `kubectl logs` command. If an application crashes and is replaced, the new container's log is displayed, but using the `--previous` option shows the previous container's logs. The chapter also covers copying files from/to containers using `kubectl cp`, including logging files.\n",
      "Summary for page:443\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "Kubernetes provides no centralized logging by itself, requiring additional components to store and analyze container logs. Deploying a centralized logging solution like the EFK stack (FluentD, ElasticSearch, Kibana) is easy through YAML/JSON manifests or Helm charts, allowing for historical log examination and trend analysis.\n",
      "Summary for page:444\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "The chapter discusses best practices for developing apps in Kubernetes. It highlights the importance of handling multi-line log statements by outputting JSON logs instead of plain text, which can be stored and shown as a single entry in Kibana. The solution is to keep outputting human-readable logs while writing JSON logs to a file and having them processed by FluentD. Additionally, it suggests running apps outside of Kubernetes during development on local machines or IDEs without the need for containerization. It also advises connecting to backend services manually or temporarily making them accessible externally using NodePort or LoadBalancer-type Services.\n",
      "Summary for page:445\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "When developing an app that requires access to the Kubernetes API server, you can easily talk to the API server from outside the cluster during development using ServiceAccount's token or ambassador container. Alternatively, you can run your app inside a Kubernetes cluster using Minikube, which provides a single worker node but is valuable for trying out your app in Kubernetes and developing resource manifests. You can also mount local files into the Minikube VM and containers using hostPath volume or use the Docker daemon inside the Minikube VM to build container images.\n",
      "Summary for page:446\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "Minikube allows developers to build apps locally without pushing images to a registry. Environment variables can be set using \"eval $(minikube docker-env)\" to use the Minikube VM's Docker daemon. Images can also be built locally and copied over to the Minikube VM, or combined with a proper Kubernetes cluster for development and deployment.\n",
      "Summary for page:447\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "The document discusses best practices for development and testing in Kubernetes, including using tools like kube-applier to manage running apps through version control systems. It also introduces Ksonnet as an alternative to writing YAML/JSON manifests, allowing users to define parameterized JSON fragments and build full manifests with much less code.\n",
      "Summary for page:448\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "The chapter emphasizes the importance of using Ksonnet and Jsonnet for consistent manifests, employing Continuous Integration and Continuous Delivery (CI/CD) pipelines with tools like Fabric8 or Google Cloud Platform's online labs to automate deployment, and understanding Kubernetes' distributed nature and eventual consistency model. It also highlights the need for apps to shut down properly without breaking client connections.\n",
      "Summary for page:449\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "The document provides small tips for app management by keeping image sizes small, adding annotations and labels, and making termination reasons clear. It also teaches how to develop Kubernetes apps locally or in Mini-kube before deploying them on a multi-node cluster. Finally, it explains how to extend Kubernetes with custom API objects and controllers, enabling the creation of Platform-as-a-Service solutions.\n",
      "Summary for page:450\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "This chapter covers extending Kubernetes by defining custom API objects, creating controllers for those objects, and adding custom API servers. It also explores how others have built Platform-as-a-Service solutions on top of Kubernetes, including Red Hat's OpenShift Container Platform and Deis Workflow.\n",
      "Summary for page:451\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "Kubernetes allows defining custom API objects through CustomResourceDefinitions (CRD) which is a description of the custom resource type. A CRD can be posted to the Kubernetes API server, enabling users to create instances of the custom resource. Each CRD typically has an associated controller that makes something tangible happen in the cluster, such as spinning up a new web server pod and exposing it through a Service when creating an instance of the Website resource.\n",
      "Summary for page:452\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "Kubernetes custom resource is created by posting a CustomResourceDefinition to the API server. A custom resource definition object has apiVersion, kind, metadata name and spec with scope as Namespaced. It's used to make Kubernetes accept instances of custom Website resources which will result in creation of Service and Pod for each instance.\n",
      "Summary for page:453\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "A custom API object called Website is defined with group: extensions.example.com, version: v1, and names: kind: Website. After posting the descriptor to Kubernetes, instances of the custom Website resource can be created. A YAML manifest for a Website resource instance is shown, specifying apiVersion: extensions.example.com/v1, kind: Website, metadata: name: kubia, and spec: gitRepo: https://github.com/luksa/kubia-website-example.git.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:454\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "You can now store, retrieve and delete custom resources through the Kubernetes API server after creating a CustomResourceDefinition object. These objects don't do anything yet and you'll need to create a controller to make them functional.\n",
      "Summary for page:455\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "A custom API object, such as a Website object, can be created to trigger the spinning up of a web server serving Git repository contents. A custom controller is needed to automate this process by watching the API server for Website object creation and creating a Deployment and Service for each one. This allows the Pod to be managed and survive node failures.\n",
      "Summary for page:456\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "The Website controller connects to the kubectl proxy process, which forwards requests to the API server, allowing the API server to send watch events for every change to any Website object. When a new Website object is created, the API server sends an ADDED event, triggering the controller to create a Deployment and Service object with a template for a pod containing an nginx server and a git-sync process, exposing the web server through a random port on each node.\n",
      "Summary for page:457\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "A custom API controller is created to manage Website resources, which are deleted by the API server and watched through periodic re-listing. The controller runs as a pod in Kubernetes for development and deployment, using a Deployment resource to ensure proper execution.\n",
      "Summary for page:458\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "A Kubernetes Deployment is created with two containers, main and proxy, running under a special ServiceAccount. The controller watches for events and creates resources as needed. A ClusterRoleBinding is required to enable access control. The deployment can be tested by creating a kubia Website resource and checking the controller's logs for watch event and resource creation.\n",
      "Summary for page:459\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "A custom API object was successfully created by the controller, which received an ADDED event and created a Service and a Deployment for the kubia-website Website. The API server responded with a 201 Created response, and the resulting Pod was also created. However, users can create invalid Website objects without validation schema in the CustomResourceDefinition. The controller can only validate the object when it receives it in a watch event, and if invalid, write an error message to the Website object.\n",
      "Summary for page:460\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "To extend Kubernetes, validation of custom objects was introduced in version 1.8, enabling the API server to validate custom objects immediately. Alternatively, implementing a custom API server and integrating it with the main Kubernetes API server through API server aggregation allows for more control over custom object handling. This approach eliminates the need for a CRD and enables direct implementation of custom object types within the custom API server.\n",
      "Summary for page:461\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "Kubernetes can be extended by creating Custom Resource Definitions (CRDs) in the core API server's etcd store. A custom API server can be added to a cluster by deploying it as a pod and exposing it through a Service, then integrating it into the main API server using an APIService resource. This allows client requests to be forwarded to the custom API server for specific resources. Additionally, custom clients can be built to create custom objects, making deployment easier. The Kubernetes Service Catalog API server will also be added through API server aggregation, enabling pods to consume services.\n",
      "Summary for page:462\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "Kubernetes' Service Catalog is a feature that allows users to provision services without dealing with underlying components like Pods and Services. It uses four generic API resources: ClusterServiceBroker, ClusterServiceClass, ServiceInstance, and ServiceBinding. A cluster admin creates a ClusterServiceBroker resource for each service broker, which lists available services. Users create a ServiceInstance resource for the required service, and a ServiceBinding to bind it to client pods.\n",
      "Summary for page:463\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "The Kubernetes Service Catalog is a distributed system composed of three components: API Server, etcd, and Controller Manager. The Service Catalog API server stores resources created by posting YAML/JSON manifests to itself or uses CustomResourceDefinitions in the main API server. Controllers running in the Controller Manager talk to the Service Catalog API server and provision services using external service brokers registered with ServiceBroker resources.\n",
      "Summary for page:464\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "A cluster administrator can register external ServiceBrokers in the Service Catalog through the OpenServiceBroker API, which provides operations for provisioning, updating, and deprovisioning services. A ClusterServiceBroker resource manifest is posted to the Service Catalog API to register a broker, and a controller connects to the specified URL to retrieve the list of services this broker can provision, creating ClusterServiceClass resources for each service type.\n",
      "Summary for page:465\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "The Kubernetes Service Catalog allows users to retrieve a list of available services in a cluster using kubectl get serviceclasses. ClusterServiceClasses are similar to StorageClasses, but allow users to select the type of service they want to use. An example is shown for a PostgreSQL database, with two plans: free and premium. The ClusterServiceClass is provided by the database-broker broker.\n",
      "Summary for page:466\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "To provision a service instance, create a Service-Instance resource with ClusterServiceClass and plan specified. The Service Catalog will contact the broker, passing on chosen class, plan, and parameters. The broker then provisions the service according to its configuration, potentially spinning up a new instance of a database or running it in a VM. Successful provisioning can be checked by inspecting the status section of the created ServiceInstance.\n",
      "Summary for page:467\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "A Kubernetes Service Catalog is extended by binding a provisioned ServiceInstance to pods using a ServiceBinding resource. A Secret is created with necessary credentials, which can be manually mounted into pods for access to the ServiceInstance.\n",
      "Summary for page:468\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "The Service Catalog allows service providers to expose services in any Kubernetes cluster by registering a broker. A Secret can be created with credentials for connecting to a service instance, and can be used by multiple pods. Once no longer needed, the ServiceBinding can be deleted, which will delete the Secret and perform an unbinding operation on the service broker. Additionally, if not needed, the ServiceInstance resource should also be deleted to deprovision the service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for page:469\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "Kubernetes is becoming a widely accepted foundation for Platform-as-a-Service (PaaS) offerings. Platforms built on top of Kubernetes, such as Deis Workflow and Red Hat's OpenShift, provide features like easy provisioning, automated rollouts and scaling, user and group management, and additional API objects. Red Hat OpenShift automates application image building and deployment without requiring a Continuous Integration solution.\n",
      "Summary for page:470\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "OpenShift provides powerful user management features, allowing users to access certain Projects (Kubernetes Namespaces with additional annotations) and granting access by a cluster administrator. Application Templates in OpenShift are parameterizable JSON or YAML manifests that can be instantiated with placeholder values replaced with parameter values.\n",
      "Summary for page:471\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "OpenShift provides pre-fabricated templates for complex applications, allowing users to quickly run them with minimal arguments. It also enables automatic deployment of newly built images by creating a DeploymentConfig object and pointing it to an ImageStream. BuildConfigs can trigger builds immediately after changes are committed to the source Git repository, building container images without manual intervention.\n",
      "Summary for page:472\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "Kubernetes can be extended with features from OpenShift, such as DeploymentConfig, which provides pre- and post-deployment hooks and creates ReplicationControllers instead of ReplicaSets. Routes are used to expose Services externally, providing additional configuration for TLS termination and traffic splitting. Minishift is available for trying out OpenShift, along with OpenShift Online Starter. Deis Workflow, also built on Kubernetes, provides a PaaS with features like BuildConfigs and DeploymentConfigs.\n",
      "Summary for page:473\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "Deis Workflow is a tool built on top of Kubernetes that creates services and replication controllers, providing developers with a simple environment. Deploying new versions of an app can be triggered by pushing changes with 'git push deis master'. The Helm tool is a package manager for Kubernetes, allowing the deployment and management of application packages called Charts.\n",
      "Summary for page:474\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "When extending Kubernetes, instead of writing manifests for apps like PostgreSQL or MySQL, check if someone has prepared a Helm chart for it. Once installed, running the app takes a single command and creates necessary Deployments, Services, Secrets, and PersistentVolumeClaims.\n",
      "Summary for page:475\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "This final chapter shows how to extend Kubernetes' functionalities by registering custom resources, implementing custom controllers, and using API aggregation, Service Catalog, and platforms-as-a-service built on top of Kubernetes. A package manager called Helm is also introduced for deploying existing apps without requiring resource manifests.\n",
      "Summary for page:476\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "To switch between Minikube and Google Kubernetes Engine (GKE) clusters using kubectl, simply run 'minikube start' to configure kubectl for Minikube or use 'gcloud container clusters get-credentials my-gke-cluster' to set up GKE. Minikube reconfigures kubectl every time you start the cluster, making it easy to switch between the two.\n",
      "Summary for page:477\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n",
      "Summary\n",
      "To switch between different Kubernetes clusters or namespaces without specifying the --namespace option every time, configure the kubeconfig file's location using the KUBECONFIG environment variable. This file contains four sections: clusters (list of available clusters), users (list of user credentials), contexts (defined by a cluster and a user), and current-context (the currently used context). By listing multiple config files in KUBECONFIG, kubectl can use them all at once.\n",
      "Summary for page:478\n",
      "A post-start hook in Kubernetes executes immediately after a container's main process is started.\n"
     ]
    }
   ],
   "source": [
    "final_summary=\"\"\n",
    "idx=0\n",
    "for summary in document_txt:\n",
    "    print(\"Summary\")\n",
    "    print(summary)\n",
    "    final_summary=final_summary+summary\n",
    "    final_summary=extract_highlights_from_summary(final_summary)\n",
    "    #highlights=extract_highlights_from_summary(summary)\n",
    "    print(\"Summary for page:\"+str(idx))\n",
    "    print(final_summary)\n",
    "    idx=idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c269461a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kubernetes allows running multiple copies of the same component and different versions concurrently.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ba6a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
