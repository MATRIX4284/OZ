{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c0eab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import ollama\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "#rag_test_input_path='/home/matrix4284/MY_GEN_AI_PROJECTS/RAG/GraphRAG/graphrag-local-ollama/ragtest/input/'+file_name\n",
    "import os\n",
    "# importing shutil module\n",
    "import shutil\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "#embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import XMLOutputParser\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30e7e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_enrichment_output_dir = '../pdf_enrichment/pdf_enriched_output/'  \n",
    "pdf_enrichment_output_file = 'pdf_enriched_content_dict_phase5_extract_highligts_478_final.pickle'\n",
    "\n",
    "with open(pdf_enrichment_output_dir+pdf_enrichment_output_file, 'rb') as handle:\n",
    "    document_dict_deserialized = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7072524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubernetes in Action\n",
      " www.allitebooks.com\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_file=\"../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf\"\n",
    "pdf_loader = PyPDFLoader(pdf_file)\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(pages[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_doc_summary=[]\n",
    "doc_len=len(document_dict_deserialized)\n",
    "for i in range(0,doc_len):\n",
    "    summary=document_dict_deserialized[i][\"text\"]\n",
    "    full_doc_summary.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd132e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_doc_summary[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3df369d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Model for Prompt Tuning\n",
    "model_name=\"llama3.1\"\n",
    "llm = ChatOllama(base_url=\"http://192.168.50.100:11434\",model=model_name,temperature=0.15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab4d9ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0af06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "\n",
    "#llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "map_prompt_template = \"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "                      Write a concise summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1410d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "Time taken:58 minutes 47 seconds for 4666 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "783387e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a851cbba1bd46469403a6e9883b2e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b154a5957934a6b9257f94c2ded96db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b827caf2cd4bc1a667c26a7f50fb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a23d3dbf164afabb2f8dbf9d4764e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8c0043e9994de58927119f26d871a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (127533 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "map_reduce_outputs = map_reduce_chain({\"input_documents\": pages[87:566]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a819b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 87}, page_content='56 CHAPTER  3Pods: running containers in Kubernetes\\n3.1 Introducing pods\\nYou’ve already learned that a pod is a co-l ocated group of containers and represents\\nthe basic building block in Kubernetes. In stead of deploying containers individually,\\nyou always deploy and operate on a pod of containers. We’re not implying that a pod\\nalways includes more than one container—it ’s common for pods to contain only a sin-\\ngle container. The key thing about pods is that when a pod does contain multiple con-\\ntainers, all of them are always run on a single worker node—it never spans multiple\\nworker nodes, as shown in figure 3.1.\\n3.1.1 Understanding why we need pods\\nBut why do we even need pods? Why can’t we use containers directly? Why would we\\neven need to run multiple containers toge ther? Can’t we put all our processes into a\\nsingle container? We’ll answer those questions now.\\nUNDERSTANDING  WHY MULTIPLE  CONTAINERS  ARE BETTER  THAN ONE CONTAINER  RUNNING  \\nMULTIPLE  PROCESSES\\nImagine an app consisting of multiple pr ocesses that either communicate through\\nIPC (Inter-Process Communication) or throug h locally stored files, which requires\\nthem to run on the same machine. Because in Kubernetes you always run processes in\\ncontainers and each container is much like an isolated machine, you may think it\\nmakes sense to run multiple processes in a single container, but you shouldn’t do that. \\n Containers are designed to run only a single process per container (unless the\\nprocess itself spawns child processes). If you run multiple unrelated processes in a\\nsingle container, it is your responsibilit y to keep all those processes running, man-\\nage their logs, and so on. For example, yo u’d have to include a mechanism for auto-\\nmatically restarting individual processes if they crash. Also, all those processes would\\nlog to the same standard output, so you’d have a hard time figuring out what pro-\\ncess logged what. Node 1Pod 2\\nIP: 10.1.0.2Container 1\\nContainer 2\\nPod 1\\nIP: 10.1.0.1Container\\nNode 2Pod 4\\nIP: 10.1.1.2Container 1\\nPod 5\\nIP: 10.1.1.3Container 1\\nContainer 2Pod 3Container 1 Container 2\\nFigure 3.1 All containers of a pod run on the same node. A pod never spans two nodes.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 88}, page_content='57 Introducing pods\\n Therefore, you need to run each proce ss in its own container. That’s how Docker\\nand Kubernetes are meant to be used. \\n3.1.2 Understanding pods\\nBecause you’re not supposed to group multiple  processes into a single container, it’s\\nobvious you need another higher-level construct that will allow you to bind containers\\ntogether and manage them as a single un it. This is the reasoning behind pods. \\n A pod of containers allows you to run closely related processes together and pro-\\nvide them with (almost) the same environmen t as if they were all running in a single\\ncontainer, while keeping them somewhat isol ated. This way, you get the best of both\\nworlds. You can take advantage of all the fe atures containers provide, while at the\\nsame time giving the processes th e illusion of running together. \\nUNDERSTANDING  THE PARTIAL  ISOLATION  BETWEEN  CONTAINERS  OF THE SAME POD\\nIn the previous chapter, you learned that  containers are completely isolated from\\neach other, but now you see that you want to isolate groups of containers instead of\\nindividual ones. You want containers inside  each group to share certain resources,\\nalthough not all, so that they’re not fully is olated. Kubernetes achieves this by config-\\nuring Docker to have all containers of a pod share the same set of Linux namespaces\\ninstead of each container having its own set. \\n Because all containers of a pod run un der the same Network and UTS namespaces\\n(we’re talking about Linux namespaces here ), they all share the same hostname and\\nnetwork interfaces. Similarly, all containers  of a pod run under the same IPC namespace\\nand can communicate through IPC. In the la test Kubernetes and Docker versions, they\\ncan also share the same PID namespace, bu t that feature isn’t enabled by default. \\nNOTE When containers of the same pod use separate PID namespaces, you\\nonly see the container’s own processes when running ps aux in the container.\\nBut when it comes to the filesystem, things are a little different. Because most of the\\ncontainer’s filesystem comes from the contai ner image, by default, the filesystem of\\neach container is fully isolated from othe r containers. However, it’s possible to have\\nthem share file directories using a Kubernetes concept called a Volume , which we’ll\\ntalk about in chapter 6.\\nUNDERSTANDING  HOW CONTAINERS  SHARE  THE SAME IP AND PORT SPACE\\nOne thing to stress here is that because co ntainers in a pod run in the same Network\\nnamespace, they share the same IP address and port space. This means processes run-\\nning in containers of the same pod need to  take care not to bind to the same port\\nnumbers or they’ll run into port conflicts. But this only concerns containers in the\\nsame pod. Containers of different pods ca n never run into port conflicts, because\\neach pod has a separate port space. All th e containers in a pod also have the same\\nloopback network interface, so a container can communicat e with other containers in\\nthe same pod through localhost.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 89}, page_content='58 CHAPTER  3Pods: running containers in Kubernetes\\nINTRODUCING  THE FLAT INTER-POD NETWORK\\nAll pods in a Kubernetes cluster reside in a single flat, shared, network-address space\\n(shown in figure 3.2), which means every pod can access every other pod at the other\\npod’s IP address. No NAT (Network Address Translation) gateways exist between them.\\nWhen two pods send network packets betwee n each other, they’ll each see the actual\\nIP address of the other as th e source IP in the packet.\\nConsequently, communication between pods is  always simple. It doesn’t matter if two\\npods are scheduled onto a single or onto different worker nodes; in both cases the\\ncontainers inside those pods can communic ate with each other across the flat NAT-\\nless network, much like computers on a local area network (LAN), regardless of theactual inter-node network topology. Like a computer on a LAN, each pod gets its own\\nIP address and is accessible from all other pods through this network established spe-\\ncifically for pods. This is usually achieved through an additional software-defined net-\\nwork layered on top of the actual network.\\n To sum up what’s been covered in this section: pods are logical hosts and behave\\nmuch like physical hosts or VMs in the no n-container world. Processes running in the\\nsame pod are like processes running on the same physical or virtual machine, except\\nthat each process is encapsulated in a container. \\n3.1.3 Organizing containers across pods properly\\nYou should think of pods as separate machines, but where each one hosts only a cer-\\ntain app. Unlike the old days, when we used  to cram all sorts of apps onto the same\\nhost, we don’t do that with pods. Because po ds are relatively lightweight, you can have\\nas many as you need without incurring almo st any overhead. Instead of stuffing every-\\nthing into a single pod, you should organize apps into multiple pods, where each one\\ncontains only tightly related components or processes.Node 1Pod A\\nIP: 10.1.1.6Container 1\\nContainer 2\\nPod B\\nIP: 10.1.1.7Container 1\\nContainer 2\\nNode 2\\nFlat networkPod C\\nIP: 10.1.2.5Container 1\\nContainer 2\\nPod D\\nIP: 10.1.2.7Container 1\\nContainer 2\\nFigure 3.2 Each pod gets a routable IP address and all other pods see the pod under \\nthat IP address.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 90}, page_content='59 Introducing pods\\n Having said that, do you think a multi- tier application consisting of a frontend\\napplication server and a backend database sh ould be configured as a single pod or as\\ntwo pods?\\nSPLITTING  MULTI -TIER APPS INTO MULTIPLE  PODS\\nAlthough nothing is stopping you from running both the frontend server and thedatabase in a single pod with two containers , it isn’t the most appropriate way. We’ve\\nsaid that all containers of the same pod always run co-located, but do the web server\\nand the database really need to run on the same machine? The answer is obviously no,\\nso you don’t want to put them into a single pod. But is it wrong to do so regardless? In\\na way, it is.\\n If both the frontend and backend are in the same pod, then both will always be\\nrun on the same machine. If you have a two-node Kubernetes cluster and only this sin-\\ngle pod, you’ll only be using a single wo rker node and not taking advantage of the\\ncomputational resources (CPU and memory) yo u have at your disposal on the second\\nnode. Splitting the pod into two would allo w Kubernetes to schedule the frontend to\\none node and the backend to the other no de, thereby improving the utilization of\\nyour infrastructure.\\nSPLITTING  INTO MULTIPLE  PODS TO ENABLE  INDIVIDUAL  SCALING\\nAnother reason why you shouldn’t put them bo th into a single pod is scaling. A pod is\\nalso the basic unit of scaling. Kubernetes can’t horizontally scale individual contain-\\ners; instead, it scales whole pods. If your pod consists of a frontend and a backend con-\\ntainer, when you scale up the number of instan ces of the pod to, let’s say, two, you end\\nup with two frontend containers and two backend containers. \\n Usually, frontend compon ents have completely different scaling requirements\\nthan the backends, so we tend to scale them  individually. Not to mention the fact that\\nbackends such as databases are usually much  harder to scale compared to (stateless)\\nfrontend web servers. If you need to scale a container individually, this is a clear indi-\\ncation that it needs to be deployed in a separate pod. \\nUNDERSTANDING  WHEN TO USE MULTIPLE  CONTAINERS  IN A POD\\nThe main reason to put multiple containers into a single pod is when the application\\nconsists of one main process and one or more complementary processes, as shown in\\nfigure 3.3.\\nPodMain container\\nSupporting\\ncontainer 1\\nSupporting\\ncontainer 2Volume\\nFigure 3.3 Pods should contain tightly coupled \\ncontainers, usually a main container and containers \\nthat support the main one.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 91}, page_content='60 CHAPTER  3Pods: running containers in Kubernetes\\nFor example, the main container in a pod co uld be a web server that serves files from\\na certain file directory, while an addition al container (a sidecar container) periodi-\\ncally downloads content from an external source and stores it in the web server’s\\ndirectory. In chapter 6 you’ll see that you need to use a Kubernetes Volume that you\\nmount into both containers. \\n Other examples of sidecar containers include log rotators and collectors, data pro-\\ncessors, communication adapters, and others.\\nDECIDING  WHEN TO USE MULTIPLE  CONTAINERS  IN A POD\\nTo recap how containers should be grouped into pods—when deciding whether toput two containers into a single pod or into  two separate pods, you always need to ask\\nyourself the following questions:\\n\\uf0a1Do they need to be run together or can they run on different hosts?\\n\\uf0a1Do they represent a single whole or  are they independent components?\\n\\uf0a1Must they be scaled together or individually? \\nBasically, you should always gravitate towa rd running containers in separate pods,\\nunless a specific reason requires them to be part of the same pod. Figure 3.4 will help\\nyou memorize this.\\nAlthough pods can contain multiple containe rs, to keep things simple for now, you’ll\\nonly be dealing with single-container pods  in this chapter. You’ll see how multiple\\ncontainers are used in the same pod later, in chapter 6. PodFrontend\\nprocess\\nBackend\\nprocess\\nContainer\\nPodFrontend\\nprocess\\nFrontend\\ncontainerFrontend podFrontend\\nprocess\\nFrontend\\ncontainer\\nBackend podBackend\\nprocess\\nBackend\\ncontainerBackend\\nprocess\\nBackend\\ncontainer\\nFigure 3.4 A container shouldn’t run multiple processes. A pod shouldn’t contain multiple \\ncontainers if they don’t need to run on the same machine.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 92}, page_content='61 Creating pods from YAML or JSON descriptors\\n3.2 Creating pods from YAML or JSON descriptors\\nPods and other Kubernetes resources are usually created by posting a JSON or YAML\\nmanifest to the Kubernetes REST API endpo int. Also, you can use other, simpler ways\\nof creating resources, such as the kubectl  run command you used in the previous\\nchapter, but they usually only allow you to  configure a limited set of properties, not\\nall. Additionally, defining al l your Kubernetes objects from YAML files makes it possi-\\nble to store them in a version control sy stem, with all the benefits it brings.\\n To configure all aspects of each type of  resource, you’ll need to know and under-\\nstand the Kubernetes API object definitions. You’ll get to know most of them as you\\nlearn about each resource type throughout  this book. We won’t explain every single\\nproperty, so you should also refer to th e Kubernetes API reference documentation at\\nhttp:/ /kubernetes.io/docs/reference/  when creating objects.\\n3.2.1 Examining a YAML descriptor of an existing pod\\nYou already have some existing pods you created in the previous chapter, so let’s look\\nat what a YAML definition for one of those pods looks like. You’ll use the kubectl  get\\ncommand with the -o yaml  option to get the whole YAML definition of the pod, as\\nshown in the following listing.\\n$ kubectl get po kubia-zxzij -o yaml\\napiVersion: v1                         \\nkind: Pod                                       \\nmetadata:                                                 \\n  annotations:                                            \\n    kubernetes.io/created-by: ...                           creationTimestamp: 2016-03-18T12:37:50Z                 \\n  generateName: kubia-                                    \\n  labels:                                                     run: kubia                                            \\n  name: kubia-zxzij                                       \\n  namespace: default                                        resourceVersion: \"294\"                                  \\n  selfLink: /api/v1/namespaces/default/pods/kubia-zxzij   \\n  uid: 3a564dc0-ed06-11e5-ba3b-42010af00004               spec:                                                   \\n  containers:                                           \\n  - image: luksa/kubia                                      imagePullPolicy: IfNotPresent                       \\n    name: kubia                                         \\n    ports:                                                  - containerPort: 8080                               \\n      protocol: TCP                                     \\n    resources:                                                requests:                                         \\n        cpu: 100m                                       Listing 3.1 Full YAML of a deployed pod\\nKubernetes API version used \\nin this YAML descriptor\\nType of Kubernetes \\nobject/resource\\nPod metadata (name, \\nlabels, annotations, and so on)\\nPod specification/\\ncontents (list of pod’s containers, \\nvolumes, and so on)'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 93}, page_content='62 CHAPTER  3Pods: running containers in Kubernetes\\n    terminationMessagePath: /dev/termination-log      \\n    volumeMounts:                                     \\n    - mountPath: /var/run/secrets/k8s.io/servacc            name: default-token-kvcqa                       \\n      readOnly: true                                  \\n  dnsPolicy: ClusterFirst                               nodeName: gke-kubia-e8fe08b8-node-txje              \\n  restartPolicy: Always                               \\n  serviceAccount: default                             \\n  serviceAccountName: default                         \\n  terminationGracePeriodSeconds: 30                     volumes:                                            \\n  - name: default-token-kvcqa                         \\n    secret:                                                 secretName: default-token-kvcqa                 \\nstatus:                                                   \\n  conditions:                                               - lastProbeTime: null                                   \\n    lastTransitionTime: null                              \\n    status: \"True\"                                            type: Ready                                           \\n  containerStatuses:                                      \\n  - containerID: docker://f0276994322d247ba...                image: luksa/kubia                                    \\n    imageID: docker://4c325bcc6b40c110226b89fe...         \\n    lastState: {}                                             name: kubia                                           \\n    ready: true                                           \\n    restartCount: 0                                           state:                                                \\n      running:                                            \\n        startedAt: 2016-03-18T12:46:05Z                     hostIP: 10.132.0.4                                      \\n  phase: Running                                          \\n  podIP: 10.0.2.3                                         \\n  startTime: 2016-03-18T12:44:32Z                         \\nI know this looks complicated, but it beco mes simple once you understand the basics\\nand know how to distinguish between the im portant parts and the minor details. Also,\\nyou can take comfort in the fact that when  creating a new pod, the YAML you need to\\nwrite is much shorter, as you’ll see later.\\nINTRODUCING  THE MAIN PARTS  OF A POD DEFINITION\\nThe pod definition consists of a few parts. First, there’s the Kubernetes API versionu s e d  i n  t h e  Y A M L  a n d  t h e  t y p e  o f  r e s o u r c e  t h e  Y A M L  i s  d e s c r i b i n g .  T h e n ,  t h r e eimportant sections are found in almost all Kubernetes resources:\\n\\uf0a1Metadata  includes the name, namespace, labels, and other information about\\nthe pod.\\n\\uf0a1Spec contains the actual description of the pod’s contents, such as the pod’s con-\\ntainers, volumes, and other data.Pod specification/\\ncontents (list of \\npod’s containers, volumes, and so on)\\nDetailed status \\nof the pod and \\nits containers'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 94}, page_content='63 Creating pods from YAML or JSON descriptors\\n\\uf0a1Status  contains the current information about the running pod, such as what\\ncondition the pod is in, the description and status of each container, and the\\npod’s internal IP and other basic info.\\nListing 3.1 showed a full description of a running pod, including its status. The status\\npart contains read-only runtime data that shows the state of the resource at a given\\nmoment. When creating a new pod,  you never need to provide the status  part. \\n The three parts described previously show the typical structure of a Kubernetes\\nAPI object. As you’ll see throughout the b ook, all other objects have the same anat-\\nomy. This makes understanding new objects relatively easy.\\n Going through all the individual properties in the previous YAML doesn’t make\\nmuch sense, so, instead, let’s see what th e most basic YAML for creating a pod looks\\nlike. \\n3.2.2 Creating a simple YAML descriptor for a pod\\nYou’re going to create a file called kubi a-manual.yaml (you can create it in any\\ndirectory you want), or download the book ’s code archive, where you’ll find the\\nfile inside the Chapter03 directory. The fo llowing listing shows the entire contents\\nof the file.\\napiVersion: v1         \\nkind: Pod                             \\nmetadata:     \\n  name: kubia-manual         \\nspec:   containers: \\n  - image: luksa/kubia          \\n    name: kubia             ports: \\n    - containerPort: 8080     \\n      protocol: TCP\\nI’m sure you’ll agree this is much simpler than the definition in listing 3.1. Let’s exam-\\nine this descriptor in detail. It conforms to the v1 version of the Kubernetes API. The\\ntype of resource you’re describing is a pod, with the name kubia-manual . The pod\\nconsists of a single container based on the luksa/kubia  image. You’ve also given a\\nname to the container and indicate d that it’s listening on port 8080 . \\nSPECIFYING  CONTAINER  PORTS\\nSpecifying ports in the pod definition is purely informational. Omitting them has no\\neffect on whether clients can connect to th e pod through the port or not. If the con-Listing 3.2 A basic pod manifest: kubia-manual.yaml\\nDescriptor conforms\\nto version v1 of\\nKubernetes APIYou’re \\ndescribing a pod.\\nThe name \\nof the pod\\nContainer image to create \\nthe container from\\nName of the container\\nThe port the app \\nis listening on'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 95}, page_content=\"64 CHAPTER  3Pods: running containers in Kubernetes\\ntainer is accepting connections through a port bound to the 0.0.0.0 address, other\\npods can always connect to it, even if the port  isn’t listed in the pod spec explicitly. But\\nit makes sense to define the ports explicitly  so that everyone using your cluster can\\nquickly see what ports each pod exposes. Ex plicitly defining ports also allows you to\\nassign a name to each port, which can come in handy, as you’ll see later in the book.\\nUsing kubectl explain to discover possible API object fields\\nWhen preparing a manifest, you can either turn to the Kubernetes reference\\ndocumentation at http:/ /kubernetes.io/docs/api  to see which attributes are\\nsupported by each API object, or you can use the kubectl  explain  command.\\nFor example, when creating a pod manifest from scratch, you can start by asking\\nkubectl  to explain pods:\\n$ kubectl explain pods\\nDESCRIPTION:\\nPod is a collection of containers that can run on a host. This resource \\nis created by clients and scheduled onto hosts.\\nFIELDS:\\n   kind      <string>     Kind is a string value representing the REST resource this object\\n     represents...\\n   metadata  <Object>\\n     Standard object's metadata...\\n   spec      <Object>\\n     Specification of the desired behavior of the pod...\\n   status    <Object>\\n     Most recently obse rved status of the pod. This data may not be up to\\n     date...\\nKubectl prints out the explanation of t he object and lists the attributes the object\\ncan contain. You can then drill deeper to fi nd out more about each attribute. For\\nexample, you can examine the spec attribute like this:\\n$ kubectl explain pod.spec\\nRESOURCE: spec <Object>\\nDESCRIPTION:\\n    Specification of the desired behavior of the pod...\\n    podSpec is a description of a pod.\\nFIELDS:\\n   hostPID   <boolean>     Use the host's pid namespace. Optional: Default to false.\\n   ...\\n   volumes   <[]Object>\\n     List of volumes that can be  mounted by containers  belonging to the\\n     pod.\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 96}, page_content='65 Creating pods from YAML or JSON descriptors\\n3.2.3 Using kubectl create to create the pod\\nTo create the pod from your YAML file, use the kubectl  create  command:\\n$ kubectl create -f kubia-manual.yaml\\npod \"kubia-manual\" created\\nThe kubectl  create  -f command is used for creating any resource (not only pods)\\nfrom a YAML or JSON file. \\nRETRIEVING  THE WHOLE  DEFINITION  OF A RUNNING  POD\\nAfter creating the pod, you can ask Kubernet es for the full YAML of the pod. You’ll\\nsee it’s similar to the YAML you saw earlie r. You’ll learn about the additional fields\\nappearing in the returned definition in the next sections. Go ahead and use the fol-\\nlowing command to see the full descriptor of the pod:\\n$ kubectl get po kubia-manual -o yaml\\nIf you’re more into JSON, you can also tell kubectl  to return JSON instead of YAML\\nlike this (this works even if you used YAML to create the pod):\\n$ kubectl get po kubia-manual -o json\\nSEEING YOUR NEWLY  CREATED  POD IN THE LIST OF PODS\\nYour pod has been created, but how do you kn ow if it’s running? Let’s list pods to see\\ntheir statuses:\\n$ kubectl get pods\\nNAME            READY   STATUS    RESTARTS   AGE\\nkubia-manual    1/1     Running   0          32skubia-zxzij     1/1     Running   0          1d    \\nThere’s your kubia-manual  pod. Its status shows that it’s running. If you’re like me,\\nyou’ll probably want to confirm that’s true by talking to the pod. You’ll do that in aminute. First, you’ll look at the app’s log to check for any errors.\\n3.2.4 Viewing application logs\\nYour little Node.js application logs to th e process’s standard output. Containerized\\napplications usually log to the standard ou tput and standard error stream instead of   Containers  <[]Object> -required-\\n     List of containers belong ing to the pod. Containe rs cannot currently\\n     Be added or re moved. There must be at lea st one container in a pod.\\n     Cannot be updated. More info:\\n     http://releases.k8s.io/release-1.4/docs/user-guide/containers.md'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 97}, page_content='66 CHAPTER  3Pods: running containers in Kubernetes\\nwriting their logs to files. This is to allow users to view logs of different applications in\\na simple, standard way. \\n The container runtime (Docke r in your case) redirects those streams to files and\\nallows you to get the container’s log by running\\n$ docker logs <container id>\\nYou could use ssh to log into the node where your pod is running and retrieve its logs\\nwith docker  logs , but Kubernetes provides an easier way. \\nRETRIEVING  A POD’S LOG WITH KUBECTL  LOGS\\nTo see your pod’s log (more precisely, the container’s log) you run the following com-\\nmand on your local machine (no need to ssh anywhere):\\n$ kubectl logs kubia-manual\\nKubia server starting...\\nYou haven’t sent any web requests to your Node.js app, so the log only shows a single\\nlog statement about the server starting up. As you can see, retrieving logs of an appli-cation running in Kubernetes is incredibly  simple if the pod only contains a single\\ncontainer. \\nNOTE Container logs are automatically rota ted daily and every time the log file\\nreaches 10MB in size. The kubectl  logs  command only shows the log entries\\nfrom the last rotation.\\nSPECIFYING  THE CONTAINER  NAME WHEN GETTING  LOGS OF A MULTI -CONTAINER  POD\\nIf your pod includes multiple containers, yo u have to explicitly specify the container\\nname by including the -c <container  name>  option when running kubectl  logs . In\\nyour kubia-manual  pod, you set the container’s name to kubia , so if additional con-\\ntainers exist in the pod, you’d have to get its logs like this:\\n$ kubectl logs kubia-manual -c kubia\\nKubia server starting...\\nNote that you can only retrieve container logs  of pods that are st ill in existence. When\\na pod is deleted, its logs are also deleted. To make a pod’s logs available even after the\\npod is deleted, you need to set up centraliz ed, cluster-wide logging, which stores all\\nthe logs into a central store. Chapter 17 explains how centralized logging works.\\n3.2.5 Sending requests to the pod\\nThe pod is now running—at least that’s what kubectl  get and your app’s log say. But\\nhow do you see it in action? In th e previous chapter, you used the kubectl  expose\\ncommand to create a service to gain access to the pod externally. You’re not going to\\ndo that now, because a whole chapter is de dicated to services, and you have other ways\\nof connecting to a pod for testing and debu gging purposes. One of them is through\\nport forwarding .'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 98}, page_content='67 Organizing pods with labels\\nFORWARDING  A LOCAL  NETWORK  PORT TO A PORT IN THE POD\\nWhen you want to talk to a specific pod without going through a service (for debug-\\nging or other reasons), Kubernetes allows you to configure port forwarding to the\\npod. This is done through the kubectl  port-forward  command. The following\\ncommand will forward your machine’s local port 8888  to port 8080  of your kubia-\\nmanual  pod:\\n$ kubectl port-forward kubia-manual 8888:8080\\n... Forwarding from 127.0.0.1:8888 -> 8080... Forwarding from [::1]:8888 -> 8080\\nThe port forwarder is running and you ca n now connect to your pod through the\\nlocal port. \\nCONNECTING  TO THE POD THROUGH  THE PORT FORWARDER\\nIn a different terminal, you can now use curl  to send an HTTP request to your pod\\nthrough the kubectl  port-forward  proxy running on localhost:8888 :\\n$ curl localhost:8888\\nYou’ve hit kubia-manual\\nFigure 3.5 shows an overly simplified view of what happens when you send the request.\\nIn reality, a couple of additional components sit between the kubectl  process and the\\npod, but they aren’t relevant right now.\\nUsing port forwarding like this is an effe ctive way to test an individual pod. You’ll\\nlearn about other similar methods throughout the book. \\n3.3 Organizing pods with labels\\nAt this point, you have two pods running  in your cluster. When deploying actual\\napplications, most users will end up ru nning many more pods. As the number of\\npods increases, the need for categorizing them into subsets becomes more and\\nmore evident.\\n For example, with microservices architec tures, the number of deployed microser-\\nvices can easily exceed 20 or more. Those components will probably be replicatedKubernetes clusterPort\\n8080\\nLocal machinekubectl\\nport-forward\\nprocesscurlPort\\n8888\\nPod:\\nkubia-manual\\nFigure 3.5 A simplified view of what happens when you use curl  with kubectl  port-forward'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 99}, page_content='68 CHAPTER  3Pods: running containers in Kubernetes\\n(multiple copies of the same component wi ll be deployed) and multiple versions or\\nreleases (stable, beta, canary, and so on) will run concurrently. This can lead to hun-\\ndreds of pods in the system. Without a mechanism for organizing them, you end up\\nwith a big, incomprehensible mess, such as  the one shown in figure 3.6. The figure\\nshows pods of multiple microservices, with several running multiple replicas, and others\\nrunning different releases of the same microservice.\\nIt’s evident you need a way of organizing th em into smaller groups based on arbitrary\\ncriteria, so every developer and system admi nistrator dealing with your system can eas-\\nily see which pod is which. And you’ll want to operate on every pod belonging to a cer-\\ntain group with a single action instead of having to perform the action for each podindividually. \\n Organizing pods and all other Ku bernetes objects is done through labels .\\n3.3.1 Introducing labels\\nLabels are a simple, yet incredibly powerf ul, Kubernetes feature for organizing not\\nonly pods, but all other Kubernetes resources. A label is an arbitrary key-value pair youattach to a resource, which is then utilized when selecting resources using label selectors\\n(resources are filtered based on whether th ey include the label specified in the selec-\\ntor). A resource can have more than one label,  as long as the keys of those labels are\\nunique within that resource. You usually at tach labels to resources when you create\\nthem, but you can also add additional labels or even modify the values of existinglabels later without having to recreate the resource. UI pod\\nUI podUI podAccount\\nServicepodProductCatalogpod\\nProductCatalogpod\\nProductCatalogpodShopping\\nCartpodShoppingCartpod\\nOrder\\nServicepod\\nUI podUI pod\\nProductCatalogpodProductCatalogpod\\nOrder\\nServicepodAccountServicepod\\nProductCatalogpod\\nProductCatalogpodOrder\\nServicepod\\nFigure 3.6 Uncategorized pods in a microservices architecture'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 100}, page_content='69 Organizing pods with labels\\n Let’s turn back to the microservices exam ple from figure 3.6. By adding labels to\\nthose pods, you get a much-better-organize d system that everyone can easily make\\nsense of. Each pod is labeled with two labels:\\n\\uf0a1app, which specifies which app, component,  or microservice the pod belongs to. \\n\\uf0a1rel, which shows whether the application ru nning in the pod is a stable, beta,\\nor a canary release.\\nDEFINITION A canary release is when you depl oy a new version of an applica-\\ntion next to the stable version, and only let a small fraction of users hit thenew version to see how it behaves before rolling it out to all users. This pre-\\nvents bad releases from being exposed to too many users.\\nBy adding these two labels, you’ve essentially organized your pods into two dimen-\\nsions (horizontally by app and vertically  by release), as shown in figure 3.7.\\nEvery developer or ops person with access to  your cluster can now easily see the sys-\\ntem’s structure and where each pod fits  in by looking at the pod’s labels.\\n3.3.2 Specifying labels when creating a pod\\nNow, you’ll see labels in action by creating a new pod with two labels. Create a new file\\ncalled kubia-manual-with-labels.yaml with the contents of the following listing.\\napiVersion: v1                                         \\nkind: Pod                                              metadata:                                              \\n  name: kubia-manual-v2Listing 3.3 A pod with labels: kubia-manual-with-labels.yamlUI podapp: ui\\nrel: stable\\nrel=stableapp=ui\\nAccount\\nServicepodapp: as\\nrel: stableapp=as\\napp: pc\\nrel: stableapp=pc\\napp: sc\\nrel: stableapp=sc\\napp: os\\nrel: stableapp=os\\nProductCatalogpodShopping\\nCartpodOrder\\nServicepod\\nUI podapp: ui\\nrel: beta\\nrel=betaapp: pc\\nrel: betaapp: os\\nrel: betaProduct\\nCatalogpodOrder\\nServicepod\\nrel=canaryAccountServicepodapp: as\\nrel: canaryapp: pc\\nrel: canaryapp: os\\nrel: canaryProduct\\nCatalogpodOrder\\nServicepod\\nFigure 3.7 Organizing pods in a microservices architecture with pod labels'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 101}, page_content='70 CHAPTER  3Pods: running containers in Kubernetes\\n  labels:    \\n    creation_method: manual          \\n    env: prod                        spec: \\n  containers: \\n  - image: luksa/kubia    name: kubia\\n    ports: \\n    - containerPort: 8080\\n      protocol: TCP\\nYou’ve included the labels creation_method=manual  and env=data.labels  section.\\nYou’ll create this pod now:\\n$ kubectl create -f kubia-manual-with-labels.yaml\\npod \"kubia-manual-v2\" created\\nThe kubectl  get pods  command doesn’t list any labels by default, but you can see\\nthem by using the --show-labels  switch:\\n$ kubectl get po --show-labels\\nNAME            READY  STATUS   RESTARTS  AGE LABELS\\nkubia-manual    1/1    Running  0         16m <none>\\nkubia-manual-v2 1/1    Running  0         2m  creat_method=manual,env=prod\\nkubia-zxzij     1/1    Running  0         1d  run=kubia\\nInstead of listing all labels, if you’re only interested in certain labels, you can specify\\nthem with the -L switch and have each displayed in  its own column. List pods again\\nand show the columns for the two la bels you’ve attached to your kubia-manual-v2  pod:\\n$ kubectl get po -L creation_method,env\\nNAME            READY   STATUS     RESTARTS   AGE   CREATION_METHOD   ENV\\nkubia-manual    1/1     Running   0          16m   <none>            <none>\\nkubia-manual-v2 1/1     Running   0          2m    manual            prod\\nkubia-zxzij     1/1     Running   0          1d    <none>            <none>\\n3.3.3 Modifying labels of existing pods\\nLabels can also be added to and modified on existing pods. Because the kubia-man-\\nual pod was also created manually, let’s add the creation_method=manual  label to it: \\n$ kubectl label po kubia-manual creation_method=manual\\npod \"kubia-manual\" labeled\\nNow, let’s also change the env=prod  label to env=debug  on the kubia-manual-v2  pod,\\nto see how existing labels can be changed.\\nNOTE You need to use the --overwrite  option when changing existing labels.\\n$ kubectl label po kubia-manual-v2 env=debug --overwrite\\npod \"kubia-manual-v2\" labeledTwo labels are \\nattached to the pod.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 102}, page_content=\"71 Listing subsets of pods through label selectors\\nList the pods again to see the updated labels:\\n$ kubectl get po -L creation_method,env\\nNAME            READY   STATUS     RESTARTS   AGE   CREATION_METHOD   ENV\\nkubia-manual    1/1     Running   0          16m   manual            <none>\\nkubia-manual-v2 1/1     Running   0          2m    manual            debug\\nkubia-zxzij     1/1     Running   0          1d    <none>            <none>\\nAs you can see, attaching labels to resource s is trivial, and so is changing them on\\nexisting resources. It may not be evident ri ght now, but this is an incredibly powerful\\nfeature, as you’ll see in the next chapter. Bu t first, let’s see what you can do with these\\nlabels, in addition to displa ying them when listing pods.\\n3.4 Listing subsets of pods through label selectors\\nAttaching labels to resources so you can see th e labels next to each resource when list-\\ning them isn’t that interesting. But labels go hand in hand with label selectors . Label\\nselectors allow you to select a subset of po ds tagged with certain labels and perform an\\noperation on those pods. A label selector is  a criterion, which filters resources based\\non whether they include a certain label with a certain value. \\n A label selector can select reso urces based on whether the resource\\n\\uf0a1Contains (or doesn’t contain) a label with a certain key\\n\\uf0a1Contains a label with a certain key and value\\n\\uf0a1Contains a label with a certain key, bu t with a value not equal to the one you\\nspecify\\n3.4.1 Listing pods using a label selector\\nLet’s use label selectors on the pods you’ve created so far. To see all pods you created\\nmanually (you labeled them with creation_method=manual ), do the following:\\n$ kubectl get po -l creation_method=manual\\nNAME              READY     STATUS    RESTARTS   AGEkubia-manual      1/1       Running   0          51m\\nkubia-manual-v2   1/1       Running   0          37m\\nTo list all pods that include the env label, whatever its value is:\\n$ kubectl get po -l env\\nNAME              READY     STATUS    RESTARTS   AGEkubia-manual-v2   1/1       Running   0          37m\\nAnd those that don’t have the env label:\\n$ kubectl get po -l '!env'\\nNAME           READY     STATUS    RESTARTS   AGE\\nkubia-manual   1/1       Running   0          51mkubia-zxzij    1/1       Running   0          10d\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 103}, page_content='72 CHAPTER  3Pods: running containers in Kubernetes\\nNOTE Make sure to use single quotes around !env , so the bash shell doesn’t\\nevaluate the exclamation mark.\\nSimilarly, you could also match pods  with the following label selectors:\\n\\uf0a1creation_method!=manual  to select pods with the creation_method  label with\\nany value other than manual\\n\\uf0a1env in (prod,devel)  t o  s e l e c t  p o d s  w i t h  t h e  env label set to either prod  or\\ndevel\\n\\uf0a1env notin  (prod,devel)  to select pods with the env label set to any value other\\nthan prod  or devel\\nTurning back to the pods in the microser vices-oriented architecture example, you\\ncould select all pods that are part of th e product catalog microservice by using the\\napp=pc  label selector (shown in the following figure).\\n3.4.2 Using multiple conditions in a label selector\\nA selector can also include multiple co mma-separated criteria. Resources need to\\nmatch all of them to match the selector. If, for example, you want to select only pods\\nrunning the beta release of the product ca talog microservice, you’d use the following\\nselector: app=pc,rel=beta  (visualized in figure 3.9).\\n Label selectors aren’t useful only for lis ting pods, but also for performing actions\\non a subset of all pods. For example, later in the chapter, you’ll  see how to use label\\nselectors to delete multiple pods at once . But label selectors aren’t used only by\\nkubectl . They’re also used internally, as you’ll see next.UI podapp: ui\\nrel: stable\\nrel=stableapp=ui\\nAccount\\nServicepodapp: as\\nrel: stableapp=as\\napp: pc\\nrel: stableapp=pc\\napp: sc\\nrel: stableapp=sc\\napp: os\\nrel: stableapp=os\\nProductCatalogpodShopping\\nCartpodOrder\\nServicepod\\nUI podapp: ui\\nrel: beta\\nrel=betaapp: pc\\nrel: betaapp: os\\nrel: betaProduct\\nCatalog\\npodOrder\\nService\\npod\\nrel=canaryAccountService\\npodapp: as\\nrel: canaryapp: pc\\nrel: canaryapp: os\\nrel: canaryProduct\\nCatalog\\npodOrder\\nService\\npod\\nFigure 3.8 Selecting the product catalog micr oservice pods using the “app=pc” label selector'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 104}, page_content='73 Using labels and selectors to constrain pod scheduling\\n3.5 Using labels and selector s to constrain pod scheduling\\nAll the pods you’ve created so far have be en scheduled pretty much randomly across\\nyour worker nodes. As I’ve mentioned in th e previous chapter, this is the proper way\\nof working in a Kubernetes cluster. Becaus e Kubernetes exposes all the nodes in the\\ncluster as a single, large deployment platfo rm, it shouldn’t matter to you what node a\\npod is scheduled to. Because each pod ge ts the exact amount of computational\\nresources it requests (CPU, memory, and so  on) and its accessibility from other pods\\nisn’t at all affected by the node the pod is  scheduled to, usually there shouldn’t be any\\nneed for you to tell Kubernetes ex actly where to schedule your pods. \\n Certain cases exist, however, where you’ll wa nt to have at least a little say in where\\na pod should be scheduled. A good exampl e is when your hardware infrastructure\\nisn’t homogenous. If part of your worker nodes have spinning hard drives, whereas\\nothers have SSDs, you may want to schedule  certain pods to one group of nodes and\\nthe rest to the other. Another example is  when you need to schedule pods perform-\\ning intensive GPU-based computation only to nodes that provide the required GPU\\nacceleration. \\n You never want to say specifically what node a pod should be scheduled to, because\\nthat would couple the application to the infrastructure, whereas the whole idea of\\nKubernetes is hiding the actual infrastructure  from the apps that run on it. But if you\\nwant to have a say in where a pod should be  scheduled, instead of specifying an exact\\nnode, you should describe the node requir ements and then let Kubernetes select a\\nnode that matches those requirements. This can be done through node labels and\\nnode label selectors. UI podapp: ui\\nrel: stable\\nrel=stableapp=ui\\nAccount\\nServicepodapp: as\\nrel: stableapp=as\\napp: pc\\nrel: stableapp=pc\\napp: sc\\nrel: stableapp=sc\\napp: os\\nrel: stableapp=os\\nProductCatalogpodShopping\\nCartpodOrder\\nServicepod\\nUI podapp: ui\\nrel: beta\\nrel=betaapp: pc\\nrel: betaapp: os\\nrel: betaProduct\\nCatalog\\npodOrder\\nService\\npod\\nrel=canaryAccountService\\npodapp: as\\nrel: canaryapp: pc\\nrel: canaryapp: os\\nrel: canaryProduct\\nCatalog\\npodOrder\\nService\\npod\\nFigure 3.9 Selecting pods with multiple label selectors'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 105}, page_content='74 CHAPTER  3Pods: running containers in Kubernetes\\n3.5.1 Using labels for categorizing worker nodes\\nAs you learned earlier, pods aren’t the on ly Kubernetes resource type that you can\\nattach a label to. Labels can be attached to any Kubernetes object, including nodes.\\nUsually, when the ops team adds a new node to the cluster, they’ll categorize the nodeby attaching labels specifying the type of hardware the node provides or anything else\\nthat may come in handy when scheduling pods. \\n Let’s imagine one of the nodes in your cluster contains a GPU meant to be used\\nfor general-purpose GPU computing. You want to add a label to the node showing this\\nfeature. You’re going to add the label \\ngpu=true  to one of your nodes (pick one out of\\nthe list returned by kubectl  get nodes ):\\n$ kubectl label node gke-kubia-85f6-node-0rrx gpu=true\\nnode \"gke-kubia-85f6-node-0rrx\" labeled\\nNow you can use a label selector when listing the nodes, like you did before with pods.\\nList only nodes that include the label gpu=true :\\n$ kubectl get nodes -l gpu=true\\nNAME                      STATUS AGE\\ngke-kubia-85f6-node-0rrx  Ready  1d\\nAs expected, only one node has this label. You can also try listing all the nodes and tell\\nkubectl  to display an additional column showing the values of each node’s gpu label\\n(kubectl  get nodes  -L gpu).\\n3.5.2 Scheduling pods to specific nodes\\nNow imagine you want to deploy a new pod that needs a GPU to perform its work.\\nTo ask the scheduler to only choose am ong the nodes that provide a GPU, you’ll\\nadd a node selector to the pod’s YAML. Create a file called kubia-gpu.yaml with thefollowing listing’s contents and then use \\nkubectl  create  -f kubia-gpu.yaml  to cre-\\nate the pod.\\napiVersion: v1                                         \\nkind: Pod                                              \\nmetadata:                                              \\n  name: kubia-gpuspec: \\n  nodeSelector:               \\n    gpu: \"true\"                 containers: \\n  - image: luksa/kubia\\n    name: kubiaListing 3.4 Using a label selector to schedul e a pod to a specific node: kubia-gpu.yaml\\nnodeSelector tells Kubernetes \\nto deploy this pod only to \\nnodes containing the gpu=true label.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 106}, page_content='75 Annotating pods\\nYou’ve added a nodeSelector  field under the spec  section. When you create the pod,\\nthe scheduler will only choose among the nodes that contain the gpu=true  label\\n(which is only a single node in your case). \\n3.5.3 Scheduling to one specific node\\nSimilarly, you could also schedule a pod to an exact node, because each node also has\\na unique label with the key kubernetes.io/hostname  and value set to the actual host-\\nname of the node. But setting the nodeSelector  to a specific node by the hostname\\nlabel may lead to the pod being unschedulable  if the node is offline. You shouldn’t\\nthink in terms of individual nodes. Always th ink about logical groups of nodes that sat-\\nisfy certain criteria specified through label selectors.\\n This was a quick demonstration of how labels and label selectors work and how\\nthey can be used to influence the operat ion of Kubernetes. The importance and use-\\nfulness of label selectors will become even mo re evident when we talk about Replication-\\nControllers and Services in the next two chapters. \\nNOTE Additional ways of influencing whic h node a pod is scheduled to are\\ncovered in chapter 16.\\n3.6 Annotating pods\\nIn addition to labels, pods and other objects can also contain annotations . Annotations\\nare also key-value pairs, so in essence, they ’re similar to labels, but they aren’t meant to\\nhold identifying information. They can’t be  used to group objects the way labels can.\\nWhile objects can be selected through labe l selectors, there’s no such thing as an\\nannotation selector. \\n On the other hand, annotations can hold much larger pieces of information and\\nare primarily meant to be used by tools. Certain annotations are automatically added\\nto objects by Kubernetes, but othe rs are added by users manually.\\n Annotations are also commonly used wh en introducing new features to Kuberne-\\ntes. Usually, alpha and beta versions of ne w features don’t introduce any new fields to\\nAPI objects. Annotations are used instead of  fields, and then once the required API\\nchanges have become clear and been agreed  upon by the Kubernetes developers, new\\nfields are introduced and the related annotations deprecated.\\n A great use of annotations is adding desc riptions for each pod or other API object,\\nso that everyone using the cluster can quic kly look up information about each individ-\\nual object. For example, an annotation used  to specify the name of the person who\\ncreated the object can make collaboration between everyone working on the cluster\\nmuch easier.\\n3.6.1 Looking up an object’s annotations\\nLet’s see an example of an annotation that Kubernetes added automatically to thepod you created in the previous chapter. To see the annotation s, you’ll need to'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 107}, page_content='76 CHAPTER  3Pods: running containers in Kubernetes\\nrequest the full YAML of the pod or use the kubectl  describe  command. You’ll use the\\nfirst option in the following listing.\\n$ kubectl get po kubia-zxzij -o yaml\\napiVersion: v1\\nkind: pod\\nmetadata:  annotations:\\n    kubernetes.io/created-by: |\\n      {\"kind\":\"SerializedReference\", \"apiVersion\":\"v1\",       \"reference\" :{\"kind\":\"ReplicationController\", \"namespace\":\"default\", ...\\nWithout going into too many details, as you can see, the kubernetes.io/created-by\\nannotation holds JSON data about the object  that created the pod. That’s not some-\\nthing you’d want to put into a label. Labels  should be short, whereas annotations can\\ncontain relatively large blobs of data (up to 256 KB in total).\\nNOTE The kubernetes.io/created-by  annotations was deprecated in ver-\\nsion 1.8 and will be removed in 1.9, so  you will no longer see it in the YAML.\\n3.6.2 Adding and modifying annotations\\nAnnotations can obviously be added to pods at creation time, the same way labels can.\\nThey can also be added to or modified on ex isting pods later. The simplest way to add\\nan annotation to an existing object is through the kubectl  annotate  command. \\n You’ll try adding an annotation to your kubia-manual  pod now:\\n$ kubectl annotate pod kubia-m anual mycompany.com/someannotation=\"foo bar\"\\npod \"kubia-manual\" annotated\\nYou added the annotation mycompany.com/s omeannotation  with the value foo bar.\\nIt’s a good idea to use this  format for annotation keys to prevent key collisions. When\\ndifferent tools or libraries add annotations to objects, they may accidentally override\\neach other’s annotations if they don’t use unique prefixes like you did here.\\n You can use kubectl  describe  to see the annotation you added:\\n$ kubectl describe pod kubia-manual\\n...\\nAnnotations:    mycompany.com/someannotation=foo bar...\\n3.7 Using namespaces to group resources\\nLet’s turn back to labels for a moment. We ’ve seen how they organize pods and other\\nobjects into groups. Because each object can have multiple labels, those groups of\\nobjects can overlap. Plus, when wo rking with the cluster (through kubectl  for example),\\nif you don’t explicitly specify a label selector, you’ll always see all objects. Listing 3.5 A pod’s annotations'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 108}, page_content='77 Using namespaces to group resources\\n But what about times when you want to sp lit objects into separate, non-overlapping\\ngroups? You may want to only operate inside  one group at a time. For this and other\\nreasons, Kubernetes also groups objects into namespaces. These aren’t the Linux\\nnamespaces we talked about in chapter 2,  which are used to isolate processes from\\neach other. Kubernetes namespaces provide a scope for objects names. Instead of hav-\\ning all your resources in one single namespac e, you can split them into multiple name-\\nspaces, which also allows you to use the sa me resource names multiple times (across\\ndifferent namespaces).\\n3.7.1 Understanding the need for namespaces\\nUsing multiple namespaces allows you to split complex systems with numerous com-\\nponents into smaller distinct groups. They can also be used for separating resources\\nin a multi-tenant environment, splitting up resources into production, development,\\nand QA environments, or in any other wa y you may need. Resource names only need\\nto be unique within a namespace. Two di fferent namespaces can contain resources of\\nthe same name. But, while most types of resources are namespaced, a few aren’t. One\\no f  t h e m  i s  t h e  N o d e  r e s o u r c e ,  w h i c h  i s  g l o b a l  a n d  n o t  t i e d  t o  a  s i n g l e  n a m e s p a c e .You’ll learn about other cluster-le vel resources in later chapters.\\n Let’s see how to use namespaces now.\\n3.7.2 Discovering other namespaces and their pods\\nFirst, let’s list all namespaces in your cluster:\\n$ kubectl get ns\\nNAME          LABELS    STATUS    AGE\\ndefault       <none>    Active    1h\\nkube-public   <none>    Active    1h\\nkube-system   <none>    Active    1h\\nUp to this point, you’ve operated only in the default  namespace. When listing resources\\nwith the kubectl  get command, you’ve never specif ied the namespace explicitly, so\\nkubectl  always defaulted to the default  namespace, showing you only the objects in\\nthat namespace. But as you can see from the list, the kube-public  and the kube-system\\nnamespaces also exist. Let’s look at the pods that belong to the kube-system  name-\\nspace, by telling kubectl  to list pods in that namespace only:\\n$ kubectl get po --namespace kube-system\\nNAME                                 READY     STATUS    RESTARTS   AGE\\nfluentd-cloud-kubia-e8fe-node-txje   1/1       Running    0          1h\\nheapster-v11-fz1ge                   1/1       Running   0          1h\\nkube-dns-v9-p8a4t                    0/4       Pending   0          1h\\nkube-ui-v4-kdlai                     1/1       Running   0          1h\\nl7-lb-controller-v0.5.2-bue96        2/2       Running   92         1h\\nTIP You can also use -n instead of --namespace .'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 109}, page_content='78 CHAPTER  3Pods: running containers in Kubernetes\\nYou’ll learn about these pods later in the book (don’t worry if the pods shown here\\ndon’t match the ones on your system exactly). It’s clear from the name of the name-\\nspace that these are resources related to the Kubernetes system itself. By having\\nthem in this separate namespace, it keeps everything nicely organized. If they were\\nall in the default namespace, mixed in with  the resources you create yourself, you’d\\nhave a hard time seeing what belongs wher e, and you might inadvertently delete sys-\\ntem resources. \\n Namespaces enable you to separate resour ces that don’t belong together into non-\\noverlapping groups. If several users or gro ups of users are using the same Kubernetes\\ncluster, and they each manage their own distin ct set of resources, they should each use\\ntheir own namespace. This way, they don’t need to take any special care not to inad-\\nvertently modify or delete the other users’  resources and don’t need to concern them-\\nselves with name conflicts, because name spaces provide a scope for resource names,\\nas has already been mentioned.\\n  Besides isolating resources, namespaces ar e also used for allowing only certain users\\naccess to particular resources and even fo r limiting the amount of computational\\nresources available to individual users. You’ ll learn about this in chapters 12 through 14.\\n3.7.3 Creating a namespace\\nA namespace is a Kubernetes resource like an y other, so you can create it by posting a\\nYAML file to the Kubernetes API serv er. Let’s see how to do this now. \\nCREATING  A NAMESPACE  FROM A YAML FILE\\nFirst, create a custom-namespace.yaml file wi th the following listing’s contents (you’ll\\nfind the file in the book’s code archive).\\napiVersion: v1\\nkind: Namespace         \\nmetadata:  name: custom-namespace  \\nNow, use kubectl  to post the file to the Kubernetes API server:\\n$ kubectl create -f custom-namespace.yamlnamespace \"custom-namespace\" created\\nCREATING  A NAMESPACE  WITH KUBECTL  CREATE  NAMESPACE\\nAlthough writing a file like the previous one isn’t a big deal, it’s still a hassle. Luckily,\\nyou can also create namespaces with the dedicated kubectl  create  namespace  com-\\nmand, which is quicker than writing a YAML file. By having you create a YAML mani-fest for the namespace, I wanted to reinfo rce the idea that everything in KubernetesListing 3.6 A YAML definition of a namespace: custom-namespace.yaml\\nThis says you’re \\ndefining a namespace.\\nThis is the name \\nof the namespace.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 110}, page_content='79 Using namespaces to group resources\\nhas a corresponding API object that you can create, read, update, and delete by post-\\ning a YAML manifest to the API server.\\n You could have created the namespace like this:\\n$ kubectl create namespace custom-namespace\\nnamespace \"custom-namespace\" created\\nNOTE Although most objects’ names must conform to the naming conven-\\ntions specified in RFC 1035 (Domain names), which means they may containonly letters, digits, dashes, and dots, namespaces (and a few others) aren’tallowed to contain dots. \\n3.7.4 Managing objects in other namespaces\\nTo create resources in the namesp ace you’ve created, either add a namespace:  custom-\\nnamespace  entry to the metadata  section, or specify the namespace when creating the\\nresource with the kubectl  create  command:\\n$ kubectl create -f kubia-manual.yaml -n custom-namespace\\npod \"kubia-manual\" created\\nYou now have two pods with the same name ( kubia-manual ). One is in the default\\nnamespace, and the other is in your custom-namespace .\\n When listing, describing, modifying, or deleting objects in other namespaces, you\\nneed to pass the --namespace  (or -n) flag to kubectl . If you don’t specify the name-\\nspace, kubectl  performs the action in the default namespace configured in the cur-\\nrent kubectl  context. The current context’s name space and the current context itself\\ncan be changed through kubectl  config  commands. To learn more about managing\\nkubectl  contexts, refer to appendix A. \\nTIP To quickly switch to a different namespace, you can set up the following\\nalias: alias  kcd=\\'kubectl  config  set-context  $(kubectl  config  current-\\ncontext)  --namespace  \\'. You can then switch between namespaces using kcd\\nsome-namespace .\\n3.7.5 Understanding the isolation provided by namespaces\\nTo wrap up this section about namespaces, let me explain what namespaces don’t pro-\\nvide—at least not out of the box. Although  namespaces allow you to isolate objects\\ninto distinct groups, which allows you to op erate only on those belonging to the speci-\\nfied namespace, they don’t provide any kind of isolation of running objects. \\n For example, you may think that when di fferent users deploy pods across different\\nnamespaces, those pods are isolated from each other and can’t communicate, but that’s\\nnot necessarily the case. Whether namespac es provide network isolation depends on\\nwhich networking solution is deployed with Kubernetes. When the solution doesn’t\\nprovide inter-namespace network isolation, if a pod in namespace foo knows the IP'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 111}, page_content='80 CHAPTER  3Pods: running containers in Kubernetes\\naddress of a pod in namespace bar, there is nothing preventing it from sending traffic,\\nsuch as HTTP requests, to the other pod. \\n3.8 Stopping and removing pods\\nYou’ve created a number of pods, which sh ould all still be running. You have four\\npods running in the default  namespace and one pod in custom-namespace . You’re\\ngoing to stop all of them now, because you don’t need them anymore.\\n3.8.1 Deleting a pod by name\\nFirst, delete the kubia-gpu  pod by name:\\n$ kubectl delete po kubia-gpu\\npod \"kubia-gpu\" deleted\\nBy deleting a pod, you’re instructing Kubernet es to terminate all the containers that are\\npart of that pod. Kubernetes sends a SIGTERM  signal to the process and waits a certain\\nnumber of seconds (30 by default) for it to shut down gracefully. If it doesn’t shut down\\nin time, the process is then killed through SIGKILL . To make sure your processes are\\nalways shut down gracefully, they need to handle the SIGTERM  signal properly. \\nTIP You can also delete more than one po d by specifying multiple, space-sep-\\narated names (for example, kubectl  delete  po pod1  pod2 ).\\n3.8.2 Deleting pods using label selectors\\nInstead of specifying each pod to delete by  name, you’ll now use what you’ve learned\\nabout label selectors to stop both the kubia-manual  and the kubia-manual-v2  pod.\\nBoth pods include the creation_method=manual  label, so you can delete them by\\nusing a label selector:\\n$ kubectl delete po -l creation_method=manual\\npod \"kubia-manual\" deleted\\npod \"kubia-manual-v2\" deleted \\nIn the earlier microservices example, wher e you had tens (or possibly hundreds) of\\npods, you could, for instance, delete all c a n a r y  p o d s  a t  o n c e  b y  s p e c i f y i n g  t h e\\nrel=canary  label selector (visualized in figure 3.10):\\n$ kubectl delete po -l rel=canary\\n3.8.3 Deleting pods by deleting the whole namespace\\nOkay, back to your real pods. What about the pod in the custom-namespace ? You no\\nlonger need either the pods in that na mespace, or the namespace itself. You can'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 112}, page_content='81 Stopping and removing pods\\ndelete the whole namespace (the pods will be deleted along with the namespace auto-\\nmatically), using the following command:\\n$ kubectl delete ns custom-namespace\\nnamespace \"custom-namespace \" deleted\\n3.8.4 Deleting all pods in a namespace, while keeping the namespace\\nYou’ve now cleaned up almost everything . But what about the pod you created with\\nthe kubectl  run command in chapter 2? That one is still running:\\n$ kubectl get pods\\nNAME            READY   STATUS    RESTARTS   AGEkubia-zxzij     1/1     Running   0          1d    \\nThis time, instead of deleting the specific pod,  tell Kubernetes to delete all pods in the\\ncurrent namespace by using the --all  option:\\n$ kubectl delete po --all\\npod \"kubia-zxzij\" deleted\\nNow, double check that no pods were left running:\\n$ kubectl get podsNAME            READY   STATUS        RESTARTS   AGE\\nkubia-09as0     1/1     Running       0          1d    \\nkubia-zxzij     1/1     Terminating   0          1d    UI podapp: ui\\nrel: stable\\nrel=stableapp=ui\\nAccount\\nServicepodapp: as\\nrel: stableapp=as\\napp: pc\\nrel: stableapp=pc\\napp: sc\\nrel: stableapp=sc\\napp: os\\nrel: stableapp=os\\nProductCatalogpodShopping\\nCartpodOrder\\nServicepod\\nUI podapp: ui\\nrel: beta\\nrel=betaapp: pc\\nrel: betaapp: os\\nrel: betaProduct\\nCatalogpodOrder\\nServicepod\\nrel=canaryAccountServicepodapp: as\\nrel: canaryapp: pc\\nrel: canaryapp: os\\nrel: canaryProduct\\nCatalog\\npodOrder\\nService\\npod\\nFigure 3.10 Selecting and deleting all canary pods through the rel=canary  label selector'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 113}, page_content='82 CHAPTER  3Pods: running containers in Kubernetes\\nWait, what!?! The kubia-zxzij  pod is terminating, but a new pod called kubia-09as0 ,\\nwhich wasn’t there before, has appeared. No matter how many times you delete all\\npods, a new pod called kubia-something  will emerge. \\n You may remember you created your first pod with the kubectl  run command. In\\nchapter 2, I mentioned that this doesn’t create a pod directly, but instead creates a\\nReplicationController, which then creates th e pod. As soon as you delete a pod cre-\\nated by the ReplicationController, it imme diately creates a new one. To delete the\\npod, you also need to delete  the ReplicationController. \\n3.8.5 Deleting (almost) all resources in a namespace\\nYou can delete the ReplicationController an d the pods, as well as all the Services\\nyou’ve created, by deleting all resource s in the current namespace with a single\\ncommand:\\n$ kubectl delete all --all\\npod \"kubia-09as0\" deleted\\nreplicationcontroller \"kubia\" deleted\\nservice \"kubernetes\" deletedservice \"kubia-http\" deleted\\nThe first all in the command specifies that you’re deleting resources of all types, and\\nthe --all  option specifies that you’re deleting all resource instances instead of speci-\\nfying them by name (you already used this  option when you ran the previous delete\\ncommand).\\nNOTE Deleting everything with the all keyword doesn’t delete absolutely\\neverything. Certain resources (like Secrets, which we’ll introduce in chapter 7)are preserved and need to be deleted explicitly.\\nAs it deletes resources, \\nkubectl  will print the name of every resource it deletes. In the\\nlist, you should see the kubia  ReplicationController and the kubia-http  Service you\\ncreated in chapter 2. \\nNOTE The kubectl  delete  all --all  command also deletes the kubernetes\\nService, but it should be recrea ted automatically in a few moments.\\n3.9 Summary\\nAfter reading this chapter, you should now have a decent knowledge of the central\\nbuilding block in Kubernetes. Every other co ncept you’ll learn about in the next few\\nchapters is directly related to pods. \\n In this chapter, you’ve learned\\n\\uf0a1How to decide whether certain containers  should be grouped together in a pod\\nor not.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 114}, page_content='83 Summary\\n\\uf0a1Pods can run multiple processes and are similar to physical hosts in the non-\\ncontainer world.\\n\\uf0a1YAML or JSON descriptors can be writ ten and used to create pods and then\\nexamined to see the specificatio n of a pod and its current state.\\n\\uf0a1Labels and label selectors should be used  to organize pods and easily perform\\noperations on multiple pods at once.\\n\\uf0a1You can use node labels and selectors to schedule pods only to nodes that have\\ncertain features.\\n\\uf0a1Annotations allow attaching larger blobs of data to pods either by people ortools and libraries.\\n\\uf0a1Namespaces can be used to allow diff erent teams to use the same cluster as\\nthough they were using separate Kubernetes clusters.\\n\\uf0a1How to use the kubectl  explain  command to quickly look up the information\\non any Kubernetes resource. \\nIn the next chapter, you’ll learn about ReplicationControllers and other resources\\nthat manage pods.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 115}, page_content='84Replication and other\\ncontrollers: deploying\\nmanaged pods\\nAs you’ve learned so far, pods represent the basic deployable unit in Kubernetes.\\nYou know how to create, supervise, and manage them manually. But in real-world\\nuse cases, you want your deployments to stay up and running automatically and\\nremain healthy without any manual interven tion. To do this, you almost never cre-\\nate pods directly. Instead, you create othe r types of resources, such as Replication-\\nControllers or Deployments, which then  create and manage the actual pods.\\n When you create unmanaged pods (such as the ones you created in the previ-\\nous chapter), a cluster node is selected to  run the pod and then its containers are\\nrun on that node. In this chapter, you’ll learn that Kubernetes then monitorsThis chapter covers\\n\\uf0a1Keeping pods healthy\\n\\uf0a1Running multiple instances of the same pod\\n\\uf0a1Automatically rescheduling pods after a node fails\\n\\uf0a1Scaling pods horizontally\\n\\uf0a1Running system-level pods on each cluster node\\n\\uf0a1Running batch jobs\\n\\uf0a1Scheduling jobs to run periodically or once in \\nthe future'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 116}, page_content='85 Keeping pods healthy\\nthose containers and automatically restarts them if they fail. But if the whole node\\nfails, the pods on the node are lost and will not be replaced with new ones, unless\\nthose pods are managed by the previously mentioned ReplicationControllers or simi-\\nlar. In this chapter, you’ll learn how Kubern etes checks if a container is still alive and\\nrestarts it if it isn’t. You’ll also learn how to run managed pods—both those that run\\nindefinitely and those that perform a single task and then stop. \\n4.1 Keeping pods healthy\\nOne of the main benefits of using Kubernetes is the ability to give it a list of contain-\\ners and let it keep those containers runnin g somewhere in the cluster. You do this by\\ncreating a Pod resource and letting Kubernetes pick a worker node for it and run\\nthe pod’s containers on that node. But what  if one of those containers dies? What if\\nall containers of a pod die? \\n As soon as a pod is scheduled to a node, the Kubelet on that node will run its con-\\ntainers and, from then on, keep them runn ing as long as the pod exists. If the con-\\ntainer’s main process crashes, the Kube let will restart the container. If your\\napplication has a bug that causes it to crash every once in a while, Kubernetes will\\nrestart it automatically, so even without do ing anything special in the app itself, run-\\nning the app in Kubernetes automaticall y gives it the ability to heal itself. \\n But sometimes apps stop working without their process crashing. For example, a\\nJava app with a memory leak will start throwing OutOfMemoryErrors, but the JVM\\nprocess will keep running. It would be great to have a way for an app to signal to\\nKubernetes that it’s no longer functionin g properly and have Kubernetes restart it. \\n We’ve said that a container that crashes is restarted automatically, so maybe you’re\\nthinking you could catch these types of errors in the app and exit the process when\\nthey occur. You can certainly do that, but it still doesn’t solve all your problems. \\n For example, what about those situations  when your app stops responding because\\nit falls into an infinite loop or a deadlock ? To make sure applications are restarted in\\nsuch cases, you must check an applicatio n’s health from the outside and not depend\\non the app doing it internally. \\n4.1.1 Introducing liveness probes\\nKubernetes can check if a cont ainer is still alive through liveness probes . You can specify\\na liveness probe for each container in the pod’s specification. Kubernetes will periodi-cally execute the probe and restart the container if the probe fails. \\nNOTE Kubernetes also supports readiness probes , which we’ll learn about in the\\nnext chapter. Be sure not to confuse the two. They’re used for two differentthings.\\nKubernetes can probe a container using one of the three mechanisms:\\n\\uf0a1An HTTP GET  probe performs an HTTP GET request on the container’s IP\\naddress, a port and path you specify. If the probe receives a response, and the'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 117}, page_content='86 CHAPTER  4Replication and other controllers: deploying managed pods\\nresponse code doesn’t represent an error (in other words, if the HTTP response\\ncode is 2xx or 3xx), the probe is consid ered successful. If the server returns an\\nerror response code or if it doesn’t resp ond at all, the probe is considered a fail-\\nure and the container will be restarted as a result.\\n\\uf0a1A TCP Socket  probe tries to open a TCP connection to the specified port of the\\ncontainer. If the connection is establis hed successfully, the probe is successful.\\nOtherwise, the container is restarted.\\n\\uf0a1An Exec probe executes an arbitrary command inside the container and checks\\nthe command’s exit status code. If the st atus code is 0, the probe is successful.\\nAll other codes are considered failures. \\n4.1.2 Creating an HTTP-based liveness probe\\nLet’s see how to add a liveness probe to yo ur Node.js app. Because it’s a web app, it\\nmakes sense to add a liveness probe that w ill check whether its web server is serving\\nrequests. But because this particular Node.js app is too simple to ever fail, you’ll need\\nto make the app fail artificially. \\n To properly demo liveness probes, you’ ll modify the app slightly and make it\\nreturn a 500 Internal Server Error HTTP st atus code for each request after the fifth\\none—your app will handle the first five client requests properly and then return an\\nerror on every subsequent request. Thanks to  the liveness probe, it should be restarted\\nwhen that happens, allowing it to pr operly handle client requests again.\\n You can find the code of the new app in  the book’s code archive (in the folder\\nChapter04/kubia-unhealthy). I’ve pushed th e container image to Docker Hub, so you\\ndon’t need to build it yourself. \\n You’ll create a new pod that includes an HTTP GET liveness probe. The following\\nlisting shows the YAML for the pod.\\napiVersion: v1\\nkind: pod\\nmetadata:\\n  name: kubia-livenessspec:\\n  containers:\\n  - image: luksa/kubia-unhealthy       name: kubia\\n    livenessProbe:                 \\n      httpGet:                             path: /                     \\n        port: 8080       Listing 4.1 Adding a liveness probe to a pod: kubia-liveness-probe.yaml\\nThis is the image \\ncontaining the (somewhat) \\nbroken app.\\nA liveness probe that will \\nperform an HTTP GET\\nThe path to \\nrequest in the HTTP requestThe network port\\nthe probe should\\nconnect to'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 118}, page_content='87 Keeping pods healthy\\nThe pod descriptor defines an httpGet  liveness probe, which tells Kubernetes to peri-\\nodically perform HTTP GET requests on path / on port 8080  to determine if the con-\\ntainer is still healthy. These requests start as soon as the container is run.\\n After five such requests (or actual clie nt requests), your app starts returning\\nHTTP status code 500, which Kubernetes will treat as a probe failure, and will thus\\nrestart the container. \\n4.1.3 Seeing a liveness probe in action\\nTo see what the liveness probe does, try crea ting the pod now. After about a minute and\\na half, the container will be restar ted. You can see that by running kubectl  get:\\n$ kubectl get po kubia-liveness\\nNAME             READY     STATUS    RESTARTS   AGE\\nkubia-liveness   1/1       Running   1          2m\\nThe RESTARTS  column shows that the pod’s contai ner has been restarted once (if you\\nwait another minute and a half, it gets re started again, and then the cycle continues\\nindefinitely).\\nYou can see why the container had to be restarted by looking at what kubectl  describe\\nprints out, as shown in the following listing.\\n$ kubectl describe po kubia-liveness\\nName:           kubia-liveness...\\nContainers:\\n  kubia:    Container ID:       docker://480986f8\\n    Image:              luksa/kubia-unhealthy\\n    Image ID:           docker://sha256:2b208508    Port:\\n    State:              Running                            \\n      Started:          Sun, 14 May 2017 11:41:40 +0200    Obtaining the application log of a crashed container\\nIn the previous chapter, you learned how to print the application’s log with kubectl\\nlogs. If your container is restarted, the kubectl  logs command will show the log of\\nthe current container. \\nWhen you want to figure out why the previous container terminated, you’ll want to\\nsee those logs instead of the current co ntainer’s logs. This can be done by using\\nthe --previous  option:\\n$ kubectl logs mypod --previous\\nListing 4.2 A pod’s description after its container is restarted\\nThe container is \\ncurrently running.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 119}, page_content='88 CHAPTER  4Replication and other controllers: deploying managed pods\\n    Last State:         Terminated                         \\n      Reason:           Error                              \\n      Exit Code:        137                                      Started:          Mon, 01 Jan 0001 00:00:00 +0000    \\n      Finished:         Sun, 14 May 2017 11:41:38 +0200    \\n    Ready:              True    Restart Count:      1                                 \\n    Liveness:           http-get http://:8080/ delay=0s timeout=1s\\n                        period=10s #success=1 #failure=3\\n    ...\\nEvents:... Killing container with id docker://95246981:pod \"kubia-liveness ...\"\\n    container \"kubia\" is unhea lthy, it will be killed and re-created.\\nYou can see that the container is current ly running, but it pr eviously terminated\\nbecause of an error. The exit code was 137, which has a special meaning—it denotes\\nthat the process was terminated by an external signal. The number 137 is a sum of two\\nnumbers: 128+x , where x is the signal number sent to the process that caused it to ter-\\nminate. In the example, x equals 9, which is the number of the SIGKILL  signal, mean-\\ning the process was killed forcibly.\\n The events listed at the bottom show why the container was killed—Kubernetes\\ndetected the container was unhealthy,  so it killed and re-created it. \\nNOTE When a container is killed, a completely new container is created—it’s\\nnot the same container being restarted again.\\n4.1.4 Configuring additional properties of the liveness probe\\nYou may have noticed that kubectl  describe  also displays additional information\\nabout the liveness probe:\\nLiveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 \\n          ➥ #failure=3\\nBeside the liveness probe options you specified explicitly, you can also see additional\\nproperties, such as delay , timeout , period , and so on. The delay=0s  part shows that\\nthe probing begins immediately afte r the container is started. The timeout  is set to\\nonly 1 second, so the container must return  a response in 1 second or the probe is\\ncounted as failed. The container is probed every 10 seconds ( period=10s ) and the\\ncontainer is restarted after the pr obe fails three consecutive times ( #failure=3 ). \\n These additional parameters can be cu stomized when defining the probe. For\\nexample, to set the initial delay, add the initialDelaySeconds  property to the live-\\nness probe as shown in the following listing.\\n   livenessProbe:          \\n     httpGet:                     path: /             Listing 4.3 A liveness probe with an initial delay: kubia-liveness-probe-initial-delay.yamlThe previous \\ncontainer terminated \\nwith an error and \\nexited with code 137.\\nThe container \\nhas been \\nrestarted once.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 120}, page_content='89 Keeping pods healthy\\n       port: 8080          \\n     initialDelaySeconds: 15   \\nIf you don’t set the initial delay, the prober will start probing the container as soon as\\nit starts, which usually leads to the probe failing, because the app isn’t ready to start\\nreceiving requests. If the number of failures exceeds the failure threshold, the con-tainer is restarted before it’s even able to start responding to requests properly. \\nTIP Always remember to set an initial de lay to account for your app’s startup\\ntime.\\nI’ve seen this on many occasions and us ers were confused why their container was\\nbeing restarted. But if they’d used kubectl  describe , they’d have seen that the con-\\ntainer terminated with exit code 137 or 143, telling them that the pod was terminated\\nexternally. Additionally, the listing of the pod’s events would show that the container\\nwas killed because of a failed liveness probe.  If you see this happening at pod startup,\\nit’s because you failed to set initialDelaySeconds  appropriately.\\nNOTE Exit code 137 signals that the process was killed by an external signal\\n(exit code is 128 + 9 (SIGKILL). Likewi se, exit code 143 corresponds to 128 +\\n15 (SIGTERM).\\n4.1.5 Creating effective liveness probes\\nFor pods running in production, you should  always define a liveness probe. Without\\none, Kubernetes has no way of knowing whethe r your app is still alive or not. As long\\nas the process is still running, Kubernetes will consider the container to be healthy. \\nWHAT A LIVENESS  PROBE  SHOULD  CHECK\\nYour simplistic liveness probe simply checks if the server is responding. While this may\\nseem overly simple, even a liveness probe lik e this does wonders, because it causes the\\ncontainer to be restarted if the web se rver running within the container stops\\nresponding to HTTP requests. Compared to having no liveness probe, this is a major\\nimprovement, and may be sufficient in most cases.\\n But for a better liveness check, you’d conf igure the probe to perform requests on a\\nspecific URL path ( /health , for example) and have the app perform an internal sta-\\ntus check of all the vital components runnin g inside the app to ensure none of them\\nhas died or is unresponsive. \\nTIP Make sure the /health  HTTP endpoint doesn’t require authentication;\\notherwise the probe will always fail, ca using your container to be restarted\\nindefinitely.\\nBe sure to check only the internals of th e app and nothing influenced by an external\\nfactor. For example, a frontend web server’s  liveness probe shouldn’t return a failure\\nwhen the server can’t connect to the backen d database. If the underlying cause is in\\nthe database itself, restarting the web se rver container will not fix the problem.Kubernetes will wait 15 seconds \\nbefore executing the first probe.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 121}, page_content='90 CHAPTER  4Replication and other controllers: deploying managed pods\\nBecause the liveness probe will fail again, you’ll end up with the container restarting\\nrepeatedly until the database  becomes accessible again. \\nKEEPING  PROBES  LIGHT\\nLiveness probes shouldn’t use too many co mputational resources and shouldn’t take\\ntoo long to complete. By default, the probes are executed relatively often and are\\nonly allowed one second to complete. Having  a probe that does heavy lifting can slow\\ndown your container considerably. Later in  the book, you’ll also learn about how to\\nlimit CPU time available to a container. Th e probe’s CPU time is counted in the con-\\ntainer’s CPU time quota, so having a heavyweight liveness probe will reduce the CPU\\ntime available to the main application processes.\\nTIP If you’re running a Java app in your container, be sure to use an HTTP\\nGET liveness probe instead of an Exec probe, where you spin up a whole new\\nJVM to get the liveness information. Th e same goes for any JVM-based or sim-\\nilar applications, whose start-up proced ure requires considerable computa-\\ntional resources.\\nDON’T BOTHER  IMPLEMENTING  RETRY  LOOPS  IN YOUR PROBES\\nYou’ve already seen that the failure thresh old for the probe is configurable and usu-\\nally the probe must fail multiple times before  the container is killed. But even if you\\nset the failure threshold to 1, Kubernetes wi ll retry the probe several times before con-\\nsidering it a single failed attempt. Theref ore, implementing your own retry loop into\\nthe probe is wasted effort.\\nLIVENESS  PROBE  WRAP -UP\\nYou now understand that Kubernetes keep s your containers running by restarting\\nthem if they crash or if their liveness probe s fail. This job is performed by the Kubelet\\non the node hosting the pod—the Kubernet es Control Plane components running on\\nthe master(s) have no part in this process. \\n But if the node itself crashes, it’s the Co ntrol Plane that must create replacements for\\nall the pods that went down with the node. It doesn’t do that for pods that you create\\ndirectly. Those pods aren’t managed by anythi ng except by the Kubelet, but because the\\nKubelet runs on the node itself, it can’t do anything if the node fails. \\n To make sure your app is restarted on another node, you need to have the pod\\nmanaged by a ReplicationController or simi lar mechanism, which we’ll discuss in the\\nrest of this chapter. \\n4.2 Introducing ReplicationControllers\\nA ReplicationController is a Kubernetes resource that ensures its pods are alwayskept running. If the pod disappears for an y reason, such as in the event of a node\\ndisappearing from the cluster or becaus e the pod was evicted from the node, the\\nReplicationController notices the missing  pod and creates a replacement pod. \\n Figure 4.1 shows what happens when a node  goes down and takes two pods with it.\\nPod A was created directly and is therefore an unmanaged pod, while pod B is managed'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 122}, page_content='91 Introducing ReplicationControllers\\nby a ReplicationController. After the node fails, the ReplicationController creates a\\nnew pod (pod B2) to replace the missing po d B, whereas pod A is lost completely—\\nnothing will ever recreate it.\\n The ReplicationController in the figure ma nages only a single pod, but Replication-\\nControllers, in general, are meant to create and manage multiple copies (replicas) of a\\npod. That’s where ReplicationControllers got their name from. \\n4.2.1 The operation of a ReplicationController\\nA ReplicationController constantly monitors  the list of running pods and makes sure\\nthe actual number of pods of a “type” al ways matches the desired number. If too few\\nsuch pods are running, it creates new replic as from a pod template. If too many such\\npods are running, it removes the excess replicas. \\n You might be wondering how there can be more than the desired number of repli-\\ncas. This can happen for a few reasons: \\n\\uf0a1Someone creates a pod of the same type manually.\\n\\uf0a1Someone changes an existing pod’s “type.”\\n\\uf0a1Someone decreases the desired number of pods, and so on.Node 1\\nNode 1 failsPod A\\nPod BNode 2\\nVarious\\nother pods\\nCreates and\\nmanagesNode 1\\nPod A\\nPod BNode 2\\nVarious\\nother pods\\nReplicationController ReplicationControllerPod A goes down with Node 1 and is\\nnot recreated, because there is noReplicationController overseeing it.\\nRC notices pod B is\\nmissing and createsa new pod instance.Pod B2\\nFigure 4.1 When a node fails, only pods backed by a ReplicationController are recreated.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 123}, page_content='92 CHAPTER  4Replication and other controllers: deploying managed pods\\nI’ve used the term pod “type” a few time s. But no such thing exists. Replication-\\nControllers don’t operate on pod types, but on  sets of pods that match a certain label\\nselector (you learned about th em in the previous chapter). \\nINTRODUCING  THE CONTROLLER ’S RECONCILIATION  LOOP\\nA ReplicationController’s job is to make su re that an exact number of pods always\\nmatches its label selector. If it doesn’t, th e ReplicationController takes the appropriate\\naction to reconcile the actual with the de sired number. The operation of a Replication-\\nController is shown in figure 4.2.\\nUNDERSTANDING  THE THREE  PARTS  OF A REPLICATION CONTROLLER\\nA ReplicationController has three essent ial parts (also shown in figure 4.3):\\n\\uf0a1A label selector , which determines what pods are in the ReplicationController’s scope\\n\\uf0a1A replica count , which specifies the desired number  of pods that should be running\\n\\uf0a1A pod template , which is used when creating new pod replicasStart\\nCompare\\nmatched vs.\\ndesired pod\\ncountFind pods\\nmatching the\\nlabel selector\\nCreate additional\\npod(s) from\\ncurrent templateDelete the\\nexcess pod(s)Too many\\nJust enoughToo few\\nFigure 4.2 A ReplicationController’s reconciliation loop\\napp: kubiaPodPod templateReplicationController: kubia\\nPod selector:\\napp=kubia\\nReplicas: 3 Figure 4.3 The three key parts of a \\nReplicationController (pod selector, replica count, and pod template)'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 124}, page_content='93 Introducing ReplicationControllers\\nA ReplicationController’s replica count, the label selector, and even the pod tem-\\nplate can all be modified at any time, bu t only changes to the replica count affect\\nexisting pods. \\nUNDERSTANDING  THE EFFECT  OF CHANGING  THE CONTROLLER ’S LABEL  SELECTOR  OR POD TEMPLATE\\nChanges to the label selector and the pod te mplate have no effect on existing pods.\\nChanging the label selector makes the existing pods fall out of the scope of the\\nReplicationController, so the controller stops caring about them. ReplicationCon-\\ntrollers also don’t care about the actual “c ontents” of its pods (the container images,\\nenvironment variables, and other things) after they create the pod. The template\\ntherefore only affects new pods created by this ReplicationController. You can think\\nof it as a cookie cutter for cutting out new pods.\\nUNDERSTANDING  THE BENEFITS  OF USING  A REPLICATION CONTROLLER\\nLike many things in Kubernetes, a ReplicationController, although an incredibly sim-ple concept, provides or enables the following powerful features:\\n\\uf0a1It makes sure a pod (or multiple pod rep licas) is always running by starting a\\nnew pod when an existing one goes missing.\\n\\uf0a1When a cluster node fails, it creates re placement replicas for all the pods that\\nwere running on the failed node (those that were under the Replication-\\nController’s control).\\n\\uf0a1It enables easy horizontal scaling of  pods—both manual and automatic (see\\nhorizontal pod auto-scaling in chapter 15).\\nNOTE A pod instance is never relocated to another node. Instead, the\\nReplicationController creates a completely  new pod instance that has no rela-\\ntion to the instance it’s replacing. \\n4.2.2 Creating a ReplicationController\\nLet’s look at how to create a Replicatio nController and then see how it keeps your\\npods running. Like pods and other Kubernet es resources, you create a Replication-\\nController by posting a JSON or YAML de scriptor to the Kubernetes API server.\\n You’re going to create a YAML file called kubia-rc.yaml for your Replication-\\nController, as shown in the following listing.\\napiVersion: v1\\nkind: ReplicationController     metadata:\\n  name: kubia                      \\nspec:  replicas: 3                     \\n  selector:              \\n    app: kubia           Listing 4.4 A YAML definition of a ReplicationController: kubia-rc.yaml\\nThis manifest defines a \\nReplicationController (RC)\\nThe name of this \\nReplicationController\\nThe desired number \\nof pod instances\\nThe pod selector determining \\nwhat pods the RC is operating on'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 125}, page_content='94 CHAPTER  4Replication and other controllers: deploying managed pods\\n  template:                        \\n    metadata:                      \\n      labels:                              app: kubia                 \\n    spec:                          \\n      containers:                        - name: kubia                \\n        image: luksa/kubia         \\n        ports:                     \\n        - containerPort: 8080      \\nWhen you post the file to the API server, Kubernetes creates a new Replication-\\nController named kubia , which makes sure three pod instances always match the\\nlabel selector app=kubia . When there aren’t enough po ds, new pods will be created\\nfrom the provided pod template. The contents  of the template are almost identical to\\nthe pod definition you created in the previous chapter. \\n The pod labels in the template must obv iously match the label selector of the\\nReplicationController; otherwise the contro ller would create new pods indefinitely,\\nbecause spinning up a new pod wouldn’t brin g the actual replica count any closer to\\nthe desired number of replicas. To prevent su ch scenarios, the API server verifies the\\nReplicationController definition and will  not accept it if it’s misconfigured.\\n Not specifying the selector at all is also an option. In that case, it will be configured\\nautomatically from the labels in the pod template. \\nTIP Don’t specify a pod selector when de fining a ReplicationController. Let\\nKubernetes extract it from the pod template. This will keep your YAML\\nshorter and simpler.\\nTo create the ReplicationController, use the kubectl  create  command, which you\\nalready know:\\n$ kubectl create -f kubia-rc.yaml\\nreplicationcontroller \"kubia\" created\\nAs soon as the ReplicationController is created, it goes to work. Let’s see what\\nit does.\\n4.2.3 Seeing the ReplicationController in action\\nBecause no pods exist with the app=kubia  label, the ReplicationController should\\nspin up three new pods from the pod template . List the pods to see if the Replication-\\nController has done what it’s supposed to:\\n$ kubectl get pods\\nNAME          READY     STATUS              RESTARTS   AGE\\nkubia-53thy   0/1       ContainerCreating   0          2s\\nkubia-k0xz6   0/1       ContainerCreating   0          2skubia-q3vkg   0/1       ContainerCreating   0          2sThe pod template \\nfor creating new \\npods'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 126}, page_content='95 Introducing ReplicationControllers\\nIndeed, it has! You wanted three pods, an d it created three pods. It’s now managing\\nthose three pods. Next you’ll mess with th em a little to see how the Replication-\\nController responds. \\nSEEING THE REPLICATION CONTROLLER  RESPOND  TO A DELETED  POD\\nFirst, you’ll delete one of the pods manually  to see how the ReplicationController spins\\nup a new one immediately, bringing the nu mber of matching pods back to three:\\n$ kubectl delete pod kubia-53thy\\npod \"kubia-53thy\" deleted\\nListing the pods again shows four of them , because the one you deleted is terminat-\\ning, and a new pod has already been created:\\n$ kubectl get pods\\nNAME          READY     STATUS              RESTARTS   AGE\\nkubia-53thy   1/1       Terminating         0          3m\\nkubia-oini2   0/1       ContainerCreating   0          2skubia-k0xz6   1/1       Running             0          3m\\nkubia-q3vkg   1/1       Running             0          3m\\nThe ReplicationController has done its job ag ain. It’s a nice little helper, isn’t it?\\nGETTING  INFORMATION  ABOUT  A REPLICATION CONTROLLER\\nNow, let’s see what information the kubectl  get command shows for Replication-\\nControllers:\\n$ kubectl get rc\\nNAME      DESIRED   CURRENT   READY     AGE\\nkubia     3         3         2         3m\\nNOTE We’re using rc as a shorthand for replicationcontroller .\\nYou see three columns showing the desired number of pods, the actual number of\\npods, and how many of them are ready (you’l l learn what that means in the next chap-\\nter, when we talk about readiness probes).\\n You can see additional information abou t your ReplicationController with the\\nkubectl  describe  command, as shown in the following listing.\\n$ kubectl describe rc kubia\\nName:           kubiaNamespace:      default\\nSelector:       app=kubia\\nLabels:         app=kubiaAnnotations:    <none>\\nReplicas:       3 current / 3 desired               \\nPods Status:    4 Running / 0 Waiting / 0 Succeeded / 0 Failed  Pod Template:\\n  Labels:       app=kubia\\n  Containers:   ...Listing 4.5 Displaying details of a ReplicationController with kubectl  describe\\nThe actual vs. the \\ndesired number of \\npod instances\\nNumber of \\npod instances \\nper pod status'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 127}, page_content='96 CHAPTER  4Replication and other controllers: deploying managed pods\\n  Volumes:      <none>\\nEvents:                                                   \\nFrom                    Type      Reason           Message----                    -------  ------            -------\\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-53thy\\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-k0xz6\\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-q3vkg\\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-oini2\\nThe current number of replicas matches the desired number, because the controller\\nhas already created a new pod. It shows four  running pods because a pod that’s termi-\\nnating is still considered running, although it  isn’t counted in the current replica count. \\n The list of events at the bottom shows the actions taken by the Replication-\\nController—it has created four pods so far.\\nUNDERSTANDING  EXACTLY  WHAT CAUSED  THE CONTROLLER  TO CREATE  A NEW POD\\nThe controller is responding to the deletion of a pod by creating a new replacementpod (see figure 4.4). Well, technically, it isn’ t responding to the deletion itself, but the\\nresulting state—the inadequate number of pods.\\n While a ReplicationController is immedi ately notified about a pod being deleted\\n(the API server allows clients to watch for ch anges to resources and resource lists), that’s\\nnot what causes it to create a replacement pod. The notification triggers the controller\\nto check the actual number of po ds and take appropriate action.The events \\nrelated to this \\nReplicationController\\nBefore deletion After deletion\\nReplicationController: kubia\\nReplicas: 3\\nSelector:app=kubiaapp: kubia\\nPod:\\nkubia-q3vkg\\napp: kubia\\nPod:\\nkubia-oini2\\n[ContainerCreating] [Terminating]app: kubia\\nPod:\\nkubia-k0xz6\\napp: kubia\\nPod:\\nkubia-53thy\\nReplicationController: kubia\\nReplicas: 3\\nSelector:app=kubiaapp: kubia\\nPod:\\nkubia-q3vkgapp: kubia\\nPod:\\nkubia-k0xz6\\napp: kubia\\nPod:\\nkubia-53thyDeletekubia-53thy\\nFigure 4.4 If a pod disappears, the ReplicationController sees too few pods and creat es a new replacement pod.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 128}, page_content=\"97 Introducing ReplicationControllers\\nRESPONDING  TO A NODE FAILURE\\nSeeing the ReplicationController respond to the manual deletion of a pod isn’t too\\ninteresting, so let’s look at a better exampl e. If you’re using Google Kubernetes Engine\\nto run these examples, you have a three-node  Kubernetes cluster. You’re going to dis-\\nconnect one of the nodes from the ne twork to simulate a node failure.\\nNOTE If you’re using Minikube, you can’t do this exercise, because you only\\nhave one node that acts both as  a master and a worker node.\\nIf a node fails in the non-Kubernetes worl d, the ops team would need to migrate the\\napplications running on that node to other machines manually. Kubernetes, on theother hand, does that automatically. Soon after the ReplicationController detects that\\nits pods are down, it will spin  up new pods to replace them. \\n Let’s see this in action. You need to \\nssh into one of the nodes with the gcloud\\ncompute  ssh command and then shut down  its network interface with sudo  ifconfig\\neth0  down , as shown in the following listing.\\nNOTE Choose a node that runs at least one of your pods by listing pods with\\nthe -o wide  option.\\n$ gcloud compute ssh gke-kubia-default-pool-b46381f1-zwko\\nEnter passphrase for key '/home/luksa/.ssh/google_compute_engine':\\nWelcome to Kubernetes v1.6.4!\\n...\\nluksa@gke-kubia-default-pool-b46381f1-zwko ~ $ sudo ifconfig eth0 down\\nWhen you shut down the network interface, the ssh session will stop responding, so\\nyou need to open up another te rminal or hard-exit from the ssh session. In the new\\nterminal you can list the nodes to see if Kubernetes has detected that the node is\\ndown. This takes a minute or so. Th en, the node’s status is shown as NotReady :\\n$ kubectl get node\\nNAME                                   STATUS     AGEgke-kubia-default-pool-b46381f1-opc5   Ready      5h\\ngke-kubia-default-pool-b46381f1-s8gj   Ready      5h\\ngke-kubia-default-pool-b46381f1-zwko   NotReady   5h    \\nIf you list the pods now, yo u’ll still see the same three pods as before, because Kuber-\\nnetes waits a while before rescheduling pods  (in case the node is unreachable because\\nof a temporary network glitch or because the Kubelet is restarting). If the node stays\\nunreachable for several minutes, the status of the pods that were scheduled to that\\nnode changes to Unknown . At that point, the ReplicationController will immediately\\nspin up a new pod. You can see this by listing the pods again:Listing 4.6 Simulating a node failure by  shutting down its network interface\\nNode isn’t ready, \\nbecause it’s \\ndisconnected from \\nthe network\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 129}, page_content='98 CHAPTER  4Replication and other controllers: deploying managed pods\\n$ kubectl get pods\\nNAME          READY   STATUS    RESTARTS   AGE\\nkubia-oini2   1/1     Running   0          10mkubia-k0xz6   1/1     Running   0          10m\\nkubia-q3vkg   1/1     Unknown   0          10m    \\nkubia-dmdck   1/1     Running   0          5s    \\nLooking at the age of the pods, you see that the kubia-dmdck  pod is new. You again\\nhave three pod instances running, which means the ReplicationController has again\\ndone its job of bringing the actual stat e of the system to the desired state. \\n The same thing happens if a node fails (either breaks down or becomes unreach-\\nable). No immediate human intervention  is necessary. The system heals itself\\nautomatically. \\n To bring the node back, you need to  reset it with the following command:\\n$ gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko\\nWhen the node boots up again, its status should return to Ready , and the pod whose\\nstatus was Unknown  will be deleted.\\n4.2.4 Moving pods in and out of the scope of a ReplicationController\\nPods created by a ReplicationController ar en’t tied to the ReplicationController in\\nany way. At any moment, a ReplicationContr oller manages pods that match its label\\nselector. By changing a pod’s labels, it ca n be removed from or added to the scope\\nof a ReplicationController. It can even be moved from one ReplicationController to\\nanother.\\nTIP Although a pod isn’t tied to a Replic ationController, the pod does refer-\\nence it in the metadata.ownerReferences  field, which you can use to easily\\nfind which ReplicationCon troller a pod belongs to.\\nIf you change a pod’s labels so they no longer match a ReplicationController’s label\\nselector, the pod becomes like  any other manually created pod. It’s no longer man-\\naged by anything. If the node running the pod fails, the pod is obviously not resched-\\nuled. But keep in mind that when you changed the pod’s labels, the replication\\ncontroller noticed one pod was missing and spun up a new pod to replace it.\\n Let’s try this with your pods. Becaus e your ReplicationController manages pods\\nthat have the app=kubia  label, you need to either remove this label or change its value\\nto move the pod out of the ReplicationCon troller’s scope. Adding another label will\\nhave no effect, because the ReplicationContr oller doesn’t care if the pod has any addi-\\ntional labels. It only cares whether the pod has all the labels referenced in the label\\nselector. This pod’s status is \\nunknown, because its \\nnode is unreachable.\\nThis pod was created \\nfive seconds ago.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 130}, page_content='99 Introducing ReplicationControllers\\nADDING  LABELS  TO PODS MANAGED  BY A REPLICATION CONTROLLER\\nLet’s confirm that a ReplicationController do esn’t care if you add additional labels to\\nits managed pods:\\n$ kubectl label pod kubia-dmdck type=special\\npod \"kubia-dmdck\" labeled\\n$ kubectl get pods --show-labels\\nNAME          READY   STATUS    RESTARTS   AGE   LABELS\\nkubia-oini2   1/1     Running   0          11m   app=kubia\\nkubia-k0xz6   1/1     Running   0          11m   app=kubia\\nkubia-dmdck   1/1     Running   0          1m    app=kubia,type=special\\nYou’ve added the type=special  label to one of the pods. Listing all pods again shows\\nthe same three pods as before, because no change occurred as far as the Replication-\\nController is concerned.\\nCHANGING  THE LABELS  OF A MANAGED  POD\\nNow, you’ll change the app=kubia  label to something else. This will make the pod no\\nlonger match the ReplicationController’s labe l selector, leaving it to only match two\\npods. The ReplicationController should th erefore start a new pod to bring the num-\\nber back to three:\\n$ kubectl label pod kubia-dmdck app=foo --overwrite\\npod \"kubia-dmdck\" labeled\\nThe --overwrite  argument is necessary; otherwise kubectl  will only print out a warn-\\ning and won’t change the label, to prevent you from inadvertently changing an exist-\\ning label’s value when your intent is to add a new one. \\n Listing all the pods again should now show four pods: \\n$ kubectl get pods -L app\\nNAME         READY  STATUS             RESTARTS  AGE  APP\\nkubia-2qneh  0/1    ContainerCreating  0         2s   kubia   \\nkubia-oini2  1/1    Running            0         20m  kubiakubia-k0xz6  1/1    Running            0         20m  kubia\\nkubia-dmdck  1/1    Running            0         10m  foo    \\nNOTE You’re using the -L app option to display the app label in a column.\\nThere, you now have four pods altogether: one that isn’t managed by your Replication-\\nController and three that are. Am ong them is the newly created pod.\\n Figure 4.5 illustrates what happened when you changed the pod’s labels so they no\\nlonger matched the ReplicationController’s pod selector. You can see your three pods\\nand your ReplicationController. Afte r you change the pod’s label from app=kubia  to\\napp=foo , the ReplicationController no longer cares about the pod. Because the con-\\ntroller’s replica count is set to 3 and on ly two pods match the label selector, theNewly created pod that replaces\\nthe pod you removed from the\\nscope of the ReplicationController\\nPod no longer \\nmanaged by the \\nReplicationController'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 131}, page_content='100 CHAPTER  4Replication and other controllers: deploying managed pods\\nReplicationController spins up pod kubia-2qneh  to bring the number back up to\\nthree. Pod kubia-dmdck  is now completely independent and will keep running until\\nyou delete it manually (you can do that now, because you don’t need it anymore).\\nREMOVING  PODS FROM CONTROLLERS  IN PRACTICE\\nRemoving a pod from the scope of the Re plicationController comes in handy when\\nyou want to perform actions on a specific  pod. For example, you might have a bug\\nthat causes your pod to start behaving badl y after a specific amount of time or a spe-\\ncific event. If you know a pod is malfunctio ning, you can take it out of the Replication-\\nController’s scope, let the controller repl ace it with a new one, and then debug or\\nplay with the pod in any way you want. Once you’re done, you delete the pod. \\nCHANGING  THE REPLICATION CONTROLLER ’S LABEL  SELECTOR\\nAs an exercise to see if you fully unders tand ReplicationControllers, what do you\\nthink would happen if instead of changing  the labels of a pod, you modified the\\nReplicationController’s label selector? \\n I f  y o u r  a n s w e r  i s  t h a t  i t  w o u l d  m a k e  all the pods fall out of the scope of the\\nReplicationController, which would result in  it creating three new pods, you’re abso-\\nlutely right. And it shows that you unde rstand how ReplicationControllers work. \\n Kubernetes does allow you to change a ReplicationController’s label selector, but\\nthat’s not the case for the other resources th at are covered in the second half of thisInitial state After re-labelling\\nRe-labelkubia-dmdck\\napp: kubia\\nPod:\\nkubia-oini2\\napp: kubia\\nPod:\\nkubia-2qneh\\n[ContainerCreating]Pod:\\nkubia-dmdck\\napp: kubia\\nPod:\\nkubia-k0xz6app: kubia type: special type: specialapp: foo app: kubia\\nPod:\\nkubia-dmdck\\napp: kubia\\nPod:\\nkubia-k0xz6\\nReplicationController: kubia\\nReplicas: 3\\nSelector:app=kubiaReplicationController: kubia\\nReplicas: 3\\nSelector:app=kubiaPod:\\nkubia-oini2\\nFigure 4.5 Removing a pod from the scope of a ReplicationController by changing its labels'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 132}, page_content='101 Introducing ReplicationControllers\\nchapter and which are also used for managing  pods. You’ll never change a controller’s\\nlabel selector, but you’ll regularly change it s pod template. Let’s take a look at that.\\n4.2.5 Changing the pod template\\nA ReplicationController’s pod template can be  modified at any time. Changing the pod\\ntemplate is like replacing a cookie cutter with another one. It will only affect the cookies\\nyou cut out afterward and will have no effect on the ones you’ve already cut (see figure\\n4.6). To modify the old pods, you’d need to delete them and let the Replication-\\nController replace them with new ones based on the new template.\\nAs an exercise, you can try editing the ReplicationController and adding a label to the\\npod template. You can edit the Replicatio nController with the following command:\\n$ kubectl edit rc kubia\\nThis will open the ReplicationController’s YAML  definition in your default text editor.\\nFind the pod template section and add an additional label to the metadata. After you\\nsave your changes and exit the editor, kubectl  will update the ReplicationController\\nand print the following message:\\nreplicationcontroller \"kubia\" edited\\nYou can now list pods and their labels agai n and confirm that they haven’t changed.\\nBut if you delete the pods and wait for thei r replacements to be created, you’ll see the\\nnew label.\\n Editing a ReplicationController like this to change the container image in the pod\\ntemplate, deleting the existing  pods, and letting them be replaced with new ones from\\nthe new template could be used for upgradi ng pods, but you’ll learn a better way of\\ndoing that in chapter 9. Replication\\nControllerReplicas: 3\\nTemplate:\\nA B CReplication\\nControllerReplicas: 3\\nTemplate:\\nAReplication\\nControllerReplicas: 3\\nTemplate:\\nAReplication\\nControllerReplicas: 3\\nTemplate:\\nD A B C A B C A BChange\\ntemplateDelete\\na podRC creates\\nnew pod\\nFigure 4.6 Changing a ReplicationController’s pod template  only affects pods created afterward and has no \\neffect on existing pods.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 133}, page_content='102 CHAPTER  4Replication and other controllers: deploying managed pods\\n4.2.6 Horizontally scaling pods\\nYou’ve seen how ReplicationControllers make  sure a specific number of pod instances\\nis always running. Because it’s incredibly si mple to change the desired number of rep-\\nlicas, this also means scaling pods horizontally is trivial. \\n Scaling the number of pods up or down is as easy as changing the value of the rep-\\nlicas field in the ReplicationController re source. After the change, the Replication-\\nController will either see too many pods ex ist (when scaling down) and delete part of\\nthem, or see too few of them (when sc aling up) and create additional pods. \\nSCALING  UP A REPLICATION CONTROLLER\\nYour ReplicationController has been keep ing three instances of your pod running.\\nYou’re going to scale that number up to 10 now. As you may remember, you’ve\\nalready scaled a ReplicationController in chapter 2. You could use the same com-mand as before:\\n$ kubectl scale rc kubia --replicas=10\\nBut you’ll do it di fferently this time. \\nSCALING  A REPLICATION CONTROLLER  BY EDITING  ITS DEFINITION\\nInstead of using the kubectl  scale  command, you’re going to scale it in a declarative\\nway by editing the ReplicationController’s definition:\\n$ kubectl edit rc kubia\\nWhen the text editor opens, find the spec.replicas  field and change its value to 10,\\nas shown in the following listing.\\n# Please edit the object below. Lines beginning with a \\'#\\' will be ignored,\\n# and an empty file will abort the edit. If an error occurs while saving \\n# this file will be reopened with the relevant failures.apiVersion: v1\\nkind: ReplicationControllerConfiguring kubectl edit to use a different text editor\\nYou can tell kubectl  to use a text editor of your choice by setting the KUBE_EDITOR\\nenvironment variable. For example, if you’d like to use nano for editing Kubernetes\\nresources, execute the following command (or put it into your ~/.bashrc  or an\\nequivalent file):\\nexport KUBE_EDITOR=\"/usr/bin/nano\"\\nIf the KUBE_EDITOR  environment variable isn’t set, kubectl  edit falls back to using\\nthe default editor, usually configured through the EDITOR  environment variable.\\nListing 4.7 Editing the RC in a text editor by running kubectl  edit'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 134}, page_content='103 Introducing ReplicationControllers\\nmetadata:\\n  ...\\nspec:  replicas: 3        \\n  selector:\\n    app: kubia  ...\\nWhen you save the file and close the editor, the ReplicationController is updated and\\nit immediately scales the number of pods to 10:\\n$ kubectl get rc\\nNAME      DESIRED   CURRENT   READY     AGEkubia     10        10        4         21m\\nThere you go. If the kubectl  scale  command makes it look as though you’re telling\\nKubernetes exactly what to do, it’s now much  clearer that you’re making a declarative\\nchange to the desired state of the Replicat ionController and not telling Kubernetes to\\ndo something.\\nSCALING  DOWN  WITH THE KUBECTL  SCALE  COMMAND\\nNow scale back down to 3. You can use the kubectl  scale  command:\\n$ kubectl scale rc kubia --replicas=3\\nAll this command does is modify the spec.replicas  field of the ReplicationController’s\\ndefinition—like when you changed it through kubectl  edit . \\nUNDERSTANDING  THE DECLARATIVE  APPROACH  TO SCALING\\nHorizontally scaling pods in Kubernetes is a matter of stating your desire: “I want to\\nhave x number of instances running.” You’re not telling Kubernetes what or how to do\\nit. You’re just specifying the desired state. \\n This declarative approach makes interact ing with a Kubernetes cluster easy. Imag-\\nine if you had to manually determine the current number of running instances and\\nthen explicitly tell Kubernetes how many additional instances to run. That’s more\\nwork and is much more error-prone. Changi ng a simple number is much easier, and\\ni n  c h a p t e r  1 5 ,  y o u ’ l l  l e a r n  t h a t  e v e n  t h a t  c a n  b e  d o n e  b y  K u b e r n e t e s  i t s e l f  i f  y o u\\nenable horizontal pod auto-scaling. \\n4.2.7 Deleting a ReplicationController\\nWhen you delete a ReplicationController through kubectl  delete , the pods are also\\ndeleted. But because pods created by a Rep licationController aren’t an integral part\\nof the ReplicationController, and are only  managed by it, you can delete only the\\nReplicationController and leave the pods  running, as shown in figure 4.7.\\n This may be useful when you initially ha ve a set of pods managed by a Replication-\\nController, and then decide to replace th e ReplicationController with a ReplicaSet,\\nfor example (you’ll learn about them next.) . You can do this without affecting theChange the number 3 \\nto number 10 in \\nthis line.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 135}, page_content='104 CHAPTER  4Replication and other controllers: deploying managed pods\\npods and keep them runnin g without interruption while you replace the Replication-\\nController that manages them. \\n When deleting a ReplicationController with kubectl  delete , you can keep its\\npods running by passing the --cascade=false  option to the command. Try that now:\\n$ kubectl delete rc kubia --cascade=false\\nreplicationcontroller \"kubia\" deleted\\nYou’ve deleted the ReplicationController so  the pods are on their own. They are no\\nlonger managed. But you can always crea te a new ReplicationController with the\\nproper label selector and make them managed again.\\n4.3 Using ReplicaSets instead of ReplicationControllers\\nInitially, ReplicationControllers were the only Kubernetes component for replicating\\npods and rescheduling them when nodes failed. Later, a similar resource called a\\nReplicaSet was introduced. It’s a new generation of ReplicationController andreplaces it completely (ReplicationContr ollers will eventually be deprecated). \\n You could have started this chapter by creating a ReplicaSet instead of a Replication-\\nController, but I felt it would be a good idea  to start with what was initially available in\\nKubernetes. Plus, you’ll still see ReplicationC ontrollers used in the wild, so it’s good\\nfor you to know about them. That said, you should always create ReplicaSets insteadof ReplicationControllers from now on. Th ey’re almost identical, so you shouldn’t\\nhave any trouble using them instead. Before the RC deletion After the RC deletion\\nDelete RC\\nPod:\\nkubia-q3vkg\\nPod:kubia-53thyPod:kubia-k0xz6 Pod:kubia-q3vkg\\nPod:kubia-53thyPod:kubia-k0xz6\\nReplicationController: kubia\\nReplicas: 3\\nSelector:app=kubiaapp: kubia app: kubia\\napp: kubiaapp: kubia app: kubia\\napp: kubia\\nFigure 4.7 Deleting a replication controller with --cascade=false  leaves pods unmanaged.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 136}, page_content='105 Using ReplicaSets instead of ReplicationControllers\\n You usually won’t create them directly, but instead have them created automati-\\ncally when you create the higher-level Depl oyment resource, which you’ll learn about\\nin chapter 9. In any case, you should unders tand ReplicaSets, so let’s see how they dif-\\nfer from ReplicationControllers.\\n4.3.1 Comparing a ReplicaSet to a ReplicationController\\nA ReplicaSet behaves exactly like a Replicat ionController, but it has more expressive\\npod selectors. Whereas a ReplicationControl ler’s label selector only allows matching\\npods that include a certain label, a Replic aSet’s selector also allows matching pods\\nthat lack a certain label or pods that include a certain label key, regardless of\\nits value.\\n Also, for example, a single ReplicationC ontroller can’t match pods with the label\\nenv=production  and those with the label env=devel  at the same time. It can only match\\neither pods with the env=production  label or pods with the env=devel  label. But a sin-\\ngle ReplicaSet can match both sets of po ds and treat them as a single group. \\n Similarly, a ReplicationController can’t match pods based merely on the presence\\nof a label key, regardless of its value, wher eas a ReplicaSet can. For example, a Replica-\\nSet can match all pods that include a label with the key env, whatever its actual value is\\n(you can think of it as env=* ).\\n4.3.2 Defining a ReplicaSet\\nYou’re going to create a ReplicaSet now to  see how the orphaned pods that were cre-\\nated by your ReplicationController and th en abandoned earlier can now be adopted\\nby a ReplicaSet. First, you’ll rewrite your  ReplicationController into a ReplicaSet by\\ncreating a new file called kubia-replicaset .yaml with the contents in the following\\nlisting.\\napiVersion: apps/v1beta2      \\nkind: ReplicaSet                    \\nmetadata:  name: kubia\\nspec:\\n  replicas: 3  selector:\\n    matchLabels:                 \\n      app: kubia                   template:                        \\n    metadata:                      \\n      labels:                              app: kubia                 \\n    spec:                          \\n      containers:                        - name: kubia                \\n        image: luksa/kubia         Listing 4.8 A YAML definition of a ReplicaSet: kubia-replicaset.yaml\\nReplicaSets aren’t part of the v 1 \\nAPI, but belong to the apps API \\ngroup and version v 1beta2.\\nYou’re using the simpler matchLabels \\nselector here, which is much like a \\nReplicationController’s selector.\\nThe template is \\nthe same as in the ReplicationController.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 137}, page_content='106 CHAPTER  4Replication and other controllers: deploying managed pods\\nThe first thing to note is that ReplicaSets aren’t part of the v1 API, so you need to\\nensure you specify the proper apiVersion  when creating the resource. You’re creating a\\nresource of type ReplicaSet which has much the same contents as the Replication-\\nController you created earlier. \\n The only difference is in the selector. Instead of listing labels the pods need to\\nhave directly under the selector  property, you’re specifying them under selector\\n.matchLabels . This is the simpler (and less expressive) way of defining label selectors\\nin a ReplicaSet. Later, you’ll look at  the more expressive option, as well.\\nBecause you still have three pods matching the app=kubia  selector running from ear-\\nlier, creating this ReplicaSet will not caus e any new pods to be created. The ReplicaSet\\nwill take those existing three pods under its wing. \\n4.3.3 Creating and examining a ReplicaSet\\nCreate the ReplicaSet from the YAML file with the kubectl  create  command. After\\nthat, you can examine the ReplicaSet with kubectl  get and kubectl  describe :\\n$ kubectl get rs\\nNAME      DESIRED   CURRENT   READY     AGE\\nkubia     3         3         3         3s\\nTIP Use rs shorthand, which stands for replicaset .\\n$ kubectl describe rsName:           kubiaNamespace:      default\\nSelector:       app=kubia\\nLabels:         app=kubiaAnnotations:    <none>\\nReplicas:       3 current / 3 desired\\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 FailedPod Template:\\n  Labels:       app=kubiaAbout the API version attribute\\nThis is your first oppo rtunity to see that the apiVersion  property specifies two things:\\n\\uf0a1The API group (which is apps in this case)\\n\\uf0a1The actual API version ( v1beta2 )\\nYou’ll see throughout the book that certain Kubernetes resources are in what’s called\\nthe core API group, which does n’t need to be specified in the apiVersion  field (you\\njust specify the version—for example, you’ve been using apiVersion:  v1 when\\ndefining Pod resources). Other resources, which were introduced in later Kubernetes\\nversions, are categorized into several API  groups. Look at the inside of the book’s\\ncovers to see all resources and their respective API groups.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 138}, page_content='107 Using ReplicaSets instead of ReplicationControllers\\n  Containers:   ...\\n  Volumes:      <none>\\nEvents:         <none>\\nAs you can see, the ReplicaSet isn’t any di fferent from a ReplicationController. It’s\\nshowing it has three replicas matching the se lector. If you list all the pods, you’ll see\\nthey’re still the same three pods you had be fore. The ReplicaSet didn’t create any new\\nones. \\n4.3.4 Using the ReplicaSet’s more expressive label selectors\\nThe main improvements of ReplicaSets ov er ReplicationControllers are their more\\nexpressive label selectors. You intentionally used the simpler matchLabels  selector in\\nthe first ReplicaSet example to see that Re plicaSets are no different from Replication-\\nControllers. Now, you’ll rewrite the selector to use the more powerful matchExpressions\\nproperty, as shown in the following listing.\\n selector:\\n   matchExpressions:                 \\n     - key: app                  operator: In                  \\n       values:                       \\n         - kubia                     \\nNOTE Only the selector is shown. You’ll find the whole ReplicaSet definition\\nin the book’s code archive.\\nYou can add additional expressions to the se lector. As in the example, each expression\\nmust contain a key, an operator , and possibly (depending on the operator) a list of\\nvalues . You’ll see four valid operators:\\n\\uf0a1In—Label’s value must match one of the specified values .\\n\\uf0a1NotIn— Label’s value must not match any of the specified values .\\n\\uf0a1Exists— Pod must include a label with the specified key (the value isn’t import-\\nant). When using this operat or, you shouldn’t specify the values  field.\\n\\uf0a1DoesNotExist— Pod must not include a label with the specified key. The values\\nproperty must not be specified.\\nIf you specify multiple expressions, all thos e expressions must evaluate to true for the\\nselector to match a pod. If you specify both matchLabels  and matchExpressions , all\\nthe labels must match and all the expressi ons must evaluate to true for the pod to\\nmatch the selector.Listing 4.9 A matchExpressions  selector: kubia-repli caset-matchexpressions.yaml\\nThis selector requires the pod to \\ncontain a label with the “app” key.\\nThe label’s value \\nmust be “kubia”.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 139}, page_content='108 CHAPTER  4Replication and other controllers: deploying managed pods\\n4.3.5 Wrapping up ReplicaSets\\nThis was a quick introduction to ReplicaSets as an alternative to ReplicationControllers.\\nRemember, always use them instead of Rep licationControllers, but you may still find\\nReplicationControllers in other people’s deployments.\\n Now, delete the ReplicaSet to clean up your cluster a little. You can delete the\\nReplicaSet the same way you’d delete a ReplicationController:\\n$ kubectl delete rs kubia\\nreplicaset \"kubia\" deleted\\nDeleting the ReplicaSet should delete all the pods. List the pods to confirm that’s\\nthe case. \\n4.4 Running exactly one pod on each node with \\nDaemonSets\\nBoth ReplicationControllers and ReplicaSet s are used for running a specific number\\nof pods deployed anywhere in the Kubernetes  cluster. But certain cases exist when you\\nwant a pod to run on each and every node in  the cluster (and each node needs to run\\nexactly one instance of the pod, as shown in figure 4.8).\\n Those cases include infrastructure-related  pods that perform system-level opera-\\ntions. For example, you’ll wa nt to run a log collector and a resource monitor on every\\nnode. Another good example is Kubernetes ’ own kube-proxy process, which needs to\\nrun on all nodes to make services work.\\nNode 1\\nPod\\nPod\\nPod\\nReplicaSet\\nReplicas: 5Node 2\\nPod\\nPodNode 3\\nPod\\nDaemonSet\\nExactly one replica\\non each nodeNode 4\\nPod\\nPodPod\\nFigure 4.8 DaemonSets run only a single pod replica on each node, whereas ReplicaSets \\nscatter them around the whole cluster randomly.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 140}, page_content='109 Running exactly one pod on each node with DaemonSets\\nOutside of Kubernetes, such processes woul d usually be started through system init\\nscripts or the systemd daemon during node  boot up. On Kubernetes nodes, you can\\nstill use systemd to run your system processes, but then you can’t take advantage of all\\nthe features Kubernetes provides. \\n4.4.1 Using a DaemonSet to run a pod on every node\\nTo run a pod on all cluster nodes, you create a DaemonSet object, which is muchlike a ReplicationController or a ReplicaSet , except that pods created by a Daemon-\\nSet already have a target node specified and skip the Kubernetes Scheduler. They\\naren’t scattered around the cluster randomly. \\n A DaemonSet makes sure it creates as ma ny pods as there are nodes and deploys\\neach one on its own node, as shown in figure 4.8.\\n Whereas a ReplicaSet (or ReplicationCon troller) makes sure that a desired num-\\nber of pod replicas exist in the cluster, a DaemonSet doesn’t have any notion of a\\ndesired replica count. It doesn’t need it be cause its job is to ensure that a pod match-\\ning its pod selector is running on each node. \\n If a node goes down, the DaemonSet do esn’t cause the pod to be created else-\\nwhere. But when a new node is added to the cluster, the DaemonSet immediately\\ndeploys a new pod instance to it. It also does the same if someone inadvertently\\ndeletes one of the pods, leaving the node wi thout the DaemonSet’s pod. Like a Replica-\\nSet, a DaemonSet creates the pod from the pod template configured in it.\\n4.4.2 Using a DaemonSet to run pods only on certain nodes\\nA DaemonSet deploys pods to all nodes in th e cluster, unless you specify that the pods\\nsh o ul d  on ly r u n  on  a su bse t of  al l t h e n od es.  T h i s is do n e b y sp ec i fyi n g  th e  node-\\nSelector  property in the pod template, which is part of the DaemonSet definition\\n(similar to the pod template in a Re plicaSet or ReplicationController). \\n You’ve already used node selectors to deploy a pod onto specific nodes in chapter 3.\\nA node selector in a DaemonSet is simila r—it defines the nodes the DaemonSet must\\ndeploy its pods to. \\nNOTE Later in the book, you’ll learn that  nodes can be made unschedulable,\\npreventing pods from being deployed to them. A DaemonSet will deploy podseven to such nodes, because the unschedulable attribute is only used by theScheduler, whereas pods managed by a DaemonSet bypass the Schedulercompletely. This is usually desirable, because DaemonSets are meant to runsystem services, which usually need to run even on unschedulable nodes.\\nEXPLAINING  DAEMON SETS WITH AN EXAMPLE\\nLet’s imagine having a daemon called ssd-monitor  that needs to run on all nodes\\nthat contain a solid-state dr ive (SSD). You’ll create a Da emonSet that runs this dae-\\nmon on all nodes that are marked as having an SSD. The cluster administrators haveadded the \\ndisk=ssd  label to all such nodes, so yo u’ll create the DaemonSet with a\\nnode selector that only selects nodes wi th that label, as shown in figure 4.9.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 141}, page_content='110 CHAPTER  4Replication and other controllers: deploying managed pods\\nCREATING  A DAEMON SET YAML DEFINITION\\nYou’ll create a DaemonSet that runs a mock ssd-monitor  process, which prints\\n“SSD OK” to the standard output every five  seconds. I’ve already prepared the mock\\ncontainer image and pushed it to Docker Hu b, so you can use it instead of building\\nyour own. Create the YAML for the DaemonSet, as shown in the following listing.\\napiVersion: apps/v1beta2      \\nkind: DaemonSet                     \\nmetadata:\\n  name: ssd-monitorspec:                            \\n  selector:\\n    matchLabels:      app: ssd-monitor\\n  template:\\n    metadata:      labels:\\n        app: ssd-monitor\\n    spec:      nodeSelector:                \\n        disk: ssd                  \\n      containers:      - name: main\\n        image: luksa/ssd-monitor\\nYou’re defining a DaemonSet that will run a pod with a single container based on the\\nluksa/ssd-monitor  container image. An instance of this pod will be created for each\\nnode that has the disk=ssd  label.Listing 4.10 A YAML for a Daem onSet: ssd-monitor-daemonset.yamlNode 1\\nPod:\\nssd-monitorNode 2 Node 3\\nDaemonSet:\\nsssd-monitor\\nNode selector:\\ndisk=ssdNode 4\\ndisk: ssd disk: ssd disk: ssd\\nUnschedulable\\nPod:ssd-monitor Pod:ssd-monitor\\nFigure 4.9 Using a DaemonSet with a node selector to deploy system pods only on certain \\nnodes\\nDaemonSets are in the \\napps API group, \\nversion v1beta2.\\nThe pod template includes a \\nnode selector, which selects \\nnodes with the disk=ssd label.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 142}, page_content='111 Running exactly one pod on each node with DaemonSets\\nCREATING  THE DAEMON SET\\nYou’ll create the DaemonSet like you always  create resources from a YAML file:\\n$ kubectl create -f ssd-monitor-daemonset.yaml\\ndaemonset \"ssd-monitor\" created\\nLet’s see the created DaemonSet:\\n$ kubectl get dsNAME          DESIRED  CURRENT   READY  UP-TO-DATE  AVAILABLE  NODE-SELECTOR  \\nssd-monitor   0        0        0      0           0          disk=ssd\\nThose zeroes look strange. Didn’t the Da emonSet deploy any pods? List the pods:\\n$ kubectl get po\\nNo resources found.\\nWhere are the pods? Do you know what’s goin g on? Yes, you forgot to label your nodes\\nwith the disk=ssd  label. No problem—you can do that now. The DaemonSet should\\ndetect that the nodes’ labels have change d and deploy the pod to all nodes with a\\nmatching label. Let’s see if that’s true. \\nADDING  THE REQUIRED  LABEL  TO YOUR NODE(S)\\nRegardless if you’re using Minikube, GKE, or another multi-node cluster, you’ll need\\nto list the nodes first, becaus e you’ll need to know the no de’s name when labeling it:\\n$ kubectl get node\\nNAME       STATUS    AGE       VERSION\\nminikube   Ready     4d        v1.6.0\\nNow, add the disk=ssd  label to one of your nodes like this:\\n$ kubectl label node minikube disk=ssdnode \"minikube\" labeled\\nNOTE Replace minikube  with the name of one of your nodes if you’re not\\nusing Minikube.\\nThe DaemonSet should have created one pod now. Let’s see:\\n$ kubectl get po\\nNAME                READY     STATUS    RESTARTS   AGE\\nssd-monitor-hgxwq   1/1       Running   0          35s\\nOkay; so far so good. If you have multiple nodes and you add the same label to further\\nnodes, you’ll see the DaemonSet spin up pods for each of them. \\nREMOVING  THE REQUIRED  LABEL  FROM THE NODE\\nNow, imagine you’ve made a mistake and have  mislabeled one of the nodes. It has a\\nspinning disk drive, not an SSD. What happens if you change the node’s label?\\n$ kubectl label node minikube disk=hdd --overwrite\\nnode \"minikube\" labeled'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 143}, page_content='112 CHAPTER  4Replication and other controllers: deploying managed pods\\nLet’s see if the change has any effect on  the pod that was running on that node:\\n$ kubectl get po\\nNAME                READY     STATUS        RESTARTS   AGEssd-monitor-hgxwq   1/1       Terminating   0          4m\\nThe pod is being terminated. But you knew  that was going to happen, right? This\\nwraps up your exploration of DaemonSets , so you may want to delete your ssd-monitor\\nDaemonSet. If you still have any other daem on pods running, you’ll see that deleting\\nthe DaemonSet deletes those pods as well. \\n4.5 Running pods that perform a single completable task \\nUp to now, we’ve only talked about pods than need to run continuously. You’ll have\\ncases where you only want to run a task that terminates after completing its work.\\nReplicationControllers, ReplicaSets, and Da emonSets run continuous tasks that are\\nnever considered completed. Processes in su ch pods are restarted when they exit. But\\nin a completable task, after its process term inates, it should not be restarted again. \\n4.5.1 Introducing the Job resource\\nKubernetes includes support for this through the Job resource, which is similar to the\\nother resources we’ve discussed in this chap ter, but it allows you to run a pod whose\\ncontainer isn’t restarted when the process ru nning inside finishes successfully. Once it\\ndoes, the pod is considered complete. \\n In the event of a node failure, the pods on  that node that are managed by a Job will\\nbe rescheduled to other nodes the way ReplicaS et pods are. In the event of a failure of\\nthe process itself (when the process returns an  error exit code), the Job can be config-\\nured to either restart the container or not.\\n Figure 4.10 shows how a pod created by a Job is rescheduled to a new node if the\\nnode it was initially scheduled to fails. The figure also shows both a managed pod,\\nwhich isn’t rescheduled, and a pod ba cked by a ReplicaSet, which is.\\n For example, Jobs are useful for ad hoc ta sks, where it’s crucial that the task fin-\\nishes properly. You could run the task in an unmanaged pod and wait for it to finish,but in the event of a node failing or the po d being evicted from the node while it is\\nperforming its task, you’d need to manually  recreate it. Doing this manually doesn’t\\nmake sense—especially if the job takes hours to complete. \\n An example of such a job would be if  you had data stored somewhere and you\\nneeded to transform and export it somewher e. You’re going to emulate this by run-\\nning a container image built on top of the \\nbusybox  image, which invokes the sleep\\ncommand for two minutes. I’ve already built  the image and pushed it to Docker Hub,\\nbut you can peek into its Dockerf ile in the book’s code archive.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 144}, page_content='113 Running pods that perform a single completable task\\n4.5.2 Defining a Job resource\\nCreate the Job manifest as in the following listing.\\napiVersion: batch/v1        \\nkind: Job                   \\nmetadata:\\n  name: batch-jobspec:                                \\n  template: \\n    metadata:      labels:                        \\n        app: batch-job               \\n    spec:      restartPolicy: OnFailure         \\n      containers:\\n      - name: main        image: luksa/batch-job\\nJ o b s  a r e  p a r t  o f  t h e  batch  API group and v1 API version. The YAML defines a\\nresource of type Job that will run the luksa/batch-job  image, which invokes a pro-\\ncess that runs for exactly 1 20 seconds and then exits. \\n In a pod’s specification, you can specify what Kubernetes should do when the\\nprocesses running in the container finish. This is done through the restartPolicyListing 4.11 A YAML definition of a Job: exporter.yamlNode 1Pod A (unmanaged)\\nPod B (managed by a ReplicaSet)\\nPod C (managed by a Job)\\nNode 2\\nNode 1 fails Job C2 ﬁnishesTimePod B2 (managed by a ReplicaSet)\\nPod C2 (managed by a Job)Pod A isn’t rescheduled,\\nbecause there is nothingmanaging it.\\nFigure 4.10 Pods managed by Jobs are rescheduled until they finish successfully.\\nJobs are in the batch \\nAPI group, version v 1.\\nYou’re not specifying a pod \\nselector (it will be created \\nbased on the labels in the pod template).\\nJobs can’t use the \\ndefault restart policy, which is Always.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 145}, page_content='114 CHAPTER  4Replication and other controllers: deploying managed pods\\npod spec property, which defaults to Always . Job pods can’t use the default policy,\\nbecause they’re not meant to run indefinitely. Therefore, you need to explicitly set\\nthe restart policy to either OnFailure  or Never . This setting is what prevents the con-\\ntainer from being restarted when it finishes  (not the fact that the pod is being man-\\naged by a Job resource).\\n4.5.3 Seeing a Job run a pod\\nAfter you create this Job with the kubectl  create  command, you should see it start up\\na pod immediately:\\n$ kubectl get jobs\\nNAME        DESIRED   SUCCESSFUL   AGE\\nbatch-job   1         0            2s\\n$ kubectl get po\\nNAME              READY     STATUS    RESTARTS   AGE\\nbatch-job-28qf4   1/1       Running   0          4s\\nAfter the two minutes have passed, the pod w ill no longer show up in the pod list and\\nthe Job will be marked as completed. By default, completed pods aren’t shown when\\nyou list pods, unless you use the --show-all  (or -a) switch:\\n$ kubectl get po -a\\nNAME              READY     STATUS      RESTARTS   AGE\\nbatch-job-28qf4   0/1       Completed   0          2m\\nThe reason the pod isn’t deleted when it comp letes is to allow you to examine its logs;\\nfor example:\\n$ kubectl logs batch-job-28qf4\\nFri Apr 29 09:58:22 UTC 2016 Batch job starting\\nFri Apr 29 10:00:22 UTC 2016 Finished succesfully\\nThe pod will be deleted when you delete it or the Job that created it. Before you do\\nthat, let’s look at the Job resource again:\\n$ kubectl get job\\nNAME        DESIRED   SUCCESSFUL   AGE\\nbatch-job   1         1            9m\\nThe Job is shown as having completed succe ssfully. But why is that piece of informa-\\ntion shown as a number instead of as yes or true ? And what does the DESIRED  column\\nindicate? \\n4.5.4 Running multiple pod instances in a Job\\nJobs may be configured to create more than one pod instance and run them in paral-lel or sequentially. This is done by setting the \\ncompletions  and the parallelism  prop-\\nerties in the Job spec.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 146}, page_content='115 Running pods that perform a single completable task\\nRUNNING  JOB PODS SEQUENTIALLY\\nIf you need a Job to run more than once, you set completions  to how many times you\\nwant the Job’s pod to run. The following listing shows an example.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:  name: multi-completion-batch-job\\nspec:\\n  completions: 5                    template:\\n    <template is the same as in listing 4.11>\\nThis Job will run five pods one after the ot her. It initially creates one pod, and when\\nthe pod’s container finishes, it creates the se cond pod, and so on, until five pods com-\\nplete successfully. If one of the pods fails , the Job creates a new pod, so the Job may\\ncreate more than five pods overall.\\nRUNNING  JOB PODS IN PARALLEL\\nInstead of running single Job pods one afte r the other, you can also make the Job run\\nmultiple pods in parallel. You specify how many pods are allowed to run in parallel\\nwith the parallelism   Job spec property, as shown in the following listing.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: multi-completion-batch-jobspec:\\n  completions: 5                    \\n  parallelism: 2                      template:\\n    <same as in listing 4.11>\\nBy setting parallelism  to 2, the Job creates two pods and runs them in parallel:\\n$ kubectl get po\\nNAME                               READY   STATUS     RESTARTS   AGE\\nmulti-completion-batch-job-lmmnk   1/1     Running    0          21s\\nmulti-completion-batch-job-qx4nq   1/1     Running    0          21s\\nAs soon as one of them finishes, the Job w ill run the next pod, until five pods finish\\nsuccessfully.Listing 4.12 A Job requiring multiple completions: multi-completion-batch-job.yaml\\nListing 4.13 Running Job pods in parallel:  multi-completion-parallel-batch-job.yamlSetting completions to \\n5 makes this Job run five pods sequentially.\\nThis job must ensure \\nfive pods complete successfully.\\nUp to two pods \\ncan run in parallel.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 147}, page_content='116 CHAPTER  4Replication and other controllers: deploying managed pods\\nSCALING  A JOB\\nYou can even change a Job’s parallelism  property while the Job is running. This is\\nsimilar to scaling a ReplicaSet or Replicat ionController, and can be done with the\\nkubectl  scale  command:\\n$ kubectl scale job multi-completion-batch-job --replicas 3\\njob \"multi-completion-batch-job\" scaled\\nBecause you’ve increased parallelism  from 2 to 3, another pod is immediately spun\\nup, so three pods are now running.\\n4.5.5 Limiting the time allowed for a Job pod to complete\\nWe need to discuss one final thing about Jo bs. How long should the Job wait for a pod\\nto finish? What if the pod gets stuck and can’t finish at all (or it can’t finish fastenough)?\\n A pod’s time can be limited by setting the \\nactiveDeadlineSeconds  property in the\\npod spec. If the pod runs longer than that, the system will try to terminate it and will\\nmark the Job as failed. \\nNOTE You can configure how many times a Job can be retried before it is\\nmarked as failed by specifying the spec.backoffLimit  field in the Job mani-\\nfest. If you don\\'t explicitly specify it, it defaults to 6.\\n4.6 Scheduling Jobs to run periodically or once \\nin the future\\nJob resources run their pods immediately wh en you create the Job resource. But many\\nbatch jobs need to be run at a specific time in the future or repeatedly in the specified\\ninterval. In Linux- and UNIX-like operating systems, these jobs are better known ascron jobs. Kubernetes supports them, too.\\n A cron job in Kubernetes is configured by creating a CronJob resource. The\\nschedule for running the job is specified in  the well-known cron format, so if you’re\\nfamiliar with regular cron jobs, you’ll understand Kubernetes’ CronJobs in a matter\\nof seconds.\\n At the configured time, Kubernetes will create a Job resource according to the Job\\ntemplate configured in the CronJob object. When the Job resource is created, one or\\nmore pod replicas will be created and star ted according to the Job’s pod template, as\\nyou learned in the previous sectio n. There’s nothing more to it.\\n Let’s look at how to create CronJobs. \\n4.6.1 Creating a CronJob\\nImagine you need to run the batch job from your previous example every 15 minutes.To do that, create a CronJob resour ce with the following specification.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 148}, page_content='117 Scheduling Jobs to run periodically or once in the future\\napiVersion: batch/v1beta1               \\nkind: CronJobmetadata:\\n  name: batch-job-every-fifteen-minutes\\nspec:  schedule: \"0,15,30,45 * * * *\"           \\n  jobTemplate:\\n    spec:\\n      template:                            \\n        metadata:                                    labels:                          \\n            app: periodic-batch-job        \\n        spec:                                        restartPolicy: OnFailure         \\n          containers:                      \\n          - name: main                                 image: luksa/batch-job         \\nAs you can see, it’s not too complicated. You’ve specified a schedule and a template\\nfrom which the Job objects will be created. \\nCONFIGURING  THE SCHEDULE\\nIf you’re unfamiliar with the cron schedule format, you’ll find great tutorials and\\nexplanations online, but as a quick introducti on, from left to righ t, the schedule con-\\ntains the following five entries:\\n\\uf0a1Minute\\n\\uf0a1Hour\\n\\uf0a1Day of month\\n\\uf0a1Month\\n\\uf0a1Day of week.\\nIn the example, you want to run the job every 15 minutes, so the schedule needs to be\\n\"0,15,30,45  * * * *\", which means at the 0, 15, 30 and 45 minutes mark of every hour\\n(first asterisk), of every day of the mont h (second asterisk), of every month (third\\nasterisk) and on every day of the week (fourth asterisk). \\n If, instead, you wanted it to run every 30 minutes, but only on the first day of the\\nmonth, you’d set the schedule to \"0,30  * 1 * *\", and if you want it to run at 3AM every\\nSunday, you’d set it to \"0 3 * * 0\" (the last zero stands for Sunday).\\nCONFIGURING  THE JOB TEMPLATE\\nA CronJob creates Job resources from the jobTemplate  property configured in the\\nCronJob spec, so refer to section 4.5 for more information on how to configure it.\\n4.6.2 Understanding how scheduled jobs are run\\nJob resources will be created from the Cr onJob resource at approximately the sched-\\nuled time. The Job then creates the pods. Listing 4.14 YAML for a CronJob resource: cronjob.yaml\\nAPI group is batch, \\nversion is v 1beta1\\nThis job should run at the \\n0, 15, 30 and 45 minutes of \\nevery hour, every day.\\nThe template for the \\nJob resources that \\nwill be created by this CronJob'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 149}, page_content='118 CHAPTER  4Replication and other controllers: deploying managed pods\\n It may happen that the Job or pod is created and run relatively late. You may have\\na hard requirement for the job to not be started too far over the scheduled time. In\\nthat case, you can specify a deadline by specifying the startingDeadlineSeconds  field\\nin the CronJob specification as shown in the following listing.\\napiVersion: batch/v1beta1\\nkind: CronJobspec:\\n  schedule: \"0,15,30,45 * * * *\"\\n  startingDeadlineSeconds: 15      ...\\nIn the example in listing 4.15, one of the ti mes the job is supposed to run is 10:30:00.\\nIf it doesn’t start by 10:30:15 for whatev er reason, the job will not run and will be\\nshown as Failed. \\n In normal circumstances, a CronJob always  creates only a single Job for each exe-\\ncution configured in the schedule, but it may happen that two Jobs are created at thesame time, or none at all. To combat the first problem, your jobs should be idempo-\\ntent (running them multiple times inst ead of once shouldn’t lead to unwanted\\nresults). For the second problem, make sure  that the next job run performs any work\\nthat should have been done by the previous (missed) run.\\n4.7 Summary\\nYou’ve now learned how to keep pods ru nning and have them rescheduled in the\\nevent of node failures. You should now know that\\n\\uf0a1You can specify a liveness probe to have  Kubernetes restart your container as\\nsoon as it’s no longer healthy (whe re the app defines what’s considered\\nhealthy).\\n\\uf0a1Pods shouldn’t be created directly, because they will not be re-created if they’re\\ndeleted by mistake, if the node they’re running on fails, or if they’re evicted\\nfrom the node.\\n\\uf0a1ReplicationControllers always keep the desired number of pod replicas\\nrunning.\\n\\uf0a1Scaling pods horizontally is as easy as changing the desired replica count on aReplicationController.\\n\\uf0a1Pods aren’t owned by the ReplicationControllers and can be moved betweenthem if necessary.\\n\\uf0a1A ReplicationController creates new pods from a pod template. Changing the\\ntemplate has no effect on existing pods.Listing 4.15 Specifying a startingDeadlineSeconds  for a CronJob\\nAt the latest, the pod must \\nstart running at 15 seconds \\npast the scheduled time.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 150}, page_content='119 Summary\\n\\uf0a1ReplicationControllers should be repl aced with ReplicaSets and Deployments,\\nwhich provide the same functionality, bu t with additional powerful features.\\n\\uf0a1ReplicationControllers and ReplicaSets schedule pods to random cluster nodes,\\nwhereas DaemonSets make sure every node runs a single instance of a pod\\ndefined in the DaemonSet.\\n\\uf0a1Pods that perform a batch task should  be created through a Kubernetes Job\\nresource, not directly or through a Re plicationController or similar object.\\n\\uf0a1Jobs that need to run sometime in th e future can be created through CronJob\\nresources.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 151}, page_content='120Services: enabling\\nclients to discover\\nand talk to pods\\nYou’ve learned about pods and how to depl oy them through ReplicaSets and similar\\nresources to ensure they keep running. Although certain pods can do their work\\nindependently of an external stimulus, ma ny applications these days are meant to\\nrespond to external requests. For example, in the case of microservices, pods will\\nusually respond to HTTP requests coming ei ther from other pods inside the cluster\\nor from clients outside the cluster. \\n Pods need a way of finding other pods if  they want to consume the services they\\nprovide. Unlike in the non-Kubernetes world, where a sysadmin would configureThis chapter covers\\n\\uf0a1Creating Service resources to expose a group of \\npods at a single address\\n\\uf0a1Discovering services in the cluster\\n\\uf0a1Exposing services to external clients\\n\\uf0a1Connecting to external services from inside the cluster\\n\\uf0a1Controlling whether a pod is ready to be part of the service or not\\n\\uf0a1Troubleshooting services'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 152}, page_content='121 Introducing services\\neach client app by specifying the exact IP address or hostname of the server providing\\nthe service in the client’s configuration files, doing the same in Kubernetes wouldn’t\\nwork, because\\n\\uf0a1Pods are ephemeral —They may come and go at any time, whether it’s because a\\npod is removed from a node to make room for other pods, because someone\\nscaled down the number of pods, or  because a cluster node has failed.\\n\\uf0a1Kubernetes assigns an IP address to a  pod after the  pod has been scheduled to a node\\nand before it’s started —Clients thus can’t know the IP address of the server pod\\nup front.\\n\\uf0a1Horizontal scaling means multiple po ds may provide the same service —Each of those\\npods has its own IP address. Clients sh ouldn’t care how many pods are backing\\nthe service and what their IPs are. They sh ouldn’t have to keep a list of all the\\nindividual IPs of pods. Instead, all thos e pods should be accessible through a\\nsingle IP address.\\nTo solve these problems, Kubernetes also provides another resource type—Services—\\nthat we’ll discuss in this chapter.\\n5.1 Introducing services\\nA Kubernetes Service is a resource you cr eate to make a single, constant point of\\nentry to a group of pods providing the same  service. Each service has an IP address\\nand port that never change while the service exists. Clients can open connections to\\nthat IP and port, and those connections are then routed to one of the pods backing\\nthat service. This way, clients of a service don’t need to know the location of individ-\\nual pods providing the service, allowing th ose pods to be moved around the cluster\\nat any time. \\nEXPLAINING  SERVICES  WITH AN EXAMPLE\\nLet’s revisit the example where you have a frontend web server and a backend data-\\nbase server. There may be multiple pods th at all act as the frontend, but there may\\nonly be a single backend database pod. Yo u need to solve two problems to make the\\nsystem function:\\n\\uf0a1External clients need to connect to the frontend pods without caring if there’s\\nonly a single web server or hundreds.\\n\\uf0a1The frontend pods need to connect to the backend database. Because the data-\\nbase runs inside a pod, it may be move d around the cluster over time, causing\\nits IP address to change. You don’t want to reconfigure the frontend pods every\\ntime the backend database is moved.\\nBy creating a service for the frontend pods  and configuring it to be accessible from\\noutside the cluster, you expose a single, constant IP address through which external\\nclients can connect to the pods. Similarly, by also creating a service for the backend\\npod, you create a stable address for the backend pod. The service address doesn’t'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 153}, page_content='122 CHAPTER  5Services: enabling clients to discover and talk to pods\\nchange even if the pod’s IP address changes.  Additionally, by creating the service, you\\nalso enable the frontend pods to easily fi nd the backend service by its name through\\neither environment variables or DNS. All th e components of your system (the two ser-\\nvices, the two sets of pods backing those services, and the interdependencies between\\nthem) are shown in figure 5.1.\\nYou now understand the basic idea behind se rvices. Now, let’s dig deeper by first see-\\ning how they can be created.\\n5.1.1 Creating services\\nAs you’ve seen, a service can be backed by more than one pod. Connections to the ser-vice are load-balanced across all the backing pods. But how exactly do you define\\nwhich pods are part of the service and which aren’t? \\n You probably remember label selector s and how they’re us ed in Replication-\\nControllers and other pod controllers to sp ecify which pods belong to the same set.\\nThe same mechanism is used by services in the same way, as you can see in figure 5.2.\\n In the previous chapter, you created a ReplicationController which then ran three\\ninstances of the pod containing the Node.j s app. Create the ReplicationController\\nagain and verify three pod instances are up and running. After that, you’ll create a\\nService for those three pods. Frontend pod 1\\nIP: 2.1.1.1External client\\nFrontend pod 2\\nIP: 2.1.1.2Frontend pod 3\\nIP: 2.1.1.3\\nBackend pod\\nIP: 2.1.1.4Frontend service\\nIP: 1.1.1.1\\nBackend service\\nIP: 1.1.1.2Frontend components\\nBackend components\\nFigure 5.1 Both internal and external clients usually connect to pods through services.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 154}, page_content='123 Introducing services\\nCREATING  A SERVICE  THROUGH  KUBECTL  EXPOSE\\nThe easiest way to create a service is through kubectl  expose , which you’ve already\\nused in chapter 2 to expose the Replic ationController you created earlier. The\\nexpose  command created a Service resource wi th the same pod selector as the one\\nused by the ReplicationController, thereby exposing all its pods through a single IP\\naddress and port. \\n Now, instead of using the expose  command, you’ll create a service manually by\\nposting a YAML to the Kubernetes API server. \\nCREATING  A SERVICE  THROUGH  A YAML DESCRIPTOR\\nCreate a file called kubia-svc.yaml with the following listing’s contents.\\napiVersion: v1\\nkind: Service             \\nmetadata:  name: kubia              \\nspec:\\n  ports:  - port: 80              \\n    targetPort: 8080       \\n  selector:                     app: kubia              \\nYou’re defining a service called kubia , which will accept connections on port 80 and\\nroute each connection to port 8080 of one of the pods matching the app=kubia\\nlabel selector. \\n Go ahead and create the service by posting the file using kubectl  create .Listing 5.1 A definition of a service: kubia-svc.yamlapp: kubia\\nPod: kubia-q3vkg\\nPod: kubia-k0xz6Pod: kubia-53thyClientService: kubia\\nSelector:app=kubia\\napp: kubia\\napp: kubia\\nFigure 5.2 Label selectors \\ndetermine which pods belong \\nto the Service.\\nThe port this service \\nwill be available on\\nThe container port the \\nservice will forward to\\nAll pods with the app=kubia \\nlabel will be part of this service.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 155}, page_content='124 CHAPTER  5Services: enabling clients to discover and talk to pods\\nEXAMINING  YOUR NEW SERVICE\\nAfter posting the YAML, you can list all Serv ice resources in your namespace and see\\nthat an internal cluster IP has been assigned to your service:\\n$ kubectl get svc\\nNAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGEkubernetes   10.111.240.1     <none>        443/TCP   30d\\nkubia        10.111.249.153   <none>        80/TCP    6m     \\nThe list shows that the IP address assigned  to the service is 10.111.249.153. Because\\nthis is the cluster IP, it’s only accessible from inside the cluster. The primary purpose\\nof services is exposing groups of pods to other pods in the cluster, but you’ll usually\\nalso want to expose services externally. You’ ll see how to do that later. For now, let’s\\nuse your service from inside th e cluster and see what it does.\\nTESTING  YOUR SERVICE  FROM WITHIN  THE CLUSTER\\nYou can send requests to your service from within the cluster in a few ways:\\n\\uf0a1The obvious way is to create a pod that  will send the request to the service’s\\ncluster IP and log the response. You ca n then examine the pod’s log to see\\nwhat the service’s response was.\\n\\uf0a1You can ssh into one of the Kubernetes nodes and use the curl  command.\\n\\uf0a1You can execute the curl  command inside one of your existing pods through\\nthe kubectl  exec  command.\\nLet’s go for the last option, so you also le arn how to run commands in existing pods. \\nREMOTELY  EXECUTING  COMMANDS  IN RUNNING  CONTAINERS\\nThe kubectl  exec  command allows you to remotely run arbitrary commands inside\\nan existing container of a pod. This comes in handy when you want to examine the\\ncontents, state, and/or environment of a container. List the pods with the kubectl\\nget pods  command and choose one as your target for the exec  command (in the fol-\\nlowing example, I’ve chosen the kubia-7nog1  pod as the target). You’ll also need to\\nobtain the cluster IP of your service (using kubectl  get svc, for example). When run-\\nning the following commands yourself, be sure to replace the pod name and the ser-\\nvice IP with your own: \\n$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153\\nYou’ve hit kubia-gzwli\\nIf you’ve used ssh to execute commands on a remote system before, you’ll recognize\\nthat kubectl  exec  isn’t much different.\\n \\n \\n  Here’s your \\nservice.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 156}, page_content='125 Introducing services\\nLet’s go over what transpired when you ran the command. Figure 5.3 shows the\\nsequence of events. You instruct ed Kubernetes to execute the curl  command inside the\\ncontainer of one of your pods. Curl sent an HTTP request to the service IP, which is\\nbacked by three pods. The Kubernetes se rvice proxy intercepted the connection,\\nselected a random pod among the three pods , and forwarded the request to it. Node.js\\nrunning inside that pod then handled the re quest and returned an HTTP response con-\\ntaining the pod’s name. Curl then printed th e response to the standard output, which\\nwas intercepted and printed to its standard output on your local machine by kubectl .Why the double dash?\\nThe double dash ( --) in the command signals the end of command options for\\nkubectl . Everything after the double dash is  the command that should be executed\\ninside the pod. Using the double dash isn’t necessary if the command has no\\narguments that start with a dash. But in your case, if you don’t use the double dash\\nthere, the -s option would be interpreted as an option for kubectl  exec and would\\nresult in the following strange and highly misleading error:\\n$ kubectl exec kubia-7nog1 curl -s http://10.111.249.153\\nThe connection to the server 10.111.249.153 was refused – did you \\nspecify the right host or port?\\nThis has nothing to do with your servic e refusing the connection. It’s because\\nkubectl  is not able to connect to an AP I server at 10.111.249.153 (the -s option\\nis used to tell kubectl  to connect to a different API server than the default).\\n3. Curl sends HTTP\\nGET request4. Service redirects HTTP\\nconnection to a randomlyselected pod\\n2. Curl is executed\\ninside the containerrunning node.js\\n6. The output of the\\ncommand is sentcurl\\nback to kubectl andprinted by it5. HTTP response is\\nsent back to curlPod: kubia-7nog1\\nContainer\\nnode.js\\ncurl http://\\n10.111.249.153Pod: kubia-gzwli\\nContainer\\nnode.jsPod: kubia-5fje3\\nContainer\\nnode.js\\n1.kubectl execService: kubia\\n10.111.249.153:80\\nFigure 5.3 Using kubectl exec to test out a connection to the service by running curl in one of the pods'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 157}, page_content='126 CHAPTER  5Services: enabling clients to discover and talk to pods\\nIn the previous example, you executed the curl  command as a separate process, but\\ninside the pod’s main container. This isn’t much different from the actual main pro-\\ncess in the container talking to the service.\\nCONFIGURING  SESSION  AFFINITY  ON THE SERVICE\\nIf you execute the same command a few more  times, you should hit a different pod\\nwith every invocation, because the service proxy normally forwards each connection\\nto a randomly selected backing pod, even  if the connections are coming from the\\nsame client. \\n If, on the other hand, you want all requests made by a certain client to be redi-\\nrected to the same pod every time, you can set the service’s sessionAffinity  property\\nto ClientIP  (instead of None , which is the default), as shown in the following listing.\\napiVersion: v1\\nkind: Service             spec:\\n  sessionAffinity: ClientIP\\n  ...\\nThis makes the service proxy redirect all re quests originating from the same client IP\\nto the same pod. As an exercise, you can crea te an additional service with session affin-\\nity set to ClientIP  and try sending requests to it.\\n Kubernetes supports only two ty pes of service session affinity: None  and ClientIP .\\nYou may be surprised it doesn’t have a cook ie-based session affinity option, but you\\nneed to understand that Kubernetes services  don’t operate at the HTTP level. Services\\ndeal with TCP and UDP packets and don’t ca re about the payload they carry. Because\\ncookies are a construct of the HTTP protoc ol, services don’t know about them, which\\nexplains why session affinity cannot be based on cookies. \\nEXPOSING  MULTIPLE  PORTS  IN THE SAME SERVICE\\nYour service exposes only a single port, but se rvices can also support multiple ports. For\\nexample, if your pods listened on two ports—let’s say 8080 for HTTP and 8443 for\\nHTTPS—you could use a single service to forward both port 80 and 443 to the pod’s\\nports 8080 and 8443. You don’t need to create tw o different services in such cases. Using\\na single, multi-port service exposes all the service’s ports through a single cluster IP.\\nNOTE When creating a service with multip le ports, you must specify a name\\nfor each port.\\nThe spec for a multi-port service is shown in the following listing.\\napiVersion: v1\\nkind: Service             metadata:\\n  name: kubia              Listing 5.2 A example of a service with ClientIP  session affinity configured\\nListing 5.3 Specifying multiple ports in a service definition'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 158}, page_content='127 Introducing services\\nspec:\\n  ports:\\n  - name: http                  port: 80                \\n    targetPort: 8080        \\n  - name: https                 port: 443               \\n    targetPort: 8443        \\n  selector:                 \\n    app: kubia              \\nNOTE The label selector applies to the serv ice as a whole—it can’t be config-\\nured for each port individually. If you want different ports to map to differentsubsets of pods, you need to create two services.\\nBecause your \\nkubia  pods don’t listen on multiple ports, creating a multi-port service\\nand a multi-port pod is left as an exercise to you.\\nUSING NAMED  PORTS\\nIn all these examples, you’ve referred to the target port by its number, but you can also\\ngive a name to each pod’s port and refer to it by name in the service spec. This makes\\nthe service spec slightly clearer, especia lly if the port numbers aren’t well-known.\\n For example, suppose your pod defines na mes for its ports as shown in the follow-\\ning listing.\\nkind: Pod\\nspec:\\n  containers:  - name: kubia\\n    ports:\\n    - name: http               \\n      containerPort: 8080      \\n    - name: https                    containerPort: 8443      \\nYou can then refer to those ports by name in the service spec, as shown in the follow-\\ning listing.\\napiVersion: v1\\nkind: Service             spec:\\n  ports:\\n  - name: http                  port: 80                \\n    targetPort: http        \\n  - name: https                 port: 443               \\n    targetPort: https       Listing 5.4 Specifying port names in a pod definition\\nListing 5.5 Referring to named ports in a servicePort 80 is mapped to \\nthe pods’ port 8080.\\nPort 443 is mapped to \\npods’ port 8443.\\nThe label selector always \\napplies to the whole service.\\nContainer’s port \\n8080 is called http\\nPort 8443 is called https.\\nPort 80 is mapped to the \\ncontainer’s port called http.\\nPort 443 is mapped to the container’s \\nport, whose name is https.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 159}, page_content='128 CHAPTER  5Services: enabling clients to discover and talk to pods\\nBut why should you even bother with naming  ports? The biggest benefit of doing so is\\nthat it enables you to change port numbers later without having to change the service\\nspec. Your pod currently uses port 8080 for http, but what if you later decide you’d\\nlike to move that to port 80? \\n If you’re using named ports, all you need to do is change the port number in the\\npod spec (while keeping the port’s name unchanged). As you spin up pods with the\\nnew ports, client connections will be forwarded to the appropriate port numbers,depending on the pod receiving the connect ion (port 8080 on old pods and port 80\\non the new ones).\\n5.1.2 Discovering services\\nBy creating a service, you now have a single  and stable IP address and port that you\\ncan hit to access your pods. This addre ss will remain unchanged throughout the\\nwhole lifetime of the service. Pods behind this service may come and go, their IPs may\\nchange, their number can go up or down, but they’ll always be accessible through the\\nservice’s single and constant IP address. \\n But how do the client pods know the IP an d port of a service? Do you need to cre-\\nate the service first, then manually look up its IP address and pass the IP to the config-\\nuration options of the client pod? Not really . Kubernetes also provides ways for client\\npods to discover a service’s IP and port.\\nDISCOVERING  SERVICES  THROUGH  ENVIRONMENT  VARIABLES\\nWhen a pod is started, Kubernetes initializ es a set of environment variables pointing\\nto each service that exists at that moment. If you create the service before creating the\\nclient pods, processes in those pods can ge t the IP address and port of the service by\\ninspecting their environment variables. \\n Let’s see what those environment variable s look like by examining the environment\\nof one of your running pods. You’ve already learned that you can use the kubectl  exec\\ncommand to run a command in the pod, but because you created the service only\\nafter your pods had been created, the en vironment variables for the service couldn’t\\nhave been set yet. You’ll need to address that first.\\n Before you can see environment variables fo r your service, you first need to delete\\nall the pods and let the ReplicationContr oller create new ones. You may remember\\nyou can delete all pods without sp ecifying their names like this:\\n$ kubectl delete po --all\\npod \"kubia-7nog1\" deletedpod \"kubia-bf50t\" deleted\\npod \"kubia-gzwli\" deleted\\nNow you can list the new pods (I’m sure you know how to do that) and pick one as\\nyour target for the kubectl  exec  command. Once you’ve selected your target pod,\\nyou can list environment variables by running the env command inside the container,\\nas shown in the following listing.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 160}, page_content='129 Introducing services\\n$ kubectl exec kubia-3inly env\\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\nHOSTNAME=kubia-3inlyKUBERNETES_SERVICE_HOST=10.111.240.1\\nKUBERNETES_SERVICE_PORT=443\\n...\\nKUBIA_SERVICE_HOST=10.111.249.153             \\nKUBIA_SERVICE_PORT=80                            ...\\nTwo services are defined in your cluster: the kubernetes  and the kubia  service (you\\nsaw this earlier with the kubectl  get svc command); consequently, two sets of service-\\nrelated environment variables are in the list. Among the variables that pertain to the\\nkubia  service you created at the beginning of the chapter, you’ll see the KUBIA_SERVICE\\n_HOST  and the KUBIA_SERVICE_PORT  environment variables, which hold the IP address\\nand port of the kubia  service, respectively. \\n Turning back to the frontend-backend ex ample we started this chapter with, when\\nyou have a frontend pod that requires the use of a backend database server pod, you\\ncan expose the backend pod through a service called backend-database  and then\\nhave the frontend pod look up its IP addr ess and port through the environment vari-\\nables BACKEND_DATABA SE_SERVICE_HOST  and BACKEND_DATABASE _SERVICE_PORT.\\nNOTE Dashes in the service name are converted to underscores and all let-\\nters are uppercased when the service name is used as the prefix in the envi-ronment variable’s name. \\nEnvironment variables are one way of looking up the IP and port of a service, but isn’t\\nthis usually the domain of DNS? Why doesn’t Kubernetes include a DNS server andallow you to look up service IPs through DNS instead? As it turns out, it does!\\nDISCOVERING  SERVICES  THROUGH  DNS\\nRemember in chapter 3 when you listed pods in the kube-system  namespace? One of\\nthe pods was called kube-dns . The kube-system  namespace also includes a corre-\\nsponding service with the same name.\\n As the name suggests, the pod runs a DN S server, which all other pods running in\\nthe cluster are automatically configured to  use (Kubernetes does that by modifying\\neach container’s /etc/resolv.conf  file). Any DNS query performed by a process run-\\nning in a pod will be handled by Kubernetes’ own DNS server, which knows all the ser-vices running in your system. \\nNOTE Whether a pod uses the internal DNS server or not is configurable\\nthrough the dnsPolicy  property in each pod’s spec.\\nEach service gets a DNS entry in the intern al DNS server, and client pods that know\\nthe name of the service can access it through its fully qualified domain name (FQDN)instead of resorting to environment variables. Listing 5.6 Service-related environment variables in a container\\nHere’s the cluster \\nIP of the service.\\nAnd here’s the port the \\nservice is available on.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 161}, page_content='130 CHAPTER  5Services: enabling clients to discover and talk to pods\\nCONNECTING  TO THE SERVICE  THROUGH  ITS FQDN\\nTo revisit the frontend-backend example, a frontend pod can connect to the backend-\\ndatabase service by opening a co nnection to the following FQDN:\\nbackend-database.default.svc.cluster.local\\nbackend-database  corresponds to the service name, default  stands for the name-\\nspace the service is defined in, and svc.cluster.local  is a configurable cluster\\ndomain suffix used in all cluster local service names. \\nNOTE The client must still know the serv ice’s port number. If the service is\\nusing a standard port (for example, 80  for HTTP or 5432 for Postgres), that\\nshouldn’t be a problem. If not, the c lient can get the port number from the\\nenvironment variable.\\nConnecting to a service can be even simpler than that. You can omit the svc.cluster\\n.local  suffix and even the namespace, when the frontend pod is in the same name-\\nspace as the database pod. You can th us refer to the service simply as backend-\\ndatabase . That’s incredibly simple, right?\\n Let’s try this. You’ll try to access the kubia  service through its FQDN instead of its\\nIP. Again, you’ll need to do that inside an  existing pod. You already know how to use\\nkubectl  exec  to run a single command in a pod’s container, but this time, instead of\\nrunning the curl  command directly, you’ll run the bash  shell instead, so you can then\\nrun multiple commands in the container. This  is similar to what you did in chapter 2\\nwhen you entered the container yo u ran with Docker by using the docker  exec  -it\\nbash  command. \\nRUNNING  A SHELL  IN A POD’S CONTAINER\\nYou can use the kubectl  exec  command to run bash  (or any other shell) inside a\\npod’s container. This way you’re free to ex plore the container as long as you want,\\nwithout having to perform a kubectl  exec  for every command you want to run.\\nNOTE The shell’s binary executable must be available in the container image\\nfor this to work.\\nTo use the shell properly, you need to pass the -it option to kubectl  exec :\\n$ kubectl exec -it kubia-3inly bash\\nroot@kubia-3inly:/# \\nYou’re now inside the container. You can use the curl  command to access the kubia\\nservice in any of the following ways:\\nroot@kubia-3inly:/# curl http://kubia.default.svc.cluster.localYou’ve hit kubia-5asi2\\nroot@kubia-3inly:/# curl http://kubia.default\\nYou’ve hit kubia-3inly'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 162}, page_content='131 Connecting to services living outside the cluster\\nroot@kubia-3inly:/# curl http://kubia\\nYou’ve hit kubia-8awf3\\nYou can hit your service by using the servic e’s name as the hostname in the requested\\nURL. You can omit the namespace and the svc.cluster.local  suffix because of how\\nthe DNS resolver inside each pod’s container is configured. Look at the /etc/resolv.conf\\nfile in the container and you’ll understand:\\nroot@kubia-3inly:/# cat /etc/resolv.conf\\nsearch default.svc.cluster.local svc.clust er.local cluster.local ...\\nUNDERSTANDING  WHY YOU CAN’T PING A SERVICE  IP\\nOne last thing before we move on. You know how to create services now, so you’ll soon\\ncreate your own. But what if, for whatev er reason, you can’t access your service?\\n You’ll probably try to figure out what’s wrong by entering an existing pod and try-\\ning to access the service like you did in the last example. Then, if you still can’t access\\nthe service with a simple curl  command, maybe you’ll try to  ping the service IP to see\\nif it’s up. Let’s try that now:\\nroot@kubia-3inly:/# ping kubia\\nPING kubia.default.svc.cluster.local (10.111.249.153): 56 data bytes^C--- kubia.default.svc.cluster.local ping statistics ---\\n54 packets transmitted, 0 packets received, 100% packet loss\\nHmm. curl -ing the service works, but pinging it doesn’t. That’s because the service’s\\ncluster IP is a virtual IP, and only has meaning when combined with the service port.\\nWe’ll explain what that means and how servic es work in chapter 11. I wanted to men-\\ntion that here because it’s the first thing users do when they try to debug a broken\\nservice and it catches most of them off guard.\\n5.2 Connecting to services living outside the cluster\\nUp to now, we’ve talked about services backed by one or more pods running inside\\nthe cluster. But cases exist when you’d like  to expose external services through the\\nKubernetes services feature. Instead of having the service redirect connections to\\npods in the cluster, you want it to re direct to external IP(s) and port(s). \\n This allows you to take advantage of both service load balancing and service discov-\\nery. Client pods running in the cluster ca n connect to the external service like they\\nconnect to internal services.\\n5.2.1 Introducing service endpoints\\nBefore going into how to do this, let me first shed more light on services. Services\\ndon’t link to pods directly. Instead, a resource sits in between—the Endpoints\\nresource. You may have already noticed endpoints if you used the kubectl  describe\\ncommand on your service, as shown in the following listing.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 163}, page_content='132 CHAPTER  5Services: enabling clients to discover and talk to pods\\n$ kubectl describe svc kubia\\nName:                kubiaNamespace:           default\\nLabels:              <none>\\nSelector:            app=kubia         Type:                ClusterIP\\nIP:                  10.111.249.153\\nPort:                <unset> 80/TCP\\nEndpoints:           10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   \\nSession Affinity:    NoneNo events.\\nAn Endpoints resource (yes, plural) is a list  of IP addresses and ports exposing a ser-\\nvice. The Endpoints resource is like any othe r Kubernetes resource, so you can display\\nits basic info with kubectl  get:\\n$ kubectl get endpoints kubia\\nNAME    ENDPOINTS                                         AGEkubia   10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h\\nAlthough the pod selector is defined in th e service spec, it’s not used directly when\\nredirecting incoming connections. Instead, the selector is used to build a list of IPs\\nand ports, which is then stored in the Endpoints resource. When a client connects to a\\nservice, the service proxy selects one of those IP and port pairs and redirects the\\nincoming connection to the server  listening at that location.\\n5.2.2 Manually configuring service endpoints\\nYou may have probably realized this alread y, but having the service’s endpoints decou-\\npled from the service allows them to  be configured and updated manually. \\n If you create a service without a pod se lector, Kubernetes won’t even create the\\nEndpoints resource (after all, without a sele ctor, it can’t know which pods to include\\nin the service). It’s up to yo u to create the Endpoints reso urce to specify the list of\\nendpoints for the service.\\n To create a service with manually managed endpoints, you need to create both a\\nService and an Endpoints resource. \\nCREATING  A SERVICE  WITHOUT  A SELECTOR\\nYou’ll first create the YAML for the service itself, as shown in the following listing.\\napiVersion: v1\\nkind: Service\\nmetadata:  name: external-service     \\nspec:                       \\n  ports:  - port: 80                  Listing 5.7 Full details of a service displayed with kubectl  describe\\nListing 5.8 A service without a pod selector: external-service.yamlThe service’s pod \\nselector is used to \\ncreate the list of \\nendpoints.The list of pod\\nIPs and ports\\nthat represent\\nthe endpoints of\\nthis service\\nThe name of the service must \\nmatch the name of the Endpoints \\nobject (see next listing).\\nThis service has no \\nselector defined.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 164}, page_content='133 Connecting to services living outside the cluster\\nYou’re defining a service called external-service  that will accept incoming connec-\\ntions on port 80. You didn’t defi ne a pod selector for the service.\\nCREATING  AN ENDPOINTS  RESOURCE  FOR A SERVICE  WITHOUT  A SELECTOR\\nEndpoints are a separate resource and not an  attribute of a service. Because you cre-\\nated the service without a selector, the co rresponding Endpoints resource hasn’t been\\ncreated automatically, so it’s up to you to create it. The following listing shows its\\nYAML manifest.\\napiVersion: v1\\nkind: Endpointsmetadata:\\n  name: external-service      \\nsubsets:  - addresses:\\n    - ip: 11.11.11.11         \\n    - ip: 22.22.22.22             ports:\\n    - port: 80      \\nThe Endpoints object needs to have the same  name as the service and contain the list\\nof target IP addresses and ports for the se rvice. After both the Service and the End-\\npoints resource are posted to the server, the service is ready to be used like any regular\\nservice with a pod selector. Containers create d after the service is created will include\\nthe environment variables for th e service, and all connections to its IP:port pair will be\\nload balanced between the service’s endpoints. \\n Figure 5.4 shows three pods connecting to the service with external endpoints.\\nIf you later decide to migrate the external service to pods running inside Kubernetes,\\nyou can add a selector to the service, th ereby making its Endpoints managed automat-\\nically. The same is also true in reverse— by removing the selector from a Service,Listing 5.9 A manually created Endpoints re source: external-service-endpoints.yaml\\nThe name of the Endpoints object \\nmust match the name of the service (see previous listing).\\nThe IPs of the endpoints that the \\nservice will forward connections to\\nThe target port of the endpoints\\nPod Pod Pod\\nExternal server 1\\nIP: 11.11.11.11:80\\nExternal server 2\\nIP: 22.22.22.22:80Service\\n10.111.249.214:80\\nKubernetes clusterInternet\\nFigure 5.4 Pods consuming a service with two external endpoints.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 165}, page_content='134 CHAPTER  5Services: enabling clients to discover and talk to pods\\nKubernetes stops updating its Endpoints. This means a service IP address can remain\\nconstant while the actual implementation of the service is changed. \\n5.2.3 Creating an alias for an external service\\nInstead of exposing an external service by manually configuring the service’s End-\\npoints, a simpler method allows you to refer to  an external service by its fully qualified\\ndomain name (FQDN).\\nCREATING  AN EXTERNAL NAME SERVICE\\nTo create a service that serves as an alias for an external service, you create a Service\\nresource with the type  field set to ExternalName . For example, let’s imagine there’s a\\npublic API available at api.somecompany.com . You can define a service that points to\\nit as shown in the following listing.\\napiVersion: v1\\nkind: Servicemetadata:\\n  name: external-service\\nspec:  type: ExternalName                       \\n  externalName: someapi.somecompany.com     \\n  ports:  - port: 80\\nAfter the service is created, pods can co nnect to the external service through the\\nexternal-service.defa ult.svc.cluster.local  domain name (or even external-\\nservice ) instead of using the service’s actual FQDN. This hides the actual service\\nname and its location from pods consuming the service, allowing you to modify the\\nservice definition and point it to a differen t service any time later, by only changing\\nthe externalName  attribute or by changing the type back to ClusterIP  and creating\\nan Endpoints object for the service—either manually or by specifying a label selector\\non the service and having it created automatically.\\n ExternalName  services are implemented solely at the DNS level—a simple CNAME\\nDNS record is created for the service. Ther efore, clients connectin g to the service will\\nconnect to the external service directly, bypassing the service proxy completely. For\\nthis reason, these types of services don’t even get a cluster IP. \\nNOTE A CNAME  record points to a fully qual ified domain name instead of a\\nnumeric IP address.\\n5.3 Exposing services to external clients\\nUp to now, we’ve only talked about how serv ices can be consumed by pods from inside\\nthe cluster. But you’ll also want to expose certain services, such as frontend webserv-\\ners, to the outside, so external clients can access them, as depicted in figure 5.5.Listing 5.10 An ExternalName -type service: external-service-externalname.yaml\\nService type is set \\nto ExternalName\\nThe fully qualified domain \\nname of the actual service'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 166}, page_content='135 Exposing services to external clients\\nYou have a few ways to make a service accessible externally:\\n\\uf0a1Setting the service type to  NodePort —For a NodePort  service, each cluster node\\nopens a port on the node itself (hence  the name) and redirects traffic received\\non that port to the underlying service.  The service isn’t accessible only at the\\ninternal cluster IP and port, but also through a dedicated port on all nodes. \\n\\uf0a1Setting the service type to  LoadBalancer , an extension of the  NodePort  type—This\\nmakes the service accessible through a dedicated load balancer, provisionedfrom the cloud infrastructure Kubernetes is running on. The load balancer redi-\\nrects traffic to the node port across all the nodes. Clients connect to the service\\nthrough the load balancer’s IP.\\n\\uf0a1Creating an  Ingress  resource, a radically different mech anism for exposing multiple ser-\\nvices through a single IP address —It operates at the HTTP level (network layer 7)\\nand can thus offer more features than la yer 4 services can. We’ll explain Ingress\\nresources in section 5.4. \\n5.3.1 Using a NodePort service\\nThe first method of exposing a set of pods to external clients is by creating a serviceand setting its type to \\nNodePort . By creating a NodePort  service, you make Kubernetes\\nreserve a port on all its node s (the same port number is used across all of them) and\\nforward incoming connections to the po ds that are part of the service. \\n This is similar to a regular service (their actual type is ClusterIP ), but a NodePort\\nservice can be accessed not only through the service’s internal cluster IP, but also\\nthrough any node’s IP and the reserved node port. \\n This will make more sense when you try interacting with a NodePort  service.\\nCREATING  A NODEPORT SERVICE\\nYou’ll now create a NodePort  service to see how you can use it. The following listing\\nshows the YAML for the service.\\n Kubernetes clusterExternal client Service\\nPod Pod Pod\\nFigure 5.5 Exposing a service to external clients'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 167}, page_content='136 CHAPTER  5Services: enabling clients to discover and talk to pods\\napiVersion: v1\\nkind: Servicemetadata:\\n  name: kubia-nodeport\\nspec:\\n  type: NodePort            \\n  ports:  - port: 80                 \\n    targetPort: 8080        \\n    nodePort: 30123          selector:\\n    app: kubia\\nYou set the type to NodePort  and specify the node port this service should be bound to\\nacross all cluster nodes. Specifying the port isn’t mandatory; Kubernetes will choose a\\nrandom port if you omit it. \\nNOTE When you create the service in GKE, kubectl  prints out a warning\\nabout having to configure firewall ru les. We’ll see how to do that soon. \\nEXAMINING  YOUR NODEPORT SERVICE\\nLet’s see the basic information of yo ur service to learn more about it:\\n$ kubectl get svc kubia-nodeport\\nNAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\\nkubia-nodeport   10.111.254.223   <nodes>       80:30123/TCP   2m\\nLook at the EXTERNAL-IP  column. It shows <nodes> , indicating the service is accessible\\nthrough the IP address of any cluster node. The PORT(S)  column shows both the\\ninternal port of the cluster IP ( 80) and the node port ( 30123 ). The service is accessi-\\nble at the following addresses:\\n\\uf0a110.11.254.223:80\\n\\uf0a1<1st  node’s  IP>:30123\\n\\uf0a1<2nd  node’s  IP>:30123 , and so on.\\nFigure 5.6 shows your service exposed on po rt 30123 of both of your cluster nodes\\n(this applies if you’re running this on GKE;  Minikube only has a single node, but the\\nprinciple is the same). An incoming connection to one of those ports will be redi-\\nrected to a randomly selected pod, which may or may not be the one running on the\\nnode the connection is being made to. \\n \\n \\n Listing 5.11 A NodePort  service definition: kubia-svc-nodeport.yaml\\nSet the service \\ntype to NodePort.\\nThis is the port of the \\nservice’s internal cluster IP.\\nThis is the target port \\nof the backing pods.\\nThe service will be accessible \\nthrough port 30 123 of each of \\nyour cluster nodes.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 168}, page_content='137 Exposing services to external clients\\nA connection received on port 30123 of the first node might be forwarded either to\\nthe pod running on the first node or to on e of the pods running on the second node.\\nCHANGING  FIREWALL  RULES  TO LET EXTERNAL  CLIENTS  ACCESS  OUR NODEPORT SERVICE\\nAs I’ve mentioned previously, before you can access your service through the node\\nport, you need to configure the Google Cl oud Platform’s firewalls to allow external\\nconnections to your nodes on that port. You’ll do this now:\\n$ gcloud compute firewall-rule s create kubia-svc-rule --allow=tcp:30123\\nCreated [https://www.googleapis.com/compute/v1/projects/kubia-\\n1295/global/firewalls/kubia-svc-rule].\\nNAME            NETWORK  SRC_RANGES  RULES      SRC_TAGS  TARGET_TAGS\\nkubia-svc-rule  default  0.0.0.0/0   tcp:30123\\nYou can access your service through port 3012 3 of one of the node’s IPs. But you need\\nto figure out the IP of a node first. Refer to the sidebar on how to do that.\\n \\n \\n Kubernetes clusterExternal client\\nPod\\nNode 2\\nIP: 130.211.99.206Node 1\\nIP: 130.211.97.55Port 30123\\nPort 8080\\nPod Port 8080PodPort 30123\\nPort 8080Service\\nFigure 5.6 An external client connecting to a NodePort  service either through Node 1 or 2'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 169}, page_content='138 CHAPTER  5Services: enabling clients to discover and talk to pods\\nOnce you know the IPs of your nodes, you can try accessing your service through them:\\n$ curl http://130.211.97.55:30123\\nYou\\'ve hit kubia-ym8or$ curl http://130.211.99.206:30123\\nYou\\'ve hit kubia-xueq1\\nTIP When using Minikube, you can easily access your NodePort  services\\nthrough your browser by running minikube  service  <service-name>  [-n\\n<namespace>] .\\nAs you can see, your pods are now accessibl e to the whole internet through port 30123\\non any of your nodes. It doesn’t matter wh at node a client sends the request to. But if\\nyou only point your clients to the first node , when that node fails, your clients can’t\\naccess the service anymore. That’s why it ma kes sense to put a load balancer in front\\nof the nodes to make sure you’re spread ing requests across all healthy nodes and\\nnever sending them to a node th at’s offline at that moment. \\n If your Kubernetes cluster supports it (w hich is mostly true when Kubernetes is\\ndeployed on cloud infrastructure), the lo ad balancer can be provisioned automati-\\ncally by creating a LoadBalancer  instead of a NodePort  service. We’ll look at this next.\\n5.3.2 Exposing a service through an external load balancer\\nKubernetes clusters running on cloud prov iders usually support the automatic provi-\\nsion of a load balancer from the cloud infr astructure. All you need to do is set theUsing JSONPath to get the IPs of all your nodes \\nYou can find the IP in the JSON or YAML descriptors of the nodes. But instead of\\nsifting through the relatively large JSON, you can tell kubectl  to print out only the\\nnode IP instead of the whole service definition: \\n$ kubectl get nodes -o jsonpath=\\'{.items[*].status.\\n➥ addresses[?(@.type==\"ExternalIP\")].address}\\'\\n130.211.97.55 130.211.99.206\\nYou’re telling kubectl  to only output the informat ion you want by specifying a\\nJSONPath. You’re probably familiar with XPath and how it’s used with XML. JSONPath\\nis basically XPath for JSON. The JSONPa th in the previous example instructs kubectl\\nto do the following:\\n\\uf0a1Go through all the elements in the items  attribute.\\n\\uf0a1For each element, enter the status  attribute.\\n\\uf0a1Filter elements of the addresses  attribute, taking only those that have the\\ntype attribute set to ExternalIP.\\n\\uf0a1Finally, print the address  attribute of the filtered elements.\\nTo learn more about how to use JSONPath with kubectl , refer to the documentation\\nat http:/ /kubernetes.io/docs/user-guide/jsonpath.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 170}, page_content=\"139 Exposing services to external clients\\nservice’s type to LoadBalancer  instead of NodePort . The load balancer will have its\\nown unique, publicly accessible IP address and will redirect all connections to your\\nservice. You can thus access your service through the load balancer’s IP address. \\n If Kubernetes is running in an  environment that doesn’t support LoadBalancer\\nservices, the load balancer will not be provis ioned, but the service will still behave like\\na NodePort  service. That’s because a LoadBalancer  service is an extension of a Node-\\nPort  service. You’ll run this example on Go ogle Kubernetes Engine, which supports\\nLoadBalancer  services. Minikube doesn’t, at least not as of this writing. \\nCREATING  A LOADBALANCER  SERVICE\\nTo create a service with a load balancer in  front, create the service from the following\\nYAML manifest, as shown in the following listing.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: kubia-loadbalancerspec:\\n  type: LoadBalancer          \\n  ports:  - port: 80\\n    targetPort: 8080\\n  selector:    app: kubia\\nThe service type is set to LoadBalancer  instead of NodePort . You’re not specifying a spe-\\ncific node port, although you could (you’r e letting Kubernetes choose one instead). \\nCONNECTING  TO THE SERVICE  THROUGH  THE LOAD BALANCER\\nAfter you create the service, it takes time for the cloud infrastructure to create the\\nload balancer and write its IP address into the Service object. Once it does that, the IP\\naddress will be listed as the exte rnal IP address of your service:\\n$ kubectl get svc kubia-loadbalancer\\nNAME                 CLUSTER-IP       EXTERNAL-IP      PORT(S)         AGE\\nkubia-loadbalancer   10.111.241.153   130.211.53.173    80:32143/TCP    1m\\nIn this case, the load balancer is availabl e at IP 130.211.53.173, so you can now access\\nthe service at that IP address:\\n$ curl http://130.211.53.173\\nYou've hit kubia-xueq1\\nSuccess! As you may have noticed, this time  you didn’t need to mess with firewalls the\\nway you had to before with the NodePort  service.Listing 5.12 A LoadBalancer -type service: kubia-svc-loadbalancer.yaml\\nThis type of service obtains \\na load balancer from the \\ninfrastructure hosting the \\nKubernetes cluster.\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 171}, page_content='140 CHAPTER  5Services: enabling clients to discover and talk to pods\\nSee figure 5.7 to see how HTTP requests ar e delivered to the pod. External clients\\n(curl  in your case) connect to port 80 of the load balancer and get routed to theSession affinity and web browsers\\nBecause your service is now exposed externa lly, you may try accessing it with your\\nweb browser. You’ll see something that ma y strike you as odd—the browser will hit\\nthe exact same pod every time. Did the se rvice’s session affinity change in the\\nmeantime? With kubectl  explain , you can double-check that the service’s session\\naffinity is still set to None, so why don’t different browser requests hit different\\npods, as is the case when using curl?\\nLet me explain what’s happening. The brow ser is using keep-alive connections and\\nsends all its requests through a single connection, whereas curl opens a new\\nconnection every time. Services work at t he connection level, so when a connection to a\\nservice is first opened, a r andom pod is selected and then all network packets belonging\\nto that connection are all sent to that single pod. Even if session affinity is set to None,\\nusers will always hit the same pod (until the connection is closed).\\nKubernetes clusterExternal client\\nLoad balancer\\nIP: 130.211.53.173:80\\nPod\\nNode 2\\nIP: 130.211.99.206Node 1\\nIP: 130.211.97.55Port 32143\\nPort 8080\\nPod Port 8080PodPort 32143\\nPort 8080Service\\nFigure 5.7 An external client connecting to a LoadBalancer  service'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 172}, page_content='141 Exposing services to external clients\\nimplicitly assigned node port on one of th e nodes. From there, the connection is for-\\nwarded to one of the pod instances.\\n As already mentioned, a LoadBalancer -type service is a NodePort  service with an\\nadditional infrastructure-provided load balancer. If you use kubectl  describe  to dis-\\nplay additional info about the service, you’ll  see that a node port has been selected for\\nthe service. If you were to open the firewall  for this port, the way you did in the previ-\\nous section about NodePort  services, you could access the service through the node\\nIPs as well.\\nTIP If you’re using Minikube, even though the load balancer will never be\\nprovisioned, you can still access the service through the node port (at theMinikube VM’s IP address).\\n5.3.3 Understanding the peculiarities of external connections\\nYou must be aware of several things relate d to externally originating connections to\\nservices. \\nUNDERSTANDING  AND PREVENTING  UNNECESSARY  NETWORK  HOPS\\nWhen an external client connects to a service through the node port (this alsoincludes cases when it goes through the load balancer first), the randomly chosen\\npod may or may not be running on the same node that received the connection. An\\nadditional network hop is required to re ach the pod, but this may not always be\\ndesirable. \\n You can prevent this additional hop by co nfiguring the service to redirect external\\ntraffic only to pods running on the node th at received the connection. This is done by\\nsetting the \\nexternalTrafficPolicy  field in the service’s spec  section:\\nspec:\\n  externalTrafficPolicy: Local\\n  ...\\nIf a service definition includes this setti ng and an external connection is opened\\nthrough the service’s node port, the service proxy will choose a locally running pod. If\\nno local pods exist, the connection will hang (it won’t be forwarded to a random\\nglobal pod, the way connections are when not using the annotation). You therefore\\nneed to ensure the load balancer forwards connections only to nodes that have atleast one such pod.\\n Using this annotation also has other dr awbacks. Normally, connections are spread\\nevenly across all the pods, but when using th is annotation, that’s no longer the case.\\n Imagine having two nodes and three po ds. Let’s say node A runs one pod and\\nnode B runs the other two. If the load ba lancer spreads connections evenly across the\\ntwo nodes, the pod on node A will receive 50 % of all connections, but the two pods on\\nnode B will only receive 25% each, as shown in figure 5.8.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 173}, page_content='142 CHAPTER  5Services: enabling clients to discover and talk to pods\\nBEING AWARE  OF THE NON-PRESERVATION  OF THE CLIENT ’S IP\\nUsually, when clients inside the cluster conne ct to a service, the pods backing the ser-\\nvice can obtain the client’s IP address. Bu t when the connection is received through a\\nnode port, the packets’ source IP is chan ged, because Source Network Address Trans-\\nlation (SNAT) is performed on the packets. \\n The backing pod can’t see the actual clie nt’s IP, which may be a problem for some\\napplications that need to know the client’s IP. In the case of a web server, for example,\\nthis means the access log won’t show the browser’s IP.\\n The Local  external traffic policy described in the previous section affects the pres-\\nervation of the client’s IP, because there’s no additional hop between the node receiv-\\ning the connection and the node hosting the target pod (SNAT isn’t performed).\\n5.4 Exposing services externally through an Ingress \\nresource\\nYou’ve now seen two ways of exposing a se rvice to clients outside the cluster, but\\nanother method exists—creating an Ingress resource.\\nDEFINITION Ingress  (noun)—The act of going in or entering; the right to\\nenter; a means or place of entering; entryway. \\nLet me first explain why you need another wa y to access Kubernetes services from the\\noutside. \\nUNDERSTANDING  WHY INGRESSES  ARE NEEDED\\nOne important reason is that each LoadBalancer  service requires its own load bal-\\nancer with its own public IP address, wherea s an Ingress only requires one, even when\\nproviding access to dozens of services. Wh en a client sends an HTTP request to the\\nIngress, the host and path in the request determine which service the request is for-\\nwarded to, as shown in figure 5.9.\\n 50%50% 50%\\n25% 25%\\nNode APod\\nNode BPod PodLoad balancer\\nFigure 5.8 A Service using \\nthe Local  external traffic \\npolicy may lead to uneven load distribution across pods.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 174}, page_content='143 Exposing services externally through an Ingress resource\\nIngresses operate at the application laye r of the network stack (HTTP) and can pro-\\nvide features such as cookie-based session affinity and the like, which services can’t.\\nUNDERSTANDING  THAT AN INGRESS  CONTROLLER  IS REQUIRED\\nBefore we go into the features an Ingres s object provides, let me emphasize that to\\nmake Ingress resources work, an Ingress cont roller needs to be running in the cluster.\\nDifferent Kubernetes environments use diff erent implementations of the controller,\\nbut several don’t provide a default controller at all. \\n For example, Google Kubernetes Engine  uses Google Cloud Platform’s own HTTP\\nload-balancing features to provide the Ingres s functionality. Initially, Minikube didn’t\\nprovide a controller out of the box, but it now includes an add-on that can be enabled\\nto let you try out the Ingress functionality.  Follow the instructions in the following\\nsidebar to ensure it’s enabled.\\nEnabling the Ingress add-on in Minikube\\nIf you’re using Minikube to run the examples in this book, you’ll need to ensure the\\nIngress add-on is enabled. You can check wheth er it is by listing all the add-ons:\\n$ minikube addons list\\n- default-storageclass: enabled\\n- kube-dns: enabled\\n- heapster: disabled- ingress: disabled               \\n- registry-creds: disabled\\n- addon-manager: enabled- dashboard: enabled\\nYou’ll learn about what these add-ons are throughout the book, but it should be\\npretty clear what the dashboard  and the kube-dns  add-ons do. Enable the Ingress\\nadd-on so you can see Ingresses in action:\\n$ minikube addons enable ingress\\ningress was successfully enabledPod Pod Pod\\nPod Pod Pod\\nPod Pod Pod\\nPod Pod PodIngress ClientServicekubia.example.com/kubia\\nfoo.example.comkubia.example.com/foo\\nService\\nbar.example.comService\\nService\\nFigure 5.9 Multiple services can be exposed through a single Ingress.\\nThe Ingress add-on \\nisn’t enabled.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 175}, page_content='144 CHAPTER  5Services: enabling clients to discover and talk to pods\\nTIP The --all-namespaces  option mentioned in the sidebar is handy when\\nyou don’t know what namespace your pod (or other type of resource) is in, orif you want to list resources across all namespaces.\\n5.4.1 Creating an Ingress resource\\nYou’ve confirmed there’s an Ingress controll er running in your cluster, so you can\\nnow create an Ingress resource. The follow ing listing shows what the YAML manifest\\nfor the Ingress looks like.\\napiVersion: extensions/v1beta1\\nkind: Ingress\\nmetadata:  name: kubia\\nspec:\\n  rules:  - host: kubia.example.com             \\n    http:\\n      paths:      - path: /                           \\n        backend:\\n          serviceName: kubia-nodeport               servicePort: 80                 \\nThis defines an Ingress with a single rule, which makes sure all HTTP requests received\\nby the Ingress controller, in which the host kubia.example.com  is requested, will be\\nsent to the kubia-nodeport  service on port 80. (continued)\\nThis should have spun up an Ingress con troller as another pod. Most likely, the\\ncontroller pod will be in the kube-system  namespace, but not neces sarily, so list all\\nthe running pods across all namespaces by using the --all-namespaces  option:\\n$ kubectl get po  --all-namespaces\\nNAMESPACE    NAME                            READY  STATUS    RESTARTS AGE\\ndefault      kubia-rsv5 m                     1/1    Running   0        13h\\ndefault      kubia-fe4a d                     1/1    Running   0        13h\\ndefault      kubia-ke82 3                     1/1    Running   0        13h\\nkube-system  default- http-backend-5wb0h      1/1    Running   0        18m\\nkube-system  kube-add on-manager-minikube     1/1    Running   3        6d\\nkube-system  kube-dns-v 20-101vq              3/3    Runni ng   9        6d\\nkube-system  kubernet es-dashboard-jxd9l      1/1    Running   3        6d\\nkube-system  nginx-in gress-controller-gdts0  1/1    Running   0        18m\\nAt the bottom of the output, you see the Ingress  controller pod. The name suggests\\nthat Nginx (an open-source HTTP server and reverse proxy) is used to provide the\\nIngress  functionality.\\nListing 5.13 An Ingress resource definition: kubia-ingress.yaml\\nThis Ingress maps the \\nkubia.example.com  domain \\nname to your service.\\nAll requests will be sent to \\nport 80 of the kubia-nodeport service.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 176}, page_content=\"145 Exposing services externally through an Ingress resource\\nNOTE Ingress controllers on cloud provid ers (in GKE, for example) require\\nthe Ingress to point to a NodePort  service. But that’s not a requirement of\\nKubernetes itself.\\n5.4.2 Accessing the service through the Ingress\\nTo access your service through http:/ /kubia.example.com , you’ll need to make sure\\nthe domain name resolves to th e IP of the Ingress controller. \\nOBTAINING  THE IP ADDRESS  OF THE INGRESS\\nTo look up the IP, you need to list Ingresses:\\n$ kubectl get ingresses\\nNAME      HOSTS               ADDRESS          PORTS     AGEkubia     kubia.example.com   192.168.99.100   80        29m\\nNOTE When running on cloud providers, th e address may take time to appear,\\nbecause the Ingress controller provisions a load balancer behind the scenes.\\nThe IP is shown in the ADDRESS  column. \\nENSURING  THE HOST CONFIGURED  IN THE INGRESS  POINTS  TO THE INGRESS ’ IP ADDRESS\\nOnce you know the IP, you can then either  configure your DNS servers to resolve\\nkubia.example.com  to that IP or you can add the following line to /etc/hosts  (or\\nC:\\\\windows\\\\system32 \\\\drivers\\\\etc\\\\hosts  on Windows):\\n192.168.99.100    kubia.example.com\\nACCESSING  PODS THROUGH  THE INGRESS\\nEverything is now set up, so you can access the service at http:/ /kubia.example.com\\n(using a browser or curl ):\\n$ curl http://kubia.example.com\\nYou've hit kubia-ke823\\nYou’ve successfully accessed the service throug h an Ingress. Let’s take a better look at\\nhow that unfolded.\\nUNDERSTANDING  HOW INGRESSES  WORK\\nFigure 5.10 shows how the client connected to one of the pods through the Ingress\\ncontroller. The client firs t performed a DNS lookup of kubia.example.com , and the\\nDNS server (or the local operating system) returned the IP of the Ingress controller.\\nThe client then sent an HTTP request to the Ingress controller and specified\\nkubia.example.com  in the Host  header. From that header, the controller determined\\nwhich service the client is trying to acce ss, looked up the pod IPs through the End-\\npoints object associated with the service, and forwarded the client’s request to one of\\nthe pods.\\n As you can see, the Ingress controller di dn’t forward the request to the service. It\\nonly used it to select a pod. Most, if  not all, controllers work like this.\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 177}, page_content='146 CHAPTER  5Services: enabling clients to discover and talk to pods\\n5.4.3 Exposing multiple services through the same Ingress\\nIf you look at the Ingress spec closely, you’ll see that both rules  and paths  are arrays,\\nso they can contain multiple items. An Ingress can map multiple hosts and paths to\\nmultiple services, as you’ll see next. Let’s focus on paths  first. \\nMAPPING  DIFFERENT  SERVICES  TO DIFFERENT  PATHS  OF THE SAME HOST\\nYou can map multiple paths  on the same host to different services, as shown in the\\nfollowing listing.\\n...\\n  - host: kubia.example.com\\n    http:\\n      paths:      - path: /kubia                \\n        backend:                    \\n          serviceName: kubia                  servicePort: 80           \\n      - path: /foo                \\n        backend:                            serviceName: bar        \\n          servicePort: 80         \\nIn this case, requests will be sent to tw o different services, depending on the path in\\nthe requested URL. Clients can therefore re ach two different services through a single\\nIP address (that of the Ingress controller).Listing 5.14 Ingress exposing multiple se rvices on same host, but different path sNode APod\\nNode BPod PodIngress\\ncontrollerEndpoints Service Ingress\\nClient2. Client sends HTTP GET\\nrequest with header\\nHost: kubia.example.com3. Controller sends\\nrequest to one of\\nthe pods.1. Client looks up\\nkubia.example.comDNS\\nFigure 5.10 Accessing pods through an Ingress\\nRequests to kubia.example.com/kubia \\nwill be routed to the kubia service.\\nRequests to kubia.example.com/bar \\nwill be routed to the bar service.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 178}, page_content='147 Exposing services externally through an Ingress resource\\nMAPPING  DIFFERENT  SERVICES  TO DIFFERENT  HOSTS\\nSimilarly, you can use an Ingress to map to different services based on the host in the\\nHTTP request instead of (only) the path, as shown in the next listing.\\nspec:\\n  rules:\\n  - host: foo.example.com              http:\\n      paths:\\n      - path: /         backend:\\n          serviceName: foo         \\n          servicePort: 80  - host: bar.example.com          \\n    http:\\n      paths:      - path: /\\n        backend:\\n          serviceName: bar                   servicePort: 80\\nRequests received by the controller wi ll be forwarded to either service foo or bar,\\ndepending on the Host  header in the request (the way virtual hosts are handled in\\nweb servers). DNS needs to point both the foo.example.com  and the bar.exam-\\nple.com  domain names to the Ingres s controller’s IP address. \\n5.4.4 Configuring Ingress to handle TLS traffic\\nYou’ve seen how an Ingress forwards HTTP traffic. But what about HTTPS? Let’s take\\na quick look at how to config ure Ingress to support TLS. \\nCREATING  A TLS CERTIFICATE  FOR THE INGRESS\\nWhen a client opens a TLS connection to an  Ingress controller, the controller termi-\\nnates the TLS connection. The communicatio n between the client and the controller\\nis encrypted, whereas the communication between the controller and the backend\\npod isn’t. The application running in the po d doesn’t need to support TLS. For exam-\\nple, if the pod runs a web server, it can ac cept only HTTP traffic and let the Ingress\\ncontroller take care of everything related to  TLS. To enable the controller to do that,\\nyou need to attach a certificate and a priv ate key to the Ingress. The two need to be\\nstored in a Kubernetes resource called a Secret, which is then referenced in the\\nIngress manifest. We’ll explain Secrets in detail in chapter 7. For now, you’ll create the\\nSecret without paying too much attention to it.\\n First, you need to create the private key and certificate:\\n$ openssl genrsa -out tls.key 2048\\n$ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj \\n➥ /CN=kubia.example.comListing 5.15 Ingress exposing multiple  services on different hosts\\nRequests for \\nfoo.example.com  will be \\nrouted to service foo.\\nRequests for \\nbar.example.com  will be \\nrouted to service bar.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 179}, page_content='148 CHAPTER  5Services: enabling clients to discover and talk to pods\\nThen you create the Secret from the two files like this:\\n$ kubectl create secret tls tl s-secret --cert=tls.cert --key=tls.key\\nsecret \"tls-secret\" created\\nThe private key and the certificate ar e now stored in the Secret called tls-secret .\\nNow, you can update your Ingress object so  i t  w i l l  a l s o  a c c e p t  H T T P S  r e q u e s t s  f o r\\nkubia.example.com . The Ingress manifest should now look like the following listing.\\napiVersion: extensions/v1beta1\\nkind: Ingress\\nmetadata:\\n  name: kubia\\nspec:  tls:                           \\n  - hosts:                        \\n    - kubia.example.com               secretName: tls-secret       \\n  rules:\\n  - host: kubia.example.com    http:\\n      paths:\\n      - path: /        backend:\\n          serviceName: kubia-nodeport\\n          servicePort: 80\\nTIP Instead of deleting the Ingress and re -creating it from the new file, you\\ncan invoke kubectl  apply  -f kubia-ingress-tls.yaml , which updates the\\nIngress resource with what ’s specified in the file.Signing certificates through the CertificateSigningRequest resource\\nInstead of signing the certificate ourselv es, you can get the certificate signed by\\ncreating a CertificateSig ningRequest  (CSR) resource. Users or their applications\\ncan create a regular certificate request, put  it into a CSR, and then either a human\\noperator or an automated process can approve the request like this:\\n$ kubectl certificate approve <name of the CSR> \\nThe signed certificate can then be retrieved from the CSR’s status.certificate\\nfield. \\nNote that a certificate signer component mu st be running in the cluster; otherwise\\ncreating CertificateSigningRequest  and approving or denying them won’t have\\nany effect.\\nListing 5.16 Ingress handling TLS traffic: kubia-ingress-tls.yaml\\nThe whole TLS configuration \\nis under this attribute.\\nTLS connections will be accepted for \\nthe kubia.example.com  hostname.\\nThe private key and the certificate \\nshould be obtained from the tls-\\nsecret you created previously.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 180}, page_content=\"149 Signaling when a pod is ready to accept connections\\nYou can now use HTTPS to access your service through the Ingress:\\n$ curl -k -v https://kubia.example.com/kubia\\n* About to connect() to kubia.example.com port 443 (#0)...\\n* Server certificate:\\n*   subject: CN=kubia.example.com...\\n> GET /kubia HTTP/1.1\\n> ...\\nYou've hit kubia-xueq1\\nThe command’s output shows the response from  the app, as well as the server certifi-\\ncate you configured the Ingress  with.\\nNOTE Support for Ingress features varies  between the different Ingress con-\\ntroller implementations, so check th e implementation-specific documenta-\\ntion to see what’s supported. \\nIngresses are a relatively new Kubernetes feature, so you can expect to see many\\nimprovements and new features in the futu re. Although they currently support only\\nL7 (HTTP/HTTPS) load balancing, support for L4 load balancing is also planned.\\n5.5 Signaling when a pod is ready to accept connections\\nThere’s one more thing we need to cove r regarding both Services and Ingresses.\\nYou’ve already learned that pods are include d as endpoints of a service if their labels\\nmatch the service’s pod selector. As soon as a new pod with proper labels is created, it\\nbecomes part of the service and requests star t to be redirected to the pod. But what if\\nthe pod isn’t ready to start se rving requests immediately? \\n The pod may need time to load either conf iguration or data, or it may need to per-\\nform a warm-up procedure to prevent the firs t user request from taking too long and\\naffecting the user experience. In such cases you don’t want the pod to start receiving\\nrequests immediately, especially when the already-running instances can process\\nrequests properly and quickly. It makes sens e to not forward requests to a pod that’s in\\nthe process of starting up  until it’s fully ready.\\n5.5.1 Introducing readiness probes\\nIn the previous chapter you learned abou t liveness probes and how they help keep\\nyour apps healthy by ensuring unhealth y containers are restarted automatically.\\nSimilar to liveness probes, Kubernetes allows you to also define a readiness probe\\nfor your pod.\\n The readiness probe is invoked periodic ally and determines whether the specific\\npod should receive client requests or not.  When a container’s readiness probe returns\\nsuccess, it’s signaling that the cont ainer is ready to accept requests. \\n This notion of being ready is obviously so mething that’s specific to each container.\\nKubernetes can merely check if the app runni ng in the container responds to a simple\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 181}, page_content='150 CHAPTER  5Services: enabling clients to discover and talk to pods\\nGET / request or it can hit a specific URL path, which causes the app to perform a\\nwhole list of checks to determine if it’s re ady. Such a detailed readiness probe, which\\ntakes the app’s specifics into account, is the app developer’s responsibility. \\nTYPES OF READINESS  PROBES\\nLike liveness probes, three types of readiness probes exist:\\n\\uf0a1An Exec probe, where a process is executed . The container’s status is deter-\\nmined by the process’ exit status code.\\n\\uf0a1An HTTP GET  probe, which sends an HTTP GET request to the container and\\nthe HTTP status code of the response  determines whether the container is\\nready or not.\\n\\uf0a1A TCP Socket  probe, which opens a TCP connect ion to a specified port of the\\ncontainer. If the connection is establis hed, the container is considered ready.\\nUNDERSTANDING  THE OPERATION  OF READINESS  PROBES\\nWhen a container is started, Kubernetes ca n be configured to wait for a configurable\\namount of time to pass before performing  the first readiness check. After that, it\\ninvokes the probe periodically and acts based on the result of the readiness probe. If a\\npod reports that it’s not ready, it’s remove d from the service. If the pod then becomes\\nready again, it’s re-added. \\n Unlike liveness probes, if a container fails the readiness check, it won’t be killed or\\nrestarted. This is an important distinct ion between liveness and readiness probes.\\nLiveness probes keep pods healthy by killi ng off unhealthy containers and replacing\\nthem with new, healthy ones, whereas readin ess probes make sure that only pods that\\nare ready to serve requests receive them. Th is is mostly necessary during container\\nstart up, but it’s also useful after the container has been running for a while. \\n As you can see in figure 5.11, if a pod’ s readiness probe fails, the pod is removed\\nfrom the Endpoints object. Clients connecting  to the service will not be redirected to\\nthe pod. The effect is the same as when the pod doesn’t match the service’s labelselector at all.\\nEndpointsService\\nSelector:app=kubia\\napp: kubia\\nPod: kubia-q3vkgapp: kubia\\nPod: kubia-k0xz6app: kubia\\nPod: kubia-53thyNot readyThis pod is no longer\\nan endpoint, because itsreadiness probe has failed.\\nFigure 5.11 A pod whose readiness probe fails is removed as an endpoint of a service.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 182}, page_content='151 Signaling when a pod is ready to accept connections\\nUNDERSTANDING  WHY READINESS  PROBES  ARE IMPORTANT\\nImagine that a group of pods (for exam ple, pods running application servers)\\ndepends on a service provided by another pod (a backend database, for example). If\\nat any point one of the frontend pods experiences connectivity problems and can’t\\nreach the database anymore, it may be wise for its readiness probe to signal to Kuber-\\nnetes that the pod isn’t ready to serve any re quests at that time. If other pod instances\\naren’t experiencing the same type of conne ctivity issues, they can serve requests nor-\\nmally. A readiness probe makes sure clients only talk to those healthy pods and never\\nnotice there’s anything wrong with the system.\\n5.5.2 Adding a readiness probe to a pod\\nNext you’ll add a readiness probe to your existing pods by modifying the Replication-\\nController’s pod template. \\nADDING  A READINESS  PROBE  TO THE POD TEMPLATE\\nYou’ll use the kubectl  edit  command to add the probe to  the pod template in your\\nexisting ReplicationController:\\n$ kubectl edit rc kubia\\nWhen the ReplicationController’s YAML open s in the text editor, find the container\\nspecification in the pod template and add the following readiness probe definition to\\nthe first container under spec.template.sp ec.containers.  The YAML should look\\nlike the following listing.\\napiVersion: v1\\nkind: ReplicationController...\\nspec:\\n  ...  template:\\n    ...\\n    spec:      containers:\\n      - name: kubia\\n        image: luksa/kubia        readinessProbe:       \\n          exec:               \\n            command:                      - ls              \\n            - /var/ready      \\n        ...\\nThe readiness probe will periodically perform the command ls /var/ready  inside the\\ncontainer. The ls command returns exit code zero if the file exists, or a non-zero exit\\ncode otherwise. If the file exists, the readin ess probe will succeed; otherwise, it will fail. Listing 5.17 RC creating a pod with a readi ness probe: kubia-rc-readinessprobe.yaml\\nA readinessProbe may \\nbe defined for each container in the pod.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 183}, page_content='152 CHAPTER  5Services: enabling clients to discover and talk to pods\\n The reason you’re defining such a strange readiness probe is so you can toggle its\\nresult by creating or removing the file in qu estion. The file doesn’t exist yet, so all the\\npods should now report not being ready, ri ght? Well, not exactly. As you may remem-\\nber from the previous chapter, changing a ReplicationController’s pod template has\\nno effect on existing pods. \\n In other words, all your existing pods still have no readiness probe defined. You\\ncan see this by listing the pods with kubectl  get pods  and looking at the READY  col-\\numn. You need to delete the pods and ha ve them re-created by the Replication-\\nController. The new pods will fail the readiness check and won’t be included as\\nendpoints of the service until you create the /var/ready file in each of them. \\nOBSERVING  AND MODIFYING  THE PODS’ READINESS  STATUS\\nList the pods again and inspec t whether they’re ready or not:\\n$ kubectl get po\\nNAME          READY     STATUS    RESTARTS   AGEkubia-2r1qb   0/1       Running   0          1m\\nkubia-3rax1   0/1       Running   0          1m\\nkubia-3yw4s   0/1       Running   0          1m\\nThe READY  column shows that none of the cont ainers are ready. Now make the readi-\\nness probe of one of them start returning success by creating the /var/ready  file,\\nwhose existence makes your mock readiness probe succeed:\\n$ kubectl exec kubia-2r1qb -- touch /var/ready\\nYou’ve used the kubectl  exec  command to execute the touch  command inside the\\ncontainer of the kubia-2r1qb  pod. The touch  command creates the file if it doesn’t\\nyet exist. The pod’s readine ss probe command should now exit with status code 0,\\nwhich means the probe is successful, and the pod should now be shown as ready. Let’s\\nsee if it is:\\n$ kubectl get po kubia-2r1qb\\nNAME          READY     STATUS    RESTARTS   AGEkubia-2r1qb   0/1       Running   0          2m\\nThe pod still isn’t ready. Is there something wrong or is this the expected result? Take\\na more detailed look at the pod with kubectl  describe . The output should contain\\nthe following line:\\nReadiness: exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1\\n➥ #failure=3\\nThe readiness probe is checked periodically—every 10 seconds by default. The pod\\nisn’t ready because the readiness probe hasn’t been invoked yet. But in 10 seconds at\\nthe latest, the pod should become ready and its IP should be listed as the only end-point of the service (run \\nkubectl  get endpoints  kubia-loadbalancer  to confirm).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 184}, page_content='153 Signaling when a pod is ready to accept connections\\nHITTING  THE SERVICE  WITH THE SINGLE  READY  POD\\nYou can now hit the service URL a few times to see that each and every request is redi-\\nrected to this one pod:\\n$ curl http://130.211.53.173\\nYou’ve hit kubia-2r1qb$ curl http://130.211.53.173\\nYou’ve hit kubia-2r1qb\\n...\\n$ curl http://130.211.53.173\\nYou’ve hit kubia-2r1qb\\nEven though there are three pods running, only a single pod is reporting as being\\nready and is therefore the only pod receivin g requests. If you now delete the file, the\\npod will be removed from the service again. \\n5.5.3 Understanding what real-world readiness probes should do\\nThis mock readiness probe is useful only for demonstrating what readiness probes do.\\nIn the real world, the readiness probe shou ld return success or failure depending on\\nwhether the app can (and wants to) receive client requests or not. \\n Manually removing pods from  services should be performed by either deleting the\\npod or changing the pod’s labels instead of  manually flipping a switch in the probe. \\nTIP If you want to add or remove a pod from a service manually, add\\nenabled=true  as a label to your pod and to the label selector of your service.\\nRemove the label when you want to  remove the pod from the service.\\nALWAYS  DEFINE  A READINESS  PROBE\\nBefore we conclude this section, there are two final notes about readiness probes that\\nI need to emphasize. First, if you don’t add a readiness probe to your pods, they’ll\\nbecome service endpoints almost immediatel y. If your application takes too long to\\nstart listening for incoming co nnections, client requests hitting the service will be for-\\nwarded to the pod while it’s still starting  up and not ready to accept incoming connec-\\ntions. Clients will therefore see “Con nection refused” types of errors. \\nTIP You should always define a readiness prob e, even if it’s as simple as send-\\ning an HTTP request to the base URL. \\nDON’T INCLUDE  POD SHUTDOWN  LOGIC INTO YOUR READINESS  PROBES\\nThe other thing I need to mention applies to the other end of the pod’s life (pod\\nshutdown) and is also related to c lients experiencing connection errors. \\n When a pod is being shut down, the app running in it usually stops accepting con-\\nnections as soon as it rece ives the termination signal. Because of this, you might think\\nyou need to make your readiness probe star t failing as soon as the shutdown proce-\\ndure is initiated, ensuring the pod is remove d from all services it’s part of. But that’s\\nnot necessary, because Kubernetes removes th e pod from all services as soon as you\\ndelete the pod.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 185}, page_content='154 CHAPTER  5Services: enabling clients to discover and talk to pods\\n5.6 Using a headless service for discovering individual pods\\nYou’ve seen how services can be used to pr ovide a stable IP addr ess allowing clients to\\nconnect to pods (or other endpoints) backin g each service. Each connection to the\\nservice is forwarded to one randomly selected backing pod. But what if the client\\nneeds to connect to all of those pods? What  if the backing pods themselves need to\\neach connect to all the other backing po ds? Connecting through the service clearly\\nisn’t the way to do this. What is?\\n For a client to connect to all pods, it need s to figure out the the IP of each individ-\\nual pod. One option is to have the client call the Kubernetes API server and get the\\nlist of pods and their IP addresses through an API call, but because you should always\\nstrive to keep your apps Kubernetes-agn ostic, using the API server isn’t ideal. \\n Luckily, Kubernetes allows clients to di scover pod IPs through DNS lookups. Usually,\\nwhen you perform a DNS lookup for a service,  the DNS server returns a single IP—the\\nservice’s cluster IP. But if you tell Kubernetes you don’t need a cluster IP for your service\\n(you do this by setting the clusterIP  field to None  in the service specification ), the DNS\\nserver will return the pod IPs in stead of the single service IP.\\n Instead of returning a single DNS A record, the DNS server will return multiple A\\nrecords for the service, each pointing to the IP of an individual pod backing the ser-vice at that moment. Clients can therefore do a simple DNS \\nA record lookup and get\\nthe IPs of all the pods that are part of the service. The client can then use that infor-mation to connect to one, many, or all of them.\\n5.6.1 Creating a headless service\\nSetting the clusterIP  field in a service spec to None  makes the service headless , as\\nKubernetes won’t assign it a cluster IP through which clients could connect to the\\npods backing it. \\n You’ll create a headless service called kubia-headless  now. The following listing\\nshows its definition.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: kubia-headlessspec:\\n  clusterIP: None       \\n  ports:  - port: 80\\n    targetPort: 8080\\n  selector:    app: kubia\\nAfter you create the service with kubectl  create , you can inspect it with kubectl  get\\nand kubectl  describe . You’ll see it has no cluster IP and its endpoints include (part of)Listing 5.18 A headless service: kubia-svc-headless.yaml\\nThis makes the \\nservice headless.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 186}, page_content='155 Using a headless service for discovering individual pods\\nthe pods matching its pod selector. I say “p art of” because your pods contain a readi-\\nness probe, so only pods that are ready wi ll be listed as endpoints of the service.\\nBefore continuing, please make sure at leas t two pods report being ready, by creating\\nthe /var/ready  file, as in the previous example:\\n$ kubectl exec <pod name> -- touch /var/ready\\n5.6.2 Discovering pods through DNS\\nWith your pods ready, you can now try performing a DNS lookup to see if you get the\\nactual pod IPs or not. You’ll need to perf orm the lookup from inside one of the pods.\\nUnfortunately, your kubia  container image doesn’t include the nslookup  (or the dig)\\nbinary, so you can’t use it to perform the DNS lookup.\\n All you’re trying to do is perform a DN S lookup from inside a pod running in the\\ncluster. Why not run a new pod based on an  image that contains the binaries you\\nneed? To perform DNS-related actions, you can use the tutum/dnsutils  container\\nimage, which is available on Docker Hub and contains both the nslookup  and the dig\\nbinaries. To run the pod, you can go th rough the whole process of creating a YAML\\nmanifest for it and passing it to kubectl  create , but that’s too much work, right?\\nLuckily, there’s a faster way.\\nRUNNING  A POD WITHOUT  WRITING  A YAML MANIFEST\\nIn chapter 1, you already created pods without writing a YAML manifest by using the\\nkubectl  run command. But this time you want to create only a pod—you don’t need\\nto create a ReplicationController to mana ge the pod. You can do that like this:\\n$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1\\n➥ --command -- sleep infinity\\npod \"dnsutils\" created\\nThe trick is in the --generator=run-pod/v1  option, which tells kubectl  to create the\\npod directly, without any kind of Replic ationController or similar behind it. \\nUNDERSTANDING  DNS A RECORDS  RETURNED  FOR A HEADLESS  SERVICE\\nLet’s use the newly created pod to perform a DNS lookup:\\n$ kubectl exec dnsutils nslookup kubia-headless\\n...\\nName:    kubia-headless.default.svc.cluster.localAddress: 10.108.1.4 \\nName:    kubia-headless.default.svc.cluster.local\\nAddress: 10.108.2.5 \\nThe DNS server returns two different IPs for the kubia-headless .default.svc\\n.cluster.local  FQDN. Those are the IPs of the tw o pods that are reporting being\\nready. You can confirm this by listing pods with kubectl  get pods  -o wide , which\\nshows the pods’ IPs.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 187}, page_content='156 CHAPTER  5Services: enabling clients to discover and talk to pods\\n This is different from what DNS returns for regular (non-headless) services, such\\nas for your kubia  service, where the returned IP is the service’s cluster IP:\\n$ kubectl exec dnsutils nslookup kubia\\n...\\nName:    kubia.default.svc.cluster.local\\nAddress: 10.111.249.153\\nAlthough headless services may seem differen t from regular services, they aren’t that\\ndifferent from the clients’ perspective. Ev en with a headless serv ice, clients can con-\\nnect to its pods by connecting to the servic e’s DNS name, as they can with regular ser-\\nvices. But with headless services, because DNS returns the pods’ IPs, clients connect\\ndirectly to the pods, instead of through the service proxy. \\nNOTE A headless services still provides load  balancing across pods, but through\\nthe DNS round-robin mechanism inst ead of through the service proxy.\\n5.6.3 Discovering all pods—even those that aren’t ready\\nYou’ve seen that only pods that are read y become endpoints of services. But some-\\ntimes you want to use the service discovery mechanism to find all pods matching the\\nservice’s label selector, even those that aren’t ready. \\n Luckily, you don’t have to resort to qu erying the Kubernetes API server. You can\\nuse the DNS lookup mechanism to find even  those unready pods. To tell Kubernetes\\nyou want all pods added to a service, rega rdless of the pod’s readiness status, you must\\nadd the following annotation to the service:\\nkind: Service\\nmetadata:\\n  annotations:\\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: \"true\"\\nWARNING As the annotation name suggests, as I’m writing this, this is an alpha\\nfeature. The Kubernetes Service API alre ady supports a new service spec field\\ncalled publishNotReadyAddresses , which will replace the tolerate-unready-\\nendpoints  annotation. In Kubernetes version 1.9.0, the field is not honored yet\\n(the annotation is what determines whether unready endpoints are included in\\nthe DNS or not). Check the documentat ion to see whether that’s changed.\\n5.7 Troubleshooting services\\nServices are a crucial Kubernetes concept and the source of frustration for many\\ndevelopers. I’ve seen many developers lose heaps of time figuring out why they can’tconnect to their pods through the service IP or FQDN. For this reason, a short look at\\nhow to troubleshoot services is in order.\\n When you’re unable to access your pods  through the service, you should start by\\ngoing through the following list:'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 188}, page_content='157 Summary\\n\\uf0a1First, make sure you’re connecting to th e service’s cluster IP from within the\\ncluster, not from the outside.\\n\\uf0a1Don’t bother pinging the service IP to figure out if the service is accessible\\n(remember, the service’s cluster IP is a virtual IP and pinging it will never work).\\n\\uf0a1If you’ve defined a readiness probe, ma ke sure it’s succeeding; otherwise the\\npod won’t be part of the service.\\n\\uf0a1To confirm that a pod is part of the service, examine the corresponding End-\\npoints object with kubectl  get endpoints .\\n\\uf0a1If you’re trying to access the service thro ugh its FQDN or a part of it (for exam-\\nple, myservice.mynamespace.svc.cluster. local or myservice.mynamespace) and\\nit doesn’t work, see if you can access it using its cluster IP instead of the FQDN.\\n\\uf0a1Check whether you’re connecting to th e port exposed by the service and not\\nthe target port.\\n\\uf0a1Try connecting to the pod IP directly to confirm your pod is accepting connec-\\ntions on the correct port.\\n\\uf0a1If you can’t even access your app through the pod’s IP, make sure your app isn’t\\nonly binding to localhost.\\nThis should help you resolve most of your  service-related problems. You’ll learn much\\nmore about how services work in chapter 11. By understanding exactly how they’re\\nimplemented, it should be much ea sier for you to troubleshoot them.\\n5.8 Summary\\nIn this chapter, you’ve learned how to cr eate Kubernetes Service resources to expose\\nthe services available in your application,  regardless of how many pod instances are\\nproviding each service. Yo u’ve learned how Kubernetes\\n\\uf0a1Exposes multiple pods that match a certain label selector under a single, stable\\nIP address and port\\n\\uf0a1Makes services accessible from inside th e cluster by default, but allows you to\\nmake the service accessible from outside the cluster by setting its type to either\\nNodePort  or LoadBalancer\\n\\uf0a1Enables pods to discover services together with their IP addresses and ports by\\nlooking up environment variables\\n\\uf0a1Allows discovery of and communication with services residing outside the\\ncluster by creating a Service resource wi thout specifying a selector, by creating\\nan associated Endpoints resource instead\\n\\uf0a1Provides a DNS CNAME  alias for external services with the ExternalName  ser-\\nvice type\\n\\uf0a1Exposes multiple HTTP services throug h a single Ingress (consuming a sin-\\ngle IP)'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 189}, page_content='158 CHAPTER  5Services: enabling clients to discover and talk to pods\\n\\uf0a1Uses a pod container’s readiness probe to determine whether a pod should or\\nshouldn’t be included as a service endpoint\\n\\uf0a1Enables discovery of pod IPs through DN S when you create a headless service\\nAlong with getting a better understanding of services, you’ve also learned how to\\n\\uf0a1Troubleshoot them\\n\\uf0a1Modify firewall rules in Google Kubernetes/Compute Engine\\n\\uf0a1Execute commands in pod containers through kubectl  exec  \\n\\uf0a1Run a bash  shell in an existing pod’s container\\n\\uf0a1Modify Kubernetes resources through the kubectl  apply  command\\n\\uf0a1Run an unmanaged ad hoc pod with kubectl  run --generator=run-pod/v1'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 190}, page_content='159Volumes: attaching\\ndisk storage to containers\\nIn the previous three chapters, we introd uced pods and other Kubernetes resources\\nthat interact with them, namely Replicat ionControllers, ReplicaSets, DaemonSets,\\nJobs, and Services. Now, we’re going back inside the pod to learn how its containerscan access external disk storage and/or share storage between them.\\n We’ve said that pods are similar to lo gical hosts where processes running inside\\nthem share resources such as CPU, RAM , network interfaces, and others. One\\nwould expect the processes to also share di sks, but that’s not the case. You’ll remem-\\nber that each container in a pod has its own isolated filesystem, because the file-\\nsystem comes from the container’s image.This chapter covers\\n\\uf0a1Creating multi-container pods\\n\\uf0a1Creating a volume to share disk storage between \\ncontainers\\n\\uf0a1Using a Git repository inside a pod\\n\\uf0a1Attaching persistent storage such as a GCE Persistent Disk to pods\\n\\uf0a1Using pre-provisioned persistent storage\\n\\uf0a1Dynamic provisioning of persistent storage'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 191}, page_content='160 CHAPTER  6Volumes: attaching disk storage to containers\\n Every new container starts off with the exact set of files that was added to the image\\nat build time. Combine this with the fact th at containers in a pod get restarted (either\\nbecause the process died or because the li veness probe signaled to Kubernetes that\\nthe container wasn’t healthy anymore) and you’ll realize that the new container will\\nnot see anything that was written to the f ilesystem by the previous container, even\\nthough the newly started container runs in the same pod.\\n In certain scenarios you want the new cont ainer to continue where the last one fin-\\nished, such as when restarting a process on  a physical machine. You may not need (or\\nwant) the whole filesystem to be persisted, but you do want to preserve the directories\\nthat hold actual data.\\n Kubernetes provides this by defining storage volumes . They aren’t top-level resources\\nlike pods, but are instead defined as a part of  a pod and share the same lifecycle as the\\npod. This means a volume is created when  the pod is started and is destroyed when\\nthe pod is deleted. Because of this, a volu me’s contents will persist across container\\nrestarts. After a container is restarted, the new container can see all the files that were\\nwritten to the volume by the previous contai ner. Also, if a pod contains multiple con-\\ntainers, the volume can be used by all of them at once. \\n6.1 Introducing volumes\\nKubernetes volumes are a component of a po d and are thus defined in the pod’s spec-\\nification—much like containers. They aren’t  a standalone Kubernetes object and can-\\nnot be created or deleted on their own. A vo lume is available to all containers in the\\npod, but it must be mounted in each cont ainer that needs to access it. In each con-\\ntainer, you can mount the volume in any location of its filesystem.\\n6.1.1 Explaining volumes in an example\\nImagine you have a pod with three containe rs (shown in figure 6.1). One container\\nruns a web server that serves HTML page s from the /var/htdocs directory and stores\\nthe access log to /var/logs. The second co ntainer runs an agent that creates HTML\\nfiles and stores them in /var/html. The th ird container processes the logs it finds in\\nthe /var/logs directory (rotates them, co mpresses them, analyzes them, or whatever).\\n Each container has a nicely defined single  responsibility, but on its own each con-\\ntainer wouldn’t be of much use. Creating a pod with these three containers without\\nthem sharing disk storage doesn’t make any sense, because the content generator\\nwould write the generated HTML files insi de its own container and the web server\\ncouldn’t access those files, as it runs in a se parate isolated container. Instead, it would\\nserve an empty directory or whatever you put in the /var/htdocs directory in its con-\\ntainer image. Similarly, the log rotator wo uld never have anything to do, because its\\n/var/logs directory would always remain empt y with nothing writing logs there. A pod\\nwith these three containers and no volumes basically does nothing.\\n But if you add two volumes to the pod an d mount them at appropriate paths inside\\nthe three containers, as shown in figure 6.2,  you’ve created a system that’s much more'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 192}, page_content='161 Introducing volumes\\nPodContainer: WebServer\\nFilesystem Webserver\\nprocess\\nWritesReads/\\nvar/\\nhtdocs/logs/\\nContainer: ContentAgent\\nFilesystem ContentAgent\\nprocess\\nWrites/\\nvar/\\nhtml/\\nContainer: LogRotator\\nFilesystem LogRotator\\nprocess\\nReads/\\nvar/\\nlogs/\\nFigure 6.1 Three containers of the \\nsame pod without shared storage\\nPodContainer: WebServer\\nFilesystem\\n/\\nvar/\\nhtdocs/logs/\\nContainer: ContentAgent\\nFilesystem\\n/\\nvar/\\nhtml/\\nContainer: LogRotator\\nFilesystem\\n/\\nvar/\\nlogs/Volume:\\npublicHtml\\nVolume:\\nlogVol\\nFigure 6.2 Three containers sharing two \\nvolumes mounted at various mount paths'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 193}, page_content='162 CHAPTER  6Volumes: attaching disk storage to containers\\nthan the sum of its parts. Linux allows you to mount a filesystem at arbitrary locations\\nin the file tree. When you do that, the cont ents of the mounted filesystem are accessi-\\nble in the directory it’s mounted into. By mounting the same volume into two contain-\\ners, they can operate on the same files. In  your case, you’re mounting two volumes in\\nthree containers. By doing this, your three containers can work together and do some-\\nthing useful. Let me explain how.\\n First, the pod has a volume called publicHtml . This volume is mounted in the Web-\\nServer  container at /var/htdocs, because that ’s the directory the web server serves\\nfiles from. The same volume is also mounted in the ContentAgent  container, but at\\n/var/html, because that’s where the agent writes  the files to. By mounting this single vol-\\nume like that, the web server will now serve the content generated by the content agent.\\n Similarly, the pod also has a volume called logVol  for storing logs. This volume is\\nmounted at /var/logs in both the WebServer  and the LogRotator  containers. Note\\nthat it isn’t mounted in the ContentAgent  container. The container cannot access its\\nfiles, even though the container and the vo lume are part of the same pod. It’s not\\nenough to define a volume in the pod; you need to define a VolumeMount  inside the\\ncontainer’s spec also, if you want th e container to be able to access it.\\n The two volumes in this example can both initially be empty, so you can use a type\\nof volume called emptyDir . Kubernetes also supports other types of volumes that are\\neither populated during initialization of th e volume from an external source, or an\\nexisting directory is mounted inside the vo lume. This process of populating or mount-\\ning a volume is performed before the pod’s containers are started. \\n A volume is bound to the lifecycle of a po d and will stay in existence only while the\\npod exists, but depending on the volume ty pe, the volume’s files may remain intact\\neven after the pod and volume disappear, and can later be moun ted into a new vol-\\nume. Let’s see what types of volumes exist.\\n6.1.2 Introducing available volume types\\nA wide variety of volume types is available.  Several are generic, while others are spe-\\ncific to the actual storage technologies us ed underneath. Don’t worry if you’ve never\\nheard of those technologies—I hadn’t heard of at least half of them. You’ll probably\\nonly use volume types for the technologies you already know and use. Here’s a list of\\nseveral of the available volume types:\\n\\uf0a1emptyDir —A simple empty directory used for storing transient data.\\n\\uf0a1hostPath —Used for mounting directories from the worker node’s filesystem\\ninto the pod.\\n\\uf0a1gitRepo —A volume initialized by checking ou t the contents of a Git repository.\\n\\uf0a1nfs—An NFS share mounted into the pod.\\n\\uf0a1gcePersistentDisk  (Google Compute Engine Persistent Disk), awsElastic-\\nBlockStore  (Amazon Web Services Elastic Block Store Volume), azureDisk\\n(Microsoft Azure Disk Volume)—Used for mounting cloud provider-specific\\nstorage.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 194}, page_content='163 Using volumes to share data between containers\\n\\uf0a1cinder , cephfs , iscsi , flocker , glusterfs , quobyte , rbd, flexVolume , vsphere-\\nVolume , photonPersistentDisk , scaleIO —Used for mounting other types of\\nnetwork storage.\\n\\uf0a1configMap , secret , downwardAPI —Special types of volumes used to expose cer-\\ntain Kubernetes resources and cluster information to the pod.\\n\\uf0a1persistentVolumeClaim —A way to use a pre- or dynamically provisioned per-\\nsistent storage. (We’ll talk about them in the last section of this chapter.)\\nThese volume types serve various purposes. You’ll learn about some of them in the\\nfollowing sections. Special types of volumes ( secret , downwardAPI , configMap ) are\\ncovered in the next two chapters, because th ey aren’t used for storing data, but for\\nexposing Kubernetes metadata to apps running in the pod. \\n A single pod can use multiple volumes of different types at the same time, and, as\\nwe’ve mentioned before, each of the pod’ s containers can either have the volume\\nmounted or not.\\n6.2 Using volumes to share data between containers\\nAlthough a volume can prove useful even when  used by a single container, let’s first\\nfocus on how it’s used for sharing data  between multiple containers in a pod.\\n6.2.1 Using an emptyDir volume\\nThe simplest volume type is the emptyDir  volume, so let’s look at  it in the first exam-\\nple of how to define a volume in a pod. As  the name suggests, the volume starts out as\\nan empty directory. The app running inside the pod can then write any files it needs\\nto it. Because the volume’s lifetime is tied to  that of the pod, the volume’s contents are\\nlost when the pod is deleted.\\n An emptyDir  volume is especially useful for sharing files between containers\\nrunning in the same pod. But it can also be  used by a single container for when a con-\\ntainer needs to write data to disk temp orarily, such as when performing a sort\\noperation on a large dataset, which can’t fi t into the available memory. The data could\\nalso be written to the container’s filesystem  itself (remember the top read-write layer\\nin a container?), but subtle differences ex ist between the two options. A container’s\\nfilesystem may not even be writable (we’ll talk about this toward the end of the book),\\nso writing to a mounted volume might be the only option. \\nUSING AN EMPTY DIR VOLUME  IN A POD\\nLet’s revisit the previous example where a we b server, a content agent, and a log rota-\\ntor share two volumes, but let’s simplify a bit. You’ll build a pod with only the web\\nserver container and the content agen t and a single volume for the HTML. \\n You’ll use Nginx as the web server and the UNIX fortune  command to generate\\nthe HTML content. The fortune  command prints out a random quote every time you\\nrun it. You’ll create a script that invokes the fortune  command every 10 seconds and\\nstores its output in index.html. You’ll find an existing Nginx image available on'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 195}, page_content='164 CHAPTER  6Volumes: attaching disk storage to containers\\nDocker Hub, but you’ll need to either create the fortune  image yourself or use the\\none I’ve already built and pushed to Docker Hub under luksa/fortune . If you want a\\nrefresher on how to build Docker  images, refer to the sidebar.\\nCREATING  THE POD\\nNow that you have the two images required to  run your pod, it’s time to create the pod\\nmanifest. Create a file called fortune-pod.ya ml with the contents shown in the follow-\\ning listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: fortunespec:\\n  containers:Building the fortune container image\\nHere’s how to build the image. Create a new directory called fortune and then inside\\nit, create a fortuneloop.sh  shell script with the following contents:\\n#!/bin/bash\\ntrap \"exit\" SIGINT\\nmkdir /var/htdocswhile :\\ndo\\n  echo $(date) Writing fortune to /var/htdocs/index.html  /usr/games/fortune > /var/htdocs/index.html\\n  sleep 10\\ndone\\nThen, in the same directory, create a file called Dockerfile containing the following:\\nFROM ubuntu:latest\\nRUN apt-get update ; apt-get -y install fortune\\nADD fortuneloop.sh /bin/fortuneloop.shENTRYPOINT /bin/fortuneloop.sh\\nThe image is based on the ubuntu:latest  image, which doesn’t include the fortune\\nbinary by default. That’s why in the seco nd line of the Dockerfile you install it with\\napt-get . After that, you add the fortuneloop.sh  script to the image’s /bin folder.\\nIn the last line of the Docke rfile, you specify that the fortuneloop.sh  script should\\nbe executed when the image is run.\\nAfter preparing both files, build and upload the image to Docker Hub with the following\\ntwo commands (replace luksa  with your own Docker Hub user ID):\\n$ docker build -t luksa/fortune .\\n$ docker push luksa/fortune\\nListing 6.1 A pod with two containers sharing the same volume: fortune-pod.yaml'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 196}, page_content='165 Using volumes to share data between containers\\n  - image: luksa/fortune                   \\n    name: html-generator                   \\n    volumeMounts:                              - name: html                           \\n      mountPath: /var/htdocs               \\n  - image: nginx:alpine                       name: web-server                      \\n    volumeMounts:                         \\n    - name: html                          \\n      mountPath: /usr/share/nginx/html    \\n      readOnly: true                          ports:\\n    - containerPort: 80\\n      protocol: TCP  volumes:                 \\n  - name: html             \\n    emptyDir: {}           \\nThe pod contains two containers and a single volume that’s mounted in both of\\nthem, yet at different paths. When the html-generator  container starts, it starts writ-\\ning the output of the fortune  command to the /var/htdocs/index.html file every 10\\nseconds. Because the volume is mounted at /var/htdocs, the index.html file is writ-\\nten to the volume instead of the container’s top layer. As soon as the web-server  con-\\ntainer starts, it starts serving whatever HTML files are in the /usr/share/nginx/html\\ndirectory (this is the default directory Ngin x serves files from). Because you mounted\\nthe volume in that exact location, Nginx wi ll serve the index.html file written there\\nby the container running the fortune loop. Th e end effect is that a client sending an\\nHTTP request to the pod on port 80 wi ll receive the current fortune message as\\nthe response. \\nSEEING THE POD IN ACTION\\nTo see the fortune message, you need to enab le access to the pod. You’ll do that by\\nforwarding a port from your local machine to the pod:\\n$ kubectl port-forward fortune 8080:80\\nForwarding from 127.0.0.1:8080 -> 80\\nForwarding from [::1]:8080 -> 80\\nNOTE As an exercise, you can also expose  the pod through a service instead\\nof using port forwarding.\\nNow you can access the Nginx server throug h port 8080 of your local machine. Use\\ncurl  to do that:\\n$ curl http://localhost:8080\\nBeware of a tall blond man with one black shoe.\\nIf you wait a few seconds and send another request, you should receive a different\\nmessage. By combining two containers, you created a simple app to see how a volume\\ncan glue together two containers and enhance what each of them does.The first container is called html-generator \\nand runs the luksa/fortune image.\\nThe volume called html is mounted \\nat /var/htdocs in the container.\\nThe second container is called web-server \\nand runs the nginx:alpine image.\\nThe same volume as above is \\nmounted at /usr/share/nginx/html as read-only.\\nA single emptyDir volume \\ncalled html that’s mounted in the two containers above'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 197}, page_content='166 CHAPTER  6Volumes: attaching disk storage to containers\\nSPECIFYING  THE MEDIUM  TO USE FOR THE EMPTY DIR\\nThe emptyDir  you used as the volume was created on the actual disk of the worker\\nnode hosting your pod, so its performance depends on the type of the node’s disks.\\nBut you can tell Kubernetes to create the emptyDir  on a tmpfs filesystem (in memory\\ninstead of on disk). To do this, set the emptyDir ’s medium  to Memory  like this:\\nvolumes:\\n  - name: html\\n    emptyDir:      medium: Memory    \\nAn emptyDir  volume is the simplest type of volume, but other types build upon it.\\nAfter the empty directory is created, they po pulate it with data. One such volume type\\nis the gitRepo  volume type, which we’ll introduce next.\\n6.2.2 Using a Git repository as the starting point for a volume \\nA gitRepo  volume is basically an emptyDir  volume that gets populated by cloning a\\nGit repository and checking out a specific revision when the pod is starting up (but\\nbefore its containers are created). Figure 6.3 shows how this unfolds.\\nNOTE After the gitRepo  volume is created, it isn’t kept in sync with the repo\\nit’s referencing. The files in the vo lume will not be updated when you push\\nadditional commits to the Git repository . However, if your pod is managed by\\na ReplicationController, deleting the po d will result in a new pod being cre-\\nated and this new pod’s volume will  then contain the latest commits. \\nFor example, you can use a Git repository to  store static HTML files of your website\\nand create a pod containing a web server container and a gitRepo  volume. Every time\\nthe pod is created, it pulls the latest versio n of your website and starts serving it. TheThis emptyDir’s \\nfiles should be \\nstored in memory.\\nPodContainerUser\\ngitRepo\\nvolume1. User (or a replication\\ncontroller) creates podwith gitRepo volume\\n2. Kubernetes creates\\nan empty directory andclones the speciﬁed Gitrepository into it\\n3. The pod’s container is started\\n(with the volume mounted atthe mount path)\\nRepository\\nFigure 6.3 A gitRepo  volume is an emptyDir  volume initially populated  with the contents of a \\nGit repository.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 198}, page_content='167 Using volumes to share data between containers\\nonly drawback to this is that you need to  delete the pod every time you push changes\\nto the gitRepo  and want to start serving the new version of the website. \\n Let’s do this right now. It’s not th at different from what you did before. \\nRUNNING  A WEB SERVER  POD SERVING  FILES FROM A CLONED  GIT REPOSITORY\\nBefore you create your pod, yo u’ll need an actual Git repository with HTML files in it.\\nI’ve created a repo on GitHub at https:/ /github.com/luksa/kubia-website-example.git .\\nYou’ll need to fork it (create your own co py of the repo on GitHub) so you can push\\nchanges to it later. \\n Once you’ve created your fork, you can move on to creating the pod. This time,\\nyou’ll only need a single Nginx container and a single gitRepo  volume in the pod (be\\nsure to point the gitRepo  volume to your own fork of my repository), as shown in the\\nfollowing listing.\\napiVersion: v1\\nkind: Podmetadata:\\n  name: gitrepo-volume-pod\\nspec:  containers:\\n  - image: nginx:alpine\\n    name: web-server    volumeMounts:\\n    - name: html\\n      mountPath: /usr/share/nginx/html      readOnly: true\\n    ports:\\n    - containerPort: 80      protocol: TCP\\n  volumes:\\n  - name: html\\n    gitRepo:                     \\n      repository: https://github.com/luksa/kubia-website-example.git         revision: master                     \\n      directory: .      \\nWhen you create the pod, the volume is first initialized as an empty directory and then\\nthe specified Git repository is cloned into  it. If you hadn’t set the directory to . (dot),\\nthe repository would have been cloned in to the kubia-website-example subdirectory,\\nwhich isn’t what you want. You want the repo  to be cloned into the root directory of\\nyour volume. Along with the repository, you also specified you want Kubernetes to\\ncheck out whatever revision the master bran ch is pointing to at the time the volume\\nis created. \\n With the pod running, you can try hitting it  through port forwarding, a service, or by\\nexecuting the curl  command from within the pod (or any other pod inside the cluster). Listing 6.2 A pod using a gitRepo  volume: gitrepo-volume-pod.yaml\\nYou’re creating a \\ngitRepo volume.The volume will clone\\nthis Git repository.\\nThe master branch \\nwill be checked out.You want the repo to \\nbe cloned into the root dir of the volume.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 199}, page_content='168 CHAPTER  6Volumes: attaching disk storage to containers\\nCONFIRMING  THE FILES AREN’T KEPT IN SYNC WITH THE GIT REPO\\nNow you’ll make changes to the index.html file in your GitHub repository. If you\\ndon’t use Git locally, you can edit the file on GitHub directly—click on the file in yourG i t H u b  r e p o s i t o r y  t o  o p e n  i t  a n d  t h e n  c l i c k  o n  t h e  p e n c i l  i c o n  t o  s t a r t  e d i t i n g  i t .\\nChange the text and then commit the change s by clicking the button at the bottom.\\n The master branch of the Git repository now includes the changes you made to the\\nHTML file. These changes will not be visibl e on your Nginx web server yet, because\\nthe \\ngitRepo  volume isn’t kept in sync with the Git repository. You can confirm this by\\nhitting the pod again. \\n To see the new version of the website,  you need to delete the pod and create\\nit again. Instead of having to delete the pod every time you make changes, you could\\nrun an additional process, which keeps your  volume in sync with the Git repository.\\nI won’t explain in detail how to do this. In stead, try doing this yourself as an exer-\\ncise, but here are a few pointers.\\nINTRODUCING  SIDECAR  CONTAINERS\\nThe Git sync process shouldn’t run in the sa me container as the Nginx web server, but\\nin a second container: a sidecar container . A sidecar container is a container that aug-\\nments the operation of the main container of the pod. You add a sidecar to a pod so\\nyou can use an existing container image instead of cramming additional logic into the\\nmain app’s code, which would make it  overly complex and less reusable. \\n To find an existing container image, which keeps a local directory synchronized\\nwith a Git repository, go to Docker Hub and search for “git sync.” You’ll find many\\nimages that do that. Then use the image in  a new container in the pod from the previ-\\nous example, mount the pod’s existing gitRepo  volume in the new container, and\\nconfigure the Git sync container to keep the f iles in sync with your Git repo. If you set\\neverything up correctly, you should see that the files the web server is serving are keptin sync with your GitHub repo. \\nNOTE An example in chapter 18 includes using a Git sync container like the\\none explained here, so you can wait un til you reach chapter 18 and follow the\\nstep-by-step instructions then instead of  doing this exercise on your own now. \\nUSING A GITREPO VOLUME  WITH PRIVATE  GIT REPOSITORIES\\nThere’s one other reason for having to re sort to Git sync sidecar containers. We\\nhaven’t talked about whether you can use a gitRepo  volume with a private Git repo. It\\nturns out you can’t. The current consensus among Kubernetes developers is to keep\\nthe gitRepo  volume simple and not add any suppo rt for cloning private repositories\\nthrough the SSH protocol, because that would require adding additional config\\noptions to the gitRepo  volume. \\n If you want to clone a private Git repo into your container, you should use a git-\\nsync sidecar or a similar method instead of a gitRepo  volume.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 200}, page_content='169 Accessing files on the worker node’s filesystem\\nWRAPPING  UP THE GITREPO VOLUME\\nA gitRepo  volume, like the emptyDir  volume, is basically a dedicated directory cre-\\nated specifically for, and used exclusivel y by, the pod that contains the volume. When\\nthe pod is deleted, the volume and its cont ents are deleted. Other types of volumes,\\nhowever, don’t create a new directory, but instead mount an existing external direc-\\ntory into the pod’s container’s filesystem. The contents of that volume can survivemultiple pod instantiations. We’ll lear n about those types of volumes next.\\n6.3 Accessing files on the worker node’s filesystem\\nMost  pods should be oblivious of their host node, so they shouldn’t access any files on\\nthe node’s filesystem. But certain system-lev el pods (remember, these will usually be\\nmanaged by a DaemonSet) do need to either read the node’s files or use the node’s\\nfilesystem to access the node’s devices th rough the filesystem. Kubernetes makes this\\npossible through a hostPath  volume. \\n6.3.1 Introducing the hostPath volume\\nA hostPath  volume points to a specific file or directory on the node’s filesystem (see\\nfigure 6.4). Pods running on the same node and using the same path in their host-\\nPath  volume see the same files.\\nhostPath  volumes are the first type of persistent storage we’re introducing, because\\nboth the gitRepo  and emptyDir  volumes’ contents get de leted when a pod is torn\\ndown, whereas a hostPath  volume’s contents don’t. If  a pod is deleted and the next\\npod uses a hostPath  volume pointing to the same path on the host, the new pod will\\nsee whatever was left behind by the previous pod, but only if it’s scheduled to the same\\nnode as the first pod.Node 1\\nPod\\nhostPath\\nvolumePod\\nhostPath\\nvolumeNode 2\\nPod\\nhostPath\\nvolume\\n/some/path/on/host /some/path/on/host\\nFigure 6.4 A hostPath  volume mounts a file or directory on the worker node into \\nthe container’s filesystem.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 201}, page_content='170 CHAPTER  6Volumes: attaching disk storage to containers\\n If you’re thinking of using a hostPath  volume as the place to store a database’s\\ndata directory, think again. Because the vo lume’s contents are stored on a specific\\nnode’s filesystem, when the database pod ge ts rescheduled to another node, it will no\\nlonger see the data. This explains why it’s not a good idea to use a hostPath  volume\\nfor regular pods, because it makes the pod se nsitive to what node it’s scheduled to.\\n6.3.2 Examining system pods that use hostPath volumes\\nLet’s see how a hostPath  volume can be used properly . Instead of creating a new pod,\\nlet’s see if any existing system-wide pods ar e already using this type of volume. As you\\nmay remember from one of the previous ch apters, several such pods are running in\\nthe kube-system  namespace. Let’s list them again:\\n$ kubectl get pod s --namespace kube-system\\nNAME                          READY     STATUS    RESTARTS   AGE\\nfluentd-kubia-4ebc2f1e-9a3e   1/1       Running   1          4d\\nfluentd-kubia-4ebc2f1e-e2vz   1/1       Running   1          31d...\\nPick the first one and see what kinds of volu mes it uses (shown in the following listing).\\n$ kubectl describe po fluentd- kubia-4ebc2f1e-9a3e --namespace kube-system\\nName:           fluentd-cloud-logging-gke-kubia-default-pool-4ebc2f1e-9a3e\\nNamespace:      kube-system\\n...\\nVolumes:  varlog:\\n    Type:       HostPath (bare host directory volume)\\n    Path:       /var/log\\n  varlibdockercontainers:\\n    Type:       HostPath (bare host directory volume)    Path:       /var/lib/docker/containers\\nTIP If you’re using Minikube, try the kube-addon-man ager-minikube  pod.\\nAha! The pod uses two hostPath  volumes to gain access to the node’s /var/log and\\nthe /var/lib/docker/containers directories. You’d think you were lucky to find a podusing a \\nhostPath  volume on the first try, but not really (at least not on GKE). Check\\nthe other pods, and you’ll see most use this type of volume either to access the node’s\\nlog files, kubeconfig (the Kubernetes config file), or the CA certificates.\\n If you inspect the other pods, yo u’ll see none of them uses the hostPath  volume\\nfor storing their own data. They all use it to get access to the node’s data. But as we’ll\\nsee later in the chapter, hostPath  volumes are often used for trying out persistent stor-\\nage in single-node clusters, such as the on e created by Minikube. Read on to learn\\nabout the types of volumes you should use for storing persistent data properly even ina multi-node cluster.Listing 6.3  A pod using hostPath  volumes to access the node’s logs'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 202}, page_content='171 Using persistent storage\\nTIP Remember to use hostPath  volumes only if you need to read or write sys-\\ntem files on the node. Never use them to persist data across pods. \\n6.4 Using persistent storage\\nWhen an application running in a pod need s to persist data to disk and have that\\nsame data available even when the pod is rescheduled to another node, you can’t use\\nany of the volume types we’ve mentioned so fa r. Because this data needs to be accessi-\\nble from any cluster node, it must be stored  on some type of network-attached stor-\\nage (NAS).\\n To learn about volumes that allow persisting  data, you’ll create a pod that will run\\nthe MongoDB document-oriented NoSQL database. Running a database pod without\\na volume or with a non-persistent volume  doesn’t make sense, except for testing\\npurposes, so you’ll add an appropriate type of volume to the pod and mount it in the\\nMongoDB container. \\n6.4.1 Using a GCE Persistent Disk in a pod volume\\nIf you’ve been running these examples on Google Kubernetes Engine, which runsyour cluster nodes on Google Compute En gine (GCE), you’ll use a GCE Persistent\\nDisk as your underlying storage mechanism. \\n In the early versions, Kubernetes didn’t provision the underlying storage automati-\\ncally—you had to do that manually. Automatic provisioning is now possible, and you’lllearn about it later in the chapter, but first, you’ll start by provisioning the storage\\nmanually. It will give you a chance to le arn exactly what’s going on underneath. \\nCREATING  A GCE P ERSISTENT  DISK\\nYou’ll start by creating the GCE persistent disk  first. You need to create it in the same\\nzone as your Kubernetes cluster. If you don’t remember what zone you created the\\ncluster in, you can see it by listing your Kubernetes clusters with the gcloud  command\\nlike this:\\n$ gcloud container clusters list\\nNAME   ZONE            MASTER_VERSION  MASTER_IP       ...\\nkubia  europe-west1-b  1.2.5           104.155.84.137  ...\\nThis shows you’ve create d your cluster in zone europe-west1-b , so you need to create\\nthe GCE persistent disk in the same zone as well. You create the disk like this:\\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b mongodb\\nWARNING: You have selected a disk size of under [200GB]. This may result in \\npoor I/O performance. For more information, see: \\nhttps://developers.google.com/compute/docs/disks#pdperformance.\\nCreated [https://www.googleapi s.com/compute/v1/projects/rapid-pivot-\\n136513/zones/europe-west1-b/disks/mongodb].\\nNAME     ZONE            SIZE_GB  TYPE         STATUSmongodb  europe-west1-b  1        pd-standard  READY'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 203}, page_content='172 CHAPTER  6Volumes: attaching disk storage to containers\\nThis command creates a 1 GiB larg e GCE persistent disk called mongodb . You can\\nignore the warning about the disk size, beca use you don’t care about the disk’s perfor-\\nmance for the tests you’re about to run.\\nCREATING  A POD USING  A GCEPERSISTENT DISK VOLUME\\nNow that you have your physical storage pr operly set up, you can use it in a volume\\ninside your MongoDB pod. You’re going to  prepare the YAML for the pod, which is\\nshown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: mongodb spec:\\n  volumes:\\n  - name: mongodb-data              gcePersistentDisk:           \\n      pdName: mongodb            \\n      fsType: ext4               containers:\\n  - image: mongo\\n    name: mongodb    volumeMounts:                \\n    - name: mongodb-data         \\n      mountPath: /data/db          ports:\\n    - containerPort: 27017\\n      protocol: TCP\\nNOTE If you’re using Minikube, you can’t use a GCE Persistent Disk, but you\\ncan deploy mongodb-pod-hostpath.yaml , which uses a hostPath  volume\\ninstead of a GCE PD.\\nThe pod contains a single container and a single volume backed by the GCE Per-\\nsistent Disk you’ve created (as shown in figure 6.5). You’re mounting the volume\\ninside the container at /data/db, becaus e that’s where MongoDB stores its data.Listing 6.4 A pod using a gcePersistentDisk  volume: mongodb-pod-gcepd.yaml\\nThe name\\nof the\\nvolume\\n(also\\nreferenced\\nwhen\\nmounting\\nthe volume)The type of the volume \\nis a GCE Persistent Disk.\\nThe name of the persistent \\ndisk must match the actual \\nPD you created earlier.\\nThe filesystem type is EXT4 \\n(a type of Linux filesystem).\\nThe path where MongoDB \\nstores its data\\nPod: mongodb\\nContainer: mongodb\\nvolumeMounts:\\nname: mongodb-datamountPath: /data/dbgcePersistentDisk:\\npdName: mongodb\\nGCE\\nPersistent Disk:\\nmongodbVolume:\\nmongodb\\nFigure 6.5 A pod with a single container running  MongoDB, which mounts a volume referencing an \\nexternal GCE Persistent Disk'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 204}, page_content='173 Using persistent storage\\nWRITING  DATA TO THE PERSISTENT  STORAGE  BY ADDING  DOCUMENTS  TO YOUR MONGODB DATABASE\\nNow that you’ve created the pod and the co ntainer has been started, you can run the\\nMongoDB shell inside the container and use it  to write some data to the data store.\\n You’ll run the shell as shown in the following listing.\\n$ kubectl exec -it mongodb mongo\\nMongoDB shell version: 3.2.8\\nconnecting to: mongodb://127.0.0.1:27017Welcome to the MongoDB shell.\\nFor interactive help, type \"help\".\\nFor more comprehensive documentation, see    http://docs.mongodb.org/\\nQuestions? Try the support group\\n    http://groups.google.com/group/mongodb-user...\\n> \\nMongoDB allows storing JSON documents, so yo u’ll store one to see if it’s stored per-\\nsistently and can be retrieved after the pod is re-created. Insert a new JSON document\\nwith the following commands: \\n> use mystore\\nswitched to db mystore\\n> db.foo.insert({name:\\'foo\\'})\\nWriteResult({ \"nInserted\" : 1 })\\nYou’ve inserted a simple JSON document with a single property ( name:  ’foo’) . Now,\\nuse the find()  command to see the document you inserted:\\n> db.foo.find()\\n{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\\nThere it is. The document should be stor ed in your GCE persistent disk now. \\nRE-CREATING  THE POD AND VERIFYING  THAT IT CAN READ THE DATA PERSISTED  BY THE PREVIOUS  POD\\nYou can now exit the mongodb  shell (type exit  and press Enter), and then delete the\\npod and recreate it:\\n$ kubectl delete pod mongodb\\npod \"mongodb\" deleted\\n$ kubectl create -f mongodb-pod-gcepd.yamlpod \"mongodb\" created\\nThe new pod uses the exact same GCE persistent disk as the previous pod, so the\\nMongoDB container running inside it should see the exact same data, even if the pod\\nis scheduled to a different node.\\nTIP You can see what node a pod is scheduled to by running kubectl  get po\\n-o wide .Listing 6.5 Entering the MongoDB shell inside the mongodb  pod'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 205}, page_content='174 CHAPTER  6Volumes: attaching disk storage to containers\\nOnce the container is up, you can again run the MongoDB shell and check to see if the\\ndocument you stored earlier can still be retr ieved, as shown in the following listing.\\n$ kubectl exec -it mongodb mongo\\nMongoDB shell version: 3.2.8\\nconnecting to: mongodb://127.0.0.1:27017\\nWelcome to the MongoDB shell....\\n> use mystore\\nswitched to db mystore> db.foo.find()\\n{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\\nAs expected, the data is still there, even though you deleted the pod and re-created it.\\nThis confirms you can use a GCE persistent disk to persist data across multiple pod\\ninstances. \\n You’re done playing with the MongoDB pod,  so go ahead and delete it again, but\\nhold off on deleting the underlying GCE persistent disk. You’ll use it again later in\\nthe chapter.\\n6.4.2 Using other types of volumes with underlying persistent storage\\nThe reason you created the GCE Persistent Disk volume is because your Kubernetes\\ncluster runs on Google Kubernetes Engine. When you run your cluster elsewhere, you\\nshould use other types of volumes, depe nding on the underlying infrastructure.\\n If your Kubernetes cluster is running on Amazon’s AWS EC2, for example, you can\\nuse an awsElasticBlockStore  volume to provide persistent storage for your pods. If\\nyour cluster runs on Microsoft Azure, you can use the azureFile  or the azureDisk\\nvolume. We won’t go into detail on how to do that here, but it’s virtually the same as in\\nthe previous example. First, you need to create the actual underlying storage, and\\nthen set the appropriate properties in the volume definition.\\nUSING AN AWS E LASTIC  BLOCK STORE VOLUME\\nFor example, to use an AWS elastic block store instead of the GCE Persistent Disk,\\nyou’d only need to change the volume defini tion as shown in the following listing (see\\nthose lines printed in bold).\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: mongodb \\nspec:\\n  volumes:                         - name: mongodb-data           \\n    awsElasticBlockStore:          Listing 6.6 Retrieving MongoDB’s persisted data in a new pod\\nListing 6.7 A pod using an awsElasticBlockStore  volume: mongodb-pod-aws.yaml\\nUsing awsElasticBlockStore \\ninstead of gcePersistentDisk'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 206}, page_content='175 Using persistent storage\\n      volumeId: my-volume          \\n      fsType: ext4       \\n  containers:  - ...\\nUSING AN NFS VOLUME\\nIf your cluster is running on your own set of  servers, you have a vast array of other sup-\\nported options for mounting external stor age inside your volu me. For example, to\\nmount a simple NFS share, you only need to specify the NFS server and the path\\nexported by the server, as shown in the following listing.\\n  volumes:                       \\n  - name: mongodb-data               nfs:                     \\n      server: 1.2.3.4         \\n      path: /some/path     \\nUSING OTHER  STORAGE  TECHNOLOGIES\\nOther supported options include iscsi  for mounting an ISCSI disk resource, glusterfs\\nfor a GlusterFS mount, rbd for a RADOS Block Device, flexVolume , cinder , cephfs ,\\nflocker , fc (Fibre Channel), and others. You don’t need to know all of them if you’re\\nnot using them. They’re mentioned here to  show you that Kubernetes supports a\\nbroad range of storage technologies and you can use whichever you prefer and are\\nused to.\\n To see details on what properties you need to set for each of these volume types,\\nyou can either turn to the Kubernetes AP I definitions in the Kubernetes API refer-\\nence or look up the information through kubectl  explain , as shown in chapter 3. If\\nyou’re already familiar with a partic ular storage technology, using the explain  com-\\nmand should allow you to easily figure ou t how to mount a volume of the proper type\\nand use it in your pods.\\n But does a developer need to know all th is stuff? Should a developer, when creat-\\ning a pod, have to deal with infrastructure -related storage details, or should that be\\nleft to the cluster administrator? \\n Having a pod’s volumes refer to the actu al underlying infrastructure isn’t what\\nKubernetes is about, is it? For example, fo r a developer to have to specify the host-\\nname of the NFS server feels wrong. And that’s not even the worst thing about it. \\n Including this type of infrastructure-r elated information into a pod definition\\nmeans the pod definition is pretty much ti ed to a specific Kubernetes cluster. You\\ncan’t use the same pod definition in anothe r one. That’s why using volumes like this\\nisn’t the best way to attach persistent storag e to your pods. You’ll learn how to improve\\non this in the next section.Listing 6.8 A pod using an nfs volume: mongodb-pod-nfs.yamlSpecify the ID of the EBS \\nvolume you created. The filesystem type \\nis EXT4 as before.\\nThis volume is backed \\nby an NFS share.\\nThe IP of the \\nNFS server The path exported \\nby the server'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 207}, page_content='176 CHAPTER  6Volumes: attaching disk storage to containers\\n6.5 Decoupling pods from the underlying storage technology\\nAll the persistent volume types we’ve explored  so far have required the developer of the\\npod to have knowledge of the actual network storage infrastructure available in the clus-ter. For example, to create a NFS-backed volume, the developer has to know the actual\\nserver the NFS export is located on. This is  against the basic idea of Kubernetes, which\\naims to hide the actual infrastructure from both the application and its developer, leav-\\ning them free from worrying about the specif ics of the infrastructure and making apps\\nportable across a wide array of clo ud providers and on-premises datacenters.\\n Ideally, a developer deploying their a pps on Kubernetes should never have to\\nknow what kind of storage technology is used underneath, the same way they don’t\\nhave to know what type of physical servers are being used to run their pods. Infrastruc-\\nture-related dealings should be the so le domain of the cluster administrator.\\n When a developer needs a certain amount of persistent storage for their applica-\\ntion, they can request it from Kubernetes , the same way they can request CPU, mem-\\nory, and other resources when creating a po d. The system administrator can configure\\nthe cluster so it can give the apps what they request.\\n6.5.1 Introducing PersistentVolumes and PersistentVolumeClaims\\nTo enable apps to request storage in a Kube rnetes cluster without having to deal with\\ninfrastructure specifics, two new resource s were introduced. They are Persistent-\\nVolumes and PersistentVolumeClaims. The names may be a bit misleading, because as\\nyou’ve seen in the previous few sections , even regular Kubernetes volumes can be\\nused to store persistent data. \\n Using a PersistentVolume inside a pod is a little more complex than using a regular\\npod volume, so let’s illustrate how pods, PersistentVolumeClaims, PersistentVolumes,\\nand the actual underlying storage re late to each other in figure 6.6.\\nPodAdmin\\nVolume1. Cluster admin sets up some type of\\nnetwork storage (NFS export or similar)\\n2. Admin then creates a PersistentVolume (PV)\\nby posting a PV descriptor to the Kubernetes APINFS\\nexport\\nPersistent\\nVolume\\nUser\\nPersistent\\nVolumeClaim3. User creates a\\nPersistentVolumeClaim (PVC)\\n4. Kubernetes ﬁnds a PV of\\nadequate size and accessmode and binds the PVCto the PV5. User creates a\\npod with a volumereferencing the PVC\\nFigure 6.6 PersistentVolumes are provisioned by cluster admins and consumed by pods \\nthrough PersistentVolumeClaims.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 208}, page_content='177 Decoupling pods from the underlying storage technology\\nInstead of the developer adding a technology -specific volume to their pod, it’s the\\ncluster administrator who sets up the unde rlying storage and then registers it in\\nKubernetes by creating a PersistentVolu me resource through the Kubernetes API\\nserver. When creating the PersistentVolume, the admin specifies its size and the access\\nmodes it supports. \\n When a cluster user needs to use persistent  storage in one of their pods, they first\\ncreate a PersistentVolumeClaim manifest, specifying the minimum size and the accessmode they require. The user then submits the PersistentVolumeClaim manifest to the\\nKubernetes API server, and Kubernetes finds the appropriate PersistentVolume and\\nbinds the volume to the claim. \\n The PersistentVolumeClaim can then be us ed as one of the volumes inside a pod.\\nOther users cannot use the same PersistentVolu me until it has been released by delet-\\ning the bound PersistentVolumeClaim.\\n6.5.2 Creating a PersistentVolume\\nLet’s revisit the MongoDB example, but un like before, you won’t reference the GCE\\nPersistent Disk in the pod directly. Instead,  y o u ’ll  f ir s t a s s u me t h e r o le  of  a  c lu s te r\\nadministrator and create a PersistentVolume  backed by the GCE Persistent Disk. Then\\nyou’ll assume the role of the application developer and first claim the PersistentVol-\\nume and then use it inside your pod.\\n In section 6.4.1 you set up the physical storage by provisioning the GCE Persistent\\nDisk, so you don’t need to do that again. Al l you need to do is create the Persistent-\\nVolume resource in Kubernetes by preparin g the manifest shown in the following list-\\ning and posting it to the API server.\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: mongodb-pvspec:\\n  capacity:                  \\n    storage: 1Gi               accessModes:                              \\n  - ReadWriteOnce                           \\n  - ReadOnlyMany                              persistentVolumeReclaimPolicy: Retain    \\n  gcePersistentDisk:                      \\n    pdName: mongodb                           fsType: ext4                          Listing 6.9 A gcePersistentDisk  PersistentVolume: mongodb-pv-gcepd.yaml\\nDefining the \\nPersistentVolume’s size It can either be mounted by a single \\nclient for reading and writing or by multiple clients for reading only.\\nAfter the claim is released, \\nthe PersistentVolume \\nshould be retained (not \\nerased or deleted).The PersistentVolume is \\nbacked by the GCE Persistent Disk you created earlier.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 209}, page_content='178 CHAPTER  6Volumes: attaching disk storage to containers\\nNOTE If you’re using Minikube, create the PV using the mongodb-pv-host-\\npath.yaml file.\\nWhen creating a PersistentVolume, the admini strator needs to tell Kubernetes what its\\ncapacity is and whether it can be read from  and/or written to by a single node or by\\nmultiple nodes at the same time. They also ne ed to tell Kubernetes what to do with the\\nPersistentVolume when it’s released (when the PersistentVolumeClaim it’s bound to is\\ndeleted). And last, but certainly not least, th ey need to specify the type, location, and\\nother properties of the actual storage this PersistentVolume is backed by. If you look\\nclosely, this last part is exactly the same as earlier, when you referenced the GCE Per-\\nsistent Disk in the pod volume directly  (shown again in the following listing).\\nspec:\\n  volumes:                         - name: mongodb-data           \\n    gcePersistentDisk:           \\n      pdName: mongodb                  fsType: ext4               \\n  ...\\nAfter you create the PersistentVolume with the kubectl  create  command, it should\\nbe ready to be claimed. See if it is by listing all PersistentVolumes:\\n$ kubectl get pv\\nNAME         CAPACITY   RECLAIMPOLICY   ACCESSMODES   STATUS      CLAIM\\nmongodb-pv   1Gi        Retain          RWO,ROX       Available   \\nNOTE Several columns are omitted. Also, pv is used as a shorthand for\\npersistentvolume .\\nAs expected, the PersistentVolume is shown as Available, because you haven’t yet cre-\\nated the PersistentVolumeClaim. \\nNOTE PersistentVolumes don’t belong to  any namespace (see figure 6.7).\\nThey’re cluster-level resources like nodes.Listing 6.10 Referencing a GCE PD in a pod’s volume'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 210}, page_content='179 Decoupling pods from the underlying storage technology\\n6.5.3 Claiming a PersistentVolume by creating a \\nPersistentVolumeClaim\\nNow let’s lay down our admin hats and put our developer hats back on. Say you need\\nto deploy a pod that requires persistent storage. You’ll use the PersistentVolume you\\ncreated earlier. But you can’t use it directly  in the pod. You need to claim it first.\\n Claiming a PersistentVolume is a completely separate process from creating a pod,\\nbecause you want the same PersistentVolumeClaim to stay available even if the pod isrescheduled (remember, rescheduling means the previous pod is deleted and a new\\none is created). \\nCREATING  A PERSISTENT VOLUME CLAIM\\nYou’ll create the claim now. You need to  prepare a PersistentVolumeClaim manifest\\nlike the one shown in the following listing an d post it to the Kubernetes API through\\nkubectl  create .\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: mongodb-pvc          Listing 6.11 A PersistentVolumeClaim : mongodb-pvc.yamlPod(s) Pod(s)Persistent\\nVolumePersistent\\nVolumePersistent\\nVolumePersistent\\nVolume...\\nUser A\\nPersistent\\nVolume\\nClaim(s)Persistent\\nVolume\\nClaim(s)Namespace A\\nUser BNamespace B\\nNode Node Node Node Node NodePersistent\\nVolume\\nFigure 6.7 PersistentVolumes, like cluster Nodes , don’t belong to any namespace, unlike pods and \\nPersistentVolumeClaims.\\nThe name of your claim—you’ll \\nneed this later when using the \\nclaim as the pod’s volume.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 211}, page_content='180 CHAPTER  6Volumes: attaching disk storage to containers\\nspec:\\n  resources:\\n    requests:                      storage: 1Gi           \\n  accessModes:              \\n  - ReadWriteOnce             storageClassName: \"\"     \\nAs soon as you create the claim, Kubern etes finds the appropriate PersistentVolume\\nand binds it to the claim. The PersistentVolume’s capacity must be large enough toaccommodate what the claim requests. Addi tionally, the volume’s access modes must\\ninclude the access modes reques ted by the claim. In your ca se, the claim requests 1 GiB\\nof storage and a \\nReadWriteOnce  access mode. The PersistentVolume you created ear-\\nlier matches those two requirements so it is  bound to your claim. You can see this by\\ninspecting the claim.\\nLISTING  PERSISTENT VOLUME CLAIMS\\nList all PersistentVolumeClaims to see the state of your PVC:\\n$ kubectl get pvc\\nNAME          STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE\\nmongodb-pvc   Bound     mongodb-pv   1Gi        RWO,ROX       3s\\nNOTE We’re using pvc as a shorthand for persistentvolumeclaim .\\nThe claim is shown as Bound  to PersistentVolume mongodb-pv . Note the abbreviations\\nused for the access modes:\\n\\uf0a1RWO—ReadWriteOnce —Only a single node can mount the volume for reading\\nand writing.\\n\\uf0a1ROX—ReadOnlyMany —Multiple nodes can mount the volume for reading.\\n\\uf0a1RWX—ReadWriteMany —Multiple nodes can mount the volume for both reading\\nand writing.\\nNOTERWO, ROX, and RWX pertain to the number of worker nodes that can use\\nthe volume at the same time, not to the number of pods!\\nLISTING  PERSISTENT VOLUMES\\nYou can also see that the PersistentVolume is now Bound  and no longer Available  by\\ninspecting it with kubectl  get:\\n$ kubectl get pv\\nNAME         CAPACITY   ACCESSMODES   STATUS   CLAIM                 AGE\\nmongodb-pv   1Gi        RWO,ROX       Bound    default/mongodb-pvc   1m\\nThe PersistentVolume shows it’s bound to claim default/mongodb-pvc . The default\\npart is the namespace the claim resides in  (you created the claim in the defaultRequesting 1 GiB of storage\\nYou want the storage to support a single \\nclient (performing bo th reads and writes).\\nYou’ll learn about this in the section \\nabout dynamic provisioning.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 212}, page_content='181 Decoupling pods from the underlying storage technology\\nnamespace). We’ve already said that Pers istentVolume resources are cluster-scoped\\nand thus cannot be created in a specific namespace, but PersistentVolumeClaims can\\nonly be created in a specific namespace. They can then only be used by pods in the\\nsame namespace.\\n6.5.4 Using a PersistentVolumeClaim in a pod\\nThe PersistentVolume is now yours to use. Nobody else can claim the same volumeuntil you release it. To use it inside a po d, you need to reference the Persistent-\\nVolumeClaim by name inside the pod’s volu me (yes, the PersistentVolumeClaim, not\\nthe PersistentVolume directly!), as  shown in the following listing.\\napiVersion: v1\\nkind: Podmetadata:\\n  name: mongodb \\nspec:  containers:\\n  - image: mongo\\n    name: mongodb    volumeMounts:\\n    - name: mongodb-data\\n      mountPath: /data/db    ports:\\n    - containerPort: 27017\\n      protocol: TCP  volumes:\\n  - name: mongodb-data\\n    persistentVolumeClaim:       \\n      claimName: mongodb-pvc     \\nGo ahead and create the pod. Now, check to  see if the pod is indeed using the same\\nPersistentVolume and its underlying GCE PD . You should see the data you stored ear-\\nlier by running the MongoDB shell again, as shown in the following listing.\\n$ kubectl exec -it mongodb mongo\\nMongoDB shell version: 3.2.8\\nconnecting to: mongodb://127.0.0.1:27017Welcome to the MongoDB shell.\\n...\\n> use mystoreswitched to db mystore\\n> db.foo.find()\\n{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\\nAnd there it is. You‘re able to retrieve the document you stored into MongoDB\\npreviously.Listing 6.12 A pod using a PersistentVo lumeClaim volume: mongodb-pod-pvc.yaml\\nListing 6.13 Retrieving MongoDB’s persisted data in the pod using the PVC and PVReferencing the PersistentVolumeClaim \\nby name in the pod volume'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 213}, page_content='182 CHAPTER  6Volumes: attaching disk storage to containers\\n6.5.5 Understanding the benefits of using PersistentVolumes and claims\\nExamine figure 6.8, which shows both ways a pod can use a GCE Persistent Disk—\\ndirectly or through a Pe rsistentVolume and claim.\\nConsider how using this indirect method of obtaining storage from the infrastructure\\nis much simpler for the application develope r (or cluster user). Yes, it does require\\nthe additional steps of creating the PersistentVolume and the PersistentVolumeClaim,but the developer doesn’t have to know an ything about the actual storage technology\\nused underneath. \\n Additionally, the same pod and claim manifests can now be used on many different\\nKubernetes clusters, because they don’t refe r to anything infrastructure-specific. The\\nclaim states, “I need x amount of storage and I need to be able to read and write to it\\nby a single client at once,” and then the pod references the claim by name in one of\\nits volumes.Pod: mongodb\\nContainer: mongodb\\nvolumeMounts:\\nname: mongodb-datamountPath: /data/dbgcePersistentDisk:\\npdName: mongodb\\nGCE\\nPersistent Disk:\\nmongodbVolume:\\nmongodb\\nPod: mongodb\\nContainer: mongodb\\nvolumeMounts:\\nname: mongodb-datamountPath: /data/db\\npersistentVolumeClaim:\\nclaimName: mongodb-pvcgcePersistentDisk:\\npdName: mongodbGCE\\nPersistent Disk:\\nmongodb\\nPersistentVolume:\\nmongodb-pv\\n(1 Gi, RWO, RWX)Volume:\\nmongodb\\nClaim lists\\n1Gi and\\nReadWriteOnce\\naccessPersistentVolumeClaim:\\nmongodb-pvc\\nFigure 6.8 Using the GCE Persistent Disk directly or through a PVC and PV'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 214}, page_content='183 Decoupling pods from the underlying storage technology\\n6.5.6 Recycling PersistentVolumes\\nBefore you wrap up this section on Persiste ntVolumes, let’s do one last quick experi-\\nment. Delete the pod and the PersistentVolumeClaim:\\n$ kubectl delete pod mongodb\\npod \"mongodb\" deleted$ kubectl delete pvc mongodb-pvc\\npersistentvolumeclaim \"mongodb-pvc\" deleted\\nWhat if you create the PersistentVolumeClaim again? Will it be bound to the Persistent-\\nVolume or not? After you create the claim, what does kubectl  get pvc show?\\n$ kubectl get pvc\\nNAME           STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE\\nmongodb-pvc    Pending                                         13s\\nThe claim’s status is shown as Pending . Interesting. When you created the claim ear-\\nlier, it was immediately bound to the Pers istentVolume, so why wasn’t it bound now?\\nMaybe listing the PersistentVolumes can shed more light on this:\\n$ kubectl get pv\\nNAME        CAPACITY  ACCESSMODES  STATUS    CLAIM               REASON AGE\\nmongodb-pv  1Gi       RWO,ROX      Released  default/mongodb-pvc        5m\\nThe STATUS  column shows the PersistentVolume as Released , not Available  like\\nbefore. Because you’ve already used the volume, it may contain data and shouldn’t be\\nbound to a completely new claim without givi ng the cluster admin a chance to clean it\\nup. Without this, a new pod using the same  PersistentVolume could read the data\\nstored there by the previous pod, even if the claim and pod were created in a different\\nnamespace (and thus likely belong  to a different cluster tenant).\\nRECLAIMING  PERSISTENT VOLUMES  MANUALLY\\nYou told Kubernetes you wanted your Persis tentVolume to behave like this when you\\ncreated it—by setting its persistentVolu meReclaimPolicy  to Retain . You wanted\\nKubernetes to retain the volume and its contents after it’s released from its claim. As\\nfar as I’m aware, the only way to manually  recycle the PersistentVolume to make it\\navailable again is to delete and recreate  the PersistentVolume resource. As you do\\nthat, it’s your decision what to do with the files on the underlying storage: you can\\neither delete them or leave them alone so  they can be reused by the next  pod.\\nRECLAIMING  PERSISTENT VOLUMES  AUTOMATICALLY\\nTwo other possible reclaim policies exist: Recycle  and Delete . The first one deletes\\nthe volume’s contents and makes the volume available to be claimed again. This way,\\nthe PersistentVolume can be reused multip le times by different PersistentVolume-\\nClaims and different pods, as you can see in figure 6.9.\\n The Delete  policy, on the other hand, deletes the underlying storage. Note that\\nthe Recycle  option is currently not available for GCE Persistent Disks. This type of'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 215}, page_content='184 CHAPTER  6Volumes: attaching disk storage to containers\\nA PersistentVolume only supports the Retain  or Delete  policies. Other Persistent-\\nVolume types may or may not support each of  these options, so before creating your\\nown PersistentVolume, be sure to check wh at reclaim policies are supported for the\\nspecific underlying storage you’ll use in the volume.\\nTIP You can change the PersistentVolume reclaim policy on an existing\\nPersistentVolume. For example, if it’s initially set to Delete , you can easily\\nchange it to Retain  to prevent losing valuable data.\\n6.6 Dynamic provisioning of PersistentVolumes\\nYou’ve seen how using PersistentVolumes and PersistentVolumeClaims makes it easy\\nto obtain persistent storage without the developer having to deal with the actual stor-\\nage technology used underneath . But this still requires a cluster administrator to pro-\\nvision the actual storage up front. Luckily, Kubernetes can also perform this jobautomatically through dynamic provisioning of PersistentVolumes.\\n The cluster admin, instead of creating PersistentVolumes, can deploy a Persistent-\\nVolume provisioner and define one or more StorageClass objects to let users choosewhat type of PersistentVolume they want. The users can refer to the \\nStorageClass  in\\ntheir PersistentVolumeClaims and the provisioner will take that into account when\\nprovisioning the persistent storage. \\nNOTE Similar to PersistentVolumes, Storag eClass resources aren’t namespaced.\\nKubernetes includes provisioners for the mo st popular cloud providers, so the admin-\\nistrator doesn’t always need to deploy a provisioner. But if Kubernetes is deployed\\non-premises, a custom provisioner needs to be deployed.PersistentVolume\\nPersistentVolumeClaim 1\\nPod 1 Pod 2PersistentVolumeClaim 2\\nPod 3\\nPVC is deleted;\\nPV is automatically\\nrecycled and ready\\nto be claimed and\\nre-used againUser creates\\nPersistentVolumeClaimPod 2\\nunmounts\\nPVCPod 2\\nmounts\\nPVCPod 1\\nmounts\\nPVCPod 1\\nunmounts\\nPVCAdmin deletes\\nPersistentVolumeAdmin creates\\nPersistentVolumeTime\\nFigure 6.9 The lifespan of a PersistentVolume, PersistentVolumeClaims, and pods using them'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 216}, page_content='185 Dynamic provisioning of PersistentVolumes\\n Instead of the administrator pre-provisioning a bunch of PersistentVolumes, they\\nneed to define one or two (or more) Storag eClasses and let the system create a new\\nPersistentVolume each time one is reques ted through a PersistentVolumeClaim. The\\ngreat thing about this is that it’s impossibl e to run out of PersistentVolumes (obviously,\\nyou can run out of storage space). \\n6.6.1 Defining the available storage types through StorageClass \\nresources\\nBefore a user can create a PersistentVolumeClaim, which will result in a new Persistent-\\nVolume being provisioned, an admin need s to create one or more StorageClass\\nresources. Let’s look at an exampl e of one in the following listing.\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:  name: fast\\nprovisioner: kubernetes.io/gce-pd       \\nparameters:  type: pd-ssd                     \\n  zone: europe-west1-b             \\nNOTE If using Minikube, deploy the file  storageclass-fast-hostpath.yaml.\\nThe StorageClass resource specifies which provisioner should be used for provision-\\ning the PersistentVolume when a PersistentVolumeClaim requests this StorageClass.\\nThe parameters defined in the StorageClass definition are passed to the provisionerand are specific to each provisioner plugin. \\n The StorageClass uses the Google Compute Engine (GCE) Persistent Disk (PD)\\nprovisioner, which means it can be used when Kubernetes is running in GCE. For\\nother cloud providers, other provisioners need to be used.\\n6.6.2 Requesting the storage class in a PersistentVolumeClaim\\nAfter the StorageClass resource is created, users can refer to the storage class by name\\nin their PersistentVolumeClaims. \\nCREATING  A PVC DEFINITION  REQUESTING  A SPECIFIC  STORAGE  CLASS\\nYou can modify your mongodb-pvc  to use dynamic provisioning. The following listing\\nshows the updated YAML definition of the PVC.\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:  name: mongodb-pvc Listing 6.14 A StorageClass definition: storageclass-fast-gcepd.yaml\\nListing 6.15 A PVC with dynamic provisioning: mongodb-pvc-dp.yamlThe volume plugin to \\nuse for provisioning \\nthe PersistentVolume\\nThe parameters passed \\nto the provisioner'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 217}, page_content='186 CHAPTER  6Volumes: attaching disk storage to containers\\nspec:\\n  storageClassName: fast     \\n  resources:    requests:\\n      storage: 100Mi\\n  accessModes:    - ReadWriteOnce\\nApart from specifying the size and access  modes, your PersistentVolumeClaim now\\nalso specifies the class of storage you want  to use. When you create the claim, the\\nPersistentVolume is created by th e provisioner referenced in the fast  StorageClass\\nresource. The provisioner is used even if an  existing manually provisioned Persistent-\\nVolume matches the PersistentVolumeClaim. \\nNOTE If you reference a non-existing stor age class in a PVC, the provisioning\\nof the PV will fail (you’ll see a ProvisioningFailed  event when you use\\nkubectl  describe  on the PVC).\\nEXAMINING  THE CREATED  PVC AND THE DYNAMICALLY  PROVISIONED  PV\\nNext you’ll create the PVC and then use kubectl  get to see it:\\n$ kubectl get pvc mongodb-pvc\\nNAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS\\nmongodb-pvc   Bound    pvc-1e6bc048   1Gi        RWO           fast \\nThe VOLUME  column shows the PersistentVolume that’s bound to this claim (the actual\\nname is longer than what’s shown above). You can try listing PersistentVolumes now to\\nsee that a new PV has indeed been created automatically:\\n$ kubectl get pv\\nNAME           CAPACITY  ACCESSMOD ES  RECLAIMPOLICY  STATUS    STORAGECLASS   \\nmongodb-pv     1Gi       RWO,ROX      Retain         Released \\npvc-1e6bc048   1Gi       RWO          Delete         Bound     fast\\nNOTE Only pertinent columns are shown.\\nYou can see the dynamically provisioned Pe rsistentVolume. Its capacity and access\\nmodes are what you requested in the PVC. Its reclaim policy is Delete , which means\\nthe PersistentVolume will be deleted when the PVC is deleted. Beside the PV, the pro-\\nvisioner also provisioned the actual storage. Your fast  StorageClass is configured to\\nuse the kubernetes.io/gce-pd  provisioner, which provisions GCE Persistent Disks.\\nYou can see the disk with the following command:\\n$ gcloud compute disks list\\nNAME                          ZONE            SIZE_GB  TYPE         STATUSgke-kubia-dyn-pvc-1e6bc048    europe-west1-d  1        pd-ssd       READY\\ngke-kubia-default-pool-71df   europe-west1-d  100      pd-standard  READY\\ngke-kubia-default-pool-79cd   europe-west1-d  100      pd-standard  READYgke-kubia-default-pool-blc4   europe-west1-d  100      pd-standard  READY\\nmongodb                       europe-west1-d  1        pd-standard  READYThis PVC requests the \\ncustom storage class.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 218}, page_content='187 Dynamic provisioning of PersistentVolumes\\nAs you can see, the first persistent disk’s name suggests it was provisioned dynamically\\nand its type shows it’s an SSD, as specifie d in the storage class you created earlier. \\nUNDERSTANDING  HOW TO USE STORAGE  CLASSES\\nThe cluster admin can create multiple storage classes with different performance or\\nother characteristics. The developer then decides which one is most appropriate for\\neach claim they create. \\n The nice thing about StorageClasses is the fact that claims refer to them by\\nname. The PVC definitions are therefore portable across different clusters, as long\\nas the StorageClass names are the same ac ross all of them. To see this portability\\nyourself, you can try running the same example on Minikube, if you’ve been usingGKE up to this point. As a cluster admin,  you’ll have to create a different storage\\nclass (but with the same name). The stor age class defined in the storageclass-fast-\\nhostpath.yaml file is tailor-made for use in  Minikube. Then, once you deploy the stor-\\nage class, you as a cluster user can deploy  the exact same PVC manifest and the exact\\nsame pod manifest as before. This shows how the pods and PVCs are portable across\\ndifferent clusters.\\n6.6.3 Dynamic provisioning without specifying a storage class\\nAs we’ve progressed through this chapter, attaching persistent storage to pods has\\nbecome ever simpler. The sections in this chapter reflect how provisioning of storage\\nhas evolved from early Kubernetes versions to  now. In this final section, we’ll look at\\nthe latest and simplest way of atta ching a PersistentVolume to a pod. \\nLISTING  STORAGE  CLASSES\\nWhen you created your custom storage class called fast , you didn’t check if any exist-\\ning storage classes were already defined in your cluster. Why don’t you do that now?\\nHere are the storage classes available in GKE:\\n$ kubectl get sc\\nNAME                 TYPEfast                 kubernetes.io/gce-pd\\nstandard (default)   kubernetes.io/gce-pd\\nNOTE We’re using sc as shorthand for storageclass .\\nBeside the fast  storage class, which you created yourself, a standard  storage class\\nexists and is marked as default. You’ll learn what that means in a moment. Let’s list the\\nstorage classes available in Minikube, so we can compare:\\n$ kubectl get sc\\nNAME                 TYPE\\nfast                 k8s.io/minikube-hostpath\\nstandard (default)   k8s.io/minikube-hostpath\\nAgain, the fast  storage class was created by you and a default standard  storage class\\nexists here as well. Comparing the TYPE  columns in the two listings, you see GKE is'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 219}, page_content='188 CHAPTER  6Volumes: attaching disk storage to containers\\nusing the kubernetes.io/gce-pd  provisioner, whereas Minikube is using k8s.io/\\nminikube-hostpath . \\nEXAMINING  THE DEFAULT  STORAGE  CLASS\\nYou’re going to use kubectl  get to see more info about the standard storage class in a\\nGKE cluster, as shown in the following listing.\\n$ kubectl get sc standard -o yaml\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:  annotations:\\n    storageclass.beta.kubernetes.io/is-default-class: \"true\"   \\n  creationTimestamp: 2017-05-16T15:24:11Z  labels:\\n    addonmanager.kubernetes.io/mode: EnsureExists\\n    kubernetes.io/cluster-service: \"true\"  name: standard\\n  resourceVersion: \"180\"\\n  selfLink: /apis/storage.k8s.io/v1/storageclassesstandard  uid: b6498511-3a4b-11e7-ba2c-42010a840014\\nparameters:                                    \\n  type: pd-standard                            provisioner: kubernetes.io/gce-pd      \\nIf you look closely toward the top of the li sting, the storage class definition includes an\\nannotation, which makes this the default st orage class. The default storage class is\\nwhat’s used to dynamically provision a PersistentVolume if the PersistentVolumeClaimdoesn’t explicitly say which storage class to use. \\nCREATING  A PERSISTENT VOLUME CLAIM WITHOUT  SPECIFYING  A STORAGE  CLASS\\nYou can create a PVC without specifying the storageClassName  attribute and (on\\nGoogle Kubernetes Engine) a GCE Persistent Disk of type pd-standard  will be provi-\\nsioned for you. Try this by creating a cl aim from the YAML in the following listing.\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:  name: mongodb-pvc2\\nspec:                        \\n  resources:                     requests:                \\n      storage: 100Mi         \\n  accessModes:                   - ReadWriteOnce          Listing 6.16 The definition of the standard storage class on GKE\\nListing 6.17 PVC with no storage class def ined: mongodb-pvc-dp-nostorageclass.yamlThis annotation \\nmarks the storage class as default.\\nThe type parameter is used by the provisioner \\nto know what type of GCE PD to create.\\nThe GCE Persistent Disk provisioner \\nis used to provision PVs of this class.\\nYou’re not specifying \\nthe storageClassName \\nattribute (unlike earlier examples).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 220}, page_content='189 Dynamic provisioning of PersistentVolumes\\nThis PVC definition includes only the storage size request and the desired access\\nmodes, but no storage class. When you cr eate the PVC, whatever storage class is\\nmarked as default will be used. You can confirm that’s the case:\\n$ kubectl get pvc mongodb-pvc2\\nNAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS\\nmongodb-pvc2  Bound    pvc-95a5ec12   1Gi        RWO           standard\\n$ kubectl get pv pvc-95a5ec12\\nNAME           CAPACITY  ACCESSMOD ES  RECLAIMPOLICY  STATUS    STORAGECLASS   \\npvc-95a5ec12   1Gi       RWO          Delete         Bound     standard\\n$ gcloud compute disks list\\nNAME                          ZONE            SIZE_GB  TYPE         STATUS\\ngke-kubia-dyn-pvc-95a5ec12    europe-west1-d  1        pd-standard  READY\\n...\\nFORCING  A PERSISTENT VOLUME CLAIM TO BE BOUND  TO ONE OF THE PRE-PROVISIONED  \\nPERSISTENT VOLUMES\\nThis finally brings us to why you set storageClassName  to an empty stri ng in listing 6.11\\n(when you wanted the PVC to bind to th e PV you’d provisioned manually). Let me\\nrepeat the relevant lines of  that PVC definition here:\\nkind: PersistentVolumeClaim\\nspec:  storageClassName: \"\"       \\nIf you hadn’t set the storageClassName  attribute to an empty string, the dynamic vol-\\nume provisioner would have provisioned a new PersistentVolume, despite there being\\nan appropriate pre-provisioned PersistentVo lume. At that point, I wanted to demon-\\nstrate how a claim gets bound to a manually  pre-provisioned PersistentVolume. I didn’t\\nwant the dynamic provisioner to interfere. \\nTIP Explicitly set storageClassName  to \"\" if you want the PVC to use a pre-\\nprovisioned PersistentVolume.\\nUNDERSTANDING  THE COMPLETE  PICTURE  OF DYNAMIC  PERSISTENT VOLUME  PROVISIONING\\nThis brings us to the end of this chapter.  To summarize, the best way to attach per-\\nsistent storage to a pod is to only crea te the PVC (with an explicitly specified storage-\\nClassName  if necessary) and the pod (which refe rs to the PVC by name). Everything\\nelse is taken care of by the dy namic PersistentVolume provisioner.\\n To get a complete picture of the steps involved in getting a dynamically provi-\\nsioned PersistentVolume, examine figure 6.10.\\n \\n \\n Specifying an empty string as the storage class \\nname ensures the PVC binds to a pre-provisioned \\nPV instead of dynamically provisioning a new one.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 221}, page_content='190 CHAPTER  6Volumes: attaching disk storage to containers\\n6.7 Summary\\nThis chapter has shown you how volumes are used to provide either temporary or per-\\nsistent storage to a pod’s cont ainers. You’ve learned how to\\n\\uf0a1Create a multi-container pod and have the pod’s containers operate on the\\nsame files by adding a volume to the pod and mounting it in each container\\n\\uf0a1Use the emptyDir  volume to store temporary, non-persistent data\\n\\uf0a1Use the gitRepo  volume to easily populate a dire ctory with the contents of a Git\\nrepository at pod startup\\n\\uf0a1Use the hostPath  volume to access files from the host node\\n\\uf0a1Mount external storage in a volume to  persist pod data across pod restarts\\n\\uf0a1Decouple the pod from the storage infras tructure by using PersistentVolumes\\nand PersistentVolumeClaims\\n\\uf0a1Have PersistentVolumes of the desired (or the default) storage class dynami-\\ncally provisioned for each PersistentVolumeClaim\\n\\uf0a1Prevent the dynamic provisioner from inte rfering when you want the Persistent-\\nVolumeClaim to be bound to a pre-provisioned PersistentVolume\\nIn the next chapter, you’ll see what mechan isms Kubernetes prov ides to deliver con-\\nfiguration data, secret information, and me tadata about the pod and container to the\\nprocesses running inside a pod. This is done with the special types of volumes we’ve\\nmentioned in this chapter, but not yet explored.PodAdmin\\nVolume1. Cluster admin sets up a PersistentVolume\\nprovisioner (if one’s not already deployed)\\n2. Admin creates one or\\nmore StorageClassesand marks one as thedefault (it may alreadyexist)Actual\\nstoragePersistent\\nVolume\\nUserPersistent\\nVolume\\nprovisioner\\nPersistent\\nVolumeClaimStorage\\nClass\\n3. User creates a PVC referencing one of the\\nStorageClasses (or none to use the default)\\n6. User creates a pod with\\na volume referencing thePVC by name4. Kubernetes looks up the\\nStorageClass and the provisionerreferenced in it and asks the provisionerto provision a new PV based on thePVC’s requested access mode andstorage size and the parametersin the StorageClass5. Provisioner provisions the\\nactual storage, createsa PersistentVolume, andbinds it to the PVC\\nFigure 6.10 The complete picture of dynamic provisioning of PersistentVolumes'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 222}, page_content='191ConfigMaps and Secrets:\\nconfiguring applications\\nUp to now you haven’t had to pass any kind  of configuration data to the apps you’ve\\nrun in the exercises in this book. Because almost all apps require configuration (set-\\ntings that differ between deployed instance s, credentials for accessing external sys-\\ntems, and so on), which shouldn’t be baked into the built app itself, let’s see how to\\npass configuration options to your a pp when running it in Kubernetes.\\n7.1 Configuring containerized applications\\nBefore we go over how to pass configurat ion data to apps running in Kubernetes,\\nlet’s look at how containerized a pplications are usually configured.\\n If you skip the fact that you can bake  the configuration into the application\\nitself, when starting development of a new app, you usually start off by having theThis chapter covers\\n\\uf0a1Changing the main process of a container\\n\\uf0a1Passing command-line options to the app\\n\\uf0a1Setting environment variables exposed to the app\\n\\uf0a1Configuring apps through ConfigMap s\\n\\uf0a1Passing sensitive information through Secret s'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 223}, page_content='192 CHAPTER  7ConfigMaps and Secrets: configuring applications\\napp configured through command-line argume nts. Then, as the list of configuration\\noptions grows, you can move the co nfiguration into a config file. \\n Another way of passing configuration opti ons to an application that’s widely popu-\\nlar in containerized applications is throug h environment variables. Instead of having\\nthe app read a config file or command-line arguments, the app looks up the value of a\\ncertain environment variable. The official MySQL container image, for example, uses\\nan environment variable called MYSQL_ROOT_PASSWORD  for setting the password for the\\nroot super-user account. \\n But why are environment variables so po pular in containers? Using configuration\\nfiles inside Docker containers is a bit tricky , because you’d have to bake the config file\\ninto the container image itself or mount a volume containing the file into the con-\\ntainer. Obviously, baking files into the im age is similar to hardcoding configuration\\ninto the source code of the application, be cause it requires you to rebuild the image\\nevery time you want to change the config. Plus, everyone with access to the image can\\nsee the config, including any information that  should be kept secret, such as creden-\\ntials or encryption keys. Using a volume is better, but still requires you to make sure\\nthe file is written to the volume before the container is started. \\n If you’ve read the previous chapter, you might think of using a gitRepo  volume as\\na configuration source. That’s not a bad idea , because it allows you to keep the config\\nnicely versioned and enables you to easily rollback a config change if necessary. But a\\nsimpler way allows you to put the configuration data into a top-level Kubernetesresource and store it and all the other reso urce definitions in the same Git repository\\nor in any other file-based storage. The Ku bernetes resource for storing configuration\\ndata is called a ConfigMap. We’ll le arn how to use it in this chapter.\\n Regardless if you’re using a ConfigMap to store configuration data or not, you can\\nconfigure your apps by\\n\\uf0a1Passing command-line arguments to containers\\n\\uf0a1Setting custom environment variables for each container\\n\\uf0a1Mounting configuration files into contai ners through a special type of volume\\nWe’ll go over all these options in the next few sections, but before we start, let’s look\\nat config options from a security perspe ctive. Though most configuration options\\ndon’t contain any sensitive information, se veral can. These include credentials, pri-\\nvate encryption keys, and similar data that n eeds to be kept secure. This type of infor-\\nmation needs to be handled with specia l care, which is why Kubernetes offers\\nanother type of first-class object called a Secr et. We’ll learn about it in the last part of\\nthis chapter.\\n7.2 Passing command-line arguments to containers\\nIn all the examples so far, you’ve created containers that ran the default command\\ndefined in the container image, but Kube rnetes allows overriding the command as\\npart of the pod’s container definition wh en you want to run a different executable'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 224}, page_content='193 Passing command-line arguments to containers\\ninstead of the one specified in the image, or want to run it with a different set of com-\\nmand-line arguments. We’ll look at how to do that now.\\n7.2.1 Defining the command and arguments in Docker\\nThe first thing I need to explain is that th e whole command that gets executed in the\\ncontainer is composed of two parts: the command  and the arguments . \\nUNDERSTANDING  ENTRYPOINT AND CMD\\nIn a Dockerfile, two instruct ions define the two parts:\\n\\uf0a1ENTRYPOINT  defines the executable invoked when the container is started.\\n\\uf0a1CMD specifies the arguments that get passed to the ENTRYPOINT .\\nAlthough you can use the CMD instruction to specify the command you want to execute\\nwhen the image is run, the correct way is to do it through the ENTRYPOINT  instruction\\nand to only specify the CMD if you want to define the de fault arguments. The image can\\nthen be run without specifying any arguments\\n$ docker run <image>\\nor with additional arguments, wh ich override whatever’s set under CMD in the Dockerfile:\\n$ docker run <image> <arguments>\\nUNDERSTANDING  THE DIFFERENCE  BETWEEN  THE SHELL  AND EXEC FORMS\\nBut there’s more. Both instructio ns support two different forms:\\n\\uf0a1shell  form—For example, ENTRYPOINT  node  app.js.\\n\\uf0a1exec  form—For example, ENTRYPOINT  [\"node\",  \"app.js\"] .\\nThe difference is whether the specified comm and is invoked inside a shell or not. \\n In the kubia  image you created in chapter 2, you used the exec  form of the ENTRY-\\nPOINT  instruction: \\nENTRYPOINT [\"node\", \"app.js\"]\\nThis runs the node process directly (not in side a shell), as you can see by listing the\\nprocesses running inside the container:\\n$ docker exec 4675d ps x\\n  PID TTY      STAT   TIME COMMAND\\n    1 ?        Ssl    0:00 node app.js   12 ?        Rs     0:00 ps x\\nIf you’d used the shell  form (ENTRYPOINT  node  app.js ), these would have been the\\ncontainer’s processes:\\n$ docker exec -it e4bad ps x\\n  PID TTY      STAT   TIME COMMAND\\n    1 ?        Ss     0:00 /bin/sh -c node app.js'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 225}, page_content='194 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n    7 ?        Sl     0:00 node app.js\\n   13 ?        Rs+    0:00 ps x\\nAs you can see, in that case, the main process ( PID 1) would be the shell  process\\ninstead of the node process. The node process ( PID 7) would be started from that\\nshell. The shell  process is unnecessary, which is why you should always use the exec\\nform of the ENTRYPOINT instruction.\\nMAKING  THE INTERVAL  CONFIGURABLE  IN YOUR FORTUNE  IMAGE\\nLet’s modify your fortune script and image so  the delay interval in the loop is configu-\\nrable. You’ll add an INTERVAL  variable and initialize it with the value of the first com-\\nmand-line argument, as shown in the following listing.\\n#!/bin/bash\\ntrap \"exit\" SIGINT\\nINTERVAL=$1echo Configured to generate new fortune every $INTERVAL seconds\\nmkdir -p /var/htdocs\\nwhile :do\\n  echo $(date) Writing fortune to /var/htdocs/index.html\\n  /usr/games/fortune > /var/htdocs/index.html  sleep $INTERVAL\\ndone\\nYou’ve added or modified the lines in bold font. Now, you’ll modify the Dockerfile so\\nit uses the exec  version of the ENTRYPOINT  instruction and sets the default interval to\\n10 seconds using the CMD instruction, as shown in the following listing.\\nFROM ubuntu:latest\\nRUN apt-get update ; apt-get -y install fortune\\nADD fortuneloop.sh /bin/fortuneloop.sh\\nENTRYPOINT [\"/bin/fortuneloop.sh\"]        CMD [\"10\"]                                \\nYou can now build and push the image to Dock er Hub. This time, you’ll tag the image\\nas args  instead of latest :\\n$ docker build -t docker.io/luksa/fortune:args .\\n$ docker push docker.io/luksa/fortune:args\\nYou can test the image by runni ng it locally with Docker:\\n$ docker run -it docker.io/luksa/fortune:args\\nConfigured to generate new fortune every 10 seconds\\nFri May 19 10:39:44 UTC 2017 Writing fortune to /var/htdocs/index.htmlListing 7.1 Fortune script with interval configurable through argument: fortune-args/\\nfortuneloop.sh\\nListing 7.2 Dockerfile for the updated fortune  image: fortune-args/Dockerfile\\nThe exec form of the \\nENTRYPOINT instruction\\nThe default argument \\nfor the executable'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 226}, page_content='195 Passing command-line arguments to containers\\nNOTE You can stop the script with Control+C.\\nAnd you can override the default sleep in terval by passing it as an argument:\\n$ docker run -it docker.io/luksa/fortune:args 15\\nConfigured to generate new fortune every 15 seconds\\nNow that you’re sure your image honors the argument passed to it, let’s see how to use\\nit in a pod.\\n7.2.2 Overriding the command and arguments in Kubernetes\\nIn Kubernetes, when specifying a contai ner, you can choose to override both ENTRY-\\nPOINT  and CMD. To do that, you set the properties command  and args  in the container\\nspecification, as shown in the following listing.\\nkind: Pod\\nspec:\\n  containers:  - image: some/image\\n    command: [\"/bin/command\"]\\n    args: [\"arg1\", \"arg2\", \"arg3\"]\\nIn most cases, you’ll only set custom arguments and rarely override the command\\n(except in general-purpose images such as busybox , which doesn’t define an ENTRY-\\nPOINT  at all). \\nNOTE The command  and args  fields can’t be updated after the pod is created.\\nThe two Dockerfile instructions and the equiva lent pod spec fields are shown in table 7.1.\\nRUNNING  THE FORTUNE  POD WITH A CUSTOM  INTERVAL\\nTo run the fortune pod with a custom de lay interval, you’ll copy your fortune-\\npod.yaml into fortune-pod-args.yaml and modi fy it as shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: fortune2s         Listing 7.3 A pod definition specifyin g a custom command and arguments\\nTable 7.1 Specifying the executable and i ts arguments in Docker vs Kubernetes\\nDocker Kubernetes Description\\nENTRYPOINT command The executable that’s executed inside the container\\nCMD args The arguments passed to the executable\\nListing 7.4 Passing an argument in the pod definition: fortune-pod-args.yaml\\nYou changed the \\npod’s name.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 227}, page_content='196 CHAPTER  7ConfigMaps and Secrets: configuring applications\\nspec:\\n  containers:\\n  - image: luksa/fortune:args          args: [\"2\"]                  \\n    name: html-generator\\n    volumeMounts:    - name: html\\n      mountPath: /var/htdocs\\n...\\nYou added the args  array to the container definition . Try creating this pod now. The\\nvalues of the array will be passed to the container as command-line arguments when it\\nis run. \\n The array notation used in this listing is great if you have one argument or a few. If\\nyou have several, you can also  use the following notation:\\n    args:\\n    - foo\\n    - bar    - \"15\"\\nTIP You don’t need to enclose string va lues in quotations marks (but you\\nmust enclose numbers). \\nSpecifying arguments is one way of passing config\\noptions to your containers through command-\\nline arguments. Next, you’ll see how to do it\\nthrough environment variables.\\n7.3 Setting environment variables for \\na container\\nAs I’ve already mentioned, containerized appli-\\ncations often use environment variables as a\\nsource of configuration options. Kubernetes\\nallows you to specify a custom list of environ-ment variables for each container of a pod, as\\nshown in figure 7.1. Although it would be use-\\nful to also define environment variables at thepod level and have them be inherited by its\\ncontainers, no such option currently exists.\\nNOTE Like the container’s command and\\narguments, the list of environment variablesalso cannot be updated after the pod is created.Using fortune:args \\ninstead of fortune:latest\\nThis argument makes the \\nscript generate a new fortune every two seconds.\\nPodContainer A\\nEnvironment variables\\nFOO=BAR\\nABC=123\\nContainer B\\nEnvironment variables\\nFOO=FOOBAR\\nBAR=567\\nFigure 7.1 Environment variables can \\nbe set per container.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 228}, page_content='197 Setting environment variables for a container\\nMAKING  THE INTERVAL  IN YOUR FORTUNE  IMAGE  CONFIGURABLE  THROUGH  AN ENVIRONMENT  VARIABLE\\nLet’s see how to modify your fortuneloop.sh sc ript once again to allow it to be config-\\nured from an environment variable, as shown in the following listing.\\n#!/bin/bash\\ntrap \"exit\" SIGINTecho Configured to generate new fortune every $INTERVAL seconds\\nmkdir -p /var/htdocs\\nwhile :do\\n  echo $(date) Writing fortune to /var/htdocs/index.html\\n  /usr/games/fortune > /var/htdocs/index.html  sleep $INTERVAL\\ndone\\nAll you had to do was remove the row where the INTERVAL  variable is initialized. Because\\nyour “app” is a simple bash script, you didn’t  need to do anything else. If the app was\\nwritten in Java you’d use System.getenv( \"INTERVAL\") , whereas in Node.JS you’d use\\nprocess.env.INTERVAL , and in Python you’d use os.environ[\\'INTERVAL\\'] .\\n7.3.1 Specifying environment variables in a container definition\\nAfter building the new imag e (I’ve tagged it as luksa/fortune:env  this time) and\\npushing it to Docker Hub, yo u can run it by creating a new pod, in which you pass the\\nenvironment variable to the script by including it in your container definition, as\\nshown in the following listing.\\nkind: Pod\\nspec:\\n containers:\\n - image: luksa/fortune:env   env:                        \\n   - name: INTERVAL            \\n     value: \"30\"                  name: html-generator\\n...\\nAs mentioned previously, you set the environm ent variable inside the container defini-\\ntion, not at the pod level. \\nNOTE Don’t forget that in each container, Kubernetes also automatically\\nexposes environment variables for each service in the same namespace. These\\nenvironment variables are basica lly auto-injected configuration.Listing 7.5 Fortune script with interval  configurable through env var: fortune-env/\\nfortuneloop.sh\\nListing 7.6 Defining an environment va riable in a pod: fortune-pod-env.yaml\\nAdding a single variable to \\nthe environment variable list'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 229}, page_content='198 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n7.3.2 Referring to other environment variables in a variable’s value\\nIn the previous example, you set a fixed value for the environment variable, but you\\ncan also reference previously defined environment variables or any other existing vari-\\nables by using the $(VAR)  syntax. If you define two environment variables, the second\\none can include the value of the first one as shown in the following listing.\\nenv:\\n- name: FIRST_VAR\\n  value: \"foo\"\\n- name: SECOND_VAR\\n  value: \"$(FIRST_VAR)bar\"\\nIn this case, the SECOND_VAR ’s value will be \"foobar\" . Similarly, both the command  and\\nargs  attributes you learned about in section 7.2 can also refer to environment vari-\\nables like this. You’ll use this method in section 7.4.5.\\n7.3.3 Understanding the drawback of hardcoding environment \\nvariables\\nHaving values effectively hardcoded in th e pod definition means you need to have\\nseparate pod definitions for your producti on and your development pods. To reuse\\nthe same pod definition in multiple environments, it makes sense to decouple the\\nconfiguration from the pod descriptor. Luckily, you can do that using a ConfigMapresource and using it as a source for environment variable values using the \\nvalueFrom\\ninstead of the value  field. You’ll learn about this next. \\n7.4 Decoupling configuration with a ConfigMap\\nThe whole point of an app’s configuration is to keep the config options that varybetween environments, or change frequently , separate from the application’s source\\ncode. If you think of a pod descriptor as sour ce code for your app (and in microservices\\narchitectures that’s what it really is, beca use it defines how to compose the individual\\ncomponents into a functioning system), it’s  clear you should move the configuration\\nout of the pod description.\\n7.4.1 Introducing ConfigMaps\\nKubernetes allows separating configuration options into a separate object called a\\nConfigMap, which is a map containing key/ value pairs with the values ranging from\\nshort literals to full config files. \\n An application doesn’t need to read the ConfigMap directly or even know that it\\nexists. The contents of the map are instea d passed to containers as either environ-\\nment variables or as files in a volume (see figure 7.2). And because environmentListing 7.7 Referring to an environment variable inside another one'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 230}, page_content='199 Decoupling configuration with a ConfigMap\\nvariables can be referenced in command-line arguments using the $(ENV_VAR)  syn-\\ntax, you can also pass Conf igMap entries to processes as command-line arguments.\\nSure, the application can also read the contents of a ConfigMap directly through the\\nKubernetes REST API endpoint if needed, but unless you have a real need for this,\\nyou should keep your app Kubernet es-agnostic as much as possible.\\n Regardless of how an app consumes a Co nfigMap, having the config in a separate\\nstandalone object like this allows you to keep multiple manifests for ConfigMaps with\\nthe same name, each for a different enviro nment (development, testing, QA, produc-\\ntion, and so on). Because pods reference the ConfigMap by name, you can use a dif-\\nferent config in each environment while us ing the same pod specification across all of\\nthem (see figure 7.3).PodEnvironment variables\\nConﬁgMap\\nkey1=value1\\nkey2=value2\\n...\\nconﬁgMap\\nvolume\\nFigure 7.2 Pods use ConfigMaps \\nthrough environment variables and \\nconfigMap  volumes.\\nConﬁgMap:\\napp-conﬁgNamespace: development\\n(contains\\ndevelopment\\nvalues)Pod(s)ConﬁgMaps created\\nfrom different manifests\\nPods created from the\\nsame pod manifestsNamespace: production\\nConﬁgMap:\\napp-conﬁg\\n(contains\\nproduction\\nvalues)Pod(s)\\nFigure 7.3 Two different ConfigMaps with the same name used in different \\nenvironments'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 231}, page_content='200 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n7.4.2 Creating a ConfigMap\\nLet’s see how to use a ConfigMap in one of your pods. To start with the simplest exam-\\nple, you’ll first create a map with a single key and use it to fill the INTERVAL  environment\\nvariable from your previous example. You’ ll create the ConfigMap with the special\\nkubectl  create  configmap  command instead of posting a YAML with the generic\\nkubectl  create  -f command. \\nUSING THE KUBECTL  CREATE  CONFIGMAP  COMMAND\\nYou can define the map’s entries by passing literals to the kubectl  command or you\\ncan create the ConfigMap from files stored on your disk. Use a simple literal first:\\n$ kubectl create configmap fortune-config --from-literal=sleep-interval=25\\nconfigmap \"fortune-config\" created\\nNOTE ConfigMap keys must be a valid DNS subdomain (they may only con-\\ntain alphanumeric characters, dashes, underscores, and dots). They mayoptionally include a leading dot.\\nThis creates a ConfigMap called \\nfortune-config  with the single-entry sleep-interval\\n=25 (figure 7.4).\\nConfigMaps usually contain more than one entry. To create a ConfigMap with multi-\\nple literal entries, you add multiple --from-literal  arguments:\\n$ kubectl create configmap myconfigmap\\n➥  --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two\\nLet’s inspect the YAML descriptor of the ConfigMap you created by using the kubectl\\nget command, as shown in the following listing.\\n$ kubectl get configmap fortune-config -o yaml\\napiVersion: v1\\ndata:\\n  sleep-interval: \"25\"                      \\nkind: ConfigMap                              metadata:\\n  creationTimestamp: 2016-08-11T20:31:08Z\\n  name: fortune-config                        namespace: default\\n  resourceVersion: \"910025\"\\n  selfLink: /api/v1/namespaces/default/configmaps/fortune-config  uid: 88c4167e-6002-11e6-a50d-42010af00237Listing 7.8 A ConfigMap definitionsleep-interval 25ConﬁgMap: fortune-conﬁg\\nFigure 7.4 The fortune-config  \\nConfigMap containing a single entry\\nThe single entry \\nin this map\\nThis descriptor \\ndescribes a ConfigMap.\\nThe name of this map \\n(you’re referencing it \\nby this name)'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 232}, page_content='201 Decoupling configuration with a ConfigMap\\nNothing extraordinary. You could easily have  written this YAML yourself (you wouldn’t\\nneed to specify anything but the name in the metadata  section, of course) and posted\\nit to the Kubernetes API with the well-known\\n$ kubectl create -f fortune-config.yaml\\nCREATING  A CONFIG MAP ENTRY  FROM THE CONTENTS  OF A FILE\\nConfigMaps can also store coarse-grained co nfig data, such as complete config files.\\nTo do this, the kubectl  create  configmap  command also supports reading files from\\ndisk and storing them as individual entries in the ConfigMap:\\n$ kubectl create configmap my-config --from-file=config-file.conf\\nWhen you run the previous comman d, kubectl looks for the file config-file.conf  in\\nthe directory you run kubectl  in. It will then store the contents of the file under the\\nkey config-file.conf  in the ConfigMap (the filename  is used as the map key), but\\nyou can also specify a key manually like this:\\n$ kubectl create configmap my-config --from-file=customkey=config-file.conf\\nThis command will store the file’s contents under the key customkey . As with literals,\\nyou can add multiple files by using the --from-file  argument multiple times. \\nCREATING  A CONFIG MAP FROM FILES IN A DIRECTORY\\nInstead of importing each file individually , you can even import all files from a file\\ndirectory:\\n$ kubectl create configmap my-config --from-file=/path/to/dir\\nIn this case, kubectl  will create an individual map entry for each file in the specified\\ndirectory, but only for files whos e name is a valid ConfigMap key. \\nCOMBINING  DIFFERENT  OPTIONS\\nWhen creating ConfigMaps, you can use a combination of all the options mentioned\\nhere (note that these files aren’t included in the book’s code archive—you can createthem yourself if you’d like to try out the command):\\n$ kubectl create configmap my-config  \\n➥  --from-file=foo.json                  \\n➥  --from-file=bar=foobar.conf              \\n➥  --from-file=config-opts/               \\n➥  --from-literal=some=thing    \\nHere, you’ve created the ConfigMap from mult iple sources: a whole directory, a file,\\nanother file (but stored under a custom key instead of using the filename as the key),\\nand a literal value. Figure 7.5 shows all these sources and the resulting ConfigMap.A single file\\nA file stored under \\na custom key\\nA whole directory\\nA literal value'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 233}, page_content='202 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n7.4.3 Passing a ConfigMap entry to a container as an environment \\nvariable\\nHow do you now get the values from this ma p into a pod’s container? You have three\\noptions. Let’s start with the simplest—setti ng an environment variable. You’ll use the\\nvalueFrom  field I mentioned in section 7.3.3. The pod descriptor should look like\\nthe following listing.\\napiVersion: v1\\nkind: PodListing 7.9 Pod with env var from a config map: fortune-pod-env-configmap.yamlConﬁgMap: my-conﬁg\\nKeyfoo.json\\nfoo.jsonValue\\nbar abc\\ndebug true\\nrepeat 100\\nsome thing{\\nfoo: barbaz: 5\\n}conﬁg-opts directory\\nLiteral\\nsome=thing{\\nfoo: barbaz: 5\\n}\\n--from-ﬁle=foo.json\\n--from-ﬁle=conﬁg-opts/\\n--from-literal=some=thing\\nfoobar.confabcdebugtrue\\nrepeat100\\n--from-ﬁle=bar=foobar.conf\\nFigure 7.5 Creating a ConfigMap from individual files, a directory, and a literal value'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 234}, page_content=\"203 Decoupling configuration with a ConfigMap\\nmetadata:\\n  name: fortune-env-from-configmap\\nspec:  containers:\\n  - image: luksa/fortune:env\\n    env:                                 - name: INTERVAL                 \\n      valueFrom:                       \\n        configMapKeyRef:               \\n          name: fortune-config      \\n          key: sleep-interval    ...\\nYou defined an environment variable called INTERVAL  and set its value to whatever is\\nstored in the fortune-config  ConfigMap under the key sleep-interval . When the\\nprocess running in the html-generator  container reads the INTERVAL  environment\\nvariable, it will see the value 25 (shown in figure 7.6).\\nREFERENCING  NON-EXISTING  CONFIG MAPS IN A POD\\nYou might wonder what happens if the refe renced ConfigMap doesn’t exist when you\\ncreate the pod. Kubernetes schedules the po d normally and tries to run its containers.\\nThe container referencing the non-existing Co nfigMap will fail to start, but the other\\ncontainer will start normally. If you then create the missing ConfigMap, the failed con-\\ntainer is started without requiring you to recreate the pod.\\nNOTE You can also mark a reference to a ConfigMap as optional (by setting\\nconfigMapKeyRef.optional:  true ). In that case, the container starts even if\\nthe ConfigMap doesn’t exist.\\nThis example shows you how to decouple the configuration from the pod specifica-\\ntion. This allows you to keep all the conf iguration options closely together (even for\\nmultiple pods) instead of having them spla ttered around the pod definition (or dupli-\\ncated across multiple pod manifests). You’re setting the environment \\nvariable called INTERVAL.\\nInstead of setting a fixed value, you're \\ninitializing it from a ConfigMap key.\\nThe name of the ConfigMap \\nyou're referencing\\nYou're setting the variable to whatever is\\nstored under this key in the ConfigMap.\\nConﬁgMap: fortune-conﬁg\\nsleep-interval 25\\nPodContainer: web-server\\nContainer: html-generator\\nEnvironment variables\\nINTERVAL=25fortuneloop.sh\\nprocess\\nFigure 7.6 Passing a ConfigMap entry as \\nan environment variable to a container\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 235}, page_content='204 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n7.4.4 Passing all entries of a ConfigMap as environment variables \\nat once\\nWhen your ConfigMap contains more than just a few entries, it becomes tedious and\\nerror-prone to create environment variable s from each entry individually. Luckily,\\nKubernetes version 1.6 provides a way to ex pose all entries of a ConfigMap as environ-\\nment variables. \\n Imagine having a ConfigMap with three keys called FOO, BAR, and FOO-BAR . You can\\nexpose them all as environment variables by using the envFrom  attribute, instead of\\nenv the way you did in previous examples. The following listing shows an example.\\nspec:\\n  containers:\\n  - image: some-image    envFrom:                 \\n    - prefix: CONFIG_             \\n      configMapRef:                      name: my-config-map      \\n...\\nAs you can see, you can also specify a prefix for the environment variables ( CONFIG_  in\\nthis case). This results in the following two environment variables being present inside\\nthe container: CONFIG_FOO  and CONFIG_BAR . \\nNOTE The prefix is optional, so if you om it it the environment variables will\\nhave the same name as the keys. \\nDid you notice I said two variables, but earl ier, I said the ConfigMap has three entries\\n(FOO, BAR, and FOO-BAR )? Why is there no environment variable for the FOO-BAR\\nConfigMap entry?\\n T h e  r e a s o n  i s  t h a t  CONFIG_FOO-BAR  isn’t a valid environment variable name\\nbecause it contains a dash. Kubernetes does n’t convert the keys in any way (it doesn’t\\nconvert dashes to underscores, for example). If a ConfigMap key isn’t in the proper\\nformat, it skips the entry (but it does re cord an event informing you it skipped it).\\n7.4.5 Passing a ConfigMap entry as a command-line argument\\nNow, let’s also look at how to pass values  from a ConfigMap as arguments to the main\\nprocess running in the container. You can’ t reference ConfigMap entries directly in\\nthe pod.spec.containers.args  field, but you can first initialize an environment vari-\\nable from the ConfigMap entry and then refe r to the variable inside the arguments as\\nshown in figure 7.7.\\n Listing 7.11 shows an example of  how to do this in the YAML.\\n Listing 7.10 Pod with env vars from all entries of a ConfigMap\\nUsing envFrom instead of env\\nAll environment variables will \\nbe prefixed with CONFIG_.\\nReferencing the ConfigMap \\ncalled my-config-map'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 236}, page_content='205 Decoupling configuration with a ConfigMap\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: fortune-args-from-configmapspec:\\n  containers:\\n  - image: luksa/fortune:args         \\n    env:                               \\n    - name: INTERVAL                         valueFrom:                       \\n        configMapKeyRef:               \\n          name: fortune-config                   key: sleep-interval          \\n    args: [\"$(INTERVAL)\"]      \\n...\\nYou defined the environment variable exactly as you did before, but then you used the\\n$(ENV_VARIABLE_NAME)  syntax to have Kubernetes inje ct the value of the variable into\\nthe argument. \\n7.4.6 Using a configMap volume to expose ConfigMap entries as files\\nPassing configuration options as environm ent variables or command-line arguments\\nis usually used for short variable values. A ConfigMap, as you’ve seen, can also con-tain whole config files. When you want to expose those to the container, you can use\\none of the special volume types I mentio ned in the previous chapter, namely a\\nconfigMap  volume.\\n A configMap  volume will expose each entry of  the ConfigMap as a file. The pro-\\ncess running in the container can obtain th e entry’s value by reading the contents of\\nthe file.Listing 7.11 Using ConfigMap entries as arguments: fortune-pod-args-configmap.yamlConﬁgMap: fortune-conﬁg\\nsleep-interval 25\\nPodContainer: web-server\\nContainer: html-generator\\nEnvironment variables\\nINTERVAL=25 fortuneloop.sh $(INTERVAL)\\nFigure 7.7 Passing a ConfigMap entry as a command-line argument\\nUsing the image that takes the \\ninterval from the first argument, not from an environment variable\\nDefining the \\nenvironment variable \\nexactly as before\\nReferencing the environment \\nvariable in the argument'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 237}, page_content='206 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n Although this method is mostly meant for passing large config files to the con-\\ntainer, nothing prevents you from passi ng short single values this way. \\nCREATING  THE CONFIG MAP\\nInstead of modifying your fortuneloop.sh scri pt once again, you’ll now try a different\\nexample. You’ll use a config fi le to configure the Nginx we b server running inside the\\nfortune  pod’s web-server container. Let’s say you want your Nginx server to compress\\nresponses it sends to the client. To enable compression, the config file for Nginxneeds to look like the following listing.\\nserver {\\n  listen              80;\\n  server_name         www.kubia-example.com;\\n  gzip on;                                       \\n  gzip_types text/plain application/xml;         \\n  location / {\\n    root   /usr/share/nginx/html;    index  index.html index.htm;\\n  }\\n}\\nNow delete your existing fortune-config  ConfigMap with kubectl  delete  config-\\nmap fortune-config , so that you can replace it with a new one, which will include the\\nNginx config file. You’ll create the ConfigMa p from files stored on your local disk. \\n Create a new directory called configmap-fi les and store the Nginx config from the\\nprevious listing into configmap-files/my-nginx-config.conf. To make the ConfigMap\\nalso contain the sleep-interval  entry, add a plain text file called sleep-interval to the\\nsame directory and store the numb er 25 in it (see figure 7.8).\\nNow create a ConfigMap from all the files in the directory like this:\\n$ kubectl create configmap fortune-config --from-file=configmap-files\\nconfigmap \"fortune-config\" createdListing 7.12 An Nginx config with enabled gzip compression: my-nginx-config.conf\\nThis enables gzip compression \\nfor plain text and XML files.\\nconﬁgmap-ﬁles/\\nmy-nginx-conﬁg.confserver {\\nlisten 80;server_name www.kubia......\\n}\\nsleep-interval25\\nFigure 7.8 The contents of the \\nconfigmap-files directory and its files'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 238}, page_content=\"207 Decoupling configuration with a ConfigMap\\nThe following listing shows what the YAML of this ConfigMap looks like.\\n$ kubectl get configmap fortune-config -o yaml\\napiVersion: v1\\ndata:\\n  my-nginx-config.conf: |                            \\n    server {                                               listen              80;                        \\n      server_name         www.kubia-example.com;     \\n      gzip on;                                       \\n      gzip_types text/plain application/xml;         \\n      location / {                                   \\n        root   /usr/share/nginx/html;                        index  index.html index.htm;                 \\n      }                                              \\n    }                                                  sleep-interval: |         \\n    25                      \\nkind: ConfigMap...\\nNOTE The pipeline character after the colon in the first line of both entries\\nsignals that a literal multi-line value follows.\\nThe ConfigMap contains two entries, with keys corresponding to the actual names\\nof the files they were created from. You’ ll now use the ConfigMap in both of your\\npod’s containers.\\nUSING THE CONFIG MAP'S ENTRIES  IN A VOLUME\\nCreating a volume populated with the conten ts of a ConfigMap is as easy as creating\\na volume that references the ConfigMap by name and mounting the volume in a\\ncontainer. You already learned how to create volumes and mount them, so the onlything left to learn is how to initialize the volume with files created from a Config-\\nMap’s entries.\\n Nginx reads its config file from /e tc/nginx/nginx.conf. The Nginx image\\nalready contains this file with default configuration options, which you don’t want\\nto override, so you don’t want to replace this file as a whole. Luckily, the default\\nconfig file automatically includes all .con f files in the /etc/nginx/conf.d/ subdirec-\\ntory as well, so you should add your config  file in there. Figure 7.9 shows what you\\nwant to achieve.\\n The pod descriptor is shown in listing 7. 14 (the irrelevant parts are omitted, but\\nyou’ll find the complete file in the code archive).\\n  Listing 7.13 YAML definition of a config map created from a file\\nThe entry holding the \\nNginx config file’s \\ncontents\\nThe sleep-interval entry\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 239}, page_content='208 CHAPTER  7ConfigMaps and Secrets: configuring applications\\napiVersion: v1\\nkind: Podmetadata:\\n  name: fortune-configmap-volume\\nspec:  containers:\\n  - image: nginx:alpine\\n    name: web-server\\n    volumeMounts:\\n    ...    - name: config\\n      mountPath: /etc/nginx/conf.d      \\n      readOnly: true    ...\\n  volumes:\\n  ...  - name: config              \\n    configMap:                 \\n      name: fortune-config       ...\\nThis pod definition includes a volume, which references your fortune-config\\nConfigMap. You mount the volume into the /etc/nginx/conf.d directory to make\\nNginx use it. \\nVERIFYING  NGINX IS USING  THE MOUNTED  CONFIG  FILE\\nThe web server should now be configured to  compress the responses it sends. You can\\nverify this by enabling port-forwarding from  localhost:8080 to the pod’s port 80 and\\nchecking the server’s response with curl , as shown in the following listing.\\n Listing 7.14 A pod with ConfigMap entries mounted as files: fortune-pod-configmap-\\nvolume.yamlPodContainer: html-generatorContainer: web-server\\nFilesystem\\n/\\netc/\\nnginx/\\nconf.d/ConﬁgMap: fortune-conﬁg\\nmy-nginx-conﬁg.conf server {\\n…\\n}Volume:\\nconﬁg\\nFigure 7.9 Passing ConfigMap entries to a pod as files in a volume\\nYou’re mounting the \\nconfigMap volume at this location.\\nThe volume refers to your \\nfortune-config ConfigMap.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 240}, page_content='209 Decoupling configuration with a ConfigMap\\n$ kubectl port-forward fortune-configmap-volume 8080:80 &\\nForwarding from 127.0.0.1:8080 -> 80Forwarding from [::1]:8080 -> 80\\n$ curl -H \"Accept-Encoding: gzip\" -I localhost:8080\\nHTTP/1.1 200 OKServer: nginx/1.11.1\\nDate: Thu, 18 Aug 2016 11:52:57 GMT\\nContent-Type: text/html\\nLast-Modified: Thu, 18 Aug 2016 11:52:55 GMT\\nConnection: keep-aliveETag: W/\"57b5a197-37\"\\nContent-Encoding: gzip           \\nEXAMINING  THE MOUNTED  CONFIG MAP VOLUME ’S CONTENTS\\nThe response shows you achieved what you wanted, but let’s look at what’s in the\\n/etc/nginx/conf.d directory now:\\n$ kubectl exec fortune-configmap-volume -c web-server ls /etc/nginx/conf.d\\nmy-nginx-config.conf\\nsleep-interval\\nBoth entries from the ConfigMap have been  added as files to the directory. The\\nsleep-interval  entry is also included, although it has no business being there,\\nbecause it’s only meant to be used by the fortuneloop  container. You could create\\ntwo different ConfigMaps and use one to configure the fortuneloop  container and\\nthe other one to configure the web-server  container. But somehow it feels wrong to\\nuse multiple ConfigMaps to configure contai ners of the same pod. After all, having\\ncontainers in the same pod implies that th e containers are closely related and should\\nprobably also be configured as a unit. \\nEXPOSING  CERTAIN  CONFIG MAP ENTRIES  IN THE VOLUME\\nLuckily, you can populate a configMap  v o l u m e  w i t h  o n l y  p a r t  o f  t h e  C o n f i g M a p ’ s\\nentries—in your case, only the my-nginx-config.conf  entry. This won’t affect the\\nfortuneloop  container, because you’re passing the sleep-interval  entry to it through\\nan environment variable and not through the volume. \\n To define which entries should be exposed as files in a configMap  volume, use the\\nvolume’s items  attribute as shown in the following listing.\\n  volumes:\\n  - name: config                  configMap:                                  \\n      name: fortune-config                      \\n      items:                             - key: my-nginx-config.conf        \\n        path: gzip.conf                  Listing 7.15 Seeing if nginx responses have compression enabled\\nListing 7.16 A pod with a specific ConfigMap entry mounted into a file directory: \\nfortune-pod-configmap-volume-with-items.yamlThis shows the response \\nis compressed.\\nSelecting which entries to include \\nin the volume by listing them\\nYou want the entry \\nunder this key included.\\nThe entry’s value should \\nbe stored in this file.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 241}, page_content='210 CHAPTER  7ConfigMaps and Secrets: configuring applications\\nWhen specifying individual entries, you n eed to set the filename for each individual\\nentry, along with the entry’s key. If you run the pod from the previous listing, the\\n/etc/nginx/conf.d directory is kept nice and clean, because it only contains the\\ngzip.conf file and nothing else. \\nUNDERSTANDING  THAT MOUNTING  A DIRECTORY  HIDES EXISTING  FILES IN THAT DIRECTORY\\nThere’s one important thing to discuss at this point. In both this and in your previousexample, you mounted the volume as a direct ory, which means you’ve hidden any files\\nthat are stored in the /etc/nginx/conf.d directory in the container image itself. \\n This is generally what happens in Linux when you mount a filesystem into a non-\\nempty directory. The directory then only co ntains the files from the mounted filesys-\\ntem, whereas the original files in that directory are inaccessible for as long as the\\nfilesystem is mounted. \\n In your case, this has no terrible side effects, but imagine mounting a volume to\\nthe /etc directory, which usually contains ma ny important files. This would most likely\\nbreak the whole container, because all of the original files that should be in the /etc\\ndirectory would no longer be there. If you ne ed to add a file to a directory like /etc,\\nyou can’t use this method at all.\\nMOUNTING  INDIVIDUAL  CONFIGMAP ENTRIES  AS FILES WITHOUT  HIDING  OTHER  FILES IN THE DIRECTORY\\nNaturally, you’re now wondering how to ad d individual files from a ConfigMap into\\nan existing directory without hiding exis ting files stored in it. An additional subPath\\nproperty on the volumeMount  allows you to mount either a single file or a single direc-\\ntory from the volume instead of mounting the whole volume. Perhaps this is easier to\\nexplain visually (see figure 7.10).\\n Say you have a configMap  volume containing a myconfig.conf file, which you want\\nto add to the /etc directory as someconfig.conf. You can use the subPath  property to\\nmount it there without affecting any other file s in that directory. The relevant part of\\nthe pod definition is shown in the following listing.\\nPod\\nContainer\\nFilesystem\\n/\\netc/\\nsomeconﬁg.confexistingﬁle1existingﬁle2ConﬁgMap: app-conﬁg\\nmyconﬁg.conf Contents\\nof the ﬁle\\nanother-ﬁle Contents\\nof the ﬁleconﬁgMap\\nvolume\\nmyconﬁg.conf\\nanother-ﬁle\\nexistingﬁle1\\nand existingﬁle2aren’t hidden.Only myconﬁg.conf is mounted\\ninto the container (yet under adifferent ﬁlename). another-ﬁle isn’tmounted into thecontainer.\\nFigure 7.10 Mounting a single file from a volume'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 242}, page_content='211 Decoupling configuration with a ConfigMap\\nspec:\\n  containers:  - image: some/image\\n    volumeMounts:\\n    - name: myvolume      mountPath: /etc/someconfig.conf     \\n      subPath: myconfig.conf            \\nThe subPath  property can be used when mounti ng any kind of volume. Instead of\\nmounting the whole volume, you can mount pa rt of it. But this method of mounting\\nindividual files has a relatively big deficien cy related to updating files. You’ll learn\\nmore about this in the following section, but first, let’s finish talking about the initial\\nstate of a configMap  volume by saying a few wo rds about file permissions.\\nSETTING  THE FILE PERMISSIONS  FOR FILES IN A CONFIG MAP VOLUME\\nBy default, the permissions on all files in a configMap  volume are set to 644 ( -rw-r—r-- ).\\nYou can change this by setting the defaultMode  property in the volume spec, as shown\\nin the following listing.\\n  volumes:\\n  - name: config\\n    configMap:\\n      name: fortune-config      defaultMode: \"6600\"       \\nAlthough ConfigMaps should be used for non-sensitive configuration data, you may\\nwant to make the file readable and writab le only to the user and group the file is\\nowned by, as the example in the previous listing shows. \\n7.4.7 Updating an app’s config without having to restart the app\\nWe’ve said that one of the drawbacks of using environment variables or command-line\\narguments as a configuration source is th e inability to update them while the pro-\\ncess is running. Using a ConfigMap and ex posing it through a volume brings the\\nability to update the configurat ion without having to recreate the pod or even restart\\nthe container. \\n When you update a ConfigMap, the file s in all the volumes referencing it are\\nupdated. It’s then up to the process to de tect that they’ve be en changed and reload\\nthem. But Kubernetes will most likely eventu ally also support sending a signal to the\\ncontainer after updating the files.\\nWARNING Be aware that as I’m writing this, it takes a surprisingly long time\\nfor the files to be updated after you update the ConfigMap (it can take up to\\none whole minute).Listing 7.17 A pod with a specific config map entry mounted into a specific file\\nListing 7.18 Setting file permissions: fort une-pod-configmap-volume-defaultMode.yaml You’re mounting into \\na file, not a directory.\\nInstead of mounting the whole \\nvolume, you’re only mounting \\nthe myconfig.conf entry.\\nThis sets the permissions \\nfor all files to -rw-rw------.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 243}, page_content='212 CHAPTER  7ConfigMaps and Secrets: configuring applications\\nEDITING  A CONFIG MAP\\nLet’s see how you can change a ConfigMap and have the process running in the pod\\nreload the files exposed in the configMap  volume. You’ll modify the Nginx config file\\nfrom your previous exampl e and make Nginx use the new config without restarting\\nthe pod. Try switching gzip compression off by editing the fortune-config  Config-\\nMap with kubectl  edit :\\n$ kubectl edit configmap fortune-config\\nOnce your editor opens, change the gzip  on line to gzip  off, save the file, and then\\nclose the editor. The ConfigMap is then updated, and soon afterward, the actual file\\nin the volume is updated as well. You can confirm this by printing the contents of the\\nfile with kubectl  exec :\\n$ kubectl exec fortune-configmap-volume -c web-server\\n➥  cat /etc/nginx/conf.d/my-nginx-config.conf\\nIf you don’t see the update yet, wait a while  and try again. It takes a while for the\\nfiles to get updated. Eventually, you’ll s ee the change in the config file, but you’ll\\nfind this has no effect on Nginx, because it doesn’t watch the files and reload them\\nautomatically. \\nSIGNALING  NGINX TO RELOAD  THE CONFIG\\nNginx will continue to compress its responses un til you tell it to reload its config files,\\nwhich you can do with the following command:\\n$ kubectl exec fortune-configmap-volume -c  web-server -- nginx -s reload\\nNow, if you try hitting the server again with curl , you should see the response is no\\nlonger compressed (it no longer contains the Content-Encoding:  gzip  header).\\nYou’ve effectively changed the app’s config without having to restart the container or\\nrecreate the pod. \\nUNDERSTANDING  HOW THE FILES ARE UPDATED  ATOMICALLY\\nYou may wonder what happens if an app can detect config file changes on its own and\\nreloads them before Kubernetes has fi nished updating all the files in the configMap\\nvolume. Luckily, this can’t happen, because all the files are updated atomically, which\\nmeans all updates occur at once. Kubernetes achieves this by using symbolic links. If\\nyou list all the files in the mounted configMap  volume, you’ll see something like the\\nfollowing listing.\\n$ kubectl exec -it fortune-configmap-volum e -c web-server -- ls -lA \\n➥  /etc/nginx/conf.d\\ntotal 4\\ndrwxr-xr-x  ... 12:15 ..4984_09_04_12_15_06.865837643Listing 7.19 Files in a mounted configMap  volume'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 244}, page_content='213 Using Secrets to pass sensitive data to containers\\nlrwxrwxrwx  ... 12:15 ..data -> ..4984_09_04_12_15_06.865837643\\nlrwxrwxrwx  ... 12:15 my-nginx -config.conf -> ..data/my-nginx-config.conf\\nlrwxrwxrwx  ... 12:15 sleep-interval -> ..data/sleep-interval\\nAs you can see, the files in the mounted configMap  volume are symbolic links point-\\ning to files in the ..data  dir. The ..data  dir is also a symbolic link pointing to a direc-\\ntory called ..4984_09_04_something . When the ConfigMap is updated, Kubernetes\\ncreates a new directory like this, writes a ll the files to it, and then re-links the ..data\\nsymbolic link to the new directory, ef fectively changing all files at once.\\nUNDERSTANDING  THAT FILES MOUNTED  INTO EXISTING  DIRECTORIES  DON’T GET UPDATED\\nOne big caveat relates to updating ConfigMa p-backed volumes. If you’ve mounted a\\nsingle file in the container instead of th e whole volume, the file will not be updated!\\nAt least, this is true at the time of writing this chapter. \\n For now, if you need to add an individua l file and have it updated when you update\\nits source ConfigMap, one workaround is to  mount the whole volume into a different\\ndirectory and then create a symbolic link po inting to the file in question. The sym-\\nlink can either be created in the contai ner image itself, or you could create the\\nsymlink when the container starts.\\nUNDERSTANDING  THE CONSEQUENCES  OF UPDATING  A CONFIG MAP\\nOne of the most important features of cont ainers is their immutability, which allows\\nus to be certain that no differences exis t between multiple running containers created\\nfrom the same image, so is it wrong to by pass this immutability by modifying a Config-\\nMap used by running containers? \\n The main problem occurs when the app doesn’t support reloading its configura-\\ntion. This results in different running in stances being configured differently—those\\npods that are created after the ConfigMap is  changed will use the new config, whereas\\nthe old pods will still use the old one. And th is isn’t limited to new pods. If a pod’s con-\\ntainer is restarted (for whatever reason), the new process will also see the new config.\\nTherefore, if the app doesn’t reload its co nfig automatically, modifying an existing\\nConfigMap (while pods are using it) may not be a good idea. \\n If the app does support reloading, modifying the ConfigMap usually isn’t such a\\nbig deal, but you do need to be aware th at because files in the ConfigMap volumes\\naren’t updated synchronously across all running  instances, the files in individual pods\\nmay be out of sync for up to a whole minute.\\n7.5 Using Secrets to pass sensitive data to containers\\nAll the information you’ve passed to your co ntainers so far is regular, non-sensitive\\nconfiguration data that doesn’t need to be  kept secure. But as we mentioned at the\\nstart of the chapter, the config usually also includes sensitive information, such as cre-\\ndentials and private encryption keys , which need to be kept secure.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 245}, page_content='214 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n7.5.1 Introducing Secrets\\nTo store and distribute such information, Ku bernetes provides a separate object called\\na Secret. Secrets are much like ConfigMaps —they’re also maps that hold key-value\\npairs. They can be used the same way as a ConfigMap. You can\\n\\uf0a1Pass Secret entries to the container as environment variables\\n\\uf0a1Expose Secret entries as files in a volume\\nKubernetes helps keep your Secrets safe by making sure each Secret is only distributed\\nto the nodes that run the pods that need  access to the Secret. Also, on the nodes\\nthemselves, Secrets are always stored in me mory and never written to physical storage,\\nwhich would require wiping the disks a fter deleting the Secrets from them. \\n On the master node itself (more specifically in etcd), Secrets used to be stored in\\nunencrypted form, which meant the master node  needs to be secured to keep the sensi-\\nt i v e  d a t a  s t o r e d  i n  S e c r e t s  s e c u r e .  T h i s  d i dn’t only include keeping the etcd storage\\nsecure, but also preventing unauthorized us ers from using the API server, because any-\\none who can create pods can mount the Secret  into the pod and gain access to the sen-\\nsitive data through it. From Kubernetes vers ion 1.7, etcd stores Secrets in encrypted\\nform, making the system much more secure. Because of this, it’s imperative you prop-\\nerly choose when to use a Secret or a Co nfigMap. Choosing between them is simple:\\n\\uf0a1Use a ConfigMap to store non-sens itive, plain configuration data.\\n\\uf0a1Use a Secret to store any data that is sensitive in nature and needs to be kept\\nunder key. If a config file includes bo th sensitive and not-sensitive data, you\\nshould store the file in a Secret.\\nYou already used Secrets in chapter 5, when you created a Secret to hold the TLS certifi-\\ncate needed for the Ingress resource. No w you’ll explore Secrets in more detail.\\n7.5.2 Introducing the default token Secret\\nYou’ll start learning about Secrets by exam ining a Secret that’s mounted into every\\ncontainer you run. You may have noticed it when using kubectl  describe  on a pod.\\nThe command’s output has always contained something like this:\\nVolumes:\\n  default-token-cfee9:\\n    Type:       Secret (a volume populated by a Secret)    SecretName: default-token-cfee9\\nEvery pod has a secret  volume attached to it automatically. The volume in the previ-\\nous kubectl  describe  output refers to a Secret called default-token-cfee9 . Because\\nSecrets are resources, you can list them with kubectl  get secrets  and find the\\ndefault-token  Secret in that list. Let’s see:\\n$ kubectl get secrets\\nNAME                  TYPE                                  DATA      AGE\\ndefault-token-cfee9   kubernetes.io/service-account-token   3         39d'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 246}, page_content='215 Using Secrets to pass sensitive data to containers\\nYou can also use kubectl  describe  to learn a bit more about it, as shown in the follow-\\ning listing.\\n$ kubectl describe secrets\\nName:        default-token-cfee9\\nNamespace:   default\\nLabels:      <none>Annotations: kubernetes.io/service-account.name=default\\n             kubernetes.io/service-account.uid=cc04bb39-b53f-42010af00237\\nType:        kubernetes.io/service-account-token\\nData\\n====\\nca.crt:      1139 bytes                                   namespace:   7 bytes                                      \\ntoken:       eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...      \\nYou can see that the Secret contains three entries— ca.crt , namespace , and token —\\nwhich represent everything you need to se curely talk to the Kubernetes API server\\nfrom within your pods, should you need to  do that. Although ideally you want your\\napplication to be completely Kubernetes-agnostic, when there’s no alternative other\\nthan to talk to Kubernetes directly, yo u’ll use the files provided through this secret\\nvolume. \\n The kubectl  describe  pod command shows where the secret  volume is mounted:\\nMounts:\\n  /var/run/secrets/kubernetes.io/serviceaccount  from default-token-cfee9\\nNOTE By default, the default-token  Secret is mounted into every container,\\nbut you can disable that in each pod by setting the automountService-\\nAccountToken  field in the pod spec to false  or by setting it to false  on the\\nservice account the pod is using. (You’l l learn about service accounts later in\\nthe book.)\\nTo help you visualize where and how the de fault token Secret is mounted, see fig-\\nure 7.11.\\n We’ve said Secrets are like ConfigMaps, so because this Secret contains three\\nentries, you can expect to see three files in the directory the secret  volume is mounted\\ninto. You can check this easily with kubectl  exec :\\n$ kubectl exec mypod ls /var/run/secrets/kubernetes.io/serviceaccount/\\nca.crt\\nnamespace\\ntoken\\nYou’ll see how your app can use these files to  access the API server in the next chapter.Listing 7.20 Describing a Secret\\nThis secret \\ncontains three \\nentries.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 247}, page_content='216 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n7.5.3 Creating a Secret\\nNow, you’ll create your own little Secret . You’ll improve your fortune-serving Nginx\\ncontainer by configuring it to also serve H TTPS traffic. For this, you need to create a\\ncertificate and a private key. The private key needs to be kept secure, so you’ll put it\\nand the certificate into a Secret.\\n First, generate the certific ate and private key files (do this on your local machine).\\nYou can also use the files in the book’s code archive (the cert and key files are in the\\nfortune-https  directory):\\n$ openssl genrsa -out https.key 2048\\n$ openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj \\n/CN=www.kubia-example.com\\nNow, to help better demons trate a few things about Secr ets, create an additional\\ndummy file called foo and make it contain the string bar. You’ll understand why you\\nneed to do this in a moment or two:\\n$ echo bar > foo\\nNow you can use kubectl  create  secret  to create a Secret from the three files:\\n$ kubectl create secret generic fortune-https --from-file=https.key\\n➥  --from-file=https.cert --from-file=foo\\nsecret \"fortune-https\" created\\nThis isn’t very different from creating Co nfigMaps. In this case, you’re creating a\\ngeneric  Secret called fortune-https  and including two entries in it (https.key with\\nthe contents of the https.key file and likewise for the https.cert key/file). As you\\nlearned earlier, you could also include the whole directory with --from-file=fortune-\\nhttps  instead of specifying each file individually.PodContainer\\nFilesystem\\n/\\nvar/\\nrun/\\nsecrets/\\nkubernetes.io/\\nserviceaccount/Default token Secret\\nDefault token\\nsecret\\nvolumeca.crt ...\\n...\\n...namespace\\ntoken\\nFigure 7.11 The default-token  Secret is created automatically and a corresponding \\nvolume is mounted in each pod automatically.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 248}, page_content='217 Using Secrets to pass sensitive data to containers\\nNOTE You’re creating a generic Secret, but you could also have created a tls\\nSecret with the kubectl  create  secret  tls command, as you did in chapter 5.\\nThis would create the Secret with different entry names, though.\\n7.5.4 Comparing ConfigMaps and Secrets\\nSecrets and ConfigMaps have a pretty big difference. This is what drove Kubernetes\\ndevelopers to create ConfigMaps after Kube rnetes had already supported Secrets for a\\nwhile. The following listing shows th e YAML of the Secret you created.\\n$ kubectl get secret fortune-https -o yaml\\napiVersion: v1data:\\n  foo: YmFyCg==\\n  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...\\nkind: Secret\\n...\\nNow compare this to the YAML of the Conf igMap you created earlier, which is shown\\nin the following listing.\\n$ kubectl get configmap fortune-config -o yaml\\napiVersion: v1\\ndata:  my-nginx-config.conf: |\\n    server {\\n      ...\\n    }\\n  sleep-interval: |    25\\nkind: ConfigMap\\n...\\nNotice the difference? The contents of a Secret’s entries are shown as Base64-encoded\\nstrings, whereas those of a ConfigMap are shown in clear text. This initially made\\nworking with Secrets in YAML and JSON manifests a bit more painful, because youhad to encode and decode them when setting and reading their entries. \\nUSING SECRETS  FOR BINARY  DATA\\nThe reason for using Base64 encoding is simple. A Secret’s entries can contain binaryvalues, not only plain-text. Base64 encoding allows you to include the binary data in\\nYAML or JSON, which are both plain-text formats. \\nTIP You can use Secrets even for non-sens itive binary data, but be aware that\\nthe maximum size of a Se cret is limited to 1MB.Listing 7.21 A Secret’s YAML definition\\nListing 7.22 A ConfigMap’s YAML definition'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 249}, page_content='218 CHAPTER  7ConfigMaps and Secrets: configuring applications\\nINTRODUCING  THE STRING DATA FIELD\\nBecause not all sensitive data is in binary fo rm, Kubernetes also allows setting a Secret’s\\nvalues through the stringData  field. The following listing shows how it’s used.\\nkind: Secret\\napiVersion: v1\\nstringData:             foo: plain text      \\ndata:\\n  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...\\nThe stringData  field is write-only (note: write-only, not read-only). It can only be\\nused to set values. When you re trieve the Secret’s YAML with kubectl  get -o yaml , the\\nstringData  field will not be shown. Instead, all entries you specified in the string-\\nData  field (such as the foo entry in the previous example) will be shown under data\\nand will be Base64-encoded like all the other entries. \\nREADING  A SECRET ’S ENTRY  IN A POD\\nWhen you expose the Secret to a container through a secret  volume, the value of the\\nSecret entry is decoded and written to the file in its actual form (regardless if it’s plain\\ntext or binary). The same is also true when  exposing the Secret entry through an envi-\\nronment variable. In both cases, the app do esn’t need to decode it, but can read the\\nfile’s contents or look up the environm ent variable value and use it directly.\\n7.5.5 Using the Secret in a pod\\nWith your fortune-https Secret containing bo th the cert and key files, all you need to\\ndo now is configure Nginx to use them. \\nMODIFYING  THE FORTUNE -CONFIG  CONFIG MAP TO ENABLE  HTTPS\\nFor this, you need to modify the config  file again by editing the ConfigMap:\\n$ kubectl edit configmap fortune-config\\nAfter the text editor opens, modify the part that defines the contents of the my-nginx-\\nconfig.conf  entry so it looks like the following listing.\\n...\\ndata:\\n  my-nginx-config.conf: |\\n    server {      listen              80;\\n      listen              443 ssl;\\n      server_name         www.kubia-example.com;Listing 7.23 Adding plain text entries to a Secret  using the stringData  field\\nListing 7.24 Modifying the fortune-config  ConfigMap’s dataThe stringData can be used \\nfor non-binary Secret data.\\nSee, “plain text” is not Base64-encoded.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 250}, page_content='219 Using Secrets to pass sensitive data to containers\\n      ssl_certificate     certs/https.cert;           \\n      ssl_certificate_key certs/https.key;            \\n      ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;      ssl_ciphers         HIGH:!aNULL:!MD5;\\n      location / {\\n        root   /usr/share/nginx/html;        index  index.html index.htm;\\n      }\\n    }\\n  sleep-interval: |\\n...\\nThis configures the server to read the cert ificate and key files from /etc/nginx/certs,\\nso you’ll need to mount the secret  volume there. \\nMOUNTING  THE FORTUNE -HTTPS  SECRET  IN A POD\\nNext, you’ll create a new fortune-https pod and mount the secret  volume holding\\nthe certificate and key into the proper location in the web-server  container, as shown\\nin the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: fortune-https\\nspec:\\n  containers:  - image: luksa/fortune:env\\n    name: html-generator\\n    env:    - name: INTERVAL\\n      valueFrom: \\n        configMapKeyRef:\\n          name: fortune-config\\n          key: sleep-interval    volumeMounts:\\n    - name: html\\n      mountPath: /var/htdocs  - image: nginx:alpine\\n    name: web-server\\n    volumeMounts:    - name: html\\n      mountPath: /usr/share/nginx/html\\n      readOnly: true    - name: config\\n      mountPath: /etc/nginx/conf.d\\n      readOnly: true    - name: certs                         \\n      mountPath: /etc/nginx/certs/        \\n      readOnly: true                          ports:\\n    - containerPort: 80Listing 7.25 YAML definition of the fortune-https  pod: fortune-pod-https.yamlThe paths are \\nrelative to /etc/nginx.\\nYou configured Nginx to read the cert and \\nkey file from /etc/nginx/certs, so you need \\nto mount the Secret volume there.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 251}, page_content='220 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n    - containerPort: 443\\n  volumes:\\n  - name: html    emptyDir: {}\\n  - name: config\\n    configMap:      name: fortune-config\\n      items:\\n      - key: my-nginx-config.conf\\n        path: https.conf\\n  - name: certs                                secret:                                \\n      secretName: fortune-https            \\nMuch is going on in this pod descriptor, so  let me help you visualize it. Figure 7.12\\nshows the components defined in the YAML. The default-token  Secret, volume, and\\nvolume mount, which aren’t part of the YAML, but are added to your pod automati-\\ncally, aren’t shown in the figure.\\nNOTE Like configMap  volumes, secret  volumes also support specifying file\\npermissions for the files expose d in the volume through the defaultMode\\nproperty.You define the secret \\nvolume here, referring to \\nthe fortune-https Secret.\\nContainer: web-server\\nContainer: html-generatorSecret: fortune-https\\nDefault token Secret and volume not shownsecret\\nvolume:\\ncerts\\nemptyDir\\nvolume:\\nhtmlconﬁgMap\\nvolume:\\nconﬁg\\nhttps.cert ...\\n...\\n...https.key\\nfoo/etc/nginx/conf.d/\\n/etc/nginx/certs/\\n/usr/share/nginx/html/\\n/var/htdocsConﬁgMap: fortune-conﬁg\\nmy-nginx-conﬁg.conf server {\\n…\\n}\\nPodEnvironment variables:\\nINTERVAL=25sleep-interval 25\\nFigure 7.12 Combining a ConfigMap and a Secret to run your fortune-https pod'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 252}, page_content='221 Using Secrets to pass sensitive data to containers\\nTESTING  WHETHER  NGINX IS USING  THE CERT AND KEY FROM THE SECRET\\nOnce the pod is running, you can see if it ’s serving HTTPS traffic by opening a port-\\nforward tunnel to the pod’s port 443 and us ing it to send a request to the server\\nwith curl : \\n$ kubectl port-forward fortune-https 8443:443 &\\nForwarding from 127.0.0.1:8443 -> 443\\nForwarding from [::1]:8443 -> 443\\n$ curl https://localhost:8443 -k\\nIf you configured the server properly, you should get a response. You can check the\\nserver’s certificate to see if it matches the one you generated earlier. This can also be\\ndone with curl  by turning on verbose logging using the -v option, as shown in the fol-\\nlowing listing.\\n$ curl https://localhost:8443 -k -v\\n* About to connect() to localhost port 8443 (#0)*   Trying ::1...\\n* Connected to localhost (::1) port 8443 (#0)\\n* Initializing NSS with certpath: sql:/etc/pki/nssdb* skipping SSL peer certificate verification\\n* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\\n* Server certificate:*   subject: CN=www.kubia-example.com          \\n*   start date: aug 16 18:43:13 2016 GMT       \\n*   expire date: aug 14 18:43:13 2026 GMT      *   common name: www.kubia-example.com         \\n*   issuer: CN=www.kubia-example.com           \\nUNDERSTANDING  SECRET  VOLUMES  ARE STORED  IN MEMORY\\nYou successfully delivered your certificate an d private key to your container by mount-\\ning a secret  volume in its directory tree at /etc/nginx/certs. The secret  volume uses\\nan in-memory filesystem (tmpfs) for the Secret  files. You can see this if you list mounts\\nin the container:\\n$ kubectl exec fortune-https -c web-server -- mount | grep certs\\ntmpfs on /etc/nginx/certs type tmpfs (ro,relatime) \\nBecause tmpfs is used, the sensitive data stored in the Secret is never written to disk,\\nwhere it could be compromised. \\nEXPOSING  A SECRET ’S ENTRIES  THROUGH  ENVIRONMENT  VARIABLES\\nInstead of using a volume, you could also have exposed individual entries from the\\nsecret  as environment variables, the way you did with the sleep-interval  entry from\\nthe ConfigMap. For example, if  you wanted to expose the foo key from your Secret as\\nenvironment variable FOO_SECRET , you’d add the snippet from the following listing to\\nthe container definition.Listing 7.26 Displaying the server certificate sent by Nginx\\nThe certificate \\nmatches the one you \\ncreated and stored in the Secret.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 253}, page_content='222 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n    env:\\n    - name: FOO_SECRET      valueFrom:                  \\n        secretKeyRef:             \\n          name: fortune-https              key: foo           \\nThis is almost exactly like when you set the INTERVAL environment variable, except\\nthat this time you’re referring to a Secret by using secretKeyRef  instead of config-\\nMapKeyRef , which is used to refer to a ConfigMap.\\n Even though Kubernetes enables you to expose Secrets through environment vari-\\nables, it may not be the best idea to use this feature. Applications usually dump envi-\\nronment variables in error reports or even wr ite them to the application log at startup,\\nwhich may unintentionally expose them. Addi tionally, child processes inherit all the\\nenvironment variables of the parent process, so if your app runs a third-party binary,\\nyou have no way of knowing what happens with your secret data. \\nTIP Think twice before using environmen t variables to pass your Secrets to\\nyour container, because they may get exposed inadvertently. To be safe, alwaysuse \\nsecret  volumes for exposing Secrets.\\n7.5.6 Understanding image pull Secrets\\nYou’ve learned how to pass Secrets to your applications and use the data they contain.\\nBut sometimes Kubernetes itself requires you to pass credentials to it—for example,\\nwhen you’d like to use images from a private container image registry. This is also\\ndone through Secrets.\\n Up to now all your container images have  been stored on public image registries,\\nwhich don’t require any special credentials to pull images from them. But most orga-\\nnizations don’t want their images to be available to everyone and thus use a private\\nimage registry. When deploying a pod, whose container images reside in a private reg-\\nistry, Kubernetes needs to know the creden tials required to pull the image. Let’s see\\nhow to do that.\\nUSING A PRIVATE  IMAGE  REPOSITORY  ON DOCKER  HUB\\nDocker Hub, in addition to public image repo sitories, also allows you to create private\\nrepositories. You can mark a reposito ry as private by logging in at http:/ /hub.docker\\n.com  with your web browser, finding the repository and checking a checkbox. \\n To run a pod, which uses an image from the private repository, you need to do\\ntwo things:\\n\\uf0a1Create a Secret holding the credentials for the Docker registry.\\n\\uf0a1Reference that Secret in the imagePullSecrets  field of the pod manifest.Listing 7.27 Exposing a Secret’s entry as an environment variable\\nThe variable should be set \\nfrom the entry of a Secret.\\nThe name of the Secret \\nholding the key The key of the Secret \\nto expose'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 254}, page_content='223 Using Secrets to pass sensitive data to containers\\nCREATING  A SECRET  FOR AUTHENTICATING  WITH A DOCKER  REGISTRY\\nCreating a Secret holding the credentials for authenticating with a Docker registry\\nisn’t that different from creating the generic Secret you created in section 7.5.3. Youuse the same \\nkubectl  create  secret  command, but with a different type and\\noptions:\\n$ kubectl create secret docker-registry mydockerhubsecret \\\\\\n  --docker-username=myusername --docker-password=mypassword \\\\   --docker-email=my.email@provider.com\\nRather than create a generic  secret, you’re creating a docker-registry  Secret called\\nmydockerhubsecret . You’re specifying your Docker Hub username, password, and\\nemail. If you inspect the contents of the newly created Secret with kubectl  describe ,\\nyou’ll see that it includes a single entry called .dockercfg . This is equivalent to the\\n.dockercfg file in your home directory, which is created by Docker when you run the\\ndocker  login  command.\\nUSING THE DOCKER -REGISTRY  SECRET  IN A POD DEFINITION\\nTo have Kubernetes use the Secret when  pulling images from your private Docker\\nHub repository, all you need to do is spec ify the Secret’s name in the pod spec, as\\nshown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: private-pod\\nspec:\\n  imagePullSecrets:                 \\n  - name: mydockerhubsecret         \\n  containers:  - image: username/private:tag\\n    name: main\\nIn the pod definition in the previo us listing, you’re specifying the mydockerhubsecret\\nSecret as one of the imagePullSecrets . I suggest you try this out yourself, because it’s\\nlikely you’ll deal with private container images soon.\\nNOT HAVING  TO SPECIFY  IMAGE  PULL SECRETS  ON EVERY  POD\\nGiven that people usually run many different  pods in their systems, it makes you won-\\nder if you need to add the same image pull Se crets to every pod. Luckily, that’s not the\\ncase. In chapter 12 you’ll learn how image pull Secrets can be added to all your pods\\nautomatically if you add the Secrets to a ServiceAccount.Listing 7.28 A pod definition using an imag e pull Secret: pod-with-private-image.yaml\\nThis enables pulling images \\nfrom a private image registry.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 255}, page_content='224 CHAPTER  7ConfigMaps and Secrets: configuring applications\\n7.6 Summary\\nThis wraps up this chapter on how to pa ss configuration data to containers. You’ve\\nlearned how to\\n\\uf0a1Override the default command defined in a container image in the pod definition\\n\\uf0a1Pass command-line arguments to the main container process\\n\\uf0a1Set environment variables for a container\\n\\uf0a1Decouple configuration from a pod specif ication and put it into a ConfigMap\\n\\uf0a1Store sensitive data in a Secret an d deliver it securely to containers\\n\\uf0a1Create a docker-registry  Secret and use it to pull images from a private image\\nregistry\\nIn the next chapter, you’ll learn how to pass pod and container metadata to applica-\\ntions running inside them. You’ll also se e how the default token Secret, which we\\nlearned about in this chapter, is used to talk to the API server from within a pod.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 256}, page_content='225Accessing pod metadata\\nand other resources\\nfrom applications\\nApplications often need information a bout the environment they’re running in,\\nincluding details about themselves and th at of other components in the cluster.\\nYou’ve already seen how Kubernetes en ables service discovery through environ-\\nment variables or DNS, but what about ot her information? In this chapter, you’ll\\nsee how certain pod and container metadata  can be passed to the container and\\nhow easy it is for an app running inside a container to talk to the Kubernetes API\\nserver to get information about the resour ces deployed in the cluster and even how\\nto create or modify those resources.This chapter covers\\n\\uf0a1Using the Downward API to pass information into \\ncontainers\\n\\uf0a1Exploring the Kubernetes REST API\\n\\uf0a1Leaving authentication and server verification to \\nkubectl  proxy\\n\\uf0a1Accessing the API server from within a container\\n\\uf0a1Understanding the ambassador container pattern\\n\\uf0a1Using Kubernetes client libraries'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 257}, page_content='226 CHAPTER  8Accessing pod metadata and other resources from applications\\n8.1 Passing metadata through the Downward API\\nIn the previous chapter you saw how you ca n pass configuration data to your appli-\\ncations through environment variables or through configMap  and secret  volumes.\\nThis works well for data that you set your self and that is known before the pod is\\nscheduled to a node and run there. But what about data that isn’t known up until\\nthat point—such as the pod’s IP, the host  node’s name, or even the pod’s own name\\n(when the name is generated; for example,  when the pod is created by a ReplicaSet\\nor similar controller)? And what about data that’s already specified elsewhere, such\\nas a pod’s labels and annotations? You don’ t want to repeat the same information in\\nmultiple places.\\n Both these problems are solved by the Ku bernetes Downward API. It allows you to\\npass metadata about the pod and its environment through environment variables orfiles (in a \\ndownwardAPI  volume). Don’t be confused by the name. The Downward API\\nisn’t like a REST endpoint that your app needs to hit so it can get the data. It’s a way of\\nhaving environment variables or files popula ted with values from the pod’s specifica-\\ntion or status, as shown in figure 8.1.\\n8.1.1 Understanding the available metadata\\nThe Downward API enables you to expose the pod’s own metadata to the processes\\nrunning inside that pod. Currently, it al lows you to pass the following information to\\nyour containers:\\n\\uf0a1The pod’s name\\n\\uf0a1The pod’s IP addressContainer: main\\nEnvironment\\nvariablesAPI serverUsed to initialize environment\\nvariables and ﬁles in thedownwardAPI volume\\nPod manifest\\n- Metadata\\n- Status\\nPoddownwardAPI\\nvolumeApp process\\nFigure 8.1 The Downward API exposes pod metadat a through environment variables or files.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 258}, page_content='227 Passing metadata through the Downward API\\n\\uf0a1The namespace the pod belongs to\\n\\uf0a1The name of the node the pod is running on\\n\\uf0a1The name of the service account the pod is running under\\n\\uf0a1The CPU and memory requests for each container\\n\\uf0a1The CPU and memory limits for each container\\n\\uf0a1The pod’s labels\\n\\uf0a1The pod’s annotations\\nMost of the items in the list shouldn’t requ ire further explanation, except perhaps the\\nservice account and CPU/memory requests and limits, which we haven’t introduced\\nyet. We’ll cover service accounts in detail in chapter 12. For now, all you need to know\\nis that a service account is the account that the pod authenticates as when talking tothe API server. CPU and memory requests and limits are explained in chapter 14.\\nThey’re the amount of CPU and memory gu aranteed to a container and the maxi-\\nmum amount it can get.\\n Most items in the list can be passed to co ntainers either through environment vari-\\nables or through a \\ndownwardAPI  volume, but labels and annotations can only be\\nexposed through the volume. Part of the da ta can be acquired by other means (for\\nexample, from the operating system direct ly), but the Downward API provides a sim-\\npler alternative.\\n Let’s look at an example to pass me tadata to your containerized process.\\n8.1.2 Exposing metadata through environment variables\\nFirst, let’s look at how you can pass the pod’s and container’s metadata to the con-\\ntainer through environment variables. You’ ll create a simple single-container pod\\nfrom the following listing’s manifest.\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: downward\\nspec:\\n  containers:  - name: main\\n    image: busybox\\n    command: [\"sleep\", \"9999999\"]    resources:\\n      requests:\\n        cpu: 15m        memory: 100Ki\\n      limits:\\n        cpu: 100m        memory: 4Mi\\n    env:\\n    - name: POD_NAMEListing 8.1 Downward API used in environment variables: downward-api-env.yaml'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 259}, page_content='228 CHAPTER  8Accessing pod metadata and other resources from applications\\n      valueFrom:                            \\n        fieldRef:                           \\n          fieldPath: metadata.name              - name: POD_NAMESPACE\\n      valueFrom:\\n        fieldRef:          fieldPath: metadata.namespace\\n    - name: POD_IP\\n      valueFrom:\\n        fieldRef:\\n          fieldPath: status.podIP    - name: NODE_NAME\\n      valueFrom:\\n        fieldRef:          fieldPath: spec.nodeName\\n    - name: SERVICE_ACCOUNT\\n      valueFrom:        fieldRef:\\n          fieldPath: spec.serviceAccountName\\n    - name: CONTAINER_CPU_REQUEST_MILLICORES      valueFrom:                                   \\n        resourceFieldRef:                          \\n          resource: requests.cpu                             divisor: 1m                            \\n    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES\\n      valueFrom:        resourceFieldRef:\\n          resource: limits.memory\\n          divisor: 1Ki\\nWhen your process runs, it can look up al l the environment variables you defined in\\nthe pod spec. Figure 8.2 shows the environm ent variables and the sources of their val-\\nues. The pod’s name, IP, and namesp ace will be exposed through the POD_NAME ,\\nPOD_IP , and POD_NAMESPACE  environment variables, respectively. The name of the\\nnode the container is running on  will be exposed through the NODE_NAME  variable.\\nThe name of the service account is made available through the SERVICE_ACCOUNT\\nenvironment variable. You’re also creating  two environment variables that will hold\\nthe amount of CPU requested for this co ntainer and the maximum amount of mem-\\nory the container is allowed to consume.\\n For environment variables exposing resource  limits or requests, you specify a divi-\\nsor. The actual value of the limit or the re quest will be divided by the divisor and the\\nresult exposed through the environment variab le. In the previous example, you’re set-\\nting the divisor for CPU requests to 1m (one milli-core, or one one-thousandth of a\\nCPU core). Because you’ve set the CPU request to 15m, the environment variable\\nCONTAINER_CPU_R EQUEST_MILLICORES  will be set to 15. Likewise, you set the memory\\nlimit to 4Mi (4 mebibytes) and the divisor to 1Ki (1 Kibibyte), so the CONTAINER_MEMORY\\n_LIMIT_KIBIBYTES  environment variable will be set to 4096 . Instead of specifying an absolute value, \\nyou’re referencing the metadata.name \\nfield from the pod manifest.\\nA container’s CPU and memory \\nrequests and limits are referenced \\nby using resourceFieldRef instead of fieldRef.\\nFor resource fields, you \\ndefine a divisor to get the \\nvalue in the unit you need.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 260}, page_content='229 Passing metadata through the Downward API\\nThe divisor for CPU limits and requests can be either 1, which means one whole core,\\nor 1m, which is one millicore. The divisor for memory limits/requests can be 1 (byte),\\n1k (kilobyte) or 1Ki (kibibyte), 1M (megabyte) or 1Mi (mebibyte), and so on.\\n After creating the pod, you can use kubectl  exec  to see all these environment vari-\\nables in your container, as shown in the following listing.\\n$ kubectl exec downward env\\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\nHOSTNAME=downwardCONTAINER_MEMORY_LIMIT_KIBIBYTES=4096\\nPOD_NAME=downward\\nPOD_NAMESPACE=defaultPOD_IP=10.0.0.10\\nNODE_NAME=gke-kubia-default-pool-32a2cac8-sgl7\\nSERVICE_ACCOUNT=defaultCONTAINER_CPU_REQUEST_MILLICORES=15\\nKUBERNETES_SERVICE_HOST=10.3.240.1\\nKUBERNETES_SERVICE_PORT=443...Listing 8.2 Environment variables in the downward podPod manifest\\nmetadata:\\nname: downwardnamespace: default\\nspec:\\nnodeName: minikubeserviceAccountName: defaultcontainers:- name: main\\nimage: busybox\\ncommand: [\"sleep\", \"9999999\"]resources:\\nrequests:\\ncpu: 15mmemory: 100Ki\\nlimits:\\ncpu: 100mmemory: 4Mi\\n...\\nstatus:\\npodIP: 172.17.0.4...\\nPod: downwardContainer: main\\nEnvironment variables\\nPOD_NAME=downward\\nPOD_NAMESPACE=defaultPOD_IP=172.17.0.4NODE_NAME=minikubeSERVICE_ACCOUNT=defaultCONTAINER_CPU_REQUEST_MILLICORES=15CONTAINER_MEMORY_LIMIT_KIBIBYTES=4096\\ndivisor: 1m\\ndivisor: 1Ki\\nFigure 8.2 Pod metadata and attributes can be expo sed to the pod through environment variables.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 261}, page_content='230 CHAPTER  8Accessing pod metadata and other resources from applications\\nAll processes running inside the container can read those variables and use them how-\\never they need. \\n8.1.3 Passing metadata through files in a downwardAPI volume\\nIf you prefer to expose the metadata thro ugh files instead of environment variables,\\nyou can define a downwardAPI  volume and mount it into your container. You must use\\na downwardAPI  volume for exposing the pod’s labe ls or its annotations, because nei-\\nther can be exposed through environmen t variables. We’ll discuss why later.\\n As with environment variables, you need to  specify each metadata field explicitly if\\nyou want to have it exposed to the process.  Let’s see how to modify the previous exam-\\nple to use a volume instead of environment va riables, as shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: downward  labels:                  \\n    foo: bar               \\n  annotations:                 key1: value1           \\n    key2: |                \\n      multi                      line                 \\n      value                \\nspec:  containers:\\n  - name: main\\n    image: busybox\\n    command: [\"sleep\", \"9999999\"]\\n    resources:      requests:\\n        cpu: 15m\\n        memory: 100Ki      limits:\\n        cpu: 100m\\n        memory: 4Mi    volumeMounts:                        \\n    - name: downward                     \\n      mountPath: /etc/downward             volumes:\\n  - name: downward                 \\n    downwardAPI:                         items:\\n      - path: \"podName\"                     \\n        fieldRef:                                     fieldPath: metadata.name          \\n      - path: \"podNamespace\"\\n        fieldRef:          fieldPath: metadata.namespaceListing 8.3 Pod with a downwardAPI  volume: downward-api-volume.yaml\\nThese labels and \\nannotations will be exposed through the \\ndownwardAPI volume.\\nYou’re mounting the \\ndownward volume under /etc/downward.\\nYou’re defining a downwardAPI \\nvolume with the name downward.\\nThe pod’s name (from the metadata.name \\nfield in the manifest) will be written to \\nthe podName file.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 262}, page_content='231 Passing metadata through the Downward API\\n      - path: \"labels\"                       \\n        fieldRef:                            \\n          fieldPath: metadata.labels               - path: \"annotations\"                       \\n        fieldRef:                                 \\n          fieldPath: metadata.annotations               - path: \"containerCpuRequestMilliCores\"\\n        resourceFieldRef:\\n          containerName: main\\n          resource: requests.cpu\\n          divisor: 1m      - path: \"containerMemoryLimitBytes\"\\n        resourceFieldRef:\\n          containerName: main          resource: limits.memory\\n          divisor: 1\\nInstead of passing the metadata through envi ronment variables, you’re defining a vol-\\nume called downward  and mounting it in your cont ainer under /etc/downward. The\\nfiles this volume will cont ain are configured under the downwardAPI.items  attribute\\nin the volume specification.\\n Each item specifies the path  (the filename) where the metadata should be written\\nto and references either a pod-level field or a container resource field whose value you\\nwant stored in the file (see figure 8.3).The pod’s labels will be written \\nto the /etc/downward/labels file.\\nThe pod’s annotations will be \\nwritten to the /etc/downward/\\nannotations file.\\ndownwardAPI volumePod manifest\\nmetadata:\\nname: downwardnamespace: defaultlabels:\\nfoo: bar\\nannotations:\\nkey1: value1...\\nspec:\\ncontainers:- name: main\\nimage: busybox\\ncommand: [\"sleep\", \"9999999\"]resources:\\nrequests:\\ncpu: 15mmemory: 100Ki\\nlimits:\\ncpu: 100mmemory: 4Mi\\n.../podName\\n/podNamespace/labels/annotations/containerCpuRequestMilliCores/containerMemoryLimitBytes\\ndivisor: 1divisor: 1mContainer: main\\nPod: downwardFilesystem\\n/\\netc/\\ndownward/\\nFigure 8.3 Using a downwardAPI  volume to pass metadata to the container'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 263}, page_content='232 CHAPTER  8Accessing pod metadata and other resources from applications\\nDelete the previous pod and create a new on e from the manifest in the previous list-\\ning. Then look at the contents of the mounted downwardAPI  volume directory. You\\nmounted the volume under /etc/downward/, so list the files in there, as shown in the\\nfollowing listing.\\n$ kubectl exec downward ls -lL /etc/downward\\n-rw-r--r--   1 root   root   134 May 25 10:23 annotations\\n-rw-r--r--   1 root   root     2 May 25 10:23 containerCpuRequestMilliCores\\n-rw-r--r--   1 root   root     7 May 25 10:23 containerMemoryLimitBytes\\n-rw-r--r--   1 root   root     9 May 25 10:23 labels\\n-rw-r--r--   1 root   root     8 May 25 10:23 podName\\n-rw-r--r--   1 root   root     7 May 25 10:23 podNamespace\\nNOTE As with the configMap  and secret  volumes, you can change the file\\npermissions through the downwardAPI  volume’s defaultMode  property in the\\npod spec.\\nE a c h  f i l e  c o r r e s p o n d s  t o  a n  i t e m  i n  t h e  v o l u m e ’ s  d e f i n i t i o n .  T h e  c o n t e n t s  o f  f i l e s ,\\nwhich correspond to the same metadata fiel ds as in the previous example, are the\\nsame as the values of environment variable s you used before, so we won’t show them\\nhere. But because you couldn’t expose la bels and annotations through environment\\nvariables before, examine the following listi ng for the contents of the two files you\\nexposed them in.\\n$ kubectl exec downward cat /etc/downward/labels\\nfoo=\"bar\"\\n$ kubectl exec downward cat /etc/downward/annotations\\nkey1=\"value1\"\\nkey2=\"multi\\\\nline\\\\nvalue\\\\n\"\\nkubernetes.io/config.seen=\"2016-11-28T14:27:45.664924282Z\"kubernetes.io/config.source=\"api\"\\nAs you can see, each label/annotation is written in the key=value  format on a sepa-\\nrate line. Multi-line values are written to a single line with newline characters denoted\\nwith \\\\n.\\nUPDATING  LABELS  AND ANNOTATIONS\\nYou may remember that labels and annotati ons can be modified while a pod is run-\\nning. As you might expect, when they chan ge, Kubernetes updates the files holding\\nthem, allowing the pod to always see up-to-dat e data. This also explains why labels and\\nannotations can’t be exposed through envi ronment variables. Because environment\\nvariable values can’t be updated afterward, if the labels or annotations of a pod were\\nexposed through environment variables, there’ s no way to expose the new values after\\nthey’re modified.Listing 8.4 Files in the downwardAPI  volume\\nListing 8.5 Displaying labels and annotations in the downwardAPI  volume'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 264}, page_content='233 Talking to the Kubernetes API server\\nREFERRING  TO CONTAINER -LEVEL METADATA  IN THE VOLUME  SPECIFICATION\\nBefore we wrap up this section, we need to point out one thing. When exposing con-\\ntainer-level metadata, such as a container’ s resource limit or requests (done using\\nresourceFieldRef ), you need to specify the name  of the container whose resource\\nfield you’re referencing, as shown in the following listing.\\nspec:\\n  volumes:  - name: downward                       \\n    downwardAPI:                         \\n      items:      - path: \"containerCpuRequestMilliCores\"\\n        resourceFieldRef:\\n          containerName: main                 resource: requests.cpu\\n          divisor: 1m\\nThe reason for this becomes obvious if you consider that volumes are defined at the\\npod level, not at the container level. When  referring to a container’s resource field\\ninside a volume specification, you need to explicitly specify the name of the container\\nyou’re referring to. This is true even for single-container pods. \\n Using volumes to expose a container’s reso urce requests and/or limits is slightly\\nmore complicated than using environment vari ables, but the benefit is that it allows\\nyou to pass one container’s resource fields  to a different cont ainer if needed (but\\nboth containers need to be in the same pod). With environment variables, a container\\ncan only be passed its own resource limits and requests. \\nUNDERSTANDING  WHEN TO USE THE DOWNWARD  API\\nAs you’ve seen, using the Downward API isn’ t complicated. It allows you to keep the\\napplication Kubernetes-agnostic. This is especially useful when you’re dealing with an\\nexisting application that expects certain data in environment variables. The Down-\\nward API allows you to expose the data to  the application without having to rewrite\\nthe application or wrap it in a shell script , which collects the data and then exposes it\\nthrough environment variables.\\n But the metadata available through the Downward API is fairly limited. If you need\\nmore, you’ll need to obtain it from the Kubernetes API server directly. You’ll learn\\nhow to do that next.\\n8.2 Talking to the Kubernetes API server\\nWe’ve seen how the Downward API provides a simple way to pass certain pod and con-\\ntainer metadata to the process running in side them. It only exposes the pod’s own\\nmetadata and a subset of all of the pod’s data. But sometimes your app will need to\\nknow more about other pods and even othe r resources defined in your cluster. The\\nDownward API doesn’t help in those cases.Listing 8.6 Referring to container-level metadata in a downwardAPI  volume\\nContainer name \\nmust be specified'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 265}, page_content='234 CHAPTER  8Accessing pod metadata and other resources from applications\\n As you’ve seen throughout the book, in formation about services and pods can be\\nobtained by looking at the service-related environment variables or through DNS. But\\nwhen the app needs data about other resource s or when it requires access to the most\\nup-to-date information as possible, it needs to  talk to the API server directly (as shown\\nin figure 8.4).\\nBefore you see how apps within pods can talk  to the Kubernetes API server, let’s first\\nexplore the server’s REST endpoints from your local machine, so you can see what\\ntalking to the API server looks like.\\n8.2.1 Exploring the Kubernetes REST API\\nYou’ve learned about different Kubernetes re source types. But if you’re planning on\\ndeveloping apps that talk to the Kubernetes  API, you’ll want to know the API first. \\n To do that, you can try hitting the API se rver directly. You can get its URL by run-\\nning kubectl  cluster-info :\\n$ kubectl cluster-info\\nKubernetes master is running at https://192.168.99.100:8443\\nBecause the server uses HTTPS and requires authentication, it’s not simple to talk to\\nit directly. You can try accessing it with curl  and using curl ’s --insecure  (or -k)\\noption to skip the server certificat e check, but that doesn’t get you far:\\n$ curl https://192.168.99.100:8443 -k\\nUnauthorized\\nLuckily, rather than dealing with authenti cation yourself, you can talk to the server\\nthrough a proxy by running the kubectl  proxy  command. \\nACCESSING  THE API SERVER  THROUGH  KUBECTL  PROXY  \\nThe kubectl  proxy  command runs a proxy server th at accepts HTTP connections on\\nyour local machine and proxies them to the API server while taking care of authenti-\\ncation, so you don’t need to pass the authen tication token in every request. It also\\nmakes sure you’re talking to the actual AP I server and not a man in the middle (by\\nverifying the server’s certificate on each request).ContainerAPI server\\nPodApp process\\nAPI objects\\nFigure 8.4 Talking to the API server \\nfrom inside a pod to get information \\nabout other API objects'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 266}, page_content='235 Talking to the Kubernetes API server\\n Running the proxy is trivial. All you ne ed to do is run the following command:\\n$ kubectl proxy\\nStarting to serve on 127.0.0.1:8001\\nYou don’t need to pass in any other arguments, because kubectl  already knows every-\\nthing it needs (the API server URL, authorization token, and so on). As soon as it starts\\nup, the proxy starts accepting connections on local port 8001. Let’s see if it works:\\n$ curl localhost:8001\\n{  \"paths\": [\\n    \"/api\",\\n    \"/api/v1\",    ...\\nVoila! You sent the request to the proxy, it  sent a request to the API server, and then\\nthe proxy returned whatever the server  returned. Now, let’s start exploring.\\nEXPLORING  THE KUBERNETES  API THROUGH  THE KUBECTL  PROXY\\nYou can continue to use curl , or you can open your web browser and point it to\\nhttp:/ /localhost:8001 . Let’s examine what the API server returns when you hit its base\\nURL more closely. The server responds with  a list of paths, as shown in the follow-\\ning listing.\\n$ curl http://localhost:8001\\n{\\n  \"paths\": [\\n    \"/api\",\\n    \"/api/v1\",                      \"/apis\",\\n    \"/apis/apps\",\\n    \"/apis/apps/v1beta1\",    ...\\n    \"/apis/batch\",              \\n    \"/apis/batch/v1\",               \"/apis/batch/v2alpha1\",     \\n    ...\\nThese paths correspond to the API groups and versions you specify in your resource\\ndefinitions when creating resources such as Pods, Services, and so on. \\n You may recognize the batch/v1  in the /apis/batch/v1  path as the API group and\\nversion of the Job resources you learne d about in chapter 4. Likewise, the /api/v1\\ncorresponds to the apiVersion:  v1 you refer to in the comm on resources you created\\n(Pods, Services, ReplicationControllers, and so on). The most common resource\\ntypes, which were introduced in the earlie st versions of Kubernetes, don’t belong toListing 8.7 Listing the API server’s RE ST endpoints: http://localhost:8001\\nMost resource types \\ncan be found here.\\nThe batch API \\ngroup and its \\ntwo versions'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 267}, page_content='236 CHAPTER  8Accessing pod metadata and other resources from applications\\nany specific group, because Kubernetes init ially didn’t even use the concept of API\\ngroups; they were introduced later. \\nNOTE These initial resource types withou t an API group are now considered\\nto belong to the core API group.\\nEXPLORING  THE BATCH  API GROUP ’S REST ENDPOINT\\nLet’s explore the Job resource API. You’ll start by looking at what’s behind the\\n/apis/batch  path (you’ll omit the version for no w), as shown in the following listing.\\n$ curl http://localhost:8001/apis/batch\\n{  \"kind\": \"APIGroup\",\\n  \"apiVersion\": \"v1\",\\n  \"name\": \"batch\",  \"versions\": [\\n    {\\n      \"groupVersion\": \"batch/v1\",                   \"version\": \"v1\"                         \\n    },\\n    {      \"groupVersion\": \"batch/v2alpha1\",       \\n      \"version\": \"v2alpha1\"                   \\n    }  ],\\n  \"preferredVersion\": {                    \\n    \"groupVersion\": \"batch/v1\",                \"version\": \"v1\"                        \\n  },\\n  \"serverAddressByClientCIDRs\": null\\n}\\nThe response shows a description of the batch  API group, including the available ver-\\nsions and the preferred version clients sh ould use. Let’s continue and see what’s\\nbehind the /apis/batch/v1  path. It’s shown in the following listing.\\n$ curl http://localhost:8001/apis/batch/v1\\n{\\n  \"kind\": \"APIResourceList\",                \"apiVersion\": \"v1\",\\n  \"groupVersion\": \"batch/v1\",             \\n  \"resources\": [                              {\\n      \"name\": \"jobs\",             \\n      \"namespaced\": true,               \"kind\": \"Job\",              Listing 8.8 Listing endpoints under /apis/batch : http://localhost:8001/apis/batch\\nListing 8.9 Resource types in batch/v1 : http://localhost:8001/apis/batch/v1The batch API \\ngroup contains \\ntwo versions.\\nClients should use the \\nv1 version instead of \\nv2alpha 1.\\nThis is a list of API resources \\nin the batch/v 1 API group.\\nHere’s an array holding \\nall the resource types \\nin this group.This describes the \\nJob resource, which \\nis namespaced.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 268}, page_content='237 Talking to the Kubernetes API server\\n      \"verbs\": [                 \\n        \"create\",                \\n        \"delete\",                        \"deletecollection\",      \\n        \"get\",                   \\n        \"list\",                          \"patch\",                 \\n        \"update\",                \\n        \"watch\"                  \\n      ]\\n    },    {\\n      \"name\": \"jobs/status\",            \\n      \"namespaced\": true,                        \"kind\": \"Job\",\\n      \"verbs\": [             \\n        \"get\",                       \"patch\",             \\n        \"update\"             \\n      ]    }\\n  ]\\n}\\nAs you can see, the API server returns a list of resource types and REST endpoints in\\nthe batch/v1  API group. One of those is the Job resource. In addition to the name  of\\nthe resource and the associated kind , the API server also includes information on\\nwhether the resource is namespaced  or not, its short name (if it has one; Jobs don’t),\\nand a list of verbs  you can use with the resource. \\n The returned list describes the REST resources exposed in the API server. The\\n\"name\":  \"jobs\"  line tells you that the API contains the /apis/batch/v1/jobs  end-\\npoint. The \"verbs\"  array says you can retrieve, upd ate, and delete Job resources\\nthrough that endpoint. For certain resour ces, additional API endpoints are also\\nexposed (such as the jobs/status  path, which allows modifying only the status of\\naJ o b ) .\\nLISTING  ALL JOB INSTANCES  IN THE CLUSTER\\nTo get a list of Jobs in your clus ter, perform a GET request on path /apis/batch/\\nv1/jobs , as shown in the following listing.\\n$ curl http://localhost:8001/apis/batch/v1/jobs\\n{  \"kind\": \"JobList\",\\n  \"apiVersion\": \"batch/v1\",\\n  \"metadata\": {    \"selfLink\": \"/apis/batch/v1/jobs\",\\n    \"resourceVersion\": \"225162\"\\n  },Listing 8.10 List of Jobs: http://localhost:8001/apis/batch/v1/jobsHere are the verbs that can be used \\nwith this resource (you can create \\nJobs; delete individual ones or a collection of them; and retrieve, \\nwatch, and update them).\\nResources also have a \\nspecial REST endpoint for \\nmodifying their status.\\nThe status can be \\nretrieved, patched, \\nor updated.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 269}, page_content='238 CHAPTER  8Accessing pod metadata and other resources from applications\\n  \"items\": [\\n    {\\n      \"metadata\": {        \"name\": \"my-job\",\\n        \"namespace\": \"default\",\\n        ...\\nYou probably have no Job resources deployed in  your cluster, so the items array will be\\nempty. You can try deploying the Job in Chapter08/my-job.yaml and hitting the REST\\nendpoint again to get the same  output as in listing 8.10.\\nRETRIEVING  A SPECIFIC  JOB INSTANCE  BY NAME\\nThe previous endpoint returned a list of all Jobs across all namespaces. To get back\\nonly one specific Job, you need to spec ify its name and namespace in the URL. To\\nretrieve the Job shown in the previous listing ( name:  my-job ; namespace:  default ),\\nyou need to request the following path: /apis/batch/v1/name spaces/default/jobs/\\nmy-job , as shown in the following listing.\\n$ curl http://localhost:8001/a pis/batch/v1/namespaces/default/jobs/my-job\\n{\\n  \"kind\": \"Job\",  \"apiVersion\": \"batch/v1\",\\n  \"metadata\": {\\n    \"name\": \"my-job\",    \"namespace\": \"default\",\\n    ...\\nAs you can see, you get back the complete JSON definition of the my-job  Job resource,\\nexactly like you do if you run:\\n$ kubectl get job my-job -o json\\nYou’ve seen that you can browse the Kube rnetes REST API server without using any\\nspecial tools, but to fully explore the REST API and interact with it, a better option is\\ndescribed at the end of this chap ter. For now, exploring it with curl  like this is enough\\nto make you understand how an applicatio n running in a pod talks to Kubernetes. \\n8.2.2 Talking to the API server from within a pod\\nYou’ve learned how to talk to the API se rver from your local machine, using the\\nkubectl  proxy . Now, let’s see how to talk to it fr om within a pod, where you (usually)\\ndon’t have kubectl . Therefore, to talk to the API server from inside a pod, you need\\nto take care of three things:\\n\\uf0a1Find the location of the API server.\\n\\uf0a1Make sure you’re talking to the API se rver and not something impersonating it.\\n\\uf0a1Authenticate with the server; otherwise it won’t let you see or do anything.Listing 8.11 Retrieving a resource  in a specific namespace by name'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 270}, page_content='239 Talking to the Kubernetes API server\\nYou’ll see how this is done in the next three sections. \\nRUNNING  A POD TO TRY OUT COMMUNICATION  WITH THE API SERVER\\nThe first thing you need is a pod from which to talk to the API se rver. You’ll run a pod\\nthat does nothing (it runs the sleep  command in its only container), and then run a\\nshell in the container with kubectl  exec . Then you’ll try to access the API server from\\nwithin that shell using curl .\\n Therefore, you need to use a co ntainer image that contains the curl  binary. If you\\nsearch for such an image on, say,  Docker Hub, you’ll find the tutum/curl  image, so\\nuse it (you can also use any other existing image containing the curl  binary or you\\ncan build your own). The pod definition is shown in the following listing.\\napiVersion: v1\\nkind: Podmetadata:\\n  name: curl\\nspec:  containers:\\n  - name: main\\n    image: tutum/curl                    command: [\"sleep\", \"9999999\"]    \\nAfter creating the pod, run kubectl  exec  to run a bash shell inside its container:\\n$ kubectl exec -it curl bashroot@curl:/#\\nYou’re now ready to talk to the API server.\\nFINDING  THE API SERVER ’S ADDRESS\\nFirst, you need to find the IP and port of  the Kubernetes API server. This is easy,\\nbecause a Service called kubernetes  is automatically expose d in the default name-\\nspace and configured to point to the API server. You may remember seeing it every\\ntime you listed services with kubectl  get svc:\\n$ kubectl get svc\\nNAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\\nkubernetes   10.0.0.1     <none>        443/TCP   46d\\nAnd you’ll remember from chapter 5 that environment variables are configured for\\neach service. You can get both the IP address and the port of the API server by looking\\nup the KUBERNETES_SERVICE_HOST  and KUBERNETES_SERVICE_PORT  variables (inside\\nthe container):\\nroot@curl:/# env | grep KUBERNETES_SERVICE\\nKUBERNETES_SERVICE_PORT=443\\nKUBERNETES_SERVICE_HOST=10.0.0.1KUBERNETES_SERVICE_PORT_HTTPS=443Listing 8.12 A pod for trying out communication with the API server: curl.yaml\\nUsing the tutum/curl image, \\nbecause you need curl available in the container\\nYou’re running the sleep \\ncommand with a long delay to keep your container running.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 271}, page_content=\"240 CHAPTER  8Accessing pod metadata and other resources from applications\\nYou may also remember that each service also gets a DNS entry, so you don’t even\\nneed to look up the environment va riables, but instead simply point curl  to\\nhttps:/ /kubernetes . To be fair, if you don’t know whic h port the service is available at,\\nyou also either need to look up the en vironment variables or perform a DNS SRV\\nrecord lookup to get the se rvice’s actual port number. \\n The environment variables shown previously say that the API server is listening on\\nport 443, which is the default port for HTTPS, so try hitting the server throughHTTPS:\\nroot@curl:/# curl https://kubernetes\\ncurl: (60) SSL certificate problem: unable  to get local issuer certificate\\n...If you'd like to turn off curl's verification of the certificate, use\\n  the -k (or --insecure) option.\\nAlthough the simplest way to get around this is to use the proposed -k option (and\\nthis is what you’d normally use when playin g with the API server manually), let’s look\\nat the longer (and correct) route. Instead of blindly trusting that the server you’re\\nconnecting to is the authentic API server, you’ll verify its identity by having curl  check\\nits certificate. \\nTIP Never skip checking the server’s certificate in an actual application.\\nDoing so could make your app expose its authentication token to an attackerusing a man-in-the-middle attack.\\nVERIFYING  THE SERVER ’S IDENTITY\\nIn the previous chapter, while discussing Se crets, we looked at an automatically cre-\\nated Secret called default-token-xyz , which is mounted into each container at\\n/var/run/secrets/kubernetes.io/serviceaccoun t/. Let’s see the contents of that Secret\\nagain, by listing files in that directory:\\nroot@curl:/# ls /var/run/secrets/kubernetes.io/serviceaccount/                                                                                                                                                                \\nca.crt    namespace    token\\nThe Secret has three entries (and therefore three files in the Secret volume). Right\\nnow, we’ll focus on the ca.crt file, which ho lds the certificate of the certificate author-\\nity (CA) used to sign the Kubernetes API serv er’s certificate. To ve rify you’re talking to\\nthe API server, you need to check if the server’s certificate is signed by the CA. curl\\nallows you to specify the CA certificate with the --cacert  option, so try hitting the API\\nserver again:\\nroot@curl:/# curl --cacert /var/run/secrets/kubernetes.io/serviceaccount\\n             ➥ /ca.crt https://kubernetes\\nUnauthorized\\nNOTE You may see a longer error description than “Unauthorized.”\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 272}, page_content='241 Talking to the Kubernetes API server\\nOkay, you’ve made progress. curl  verified the server’s identity because its certificate\\nwas signed by the CA you trust. As the Unauthorized  response suggests, you still need\\nto take care of authentication. You’ll do that in a moment, but first let’s see how to\\nm a k e  l i f e  e a s i e r  b y  s e t t i n g  t h e  CURL_CA_BUNDLE  environment variable, so you don’t\\nneed to specify --cacert  every time you run curl :\\nroot@curl:/# export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/\\n             ➥ serviceaccount/ca.crt\\nYou can now hit the API server without using --cacert :\\nroot@curl:/# curl https://kubernetes\\nUnauthorized\\nThis is much nicer now. Your client ( curl ) trusts the API server now, but the API\\nserver itself says you’re not authorized to access it, because it doesn’t know who\\nyou are.\\nAUTHENTICATING  WITH THE API SERVER\\nYou need to authenticate with the server, so it allows you to read and even update\\nand/or delete the API objects deployed in the cluster. To authenticate, you need an\\nauthentication token. Luckily, the token is  provided through the default-token Secret\\nmentioned previously, and is stored in the token  file in the secret  volume. As the\\nSecret’s name suggests, that’s th e primary purpose of the Secret. \\n You’re going to use the token to access th e API server. First, load the token into an\\nenvironment variable:\\nroot@curl:/# TOKEN=$(cat /var/run/secrets/kubernetes.io/\\n             ➥ serviceaccount/token)\\nThe token is now stored in the TOKEN  environment variable. You can use it when send-\\ning requests to the API server, as shown in the following listing.\\nroot@curl:/# curl -H \"Authorization: Bearer $TOKEN\" https://kubernetes\\n{\\n  \"paths\": [\\n    \"/api\",    \"/api/v1\",\\n    \"/apis\",\\n    \"/apis/apps\",    \"/apis/apps/v1beta1\",\\n    \"/apis/authorization.k8s.io\",    \\n    ...    \"/ui/\",\\n    \"/version\"\\n  ]}Listing 8.13 Getting a proper response from the API server'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 273}, page_content='242 CHAPTER  8Accessing pod metadata and other resources from applications\\nAs you can see, you passed the token inside the Authorization  HTTP header in the\\nrequest. The API server recognized the token as authentic and returned a proper\\nresponse. You can now explore all the resources in your cluster, the way you did a fewsections ago. \\n For example, you could list all the pods in the same namespace. But first you need\\nto know what namespace the \\ncurl pod is running in.\\nGETTING  THE NAMESPACE  THE POD IS RUNNING  IN\\nIn the first part of this chapter, you saw how to pass the namespace to the pod\\nthrough the Downward API. But if you’re paying attention, you probably noticed\\nyour secret  volume also contains a file called  namespace. It contains the name-\\nspace the pod is running in, so you can read  the file instead of having to explicitly\\npass the namespace to your pod through an environment variable. Load the con-\\ntents of the file into the NS environment va riable and then list all the pods, as shown\\nin the following listing.\\nroot@curl:/# NS=$(cat /var/run/secrets/kubernetes.io/\\n             ➥ serviceaccount/namespace)           \\nroot@curl:/# curl -H \"Authorization: Bearer $TOKEN\"\\n             ➥ https://kubernetes/api/v1/namespaces/$NS/pods\\n{  \"kind\": \"PodList\",\\n  \"apiVersion\": \"v1\",\\n  ...\\nAnd there you go. By using the three files in the mounted secret  volume directory,\\nyou listed all the pods running in the same  namespace as your pod. In the same man-\\nner, you could also retrieve other API ob jects and even update them by sending PUT or\\nPATCH  instead of simple GET requests. Disabling role-based access control (RBAC)\\nIf you’re using a Kubernetes cluster with  RBAC enabled, the service account may not\\nbe authorized to access (parts of) the API server. You’ll learn about service accounts\\nand RBAC in chapter 12. For now, the simp lest way to allow you to query the API\\nserver is to work around RBAC by running the following command:\\n$ kubectl create clusterrolebinding permissive-binding \\\\\\n  --clusterrole=cluster-admin \\\\  --group=system:serviceaccounts\\nThis gives all service accounts (we could al so say all pods) cluster-admin privileges,\\nallowing them to do whatever they want. Obviously, doing this is dangerous and\\nshould never be done on production cluste rs. For test purposes, it’s fine.\\nListing 8.14 Listing pods in the pod’s own namespace'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 274}, page_content='243 Talking to the Kubernetes API server\\nRECAPPING  HOW PODS TALK TO KUBERNETES\\nLet’s recap how an app running inside a po d can access the Kubernetes API properly:\\n\\uf0a1The app should verify whether the API server ’s certificate is signed by the certif-\\nicate authority, whose certific ate is in the ca.crt file. \\n\\uf0a1The app should authenticate itself by sending the Authorization  header with\\nthe bearer token from the token  file. \\n\\uf0a1The namespace  file should be used to pass th e namespace to the API server when\\nperforming CRUD operations on API objects inside the pod’s namespace.\\nDEFINITION CRUD stands for Create, Read, Update, and Delete. The corre-\\nsponding HTTP methods are POST , GET, PATCH /PUT,  and DELETE , respectively.\\nAll three aspects of pod to API server co mmunication are displayed in figure 8.5.\\n8.2.3 Simplifying API server communication with ambassador \\ncontainers\\nDealing with HTTPS, certificates, and au thentication tokens sometimes seems too\\ncomplicated to developers. I’ve seen develo pers disable validation of server certifi-\\ncates on way too many occasions (and I’ll ad mit to doing it myself a few times). Luck-\\nily, you can make the communication mu ch simpler while keeping it secure. API server\\nGET /api/v1/namespaces/<namespace>/pods\\nAuthorization: Bearer <token>\\nPodContainer\\nFilesystem App\\n/\\nvar/\\nrun/\\nsecrets/\\nkubernetes.io/\\nserviceaccount/\\nDefault token secret volume\\nca.crt token namespaceServer\\ncertiﬁcateValidate\\ncertiﬁcate\\nFigure 8.5 Using the files from the default-token Secret to talk to the API server'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 275}, page_content='244 CHAPTER  8Accessing pod metadata and other resources from applications\\n Remember the kubectl  proxy  command we mentioned in  section 8.2.1? You ran\\nthe command on your local machine to make it easier to access the API server. Instead\\nof sending requests to the API server dire ctly, you sent them to the proxy and let it\\ntake care of authentication, encryption, and server verification. The same method canbe used inside your pods, as well.\\nINTRODUCING  THE AMBASSADOR  CONTAINER  PATTERN\\nImagine having an application that (amo ng other things) needs to query the API\\nserver. Instead of it talking to the API server directly, as you did in the previous sec-\\ntion, you can run kubectl  proxy  in an ambassador container alongside the main con-\\ntainer and communicate with the API server through it. \\n Instead of talking to the API server dire ctly, the app in the main container can con-\\nnect to the ambassador through HTTP (ins tead of HTTPS) and let the ambassador\\nproxy handle the HTTPS connection to the AP I server, taking care of security trans-\\nparently (see figure 8.6). It does this by using the files from the default token’s secret\\nvolume.\\nBecause all containers in a pod share the sa me loopback network interface, your app\\ncan access the proxy through a port on localhost.\\nRUNNING  THE CURL POD WITH AN ADDITIONAL  AMBASSADOR  CONTAINER\\nTo see the ambassador container pattern in action, you’ll create a new pod like the\\ncurl  pod you created earlier, but this time, instead of running a single container in\\nthe pod, you’ll run an additional ambassa dor container based on a general-purpose\\nkubectl-proxy  container image I’ve created and pushed to Docker Hub. You’ll find\\nthe Dockerfile for the image in the code archive (in /Chapter08/kubectl-proxy/) if\\nyou want to build it yourself.\\n The pod’s manifest is shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: curl-with-ambassadorspec:\\n  containers:\\n  - name: mainListing 8.15 A pod with an ambassador  container: curl-with-ambassador.yamlContainer:\\nmainContainer:\\nambassadorHTTP HTTPS\\nAPI server\\nPod\\nFigure 8.6 Using an ambassador to connect to the API server'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 276}, page_content='245 Talking to the Kubernetes API server\\n    image: tutum/curl\\n    command: [\"sleep\", \"9999999\"]\\n  - name: ambassador                             image: luksa/kubectl-proxy:1.6.2         \\nThe pod spec is almost the same as before, but with a different pod name and an addi-\\ntional container. Run the pod and then enter the main container with\\n$ kubectl exec -it curl-with-ambassador -c main bash\\nroot@curl-with-ambassador:/#\\nYour pod now has two containers, and you want to run bash  in the main  container,\\nhence the -c main  option. You don’t need to specify the container explicitly if you\\nwant to run the command in the pod’s first container. But if you want to run a com-\\nmand inside any other container, you do ne ed to specify the container’s name using\\nthe -c option.\\nTALKING  TO THE API SERVER  THROUGH  THE AMBASSADOR\\nNext you’ll try connecting to the API server through the ambassador container. Bydefault, \\nkubectl  proxy  binds to port 8001, and becaus e both containers in the pod\\nshare the same network interfaces, including loopback, you can point curl  to local-\\nhost:8001 , as shown in the following listing.\\nroot@curl-with-ambassador:/# curl localhost:8001\\n{\\n  \"paths\": [    \"/api\",\\n    ...\\n  ]\\n}\\nSuccess! The output printed by curl  is the same response you saw earlier, but this time\\nyou didn’t need to deal with authenti cation tokens and server certificates. \\n To get a clear picture of what exactly happened, refer to figure 8.7. curl  sent the\\nplain HTTP request (without any authentica tion headers) to the proxy running inside\\nthe ambassador container, and then the pr oxy sent an HTTPS request to the API\\nserver, handling the client authentication  by sending the token and checking the\\nserver’s identity by validating its certificate.\\n This is a great example of how an ambassador container can be used to hide the\\ncomplexities of connecting to an extern al service and simplify the app running in\\nthe main container. The ambassador containe r is reusable across many different apps,\\nregardless of what language the main app is written in. The downside is that an addi-\\ntional process is running and consuming additional resources.Listing 8.16 Accessing the API server through the ambassador containerThe ambassador container, \\nrunning the kubectl-proxy image'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 277}, page_content='246 CHAPTER  8Accessing pod metadata and other resources from applications\\n8.2.4 Using client libraries to talk to the API server\\nIf your app only needs to perform a few si mple operations on the API server, you can\\noften use a regular HTTP client library an d perform simple HTTP requests, especially\\nif you take advantage of the kubectl-proxy  ambassador container the way you did in\\nthe previous example. But if you plan on doing more than simple API requests, it’s\\nbetter to use one of the existing Kubernetes API client libraries.\\nUSING EXISTING  CLIENT  LIBRARIES\\nCurrently, two Kubernetes API client librar ies exist that are supported by the API\\nMachinery special interest group (SIG):\\n\\uf0a1Golang  client —https:/ /github.com/kubernetes/client-go\\n\\uf0a1Python —https:/ /github.com/kubernetes-incubator/client-python\\nNOTE The Kubernetes community has a nu mber of Special Interest Groups\\n(SIGs) and Working Groups that focus on specific parts of the Kubernetes\\necosystem. You’ll find a list of them at https:/ /github.com/kubernetes/com-\\nmunity/blob/master/sig-list.md .\\nIn addition to the two official ly supported libraries, here’s a list of user-contributed cli-\\nent libraries for many other languages:\\n\\uf0a1Java client by Fabric8 —https:/ /github.com/fabric8io/kubernetes-client\\n\\uf0a1Java client by Amdatu —https:/ /bitbucket.org/amdatulabs/amdatu-kubernetes\\n\\uf0a1Node.js client by tenxcloud —https:/ /github.com/tenxcloud/node-kubernetes-client\\n\\uf0a1Node.js client by GoDaddy —https:/ /github.com/godaddy/kubernetes-client\\n\\uf0a1PHP—https:/ /github.com/devstub/kubernetes-api-php-client\\n\\uf0a1Another PHP client —https:/ /github.com/maclof/kubernetes-clientContainer: main\\nAPI serversleep curl\\nContainer: ambassador\\nkubectl proxyPort 8001GET http://localhost:8001\\nGET https://kubernetes:443\\nAuthorization: Bearer <token>\\nPod\\nFigure 8.7 Offloading encryption, authentication, and server verification to kubectl  proxy  in an \\nambassador container'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 278}, page_content='247 Talking to the Kubernetes API server\\n\\uf0a1Ruby —https:/ /github.com/Ch00k/kubr\\n\\uf0a1Another Ruby client —https:/ /github.com/abonas/kubeclient\\n\\uf0a1Clojure —https:/ /github.com/yanatan16/clj-kubernetes-api\\n\\uf0a1Scala —https:/ /github.com/doriordan/skuber\\n\\uf0a1Perl—https:/ /metacpan.org/pod/Net::Kubernetes\\nThese libraries usually support HTTPS and ta ke care of authentication, so you won’t\\nneed to use the ambassador container. \\nAN EXAMPLE  OF INTERACTING  WITH KUBERNETES  WITH THE FABRIC 8 JAVA CLIENT\\nTo give you a sense of how client libraries en able you to talk to the API server, the fol-\\nlowing listing shows an example of how to lis t services in a Java app using the Fabric8\\nKubernetes client.\\nimport java.util.Arrays;\\nimport io.fabric8.kubernetes.api.model.Pod;\\nimport io.fabric8.kubernetes.api.model.PodList;\\nimport io.fabric8.kubernetes.client.DefaultKubernetesClient;import io.fabric8.kubernetes.client.KubernetesClient;\\npublic class Test {\\n  public static void main(String[] args) throws Exception {    KubernetesClient client = new DefaultKubernetesClient();\\n    // list pods in the default namespace\\n    PodList pods = client.pods().inNamespace(\"default\").list();    pods.getItems().stream()\\n      .forEach(s -> System.out.println(\"Found pod: \" +\\n               s.getMetadata().getName()));\\n    // create a pod\\n    System.out.println(\"Creating a pod\");    Pod pod = client.pods().inNamespace(\"default\")\\n      .createNew()\\n      .withNewMetadata()        .withName(\"programmatically-created-pod\")\\n      .endMetadata()\\n      .withNewSpec()        .addNewContainer()\\n          .withName(\"main\")\\n          .withImage(\"busybox\")          .withCommand(Arrays.asList(\"sleep\", \"99999\"))\\n        .endContainer()\\n      .endSpec()      .done();\\n    System.out.println(\"Created pod: \" + pod);\\n    // edit the pod (add a label to it)\\n    client.pods().inNamespace(\"default\")\\n      .withName(\"programmatically-created-pod\")\\n      .edit()      .editMetadata()Listing 8.17 Listing, creating, updating, and deleting pods with the Fabric8 Java client'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 279}, page_content='248 CHAPTER  8Accessing pod metadata and other resources from applications\\n        .addToLabels(\"foo\", \"bar\")\\n      .endMetadata()\\n      .done();    System.out.println(\"Added label foo=bar to pod\");\\n    System.out.println(\"Waiting 1 minute before deleting pod...\");\\n    Thread.sleep(60000);\\n    // delete the pod\\n    client.pods().inNamespace(\"default\")      .withName(\"programmatically-created-pod\")\\n      .delete();\\n    System.out.println(\"Deleted the pod\");  }\\n}\\nThe code should be self-explanatory, espe cially because the Fabric8 client exposes\\na nice, fluent Domain-Specific-Language (DSL) API, which is easy to read and\\nunderstand.\\nBUILDING  YOUR OWN LIBRARY  WITH SWAGGER  AND OPENAPI\\nIf no client is available for your progra mming language of choice, you can use the\\nSwagger API framework to generate the clie nt library and documentation. The Kuber-\\nnetes API server exposes Swagger API defini tions at /swaggerapi and OpenAPI spec at\\n/swagger.json. \\n To find out more about the Swagger framework, visit the website at http:/ /swagger.io .\\nEXPLORING  THE API WITH SWAGGER  UI\\nEarlier in the chapter I said I’d point you to a better way of exploring the REST API\\ninstead of hitting the REST endpoints with curl . Swagger, which I mentioned in the\\nprevious section, is not just a tool for specif ying an API, but also provides a web UI for\\nexploring REST APIs if they expose the Swagger API definitions. The better way of\\nexploring REST APIs is through this UI.\\n Kubernetes not only exposes the Swagger API, but it also has Swagger UI inte-\\ngrated into the API server, though it’s no t enabled by default. You can enable it by\\nrunning the API server with the --enable-swagger-ui=true  option.\\nTIP If you’re using Minikube, you can en able Swagger UI when starting the\\ncluster: minikube  start  --extra-config=apiserver .Features.Enable-\\nSwaggerUI=true\\nAfter you enable the UI, you can open it  in your browser by pointing it to:\\nhttp(s)://<api server>:<port>/swagger-ui\\nI urge you to give Swagger UI a try. It not only allows you to browse the Kubernetes\\nAPI, but also interact with it (you can POST  JSON resource manifests, PATCH  resources,\\nor DELETE  them, for example).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 280}, page_content='249 Summary\\n8.3 Summary\\nAfter reading this chapter, you now know ho w your app, running inside a pod, can get\\ndata about itself, other pods, and other components deployed in the cluster. You’velearned\\n\\uf0a1How a pod’s name, namespace, and other metadata can be exposed to the pro-\\ncess either through environmen t variables or files in a downwardAPI  volume\\n\\uf0a1How CPU and memory requests and limits  are passed to your app in any unit\\nthe app requires\\n\\uf0a1How a pod  can use downwardAPI  volumes to get up-to-date metadata, which\\nmay change during the lifetime of the pod  (such as labels and annotations) \\n\\uf0a1How you can browse the Kubernetes REST API through kubectl  proxy\\n\\uf0a1How pods can find the API server’s location through environment variables orDNS, similar to any other Service defined in Kubernetes\\n\\uf0a1How an application running in a pod  can verify that it’s talking to the API\\nserver and how it can authenticate itself\\n\\uf0a1How using an ambassador  container can make talking to the API server from\\nwithin an app much simpler\\n\\uf0a1How client libraries can get you inte racting with Kubernetes in minutes\\nIn this chapter, you learned how to talk to the API server, so the next step is learning\\nmore about how it works. You’ll do that in chapter 11, but before we dive into such\\ndetails, you still need to learn about tw o other Kubernetes resources—Deployments\\nand StatefulSets. They’re explained in the next two chapters.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 281}, page_content='250Deployments: updating\\napplications declaratively\\nYou now know how to package your app components into containers, group them\\ninto pods, provide them with temporary or permanent storage, pass both secret\\nand non-secret config data to them, and a llow pods to find and talk to each other.\\nYou know how to run a full-fledged sy stem composed of independently running\\nsmaller components—microservices, if  you will. Is there anything else? \\n Eventually, you’re going to want to upda te your app. This chapter covers how to\\nupdate apps running in a Kubernetes cl uster and how Kubernetes helps you move\\ntoward a true zero-downtime update proce ss. Although this can be achieved using\\nonly ReplicationControllers or ReplicaSets, Kubernetes also provides a DeploymentThis chapter covers\\n\\uf0a1Replacing pods with newer versions\\n\\uf0a1Updating managed pods\\n\\uf0a1Updating pods declaratively using Deployment \\nresources\\n\\uf0a1Performing rolling updates\\n\\uf0a1Automatically blocking rollouts of bad versions\\n\\uf0a1Controlling the rate of the rollout\\n\\uf0a1Reverting pods to a previous version'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 282}, page_content='251 Updating applications running in pods\\nresource that sits on top of ReplicaSets and enables declarative application updates. If\\nyou’re not completely sure what that mean s, keep reading—it’s not as complicated as\\nit sounds.\\n9.1 Updating applications running in pods\\nLet’s start off with a simple example. Imagin e having a set of pod instances providing a\\nservice to other pods and/or external client s. After reading this book up to this point,\\nyou likely recognize that these pods are backed by a ReplicationController or a\\nReplicaSet. A Service also exists through wh ich clients (apps running in other pods or\\nexternal clients) access the pods. This is how a basic applicatio n looks in Kubernetes\\n(shown in figure 9.1).\\nInitially, the pods run the first version of your application—let’s suppose its image is\\ntagged as v1. You then develop a newer version of the app and push it to an image\\nrepository as a new image, tagged as v2. You’d next like to replace all the pods with\\nthis new version. Because you can’t change an existing pod’s image after the pod is\\ncreated, you need to remove the old pods  and replace them with new ones running\\nthe new image. \\n You have two ways of updating all thos e pods. You can do one of the following:\\n\\uf0a1Delete all existing pods first and then start the new ones.\\n\\uf0a1Start new ones and, once they’re up, dele te the old ones. You can do this either\\nby adding all the new pods and then deleting all the old ones at once, or\\nsequentially, by adding new pods and removing old ones gradually.\\nBoth these strategies have their benefits an d drawbacks. The first option would lead to\\na short period of time when your applic ation is unavailable. The second option\\nrequires your app to handle running two versions of the app at the same time. If your\\napp stores data in a data store, the new version shouldn’t modify the data schema or\\nthe data in such a way that breaks the previous version.ReplicationController\\nor ReplicaSetClients Service\\nPod Pod Pod\\nFigure 9.1 The basic outline of an \\napplication running in Kubernetes'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 283}, page_content='252 CHAPTER  9Deployments: updating applications declaratively\\n How do you perform these two update methods in Kubernetes? First, let’s look at\\nhow to do this manually; then, once you kn ow what’s involved in the process, you’ll\\nlearn how to have Kubernetes perform the update automatically.\\n9.1.1 Deleting old pods and replacing them with new ones\\nYou already know how to get a ReplicationController to replace all its pod instances\\nwith pods running a new version. You probably remember the pod template of a\\nReplicationController can be updated at any time. When the ReplicationController\\ncreates new instances, it uses the up dated pod template to create them.\\n If you have a ReplicationController managing a set of v1 pods, you can easily\\nreplace them by modifying the pod te mplate so it refers to version v2 of the image and\\nthen deleting the old pod instances. The ReplicationController will notice that no\\npods match its label selector and it will spin up new instances. The whole process is\\nshown in figure 9.2.\\nThis is the simplest way to update a set of  pods, if you can accept the short downtime\\nbetween the time the old pods are deleted and new ones are started.\\n9.1.2 Spinning up new pods and then deleting the old ones\\nIf you don’t want to see any downtime an d your app supports running multiple ver-\\nsions at once, you can turn the process arou nd and first spin up all the new pods andPod template\\nchangedv pods deleted1\\nmanuallyReplicationControllerService\\nPod: v1 Pod: v1\\nPod\\ntemplate: v2ReplicationController\\nPod\\ntemplate: v2Pod: v1Service\\nPod: v2 Pod: v2 Pod: v2\\nReplicationControllerService\\nPod: v1 Pod: v1\\nPod\\ntemplate: v1Pod: v1\\nReplicationControllerService\\nPod: v1 Pod: v1 Pod: v1\\nPod\\ntemplate: v2\\nShort period of\\ndowntime herev2 pods created by\\nReplicationController\\nFigure 9.2 Updating pods by changing a ReplicationController’s pod template and deleting old Pods'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 284}, page_content='253 Updating applications running in pods\\nonly then delete the old ones. This will require more hardware resources, because\\nyou’ll have double the number of pods runni ng at the same time for a short while. \\n This is a slightly more complex method  compared to the pr evious one, but you\\nshould be able to do it by combining wh at you’ve learned about ReplicationControl-\\nlers and Services so far.\\nSWITCHING  FROM THE OLD TO THE NEW VERSION  AT ONCE\\nPods are usually fronted by a Service. It’s possible to have the Service front only the\\ninitial version of the pods while you bring up the pods running the new version. Then,\\nonce all the new pods are up, you can change  the Service’s label selector and have the\\nService switch over to the new pods, as shown in figure 9.3. This is called a blue-green\\ndeployment . After switching over, and once you’re  sure the new version functions cor-\\nrectly, you’re free to delete the old pods  by deleting the old ReplicationController.\\nNOTE You can change a Service’s pod selector with the kubectl  set selec-\\ntor command.\\nPERFORMING  A ROLLING  UPDATE\\nInstead of bringing up all the new pods an d deleting the old pods at once, you can\\nalso perform a rolling update, which replaces pods step by step. You do this by slowly\\nscaling down the previous ReplicationController and scaling up the new one. In thiscase, you’ll want the Service’s pod selector  to include both the old and the new pods,\\nso it directs requests toward bo th sets of pods. See figure 9.4.\\n Doing a rolling update manually is labo rious and error-prone. Depending on the\\nnumber of replicas, you’d need to run a dozen or more commands in the proper\\norder to perform the update process. Luckily , Kubernetes allows you to perform the\\nrolling update with a single command. You’ll learn how in the next section.Service Service\\nReplicationController:\\nv1Pod: v1 Pod: v1\\nPod\\ntemplate: v1Pod: v1\\nReplicationController:\\nv2\\nPod\\ntemplate: v2Pod: v2 Pod: v2 Pod: v2\\nReplicationController:\\nv1Pod: v1 Pod: v1\\nPod\\ntemplate: v1Pod: v1\\nReplicationController:\\nv2\\nPod\\ntemplate: v2Pod: v2 Pod: v2 Pod: v2\\nFigure 9.3 Switching a Service from the old pods to the new ones'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 285}, page_content='254 CHAPTER  9Deployments: updating applications declaratively\\n9.2 Performing an automatic rolling update with a \\nReplicationController\\nInstead of performing rolling updates usin g ReplicationControllers manually, you can\\nhave kubectl  perform them. Using kubectl  to perform the update makes the process\\nmuch easier, but, as you’ll see later, this is now an outdated way of updating apps. Nev-\\nertheless, we’ll walk through this option firs t, because it was historically the first way of\\ndoing an automatic rolling update, and also allows us to discuss the process without\\nintroducing too many additional concepts. \\n9.2.1 Running the initial version of the app\\nObviously, before you can update an app, you need to have an app deployed. You’re\\ngoing to use a slightly modified version of  the kubia NodeJS app you created in chap-\\nter 2 as your initial version. In case you don’t remember what it does, it’s a simple web-\\napp that returns the pod’s host name in the HTTP response. \\nCREATING  THE V1 APP\\nYou’ll change the app so it also returns it s version number in the response, which will\\nallow you to distinguish between the differ ent versions you’re about to build. I’ve\\nalready built and pushed the ap p image to Docker Hub under luksa/kubia:v1 . The\\nfollowing listing shows the app’s code.\\nconst http = require(\\'http\\');\\nconst os = require(\\'os\\');\\nconsole.log(\"Kubia server starting...\");Listing 9.1 The v1 version of our app: v1/app.jsService\\nPod: v1 Pod: v1\\nReplication\\nController:\\nv1\\nv1Replication\\nController:\\nv2Pod: v2Service\\nPod: v2 Pod: v2 Pod: v2Service\\nPod: v1 Pod: v1 Pod: v1Service\\nPod: v1 Pod: v2 Pod: v2\\nv2Replication\\nController:\\nv1\\nv1Replication\\nController:\\nv2\\nv2Replication\\nController:\\nv1Replication\\nController:\\nv2\\nv2Replication\\nController:\\nv1\\nv1 v1Replication\\nController:\\nv2\\nv2\\nFigure 9.4 A rolling update of pods using two ReplicationControllers'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 286}, page_content='255 Performing an automatic rolling update with a ReplicationController\\nvar handler = function(request, response) {\\n  console.log(\"Received request from \" + request.connection.remoteAddress);\\n  response.writeHead(200);  response.end(\"This is v1 running in pod \" + os.hostname() + \"\\\\n\");\\n};\\nvar www = http.createServer(handler);\\nwww.listen(8080);\\nRUNNING  THE APP AND EXPOSING  IT THROUGH  A SERVICE  USING  A SINGLE  YAML FILE\\nTo run your app, you’ll create a ReplicationController and a LoadBalancer  Service to\\nenable you to access the app externally. Th is time, rather than create these two\\nresources separately, you’ll create a single YA ML for both of them and post it to the\\nKubernetes API with a single kubectl  create  command. A YAML manifest can con-\\ntain multiple objects delimited with a line containing three dashes, as shown in the\\nfollowing listing.\\napiVersion: v1\\nkind: ReplicationController\\nmetadata:  name: kubia-v1\\nspec:\\n  replicas: 3  template:\\n    metadata:\\n      name: kubia      labels:                      \\n        app: kubia                 \\n    spec:      containers:\\n      - image: luksa/kubia:v1     \\n        name: nodejs\\n---                         \\napiVersion: v1kind: Service\\nmetadata:\\n  name: kubiaspec:\\n  type: LoadBalancer\\n  selector:                                            app: kubia                                     \\n  ports:\\n  - port: 80    targetPort: 8080\\nThe YAML defines a ReplicationController called kubia-v1  and a Service called\\nkubia . Go ahead and post the YAML to Ku bernetes. After a while, your three v1 pods\\nand the load balancer should all be running , so you can look up the Service’s external\\nIP and start hitting the service with curl , as shown in the following listing.Listing 9.2 A YAML containing an RC and a Service: kubia-rc-and-service-v1.yaml\\nThe Service fronts all \\npods created by the \\nReplicationController.You’re creating a \\nReplicationController for pods running this image.\\nYAML files can contain \\nmultiple resource \\ndefinitions separated by \\na line with three dashes.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 287}, page_content='256 CHAPTER  9Deployments: updating applications declaratively\\n$ kubectl get svc kubia\\nNAME      CLUSTER-IP     EXTERNAL-IP       PORT(S)         AGE\\nkubia     10.3.246.195   130.211.109.222    80:32143/TCP    5m\\n$ while true; do curl http://130.211.109.222; doneThis is v1 running in pod kubia-v1-qr192\\nThis is v1 running in pod kubia-v1-kbtsk\\nThis is v1 running in pod kubia-v1-qr192\\nThis is v1 running in pod kubia-v1-2321o\\n...\\nNOTE If you’re using Minikube or any other Kubernetes cluster where load\\nbalancer services aren’t supported, yo u can use the Service’s node port to\\naccess the app. This was explained in chapter 5.\\n9.2.2 Performing a rolling update with kubectl\\nNext you’ll create version 2 of the app. To keep things simple, all you’ll do is change\\nthe response to say, “This is v2”:\\n  response.end(\"This is v2 running in pod \" + os.hostname() + \"\\\\n\");\\nThis new version is available in the image luksa/kubia:v2  on Docker Hub, so you\\ndon’t need to build it yourself.Listing 9.3 Getting the Service’s external IP and hitting the service in a loop with curl\\nPushing updates to the same image tag\\nModifying an app and pushing the changes to  the same image tag isn’t a good idea,\\nbut we all tend to do that during development. If you’re modifying the latest  tag,\\nthat’s not a problem, but when you’re tagging  an image with a different tag (for exam-\\nple, tag v1 instead of latest ), once the image is pulled by  a worker node, the image\\nwill be stored on the node and not pull ed again when a new pod using the same\\nimage is run (at least that’s t he default policy for pulling images).\\nThat means any changes you make to t he image won’t be picked up if you push them\\nto the same tag. If a new pod is scheduled to  the same node, the Kubelet will run the\\nold version of the image. On the other ha nd, nodes that haven’t run the old version\\nwill pull and run the new im age, so you might end up with  two different versions of\\nthe pod running. To make sure this does n’t happen, you need to set the container’s\\nimagePullPolicy  property to Always . \\nYou need to be aware that the default imagePullPolicy  depends on the image tag.\\nIf a container refers to the latest  tag (either explicitly or by not specifying the tag at\\nall), imagePullPolicy  defaults to Always , but if the container refers to any other\\ntag, the policy defaults to IfNotPresent . \\nWhen using a tag other than latest , you need to set the imagePullPolicy  properly\\nif you make changes to an image without changing the tag. Or better yet, make sure\\nyou always push changes to an image under a new tag.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 288}, page_content=\"257 Performing an automatic rolling update with a ReplicationController\\nKeep the curl  loop running and open another terminal, where you’ll get the rolling\\nupdate started. To perform the update, you’ll run the kubectl  rolling-update  com-\\nmand. All you need to do is tell it which Re plicationController you’re replacing, give a\\nname for the new ReplicationController, and specify the new image you’d like toreplace the original one with. The followin g listing shows the full command for per-\\nforming the rolling update.\\n$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2\\nCreated kubia-v2\\nScaling up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0 (keep 3 \\npods available, don't exceed 4 pods)\\n...\\nBecause you’re replacing ReplicationController kubia-v1  with one running version 2\\nof your kubia app, you’d like the ne w ReplicationController to be called kubia-v2\\nand use the luksa/kubia:v2  container image. \\n When you run the command, a new ReplicationController called kubia-v2  is cre-\\nated immediately. The state of the system  at this point is shown in figure 9.5.\\nThe new ReplicationController’s pod template references the luksa/kubia:v2  image\\nand its initial desired replica count is set to 0 , as you can see in the following listing.\\n$ kubectl describe rc kubia-v2\\nName:       kubia-v2\\nNamespace:  default\\nImage(s):   luksa/kubia:v2          Selector:   app=kubia,deployment=757d16a0f02f6a5c387f2b5edb62b155\\nLabels:     app=kubia            \\nReplicas:   0 current / 0 desired    ...Listing 9.4 Initiating a rolling-update of a ReplicationController using kubectl\\nListing 9.5 Describing the new Replicati onController created by the rolling updatePod: v1 Pod: v1 No v2 pods yet Pod: v1\\nReplicationController: kubia-v1\\nImage:kubia/v1\\nReplicas: 3ReplicationController: kubia-v2\\nImage:kubia/v2\\nReplicas: 0\\nFigure 9.5 The state of the system immedi ately after starting the rolling update\\nThe new \\nReplicationController \\nrefers to the v2 image.\\nInitially, the desired \\nnumber of replicas is zero.\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 289}, page_content='258 CHAPTER  9Deployments: updating applications declaratively\\nUNDERSTANDING  THE STEPS  PERFORMED  BY KUBECTL  BEFORE  THE ROLLING  UPDATE  COMMENCES\\nkubectl  created this ReplicationController by copying the kubia-v1  controller and\\nchanging the image in its pod template. If you look closely at the controller’s label\\nselector, you’ll notice it has been modifi ed, too. It includes not only a simple\\napp=kubia  label, but also an additional deployment  label which the pods must have in\\norder to be managed by this ReplicationController.\\n You probably know this already, but this is necessary to avoid having both the new\\nand the old ReplicationControllers operating on the same set of pods. But even if pods\\ncreated by the new controller have the additional deployment  label in addition to the\\napp=kubia  label, doesn’t this mean they’ll be selected by the first ReplicationControl-\\nler’s selector, because it’s set to app=kubia ? \\n Yes, that’s exactly what would happen, but there’s a catch. The rolling-update pro-\\ncess has modified the selector of the first ReplicationController, as well:\\n$ kubectl describe rc kubia-v1\\nName:       kubia-v1\\nNamespace:  default\\nImage(s):   luksa/kubia:v1Selector:   app=kubia, deployment=3ddd307978b502a5b975ed4045ae4964-orig \\nOkay, but doesn’t this mean the first contro ller now sees zero pods matching its selec-\\ntor, because the three pods previous ly created by it contain only the app=kubia  label?\\nNo, because kubectl  had also modified the labels of the live pods just before modify-\\ning the ReplicationController’s selector:\\n$ kubectl get po --show-labels\\nNAME            READY  STATUS   RESTARTS  AGE  LABELS\\nkubia-v1-m33mv  1/1    Running  0         2m   app=kubia, deployment=3ddd...\\nkubia-v1-nmzw9  1/1    Running  0         2m   app=kubia, deployment=3ddd...\\nkubia-v1-cdtey  1/1    Running  0         2m   app=kubia, deployment=3ddd...\\nIf this is getting too complicated, examin e figure 9.6, which shows the pods, their\\nlabels, and the two ReplicationControl lers, along with their pod selectors.\\nReplicationController: kubia-v1\\nReplicas: 3\\nSelector:app=kubia,\\ndeployment=3ddd…ReplicationController: kubia-v2\\nReplicas: 0\\nSelector:app=kubia,\\ndeployment=757d...deployment: 3ddd...app: kubia\\nPod: v1deployment: 3ddd...app: kubia\\nPod: v1deployment: 3ddd...app: kubia\\nPod: v1\\nFigure 9.6 Detailed state of the old and new Replicat ionControllers and pods at the start of a rolling \\nupdate'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 290}, page_content='259 Performing an automatic rolling update with a ReplicationController\\nkubectl  had to do all this before even starting to scale anything up or down. Now\\nimagine doing the rolling update manually. It ’s easy to see yourself making a mistake\\nhere and possibly having the ReplicationC ontroller kill off all your pods—pods that\\nare actively serving your production clients!\\nREPLACING  OLD PODS WITH NEW ONES BY SCALING  THE TWO REPLICATION CONTROLLERS\\nAfter setting up all this, kubectl  starts replacing pods by  first scaling up the new\\ncontroller to 1. The controller thus creates the first v2 pod. kubectl  then scales\\ndown the old ReplicationController by 1. Th is is shown in the next two lines printed\\nby kubectl :\\nScaling kubia-v2 up to 1\\nScaling kubia-v1 down to 2\\nBecause the Service is targeting all pods with the app=kubia  label, you should start see-\\ning your curl  requests redirected to the new v2 pod every few loop iterations:\\nThis is v2 running in pod kubia-v2-nmzw9      \\nThis is v1 running in pod kubia-v1-kbtsk\\nThis is v1 running in pod kubia-v1-2321o\\nThis is v2 running in pod kubia-v2-nmzw9      ...\\nFigure 9.7 shows the current state of the system.\\nAs kubectl  continues with the rolling update, yo u start seeing a progressively bigger\\npercentage of requests hitting v2 pods, as the update process deletes more of the v1\\npods and replaces them with those running your new image. Eventually, the originalRequests hitting the pod \\nrunning the new version\\nReplicationController: kubia-v1\\nReplicas: 2\\nSelector:app=kubia,\\ndeployment=3ddd…ReplicationController: kubia-v2\\nReplicas: 1\\nSelector:app=kubia,\\ndeployment=757d…deployment: 3ddd...app: kubia\\nPod: v1deployment: 3ddd...app: kubia\\nPod: v1deployment: 757d...app: kubia\\nPod: v2curlService\\nSelector:app=kubia\\nFigure 9.7 The Service is redirecting reques ts to both the old and new pods during the \\nrolling update.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 291}, page_content='260 CHAPTER  9Deployments: updating applications declaratively\\nReplicationController is scaled to zero, causing the last v1 pod to be deleted, which\\nmeans the Service will now be backed by v2 pods only. At that point, kubectl  will\\ndelete the original ReplicationController and the update process will be finished, as\\nshown in the following listing.\\n...\\nScaling kubia-v2 up to 2\\nScaling kubia-v1 down to 1\\nScaling kubia-v2 up to 3Scaling kubia-v1 down to 0\\nUpdate succeeded. Deleting kubia-v1\\nreplicationcontroller \"kubia-v1\" rolling updated to \"kubia-v2\"\\nYou’re now left with only the kubia-v2  ReplicationController and three v2 pods. All\\nthroughout this update process, you’ve hit your service and gotten a response everytime. You have, in fact, performed a rolling update with zero downtime. \\n9.2.3 Understanding why kubectl rolling-update is now obsolete\\nAt the beginning of this section, I ment ioned an even better way of doing updates\\nthan through kubectl  rolling-update . What’s so wrong with this process that a bet-\\nter one had to be introduced? \\n Well, for starters, I, for one, don’t like Kubernetes modifying objects I’ve created.\\nOkay, it’s perfectly fine for the scheduler to  assign a node to my pods after I create\\nthem, but Kubernetes modifying the labels of my pods and the label selectors of my\\nReplicationController s is something that I don’t ex pect and could cause me to go\\naround the office yelling at my colleagues, “Who’s been messing with my controllers!?!?” \\n But even more importantly, if you’ve pa id close attention to the words I’ve used,\\nyou probably noticed that all this time I said explicitly that the kubectl  client was the\\none performing all these steps of the rolling update. \\n You can see this by turning on verbose logging with the --v option when triggering\\nthe rolling update:\\n$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 --v 6\\nTIP Using the --v 6 option increases the logging level enough to let you see\\nthe requests kubectl  is sending to the API server.\\nUsing this option, kubectl  will print out each HTTP re quest it sends to the Kuberne-\\ntes API server. You’ll see PUT requests to\\n/api/v1/namespaces/default/replicationcontrollers/kubia-v1\\nwhich is the RESTful URL representing your kubia-v1  ReplicationController resource.\\nThese requests are the ones scaling down your ReplicationController, which showsListing 9.6 The final steps performed by kubectl  rolling-update'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 292}, page_content='261 Using Deployments for updating apps declaratively\\nthat the kubectl  client is the one doing the scalin g, instead of it being performed by\\nthe Kubernetes master. \\nTIP Use the verbose logging option when running other kubectl  commands,\\nto learn more about the communication between kubectl  and the API server. \\nBut why is it such a bad thing that the upd ate process is being performed by the client\\ninstead of on the server? Well, in your case , the update went smoo thly, but what if you\\nlost network connectivity while kubectl  was performing the update? The update pro-\\ncess would be interrupted mid-way. Pods an d ReplicationControllers would end up in\\nan intermediate state.\\n Another reason why performing an update li ke this isn’t as good as it could be is\\nbecause it’s imperative. Throughout this bo ok, I’ve stressed how Kubernetes is about\\nyou telling it the desired state of the system and having Kubernetes achieve that\\nstate on its own, by figuring out the best way to do it. This is how pods are deployed\\nand how pods are scaled up and down. Yo u never tell Kubernetes to add an addi-\\ntional pod or remove an excess one—yo u change the number of desired replicas\\nand that’s it.\\n Similarly, you will also want to change the desired image tag in your pod defini-\\ntions and have Kubernetes replace the po ds with new ones running the new image.\\nThis is exactly what drove the introduction of a new resource called a Deployment,\\nwhich is now the preferred way of deploying applications in Kubernetes. \\n9.3 Using Deployments for updating apps declaratively\\nA Deployment is a higher-level resource  meant for deploying applications and\\nupdating them declaratively, instead of doing it through a ReplicationController or\\na ReplicaSet, which are both co nsidered lower-level concepts.\\n When you create a Deployment, a ReplicaSet resource is created underneath\\n(eventually more of them). As you may re member from chapter 4, ReplicaSets are a\\nnew generation of ReplicationControllers, an d should be used instead of them. Replica-\\nSets replicate and manage pods, as well. When using a Deployment, the actual pods\\nare created and managed by the Deployment’s ReplicaSets, not by the Deploymentdirectly (the relationship is shown in figure 9.8).\\nYou might wonder why you’d want to compli cate things by introducing another object\\non top of a ReplicationController or ReplicaS et, when they’re what suffices to keep a set\\nof pod instances running. As the rolling upda te example in section 9.2 demonstrates,\\nwhen updating the app, you need to introduc e an additional ReplicationController andPods ReplicaSet Deployment Figure 9.8 A Deployment is backed \\nby a ReplicaSet, which supervises the \\ndeployment’s pods.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 293}, page_content='262 CHAPTER  9Deployments: updating applications declaratively\\ncoordinate the two controllers to dance ar ound each other without stepping on each\\nother’s toes. You need something coordina ting this dance. A Deployment resource\\ntakes care of that (it’s not the Deployment resource itself, but the controller process\\nrunning in the Kubernetes control plane that does that; but we’ll get to that in chap-\\nter 11).\\n Using a Deployment instead of the lower- level constructs makes updating an app\\nmuch easier, because you’re defining the desired state through the single Deployment\\nresource and letting Kubernetes take care of the rest, as you’ll see in the next few pages.\\n9.3.1 Creating a Deployment\\nCreating a Deployment isn’t that different from creating a ReplicationController. A\\nDeployment is also composed of a label se lector, a desired replica count, and a pod\\ntemplate. In addition to that, it also co ntains a field, which specifies a deployment\\nstrategy that defines how an update sh ould be performed when the Deployment\\nresource is modified.  \\nCREATING  A DEPLOYMENT  MANIFEST\\nLet’s see how to use the kubia-v1  ReplicationController example from earlier in this\\nchapter and modify it so it describes a De ployment instead of a ReplicationController.\\nAs you’ll see, this requires only three triv ial changes. The following listing shows the\\nmodified YAML.\\napiVersion: apps/v1beta1          \\nkind: Deployment                  metadata:\\n  name: kubia          \\nspec:\\n  replicas: 3\\n  template:    metadata:\\n      name: kubia\\n      labels:        app: kubia\\n    spec:\\n      containers:      - image: luksa/kubia:v1\\n        name: nodejs\\nNOTE You’ll find an older version of  the Deployment resource in extensions/\\nv1beta1 , and a newer one in apps/v1beta2  with different required fields and\\ndifferent defaults. Be aware that kubectl  explain  shows the older version.\\nBecause the ReplicationController from before  was managing a specific version of the\\npods, you called it kubia-v1 . A Deployment, on the other hand, is above that version\\nstuff. At a given point in time, the Deploy ment can have multiple pod versions run-\\nning under its wing, so its name shouldn’t reference the app version.Listing 9.7 A Deployment definition: kubia-deployment-v1.yaml\\nDeployments are in the apps \\nAPI group, version v 1beta1.\\nYou’ve changed the kind \\nfrom ReplicationController \\nto Deployment.\\nThere’s no need to include \\nthe version in the name of the Deployment.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 294}, page_content='263 Using Deployments for updating apps declaratively\\nCREATING  THE DEPLOYMENT  RESOURCE\\nBefore you create this Deployment, make sure you delete any ReplicationControllers\\nand pods that are still running, but keep the kubia  Service for now. You can use the\\n--all  switch to delete all those Re plicationControllers like this:\\n$ kubectl delete rc --all\\nYou’re now ready to create the Deployment: \\n$ kubectl create -f kubia-deployment-v1.yaml --record\\ndeployment \"kubia\" created\\nTIP Be sure to include the --record  command-line option when creating it.\\nThis records the command in the revision  history, which will be useful later.\\nDISPLAYING  THE STATUS  OF THE DEPLOYMENT  ROLLOUT\\nYou can use the usual kubectl  get deployment  and the kubectl  describe  deployment\\ncommands to see details of the Deployment , but let me point you to an additional\\ncommand, which is made specifically for checking a Deployment’s status:\\n$ kubectl rollout status deployment kubia\\ndeployment kubia successfully rolled out\\nAccording to this, the Deployment has been successfully rolled out, so you should see\\nthe three pod replicas up and running. Let’s see:\\n$ kubectl get po\\nNAME                     READY     STATUS    RESTARTS   AGE\\nkubia-1506449474-otnnh   1/1       Running   0          14s\\nkubia-1506449474-vmn7s   1/1       Running   0          14skubia-1506449474-xis6m   1/1       Running   0          14s\\nUNDERSTANDING  HOW DEPLOYMENTS  CREATE  REPLICA SETS, WHICH  THEN CREATE  THE PODS\\nTake note of the names of these pods. Earl ier, when you used a ReplicationController\\nto create pods, their names were composed of the name of the controller plus a ran-\\ndomly generated string (for example, kubia-v1-m33mv ). The three pods created by\\nthe Deployment include an additional nume ric value in the middle of their names.\\nWhat is that exactly?\\n The number corresponds to the hashed va lue of the pod template in the Deploy-\\nment and the ReplicaSet managing these pods. As we said earlier, a Deploymentdoesn’t manage pods directly. Instead, it creates ReplicaSets and leaves the managing\\nto them, so let’s look at the ReplicaSet created by your Deployment:\\n$ kubectl get replicasets\\nNAME               DESIRED   CURRENT   AGEkubia-1506449474   3         3         10s\\nThe ReplicaSet’s name also contains the hash  value of its pod template. As you’ll see\\nlater, a Deployment creates multiple Rep licaSets—one for each version of the pod'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 295}, page_content='264 CHAPTER  9Deployments: updating applications declaratively\\ntemplate. Using the hash value of the pod template like this allows the Deployment\\nto always use the same (possibly existing) ReplicaSet for a given version of the pod\\ntemplate.\\nACCESSING  THE PODS THROUGH  THE SERVICE\\nWith the three replicas created by this ReplicaSet now running, you can use the Ser-vice you created a while ago to access th em, because you made the new pods’ labels\\nmatch the Service’s label selector. \\n Up until this point, you probably haven’ t seen a good-enough reason why you should\\nuse Deployments over ReplicationControllers. Luckily, creating a Deployment also hasn’t\\nbeen any harder than creating a Replicatio nController. Now, you’ll start doing things\\nwith this Deployment, which will make it cl ear why Deployments ar e superior. This will\\nbecome clear in the next few moments, when you see how updating the app through\\na Deployment resource compares to updat ing it through a ReplicationController.\\n9.3.2 Updating a Deployment\\nPreviously, when you ran your app using a Re plicationController, you had to explicitly\\ntell Kubernetes to perf orm the update by running kubectl  rolling-update . You even\\nhad to specify the name for the new Replicat ionController that should replace the old\\none. Kubernetes replaced all the original pods with new ones and deleted the original\\nReplicationController at the end of the proc ess. During the process, you basically had\\nto stay around, keeping your terminal open and waiting for kubectl  to finish the roll-\\ning update. \\n Now compare this to how you’re about to update a Deployment. The only thing\\nyou need to do is modify the pod template defined in the Deployment resource and\\nKubernetes will take all the steps necessary  to get the actual system state to what’s\\ndefined in the resource. Similar to scaling a ReplicationController or ReplicaSet up or\\ndown, all you need to do is reference a new image tag in the Deployment’s pod tem-\\nplate and leave it to Kubernetes to tran sform your system so it matches the new\\ndesired state.\\nUNDERSTANDING  THE AVAILABLE  DEPLOYMENT  STRATEGIES\\nHow this new state should be achieved is go verned by the deployment strategy config-\\nured on the Deployment itself. The default strategy is to perform a rolling update (the\\nstrategy is called RollingUpdate ). The alternative is the Recreate  strategy, which\\ndeletes all the old pods at once and then  creates new ones, similar to modifying a\\nReplicationController’s pod template and then  deleting all the pods (we talked about\\nthis in section 9.1.1).\\n The Recreate  strategy causes all old pods to be deleted before the new ones are\\ncreated. Use this strategy when your appl ication doesn’t support running multiple ver-\\nsions in parallel and requires the old version to be stopped completely before thenew one is started. This strategy does involve a short period of time when your app\\nbecomes completely unavailable.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 296}, page_content='265 Using Deployments for updating apps declaratively\\n The RollingUpdate  strategy, on the other hand, removes old pods one by one,\\nwhile adding new ones at the same time, keeping the application available throughout\\nthe whole process, and ensuring there’s no drop in its capacity to handle requests.\\nThis is the default strategy. The upper and lower limits for the number of pods aboveor below the desired replica count are config urable. You should use this strategy only\\nwhen your app can handle running both th e old and new version at the same time.\\nSLOWING  DOWN  THE ROLLING  UPDATE  FOR DEMO PURPOSES\\nIn the next exercise, you’ll use the RollingUpdate  strategy, but you need to slow down\\nthe update process a little, so you can see that the update is indeed performed in a\\nrolling fashion. You can do that by setting the minReadySeconds  attribute on the\\nDeployment. We’ll explain what this attribut e does by the end of this chapter. For\\nnow, set it to 10 seconds with the kubectl  patch  command.\\n$ kubectl patch deployment kubia -p \\'{\"spec\": {\"minReadySeconds\": 10}}\\'\\n\"kubia\" patched\\nTIP The kubectl  patch  command is useful for modifying a single property\\nor a limited number of properties of a re source without having to edit its defi-\\nnition in a text editor.\\nYou used the patch command to change th e spec of the Deployment. This doesn’t\\ncause any kind of update to the pods, because you didn’t change the pod template.Changing other Deployment properties, like the desired replica count or the deploy-\\nment strategy, also doesn’t trigger a rollout, because it doesn’t affect the existing indi-\\nvidual pods in any way.\\nTRIGGERING  THE ROLLING  UPDATE\\nIf you’d like to track the update process as it progresses, first run the curl  loop again\\nin another terminal to see what’s happening with the requests (don’t forget to replace\\nthe IP with the actual exte rnal IP of your service):\\n$ while true; do curl http://130.211.109.222; done\\nTo trigger the actual rollout, you’ll change the image used in the single pod container\\nto luksa/kubia:v2 . Instead of editing the whole YA ML of the Deployment object or\\nusing the patch  command to change the image, you’ll use the kubectl  set image\\ncommand, which allows changing the image of  any resource that contains a container\\n(ReplicationControllers, ReplicaSets, Deployme nts, and so on). You’ll use it to modify\\nyour Deployment like this:\\n$ kubectl set image deployment kubia nodejs=luksa/kubia:v2\\ndeployment \"kubia\" image updated\\nWhen you execute this command, you’re updating the kubia  Deployment’s pod tem-\\nplate so the image used in its nodejs  container is changed to luksa/kubia:v2  (from\\n:v1). This is shown in figure 9.9.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 297}, page_content='266 CHAPTER  9Deployments: updating applications declaratively\\nWays of modifying Deployments and other resources\\nOver the course of this book, you’ve learned several ways how to modify an existing\\nobject. Let’s list all of them together to refresh your memory.\\nAll these methods are equivalent as far as Deployments go. What they do is change\\nthe Deployment’s specification. This change then triggers the rollout process.Image registry Pod templateDeploymentkubectl set image…\\nluksa/kubia:v2\\nContainer:\\nnodejs:v1 :v2Image registry Pod templateDeployment\\nContainer:\\nnodejs:v1 :v2\\nFigure 9.9 Updating a Deployment’s pod template to point to a new image\\nTable 9.1 Modifying an existing resource in Kubernetes\\nMethod What it does\\nkubectl  edit Opens the object’s manifest in your default editor. After making \\nchanges, saving the file, and exiting the editor, the object is updated.\\nExample: kubectl  edit  deployment  kubia\\nkubectl  patch Modifies individual properties of an object.\\nExample: kubectl  patch  deployment  kubia  -p \\'{\"spec\":  \\n{\"template\":  {\"spec\":  {\"containers\":  [{\"name\":  \\n\"nodejs\",  \"image\":  \"luksa/kubia:v2\"}]}}}}\\'\\nkubectl  apply Modifies the object by applying property values from a full YAML or \\nJSON file. If the object specified in the YAML/JSON doesn’t exist yet, \\nit’s created. The file needs to contain the full definition of the \\nresource (it can’t include only the fie lds you want to update, as is the \\ncase with kubectl  patch ).\\nExample: kubectl  apply  -f kubia-deployment-v2.yaml\\nkubectl  replace Replaces the object with a new one from a YAML/JSON file. In con-\\ntrast to the apply  command, this command requires the object to \\nexist; otherwise it prints an error.\\nExample: kubectl  replace  -f kubia-deployment-v2.yaml\\nkubectl  set image Changes the container image defined in a Pod, ReplicationControl-\\nler’s template, Deployment, DaemonSet, Job, or ReplicaSet.Example: kubectl  set image  deployment  kubia  \\nnodejs=luksa/kubia:v2'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 298}, page_content='267 Using Deployments for updating apps declaratively\\nIf you’ve run the curl  loop, you’ll see requests initially hitting only the v1 pods; then\\nmore and more of them hit the v2 pod s until, finally, all of them hit only the remain-\\ning v2 pods, after all v1 pods are deleted. This works much like the rolling update per-\\nformed by kubectl .\\nUNDERSTANDING  THE AWESOMENESS  OF DEPLOYMENTS\\nLet’s think about what has happened. By changing the pod template in your Deploy-ment resource, you’ve updated your app to a newer version—by changing a single\\nfield! \\n The controllers running as part of the Kubernetes control plane then performed\\nthe update. The process wasn’t performed by the \\nkubectl  client, like it was when you\\nused kubectl  rolling-update . I don’t know about you, but I think that’s simpler than\\nhaving to run a special command telling Ku b e r n e t e s  w h a t  t o  d o  a n d  t h e n  w a i t i n g\\naround for the process to be completed.\\nNOTE Be aware that if the pod template in the Deployment references a\\nConfigMap (or a Secret), modifyin g the ConfigMap will not trigger an\\nupdate. One way to trigger an update wh en you need to modify an app’s con-\\nfig is to create a new ConfigMap and mo dify the pod template so it references\\nthe new ConfigMap.\\nThe events that occurred below the Deployme nt’s surface during the update are simi-\\nlar to what happened during the kubectl  rolling-update . An additional ReplicaSet\\nwas created and it was then scaled up slow ly, while the previous ReplicaSet was scaled\\ndown to zero (the initial and fina l states are shown in figure 9.10).\\nYou can still see the old ReplicaSet ne xt to the new one if you list them:\\n$ kubectl get rs\\nNAME               DESIRED   CURRENT   AGEkubia-1506449474   0         0         24m\\nkubia-1581357123   3         3         23mPods: v1ReplicaSet: v1\\nReplicas: --Before After\\nReplicaSet: v2\\nReplicas: ++Deployment\\nPods: v2ReplicaSet: v1\\nReplicaSet: v2Deployment\\nFigure 9.10 A Deployment at the start and end of a rolling update'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 299}, page_content='268 CHAPTER  9Deployments: updating applications declaratively\\nSimilar to ReplicationControllers, all yo ur new pods are now managed by the new\\nReplicaSet. Unlike before, the old ReplicaSet is still there, whereas the old Replication-\\nController was deleted at the end of the ro lling-update process. You’ll soon see what\\nthe purpose of this inactive ReplicaSet is. \\n But you shouldn’t care about ReplicaS ets here, because you didn’t create them\\ndirectly. You created and operated only on the Deployment resource; the underlying\\nReplicaSets are an implementation detail. Yo u’ll agree that managing a single Deploy-\\nment object is much easier compared to dealing with and keeping track of multiple\\nReplicationControllers. \\n Although this difference may not be so a pparent when everything goes well with a\\nrollout, it becomes much more obvious wh en you hit a problem during the rollout\\nprocess. Let’s simulate one problem right now.\\n9.3.3 Rolling back a deployment\\nYou’re currently running version v2 of your image, so you’ll  need to prepare version 3\\nfirst. \\nCREATING  VERSION  3 OF YOUR APP\\nIn version 3, you’ll introduce a bug that makes your app handle only the first four\\nrequests properly. All requests from the fi fth request onward will return an internal\\nserver error (HTTP status code 500). You’ll simulate this by adding an if statement at\\nthe beginning of the handler function. The following listing shows the new code, with\\nall required changes shown in bold.\\nconst http = require(\\'http\\');\\nconst os = require(\\'os\\');\\nvar requestCount = 0;\\nconsole.log(\"Kubia server starting...\");var handler = function(request, response) {\\n  console.log(\"Received request from \" + request.connection.remoteAddress);\\n  if (++requestCount >= 5) {\\n    response.writeHead(500);\\n    response.end(\"Some internal error has occurred! This is pod \" + \\nos.hostname() + \"\\\\n\");\\n    return;\\n  }\\n  response.writeHead(200);  response.end(\"This is v3 running in pod \" + os.hostname() + \"\\\\n\");\\n};\\nvar www = http.createServer(handler);\\nwww.listen(8080); \\nAs you can see, on the fifth and all subseq uent requests, the code returns a 500 error\\nwith the message “Some internal error has occurred...”Listing 9.8 Version 3 of our a pp (a broken version): v3/app.js'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 300}, page_content='269 Using Deployments for updating apps declaratively\\nDEPLOYING  VERSION  3\\nI’ve made the v3 version of the image available as luksa/kubia:v3 . You’ll deploy this\\nnew version by changing the image in the Deployment specification again: \\n$ kubectl set image deployment kubia nodejs=luksa/kubia:v3\\ndeployment \"kubia\" image updated\\nYou can follow the progress of the rollout with kubectl  rollout  status :\\n$ kubectl rollout status deployment kubia\\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\\nWaiting for rollout to finish: 1 old replicas are pending termination...\\ndeployment \"kubia\" successfully rolled out\\nThe new version is now live. As the followin g listing shows, after a few requests, your\\nweb clients start receiving errors.\\n$ while true; do curl http://130.211.109.222; done\\nThis is v3 running in pod kubia-1914148340-lalmx\\nThis is v3 running in pod kubia-1914148340-bz35w\\nThis is v3 running in pod kubia-1914148340-w0voh...\\nThis is v3 running in pod kubia-1914148340-w0voh\\nSome internal error has occurred! This is pod kubia-1914148340-bz35wThis is v3 running in pod kubia-1914148340-w0voh\\nSome internal error has occurred! This is pod kubia-1914148340-lalmx\\nThis is v3 running in pod kubia-1914148340-w0vohSome internal error has occurred! This is pod kubia-1914148340-lalmx\\nSome internal error has occurred! This is pod kubia-1914148340-bz35w\\nSome internal error has occurred! This is pod kubia-1914148340-w0voh\\nUNDOING  A ROLLOUT\\nYou can’t have your users experiencing intern al server errors, so you need to do some-\\nthing about it fast. In section 9.3.6 you’ll see how to block bad rollouts automatically,\\nbut for now, let’s see what you can do about your bad rollout manually. Luckily,\\nDeployments make it easy to roll back to the previously deployed version by telling\\nKubernetes to undo the last  rollout of a Deployment:\\n$ kubectl rollout undo deployment kubia\\ndeployment \"kubia\" rolled back\\nThis rolls the Deployment  back to the previous revision. \\nTIP The undo  command can also be used while the rollout process is still in\\nprogress to essentially abort the rollout. Pods already created during the roll-out process are removed and repl aced with the old ones again.Listing 9.9 Hitting your broken version 3'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 301}, page_content='270 CHAPTER  9Deployments: updating applications declaratively\\nDISPLAYING  A DEPLOYMENT ’S ROLLOUT  HISTORY\\nRolling back a rollout is possible because Deployments keep a revision history. As\\nyou’ll see later, the history is stored in the underlying ReplicaSets. When a rollout\\ncompletes, the old ReplicaSet isn’t deleted, and this enables rolling back to any revi-\\nsion, not only the previous one. The revi sion history can be displayed with the\\nkubectl  rollout  history  command:\\n$ kubectl rollout history deployment kubia\\ndeployments \"kubia\":REVISION    CHANGE-CAUSE\\n2           kubectl set image deployment kubia nodejs=luksa/kubia:v2\\n3           kubectl set image deployment kubia nodejs=luksa/kubia:v3\\nRemember the --record  command-line option you used when creating the Deploy-\\nment? Without it, the CHANGE-CAUSE  column in the revision history would be empty,\\nmaking it much harder to figure out what’s behind each revision.\\nROLLING  BACK TO A SPECIFIC  DEPLOYMENT  REVISION\\nYou can roll back to a specific revision by specifying the revision in the undo  com-\\nmand. For example, if you want to roll back to the first version, you’d execute the fol-\\nlowing command:\\n$ kubectl rollout undo deployment kubia --to-revision=1\\nRemember the inactive ReplicaSet left ov er when you modified the Deployment the\\nfirst time? The ReplicaSet represents the firs t revision of your Deployment. All Replica-\\nSets created by a Deployment represent the co mplete revision history, as shown in fig-\\nure 9.11. Each ReplicaSet stores the comple te information of the Deployment at that\\nspecific revision, so you shouldn’t delete it manually. If you do, you’ll lose that specific\\nrevision from the Deployment’s history, preventing you from rolling back to it.\\nBut having old ReplicaSets cluttering your Repl icaSet list is not ideal, so the length of\\nthe revision history is limited by the revisionHistoryLimit  property on the Deploy-\\nment resource. It defaults to two, so norma lly only the current and the previous revision\\nare shown in the history (and only the cu rrent and the previous ReplicaSet are pre-\\nserved). Older ReplicaSets are deleted automatically. Deploymentv1 ReplicaSet ReplicaSet Pods: v1 ReplicaSet ReplicaSet ReplicaSetRevision 2 Revision 4 Revision 3 Revision 1\\nRevision history Current revision\\nFigure 9.11 A Deployment’s ReplicaSets also act as its revision history.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 302}, page_content='271 Using Deployments for updating apps declaratively\\nNOTE The extensions/v1beta1  version of Deployments doesn’t have a default\\nrevisionHistoryLimit , whereas the default in version apps/v1beta2  is 10.\\n9.3.4 Controlling the rate of the rollout\\nWhen you performed the rollout to v3 and tracked its progress with the kubectl\\nrollout  status  command, you saw that first a new pod was created, and when it\\nbecame available, one of the old pods was deleted and another new pod was created.This continued until there were no old pods left. The way new pods are created and\\nold ones are deleted is configurable throug h two additional properties of the rolling\\nupdate strategy. \\nINTRODUCING  THE MAXSURGE AND MAXUNAVAILABLE  PROPERTIES  OF THE ROLLING  UPDATE  STRATEGY\\nTwo properties affect how many pods are re placed at once during a Deployment’s roll-\\ning update. They are maxSurge  and maxUnavailable  a n d  c a n  b e  s e t  a s  p a r t  o f  t h e\\nrollingUpdate  sub-property of the Deployment’s strategy  attribute, as shown in\\nthe following listing.\\nspec:\\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 1      maxUnavailable: 0\\n    type: RollingUpdate\\nWhat these properties do is explained in table 9.2.\\nBecause the desired replica count in your case was three, and both these properties\\ndefault to 25%, maxSurge  allowed the number of all pods to reach four, andListing 9.10 Specifying parameters for the rollingUpdate  strategy\\nTable 9.2 Properties for configuring the rate of the rolling update\\nProperty What it does\\nmaxSurge Determines how many pod instances you allow to exist above the desired replica \\ncount configured on the Deployment. It defaults to 25%, so there can be at most 25% more pod instances than the desir ed count. If the desired replica count is \\nset to four, there will never be more th an five pod instances running at the same \\ntime during an update. When converting  a percentage to an absolute number, \\nthe number is rounded up. Instead of a percentage, the value can also be an \\nabsolute value (for example, one or two additional pods can be allowed).\\nmaxUnavailable Determines how many pod instances can be unavailable relative to the desired \\nreplica count during the update. It also defaults to 25%, so the number of avail-\\nable pod instances must never fall below 75%  of the desired replica count. Here, \\nwhen converting a percentage to an abs olute number, the number is rounded \\ndown. If the desired replica count is set to four and the percentage is 25%, only \\none pod can be unavailable. There will alway s be at least three pod instances \\navailable to serve requests during the whole rollout. As with maxSurge , you can \\nalso specify an absolute value instead of a percentage.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 303}, page_content='272 CHAPTER  9Deployments: updating applications declaratively\\nmaxUnavailable  disallowed having any unavailable pods (in other words, three pods\\nhad to be available at all times) . This is shown in figure 9.12.\\nUNDERSTANDING  THE MAXUNAVAILABLE  PROPERTY\\nThe extensions/v1beta1  version of Deployments uses different defaults—it sets both\\nmaxSurge  and maxUnavailable  to 1 instead of 25%. In the case of three replicas, max-\\nSurge  is the same as before, but maxUnavailable  is different (1 instead of 0). This\\nmakes the rollout process unwind a bit differently, as shown in figure 9.13.v1Number\\nof pods\\n34\\n2\\n1\\nTimev1\\n3 available\\n1 unavailableCreate\\none\\nv2 pod\\n4 available3 available\\n1 unavailable\\n4 available3 available\\n1 unavailablemaxSurge = 1\\nmaxUnavailable = 0Desired replica count = 3\\n3 availablev2v1 v1 v2 v2v1 v1\\nv1v1\\nv1v1\\nv1 v1v1v1 v2 v2 v2 v2v2 v2 v2\\nv2\\nv1v2v2 v2 v2\\n4 availableWait\\nuntil\\nit’s\\navailableDelete\\none v1\\npod and\\ncreate one\\nv2 podWait\\nuntil\\nit’s\\navailableDelete\\none v1\\npod and\\ncreate one\\nv2 podWait\\nuntil\\nit’s\\navailableDelete\\nlast\\nv1 pod\\nFigure 9.12 Rolling update of a Deployment with three replicas and default maxSurge  and maxUnavailable  \\nv1Number\\nof pods\\n34\\n2\\n1\\nTimev1\\n2 available\\n2 unavailable\\n4 available2 available\\n1 unavailable\\n3 availablemaxSurge = 1\\nmaxUnavailable = 1Desired replica count = 3\\nv1 v1v1\\nv1v1\\nv2v2\\nv2v2v2 v2v2\\nv2v2\\nv2Wait until\\nboth are\\navailableDelete\\ntwo v1\\npods and\\ncreate one\\nv2 podDelete v1\\npod and\\ncreate two\\nv2 podsWait\\nuntil it’s\\navailable\\nFigure 9.13 Rolling update of a Deployment with the maxSurge=1  and maxUnavailable=1'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 304}, page_content='273 Using Deployments for updating apps declaratively\\nIn this case, one replica can be unavailable,  so if the desired replica count is three,\\nonly two of them need to be available. That’s why the rollout process immediately\\ndeletes one pod and creates two new ones. This ensures two pods are available and\\nthat the maximum number of pods isn’t exceeded (the maximum is four in thiscase—three plus one from \\nmaxSurge ). As soon as the two new pods are available, the\\ntwo remaining old pods are deleted.\\n This is a bit hard to grasp, especially since the maxUnavailable  property leads you\\nto believe that that’s the maximum number of unavailable pods that are allowed. If\\nyou look at the previous figure closely, you’ll see two unavailable pods in the second\\ncolumn even though maxUnavailable  is set to 1. \\n I t ’ s  i m p o r t a n t  t o  k e e p  i n  m i n d  t h a t  maxUnavailable  is relative to the desired\\nreplica count. If the replica count is set to three and maxUnavailable  is set to one,\\nthat means that the update process must al ways keep at least two (3 minus 1) pods\\navailable, while the number of pods that aren’t available can exceed one.\\n9.3.5 Pausing the rollout process\\nAfter the bad experience with version 3 of your app, imagine you’ve now fixed the bug\\nand pushed version 4 of your image. You’re a little apprehensive about rolling it out\\nacross all your pods the way you did befo re. What you want is to run a single v4 pod\\nnext to your existing v2 pods and see how it behaves with only a fraction of all your\\nusers. Then, once you’re sure everything’s  okay, you can replace all the old pods with\\nnew ones. \\n You could achieve this by running an additional pod either directly or through an\\nadditional Deployment, ReplicationController, or ReplicaSet, but you do have anotheroption available on the Deployment itself . A Deployment can also be paused during\\nthe rollout process. This allows you to verify  that everything is fine with the new ver-\\nsion before proceeding with the rest of the rollout.\\nPAUSING  THE ROLLOUT\\nI’ve prepared the v4 image, so go ahead and trigger the rollout by changing the image\\nto luksa/kubia:v4 , but then immediately (within a few seconds) pause the rollout:\\n$ kubectl set image deployment kubia nodejs=luksa/kubia:v4\\ndeployment \"kubia\" image updated\\n$ kubectl rollout pause deployment kubia\\ndeployment \"kubia\" paused\\nA single new pod should have been created, but all original pods should also still be\\nrunning. Once the new pod is up, a part of al l requests to the service will be redirected\\nto the new pod. This way, you’ve effectively run a canary release. A canary release is a\\ntechnique for minimizing the risk of rolling  out a bad version of an application and it\\naffecting all your users. Instead of rolling out the new version to everyone, you replace\\nonly one or a small number of old pods with new ones. This way only a small number\\nof users will initially hit the new version. You can then verify whether the new version'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 305}, page_content='274 CHAPTER  9Deployments: updating applications declaratively\\nis working fine or not and then either co ntinue the rollout across all remaining pods\\nor roll back to the previous version. \\nRESUMING  THE ROLLOUT\\nIn your case, by pausing the rollout process, only a small portion of client requests will\\nhit your v4 pod, while most will still hit the v3 pods. Once you’re confident the new\\nversion works as it should, you can resume the deployment to replace all the old pods\\nwith new ones:\\n$ kubectl rollout resume deployment kubia\\ndeployment \"kubia\" resumed\\nObviously, having to pause the deployment at an exact point in the rollout process\\nisn’t what you want to do. In the future, a new upgrade strategy may do that automati-\\ncally, but currently, the proper way of perf orming a canary release is by using two dif-\\nferent Deployments and scaling them appropriately. \\nUSING THE PAUSE  FEATURE  TO PREVENT  ROLLOUTS\\nPausing a Deployment can also be used to  prevent updates to the Deployment from\\nkicking off the rollout process, allowing yo u to make multiple changes to the Deploy-\\nment and starting the rollout only when yo u’re done making all the necessary changes.\\nOnce you’re ready for changes to take ef fect, you resume the Deployment and the\\nrollout process will start.\\nNOTE If a Deployment is paused, the undo  command won’t undo it until you\\nresume the Deployment.\\n9.3.6 Blocking rollouts of bad versions\\nBefore you conclude this chapter, we need to discuss one more property of the Deploy-\\nment resource. Remember the minReadySeconds  property you set on the Deployment\\nat the beginning of section 9.3.2? You used it  to slow down the rollout, so you could see\\nit was indeed performing a rolling update an d not replacing all the pods at once. The\\nmain function of minReadySeconds  is to prevent deploying malfunctioning versions, not\\nslowing down a deployment for fun. \\nUNDERSTANDING  THE APPLICABILITY  OF MINREADYSECONDS\\nThe minReadySeconds  property specifies how long a newly created pod should be\\nready before the pod is treated as available.  Until the pod is available, the rollout pro-\\ncess will not continue (remember the maxUnavailable  property?). A pod is ready\\nwhen readiness probes of all its containers return a success. If a new pod isn’t func-\\ntioning properly and its readiness probe starts failing before minReadySeconds  have\\npassed, the rollout of the new ve rsion will effectively be blocked.\\n You used this property to slow down your rollout process by having Kubernetes\\nwait 10 seconds after a pod was ready before  continuing with the rollout. Usually,\\nyou’d set minReadySeconds  to something much higher to make sure pods keep report-\\ning they’re ready after they’ve alread y started receiving actual traffic.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 306}, page_content='275 Using Deployments for updating apps declaratively\\n Although you should obviously test your pods both in a test and in a staging envi-\\nronment before deploying them into production, using minReadySeconds  is like an\\nairbag that saves your app from making a big mess after you’ve already let a buggy ver-\\nsion slip into production. \\n With a properly configured readiness probe and a proper minReadySeconds  set-\\nting, Kubernetes would have prevented us from deploying the buggy v3 version ear-\\nlier. Let me show you how.\\nDEFINING  A READINESS  PROBE  TO PREVENT  OUR V3 VERSION  FROM BEING  ROLLED  OUT FULLY\\nYou’re going to deploy version v3 again, but this time, you’ll have the proper readi-\\nness probe defined on the pod. Your Deployment is currently at version v4, so before\\nyou start, roll back to version v2 again so you can pretend this is the first time you’re\\nupgrading to v3. If you wish, you can go straight from v4 to v3, but the text that fol-\\nlows assumes you returned to v2 first.\\n Unlike before, where you only updated the image in the pod template, you’re now\\nalso going to introduce a readiness probe fo r the container at the same time. Up until\\nnow, because there was no explicit readin ess probe defined, the container and the\\npod were always considered ready, even if the app wasn’t truly ready or was returning\\nerrors. There was no way for Kubernetes to know that the app was malfunctioning and\\nshouldn’t be exposed to clients. \\n To change the image and introduce the readiness probe at once, you’ll use the\\nkubectl  apply  command. You’ll use the following YAML to update the deployment\\n(you’ll store it as kubia-deployment-v3-w ith-readinesscheck.yaml ), as shown in\\nthe following listing.\\napiVersion: apps/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: kubiaspec:\\n  replicas: 3\\n  minReadySeconds: 10             strategy:\\n    rollingUpdate:\\n      maxSurge: 1                        maxUnavailable: 0         \\n    type: RollingUpdate\\n  template:    metadata:\\n      name: kubia\\n      labels:        app: kubia\\n    spec:\\n      containers:      - image: luksa/kubia:v3Listing 9.11 Deployment with a readine ss probe: kubia-deployment-v3-with-\\nreadinesscheck.yaml\\nYou’re keeping \\nminReadySeconds \\nset to 10.\\nYou’re keeping maxUnavailable \\nset to 0 to make the deployment \\nreplace pods one by one'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 307}, page_content='276 CHAPTER  9Deployments: updating applications declaratively\\n        name: nodejs\\n        readinessProbe:\\n          periodSeconds: 1                 httpGet:                  \\n            path: /                 \\n            port: 8080              \\nUPDATING  A DEPLOYMENT  WITH KUBECTL  APPLY\\nTo update the Deployment this time, you’ll use kubectl  apply  like this:\\n$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml deployment \"kubia\" configured\\nThe apply  command updates the Deployment with everything that’s defined in the\\nYAML file. It not only updates the image bu t also adds the readiness probe definition\\nand anything else you’ve added or modified  in the YAML. If the new YAML also con-\\ntains the replicas  field, which doesn’t match the number of replicas on the existing\\nDeployment, the apply operation will also scale the Deployment, which isn’t usually\\nwhat you want. \\nTIP To keep the desired replica count unchanged when updating a Deploy-\\nment with kubectl  apply , don’t include the replicas  field in the YAML. \\nRunning the apply  command will kick off the update process, which you can again\\nfollow with the rollout  status  command:\\n$ kubectl rollout status deployment kubia\\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\\nBecause the status says one new pod has been created, your service should be hitting it\\noccasionally, right? Let’s see:\\n$ while true; do curl http://130.211.109.222; done\\nThis is v2 running in pod kubia-1765119474-jvslkThis is v2 running in pod kubia-1765119474-jvslk\\nThis is v2 running in pod kubia-1765119474-xk5g3\\nThis is v2 running in pod kubia-1765119474-pmb26This is v2 running in pod kubia-1765119474-pmb26\\nThis is v2 running in pod kubia-1765119474-xk5g3\\n...\\nNope, you never hit the v3 pod. Why not? Is it even there? List the pods:\\n$ kubectl get poNAME                     READY     STATUS    RESTARTS   AGE\\nkubia-1163142519-7ws0i   0/1       Running   0          30s\\nkubia-1765119474-jvslk   1/1       Running   0          9mkubia-1765119474-pmb26   1/1       Running   0          9m\\nkubia-1765119474-xk5g3   1/1       Running   0          8mYou’re defining a readiness probe \\nthat will be executed every second.\\nThe readiness probe will \\nperform an HTTP GET request \\nagainst our container.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 308}, page_content='277 Using Deployments for updating apps declaratively\\nAha! There’s your problem (or as you’ll le arn soon, your blessing)! The pod is shown\\nas not ready, but I guess you’ve been expecting that, right? What has happened?\\nUNDERSTANDING  HOW A READINESS  PROBE  PREVENTS  BAD VERSIONS  FROM BEING  ROLLED  OUT\\nAs soon as your new pod starts, the readin ess probe starts being hit every second (you\\nset the probe’s interval to one second in the pod spec). On the fifth request the readi-\\nness probe began failing, because your ap p starts returning HTTP status code 500\\nfrom the fifth request onward. \\n As a result, the pod is removed as an endpoint from the service (see figure 9.14).\\nBy the time you start hitting the service in the curl  loop, the pod has already been\\nmarked as not ready. This explains  why you never hit the new pod with curl . And\\nthat’s exactly what you want, because you do n’t want clients to hit a pod that’s not\\nfunctioning properly.\\nBut what about the rollout process? The rollout  status  command shows only one\\nnew replica has started. Thankfully, the rollout process will not continue, because the\\nnew pod will never become available. To be considered available, it needs to be ready\\nfor at least 10 seconds. Until it’s availabl e, the rollout process will not create any new\\npods, and it also won’t remove any or iginal pods because you’ve set the maxUnavailable\\nproperty to 0. Service curl\\nPod: v2 Pod: v2Pod: v3\\n(unhealthy)Pod: v2\\nReplicaSet: v2\\nReplicas: 3\\nDeployment\\nReplicas: 3\\nrollingUpdate:\\nmaxSurge: 1maxUnavailable: 0ReplicaSet: v3\\nReplicas: 1Requests are not forwarded\\nto v3 pod because of failedreadiness probe\\nFigure 9.14 Deployment blocked by a failing readiness probe in the new pod'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 309}, page_content='278 CHAPTER  9Deployments: updating applications declaratively\\n The fact that the deployment is stuck is a good thing, because if it had continued\\nreplacing the old pods with the new ones, you’d end up with a completely non-working\\nservice, like you did when you first rolled out version 3, when you weren’t using the\\nreadiness probe. But now, with the readiness probe in place, there was virtually nonegative impact on your users. A few user s may have experienced the internal server\\nerror, but that’s not as big of a problem as if the rollout had replaced all pods with the\\nfaulty version 3.\\nTIP If you only define the readiness probe without setting minReadySeconds\\nproperly, new pods are considered ava ilable immediately when the first invo-\\ncation of the readiness probe succeeds. If the readiness probe starts failing\\nshortly after, the bad version is rolle d out across all pods. Therefore, you\\nshould set minReadySeconds  appropriately.\\nCONFIGURING  A DEADLINE  FOR THE ROLLOUT\\nBy default, after the rollout can’t make any progress in 10 minutes, it’s considered as\\nfailed. If you use the kubectl  describe  deployment command, you’ll see it display a\\nProgressDeadlineExceeded  condition, as shown in the following listing.\\n$ kubectl describe deploy kubia\\nName:                   kubia\\n...\\nConditions:  Type          Status  Reason\\n  ----          ------  ------\\n  Available     True    MinimumReplicasAvailable  Progressing   False   ProgressDeadlineExceeded   \\nThe time after which the Deploy ment is considered failed is configurable through the\\nprogressDeadlineSeconds  property in the Deployment spec.\\nNOTE The extensions/v1beta1  version of Deployments doesn’t set a deadline.\\nABORTING  A BAD ROLLOUT\\nBecause the rollout will never continue, the only thing to do now is abort the rollout\\nby undoing it:\\n$ kubectl rollout undo deployment kubia\\ndeployment \"kubia\" rolled back\\nNOTE In future versions, the rollout will be aborted automatically when the\\ntime specified in progressDeadlineSeconds  is exceeded.Listing 9.12 Seeing the conditions of a Deployment with kubectl  describe\\nThe Deployment \\ntook too long to make progress.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 310}, page_content='279 Summary\\n9.4 Summary\\nThis chapter has shown you how to make yo ur life easier by using a declarative\\napproach to deploying and updating appl ications in Kubernetes. Now that you’ve\\nread this chapter, you should know how to\\n\\uf0a1Perform a rolling update of pods managed by a ReplicationController\\n\\uf0a1Create Deployments instead of lower-level ReplicationControllers or ReplicaSets\\n\\uf0a1Update your pods by editing the pod te mplate in the Deployment specification\\n\\uf0a1Roll back a Deployment either to the prev ious revision or to any earlier revision\\nstill listed in the revision history\\n\\uf0a1Abort a Deployment mid-way\\n\\uf0a1Pause a Deployment to inspect how a sing le instance of the new version behaves\\nin production before allowing addition al pod instances to replace the old ones\\n\\uf0a1Control the rate of the rolling update through maxSurge  and maxUnavailable\\nproperties\\n\\uf0a1Use minReadySeconds  and readiness probes to have the rollout of a faulty ver-\\nsion blocked automatically\\nIn addition to these Deployment-spe cific tasks, you also learned how to\\n\\uf0a1Use three dashes as a separator to define multiple resources in a single YAML file\\n\\uf0a1Turn on kubectl ’s verbose logging to see exactly what it’s doing behind the\\ncurtains\\nYou now know how to deploy and manage sets of pods created from the same pod\\ntemplate and thus share the same persistent storage. You even know how to update\\nthem declaratively. But what about running sets of pods, where each instance needs to\\nuse its own persistent storage? We haven’t look ed at that yet. That’s the subject of our\\nnext chapter.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 311}, page_content='280StatefulSets:\\ndeploying replicated\\nstateful applications\\nYou now know how to run both single-i nstance and replicated stateless pods,\\nand even stateful pods utilizing persis tent storage. You can run several repli-\\ncated web-server pod instances and you can run a single database pod instance\\nthat uses persistent storage, provided either through plain pod volumes or through\\nPersistentVolumes bound by a PersistentVolumeClaim. But can you employ aReplicaSet to replicate the database pod?This chapter covers\\n\\uf0a1Deploying stateful clustered applications\\n\\uf0a1Providing separate storage for each instance of \\na replicated pod\\n\\uf0a1Guaranteeing a stable name and hostname for pod replicas\\n\\uf0a1Starting and stopping pod replicas in a predictable order\\n\\uf0a1Discovering peers through DNS SRV records'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 312}, page_content='281 Replicating stateful pods\\n10.1 Replicating stateful pods\\nReplicaSets create multiple pod replicas from a single pod template. These replicas\\ndon’t differ from each other, apart from their name and IP address. If the pod tem-plate includes a volume, which refers to a sp ecific PersistentVolu meClaim, all replicas\\nof the ReplicaSet will use the exact same  PersistentVolumeClaim and therefore the\\nsame PersistentVolume bound by th e claim (shown in figure 10.1).\\nBecause the reference to the claim is in th e pod template, which is used to stamp out\\nmultiple pod replicas, you can’t make each replica use its own separate Persistent-VolumeClaim. You can’t use a ReplicaSet to  run a distributed data store, where each\\ninstance needs its own separate storage—at least not by using a single ReplicaSet. To\\nbe honest, none of the API objects you’ve s een so far make running such a data store\\npossible. You need something else. \\n10.1.1 Running multiple replicas with separate storage for each\\nHow does one run multiple replicas of a pod and have each pod use its own storagevolume? ReplicaSets create exact copies (rep licas) of a pod; therefore you can’t use\\nthem for these types of pods. What can you use?\\nCREATING  PODS MANUALLY\\nYou could create pods manually and have ea ch of them use its own PersistentVolume-\\nClaim, but because no ReplicaSet looks af ter them, you’d need to manage them man-\\nually and recreate them when they disappe ar (as in the event of a node failure).\\nTherefore, this isn’t a viable option.\\nUSING ONE REPLICA SET PER POD INSTANCE\\nInstead of creating pods directly, you coul d create multiple ReplicaSets—one for each\\npod with each ReplicaSet’s desired replica co unt set to one, and each ReplicaSet’s pod\\ntemplate referencing a dedicated Persiste ntVolumeClaim (as shown in figure 10.2).\\n Although this takes care of the automati c rescheduling in case of node failures or\\naccidental pod deletions, it’s much more  cumbersome compared to having a single\\nReplicaSet. For example, think about how you’d scale the pods in that case. YouPersistent\\nVolume\\nClaimPersistent\\nVolumeReplicaSet Pod\\nPodPod\\nFigure 10.1 All pods from the same ReplicaSet always use the same \\nPersistentVolumeClaim and PersistentVolume.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 313}, page_content='282 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\ncouldn’t change the desired replica count—you’d have to create additional Replica-\\nSets instead. \\n Using multiple ReplicaSets is therefore not the best solution. But could you maybe\\nuse a single ReplicaSet and have each pod in stance keep its own persistent state, even\\nthough they’re all using the same storage volume? \\nUSING MULTIPLE  DIRECTORIES  IN THE SAME VOLUME\\nA trick you can use is to have all pods use the same PersistentVolume, but then have a\\nseparate file directory inside that volume for each pod (this is shown in figure 10.3).\\nBecause you can’t configure pod replicas differently from a single pod template, you\\ncan’t tell each instance what directory it should use, but you can make each instance\\nautomatically select (and possibly also cr eate) a data directory that isn’t being used\\nby any other instance at that time. This solution does require coordination between\\nthe instances, and isn’t easy to do correctly. It also makes the shared storage volume\\nthe bottleneck.\\n10.1.2 Providing a stable identity for each pod\\nIn addition to storage, certain clustered a pplications also require that each instance\\nhas a long-lived stable identity. Pods can be killed from time to time and replaced withPVC A1 PV A1 ReplicaSet A1 Pod A1-xyz\\nPVC A2 PV A2 ReplicaSet A2 Pod A2-xzy\\nPVC A3 PV A3 ReplicaSet A3 Pod A3-zyx\\nFigure 10.2 Using one ReplicaSet for each pod instance\\nPersistent\\nVolume\\nClaimPersistentVolume\\nReplicaSet Pod\\nPodPodApp\\nApp\\nApp/data/1/\\n/data/3//data/2/\\nFigure 10.3 Working around the shared storage problem by having the app \\nin each pod use a different file directory'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 314}, page_content='283 Replicating stateful pods\\nnew ones. When a ReplicaSet replaces a po d, the new pod is a completely new pod\\nwith a new hostname and IP, although the data in its storage volume may be that of\\nthe killed pod. For certain apps, starting up with the old instance’s data but with a\\ncompletely new network identity may cause problems.\\n Why do certain apps mandate a stable  network identity? This requirement is\\nfairly common in distributed stateful applications. Certain apps require the adminis-\\ntrator to list all the other cluster members and their IP addresses (or hostnames) ineach member’s configuration file. But in Kubernetes, every time a pod is resched-\\nuled, the new pod gets both a new hostname  and a new IP address, so the whole\\napplication cluster would have to be reco nfigured every time one of its members is\\nrescheduled. \\nUSING A DEDICATED  SERVICE  FOR EACH POD INSTANCE\\nA trick you can use to work around this prob lem is to provide a stable network address\\nfor cluster members by creating a dedicated Kubernetes Service for each individual\\nmember. Because service IPs are stable, you can then point to each member through\\nits service IP (rather than the pod IP) in the configuration. \\n This is similar to creating a ReplicaSet for each member to provide them with indi-\\nvidual storage, as described previously. Combining these two techniques results in the\\nsetup shown in figure 10.4 (an additional service covering all the cluster members is\\nalso shown, because you usually need  one for clients of the cluster).\\nThe solution is not only ugly , but it still doesn’t solve everything. The individual pods\\ncan’t know which Service they are exposed through (and thus can’t know their stable\\nIP), so they can’t self-register in other pods using that IP. PVC A1 PV A1 ReplicaSet A1 Pod A1-xzyService A1Service A\\nPVC A2 PV A2 ReplicaSet A2 Pod A2-xzyService A2\\nPVC A3 PV A3 ReplicaSet A3 Pod A3-zyxService A3Figure 10.4 Using one \\nService and ReplicaSet per \\npod to provide a stable \\nnetwork address and an individual volume for each \\npod, respectively'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 315}, page_content='284 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\n Luckily, Kubernetes saves us from resort ing to such complex solutions. The proper\\nclean and simple way of running these special types of applications in Kubernetes is\\nthrough a StatefulSet. \\n10.2 Understanding StatefulSets\\nInstead of using a ReplicaSet to run thes e types of pods, you create a StatefulSet\\nresource, which is specifically tailored to applications where instances of the applica-\\ntion must be treated as non-fungible indi viduals, with each one having a stable name\\nand state. \\n10.2.1 Comparing StatefulSets with ReplicaSets\\nTo understand the purpose of StatefulSets, it ’s best to compare them to ReplicaSets or\\nReplicationControllers. But first let me explai n them with a little analogy that’s widely\\nused in the field.\\nUNDERSTANDING  STATEFUL  PODS WITH THE PETS VS. CATTLE  ANALOGY\\nYou may have already heard of the pets vs. ca ttle analogy. If not, let me explain it. We\\ncan treat our apps either as pets or as cattle. \\nNOTE StatefulSets were initially called Pe tSets. That name comes from the\\npets vs. cattle analogy explained here.\\nWe tend to treat our app instances as pets , where we give each instance a name and\\ntake care of each instance individually. But it’s usually better to treat instances as cattle\\nand not pay special attention to each individu al instance. This makes it easy to replace\\nunhealthy instances without giving it a se cond thought, similar to the way a farmer\\nreplaces unhealthy cattle. \\n Instances of a stateless app, for exampl e ,  b e h a v e  m u c h  l i k e  h e a d s  o f  c a t t l e .  I t\\ndoesn’t matter if an instance dies—you can create a new instance and people won’tnotice the difference. \\n On the other hand, with stateful apps, an  app instance is more like a pet. When a\\npet dies, you can’t go buy a new one and expe ct people not to notice. To replace a lost\\npet, you need to find a new one that looks and behaves exactly like the old one. In the\\ncase of apps, this means the new instance n eeds to have the same state and identity as\\nthe old one.\\nCOMPARING  STATEFUL SETS WITH REPLICA SETS OR REPLICATION CONTROLLERS\\nPod replicas managed by a ReplicaSet or Re plicationController are much like cattle.\\nBecause they’re mostly stateless, they ca n be replaced with a completely new pod\\nreplica at any time. Stateful pods require a different approach. When a stateful pod\\ninstance dies (or the node it’s running on fails), the pod instance needs to be resur-\\nrected on another node, but the new instan ce needs to get the same name, network\\nidentity, and state as the one it’s replacing.  This is what happens when the pods are\\nmanaged through a StatefulSet.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 316}, page_content='285 Understanding StatefulSets\\n A StatefulSet makes sure pods are reschedul ed in such a way that they retain their\\nidentity and state. It also allows you to easily scale the number of pets up and down. A\\nStatefulSet, like a ReplicaSet, has a desired replica count field that determines how\\nmany pets you want running at that time. Si milar to ReplicaSets, pods are created from\\na pod template specified as part of the St atefulSet (remember the cookie-cutter anal-\\nogy?). But unlike pods created by ReplicaSets, pods created by the StatefulSet aren’t\\nexact replicas of each other. Each can ha ve its own set of volumes—in other words,\\nstorage (and thus persistent state)—which differentiates it from its peers. Pet pods\\nalso have a predictable (and stable) identity  instead of each new pod instance getting\\na completely random one. \\n10.2.2 Providing a stable network identity\\nEach pod created by a StatefulSet is assigned an ordinal index (zero-based), whichis then used to derive the pod’s name an d hostname, and to attach stable storage to\\nthe pod. The names of the pods are thus predictable, because each pod’s name is\\nderived from the StatefulSet’s name and the ordinal index of the instance. Rather\\nthan the pods having random names, they’r e nicely organized, as shown in the next\\nfigure.\\nINTRODUCING  THE GOVERNING  SERVICE\\nBut it’s not all about the pods having a pr edictable name and hostname. Unlike regu-\\nlar pods, stateful pods sometimes need to be addressable by their hostname, whereas\\nstateless pods usually don’t. After all, each  stateless pod is like any other. When you\\nneed one, you pick any one of them. But with  stateful pods, you usually want to oper-\\nate on a specific pod from the group, because they differ from each other (they hold\\ndifferent state, for example). \\n For this reason, a StatefulSet requires  you to create a corresponding governing\\nheadless Service that’s used to provide th e actual network identity to each pod.\\nThrough this Service, each pod gets its ow n DNS entry, so its peers and possibly other\\nclients in the cluster can address the pod by  its hostname. For exam ple, if the govern-\\ning Service belongs to the default  namespace and is called foo, and one of the podsReplicaSet A\\n PodA-fewrb\\nPodA-jwqec\\nPodA-dsfwx\\nStatefulSet A\\n PodA-1\\nPodA-2\\nPodA-0\\nFigure 10.5 Pods created by a Stateful Set have predictable names (and hostnames), \\nunlike those created by a ReplicaSet'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 317}, page_content='286 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\nis called A-0, you can reach the pod through its fully qualified domain name, which\\nis a-0.foo.default.sv c.cluster.local . You can’t do that with pods managed by a\\nReplicaSet.\\n Additionally, you can also use DNS to l ook up all the StatefulSet’s pods’ names by\\nlooking up SRV records for the foo.default.svc .cluster.local  domain. We’ll\\nexplain SRV records in section 10.4 and le arn how they’re used to discover members\\nof a StatefulSet.\\nREPLACING  LOST PETS\\nWhen a pod instance managed by a Stateful Set disappears (because the node the pod\\nwas running on has failed, it was evicted from the node, or someone deleted the pod\\nobject manually), the StatefulSet makes sure  it’s replaced with a new instance—similar\\nto how ReplicaSets do it. But in contrast to  ReplicaSets, the replacement pod gets the\\nsame name and hostname as the pod that has disappeared (this distinction between\\nReplicaSets and StatefulSets is  illustrated in figure 10.6).\\nNode 1 Node 2 Node 1 Node 2\\nReplicaSet B ReplicaSet BStatefulSet\\nStatefulSet APod A-0 Pod A-1\\n PodA-0\\n PodA-0\\nPod A-1Node 1 fails\\nStatefulSet A\\nNode 1 Node 2 Node 1 Node 2\\nReplicaSetNode 1 fails\\nPod B-fdawr Pod B-jkbde\\n PodB-fdawr\\nPodB-rsqkwPod B-jkbde\\nFigure 10.6 A StatefulSet replaces a lost pod with a new one with the same identity, whereas a \\nReplicaSet replaces it with a completely new unrelated pod.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 318}, page_content='287 Understanding StatefulSets\\nThe new pod isn’t necessarily scheduled to the same node, but as you learned early\\non, what node a pod runs on shouldn’t matter. This holds true even for stateful pods.\\nEven if the pod is scheduled to a different node, it will still be available and reachable\\nunder the same hostname as before. \\nSCALING  A STATEFUL SET\\nScaling the StatefulSet creates a new pod inst ance with the next unused ordinal index.\\nIf you scale up from two to three instances, the new instance will get index 2 (the exist-\\ning instances obviously have indexes 0 and 1). \\n The nice thing about scaling down a StatefulSet is the fact that you always know\\nwhat pod will be removed. Again, this is al so in contrast to scaling down a ReplicaSet,\\nwhere you have no idea what instance will  be deleted, and you can’t even specify\\nwhich one you want removed first (but this feature may be introduced in the future).\\nScaling down a StatefulSet always removes th e instances with the highest ordinal index\\nfirst (shown in figure 10.7). This makes the effects of a scale-down predictable.\\nBecause certain stateful applications don’t handle rapid scale-downs nicely, Stateful-\\nSets scale down only one pod instance at a time. A distributed data store, for example,\\nmay lose data if multiple nodes go down at  the same time. For example, if a replicated\\ndata store is configured to store two copies  of each data entry, in cases where two\\nnodes go down at the same time, a data entry would be lost if it was stored on exactlythose two nodes. If the scale-down was sequ ential, the distributed data store has time\\nto create an additional replica of the data  entry somewhere else to replace the (single)\\nlost copy.\\n For this exact reason, StatefulSets also ne ver permit scale-down operations if any of\\nthe instances are unhealthy. If an instance is unhealthy, and you scale down by one at\\nthe same time, you’ve effectively lost two cluster members at once.\\n10.2.3 Providing stable dedicated storage to each stateful instance\\nYou’ve seen how StatefulSets ensure statef ul pods have a stable identity, but what\\nabout storage? Each stateful pod instance n eeds to use its own storage, plus if a state-\\nful pod is rescheduled (replaced with a ne w instance but with the same identity as\\nbefore), the new instance must have the same  storage attached to it. How do Stateful-\\nSets achieve this?Pod\\nA-0Pod\\nA-1Pod\\nA-2\\nStatefulSet A\\nReplicas: 3Pod\\nA-0Pod\\nA-1Pod\\nA-2\\nStatefulSet A\\nReplicas: 2Pod\\nA-0Pod\\nA-1\\nStatefulSet A\\nReplicas: 1Scale down Scale down\\nFigure 10.7 Scaling down a StatefulSet always re moves the pod with the highest ordinal index first.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 319}, page_content='288 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\n Obviously, storage for stateful pods n eeds to be persistent and decoupled from\\nthe pods. In chapter 6 you learned about PersistentVolumes and PersistentVolume-\\nClaims, which allow persistent storage to be attached to a pod by referencing the\\nPersistentVolumeClaim in the pod by name. Because PersistentVolumeClaims mapto PersistentVolumes one-to-one, each pod of a StatefulSet needs to reference a dif-\\nferent PersistentVolumeClaim to have its own separate PersistentVolume. Because\\nall pod instances are stamped from the same  pod template, how can they each refer\\nto a different PersistentVolumeClaim? An d who creates these claims? Surely you’re\\nnot expected to create as many Persistent VolumeClaims as the number of pods you\\nplan to have in the StatefulSet upfront? Of course not.\\nTEAMING  UP POD TEMPLATES  WITH VOLUME  CLAIM  TEMPLATES\\nThe StatefulSet has to create the PersistentVo lumeClaims as well, the same way it’s cre-\\nating the pods. For this reason, a StatefulSe t can also have one or more volume claim\\ntemplates, which enable it to stamp out Pe rsistentVolumeClaims along with each pod\\ninstance (see figure 10.8).\\nThe PersistentVolumes for the claims can eith er be provisioned up-front by an admin-\\nistrator or just in time through dynamic prov isioning of PersistentVolumes, as explained\\nat the end of chapter 6. \\nUNDERSTANDING  THE CREATION  AND DELETION  OF PERSISTENT VOLUME CLAIMS\\nScaling up a StatefulSet by one creates tw o or more API objects (the pod and one or\\nmore PersistentVolumeClaims referenced by  the pod). Scaling down, however, deletes\\nonly the pod, leaving the claims alone. The reason for this is obvious, if you consider\\nwhat happens when a claim is deleted. After a claim is deleted, the PersistentVolume itwas bound to gets recycled or deleted and its contents are lost. \\n Because stateful pods are meant to run st ateful applications, which implies that the\\ndata they store in the volume is important, de leting the claim on scale-down of a Stateful-\\nSet could be catastrophic—especially since triggering a scale-down is as simple as\\ndecreasing the \\nreplicas  field of the StatefulSet. For this reason, you’re required to\\ndelete PersistentVolumeClaims manually to release the underlying PersistentVolume.PVC A-0 PV Pod A-0\\nPVC A-1 PV Pod A-1\\nPVC A-2 PV Pod A-2StatefulSet A\\nPod\\ntemplate\\nVolume claim\\ntemplate\\nFigure 10.8 A StatefulSet creates both pods and PersistentVolumeClaims.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 320}, page_content='289 Understanding StatefulSets\\nREATTACHING  THE PERSISTENT VOLUME CLAIM TO THE NEW INSTANCE  OF THE SAME POD\\nThe fact that the PersistentVolumeClaim remains after a scale-down means a subse-\\nquent scale-up can reattach the same cl aim along with the bound PersistentVolume\\nand its contents to the new pod instance (s hown in figure 10.9). If you accidentally\\nscale down a StatefulSet, you can undo th e mistake by scaling up again and the new\\npod will get the same persisted stat e again (as well as the same name).\\n10.2.4 Understanding StatefulSet guarantees\\nAs you’ve seen so far, StatefulSets behave differently from ReplicaSets or Replication-\\nControllers. But this doesn’t end with the pods having a stable identity and storage.\\nStatefulSets also have different guarantees regarding their pods. \\nUNDERSTANDING  THE IMPLICATIONS  OF STABLE  IDENTITY  AND STORAGE\\nWhile regular, stateless pods are fungible, st ateful pods aren’t. We’ve already seen how\\na stateful pod is always replaced with an identical pod (one having the same name and\\nhostname, using the same persistent storag e, and so on). This happens when Kuber-\\nnetes sees that the old pod is no longer th ere (for example, when you delete the pod\\nmanually). \\n B u t  w h a t  i f  K u b e r n e t e s  c a n ’ t  b e  s u r e  a b out the state of the pod? If it creates a\\nreplacement pod with the same identity, two instances of the app with the same iden-\\ntity might be running in the system. The two would also be bound to the same storage,Pod\\nA-0Pod\\nA-1\\nStatefulSet A\\nReplicas: 2Scale\\ndownScale\\nup\\nNew pod instance created\\nwith same identity as beforePVC is\\nre-attached\\nPVC\\nA-0PV\\nPVC\\nA-1PV\\nPod\\nA-0\\nStatefulSet A\\nReplicas: 1PVC\\nA-0PV\\nPVC\\nA-1PV\\nPod\\nA-0\\nPod has been deletedPod\\nA-1\\nStatefulSet A\\nReplicas: 2PVC\\nA-0PV\\nPVC\\nA-1PVC has not\\nbeen deleted\\nPV\\nFigure 10.9 StatefulSets don’t delete Persist entVolumeClaims when scaling down; then they \\nreattach them when scaling back up.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 321}, page_content='290 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\nso two processes with the same identity woul d be writing over the same files. With pods\\nmanaged by a ReplicaSet, this isn’t a proble m, because the apps are obviously made to\\nwork on the same files. Also, ReplicaSets cr eate pods with a randomly generated iden-\\ntity, so there’s no way for two proce sses to run with the same identity. \\nINTRODUCING  STATEFUL SET’S AT-MOST-ONE SEMANTICS\\nKubernetes must thus take great care to ensure two stateful pod instances are never\\nrunning with the same identity and are bo und to the same PersistentVolumeClaim. A\\nStatefulSet must guarantee at-most-one  semantics for stateful pod instances. \\n This means a StatefulSet must be absolute ly certain that a pod is no longer run-\\nning before it can create a replacement pod.  This has a big effect on how node fail-\\nures are handled. We’ll demonstrate this late r in the chapter. Before we can do that,\\nhowever, you need to create a StatefulSet an d see how it behaves. You’ll also learn a\\nfew more things about them along the way.\\n10.3 Using a StatefulSet\\nTo properly show StatefulSets in action, you’ll build your own little clustered data\\nstore. Nothing fancy—more like a data store from the Stone Age. \\n10.3.1 Creating the app and container image\\nYou’ll use the kubia app you’ve used througho ut the book as your starting point. You’ll\\nexpand it so it allows you to store and retrie ve a single data entry on each pod instance. \\n The important parts of the source code of  your data store are shown in the follow-\\ning listing.\\n...\\nconst dataFile = \"/var/data/kubia.txt\";\\n...var handler = function(request, response) {\\n  if (request.method == \\'POST\\') {                \\n    var file = fs.createWriteStream(dataFile);                         file.on(\\'open\\', function (fd) {                                \\n      request.pipe(file);                                          \\n      console.log(\"New data has been received and stored.\");             response.writeHead(200);                                     \\n      response.end(\"Data stored on pod \" + os.hostname() + \"\\\\n\");  \\n    });  } else {                                       \\n    var data = fileExists(dataFile)                                \\n      ? fs.readFileSync(dataFile, \\'utf8\\')                                : \"No data posted yet\";                                      \\n    response.writeHead(200);                                       \\n    response.write(\"You\\'ve hit \" + os.hostname() + \"\\\\n\");              response.end(\"Data stored on this pod: \" + data + \"\\\\n\");       \\n  }\\n};Listing 10.1 A simple stateful app: kubia-pet-image/app.js\\nOn POST \\nrequests, store the request’s \\nbody into a \\ndata file.\\nOn GET (and all \\nother types of) \\nrequests, return \\nyour hostname and the contents \\nof the data file.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 322}, page_content='291 Using a StatefulSet\\nvar www = http.createServer(handler);\\nwww.listen(8080);\\nWhenever the app receives a POST request, it writes the data it receives in the body of\\nthe request to the file /var/data/kubia.txt. Upon a GET request, it returns the host-\\nname and the stored data (contents of the file). Simple enough, right? This is the firstversion of your app. It’s not clustered yet,  but it’s enough to get you started. You’ll\\nexpand the app later in the chapter.\\n The Dockerfile for building the container image is shown in the following listing\\nand hasn’t changed from before.\\nFROM node:7\\nADD app.js /app.js\\nENTRYPOINT [\"node\", \"app.js\"]\\nGo ahead and build the image now, or use the one I pushed to docker.io/luksa/kubia-pet.\\n10.3.2 Deploying the app through a StatefulSet\\nTo deploy your app, you’ll need to create  two (or three) different types of objects:\\n\\uf0a1PersistentVolumes for storing your data fi les (you’ll need to create these only if\\nthe cluster doesn’t support dynamic pr ovisioning of PersistentVolumes).\\n\\uf0a1A governing Service required by the StatefulSet.\\n\\uf0a1The StatefulSet itself.\\nFor each pod instance, the StatefulSet will  create a PersistentVolumeClaim that will\\nbind to a PersistentVolume. If your cluste r supports dynamic provisioning, you don’t\\nneed to create any PersistentVolumes manua lly (you can skip the next section). If it\\ndoesn’t, you’ll need to create them as explained in the next section. \\nCREATING  THE PERSISTENT  VOLUMES\\nYou’ll need three PersistentVolumes, becaus e you’ll be scaling the StatefulSet up to\\nthree replicas. You must create more if yo u plan on scaling the StatefulSet up more\\nthan that.\\n If you’re using Minikube, deploy the PersistentVolumes defined in the Chapter06/\\npersistent-volumes-hostpath.yaml file in the book’s code archive. \\n If you’re using Google Kubernetes Engine, you’ll first need to create the actual\\nGCE Persistent Disks like this:\\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-a\\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-b\\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-c\\nNOTE Make sure to create the disks in the same zone that your nodes are\\nrunning in.Listing 10.2 Dockerfile for the stateful app: kubia-pet-image/Dockerfile'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 323}, page_content='292 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\nThen create the PersistentVolumes from the persistent-volumes-gcepd.yaml file,\\nwhich is shown in the following listing.\\nkind: List                     \\napiVersion: v1\\nitems:\\n- apiVersion: v1  kind: PersistentVolume       \\n  metadata:\\n    name: pv-a                  spec:\\n    capacity:\\n      storage: 1Mi                accessModes:\\n      - ReadWriteOnce\\n    persistentVolumeReclaimPolicy: Recycle         gcePersistentDisk:         \\n      pdName: pv-a             \\n      fsType: nfs4                         - apiVersion: v1\\n  kind: PersistentVolume\\n  metadata:    name: pv-b\\n ...\\nNOTE In the previous chapter you specified multiple resources in the same\\nYAML by delimiting them with a three-dash line. Here you’re using a differ-ent approach by defining a \\nList  object and listing the resources as items of\\nthe object. Both methods are equivalent.\\nThis manifest creates PersistentVolumes called pv-a , pv-b , and pv-c . They use GCE Per-\\nsistent Disks as the underlying storage mech anism, so they’re not appropriate for clus-\\nters that aren’t running on Google Kubern etes Engine or Google Compute Engine. If\\nyou’re running the cluster elsewhere, you mu st modify the PersistentVolume definition\\nand use an appropriate volume type, such as  NFS (Network File System), or similar.\\nCREATING  THE GOVERNING  SERVICE\\nAs explained earlier, before deploying a Stat efulSet, you first need to create a headless\\nService, which will be used to provide the network identity for your stateful pods. The\\nfollowing listing shows the Service manifest.\\napiVersion: v1\\nkind: Service\\nmetadata:  name: kubia       \\nspec:\\n  clusterIP: None    Listing 10.3 Three PersistentVolumes: persistent-volumes-gcepd.yaml\\nListing 10.4 Headless service to be used in the StatefulSet: kubia-service-headless.yamlFile describes a list \\nof three persistent \\nvolumes\\nPersistent volumes’ names \\nare pv-a, pv-b, and pv-c\\nCapacity of each persistent \\nvolume is 1 MebibyteWhen the volume \\nis released by the \\nclaim, it’s recycled to be used again.\\nThe volume uses a GCE \\nPersistent Disk as the underlying \\nstorage mechanism.\\nName of the \\nService\\nThe StatefulSet’s governing \\nService must be headless.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 324}, page_content='293 Using a StatefulSet\\n  selector:           \\n    app: kubia        \\n  ports:  - name: http\\n    port: 80\\nYou’re setting the clusterIP  field to None , which makes this a headless Service. It will\\nenable peer discovery between your pods (you ’ll need this later). Once you create the\\nService, you can move on to creating the actual StatefulSet.\\nCREATING  THE STATEFUL SET MANIFEST\\nNow you can finally create the StatefulSet.  The following listing shows the manifest.\\napiVersion: apps/v1beta1\\nkind: StatefulSet\\nmetadata:\\n  name: kubiaspec:\\n  serviceName: kubia\\n  replicas: 2  template:\\n    metadata:\\n      labels:                          app: kubia             \\n    spec:\\n      containers:      - name: kubia\\n        image: luksa/kubia-pet\\n        ports:        - name: http\\n          containerPort: 8080\\n        volumeMounts:\\n        - name: data                  \\n          mountPath: /var/data          volumeClaimTemplates:\\n  - metadata:                  \\n      name: data                   spec:                      \\n      resources:               \\n        requests:                        storage: 1Mi         \\n      accessModes:             \\n      - ReadWriteOnce          \\nThe StatefulSet manifest isn’t that differen t from ReplicaSet or Deployment manifests\\nyou’ve created so far. What’s new is the volumeClaimTemplates  list. In it, you’re defin-\\ning one volume claim template called data , which will be used to create a Persistent-\\nVolumeClaim for each pod. As you may reme mber from chapter 6,  a pod references a\\nclaim by including a persistentVolumeClaim  volume in the manifest. In the previousListing 10.5 StatefulSet manifest: kubia-statefulset.yamlAll pods with the app=kubia \\nlabel belong to this service.\\nPods created by the StatefulSet \\nwill have the app=kubia label.\\nThe container inside the pod will \\nmount the pvc volume at this path.\\nThe PersistentVolumeClaims \\nwill be created from this template.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 325}, page_content='294 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\npod template, you’ll find no such volume. Th e StatefulSet adds it to the pod specifica-\\ntion automatically and configures the volume to be bound to the claim the StatefulSet\\ncreated for the specific pod.\\nCREATING  THE STATEFUL SET\\nYou’ll create the StatefulSet now:\\n$ kubectl create -f kubia-statefulset.yaml \\nstatefulset \"kubia\" created\\nNow, list your pods:\\n$ kubectl get poNAME      READY     STATUS              RESTARTS   AGE\\nkubia-0   0/1       ContainerCreating   0          1s\\nNotice anything strange? Remember how a ReplicationController or a ReplicaSet cre-\\nates all the pod instances at the same time ? Your StatefulSet is configured to create\\ntwo replicas, but it created a single pod. \\n Don’t worry, nothing is wrong. The second pod will be created only after the first\\none is up and ready. StatefulSets behave this way because certain clustered stateful\\napps are sensitive to race conditions if two or more cluster members come up at the\\nsame time, so it’s safer to bring each me mber up fully before continuing to bring up\\nthe rest.\\n List the pods again to see how the pod creation is progressing:\\n$ kubectl get po\\nNAME      READY     STATUS              RESTARTS   AGE\\nkubia-0   1/1       Running             0          8skubia-1   0/1       ContainerCreating   0          2s\\nSee, the first pod is now running, and the second one has been created and is being\\nstarted. \\nEXAMINING  THE GENERATED  STATEFUL  POD\\nLet’s take a closer look at the first pod’s spec in the following listing to see how the\\nStatefulSet has constructed the pod from th e pod template and the PersistentVolume-\\nClaim template.\\n$ kubectl get po kubia-0 -o yaml\\napiVersion: v1kind: Pod\\nmetadata:\\n  ...spec:\\n  containers:\\n  - image: luksa/kubia-pet    ...Listing 10.6 A stateful pod created by the StatefulSet'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 326}, page_content='295 Using a StatefulSet\\n    volumeMounts:\\n    - mountPath: /var/data           \\n      name: data                         - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\\n      name: default-token-r2m41\\n      readOnly: true  ...\\n  volumes:\\n  - name: data                       \\n    persistentVolumeClaim:           \\n      claimName: data-kubia-0              - name: default-token-r2m41\\n    secret:\\n      secretName: default-token-r2m41\\nThe PersistentVolumeClaim template was us ed to create the PersistentVolumeClaim\\nand the volume inside the pod, which refe rs to the created PersistentVolumeClaim. \\nEXAMINING  THE GENERATED  PERSISTENT VOLUME CLAIMS\\nNow list the generated PersistentVolumeCl aims to confirm they were created:\\n$ kubectl get pvc\\nNAME           STATUS    VOLUME    CAPACITY   ACCESSMODES   AGEdata-kubia-0   Bound     pv-c      0                        37s\\ndata-kubia-1   Bound     pv-a      0                        37s\\nThe names of the generated PersistentVo lumeClaims are composed of the name\\ndefined in the volumeClaimTemplate  and the name of each pod. You can examine the\\nclaims’ YAML to see that they match the template.\\n10.3.3 Playing with your pods\\nWith the nodes of your data store cluster now running, you can start exploring it. You\\ncan’t communicate with your pods through the Service you created because it’s head-\\nless. You’ll need to connect to individual pods  directly (or create a regular Service, but\\nthat wouldn’t allow you to talk to a specific pod).\\n You’ve already seen ways to connect to a pod directly: by piggybacking on another\\npod and running curl  inside it, by using port-forwarding, and so on. This time, you’ll\\ntry another option. You’ll use the AP I server as a proxy to the pods. \\nCOMMUNICATING  WITH PODS THROUGH  THE API SERVER\\nOne useful feature of the API server is the ab ility to proxy connections directly to indi-\\nvidual pods. If you want to perform requests against your kubia-0  pod, you hit the fol-\\nlowing URL:\\n<apiServerHost>:<port>/api/v1/namespaces/d efault/pods/kubia-0/proxy/<path>\\nBecause the API server is secured, sending requests to pods through the API server is\\ncumbersome (among other things, you need  to pass the authorization token in each\\nrequest). Luckily, in chapter 8 you learned how to use kubectl  proxy  to talk to theThe volume mount, as \\nspecified in the manifest\\nThe volume created \\nby the StatefulSet\\nThe claim referenced \\nby this volume'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 327}, page_content='296 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\nAPI server without having to deal with au thentication and SSL certificates. Run the\\nproxy again:\\n$ kubectl proxy\\nStarting to serve on 127.0.0.1:8001\\nNow, because you’ll be talking to the API server through the kubectl  proxy, you’ll use\\nlocalhost:8001 rather than the actual API server host and port. You’ll send a request tothe \\nkubia-0  pod like this:\\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\\nYou\\'ve hit kubia-0\\nData stored on this pod: No data posted yet\\nThe response shows that the request was in deed received and handled by the app run-\\nning in your pod kubia-0 . \\nNOTE If you receive an empty response, make sure you haven’t left out that\\nlast slash character at the en d of the URL (or make sure curl  follows redirects\\nby using its -L option). \\nBecause you’re communicating with the pod through the API server, which you’re\\nconnecting to through the kubectl  proxy, the request went through two different\\nproxies (the first was the kubectl  proxy and the other was the API server, which prox-\\nied the request to the pod). For a cl earer picture, examine figure 10.10.\\nThe request you sent to the pod was a GET request, but you can also send POST\\nrequests through the API server. This is do ne by sending a POST request to the same\\nproxy URL as the one you sent the GET request to. \\n When your app receives a POST request, it stores whatever’s in the request body\\ninto a local file. Send a POST request to the kubia-0  pod:\\n$ curl -X POST -d \"Hey there! This greeting was submitted to kubia-0.\"\\n➥ localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\\nData stored on pod kubia-0kubectlproxy curlGET localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\\nGET 192.168.99.100:8443/api/v1/namespaces/default/pods/kubia-0/proxy/\\nAuthorization: Bearer <token>\\nGET 172.17.0.3:8080/\\nAPI server Pod: kubia-0\\n192.168.99.100 172.17.0.3 localhost\\nFigure 10.10 Connecting to a pod through both the kubectl  proxy and API server proxy'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 328}, page_content='297 Using a StatefulSet\\nThe data you sent should now be stored in that pod. Let’s see if it returns the stored\\ndata when you perform a GET request again:\\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\\nYou\\'ve hit kubia-0\\nData stored on this pod: Hey there! This greeting was submitted to kubia-0.\\nOkay, so far so good. Now let’s see what the other cluster node (the kubia-1  pod)\\nsays:\\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/\\nYou\\'ve hit kubia-1\\nData stored on this pod: No data posted yet\\nAs expected, each node has its own state. Bu t is that state persisted? Let’s find out.\\nDELETING  A STATEFUL  POD TO SEE IF THE RESCHEDULED  POD IS REATTACHED  TO THE SAME STORAGE\\nYou’re going to delete the kubia-0  pod and wait for it to be rescheduled. Then you’ll\\nsee if it’s still serving the same data as before:\\n$ kubectl delete po kubia-0\\npod \"kubia-0\" deleted\\nIf you list the pods, you’ll see that the pod is terminating: \\n$ kubectl get poNAME      READY     STATUS        RESTARTS   AGE\\nkubia-0   1/1       Terminating   0          3mkubia-1   1/1       Running       0          3m\\nAs soon as it terminates successfully, a new pod with the same name is created by the\\nStatefulSet:\\n$ kubectl get po\\nNAME      READY     STATUS              RESTARTS   AGEkubia-0   0/1       ContainerCreating   0          6s\\nkubia-1   1/1       Running             0          4m\\n$ kubectl get poNAME      READY     STATUS    RESTARTS   AGE\\nkubia-0   1/1       Running   0          9s\\nkubia-1   1/1       Running   0          4m\\nLet me remind you again that this new pod may be scheduled to any node in the clus-\\nter, not necessarily the same node that the old pod was scheduled to. The old pod’swhole identity (the name, hostname, and the storage) is effectively moved to the new\\nnode (as shown in figure 10.11). If you’re using Minikube, you can’t see this because it\\nonly runs a single node, but in a multi-no de cluster, you may see the pod scheduled to\\na different node than before.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 329}, page_content=\"298 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\nWith the new pod now running, let’s check to see if it has the exact same identity as in\\nits previous incarnation. The pod’s name is the same, but what about the hostnameand persistent data? You can ask the pod itself to confirm:\\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\\nYou've hit kubia-0\\nData stored on this pod: Hey there! This greeting was submitted to kubia-0.\\nThe pod’s response shows that both the hostname and the data are the same as before,\\nconfirming that a StatefulSet always replaces  a deleted pod with what’s effectively the\\nexact same pod. \\nSCALING  A STATEFUL SET\\nScaling down a StatefulSet and scaling it  back up after an extended time period\\nshould be no different than deleting a pod and having the StatefulSet recreate it\\nimmediately. Remember that scaling down a StatefulSet only deletes the pods, but\\nleaves the PersistentVolumeClaims untouched.  I’ll let you try scaling down the State-\\nfulSet yourself and confirm this behavior. \\n The key thing to remember is that sc aling down (and up) is performed gradu-\\nally—similar to how individual pods are cr eated when the StatefulSet is created ini-\\ntially. When scaling down by more than on e instance, the pod with the highest ordinal\\nnumber is deleted first. Only after the pod terminates completely is the pod with the\\nsecond highest ordinal number deleted. \\nEXPOSING  STATEFUL  PODS THROUGH  A REGULAR , NON-HEADLESS  SERVICE\\nBefore you move on to the last part of this  chapter, you’re going to add a proper, non-\\nheadless Service in front of your pods, be cause clients usually connect to the pods\\nthrough a Service rather than connecting directly.\\nNode 1\\nPod: kubia-0\\nPod: kubia-1Delete kubia-0\\nStorage Storage\\nStorage Pod: kubia-1 StorageNode 1kubia-0 rescheduled\\nNode 1\\nNode 2 Node 2 Node 2Storage\\nPod: kubia-1 Storage\\nPod: kubia-0\\nFigure 10.11 A stateful pod may be rescheduled to a diff erent node, but it retains the name, hostname, and storage.\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 330}, page_content=\"299 Discovering peers in a StatefulSet\\n You know how to create the Service by now,  but in case you don’t, the following list-\\ning shows the manifest.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: kubia-publicspec:\\n  selector:\\n    app: kubia  ports:\\n  - port: 80\\n    targetPort: 8080\\nBecause this isn’t an externally exposed Service (it’s a regular ClusterIP  Service, not\\na NodePort  or a LoadBalancer -type Service), you can only access it from inside the\\ncluster. You’ll need a pod to acce ss it from, right? Not necessarily.\\nCONNECTING  TO CLUSTER -INTERNAL  SERVICES  THROUGH  THE API SERVER\\nInstead of using a piggyback pod to access th e service from inside the cluster, you can\\nuse the same proxy feature provided by the API server to access the service the way\\nyou’ve accessed individual pods.\\n The URI path for proxy-ing requests  to Services is formed like this:\\n/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>\\nTherefore, you can run curl  on your local machine and access the service through the\\nkubectl  proxy like this (you ran kubectl  proxy  earlier and it should still be running):\\n$ curl localhost:8001/api/v1/namespaces/default/services/kubia-\\n➥ public/proxy/\\nYou've hit kubia-1\\nData stored on this pod: No data posted yet\\nLikewise, clients (inside the cluster) can use the kubia-public  service for storing to\\nand reading data from your clustered data store. Of course, each request lands on a\\nrandom cluster node, so you’ll get the da ta from a random node each time. You’ll\\nimprove this next.\\n10.4 Discovering peers in a StatefulSet\\nWe still need to cover one more important thing. An important requirement of clus-\\ntered apps is peer discover y—the ability to find other members of the cluster. Each\\nmember of a StatefulSet needs to easily fi nd all the other members. Sure, it could do\\nthat by talking to the API server, but one of Kubernetes’ aims is to expose features thathelp keep applications completely Kubernetes -agnostic. Having apps talk to the Kuber-\\nnetes API is therefore undesirable.Listing 10.7 A regular Service for accessing the stateful pods: kubia-service-public.yaml\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 331}, page_content='300 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\n How can a pod discover its peers without talking to the API? Is there an existing,\\nwell-known technology you can use that ma kes this possible? How about the Domain\\nName System (DNS)? Depending on how much you know about DNS, you probably\\nunderstand what an A, CNAME, or MX record is used for. Other lesser-known types ofDNS records also exist. One of them is the SRV record.\\nINTRODUCING  SRV RECORDS\\nSRV records are used to point to hostname s and ports of servers providing a specific\\nservice. Kubernetes creates SRV records to point to the hostnames of the pods back-\\ning a headless service. \\n You’re going to list the SRV records for your stateful pods by running the dig DNS\\nlookup tool inside a new temporary po d. This is the command you’ll use:\\n$ kubectl run -it srvlookup --image=tutum/dnsutils --rm \\n➥ --restart=Never -- dig SRV kubia.default.svc.cluster.local\\nThe command runs a one-off pod ( --restart=Never ) called srvlookup , which is\\nattached to the console ( -it) and is deleted as soon as it terminates ( --rm ). The\\npod runs a single container from the tutum/dnsutils  image and runs the following\\ncommand:\\ndig SRV kubia.default.svc.cluster.local\\nThe following listing shows what the command prints out.\\n...\\n;; ANSWER SECTION:\\nk.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.\\nk.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.\\n;; ADDITIONAL SECTION:\\nkubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4\\nkubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6\\n...\\nNOTE I’ve had to shorten the actual name to get records to fit into a single\\nline, so kubia.d.s.c.l  is actually kubia.default.sv c.cluster.local .\\nThe ANSWER  SECTION  shows two SRV records pointing to the two pods backing your head-\\nless service. Each pod also gets its own A record, as shown in ADDITIONAL  SECTION .\\n For a pod to get a list of all the other po ds of a StatefulSet, all you need to do is\\nperform an SRV DNS lookup. In Node.js, for example, the lookup is performed\\nlike this:\\ndns.resolveSrv(\"kubia.default.svc.cluster. local\", callBackFunction);\\nYou’ll use this command in your app to enable each pod to discover its peers.Listing 10.8 Listing DNS SRV records of your headless Service'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 332}, page_content='301 Discovering peers in a StatefulSet\\nNOTE The order of the returned SRV record s is random, because they all have\\nthe same priority. Don’t expect to always see kubia-0  listed before kubia-1 .\\n10.4.1 Implementing peer discovery through DNS\\nYour Stone Age data store isn’t clustered ye t. Each data store node runs completely\\nindependently of all the others—no commun ication exists between them. You’ll get\\nthem talking to each other next.\\n Data posted by clients connecting to  your data store cluster through the kubia-\\npublic  Service lands on a random cluster no de. The cluster can store multiple data\\nentries, but clients currently have no good way to see all those entries. Because ser-\\nvices forward requests to pods randomly , a client would need to perform many\\nrequests until it hit all the pods if it wanted to get the data from all the pods. \\n You can improve this by having the node  respond with data from all the cluster\\nnodes. To do this, the node needs to find all its peers. You’re going to use what you\\nlearned about StatefulSets and SRV records to do this.\\n You’ll modify your app’s source code as  shown in the following listing (the full\\nsource is available in the book’s code ar chive; the listing shows only the important\\nparts).\\n...\\nconst dns = require(\\'dns\\');\\nconst dataFile = \"/var/data/kubia.txt\";\\nconst serviceName = \"kubia.default.svc.cluster.local\";\\nconst port = 8080;...\\nvar handler = function(request, response) {\\n  if (request.method == \\'POST\\') {\\n    ...\\n  } else {    response.writeHead(200);\\n    if (request.url == \\'/data\\') {\\n      var data = fileExists(dataFile)         ? fs.readFileSync(dataFile, \\'utf8\\') \\n        : \"No data posted yet\";\\n      response.end(data);    } else {\\n      response.write(\"You\\'ve hit \" + os.hostname() + \"\\\\n\");\\n      response.write(\"Data stored in the cluster:\\\\n\");      dns.resolveSrv(serviceName, function (err, addresses) {    \\n        if (err) {\\n          response.end(\"Could not look up DNS SRV records: \" + err);\\n          return;\\n        }\\n        var numResponses = 0;        if (addresses.length == 0) {\\n          response.end(\"No peers discovered.\");\\n        } else {Listing 10.9 Discovering peers in a sa mple app: kubia-pet-peers-image/app.js\\nThe app \\nperforms a DNS \\nlookup to obtain \\nSRV records.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 333}, page_content='302 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\n          addresses.forEach(function (item) {                   \\n            var requestOptions = {\\n              host: item.name,               port: port, \\n              path: \\'/data\\'\\n            };            httpGet(requestOptions, function (returnedData) {   \\n              numResponses++;\\n              response.write(\"- \" + item.name + \": \" + returnedData);\\n              response.write(\"\\\\n\");\\n              if (numResponses == addresses.length) {                response.end();\\n              }\\n            });          });\\n        }\\n      });    }\\n  }\\n};...\\nFigure 10.12 shows what happens when a GE T request is received by your app. The\\nserver that receives the request first perf orms a lookup of SRV records for the head-\\nless kubia  service and then sends a GET request to each of the pods backing the ser-\\nvice (even to itself, which obviously isn’t necessary, but I wanted to keep the code as\\nsimple as possible). It then returns a list of  all the nodes along with the data stored on\\neach of them.\\nThe container image containing this new vers ion of the app is av ailable at docker.io/\\nluksa/kubia-pet-peers .\\n10.4.2 Updating a StatefulSet\\nYour StatefulSet is already running, so let’s see how to update its pod template so the\\npods use the new image. You’ll also set th e replica count to 3 at the same time. ToEach pod \\npointed to by \\nan SRV record is \\nthen contacted to get its data.\\ncurlDNS\\n1.GET /\\n4.GET /data\\n5.GET /data2. SRV lookup\\n6. Return collated datakubia-0 kubia-1\\nkubia-23.GET /data\\nFigure 10.12 The operation of your simplistic distributed data store'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 334}, page_content='303 Discovering peers in a StatefulSet\\nupdate the StatefulSet, use the kubectl  edit  command (the patch  command would\\nbe another option):\\n$ kubectl edit statefulset kubia\\nThis opens the StatefulSet definition in yo ur default editor. In the definition, change\\nspec.replicas  to 3 and modify the spec.template.spec. containers.image  attri-\\nbute so it points to the new image ( luksa/kubia-pet-peers  instead of luksa/kubia-\\npet). Save the file and exit the editor to update the StatefulSet. Two replicas were\\nrunning previously, so you should no w see an additional replica called kubia-2  start-\\ning. List the pods to confirm:\\n$ kubectl get po\\nNAME      READY     STATUS              RESTARTS   AGE\\nkubia-0   1/1       Running             0          25mkubia-1   1/1       Running             0          26m\\nkubia-2   0/1       ContainerCreating   0          4s\\nThe new pod instance is running the new im age. But what about the existing two rep-\\nlicas? Judging from their age, they don’t seem  to have been updated. This is expected,\\nbecause initially, StatefulSets were more  like ReplicaSets and not like Deployments,\\nso they don’t perform a rollout when the template is modified. You need to delete\\nthe replicas manually and the StatefulSet will bring them up again based on the newtemplate:\\n$ kubectl delete po kubia-0 kubia-1\\npod \"kubia-0\" deleted\\npod \"kubia-1\" deleted\\nNOTE Starting from Kubernetes version 1.7, StatefulSets support rolling\\nupdates the same way Deployments and DaemonSets do. See the StatefulSet’s\\nspec.updateStrategy  field documentation using kubectl  explain  for more\\ninformation.\\n10.4.3 Trying out your clustered data store\\nOnce the two pods are up, you can see if yo ur shiny new Stone Age data store works as\\nexpected. Post a few requests to the clus ter, as shown in the following listing.\\n$ curl -X POST -d \"The sun is shining\" \\\\\\n➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/\\nData stored on pod kubia-1\\n$ curl -X POST -d \"The weather is sweet\" \\\\\\n➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/\\nData stored on pod kubia-0\\nNow, read the stored data, as shown in the following listing.Listing 10.10 Writing to the clustered data store through the service'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 335}, page_content=\"304 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\n$ curl localhost:8001/api/v1/namespaces/default/services\\n➥ /kubia-public/proxy/\\nYou've hit kubia-2\\nData stored on each cluster node:\\n- kubia-0.kubia.default.svc.cluster.local: The weather is sweet- kubia-1.kubia.default.svc.cluster.local: The sun is shining\\n- kubia-2.kubia.default.svc.cluster.local: No data posted yet\\nNice! When a client request reaches one of your cluster nodes, it discovers all its\\npeers, gathers data from them, and sends all the data back to the client. Even if you\\nscale the StatefulSet up or down, the pod se rvicing the client’s request can always find\\nall the peers running at that time. \\n The app itself isn’t that useful, but I hope you found it a fun way to show how\\ninstances of a replicated stateful app can discover their peers and handle horizontal\\nscaling with ease.\\n10.5 Understanding how StatefulSets deal with node \\nfailures\\nIn section 10.2.4 we stated that Kubernetes must be absolutely sure that a stateful\\npod is no longer running before creati ng its replacement. When a node fails\\nabruptly, Kubernetes can’t know the state of the node or its pods. It can’t know\\nwhether the pods are no longer running, or if they still are and are possibly even still\\nreachable, and it’s only the Kubelet that  has stopped reporting the node’s state to\\nthe master.\\n Because a StatefulSet guarantees that th ere will never be two pods running with\\nthe same identity and storage, when a node appears to have failed, the StatefulSet can-\\nnot and should not create a replacement pod until it knows for certain that the pod is\\nno longer running. \\n It can only know that when the cluster administrator tells it so. To do that, the\\nadmin needs to either delete the pod or dele te the whole node (doing so then deletes\\nall the pods scheduled to the node).\\n As your final exercise in this chapter, you’ll look at what happens to StatefulSets\\nand their pods when one of the cluster no des gets disconnected from the network.\\n10.5.1 Simulating a node’s disconnection from the network \\nAs in chapter 4, you’ll simulate the node  disconnecting from the network by shutting\\ndown the node’s eth0  network interface. Because th is example requires multiple\\nnodes, you can’t run it on Minikube. You’ ll use Google Kubernetes Engine instead.\\nSHUTTING  DOWN  THE NODE’S NETWORK  ADAPTER\\nTo shut down a node’s eth0  interface, you need to ssh into one of the nodes like this:\\n$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1Listing 10.11 Reading from the data store\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 336}, page_content='305 Understanding how StatefulSets deal with node failures\\nThen, inside the node, run the following command:\\n$ sudo ifconfig eth0 down\\nYour ssh session will stop working, so you’ll need  to open another terminal to continue.\\nCHECKING  THE NODE’S STATUS  AS SEEN BY THE KUBERNETES  MASTER\\nWith the node’s network interface down, the Kubelet running on the node can no\\nlonger contact the Kubernetes API server and let it know that the node and all its pods\\nare still running.\\n After a while, the control plane will mark the node as NotReady . You can see this\\nwhen listing nodes, as the following listing shows.\\n$ kubectl get node\\nNAME                                   STATUS     AGE       VERSION\\ngke-kubia-default-pool-32a2cac8-596v   Ready      16m       v1.6.2gke-kubia-default-pool-32a2cac8-m0g1   NotReady    16m       v1.6.2\\ngke-kubia-default-pool-32a2cac8-sgl7   Ready      16m       v1.6.2\\nBecause the control plane is no longer ge tting status updates from the node, the\\nstatus of all pods on that node is Unknown . This is shown in the pod list in the follow-\\ning listing.\\n$ kubectl get po\\nNAME      READY     STATUS    RESTARTS   AGE\\nkubia-0   1/1       Unknown   0          15m\\nkubia-1   1/1       Running   0          14mkubia-2   1/1       Running   0          13m\\nAs you can see, the kubia-0  pod’s status is no longer known because the pod was (and\\nstill is) running on the node whos e network interface you shut down.\\nUNDERSTANDING  WHAT HAPPENS  TO PODS WHOSE  STATUS  IS UNKNOWN\\nIf the node were to come back online and re port its and its pod statuses again, the pod\\nwould again be marked as Running . But if the pod’s status remains unknown for more\\nthan a few minutes (this time is configurab le), the pod is automatically evicted from\\nthe node. This is done by the master (the Kubernetes control plane). It evicts the pod\\nby deleting the pod resource. \\n When the Kubelet sees that the pod has been marked for deletion, it starts ter-\\nminating the pod. In your case, the Kubele t can no longer reach the master (because\\nyou disconnected the node from the network), which means the pod will keep\\nrunning.Listing 10.12 Observing a failed node’s status change to NotReady\\nListing 10.13 Observing the pod’s status change after its node becomes NotReady'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 337}, page_content='306 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\n Let’s examine the current situation. Use kubectl  describe  to display details about\\nthe kubia-0  pod, as shown in the following listing.\\n$ kubectl describe po kubia-0\\nName:        kubia-0\\nNamespace:   default\\nNode:        gke-kubia-default-pool-32a2cac8-m0g1/10.132.0.2...\\nStatus:      Terminating (expires Tue, 23 May 2017 15:06:09 +0200)\\nReason:      NodeLostMessage:     Node gke-kubia-default-pool-32a2cac8-m0g1 which was \\n             running pod kubia-0 is unresponsive\\nThe pod is shown as Terminating , with NodeLost  listed as the reason for the termina-\\ntion. The message says the node is cons idered lost because it’s unresponsive.\\nNOTE What’s shown here is the control plan e’s view of the world. In reality,\\nthe pod’s container is still running perf ectly fine. It isn’t terminating at all.\\n10.5.2 Deleting the pod manually\\nYou know the node isn’t coming back, but you need all three pods running to handle\\nclients properly. You need to get the kubia-0  pod rescheduled to a healthy node. As\\nmentioned earlier, you need to dele te the node or the pod manually. \\nDELETING  THE POD IN THE USUAL  WAY\\nDelete the pod the way you’ve always deleted pods:\\n$ kubectl delete po kubia-0\\npod \"kubia-0\" deleted\\nAll done, right? By deleting the pod, th e StatefulSet should immediately create a\\nreplacement pod, which will get scheduled to one of the remaining nodes. List the\\npods again to confirm: \\n$ kubectl get po\\nNAME      READY     STATUS    RESTARTS   AGEkubia-0   1/1       Unknown   0          15m\\nkubia-1   1/1       Running   0          14m\\nkubia-2   1/1       Running   0          13m\\nThat’s strange. You delete d the pod a moment ago and kubectl  said it had deleted it.\\nWhy is the same pod still there? \\nNOTE The kubia-0  pod in the listing isn’t a new pod with the same name—\\nthis is clear by looking at the AGE column. If it were new, its age would be\\nmerely a few seconds.Listing 10.14 Displaying details of the pod with the unknown status'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 338}, page_content='307 Summary\\nUNDERSTANDING  WHY THE POD ISN’T DELETED\\nThe pod was marked for deletion even before you deleted it. That’s because the con-\\ntrol plane itself already deleted it (i n order to evict it from the node). \\n If you look at listing 10.14 again, you’ll see that the pod’s status is Terminating .\\nThe pod was already marked for deletion earlier and will be removed as soon as the\\nKubelet on its node notifies the API server that the pod’s containers have terminated.\\nBecause the node’s network is down, this will never happen. \\nFORCIBLY  DELETING  THE POD\\nThe only thing you can do is tell the API server to delete the pod without waiting forthe Kubelet to confirm that the pod is no  longer running. You do that like this:\\n$ kubectl delete po kubia-0 --force --grace-period 0\\nwarning: Immediate deletion does not wait for confirmation that the running \\nresource has been terminated. The resource may continue to run on the cluster indefinitely.\\npod \"kubia-0\" deleted\\nYou need to use both the --force  and --grace-period  0 options. The warning dis-\\nplayed by kubectl  notifies you of what you did. If yo u list the pods again, you’ll finally\\nsee a new kubia-0  pod created:\\n$ kubectl get po\\nNAME          READY     STATUS              RESTARTS   AGEkubia-0       0/1       ContainerCreating   0          8s\\nkubia-1       1/1       Running             0          20m\\nkubia-2       1/1       Running             0          19m\\nWARNING Don’t delete stateful pods forcibly unless you know the node is no\\nlonger running or is unreachabl e (and will remain so forever). \\nBefore continuing, you may want to bring the node you disconnected back online.\\nYou can do that by restarting the node th rough the GCE web console or in a terminal\\nby issuing the following command:\\n$ gcloud compute instances reset <node name>\\n10.6 Summary\\nThis concludes the chapter on using StatefulSets to deploy stateful apps. This chapterhas shown you how to\\n\\uf0a1Give replicated pods individual storage\\n\\uf0a1Provide a stable identity to a pod\\n\\uf0a1Create a StatefulSet and a corresp onding headless governing Service\\n\\uf0a1Scale and update a StatefulSet\\n\\uf0a1Discover other members of the StatefulSet through DNS'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 339}, page_content='308 CHAPTER  10 StatefulSets: deploying replicated stateful applications\\n\\uf0a1Connect to other members through their host names\\n\\uf0a1Forcibly delete stateful pods\\nNow that you know the major building blocks  you can use to have Kubernetes run and\\nmanage your apps, we can look more closely at how it does that. In the next chapter,\\nyou’ll learn about the individual components that control the Kubernetes cluster and\\nkeep your apps running.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 340}, page_content='309Understanding\\nKubernetes internals\\nBy reading this book up to this point, yo u’ve become familiar with what Kubernetes\\nhas to offer and what it does. But so fa r, I’ve intentionally not spent much time\\nexplaining exactly how it does all this beca use, in my opinion, it makes no sense to\\ngo into details of how a system works until you have a good understanding of what\\nthe system does. That’s why we haven’t ta lked about exactly how a pod is scheduled\\nor how the various controllers running in side the Controller Manager make deployed\\nresources come to life. Because you now know  most resources that can be deployed in\\nKubernetes, it’s time to dive  into how they’re implemented.This chapter covers\\n\\uf0a1What components make up a Kubernetes cluster\\n\\uf0a1What each component does and how it does it\\n\\uf0a1How creating a Deployment object results in a \\nrunning pod\\n\\uf0a1What a running pod is\\n\\uf0a1How the network between pods works\\n\\uf0a1How Kubernetes Services work\\n\\uf0a1How high-availability is achieved'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 341}, page_content='310 CHAPTER  11 Understanding Kubernetes internals\\n11.1 Understanding the architecture\\nBefore you look at how Kubernetes does what it does, let’s take a closer look at the\\ncomponents that make up a Kubernetes clus ter. In chapter 1, you saw that a Kuberne-\\ntes cluster is split into two parts:\\n\\uf0a1The Kubernetes Control Plane\\n\\uf0a1The (worker) nodes\\nLet’s look more closely at what these two parts do and what’s running inside them.\\nCOMPONENTS  OF THE CONTROL  PLANE\\nThe Control Plane is what controls and ma kes the whole cluster function. To refresh\\nyour memory, the components that make up the Control Plane are\\n\\uf0a1The etcd distributed persistent storage\\n\\uf0a1The API server\\n\\uf0a1The Scheduler\\n\\uf0a1The Controller Manager\\nThese components store and manage the state of the cluster, but they aren’t what runs\\nthe application containers. \\nCOMPONENTS  RUNNING  ON THE WORKER  NODES\\nThe task of running your containers is  up to the components running on each\\nworker node:\\n\\uf0a1The Kubelet\\n\\uf0a1The Kubernetes Service Proxy (kube-proxy)\\n\\uf0a1The Container Runtime (Doc ker, rkt, or others)\\nADD-ON COMPONENTS\\nBeside the Control Plane components and the components running on the nodes, a\\nfew add-on components are required for th e cluster to provide everything discussed\\nso far. This includes\\n\\uf0a1The Kubernetes DNS server\\n\\uf0a1The Dashboard\\n\\uf0a1An Ingress controller\\n\\uf0a1Heapster, which we’ll talk about in chapter 14\\n\\uf0a1The Container Network Interface network plugin (we’ll explain it later in this\\nchapter)\\n11.1.1 The distributed nature of Kubernetes components\\nThe previously mentioned components all run as individual processes. The compo-\\nnents and their inter-dependencies are shown in figure 11.1.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 342}, page_content='311 Understanding the architecture\\nTo get all the features Kubern etes provides, all these components need to be running.\\nBut several can also perform useful work individually without the other components.\\nYou’ll see how as we examine each of them.\\nHOW THESE  COMPONENTS  COMMUNICATE\\nKubernetes system components communicat e only with the API server. They don’t\\ntalk to each other directly. The API server  is the only component that communicates\\nwith etcd. None of the other components communicate with etcd directly, but insteadmodify the cluster state by talking to the API server.\\n Connections between the API server and the other components are almost always\\ninitiated by the components, as shown in fi gure 11.1. But the API server does connect\\nto the Kubelet when you use \\nkubectl  to fetch logs, use kubectl  attach  to connect to\\na running container, or use the kubectl  port-forward  command.\\nNOTE The kubectl  attach  command is similar to kubectl  exec , but it attaches\\nto the main process running in the co ntainer instead of running an addi-\\ntional one.\\nRUNNING  MULTIPLE  INSTANCES  OF INDIVIDUAL  COMPONENTS\\nAlthough the components on the worker no des all need to run on the same node,\\nthe components of the Control Plane can easily be split across multiple servers. ThereChecking the status of the Control Plane components\\nThe API server exposes an API resource called ComponentStatus, which shows the\\nhealth status of each Control Plane component. You can list the components and\\ntheir statuses with kubectl :\\n$ kubectl get componentstatuses\\nNAME                 STATUS    MESSAGE              ERRORscheduler            Healthy   ok\\ncontroller-manager   Healthy   ok\\netcd-0               Healthy   {\"health\": \"true\"}Control Plane (master node) Worker node(s)\\netcd API serverkube-proxy\\nKubelet\\nSchedulerController\\nManager Controller\\nRuntimeFigure 11.1 Kubernetes components of the Control \\nPlane and the worker nodes'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 343}, page_content='312 CHAPTER  11 Understanding Kubernetes internals\\ncan be more than one instance of each Control Plane component running to ensure\\nhigh availability. While multiple instances of  etcd and API server can be active at the\\nsame time and do perform their jobs in para llel, only a single instance of the Sched-\\nuler and the Controller Manager may be ac tive at a given time—with the others in\\nstandby mode.\\nHOW COMPONENTS  ARE RUN\\nThe Control Plane components, as well as ku be-proxy, can either be deployed on the\\nsystem directly or they can run as pods (a s shown in listing 11.1). You may be surprised\\nto hear this, but it will all make sens e later when we talk about the Kubelet. \\n The Kubelet is the only component that always runs as a regular system compo-\\nnent, and it’s the Kubelet that then runs a ll the other components as pods. To run the\\nControl Plane components as pods, the Kubelet is also deployed on the master. The\\nnext listing shows pods in the kube-system  namespace in a cluster created with\\nkubeadm , which is explained in appendix B.\\n$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName \\n➥ --sort-by spec.nodeName -n kube-system\\nPOD                              NODE\\nkube-controller-manager-master   master      kube-dns-2334855451-37d9k        master      \\netcd-master                      master      \\nkube-apiserver-master            master      kube-scheduler-master            master      \\nkube-flannel-ds-tgj9k            node1      \\nkube-proxy-ny3xm                 node1      kube-flannel-ds-0eek8            node2      \\nkube-proxy-sp362                 node2      \\nkube-flannel-ds-r5yf4            node3      \\nkube-proxy-og9ac                 node3      \\nAs you can see in the listing, all the Cont rol Plane components are running as pods on\\nthe master node. There are three worker nodes, and each one runs the kube-proxy\\nand a Flannel pod, which provides the overlay network for the pods (we’ll talk aboutFlannel later). \\nTIP As shown in the listing, you can tell kubectl  to display custom columns\\nwith the -o custom-columns  option and sort the resource list with --sort-by .\\nNow, let’s look at each of the components up  close, starting with the lowest level com-\\nponent of the Control Plane—the persistent storage.\\n11.1.2 How Kubernetes uses etcd\\nAll the objects you’ve created throughout this book—Pods, ReplicationControllers,\\nServices, Secrets, and so on—need to be stored somewhere in a persistent manner so\\ntheir manifests survive API server restarts an d failures. For this, Kubernetes uses etcd,Listing 11.1 Kubernetes components running as pods\\netcd, API server, Scheduler, \\nController Manager, and \\nthe DNS server are running \\non the master.\\nThe three nodes each run \\na Kube Proxy pod and a \\nFlannel networking pod.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 344}, page_content='313 Understanding the architecture\\nwhich is a fast, distributed, and consistent  key-value store. Because it’s distributed,\\nyou can run more than one etcd instance to provide both high availability and bet-\\nter performance.\\n The only component that talks to etcd directly is the Kubernetes API server. All\\nother components read and write data to et cd indirectly through the API server. This\\nbrings a few benefits, among them a more ro bust optimistic locking system as well as\\nvalidation; and, by abstracting away the ac tual storage mechanism from all the other\\ncomponents, it’s much simpler to replace it  in the future. It’s worth emphasizing that\\netcd is the only place Kubernetes stores cluster state and metadata.\\nHOW RESOURCES  ARE STORED  IN ETCD\\nAs I’m writing this, Kubernetes can use either etcd version 2 or version 3, but version 3\\nis now recommended because of improved perf ormance. etcd v2 stores keys in a hier-\\narchical key space, which makes key-value pa irs similar to files in a file system. Each\\nkey in etcd is either a directory, which contains other keys, or is a regular key with acorresponding value. etcd v3 doesn’t support directories, but because the key format\\nremains the same (keys can include slashes) , you can still think of them as being\\ngrouped into directories. Kubernetes stores all its data in etcd under /registry. The\\nfollowing listing shows a list of  keys stored under /registry.\\n$ etcdctl ls /registry\\n/registry/configmaps\\n/registry/daemonsets\\n/registry/deployments/registry/events\\n/registry/namespaces\\n/registry/pods...About optimistic concurrency control\\nOptimistic concurrency control (sometimes referred to as optimistic locking) is a\\nmethod where instead of locking a piece of da ta and preventing it from being read or\\nupdated while the lock is in place, the piece of data includes a version number. Every\\ntime the data is updated, the version numb er increases. When updating the data, the\\nversion number is checked to see if it has increased between the time the client read\\nthe data and the time it submits the updat e. If this happens, the update is rejected\\nand the client must re-read the new  data and try to update it again. \\nThe result is that when two clients try to  update the same data entry, only the first\\none succeeds.\\nAll Kubernetes resources include a metadata.resourceVersion  field, which clients\\nneed to pass back to the API server when updating an object. If the version doesn’t\\nmatch the one stored in etcd, the API server rejects the update.\\nListing 11.2 Top-level entries stored in etcd by Kubernetes'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 345}, page_content='314 CHAPTER  11 Understanding Kubernetes internals\\nYou’ll recognize that these keys correspond to the resource types you learned about in\\nthe previous chapters. \\nNOTE If you’re using v3 of the etcd API, you can’t use the ls command to see\\nthe contents of a directory. Instead, you can list all keys that start with a given\\nprefix with etcdctl  get /registry  --prefix=true .\\nThe following listing shows the conten ts of the /registry/pods directory.\\n$ etcdctl ls /registry/pods\\n/registry/pods/default\\n/registry/pods/kube-system\\nAs you can infer from the names, these two entries correspond to the default  and the\\nkube-system  namespaces, which means pods are stored per namespace. The follow-\\ning listing shows the entries in th e /registry/pods/default directory.\\n$ etcdctl ls /registry/pods/default\\n/registry/pods/default/kubia-159041347-xk0vc\\n/registry/pods/default/kubia-159041347-wt6ga/registry/pods/default/kubia-159041347-hp2o5\\nEach entry corresponds to an individual pod. These aren’t directories, but key-value\\nentries. The following listing show s what’s stored in one of them.\\n$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga\\n{\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\" :{\"name\":\"kubia-159041347-wt6ga\",\\n\"generateName\":\"kubia-159041347-\",\"namespa ce\":\"default\",\"selfLink\":...\\nYou’ll recognize that this is nothing other than a pod definition in JSON format. The\\nAPI server stores the complete JSON repres entation of a resource in etcd. Because of\\netcd’s hierarchical key space, you can think of all the stored resources as JSON files in\\na filesystem. Simple, right?\\nWARNING Prior to Kubernetes version 1.7, the JSON manifest of a Secret\\nresource was also stored like this (it wasn’t encrypted). If someone got direct\\naccess to etcd, they knew all your Se crets. From version 1.7, Secrets are\\nencrypted and thus stored much more securely.\\nENSURING  THE CONSISTENCY  AND VALIDITY  OF STORED  OBJECTS\\nRemember Google’s Borg and Omega syst ems mentioned in chapter 1, which are\\nwhat Kubernetes is based on? Like Kubernet es, Omega also uses a centralized store to\\nhold the state of the cluster, but in co ntrast, multiple Control Plane components\\naccess the store directly. All these components  need to make sure they all adhere toListing 11.3 Keys in the /registry/pods directory\\nListing 11.4 etcd entries for pods in the default  namespace\\nListing 11.5 An etcd entry representing a pod'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 346}, page_content='315 Understanding the architecture\\nthe same optimistic locking mechanism to handle conflicts properly. A single compo-\\nnent not adhering fully to the mechan ism may lead to inconsistent data. \\n Kubernetes improves this by requiring all other Control Plane components to go\\nthrough the API server. This way updates to th e cluster state are always consistent, because\\nthe optimistic locking mechanism is implemente d in a single place, so less chance exists,\\nif any, of error. The API server also makes su re that the data written to the store is always\\nvalid and that changes to the data are only performed by authorized clients. \\nENSURING  CONSISTENCY  WHEN ETCD IS CLUSTERED\\nFor ensuring high availability, you’ll usuall y run more than a single instance of etcd.\\nMultiple etcd instances will need to rema in consistent. Such a distributed system\\nneeds to reach a consensus on what the actual state is. etcd uses the RAFT consensus\\nalgorithm to achieve this, which ensures that at any given moment, each node’s state is\\neither what the majority of the nodes agrees is the current state or is one of the previ-ously agreed upon states. \\n Clients connecting to different nodes of an etcd cluster will either see the actual\\ncurrent state or one of the states from the past (in Kubernetes, the only etcd client is\\nthe API server, but there may be multiple instances). \\n The consensus algorithm requires a majori ty (or quorum) for the cluster to progress\\nto the next state. As a result, if the cluste r splits into two disconnected groups of nodes,\\nthe state in the two groups can never diverge, because to transition from the previous\\nstate to the new one, there needs to be mo re than half of the nodes taking part in\\nthe state change. If one group contains the majority of all nodes, the other one obvi-\\nously doesn’t. The first group can modify th e cluster state, whereas the other one can’t.\\nWhen the two groups reconnect, the second  group can catch up with the state in the\\nfirst group (see figure 11.2).\\nClients(s) Clients(s) Clients(s)etcd-0\\netcd-1 etcd-2The nodes know\\nthere are three nodesin the etcd cluster.\\netcd-0\\netcd-1These two nodes knowthey still have quorumand can accept statechanges from clients.\\netcd-2This node knows it doesnot have quorum andshould therefore notallow state changes.\\nNetwork\\nsplit\\nFigure 11.2 In a split-brain scenario, only the side which still has the majority (quorum) accepts \\nstate changes.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 347}, page_content='316 CHAPTER  11 Understanding Kubernetes internals\\nWHY THE NUMBER  OF ETCD INSTANCES  SHOULD  BE AN ODD NUMBER\\netcd is usually deployed with an odd number  of instances. I’m sure you’d like to know\\nwhy. Let’s compare having two vs. having one instance. Having two instances requires\\nboth instances to be present to have a majority . If either of them fails, the etcd cluster\\ncan’t transition to a new state because no ma jority exists. Having two instances is worse\\nthan having only a single instance. By havi ng two, the chance of the whole cluster fail-\\ning has increased by 100%, compared to that of a single-node cluster failing. \\n The same applies when comparing three vs. four etcd instances. With three instances,\\none instance can fail and a majority (of two) still exists. With four instances, you need\\nthree nodes for a majority (two aren’t enou gh). In both three- and four-instance clus-\\nters, only a single instance may fail. But wh en running four instances, if one fails, a\\nhigher possibility exists of an additional instance of the three remaining instances fail-\\ning (compared to a three-node cluster with  one failed node and two remaining nodes).\\n Usually, for large clusters, an etcd cluster of  five or seven nodes is sufficient. It can\\nhandle a two- or a three-node failure, respecti vely, which suffices in almost all situations. \\n11.1.3 What the API server does\\nThe Kubernetes API server is the centra l component used by all other components\\nand by clients, such as kubectl . It provides a CRUD (Cre ate, Read, Update, Delete)\\ninterface for querying and modifying the cluster state over a RESTful API. It stores\\nthat state in etcd.\\n In addition to providing a consistent way of storing objects in etcd, it also performs\\nvalidation of those objects, so clients can’ t store improperly configured objects (which\\nthey could if they were writing to the store directly). Along with validation, it also han-\\ndles optimistic locking, so changes to an object are never overridden by other clients\\nin the event of concurrent updates.\\n One of the API server’s clients is the command-line tool kubectl  you’ve been\\nusing from the beginning of the book. When creating a resource from a JSON file, for\\nexample, kubectl  posts the file’s contents to th e API server through an HTTP POST\\nrequest. Figure 11.3 shows what happens insi de the API server when it receives the\\nrequest. This is explained in more detail in the next few paragraphs.\\nAPI server\\netcdAuthentication\\nplugin 1\\nAuthentication\\nplugin 2\\nAuthentication\\nplugin 3Client\\n()kubectlHTTP POST\\nrequestAuthorization\\nplugin 1\\nAuthorization\\nplugin 2\\nAuthorization\\nplugin 3Admission\\ncontrol plugin 1\\nAdmission\\ncontrol plugin 2\\nAdmission\\ncontrol plugin 3Resource\\nvalidation\\nFigure 11.3 The operation of the API server'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 348}, page_content='317 Understanding the architecture\\nAUTHENTICATING  THE CLIENT  WITH AUTHENTICATION  PLUGINS\\nFirst, the API server needs to authenticate the client sending the request. This is per-\\nformed by one or more authentication plug ins configured in the API server. The API\\nserver calls these plugins in turn, until on e of them determines who is sending the\\nrequest. It does this by inspecting the HTTP request. \\n Depending on the authentication method, the user can be extracted from the cli-\\nent’s certificate or an HTTP header, such as Authorization , which you used in chap-\\nter 8. The plugin extracts the client’s user name, user ID, and groups the user belongs\\nto. This data is then used in the next stage, which is authorization.\\nAUTHORIZING  THE CLIENT  WITH AUTHORIZATION  PLUGINS\\nBesides authentication plugins, the API server is also configured to use one or more\\nauthorization plugins. Their job is to de termine whether the authenticated user can\\nperform the requested action on the reques ted resource. For example, when creating\\npods, the API server consults all authorizat ion plugins in turn, to determine whether\\nthe user can create pods in the requested namespace. As soon as a plugin says the usercan perform the action, the API serv er progresses to the next stage.\\nVALIDATING  AND/OR MODIFYING  THE RESOURCE  IN THE REQUEST  WITH ADMISSION  CONTROL  PLUGINS\\nIf the request is trying to create, modify, or delete a resource, the request is sent\\nthrough Admission Control. Again, the serv er is configured with multiple Admission\\nControl plugins. These plugins can modify the resource for different reasons. Theymay initialize fields missing from the resour ce specification to the configured default\\nvalues or even override them. They may ev en modify other related resources, which\\naren’t in the request, and can also reject  a request for whatever reason. The resource\\npasses through all Admission Control plugins.\\nNOTE When the request is only trying to read data, the request doesn’t go\\nthrough the Admission Control.\\nExamples of Admission Control plugins include\\n\\uf0a1AlwaysPullImages —Overrides the pod’s imagePullPolicy  to Always , forcing\\nthe image to be pulled every time the pod is deployed.\\n\\uf0a1ServiceAccount —Applies the default service account to pods that don’t specify\\nit explicitly.\\n\\uf0a1NamespaceLifecycle —Prevents creation of pods in namespaces that are in the\\nprocess of being deleted, as well  as in non-existing namespaces.\\n\\uf0a1ResourceQuota —Ensures pods in a certain namespace only use as much CPU\\nand memory as has been allotted to the namespace. We’ll learn more about this\\nin chapter 14.\\nYou’ll find a list of additional Admission Control plugins in the Kubernetes documen-\\ntation at https:/ /kubernetes.io/docs/admin/admission-controllers/ .'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 349}, page_content='318 CHAPTER  11 Understanding Kubernetes internals\\nVALIDATING  THE RESOURCE  AND STORING  IT PERSISTENTLY\\nAfter letting the request pass through all th e Admission Control plugins, the API server\\nthen validates the object, stores it in et cd, and returns a response to the client.\\n11.1.4 Understanding how the API server notifies clients of resource \\nchanges\\nThe API server doesn’t do anything else ex cept what we’ve discussed. For example, it\\ndoesn’t create pods when you create a Rep licaSet resource and it doesn’t manage the\\nendpoints of a service. That’s what co ntrollers in the Controller Manager do. \\n But the API server doesn’t even tell these controllers what to do. All it does is\\nenable those controllers and other comp onents to observe changes to deployed\\nresources. A Control Plane component can request to be notified when a resource is\\ncreated, modified, or deleted. This enab les the component to perform whatever task\\nit needs in response to a ch ange of the cluster metadata.\\n Clients watch for changes by opening an HTTP connection to the API server.\\nThrough this connection, the client will then  receive a stream of modifications to the\\nwatched objects. Every time an object is updated, the server sends the new version of\\nthe object to all connected clients watching  the object. Figure 11.4 shows how clients\\ncan watch for changes to pods and how a change to one of the pods is stored into etcd\\nand then relayed to all clients watching pods at that moment.\\nOne of the API server’s clients is the kubectl  tool, which also supports watching\\nresources. For example, when deploying a pod,  you don’t need to constantly poll the list\\nof pods by repeatedly executing kubectl  get pods . Instead, you can use the --watch\\nflag and be notified of each creation, modi fication, or deletion of a pod, as shown in\\nthe following listing.\\n$ kubectl get pods --watch\\nNAME                    READY     STATUS              RESTARTS   AGEListing 11.6 Watching a pod being created and then deletedVarious\\nclientskubectl\\nAPI server1.GET /.../pods? watch=true2.POST /.../pods/pod-xyz\\n5. Send updated object\\nto all watchers3. Update object\\nin etcd\\n4. Modiﬁcation\\nnotiﬁcationetcd\\nFigure 11.4 When an object is updated, the API serv er sends the updated object to all interested \\nwatchers.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 350}, page_content='319 Understanding the architecture\\nkubia-159041347-14j3i   0/1       Pending             0          0s\\nkubia-159041347-14j3i   0/1       Pending             0          0s\\nkubia-159041347-14j3i   0/1       ContainerCreating   0          1skubia-159041347-14j3i   0/1       Running             0          3s\\nkubia-159041347-14j3i   1/1       Running             0          5s\\nkubia-159041347-14j3i   1/1       Terminating         0          9skubia-159041347-14j3i   0/1       Terminating         0          17s\\nkubia-159041347-14j3i   0/1       Terminating         0          17s\\nkubia-159041347-14j3i   0/1       Terminating         0          17s\\nYou can even have kubectl  print out the whole YAML on each watch event like this:\\n$ kubectl get pods -o yaml --watch\\nThe watch mechanism is also used by the Scheduler, which is the next Control Plane\\ncomponent you’re going to learn more about.\\n11.1.5 Understanding the Scheduler\\nYou’ve already learned that you don’t usually specify which cluster node a pod shouldrun on. This is left to the Scheduler. From  afar, the operation of the Scheduler looks\\nsimple. All it does is wait for newly create d pods through the API server’s watch mech-\\nanism and assign a node to each new pod that doesn’t already have the node set. \\n The Scheduler doesn’t instruct the select ed node (or the Kubelet running on that\\nnode) to run the pod. All the Scheduler does  is update the pod definition through the\\nAPI server. The API server then notifies the Kubelet (again, through the watch mech-\\nanism described previously) that the pod has been scheduled. As soon as the Kubelet\\non the target node sees the pod has been sc heduled to its node, it creates and runs the\\npod’s containers.\\n Although a coarse-grained view of the scheduling process seems trivial, the actual\\ntask of selecting the best node for the pod isn’t that simple. Sure, the simplestScheduler could pick a random node and no t care about the pods already running on\\nthat node. On the other side of the spectr um, the Scheduler could use advanced tech-\\nniques such as machine learning to antici pate what kind of pods are about to be\\nscheduled in the next minutes or hours and schedule pods to maximize future hard-\\nware utilization without requiring any rescheduling of existing pods. Kubernetes’\\ndefault Scheduler falls somewhere in between. \\nUNDERSTANDING  THE DEFAULT  SCHEDULING  ALGORITHM\\nThe selection of a node can be broken down into two parts, as shown in figure 11.5:\\n\\uf0a1Filtering the list of all nodes to obtain a list of acceptable nodes the pod can be\\nscheduled to.\\n\\uf0a1Prioritizing the acceptable nodes and choo sing the best one. If multiple nodes\\nhave the highest score, round-robin is us ed to ensure pods are deployed across\\nall of them evenly.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 351}, page_content='320 CHAPTER  11 Understanding Kubernetes internals\\nFINDING  ACCEPTABLE  NODES\\nTo determine which nodes are acceptable for the pod, the Scheduler passes each\\nnode through a list of configured predic ate functions. These check various things\\nsuch as\\n\\uf0a1Can the node fulfill the pod’s requests fo r hardware resources? You’ll learn how\\nto specify them in chapter 14.\\n\\uf0a1Is the node running out of resources (is it reporting a memory or a disk pres-sure condition)? \\n\\uf0a1If the pod requests to be scheduled to a sp ecific node (by name), is this the node?\\n\\uf0a1Does the node have a label that matches the node selector in the pod specifica-\\ntion (if one is defined)?\\n\\uf0a1If the pod requests to be bound to a spec ific host port (discussed in chapter 13),\\nis that port already taken on this node or not? \\n\\uf0a1If the pod requests a certain type of volume, can this volume be mounted forthis pod on this node, or is another pod on the node al ready using the same\\nvolume?\\n\\uf0a1Does the pod tolerate the taints of the node? Taints and tolerations are explained\\nin chapter 16.\\n\\uf0a1Does the pod specify node and/or pod affinity or anti-affinity rules? If yes,would scheduling the pod to this node break those rules? This is also explained\\nin chapter 16.\\nAll these checks must pass for the node to be  eligible to host the pod. After perform-\\ning these checks on every node, the Scheduler  ends up with a subset of the nodes. Any\\nof these nodes could run the pod, because they have enough available resources for\\nthe pod and conform to all requirements you’ve specified in the pod definition.\\nSELECTING  THE BEST NODE FOR THE POD\\nEven though all these nodes are acceptable and can run the pod, several may be a\\nbetter choice than others. Suppose you have  a two-node cluster. Both nodes are eli-\\ngible, but one is already running 10 pods, while the other, for whatever reason, isn’t\\nrunning any pods right now. It’s obvious the Scheduler should favor the second\\nnode in this case. Node 1 Node 2\\nNode 3 Node 4\\nNode 5 ...Find acceptable\\nnodesNode 1 Node 2\\nNode 3 Node 4\\nNode 5 ...Prioritize nodes\\nand select the\\ntop one\\nNode 3\\nNode 1\\nNode 4\\nFigure 11.5 The Scheduler finds acceptable nodes for a pod and then selects the best node \\nfor the pod.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 352}, page_content='321 Understanding the architecture\\n Or is it? If these two node s are provided by the cloud infrastructure, it may be bet-\\nter to schedule the pod to the first node an d relinquish the second node back to the\\ncloud provider to save money. \\nADVANCED  SCHEDULING  OF PODS\\nConsider another example. Imagine having mu ltiple replicas of a pod. Ideally, you’d\\nwant them spread across as many nodes as possible instead of having them all sched-\\nuled to a single one. Failure of that no de would cause the service backed by those\\npods to become unavailable. But if the pods  were spread across different nodes, a sin-\\ngle node failure would barely leave a dent in the service’s capacity. \\n Pods belonging to the same Service or Re plicaSet are spread across multiple nodes\\nby default. It’s not guaranteed  that this is always the case. But you can force pods to be\\nspread around the cluster or kept close to gether by defining pod affinity and anti-\\naffinity rules, which are explained in chapter 16. \\n Even these two simple cases show ho w complex scheduling can be, because it\\ndepends on a multitude of factors. Because of  this, the Scheduler can either be config-\\nured to suit your specific needs or infrastructure specifics, or it can even be replaced\\nwith a custom implementation altogether. You could also run a Kubernetes cluster\\nwithout a Scheduler, but then you’d have  to perform the scheduling manually.\\nUSING MULTIPLE  SCHEDULERS\\nInstead of running a single Scheduler in the cluster, you can run multiple Schedulers.\\nThen, for each pod, you specify the Schedul er that should schedule this particular\\npod by setting the schedulerName  property in the pod spec.\\n Pods without this property set are sche duled using the default Scheduler, and so\\nare pods with schedulerName  set to default-scheduler . All other pods are ignored by\\nthe default Scheduler, so they need to be scheduled either manually or by another\\nScheduler watching for such pods. \\n You can implement your own Schedulers and deploy them in the cluster, or you\\ncan deploy an additional instance of Kube rnetes’ Scheduler with different configura-\\ntion options.\\n11.1.6 Introducing the controllers running in the Controller Manager\\nAs previously mentioned, the API server do esn’t do anything except store resources in\\netcd and notify clients about the change. The Scheduler only assigns a node to thepod, so you need other active  components to make sure th e actual state of the system\\nconverges toward the desired state, as spec ified in the resources deployed through the\\nAPI server. This work is done by controll ers running inside the Controller Manager. \\n The single Controller Manager process cu rrently combines a multitude of control-\\nlers performing various reconciliation tasks.  Eventually those controllers will be split\\nup into separate processes, enabling you to replace each one with a custom imple-\\nmentation if necessary. The list of  these controllers includes the\\n\\uf0a1Replication Manager (a controller for ReplicationController resources)\\n\\uf0a1ReplicaSet, DaemonSet, and Job controllers'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 353}, page_content='322 CHAPTER  11 Understanding Kubernetes internals\\n\\uf0a1Deployment controller\\n\\uf0a1StatefulSet controller\\n\\uf0a1Node controller\\n\\uf0a1Service controller\\n\\uf0a1Endpoints controller\\n\\uf0a1Namespace controller\\n\\uf0a1PersistentVolume controller\\n\\uf0a1Others\\nWhat each of these controllers does should  be evident from its name. From the list,\\nyou can tell there’s a controller for almost  every resource you can create. Resources\\nare descriptions of what should be running in the cluster, wherea s the controllers are\\nthe active Kubernetes components that perfor m actual work as a result of the deployed\\nresources.\\nUNDERSTANDING  WHAT CONTROLLERS  DO AND HOW THEY DO IT\\nControllers do many different things, but th ey all watch the API server for changes to\\nresources (Deployments, Services, and so on ) and perform operations for each change,\\nwhether it’s a creation of a new object or an  update or deletion of an existing object.\\nMost of the time, these operations include  creating other resources or updating the\\nwatched resources themselves (to update the object’s status , for example).\\n In general, controllers run a reconciliati on loop, which reconciles the actual state\\nwith the desired state (specified in the resource’s spec  section) and writes the new\\nactual state to the resource’s status  section. Controllers use the watch mechanism to\\nbe notified of changes, but because usin g watches doesn’t guarantee the controller\\nwon’t miss an event, they also perform a re -list operation periodically to make sure\\nthey haven’t missed anything.\\n Controllers never talk to each other dire ctly. They don’t even know any other con-\\ntrollers exist. Each controller connects to the API server and, through the watch\\nmechanism described in section 11.1.3, asks to be notified when a change occurs in\\nthe list of resources of any type th e controller is responsible for. \\n We’ll briefly look at what each of the controllers does, but if you’d like an in-depth\\nview of what they do, I suggest you look at their source code directly. The sidebar\\nexplains how to get started.\\nA few pointers on exploring the controllers’ source code\\nIf you’re interested in seeing exactly how these controllers operate, I strongly encour-\\nage you to browse through their source code. To make it easier, here are a few tips:\\nThe source code for the controllers is available at https:/ /github.com/kubernetes/\\nkubernetes/blob/master/pkg/controller .\\nEach controller usually has a constructor in which it creates an Informer , which is\\nbasically a listener that gets called every time an API object gets updated. Usually,'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 354}, page_content='323 Understanding the architecture\\nTHE REPLICATION  MANAGER\\nThe controller that makes Rep licationController resources co me to life is called the\\nReplication Manager. We talked about how ReplicationControllers work in chapter 4.\\nIt’s not the ReplicationControllers that do the actual work, but the Replication Man-\\nager. Let’s quickly review what the controller does, because this will help you under-stand the rest of the controllers.\\n I n  c h a p t e r  4 ,  w e  s a i d  t h a t  t h e  o p e r ation of a ReplicationController could be\\nthought of as an infinite loop, where in ea ch iteration, the controller finds the num-\\nber of pods matching its pod selector and compares the number to the desired replica\\ncount. \\n Now that you know how the API server  can notify clients through the watch\\nmechanism, it’s clear that the controller does n’t poll the pods in every iteration, but\\nis instead notified by the watch mechanis m of each change that may affect the\\ndesired replica count or the number of matched pods (see figure 11.6). Any such\\nchanges trigger the controller to recheck th e desired vs. actual replica count and act\\naccordingly.\\n You already know that when too few pod instances are running, the Replication-\\nController runs additional instances. But it doesn’t actually run them itself. It createsan Informer listens for changes to a specif ic type of resource. Looking at the con-\\nstructor will show you which resources the controller is watching.\\nNext, go look for the worker()  method. In it, you’ll find t he method that gets invoked\\neach time the controller needs to do somethin g. The actual function is often stored\\nin a field called syncHandler  or something similar. This field is also initialized in the\\nconstructor, so that’s where you’ll find t he name of the function that gets called. That\\nfunction is the place where all the magic happens.\\nController Manager\\nWatches\\nCreates and\\ndeletesReplication\\nManagerAPI server\\nReplicationController\\nresources\\nPod resources\\nOther resources\\nFigure 11.6 The Replication Manager watches for changes to API \\nobjects.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 355}, page_content='324 CHAPTER  11 Understanding Kubernetes internals\\nnew Pod manifests, posts them to the AP I server, and lets the Scheduler and the\\nKubelet do their job of sc heduling and running the pod.\\n The Replication Manager performs its work by manipulating Pod API objects\\nthrough the API server. This is  how all controllers operate.\\nTHE REPLICA SET, THE DAEMON SET, AND THE JOB CONTROLLERS\\nThe ReplicaSet controller does almost th e same thing as the Replication Manager\\ndescribed previously, so we don’t have much to add here. The DaemonSet and Job\\ncontrollers are similar. They create Pod resources from the pod template defined in\\ntheir respective resources. Like the Replic ation Manager, these controllers don’t run\\nthe pods, but post Pod definitions to the API server, letting the Kubelet create theircontainers and run them.\\nTHE DEPLOYMENT  CONTROLLER\\nThe Deployment controller takes care of ke eping the actual state of a deployment in\\nsync with the desired state specified in the corresponding Deployment API object. \\n The Deployment controller performs a rollout of a new version each time a\\nDeployment object is modified (if the modi fication should affect the deployed pods).\\nIt does this by creating a ReplicaSet and then appropriately scaling both the old and\\nthe new ReplicaSet based on the strategy specified in the Deployment, until all the old\\npods have been replaced with new ones. It doesn’t create any pods directly.\\nTHE STATEFUL SET CONTROLLER\\nThe StatefulSet controller, similarly to th e ReplicaSet controller and other related\\ncontrollers, creates, manages, and deletes Po ds according to the spec of a StatefulSet\\nresource. But while those other controllers only manage Pods, the StatefulSet control-\\nler also instantiates and manages Persis tentVolumeClaims for each Pod instance.\\nTHE NODE CONTROLLER\\nThe Node controller manages the Node reso urces, which describe the cluster’s worker\\nnodes. Among other things, a Node controller keeps the list of Node objects in syncwith the actual list of machines running in the cluster. It also monitors each node’s\\nhealth and evicts pods from unreachable nodes.\\n The Node controller isn’t the only comp onent making changes to Node objects.\\nThey’re also changed by the Kubelet, and can obviously also be modified by users\\nthrough REST API calls. \\nTHE SERVICE  CONTROLLER\\nIn chapter 5, when we talked about Services, you learned that a few different typesexist. One of them was the \\nLoadBalancer  service, which requests a load balancer from\\nthe infrastructure to make the service avai lable externally. The Service controller is\\nthe one requesting and releasing a load ba lancer from the infrastructure, when a\\nLoadBalancer -type Service is created or deleted.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 356}, page_content='325 Understanding the architecture\\nTHE ENDPOINTS  CONTROLLER\\nYou’ll remember that Services aren’t linked directly to pods, but instead contain a list\\nof endpoints (IPs and ports), which is crea ted and updated either manually or auto-\\nmatically according to the pod selector defined on the Service. The Endpoints con-\\ntroller is the active component that keep s the endpoint list constantly updated with\\nthe IPs and ports of pods ma tching the label selector.\\n As figure 11.7 shows, the controller  watches both Services and Pods. When\\nServices are added or updated or Pods are a dded, updated, or deleted, it selects Pods\\nmatching the Service’s pod selector and adds their IPs and ports to the Endpoints\\nresource. Remember, the Endpoints object is  a standalone object, so the controller\\ncreates it if necessary. Likewise, it also de letes the Endpoints object when the Service is\\ndeleted.\\nTHE NAMESPACE  CONTROLLER\\nRemember namespaces (we talked about th em in chapter 3)? Most resources belong\\nto a specific namespace. When a Namespace resource is deleted, all the resources in\\nthat namespace must also be  deleted. This is what th e Namespace controller does.\\nWhen it’s notified of the deletion of a Na mespace object, it deletes all the resources\\nbelonging to the namespace through the API server . \\nTHE PERSISTENT VOLUME  CONTROLLER\\nIn chapter 6 you learned about PersistentVolumes and PersistentVolumeClaims.Once a user creates a PersistentVolumeClai m, Kubernetes must find an appropriate\\nPersistentVolume and bind it to the claim.  This is performed by the PersistentVolume\\ncontroller. \\n When a PersistentVolumeClaim pops up, the controller finds the best match for\\nthe claim by selecting the smallest PersistentVolume with the access mode matchingthe one requested in the claim and the declared capacity above the capacity requestedController Manager\\nWatches\\nCreates, modiﬁes,\\nand deletesEndpoints\\ncontrollerAPI server\\nService resources\\nPod resources\\nEndpoints resources\\nFigure 11.7 The Endpoints controller watches Service and  Pod resources, \\nand manages Endpoints.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 357}, page_content='326 CHAPTER  11 Understanding Kubernetes internals\\nin the claim. It does this by keeping an ordered list of PersistentVolumes for each\\naccess mode by ascending capacity and re turning the first volume from the list.\\n Then, when the user deletes the Persis tentVolumeClaim, the volume is unbound\\nand reclaimed according to the volume’s reclai m policy (left as is, deleted, or emptied).\\nCONTROLLER  WRAP -UP\\nYou should now have a good feel for what each controller does and how controllers\\nwork in general. Again, all these controll ers operate on the API objects through the\\nAPI server. They don’t communicate with th e Kubelets directly or issue any kind of\\ninstructions to them. In fact, they don’t even know Kubelets exist. After a controller\\nupdates a resource in the API server, the Kubelets and Kubernetes Service Proxies,\\nalso oblivious of the controlle rs’ existence, perform their work, such as spinning up a\\npod’s containers and attaching network storage to them, or in the case of services, set-\\nting up the actual load balancing across pods. \\n The Control Plane handles one part of the operation of the whole system, so to\\nfully understand how things unfold in a Ku bernetes cluster, you also need to under-\\nstand what the Kubelet and the Kubernetes Service Proxy do. We’ll learn that next.\\n11.1.7 What the Kubelet does\\nIn contrast to all the controllers, which ar e part of the Kubernetes Control Plane and\\nrun on the master node(s), the Kubelet an d the Service Proxy both run on the worker\\nnodes, where the actual pods containers run. What does the Kubelet do exactly?\\nUNDERSTANDING  THE KUBELET ’S JOB\\nIn a nutshell, the Kubelet is the componen t responsible for everything running on a\\nworker node. Its initial job is to register the node it’s running on by creating a Node\\nresource in the API server. Then it needs to continuously monitor the API server for\\nPods that have been scheduled to the node, and start the pod’s containers. It does this\\nby telling the configured container runtime (which is Docker, CoreOS’ rkt, or some-\\nthing else) to run a container from a specif ic container image. The Kubelet then con-\\nstantly monitors running containers and reports their status, events, and resource\\nconsumption to the API server. \\n The Kubelet is also the component that runs the container liveness probes, restart-\\ning containers when the probes fail. Lastly, it terminates containers when their Pod is\\ndeleted from the API server and notifies the server that the pod has terminated.\\nRUNNING  STATIC  PODS WITHOUT  THE API SERVER\\nAlthough the Kubelet talks to the Kubernet es API server and gets the pod manifests\\nfrom there, it can also run pods based on pod manifest files in a specific local direc-\\ntory as shown in figure 11.8. This feature is used to run the containerized versions of\\nthe Control Plane components as pods, as you saw in the beginning of the chapter.\\n Instead of running Kubernetes system co mponents natively, you can put their pod\\nmanifests into the Kubelet’s manifest directory and have the Kubelet run and manage'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 358}, page_content='327 Understanding the architecture\\nthem. You can also use the same method to  run your custom system containers, but\\ndoing it through a DaemonSet is the recommended method.\\n11.1.8 The role of the Kubernetes Service Proxy\\nBeside the Kubelet, every worker node also  runs the kube-proxy, whose purpose is to\\nmake sure clients can connect to the servic es you define through the Kubernetes API.\\nThe kube-proxy makes sure connections to the service IP and port end up at one of\\nthe pods backing that service (or other, non-pod service endpoints). When a service is\\nbacked by more than one pod, the proxy performs load balancing across those pods. \\nWHY IT’S CALLED  A PROXY\\nThe initial implementation of the kube-proxy was the userspace  proxy. It used an\\nactual server process to accept connectio ns and proxy them to the pods. To inter-\\ncept connections destined to the service IPs, the proxy configured iptables  rules\\n(iptables  is the tool for managing the Linux kernel’s packet filtering features) to\\nredirect the connections to the pr oxy server. A rough diagram of the userspace  proxy\\nmode is shown in figure 11.9.Container Runtime\\n(Docker, rkt, ...)KubeletAPI server Worker node\\nRuns, monitors,\\nand manages\\ncontainersPod resource\\nContainer A\\nContainer BContainer A\\nContainer B\\nContainer CPod manifest (ﬁle)\\nLocal manifest directoryContainer C\\nFigure 11.8 The Kubelet runs pods based on pod specs fr om the API server and a local file directory.\\nClient kube-proxyConﬁgures : iptables\\nredirect through proxy server\\niptables Pod\\nFigure 11.9 The userspace  proxy mode'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 359}, page_content='328 CHAPTER  11 Understanding Kubernetes internals\\nThe kube-proxy got its name because it wa s an actual proxy, but the current, much\\nbetter performing implementation only uses iptables  rules to redirect packets to a\\nrandomly selected backend pod without pass ing them through an actual proxy server.\\nThis mode is called the iptables  proxy mode and is shown in figure 11.10.\\nThe major difference between these two mo des is whether packets pass through the\\nkube-proxy and must be handled in user space, or whether they’re handled only by\\nthe Kernel (in kernel space). This has a major impact on performance. \\n Another smaller difference is that the userspace  proxy mode balanced connec-\\ntions across pods in a true round-robin fashion, while the iptables  proxy mode\\ndoesn’t—it selects pods randomly. When only  a few clients use a service, they may not\\nbe spread evenly across pods. For example, if a service has two backing pods but only\\nfive or so clients, don’t be  surprised if you see four clients connect to pod A and only\\none client connect to pod B. With a higher number of clients or pods, this problem\\nisn’t so apparent.\\n You’ll learn exactly how iptables  proxy mode works in section 11.5. \\n11.1.9 Introducing Kubernetes add-ons\\nWe’ve now discussed the core components th at make a Kubernetes cluster work. But\\nin the beginning of the chapter, we also listed a few add-ons, which although not\\nalways required, enable features such as DNS lookup of Kubernetes services, exposing\\nmultiple HTTP services through a single external IP address, the Kubernetes web\\ndashboard, and so on.\\nHOW ADD-ONS ARE DEPLOYED\\nThese components are available as add-ons and are deployed as pods by submitting\\nYAML manifests to the API server, the way you’ve been doing throughout the book.\\nSome of these components are deployed through a Deployment resource or a Repli-\\ncationController resource, and some through a DaemonSet. \\n For example, as I’m writing this, in Minikube, the Ingress controller and the\\ndashboard add-ons are deployed as Replic ationControllers, as shown in the follow-\\ning listing.\\n ClientConﬁgures : iptables\\nredirect straight to pod\\n(no proxy server in-between)\\niptables Podkube-proxy\\nFigure 11.10 The iptables  proxy mode'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 360}, page_content='329 Understanding the architecture\\n$ kubectl get rc -n kube-system\\nNAME                       DESIRED   CURRENT   READY     AGEdefault-http-backend       1         1         1         6d\\nkubernetes-dashboard       1         1         1         6d\\nnginx-ingress-controller   1         1         1         6d\\nThe DNS add-on is deployed as a Deploy ment, as shown in the following listing.\\n$ kubectl get deploy -n kube-system\\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\nkube-dns   1         1         1            1           6d\\nLet’s see how DNS and the Ingress controllers work.\\nHOW THE DNS SERVER  WORKS\\nAll the pods in the cluster are configured to use the cluster’s internal DNS server by\\ndefault. This allows pods to easily look  u p  s e r v i c e s  b y  n a m e  o r  e v e n  t h e  p o d ’ s  I P\\naddresses in the case of headless services.\\n The DNS server pod is exposed through the kube-dns  service, allowing the pod to\\nbe moved around the cluster, like any other pod. The service’s IP address is specified\\nas the nameserver  in the /etc/resolv.conf file insi de every container deployed in the\\ncluster. The kube-dns  pod uses the API server’s watch mechanism to observe changes\\nto Services and Endpoints and updates its DN S records with every change, allowing its\\nclients to always get (fairly) up-to-date DN S information. I say fairly because during\\nthe time between the update of the Service or Endpoints resource and the time the\\nDNS pod receives the watch notificati on, the DNS records may be invalid.\\nHOW (MOST) INGRESS  CONTROLLERS  WORK\\nUnlike the DNS add-on, you’ll find a few different implementations of Ingress con-\\ntrollers, but most of them work in the same way. An Ingress controller runs a reverse\\nproxy server (like Nginx, for example), and keeps it configured according to theIngress, Service, and Endpoints resources de fined in the cluster. The controller thus\\nneeds to observe those resources (again, through the watch mechanism) and change\\nthe proxy server’s config ever y time one of them changes. \\n Although the Ingress resource’s definition  points to a Service, Ingress controllers\\nforward traffic to the service’s pod directly  instead of going through the service IP.\\nThis affects the preservation of client IP s when external clients connect through the\\nIngress controller, which makes them prefe rred over Services in certain use cases.\\nUSING OTHER  ADD-ONS\\nYou’ve seen how both the DNS server and th e Ingress controller add-ons are similar to\\nthe controllers running in the Controller Mana ger, except that they also accept client\\nconnections instead of only observing and modifying resources through the API server. Listing 11.7 Add-ons deployed with ReplicationControllers in Minikube\\nListing 11.8 The kube-dns  Deployment'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 361}, page_content='330 CHAPTER  11 Understanding Kubernetes internals\\n Other add-ons are similar. They all need  to observe the cluster state and perform\\nthe necessary actions when that changes. We’ll introduce a few other add-ons in this\\nand the remaining chapters.\\n11.1.10 Bringing it all together\\nYou’ve now learned that the whole Kubernetes  system is composed of relatively small,\\nloosely coupled components with good separation of concerns. The API server, the\\nScheduler, the individual controllers running inside the Controller Manager, the\\nKubelet, and the kube-proxy all work together to keep the actual state of the systemsynchronized with what you specify as the desired state. \\n For example, submitting a pod manifest to the API server triggers a coordinated\\ndance of various Kubernetes components, which eventually results in the pod’s con-tainers running. You’ll learn how this  dance unfolds in the next section. \\n11.2 How controllers cooperate\\nYou now know about all the components that a Kubernetes cluster is comprised of.\\nNow, to solidify your understanding of how Kubernetes works, let’s go over what hap-\\npens when a Pod resource is created. Becaus e you normally don’t create Pods directly,\\nyou’re going to create a Deployment reso urce instead and see everything that must\\nhappen for the pod’s containers to be started.\\n11.2.1 Understanding which components are involved\\nEven before you start the whole process, the controllers, the Scheduler, and the\\nKubelet are watching the API server for ch anges to their respective resource types.\\nThis is shown in figure 11.11. The components  depicted in the figure will each play a\\npart in the process you’re about to trigge r. The diagram doesn’t include etcd, because\\nit’s hidden behind the API server, and you can think of the API server as the place\\nwhere objects are stored.\\nMaster node\\nController Manager\\nWatchesDeployment\\ncontroller\\nSchedulerReplicaSet\\ncontrollerAPI server\\nDeployments\\nPodsReplicaSetsWatches\\nWatchesNode X\\nWatches\\nDockerKubelet\\nFigure 11.11 Kubernetes components watchi ng API objects through the API server'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 362}, page_content='331 How controllers cooperate\\n11.2.2 The chain of events\\nImagine you prepared the YAML file containing the Deployment manifest and you’re\\nabout to submit it to Kubernetes through kubectl . kubectl  sends the manifest to the\\nKubernetes API server in an HTTP POST request. The API server validates the Deploy-\\nment specification, stores it in  etcd, and returns a response to kubectl . Now a chain\\nof events starts to unfold, as shown in figure 11.12.\\nTHE DEPLOYMENT  CONTROLLER  CREATES  THE REPLICA SET\\nAll API server clients watching the list of Deployments through the API server’s watch\\nmechanism are notified of the newly create d Deployment resource immediately after\\nit’s created. One of those clients is the De ployment controller, which, as we discussed\\nearlier, is the active component re sponsible for handling Deployments. \\n As you may remember from chapter 9, a Deployment is backed by one or more\\nReplicaSets, which then create the actu al pods. As a new Deployment object is\\ndetected by the Deployment controller, it creates a ReplicaSet for the current speci-\\nfication of the Deployment. This involv es creating a new ReplicaSet resource\\nthrough the Kubernetes API. The Deployment  controller doesn’t deal with individ-\\nual pods at all.Master node\\nController\\nManager2. Notiﬁcation\\nthrough watch\\n3. Creates\\nReplicaSet\\n4. Notiﬁcation\\n5. Creates pod\\n6. Notiﬁcation\\nthrough watch\\n7. Assigns pod to node1. Creates Deployment\\nresource\\nDeployment\\ncontroller\\nSchedulerkubectl\\nReplicaSet\\ncontrollerAPI server\\nDeployment ADeployments\\nReplicaSets\\nPod APodsReplicaSet ANode X\\n8. Notiﬁcation\\nthrough watch\\n9. Tells Docker to\\nrun containers\\nDocker\\n10. Runs\\ncontainers\\nContainer(s)Kubelet\\nFigure 11.12 The chain of events that unfolds when a Deployment resource is posted to the API server'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 363}, page_content='332 CHAPTER  11 Understanding Kubernetes internals\\nTHE REPLICA SET CONTROLLER  CREATES  THE POD RESOURCES\\nThe newly created ReplicaSet is then picked  up by the ReplicaSet controller, which\\nwatches for creations, modifications, and deletions of ReplicaSet resources in the\\nAPI server. The controller takes into consideration the replica count and pod selec-\\ntor defined in the ReplicaSet and verifies whether enough existing Pods match\\nthe selector.\\n The controller then creates the Pod reso urces based on the pod template in the\\nReplicaSet (the pod template was copied over from the Deployment when the Deploy-\\nment controller created the ReplicaSet). \\nTHE SCHEDULER  ASSIGNS  A NODE TO THE NEWLY  CREATED  PODS\\nThese newly created Pods are now stored in etcd, but they each still lack one import-\\nant thing—they don’t have an associated node yet. Their nodeName  attribute isn’t set.\\nThe Scheduler watches for Pods like this, and when it encounters one, chooses the\\nbest node for the Pod and assigns the Pod to the node. The Pod’s definition now\\nincludes the name of the node  it should be running on.\\n Everything so far has been happening in  the Kubernetes Control Plane. None of\\nthe controllers that have taken part in this  whole process have done anything tangible\\nexcept update the resources through the API server. \\nTHE KUBELET  RUNS THE POD’S CONTAINERS\\nThe worker nodes haven’t done anything up to this point. The pod’s containershaven’t been started yet. The images for th e pod’s containers haven’t even been down-\\nloaded yet. \\n But with the Pod now scheduled to a spec ific node, the Kubelet on that node can\\nfinally get to work. The Kubelet, watching fo r changes to Pods on the API server, sees a\\nnew Pod scheduled to its node, so it inspec ts the Pod definition and instructs Docker,\\nor whatever container runtime it’s using, to start the pod’s containers. The container\\nruntime then runs the containers.\\n11.2.3 Observing cluster events\\nBoth the Control Plane components and the Kubelet emit events to the API server as\\nthey perform these actions. They do this by creating Event resources, which are like\\nany other Kubernetes resource. You’ve already seen events pertaining to specificresources every time you used \\nkubectl  describe  to inspect those resources, but you\\ncan also retrieve events directly with kubectl  get events .\\n Maybe it’s me, but using kubectl  get to inspect events is painful, because they’re\\nnot shown in proper temporal order. Instead,  if an event occurs multiple times, the\\nevent is displayed only once, showing when it was first seen, when it was last seen, and\\nthe number of times it occurred. Luckily, watching events with the --watch  option is\\nmuch easier on the eyes and useful for seeing what’s happening in the cluster. \\n The following listing shows the events emitted in the process described previously\\n(some columns have been removed and the out put is edited heavily to make it legible\\nin the limited space on the page).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 364}, page_content='333 Understanding what a running pod is\\n$ kubectl get events --watch\\n    NAME             KIND         REASON              SOURCE \\n... kubia            Deployment   ScalingReplicaSet   deployment-controller  \\n                     ➥ Scaled up replica set kubia-193 to 3\\n... kubia-193        ReplicaSe t   SuccessfulCreate    replicaset-controller  \\n                     ➥ Created pod: kubia-193-w7ll2\\n... kubia-193-tpg6j  Pod          Schedule d           default-scheduler   \\n                     ➥ Successfully assigned kubia-193-tpg6j to node1\\n... kubia-193        ReplicaSe t   SuccessfulCreate    replicaset-controller  \\n                     ➥ Created pod: kubia-193-39590\\n... kubia-193        ReplicaSe t   SuccessfulCreate    replicaset-controller  \\n                     ➥ Created pod: kubia-193-tpg6j\\n... kubia-193-39590  Pod          Scheduled           default-scheduler  \\n                     ➥ Successfully assigned kubia-193-39590 to node2\\n... kubia-193-w7ll2  Pod          Scheduled           default-scheduler  \\n                     ➥ Successfully assigned kubia-193-w7ll2 to node2\\n... kubia-193-tpg6j  Pod          Pulled              kubelet, node1  \\n                     ➥ Container image already present on machine\\n... kubia-193-tpg6j  Pod          Created             kubelet, node1  \\n                     ➥ Created container with id 13da752\\n... kubia-193-39590  Pod          Pulled              kubelet, node2  \\n                     ➥ Container image already present on machine\\n... kubia-193-tpg6j  Pod          Started             kubelet, node1  \\n                     ➥ Started container with id 13da752\\n... kubia-193-w7ll2  Pod          Pulled              kubelet, node2  \\n                     ➥ Container image already present on machine\\n... kubia-193-39590  Pod          Created             kubelet, node2  \\n                     ➥ Created container with id 8850184\\n...\\nAs you can see, the SOURCE  column shows the controller performing the action, and\\nthe NAME  and KIND  columns show the resource the controller is acting on. The REASON\\ncolumn and the MESSAGE  c o l u m n  ( s h o w n  i n  e v e r y  s e c o n d  l i n e )  g i v e  m o r e  d e t a i l s\\nabout what the controller has done.\\n11.3 Understanding what a running pod is\\nWith the pod now running, let’s look more clos ely at what a running pod even is. If a\\npod contains a single container, do you th ink that the Kubelet just runs this single\\ncontainer, or is there more to it?\\n You’ve run several pods throughout this book. If you’re the investigative type, you\\nmay have already snuck a peek at what exac tly Docker ran when you created a pod. If\\nnot, let me explain what you’d see.\\n Imagine you run a single container po d. Let’s say you create an Nginx pod:\\n$ kubectl run nginx --image=nginx\\ndeployment \"nginx\" created\\nYou can now ssh into the worker node running the pod and inspect the list of run-\\nning Docker containers. I’m using Mi nikube to test this out, so to ssh into the singleListing 11.9 Watching events emitted by the controllers'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 365}, page_content='334 CHAPTER  11 Understanding Kubernetes internals\\nnode, I use minikube  ssh. If you’re using GKE, you can ssh into a node with gcloud\\ncompute  ssh <node  name> .\\n Once you’re inside the node, you ca n list all the running containers with docker\\nps, as shown in the following listing.\\ndocker@minikubeVM:~$ docker ps\\nCONTAINER ID   IMAGE                  COMMAND                 CREATED\\nc917a6f3c3f7   nginx                  \"nginx -g \\'daemon off\"  4 seconds ago \\n98b8bf797174   gcr.io/.../pause:3.0   \"/pause\"                7 seconds ago\\n...\\nNOTE I’ve removed irrelevant informatio n from the previous listing—this\\nincludes both columns and rows. I’ve al so removed all the other running con-\\ntainers. If you’re trying this out your self, pay attention to the two containers\\nthat were created a few seconds ago. \\nAs expected, you see the Nginx container, bu t also an additional container. Judging\\nfrom the COMMAND  column, this additional container isn’t doing anything (the con-\\ntainer’s command is \"pause\") . If you look closely, you’ll see that this container was\\ncreated a few seconds before the Ng inx container. What’s its role?\\n This pause container is the container that holds all the containers of a pod\\ntogether. Remember how all containers of a pod share the same network and other\\nLinux namespaces? The pause container is an infrastructure container whose sole\\npurpose is to hold all these namespaces. Al l other user-defined containers of the pod\\nthen use the namespaces of the pod infr astructure container (see figure 11.13).\\nActual application containers may die and ge t restarted. When such a container starts\\nup again, it needs to become part of the same Linux namespaces as before. The infra-\\nstructure container makes this possible since it s lifecycle is tied to that of the pod—the\\ncontainer runs from the time the pod is sc heduled until the pod is deleted. If the\\ninfrastructure pod is killed in the meantime , the Kubelet recreates it and all the pod’s\\ncontainers.Listing 11.10 Listing running Docker containers\\nPodContainer A\\nContainer APod infrastructure\\ncontainer\\nContainer B\\nContainer BUses Linux\\nnamespaces fromUses Linux\\nnamespaces from\\nFigure 11.13 A two-container pod results in three running containers \\nsharing the same Linux namespaces.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 366}, page_content='335 Inter-pod networking\\n11.4 Inter-pod networking\\nBy now, you know that each pod gets its own unique IP address and can communicate\\nwith all other pods through a flat, NAT- less network. How exactly does Kubernetes\\nachieve this? In short, it doesn’t. The netw ork is set up by the system administrator or\\nby a Container Network Interface (CNI ) plugin, not by Kubernetes itself. \\n11.4.1 What the network must be like\\nKubernetes doesn’t require you to use a sp ecific networking technology, but it does\\nmandate that the pods (or to be more precise, their containers) can communicatewith each other, regardless if they’re runni ng on the same worker node or not. The\\nnetwork the pods use to communicate must be  such that the IP address a pod sees as\\nits own is the exact same address that all ot her pods see as the IP address of the pod in\\nquestion. \\n Look at figure 11.14. When pod A connects to (sends a network packet to) pod B,\\nthe source IP pod B sees must be the same IP that pod A sees as its own. There should\\nbe no network address translation (NAT) performed in between—the packet sent by\\npod A must reach pod B with both the so urce and destination address unchanged.\\nThis is important, because it makes networ king for applications running inside pods\\nsimple and exactly as if they were running  on machines connected to the same net-\\nwork switch. The absence of NAT between pods enables applications running inside\\nthem to self-register in other pods. Node 1\\nPod A\\nIP: 10.1.1.1\\nsrcIP: 10.1.1.1\\ndstIP: 10.1.2.1\\nsrcIP: 10.1.1.1\\ndstIP: 10.1.2.1PacketNode 2\\nPod B\\nIP: 10.1.2.1\\nsrcIP: 10.1.1.1\\ndstIP: 10.1.2.1Packet\\nNetworkNo NAT (IPs\\nare preserved)\\nFigure 11.14 Kubernetes mandates pods are connected through a NAT-less \\nnetwork.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 367}, page_content='336 CHAPTER  11 Understanding Kubernetes internals\\n For example, say you have a client pod X and pod Y, which provides a kind of noti-\\nfication service to all pods that register with it. Pod X connects to pod Y and tells it,\\n“Hey, I’m pod X, available at IP 1.2.3.4; plea se send updates to me at this IP address.”\\nThe pod providing the service can connect to the first pod by using the receivedIP address. \\n The requirement for NAT-less communicat ion between pods also extends to pod-\\nto-node and node-to-pod communication. But when a pod communicates with ser-vices out on the internet, the source IP of  the packets the pod sends does need to be\\nchanged, because the pod’s IP is private. The source IP of outbound packets is\\nchanged to the host worker node’s IP address.\\n Building a proper Kubernetes cluster in volves setting up the networking according\\nto these requirements. There are various methods and technologies available to do\\nthis, each with its own benefits or drawback s in a given scenario. Because of this, we’re\\nnot going to go into specific technologies . Instead, let’s explain how inter-pod net-\\nworking works in general. \\n11.4.2 Diving deeper into how networking works\\nIn section 11.3, we saw that a pod’s IP address and network namespace are set up and\\nheld by the infrastructure container (the pause container). The pod’s containers then\\nuse its network namespace. A pod’s network inte rface is thus whatever is set up in the\\ninfrastructure container. Let’s see how the interface is created and how it’s connected\\nto the interfaces in all the other pods. Lo ok at figure 11.15. We’ll discuss it next.\\nENABLING  COMMUNICATION  BETWEEN  PODS ON THE SAME NODE\\nBefore the infrastructure container is starte d, a virtual Ethernet interface pair (a veth\\npair) is created for the container. One in terface of the pair remains in the host’s\\nnamespace (you’ll see it listed as vethXXX  when you run ifconfig  on the node),\\nwhereas the other is moved into the container’s network namespace and renamed\\neth0 . The two virtual interfaces are like tw o ends of a pipe (or like two network\\ndevices connected by an Ethernet cable)—wha t goes in on one side comes out on the\\nother, and vice-versa. Node\\nPod A\\neth0\\n10.1.1.1veth123\\nPod B\\neth0\\n10.1.1.2veth234Bridge\\n10.1.1.0/24This is pod A’s\\nveth pair.\\nThis is pod B’s\\nveth pair.Figure 11.15 Pods on a node are \\nconnected to the same bridge through \\nvirtual Ethernet interface pairs.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 368}, page_content='337 Inter-pod networking\\n The interface in the host’s network namespace is attached to a network bridge that\\nthe container runtime is configured to use. The eth0  interface in the container is\\nassigned an IP address from the bridge’s ad dress range. Anything that an application\\nrunning inside the container sends to the eth0  network interface (t he one in the con-\\ntainer’s namespace), comes out at the othe r veth interface in the host’s namespace\\nand is sent to the bridge. This means it can be received by any network interface that’s\\nconnected to the bridge. \\n If pod A sends a network packet to pod B, the packet first goes through pod A’s\\nveth pair to the bridge and then through pod B’s veth pair. All containers on a node\\nare connected to the same bridge, which means they can all communicate with eachother. But to enable communication between containers running on different nodes,\\nthe bridges on those nodes need to be connected somehow. \\nENABLING  COMMUNICATION  BETWEEN  PODS ON DIFFERENT  NODES\\nYou have many ways to connect bridges on different nodes. This can be done withoverlay or underlay networks or by regular layer 3 routing, which we’ll look at next.\\n You know pod IP addresses must be unique  across the whole cluster, so the bridges\\nacross the nodes must use non-overlapping a ddress ranges to prevent pods on differ-\\nent nodes from getting the same IP. In the example shown in figure 11.16, the bridge\\non node A is using the 10.1.1.0/24 IP range and the bridge on node B is using\\n10.1.2.0/24, which ensures no IP address conflicts exist.\\n Figure 11.16 shows that to enable co mmunication between pods across two nodes\\nwith plain layer 3 networking, the node’s ph ysical network interface needs to be con-\\nnected to the bridge as well. Routing tables  on node A need to be configured so all\\npackets destined for 10.1.2.0/24 are routed  to node B, whereas node B’s routing\\ntables need to be configured so packets sent to 10.1.1.0/24 are routed to node A.\\n With this type of setup, when a packet is sent by a container on one of the nodes\\nto a container on the other node, the pack et first goes through the veth pair, then\\nNode A\\nPod A\\nNetworketh0\\n10.1.1.1veth123\\nPod B\\neth0\\n10.1.1.2veth234Bridge\\n10.1.1.0/24\\neth0\\n10.100.0.1Node B\\nPod C\\neth0\\n10.1.2.1veth345\\nPod D\\neth0\\n10.1.2.2veth456Bridge\\n10.1.2.0/24\\neth0\\n10.100.0.2\\nFigure 11.16 For pods on different nodes to communicate, the bridges need to be connected \\nsomehow.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 369}, page_content='338 CHAPTER  11 Understanding Kubernetes internals\\nthrough the bridge to the node’s physical adapter, then over the wire to the other\\nnode’s physical adapter, through the other node’s bridge, and finally through the veth\\npair of the destination container.\\n This works only when nodes are connected to the same network switch, without\\nany routers in between; otherwise those routers would drop the packets because\\nthey refer to pod IPs, which are private. Sure, the routers in between could be con-\\nfigured to route packets between the nodes, but this becomes increasingly difficultand error-prone as the number of router s between the nodes increases. Because of\\nthis, it’s easier to use a Software Defi ned Network (SDN), which makes the nodes\\nappear as though they’re connected to th e same network switch, regardless of the\\nactual underlying network topology, no  matter how complex it is. Packets sent\\nfrom the pod are encapsulated and sent over the network to the node running the\\nother pod, where they are de-encapsulated and delivered to the pod in their origi-nal form.\\n11.4.3 Introducing the Container Network Interface\\nTo make it easier to connect containers into a network, a project called Container\\nNetwork Interface (CNI) was started. The CN I allows Kubernetes to be configured to\\nuse any CNI plugin that’s out there. These plugins include\\n\\uf0a1Calico\\n\\uf0a1Flannel\\n\\uf0a1Romana\\n\\uf0a1Weave Net \\n\\uf0a1And others\\nWe’re not going to go into the details of thes e plugins; if you want to learn more about\\nthem, refer to https:/ /kubernetes.io/docs/concepts/cl uster-administration/addons/ .\\n Installing a network plugin isn’t difficu lt. You only need to deploy a YAML con-\\ntaining a DaemonSet and a few other suppor ting resources. This YAML is provided\\non each plugin’s project page. As you can imagine, the DaemonSet is used to deploy\\na network agent on all cluster nodes. It then  ties into the CNI interface on the node,\\nbut be aware that the Kubelet needs to be started with --network-plugin=cni  to\\nuse CNI. \\n11.5 How services are implemented\\nIn chapter 5 you learned about Services, whic h allow exposing a set of pods at a long-\\nlived, stable IP address and port. In order to  focus on what Services are meant for and\\nhow they can be used, we intentionally didn ’t go into how they work. But to truly\\nunderstand Services and have a better feel for where to look when things don’t behave\\nthe way you expect, you need to unde rstand how they are implemented.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 370}, page_content='339 How services are implemented\\n11.5.1 Introducing the kube-proxy\\nEverything related to Services is handled by the kube-proxy process running on each\\nnode. Initially, the kube-proxy was an actu al proxy waiting for connections and for\\neach incoming connection, opening a new connection to one of the pods. This wascalled the \\nuserspace  proxy mode. Later, a better-performing iptables  proxy mode\\nreplaced it. This is now the default, but yo u can still configure Kubernetes to use the\\nold mode if you want.\\n Before we continue, let’s quickly review a few things about Services, which are rele-\\nvant for understanding the next few paragraphs.\\n We’ve learned that each Service gets its own stable IP address and port. Clients\\n(usually pods) use the service by connect ing to this IP address and port. The IP\\naddress is virtual—it’s not assigned to an y network interfaces and is never listed as\\neither the source or the destination IP address in a network packet when the packetleaves the node. A key detail of Services is th at they consist of an IP and port pair (or\\nmultiple IP and port pairs in the case of mult i-port Services), so the service IP by itself\\ndoesn’t represent anything. That’s why you can’t ping them. \\n11.5.2 How kube-proxy uses iptables\\nWhen a service is created in the API server , the virtual IP address is assigned to it\\nimmediately. Soon afterward, the API server  notifies all kube-proxy agents running on\\nthe worker nodes that a new Service has be en created. Then, each kube-proxy makes\\nthat service addressable on the node it’s ru nning on. It does this by setting up a few\\niptables  rules, which make sure each packet destined for the service IP/port pair is\\nintercepted and its destination address modifi ed, so the packet is redirected to one of\\nthe pods backing the service. \\n Besides watching the API server for changes to Services, kube-proxy also watches\\nfor changes to Endpoints objects. We talk ed about them in chapter 5, but let me\\nrefresh your memory, as it’s easy to forget  they even exist, because you rarely create\\nthem manually. An Endpoints object holds th e IP/port pairs of all the pods that back\\nthe service (an IP/port pair can also poin t to something other than a pod). That’s\\nwhy the kube-proxy must also watch all En dpoints objects. After all, an Endpoints\\nobject changes every time a new backing po d is created or deleted, and when the\\npod’s readiness status changes or the pod’s labe ls change and it falls in or out of scope\\nof the service. \\n Now let’s see how kube-proxy enables clie nts to connect to those pods through the\\nService. This is shown in figure 11.17.\\n The figure shows what the kube-proxy  does and how a packet sent by a client pod\\nreaches one of the pods backing the Service. Let’s examine what happens to the\\npacket when it’s sent by the client pod (pod A in the figure). \\n The packet’s destination is initially set to the IP and port of the Service (in the\\nexample, the Service is at 172.30.0.1:80). Before being sent to the network, the'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 371}, page_content='340 CHAPTER  11 Understanding Kubernetes internals\\npacket is first handled by node  A’s kernel according to the iptables  rules set up on\\nthe node. \\n The kernel checks if the packet matches any of those iptables  rules. One of them\\nsays that if any packet has the destinatio n IP equal to 172.30.0.1 and destination port\\nequal to 80, the packet’s destination IP and port should be replaced with the IP and\\nport of a randomly selected pod. \\n The packet in the example matches that rule and so its destination IP/port is\\nchanged. In the example, pod B2 was random ly selected, so the packet’s destination\\nIP is changed to 10.1.2.1 (pod B2’s IP) and the port to 8080 (the target port specified\\nin the Service spec). From here on, it’s exac tly as if the client pod had sent the packet\\nto pod B directly instead of through the service. \\n It’s slightly more complicated than that , but that’s the most important part you\\nneed to understand.\\n Node A Node BAPI server\\nPod A Pod B1 Pod B2 Pod B3\\nPacket XSource:\\n10.1.1.1\\nDestination:\\n172.30.0.1:80\\n10.1.2.1:8080iptablesService B\\n172.30.0.1:80\\nConﬁgures\\niptables\\nPacket XSource:\\n10.1.1.1\\nDestination:\\n172.30.0.1:80kube-proxyEndpoints B\\nPod A\\nIP: 10.1.1.1Pod B1\\nIP: 10.1.1.2Pod B2\\nIP: 10.1.2.1\\nPod B3\\nIP: 10.1.2.2Watches for changes to\\nservices and endpoints\\nFigure 11.17 Network packets sent to a Service’s virtual IP/port pair are \\nmodified and redirected to a randomly selected backend pod.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 372}, page_content='341 Running highly available clusters\\n11.6 Running highly available clusters\\nOne of the reasons for running apps inside Kubernetes is to keep them running with-\\nout interruption with no or  limited manual intervention in case of infrastructure\\nfailures. For running services without interr uption it’s not only the apps that need to\\nbe up all the time, but also the Kubernet es Control Plane components. We’ll look at\\nwhat’s involved in achieving high availability next.\\n11.6.1 Making your apps highly available\\nWhen running apps in Kubernetes, the vari ous controllers make sure your app keeps\\nrunning smoothly and at the specified scale even when nodes fail. To ensure your app\\nis highly available, you only need to ru n them through a Deployment resource and\\nconfigure an appropriate number of replic as; everything else is taken care of by\\nKubernetes. \\nRUNNING  MULTIPLE  INSTANCES  TO REDUCE  THE LIKELIHOOD  OF DOWNTIME\\nThis requires your apps to be horizontally scalable, but even if that’s not the case inyour app, you should still use a Deployment with its replica count set to one. If the\\nreplica becomes unavailable, it will be repl aced with a new one quickly, although that\\ndoesn’t happen instantaneously. It takes time  for all the involved controllers to notice\\nthat a node has failed, create the new po d replica, and start the pod’s containers.\\nThere will inevitably be a short period of downtime in between. \\nUSING LEADER -ELECTION  FOR NON-HORIZONTALLY  SCALABLE  APPS\\nTo avoid the downtime, you need to run addi tional inactive replicas along with the\\nactive one and use a fast-acting lease or le ader-election mechanism to make sure only\\none is active. In case you’re unfamiliar with leader election, it’s a way for multiple app\\ninstances running in a distributed environm ent to come to an agreement on which is\\nthe leader. That leader is either the only one performing tasks, while all others are\\nwaiting for the leader to fail and then beco ming leaders themselves, or they can all be\\nactive, with the leader being the only inst ance performing writes, while all the others\\nare providing read-only access to their data , for example. This ensures two instances\\nare never doing the same job, if that would lead to unpredictable system behavior due\\nto race conditions.\\n The mechanism doesn’t need to be incorp orated into the app itself. You can use a\\nsidecar container that performs all the le ader-election operations and signals the\\nmain container when it should become active . You’ll find an example of leader elec-\\ntion in Kubernetes at https:/ /github.com/kubernetes/contrib/tree/master/election .\\n Ensuring your apps are highly availabl e is relatively simple, because Kubernetes\\ntakes care of almost everything. But what if Kubernetes itself fails? What if the servers\\nrunning the Kubernetes Control Plane co mponents go down? How are those compo-\\nnents made highly available?'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 373}, page_content='342 CHAPTER  11 Understanding Kubernetes internals\\n11.6.2 Making Kubernetes Control Plane components highly available\\nIn the beginning of this chapter, you lear ned about the few components that make up\\na Kubernetes Control Plane. To make Kubern etes highly available, you need to run\\nmultiple master nodes, which run multiple  instances of the following components:\\n\\uf0a1etcd, which is the distributed data st ore where all the API objects are kept\\n\\uf0a1API server\\n\\uf0a1Controller Manager, which is the proc ess in which all the controllers run\\n\\uf0a1Scheduler\\nWithout going into the actual details of ho w to install and run these components, let’s\\nsee what’s involved in making each of thes e components highly available. Figure 11.18\\nshows an overview of a highly available cluster.\\nRUNNING  AN ETCD CLUSTER\\nBecause etcd was designed as a distributed sy stem, one of its key features is the ability\\nto run multiple etcd instances, so making it highly available is no big deal. All youneed to do is run it on an appropriate number of machines (three, five, or seven, as\\nexplained earlier in the chapter) and make them aware of each other. You do this by\\nincluding the list of all the other instance s in every instance’s configuration. For\\nexample, when starting an instance, you sp ecify the IPs and ports where the other etcd\\ninstances can be reached. \\n etcd will replicate data across all its inst ances, so a failure of one of the nodes when\\nrunning a three-machine cluster will still al low the cluster to accept both read and\\nwrite operations. To increase the fault tole rance to more than a single node, you need\\nto run five or seven etcd nodes, which woul d allow the cluster to handle two or threeNode 1Kubelet\\nNode 2Kubelet\\nNode 3Kubelet\\nNode 4Kubelet\\nNode 5Kubelet\\n...\\nNode NKubeletLoad\\nbalancer\\nMaster 3etcd API server\\nSchedulerController\\nManager\\n[standing-by] [standing-by]\\nMaster 2etcd API server\\nSchedulerController\\nManager\\n[standing-by] [standing-by]\\nMaster 1etcd API server\\nSchedulerController\\nManager\\n[active] [active]\\nFigure 11.18 A highly-available cluster with three master nodes'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 374}, page_content='343 Running highly available clusters\\nnode failures, respectively. Having more than  seven etcd instances is almost never nec-\\nessary and begins impacting performance.\\nRUNNING  MULTIPLE  INSTANCES  OF THE API SERVER\\nMaking the API server highly available is even  simpler. Because the API server is (almost\\ncompletely) stateless (all the data  is stored in etcd, but the API server does cache it), you\\ncan run as many API servers as you need, and they don’t need to be aware of each other\\nat all. Usually, one API server is collocated with every etcd instance. By doing this, the\\netcd instances don’t need any kind of load balancer in front of them, because every APIserver instance only talks to the local etcd instance. \\n The API servers, on the other hand, do ne ed to be fronted by a load balancer, so\\nclients (\\nkubectl , but also the Controller Manager,  Scheduler, and all the Kubelets)\\nalways connect only to the healthy API server instances. \\nENSURING  HIGH AVAILABILITY  OF THE CONTROLLERS  AND THE SCHEDULER\\nCompared to the API server, where multiple replicas can run simultaneously, run-ning multiple instances of the Controller Manager or the Scheduler isn’t as simple.\\nBecause controllers and the Scheduler all ac tively watch the cluster state and act when\\nit changes, possibly modifying the cluster st ate further (for example, when the desired\\nreplica count on a ReplicaSet is increased by one, the ReplicaSet  controller creates an\\nadditional pod), running multiple instan ces of each of those components would\\nresult in all of them performing the same action. They’d be racing each other, which\\ncould cause undesired effects (creating two new pods instead of one, as mentioned in\\nthe previous example).\\n For this reason, when running multiple instances of these components, only one\\ninstance may be active at any given time. Luckily, this is a ll taken care of by the compo-\\nnents themselves (this is controlled with the \\n--leader-elect  option, which defaults to\\ntrue). Each individual component will only be active when it’s the elected leader. Only\\nthe leader performs actual work, whereas all other instances are standing by and waiting\\nfor the current leader to fail. When it does , the remaining instances elect a new leader,\\nwhich then takes over the work. This mechan ism ensures that two components are never\\noperating at the same time and doin g the same work (see figure 11.19).\\nMaster 3SchedulerController\\nManager\\n[standing-by] [standing-by]\\nMaster 1SchedulerController\\nManager\\n[active] [active]\\nMaster 2SchedulerController\\nManager\\n[standing-by] [standing-by]Only the controllers in\\nthis Controller Managerare reacting to APIresources being created,updated, and deleted.These Controller Managers\\nand Schedulers aren’t doinganything except waiting tobecome leaders.\\nOnly this Scheduleris scheduling pods.\\nFigure 11.19 Only a single Controller Manager and a single Scheduler are active; others are standing by.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 375}, page_content='344 CHAPTER  11 Understanding Kubernetes internals\\nThe Controller Manager and Scheduler can run collocated with the API server and\\netcd, or they can run on separate machines. When collocated, they can talk to the\\nlocal API server directly; otherwise they connect to the API servers through the load\\nbalancer.\\nUNDERSTANDING  THE LEADER  ELECTION  MECHANISM  USED IN CONTROL  PLANE COMPONENTS\\nWhat I find most interesting here is that these components don’t need to talk to each\\nother directly to elect a leader. The leader  election mechanism works purely by creat-\\ning a resource in the API server. And it’s not even a special kind of resource—the End-points resource is used to achieve this (a bused is probably a more appropriate term).\\n There’s nothing special about using an Endpoints object to do this. It’s used\\nbecause it has no side effects as long as no Service with the same name exists. Any\\nother resource could be used (in fact, the leader election mechanism will soon use\\nConfigMaps instead of Endpoints). \\n I’m sure you’re interested in how a reso urce can be used for this purpose. Let’s\\ntake the Scheduler, for example. All instance s of the Scheduler try to create (and later\\nupdate) an Endpoints resource called \\nkube-scheduler . You’ll find it in the kube-\\nsystem  namespace, as the following listing shows.\\n$ kubectl get endpoints kube-scheduler -n kube-system -o yaml\\napiVersion: v1kind: Endpoints\\nmetadata:\\n  annotations:    control-plane.alpha.kubernetes.io/leader: \\'{\"holderIdentity\":\\n      \\n➥ \"minikube\",\"leaseDurationSeconds\":15,\"acquireTime\":\\n      ➥ \"2017-05-27T18:54:53Z\",\"renewTime\":\"2017-05-28T13:07:49Z\",\\n      ➥ \"leaderTransitions\":0}\\'\\n  creationTimestamp: 2017-05-27T18:54:53Z  name: kube-scheduler\\n  namespace: kube-system\\n  resourceVersion: \"654059\"  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler\\n  uid: f847bd14-430d-11e7-9720-080027f8fa4e\\nsubsets: []\\nThe control-plane.alpha. kubernetes.io/leader  annotation is the important part.\\nAs you can see, it contains a field called holderIdentity , which holds the name of the\\ncurrent leader. The first instance that su cceeds in putting its name there becomes\\nthe leader. Instances race each other to do  that, but there’s always only one winner.\\n Remember the optimistic concurrency we explained earlier? That’s what ensures\\nthat if multiple instances try to write thei r name into the resource only one of them\\nsucceeds. Based on whether the write succeed ed or not, each instance knows whether\\nit is or it isn’t the leader. \\n Once becoming the leader, it must period ically update the resource (every two sec-\\nonds by default), so all other instances know that it’s still alive. When the leader fails,Listing 11.11 The kube-scheduler  Endpoints resource us ed for leader-election'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 376}, page_content='345 Summary\\nother instances see that the resource hasn’t  been updated for a while, and try to become\\nthe leader by writing their own na me to the resource. Simple, right?\\n11.7 Summary\\nHopefully, this has been an  interesting chapter that has improved your knowledge of\\nthe inner workings of Kubernetes. This chapter has shown you\\n\\uf0a1What components make up a Kubernetes cluster and what each component isresponsible for\\n\\uf0a1How the API server, Scheduler, various controllers running in the Controller\\nManager, and the Kubelet work to gether to bring a pod to life\\n\\uf0a1How the infrastructure container binds together all the containers of a pod\\n\\uf0a1How pods communicate with other pods running on the same node through\\nthe network bridge, and how those bridges on different nodes are connected,\\nso pods running on different nodes can talk to each other\\n\\uf0a1How the kube-proxy performs load balanc ing across pods in the same service by\\nconfiguring iptables  rules on the node\\n\\uf0a1How multiple instances of each compon ent of the Control Plane can be run to\\nmake the cluster highly available\\nNext, we’ll look at how to secure the API serv er and, by extension, the cluster as a whole.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 377}, page_content='346Securing the\\nKubernetes API server\\nIn chapter 8 you learned how applications running in pods can talk to the API\\nserver to retrieve or change the state of resources deployed in the cluster. To\\nauthenticate with the API server, you used the ServiceAccount token mounted into\\nthe pod. In this chapter, you’ll learn wh at ServiceAccounts are and how to config-\\nure their permissions, as well as permissions for other subjects using the cluster. \\n12.1 Understanding authentication\\nIn the previous chapter, we said the API server can be configured with one or more\\nauthentication plugins (and the same is true for authorization plugins). When a\\nrequest is received by the API server, it  goes through the list of authenticationThis chapter covers\\n\\uf0a1Understanding authentication\\n\\uf0a1What ServiceAccounts are and why they’re used\\n\\uf0a1Understanding the role-based access control \\n(RBAC) plugin\\n\\uf0a1Using Roles and RoleBindings\\n\\uf0a1Using ClusterRoles and ClusterRoleBindings\\n\\uf0a1Understanding the default roles and bindings'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 378}, page_content='347 Understanding authentication\\nplugins, so they can each examine the request and try to determine who’s sending the\\nrequest. The first plugin that can extract that information from the request returns\\nthe username, user ID, and the groups the client belongs to back to the API server\\ncore. The API server stops invoking the remaining authentication plugins and contin-ues onto the authorization phase. \\n Several authentication plugins are available. They obtain the identity of the client\\nusing the following methods:\\n\\uf0a1From the client certificate\\n\\uf0a1From an authentication toke n passed in an HTTP header\\n\\uf0a1Basic HTTP authentication\\n\\uf0a1Others\\nThe authentication plugins are enabled through command-line options when starting\\nthe API server. \\n12.1.1 Users and groups\\nAn authentication plugin returns the username and group(s) of the authenticateduser. Kubernetes doesn’t store that informatio n anywhere; it uses it to verify whether\\nthe user is authorized to perform an action or not.\\nUNDERSTANDING  USERS\\nKubernetes distinguishes between two kinds of clients connecting to the API server:\\n\\uf0a1Actual humans (users)\\n\\uf0a1Pods (more specifically, applications running inside them)\\nBoth these types of clients are authentica ted using the aforementioned authentication\\nplugins. Users are meant to be managed by an external system, such as a Single Sign\\nOn (SSO) system, but the pods use a mechanism called service accounts , which are cre-\\nated and stored in the cluster as ServiceA ccount resources. In contrast, no resource\\nrepresents user accounts, which means you ca n’t create, update, or delete users through\\nthe API server. \\n We won’t go into any details of how to manage users, but we will explore Service-\\nAccounts in detail, because they’re esse ntial for running pods. For more informa-\\ntion on how to configure the cluster for authentication of human users, cluster\\nadministrators should refer to the Kubernetes Cluster Administrator guide at http:/ /\\nkubernetes.io/docs/admin .\\nUNDERSTANDING  GROUPS\\nBoth human users and ServiceAccounts can be long to one or more groups. We’ve said\\nthat the authentication plugin returns groups along with the username and user ID.\\nGroups are used to grant permissions to se veral users at once, instead of having to\\ngrant them to individual users.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 379}, page_content='348 CHAPTER  12 Securing the Kubernetes API server\\n Groups returned by the plugin are nothing but strings, representing arbitrary\\ngroup names, but built-in groups have special meaning:\\n\\uf0a1The system:unauthenticated  group is used for requests where none of the\\nauthentication plugins could authenticate the client.\\n\\uf0a1The system:authenticated  group is automatically assigned to a user who was\\nauthenticated successfully.\\n\\uf0a1The system:serviceaccounts  group encompasses all ServiceAccounts in the\\nsystem.\\n\\uf0a1The system:serviceac counts:<namespace>  includes all ServiceAccounts in a\\nspecific namespace.\\n12.1.2 Introducing ServiceAccounts\\nLet’s explore ServiceAccounts up close. Yo u’ve already learned that the API server\\nrequires clients to authenticate themselves  before they’re allowed to perform opera-\\ntions on the server. And you’ve already seen  how pods can authenticate by sending the\\ncontents of the file /var/run/secrets/kubernete s.io/serviceac count/token , which\\nis mounted into each contai ner’s filesystem through a secret  volume.\\n But what exactly does that file represent? Every pod is associated with a Service-\\nAccount, which represents the identity of the app running in the pod. The token file\\nholds the ServiceAccount’s auth entication token. When an app uses this token to con-\\nnect to the API server, the authentication plugin authenticates the ServiceAccount\\nand passes the ServiceAccount’s username back to the API server core. Service-\\nAccount usernames are formatted like this:\\nsystem:serviceaccount:<namespace>:<service account name>\\nThe API server passes this username to th e configured authorization plugins, which\\ndetermine whether the action the app is tryi ng to perform is allowed to be performed\\nby the ServiceAccount.\\n ServiceAccounts are nothing more than a way for an application running inside a\\npod to authenticate itself with the API se rver. As already mentioned, applications do\\nthat by passing the ServiceAccount’s token in the request.\\nUNDERSTANDING  THE SERVICE ACCOUNT  RESOURCE\\nServiceAccounts are resources just like Pods , Secrets, ConfigMaps, and so on, and are\\nscoped to individual namespaces. A defaul t ServiceAccount is automatically created\\nfor each namespace (that’s the one your pods have used all along). \\n You can list ServiceAccounts like you do other resources:\\n$ kubectl get sa\\nNAME      SECRETS   AGEdefault   1         1d\\nNOTE The shorthand for serviceaccount  is sa.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 380}, page_content='349 Understanding authentication\\nAs you can see, the current namespace only contains the default  ServiceAccount. Addi-\\ntional ServiceAccounts can be added when requ ired. Each pod is associated with exactly\\none ServiceAccount, but multiple pods can use the same ServiceAccount. As you can\\nsee in figure 12.1, a pod can only use a ServiceAccount from the same namespace.\\nUNDERSTANDING  HOW SERVICE ACCOUNTS  TIE INTO AUTHORIZATION\\nYou can assign a ServiceAccount to a pod by specifying the account’s name in the pod\\nmanifest. If you don’t assign it explicitly , the pod will use the default ServiceAccount\\nin the namespace.\\n By assigning different ServiceAccounts to pods, you can control which resources\\neach pod has access to. When a request bearing the authentication token is receivedby the API server, the server uses the toke n to authenticate the client sending the\\nrequest and then determines whether or no t the related ServiceAccount is allowed to\\nperform the requested operation. The API server obtains this information from thesystem-wide authorization plugin configur ed by the cluster administrator. One of\\nthe available authorization plugins is the role-based access control (RBAC) plugin,\\nwhich is discussed later in this chapter. From Kubernetes version 1.6 on, the RBACplugin is the plugin most clusters should use.\\n12.1.3 Creating ServiceAccounts\\nWe’ve said every namespace contains its own default ServiceAccount, but additional\\nones can be created if necessary. But why should you bother with creating Service-\\nAccounts instead of using the default one for all your pods? \\n The obvious reason is cluster security. Po ds that don’t need to read any cluster\\nmetadata should run under a constrained ac count that doesn’t allow them to retrieve\\nor modify any resources deployed in the clus ter. Pods that need to retrieve resource\\nmetadata should run under a ServiceAccount that only allows reading those objects’\\nmetadata, whereas pods that need to modify  those objects should run under their own\\nServiceAccount allowing modifications of API objects. PodNamespace: foo\\nService-\\nAccount:\\ndefaultPod PodNamespace: baz\\nPodNamespace: bar\\nPod Pod\\nNot possibleService-\\nAccount:\\ndefaultAnother\\nService-Account\\nMultiple pods using the\\nsame ServiceAccount\\nFigure 12.1 Each pod is associated with a sin gle ServiceAccount in the pod’s namespace.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 381}, page_content='350 CHAPTER  12 Securing the Kubernetes API server\\n Let’s see how you can create additional Se rviceAccounts, how they relate to Secrets,\\nand how you can assign them to your pods.\\nCREATING  A SERVICE ACCOUNT\\nCreating a ServiceAccount is incredibly easy, thanks to the dedicated kubectl  create\\nserviceaccount  command. Let’s create a new ServiceAccount called foo:\\n$ kubectl create serviceaccount foo\\nserviceaccount \"foo\" created\\nNow, you can inspect the ServiceAccount with the describe  command, as shown in\\nthe following listing.\\n$ kubectl describe sa foo\\nName:               foo\\nNamespace:          default\\nLabels:             <none>\\nImage pull secrets: <none>             \\nMountable secrets:  foo-token-qzq7j    \\nTokens:             foo-token-qzq7j    \\nYou can see that a custom token Secret ha s been created and associated with the\\nServiceAccount. If you look at the Secret’s data with kubectl  describe  secret  foo-\\ntoken-qzq7j , you’ll see it contains the same item s (the CA certificate, namespace, and\\ntoken) as the default ServiceAccount’s toke n does (the token itself will obviously be\\ndifferent), as shown in the following listing.\\n$ kubectl describe secret foo-token-qzq7j\\n...ca.crt:         1066 bytes\\nnamespace:      7 bytes\\ntoken:          eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\\nNOTE You’ve probably heard of JSON We b Tokens (JWT). The authentica-\\ntion tokens used in ServiceAccounts are JWT tokens.\\nUNDERSTANDING  A SERVICE ACCOUNT ’S MOUNTABLE  SECRETS\\nThe token is shown in the Mountable  secrets  list when you inspect a ServiceAccount\\nwith kubectl  describe . Let me explain what that list represents. In chapter 7 you\\nlearned how to create Secrets and mount them inside a pod. By default, a pod canmount any Secret it wants. But the pod’s ServiceAccount can be configured to onlyListing 12.1 Inspecting a ServiceAccount with kubectl  describe\\nListing 12.2 Inspecting the custom ServiceAccount’s SecretThese will be added \\nautomatically to all pods using this ServiceAccount.\\nPods using this ServiceAccount \\ncan only mount these Secrets if \\nmountable Secrets are enforced.\\nAuthentication token(s). \\nThe first one is mounted \\ninside the container.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 382}, page_content='351 Understanding authentication\\nallow the pod to mount Secrets that are listed as mountable Secrets on the Service-\\nAccount. To enable this feature, the Serv iceAccount must contain the following anno-\\ntation: kubernetes.io/enforce-mo untable-secrets=\"true\" . \\n If the ServiceAccount is annotated with th is annotation, any pods using it can mount\\nonly the ServiceAccount’s mountable Secr ets—they can’t use any other Secret.\\nUNDERSTANDING  A SERVICE ACCOUNT ’S IMAGE  PULL SECRETS\\nA ServiceAccount can also contain a list of image pull Secrets, which we examined in\\nchapter 7. In case you don’t remember, they  are Secrets that hold the credentials for\\npulling container images from a private image repository. \\n The following listing shows an example of a ServiceAccount definition, which\\nincludes the image pull Secret you created in chapter 7.\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: my-service-accountimagePullSecrets:\\n- name: my-dockerhub-secret\\nA ServiceAccount’s image pull Secrets behave slightly differently than its mountable\\nSecrets. Unlike mountable Secrets, they do n’t determine which image pull Secrets a\\npod can use, but which ones are added automatically to all pods using the Service-Account. Adding image pull Secrets to a Serv iceAccount saves you from having to add\\nthem to each pod individually. \\n12.1.4 Assigning a ServiceAccount to a pod\\nAfter you create additional ServiceAccounts, you need to assign them to pods. This isdone by setting the name of the ServiceAccount in the \\nspec.serviceAccountName\\nfield in the pod definition. \\nNOTE A pod’s ServiceAccount must be set when creating the pod. It can’t be\\nchanged later. \\nCREATING  A POD WHICH  USES A CUSTOM  SERVICE ACCOUNT\\nIn chapter 8 you deployed a pod that ran a container based on the tutum/curl  image\\nand an ambassador container alongside it. You used it to explore the API server’s\\nREST interface. The amba ssador container ran the kubectl  proxy  process, which\\nused the pod’s ServiceAccount’s token to authenticate with the API server. \\n You can now modify the pod so it uses the foo ServiceAccount you created minutes\\nago. The next listing shows the pod definition.\\n  Listing 12.3 ServiceAccount with an image pull Secret: sa-image-pull-secrets.yaml'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 383}, page_content='352 CHAPTER  12 Securing the Kubernetes API server\\napiVersion: v1\\nkind: Podmetadata:\\n  name: curl-custom-sa\\nspec:  serviceAccountName: foo           \\n  containers:\\n  - name: main\\n    image: tutum/curl\\n    command: [\"sleep\", \"9999999\"]  - name: ambassador                  \\n    image: luksa/kubectl-proxy:1.6.2\\nTo confirm that the custom ServiceAccount ’s token is mounted into the two contain-\\ners, you can print the contents of the token as shown in the following listing.\\n$ kubectl exec -it curl-custom-sa -c main \\n➥ cat /var/run/secrets/kubernetes.io/serviceaccount/token\\neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\\nYou can see the token is the one from the foo ServiceAccount by comparing the token\\nstring in listing 12.5 with the one in listing 12.2. \\nUSING THE CUSTOM  SERVICE ACCOUNT ’S TOKEN  TO TALK TO THE API SERVER\\nLet’s see if you can talk to the API server using this token. As mentioned previously,\\nthe ambassador container uses the token when  talking to the server, so you can test\\nthe token by going through the ambassador, which listens on localhost:8001 , as\\nshown in the following listing.\\n$ kubectl exec -it curl-custom -sa -c main curl localhost:8001/api/v1/pods\\n{\\n  \"kind\": \"PodList\",  \"apiVersion\": \"v1\",\\n  \"metadata\": {\\n    \"selfLink\": \"/api/v1/pods\",    \"resourceVersion\": \"433895\"\\n  },\\n  \"items\": [  ...\\nOkay, you got back a proper response from the server, which means the custom\\nServiceAccount is allowed to list pods. This  may be because your cluster doesn’t use\\nthe RBAC authorization plugin, or you gave  all ServiceAccounts full permissions, as\\ninstructed in chapter 8. Listing 12.4 Pod using a non-default ServiceAccount: curl-custom-sa.yaml\\nListing 12.5 Inspecting the token mounted into the pod’s container(s)\\nListing 12.6 Talking to the API se rver with a custom ServiceAccountThis pod uses the \\nfoo ServiceAccount \\ninstead of the default.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 384}, page_content='353 Securing the cluster with role-based access control\\n When your cluster isn’t using proper authorization, creating and using additional\\nServiceAccounts doesn’t make much sense, since even the default ServiceAccount is\\nallowed to do anything. The only reason to  use ServiceAccounts in that case is to\\nenforce mountable Secrets or to provide image pull Secrets through the Service-Account, as explained earlier. \\n But creating additional ServiceAccounts is practically a must when you use the\\nRBAC authorization plugin, which we’ll explore next.\\n12.2 Securing the cluster with role-based access control\\nStarting with Kubernetes version 1.6.0, cluster security was ramped up considerably. Inearlier versions, if you managed to acquire the authentication token from one of the\\npods, you could use it to do anything you wa nt in the cluster. If you google around,\\nyou’ll find demos showing how a path traversal  (or directory traversal ) attack (where clients\\ncan retrieve files located outside of the web server’s web root directory) can be used to\\nget the token and use it to run your malicious  pods in an insecure Kubernetes cluster.\\n But in version 1.8.0, the RBAC author ization plugin graduated to GA (General\\nAvailability) and is now enabled by defaul t on many clusters (for example, when\\ndeploying a cluster with kubadm, as descri bed in appendix B). RBAC prevents unau-\\nthorized users from viewing or modifyin g the cluster state. The default Service-\\nAccount isn’t allowed to view cluster state, let alone modify it in any way, unless you\\ngrant it additional privileges. To write apps that communicate with the Kubernetes\\nAPI server (as described in chapter 8), you need to understand how to manage\\nauthorization through RBAC-specific resources.\\nNOTE In addition to RBAC, Kubernetes also includes other authorization\\nplugins, such as the Attribute-based access control (ABAC) plugin, a Web-Hook plugin and custom plugin implementations. RBAC is the standard,though.\\n12.2.1 Introducing the RBAC authorization plugin\\nThe Kubernetes API server can be configured  to use an authorization plugin to check\\nwhether an action is allowed to be performe d by the user requesti ng the action. Because\\nthe API server exposes a REST interface,  users perform actions by sending HTTP\\nrequests to the server. Users authenticate themselves by including credentials in the\\nrequest (an authentication token, username  and password, or a client certificate).\\nUNDERSTANDING  ACTIONS\\nBut what actions are there? As you know, REST clients send GET, POST , PUT, DELETE ,\\nand other types of HTTP requests to spec ific URL paths, which represent specific\\nREST resources. In Kubernetes, those resour ces are Pods, Services, Secrets, and so on.\\nHere are a few examples of actions in Kubernetes:\\n\\uf0a1Get Pods\\n\\uf0a1Create Services'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 385}, page_content='354 CHAPTER  12 Securing the Kubernetes API server\\n\\uf0a1Update Secrets\\n\\uf0a1And so on\\nThe verbs in those examples (get, create, update) map to HTTP methods ( GET, POST ,\\nPUT) performed by the client (the complete  mapping is shown in table 12.1). The\\nnouns (Pods, Service, Secrets) obvi ously map to Kubernetes resources. \\n An authorization plugin such as RBAC, which runs inside the API server, deter-\\nmines whether a client is allowed to perf orm the requested verb on the requested\\nresource or not.\\nNOTE The additional verb use is used for PodSecurityPolicy resources, which\\nare explained in the next chapter.\\nBesides applying security permissions to whole resource types, RBAC rules can also\\napply to specific instances of a reso urce (for example, a Service called myservice ).\\nAnd later you’ll see that permissions can also apply to non-resource URL paths,\\nbecause not every path the API server exposes maps to a resource (such as the /api\\npath itself or the server health information at /healthz ). \\nUNDERSTANDING  THE RBAC PLUGIN\\nThe RBAC authorization plugin, as the name suggests, uses user roles as the key factor\\nin determining whether the user may perform the action or not. A subject (which may\\nbe a human, a ServiceAccount, or a group of users or ServiceAccounts) is associated\\nwith one or more roles and each role is allowed to perform certain verbs on certain\\nresources. \\n If a user has multiple roles, they may do anything that any of their roles allows\\nthem to do. If none of the user’s roles contains a permission to, for example, update\\nSecrets, the API server will prevent the user from performing PUT or PATCH  requests\\non Secrets.\\n Managing authorization through the RBAC pl ugin is simple. It’s all done by creat-\\ning four RBAC-specific Kubernetes re sources, which we’ll look at next.Table 12.1 Mapping HTTP methods to authorization verbs\\nHTTP method Verb for single resource Verb for collection\\nGET, HEAD get  (and watch  for watching) list  (and watch )\\nPOST create n/a\\nPUT update n/a\\nPATCH patch n/a\\nDELETE delete deletecollection'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 386}, page_content='355 Securing the cluster with role-based access control\\n12.2.2 Introducing RBAC resources\\nThe RBAC authorization rules are configur ed through four resources, which can be\\ngrouped into two groups:\\n\\uf0a1Roles and ClusterRoles, which specify which verbs can be performed on which\\nresources.\\n\\uf0a1RoleBindings and ClusterRoleBindings, which bind the above roles to specific\\nusers, groups, or ServiceAccounts.\\nRoles define what can be done, while bindings define who can do it (this is shown in\\nfigure 12.2).\\nThe distinction between a Ro le and a ClusterRole, or between a RoleBinding and a\\nClusterRoleBinding, is that the Role and RoleBinding are namespaced resources,\\nwhereas the ClusterRole and ClusterRoleB inding are cluster-level resources (not\\nnamespaced). This is depicted in figure 12.3.\\n As you can see from the figure, multiple RoleBindings can exist in a single name-\\nspace (this is also true for Roles). Likewise, multiple ClusterRoleBindings and Cluster-\\nRoles can be created. Another thing shown in  the figure is that although RoleBindings\\nare namespaced, they can also refe rence ClusterRoles, which aren’t. \\n The best way to learn about these four reso urces and what their effects are is by try-\\ning them out in a hands-on ex ercise. You’ll do that now.\\n \\n  \\n What?\\nRole\\nBindingSome\\nresources\\nOther\\nresourcesRole\\nDoesn’t allow\\ndoing anythingwith other resourcesUser AWho?\\nAdmins groupAllows users\\nto access\\nService-\\nAccount:\\nx\\nFigure 12.2 Roles grant permissions, wh ereas RoleBindings bind Roles to subjects.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 387}, page_content='356 CHAPTER  12 Securing the Kubernetes API server\\nSETTING  UP YOUR EXERCISE\\nBefore you can explore how RBAC resources affect what you can do through the API\\nserver, you need to make sure RBAC is enabled in your cluster. First, ensure you’re\\nusing at least version 1.6 of Kubernetes and that the RBAC plugin is the only config-ured authorization plugin. There can be mu ltiple plugins enabled in parallel and if\\none of them allows an action to be  performed, the action is allowed.\\nNOTE If you’re using GKE 1.6 or 1.7, you ne ed to explicitly disable legacy autho-\\nrization by creating the cluster with the --no-enable-legac y-authorization\\noption. If you’re using Minikube, you also may need to enable RBAC by start-\\ning Minikube with --extra-config=apiserver .Authorization.Mode=RBAC\\nIf you followed the instructions on how to  disable RBAC in chapter 8, now’s the time\\nto re-enable it by running the following command:\\n$ kubectl delete clusterrolebinding permissive-binding\\nTo try out RBAC, you’ll run a pod through wh ich you’ll try to talk to the API server,\\nthe way you did in chapter 8. But this time  you’ll run two pods in different namespaces\\nto see how per-namespace security behaves.\\n In the examples in chapter 8, you ran tw o containers to demonstrate how an appli-\\ncation in one container uses the other contai ner to talk to the API server. This time,\\nyou’ll run a single container (based on the kubectl-proxy  image) and use kubectl\\nexec  to run curl  inside that container directly. The proxy will take care of authentica-\\ntion and HTTPS, so you can focus on the au thorization aspect of API server security.Namespace C\\nNamespaced\\nresources\\nCluster-level\\nresourcesRoleBinding\\nRoleBindingRoleNamespace B\\nNamespaced\\nresourcesRoleBinding RoleNamespace A\\nNamespaced\\nresourcesRoleBinding Role\\nCluster scope (resources that aren’t namespaced)ClusterRoleBinding ClusterRole\\nFigure 12.3 Roles and RoleBindings are namespaced;  ClusterRoles and ClusterRoleBindings aren’t.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 388}, page_content='357 Securing the cluster with role-based access control\\nCREATING  THE NAMESPACES  AND RUNNING  THE PODS\\nYou’re going to create one pod in namespace foo and the other one in namespace\\nbar, as shown in the following listing.\\n$ kubectl create ns foo\\nnamespace \"foo\" created\\n$ kubectl run test --image=luksa/kubectl-proxy -n foo\\ndeployment \"test\" created\\n$ kubectl create ns barnamespace \"bar\" created\\n$ kubectl run test --image=luksa/kubectl-proxy -n bar\\ndeployment \"test\" created\\nNow open two terminals and use kubectl  exec  to run a shell inside each of the two\\npods (one in each terminal). For example,  to run the shell in the pod in namespace\\nfoo, first get the name of the pod:\\n$ kubectl get po -n foo\\nNAME                   READY     STATUS    RESTARTS   AGEtest-145485760-ttq36    1/1       Running   0          1m\\nThen use the name in the kubectl  exec  command:\\n$ kubectl exec -it test-145485760-ttq36 -n foo sh\\n/ #\\nDo the same in the other terminal, but for the pod in the bar namespace.\\nLISTING  SERVICES  FROM YOUR PODS\\nTo verify that RBAC is enabled and preventi ng the pod from reading cluster state, use\\ncurl  to list Services in the foo namespace:\\n/ # curl localhost:8001/api/v1/namespaces/foo/services\\nUser \"system:serviceaccount:foo:default\" cannot list services in the \\nnamespace \"foo\".\\nYou’re connecting to localhost:8001 , which is where the kubectl  proxy  process is\\nlistening (as explained in chapter 8). The pr ocess received your re quest and sent it to\\nthe API server while authenticating as the default ServiceAccount in the foo name-\\nspace (as evident from the API server’s response). \\n The API server responded that the ServiceAccount isn’t allowed to list Services in\\nthe foo namespace, even though the pod is ru nning in that same namespace. You’re\\nseeing RBAC in action. The default permissi ons for a ServiceAccount don’t allow it to\\nlist or modify any resources. Now, let’s le arn how to allow the ServiceAccount to do\\nthat. First, you’ll need to create a Role resource.Listing 12.7 Running test pods in different namespaces'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 389}, page_content='358 CHAPTER  12 Securing the Kubernetes API server\\n12.2.3 Using Roles and RoleBindings\\nA Role resource defines what actions ca n be taken on which resources (or, as\\nexplained earlier, which types of HTTP re quests can be performed on which RESTful\\nresources). The following listing defi nes a Role, which allows users to get and list\\nServices in the foo namespace.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  namespace: foo              name: service-reader\\nrules:\\n- apiGroups: [\"\"]              verbs: [\"get\", \"list\"]     \\n  resources: [\"services\"]   \\nWARNING The plural form must be used when specifying resources.\\nThis Role resource will be created in the foo namespace. In chapter 8, you learned that\\neach resource type belongs to an API group, which you specify in the apiVersion  field\\n(along with the version) in the resource’s mani fest. In a Role definition, you need to spec-\\nify the apiGroup  for the resources listed in each rule included in the definition. If you’re\\nallowing access to resources belonging to di fferent API groups, you use multiple rules.\\nNOTE In the example, you’re allowing acce ss to all Service resources, but you\\ncould also limit access only to specific  Service instances by specifying their\\nnames through an additional resourceNames  field.\\nFigure 12.4 shows the Role, its verbs and re sources, and the namespace it will be cre-\\nated in.Listing 12.8 A definition of a Role : service-reader.yaml\\nRoles are namespaced (if namespace is \\nomitted, the current namespace is used).\\nServices are resources in the core apiGroup, \\nwhich has no name – hence the \"\".\\nGetting individual Services (by name) \\nand listing all of them is allowed.\\nThis rule pertains to services \\n(plural name must be used!).\\nAllows getting\\nAllows listingServicesRole:\\nservice-readerServicesNamespace: foo Namespace: bar\\nDoes not allow users to\\nget or list Services inother namespaces\\nFigure 12.4 The service-reader Role allows getting and listing Services in the foo namespace.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 390}, page_content='359 Securing the cluster with role-based access control\\nCREATING  A ROLE\\nCreate the previous Role in the foo namespace now:\\n$ kubectl create -f service-reader.yaml -n foo\\nrole \"service-reader\" created\\nNOTE The -n option is shorthand for --namespace .\\nNote that if you’re using GKE, the previous command may fail because you don’t have\\ncluster-admin rights. To grant the rights, run the following command:\\n$ kubectl create clusterrolebinding cluster-admin-binding \\n➥ --clusterrole=cluster-admin --user=your.email@address.com\\nInstead of creating the service-reader  Role from a YAML file, you could also create\\nit with the special kubectl  create  role  command. Let’s use this method to create the\\nRole in the bar namespace:\\n$ kubectl create role service-reader --verb=get --verb=list \\n➥ --resource=services -n bar\\nrole \"service-reader\" created\\nThese two Roles will allow you to list Services in the foo and bar namespaces from\\nwithin your two pods (running in the foo and bar namespace, respectively). But cre-\\nating the two Roles isn’t enough (you can check by executing the curl  command\\nagain). You need to bind each of the Role s to the ServiceAccounts in their respec-\\ntive namespaces. \\nBINDING  A ROLE TO A SERVICE ACCOUNT\\nA Role defines what actions can be performe d, but it doesn’t specify who can perform\\nthem. To do that, you must bind the Role to  a subject, which can be a user, a Service-\\nAccount, or a group (of us ers or ServiceAccounts).\\n Binding Roles to subjects is achieved by  creating a RoleBinding resource. To bind\\nthe Role to the default  ServiceAccount, run the following command:\\n$ kubectl create rolebinding test --role=service-reader \\n➥ --serviceaccount=foo:default -n foo\\nrolebinding \"test\" created\\nThe command should be self-explanatory. Yo u’re creating a RoleBinding, which binds\\nthe service-reader  Role to the default  ServiceAccount in namespace foo. You’re cre-\\nating the RoleBinding in namespace foo. The RoleBinding and the referenced Service-\\nAccount and Role are shown in figure 12.5.\\nNOTE To bind a Role to a user inst ead of a ServiceAccount, use the --user\\nargument to specify the username . To bind it to a group, use --group .'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 391}, page_content='360 CHAPTER  12 Securing the Kubernetes API server\\nThe following listing shows the YAML of the RoleBinding you created.\\n$ kubectl get rolebinding test -n foo -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBindingmetadata:\\n  name: test\\n  namespace: foo  ...\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io  kind: Role                         \\n  name: service-reader               \\nsubjects:\\n- kind: ServiceAccount       \\n  name: default                namespace: foo             \\nAs you can see, a RoleBinding always refere nces a single Role (as evident from the\\nroleRef  property), but can bind the Role to multiple subjects  (for example, one or\\nmore ServiceAccounts and any number of users or groups). Because this RoleBinding\\nbinds the Role to the ServiceAccount the pod in namespace foo is running under, you\\ncan now list Services from within that pod.\\n/ # curl localhost:8001/api/v1/namespaces/foo/services\\n{\\n  \"kind\": \"ServiceList\",\\n  \"apiVersion\": \"v1\",  \"metadata\": {\\n    \"selfLink\": \"/api/v1/namespaces/foo/services\",Listing 12.9 A RoleBinding referencing a Role\\nListing 12.10 Getting Services from the API serverNamespace: foo\\nRole:\\nservice-readerGet, list\\nDefault ServiceAccount\\nis allowed to get and list\\nservices in this namespaceServicesRoleBinding:\\ntestService-\\nAccount:\\ndefault\\nFigure 12.5 The test  RoleBinding binds the default  ServiceAccount with the \\nservice-reader  Role.\\nThis RoleBinding references \\nthe service-reader Role.\\nAnd binds it to the \\ndefault ServiceAccount \\nin the foo namespace.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 392}, page_content='361 Securing the cluster with role-based access control\\n    \"resourceVersion\": \"24906\"\\n  },\\n  \"items\": []     }\\nINCLUDING  SERVICE ACCOUNTS  FROM OTHER  NAMESPACES  IN A ROLEBINDING\\nThe pod in namespace bar can’t list the Services in its own namespace, and obviously\\nalso not those in the foo namespace. But you can edit your RoleBinding in the foo\\nnamespace and add the other pod’s ServiceAccount, even though it’s in a differentnamespace. Run the following command:\\n$ kubectl edit rolebinding test -n foo\\nThen add the following lines to the list of subjects , as shown in the following listing.\\nsubjects:\\n- kind: ServiceAccount\\n  name: default          \\n  namespace: bar         \\nNow you can also list Services in the foo namespace from inside the pod running in\\nthe bar namespace. Run the same command as in  listing 12.10, but do it in the other\\nterminal, where you’re running the shell in the other pod.\\n B e f o r e  m o v i n g  o n  t o  C l u s t e r R o l e s  and ClusterRoleBindings, let’s summarize\\nwhat RBAC resources you currently have . You have a RoleBinding in namespace\\nfoo, which references the service-reader  Role (also in the foo namespace) and\\nbinds the default  ServiceAccounts in both the foo and the bar namespaces, as\\ndepicted in figure 12.6.Listing 12.11 Referencing a ServiceAccount from another namespaceThe list of items is empty, \\nbecause no Services exist.\\nYou’re referencing the default \\nServiceAccount in the bar namespace.\\nNamespace: foo\\nRole:\\nservice-readerGet, list\\nBoth ServiceAccounts are\\nallowed to get and list Servicesin the foo namespaceServicesNamespace: bar\\nRoleBinding:\\ntestService-\\nAccount:\\ndefaultService-\\nAccount:\\ndefault\\nFigure 12.6 A RoleBinding binding ServiceAccount s from different namespaces to the same Role.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 393}, page_content='362 CHAPTER  12 Securing the Kubernetes API server\\n12.2.4 Using ClusterRoles and ClusterRoleBindings\\nRoles and RoleBindings are namespaced resources, meaning they reside in and apply\\nto resources in a single namespace, but, as  we saw, RoleBindings can refer to Service-\\nAccounts from other namespaces, too. \\n In addition to these namespaced resour ces, two cluster-level RBAC resources also\\nexist: ClusterRole and ClusterRoleBinding. They’re not namespaced. Let’s see why\\nyou need them.\\n A regular Role only allows access to re sources in the same namespace the Role is\\nin. If you want to allow someone access to  resources across different namespaces, you\\nhave to create a Role and RoleBinding in every one of those namespaces. If you want\\nto extend this to all namespaces (this is something a cluster administrator would prob-\\nably need), you need to create the same  Role and RoleBinding in each namespace.\\nWhen creating an additional namespace, you have to remember to create the two\\nresources there as well. \\n As you’ve learned throughout the book , certain resources aren’t namespaced at\\nall (this includes Nodes, PersistentVolumes, Namespaces, and so on). We’ve alsomentioned the API server exposes some URL paths that don’t represent resources\\n(\\n/healthz  for example). Regular Roles can’t grant access to those resources or non-\\nresource URLs, but ClusterRoles can.\\n A ClusterRole is a cluster-level resour ce for allowing access to non-namespaced\\nresources or non-resource URLs or used as a common role to be bound inside individ-\\nual namespaces, saving you from having to redefine the same role in each of them.\\nALLOWING  ACCESS  TO CLUSTER -LEVEL RESOURCES\\nAs mentioned, a ClusterRole can be used to allow access to cluster-level resources.\\nL e t ’ s  l o o k  a t  h o w  t o  a l l o w  y o u r  p o d  t o  l i s t  P e r s i s t e n t V o l u m e s  i n  y o u r  c l u s t e r .  F i r s t ,\\nyou’ll create a ClusterRole called pv-reader :\\n$ kubectl create clusterrole pv-reader --verb=get,list \\n➥ --resource=persistentvolumes\\nclusterrole \"pv-reader\" created\\nThe ClusterRole’s YAML is shown in the following listing.\\n$ kubectl get clusterrole pv-reader -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRolemetadata:                                       \\n  name: pv-reader                               \\n  resourceVersion: \"39932\"                        selfLink: ...                                 \\n  uid: e9ac1099-30e2-11e7-955c-080027e6b159     Listing 12.12 A ClusterRole definition\\nClusterRoles aren’t \\nnamespaced, hence no namespace field.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 394}, page_content='363 Securing the cluster with role-based access control\\nrules:\\n- apiGroups:                      \\n  - \"\"                              resources:                      \\n  - persistentvolumes             \\n  verbs:                            - get                           \\n  - list                          \\nBefore you bind this ClusterRole to your pod’s ServiceAccount, ve rify whether the pod\\ncan list PersistentVolumes. Run the follow ing command in the first terminal, where\\nyou’re running the shell inside the pod in the foo namespace:\\n/ # curl localhost:8001/api/v1/persistentvolumes\\nUser \"system:serviceaccount:foo:default\" cannot list persistentvolumes at the \\ncluster scope.\\nNOTE The URL contains no namespace, because PersistentVolumes aren’t\\nnamespaced. \\nAs expected, the default ServiceAccount ca n’t list PersistentVolumes. You need to\\nbind the ClusterRole to your ServiceAccount to allow it to do th at. ClusterRoles can\\nbe bound to subjects with regular RoleBi ndings, so you’ll create a RoleBinding now:\\n$ kubectl create rolebinding pv-test --clusterrole=pv-reader \\n➥ --serviceaccount=foo:default -n foo\\nrolebinding \"pv-test\" created\\nCan you list PersistentVolumes now?\\n/ # curl localhost:8001/api/v1/persistentvolumes\\nUser \"system:serviceaccount:foo:default\" cannot list persistentvolumes at the \\ncluster scope.\\nHmm, that’s strange. Let’s examine the RoleBinding’s YAML in the following listing.\\nCan you tell what (if anything) is wrong with it?\\n$ kubectl get rolebindings pv-test -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBindingmetadata:\\n  name: pv-test\\n  namespace: foo  ...\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole              \\n  name: pv-reader                Listing 12.13 A RoleBinding referencing a ClusterRoleIn this case, the \\nrules are exactly \\nlike those in a regular Role.\\nThe binding references the \\npv-reader ClusterRole.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 395}, page_content='364 CHAPTER  12 Securing the Kubernetes API server\\nsubjects:\\n- kind: ServiceAccount          \\n  name: default                   namespace: foo                \\nThe YAML looks perfectly fine. You’re re ferencing the correct ClusterRole and the\\ncorrect ServiceAccount, as shown in  figure 12.7, so what’s wrong?\\nAlthough you can create a RoleBinding and have it reference a ClusterRole when you\\nwant to enable access to namespaced resour ces, you can’t use the same approach for\\ncluster-level (non-namespaced) resources. To grant access to cluster-level resources,\\nyou must always use a ClusterRoleBinding.\\n Luckily, creating a ClusterRoleBinding is n’t that different from creating a Role-\\nBinding, but you’ll clean up and delete the RoleBinding first:\\n$ kubectl delete rolebinding pv-test\\nrolebinding \"pv-test\" deleted\\nNow create the ClusterRoleBinding:\\n$ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader \\n➥ --serviceaccount=foo:default\\nclusterrolebinding \"pv-test\" created\\nAs you can see, you replaced rolebinding  with clusterrolebinding  in the command\\nand didn’t (need to) specify the namespac e. Figure 12.8 shows what you have now.\\n Let’s see if you can list PersistentVolumes now:\\n/ # curl localhost:8001/api/v1/persistentvolumes\\n{\\n  \"kind\": \"PersistentVolumeList\",\\n  \"apiVersion\": \"v1\",...The bound subject is the \\ndefault ServiceAccount in \\nthe foo namespace.\\nNamespace: foo Cluster-level resources\\nClusterRole:\\npv-readerGet, listPersistent\\nVolumesRoleBinding:\\npv-test\\nDefault ServiceAccount\\nis unable to get and listPersistentVolumesService-\\nAccount:\\ndefault\\nFigure 12.7 A RoleBinding referencing a ClusterRole doesn’t grant access to cluster-\\nlevel resources.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 396}, page_content='365 Securing the cluster with role-based access control\\nYou can! It turns out you must use a Cl usterRole and a ClusterRoleBinding when\\ngranting access to cluster-level resources.\\nTIP Remember that a RoleBinding can’t grant access to cluster-level resources,\\neven if it references a ClusterRoleBinding.\\nALLOWING  ACCESS  TO NON-RESOURCE  URL S\\nWe’ve mentioned that the API server also exposes non-resource URLs. Access to these\\nURLs must also be granted explicitly; otherw ise the API server will reject the client’s\\nrequest. Usually, this is done for you automatically through the system:discovery\\nClusterRole and the identically named Cl usterRoleBinding, which appear among\\nother predefined ClusterRoles and Cluste rRoleBindings (we’ll explore them in sec-\\ntion 12.2.5). \\n Let’s inspect the system:discovery  ClusterRole shown in the following listing.\\n$ kubectl get clusterrole system:discovery -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRolemetadata:\\n  name: system:discovery\\n  ...rules:\\n- nonResourceURLs:      \\n  - /api                  - /api/*              \\n  - /apis               \\n  - /apis/*               - /healthz            \\n  - /swaggerapi         \\n  - /swaggerapi/*         - /version            Listing 12.14 The default system:discovery  ClusterRoleNamespace: foo Cluster-level resources\\nClusterRole:\\npv-readerGet, listPersistent\\nVolumesClusterRoleBinding:\\npv-test\\nDefault ServiceAccount in\\nfoo namespace is now allowedto get and list PersistentVolumesService-\\nAccount:\\ndefault\\nFigure 12.8 A ClusterRoleBinding and ClusterRole must be used to grant access to cluster-\\nlevel resources.\\nInstead of referring \\nto resources, this rule \\nrefers to non-resource \\nURLs.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 397}, page_content='366 CHAPTER  12 Securing the Kubernetes API server\\n  verbs:             \\n  - get              \\nYou can see this ClusterRole refers to  URLs instead of resources (field nonResource-\\nURLs  is used instead of the resources  field). The verbs  field only allows the GET HTTP\\nmethod to be used on these URLs.\\nNOTE For non-resource URLs, plain HTTP verbs such as post , put, and\\npatch  are used instead of create  or update . The verbs need to be specified in\\nlowercase.\\nAs with cluster-level resources, ClusterRol es for non-resource URLs must be bound\\nwith a ClusterRoleBinding. Binding them wi th a RoleBinding won’t have any effect.\\nThe system:discovery  ClusterRole has a corresponding system:discovery Cluster-\\nRoleBinding, so let’s see what’s in it by examining the following listing.\\n$ kubectl get clusterrolebinding system:discovery -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBindingmetadata:\\n  name: system:discovery\\n  ...roleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole                             name: system:discovery                      \\nsubjects:\\n- apiGroup: rbac.authorization.k8s.io\\n  kind: Group                                 \\n  name: system:authenticated                  - apiGroup: rbac.authorization.k8s.io\\n  kind: Group                                 \\n  name: system:unauthenticated                \\nThe YAML shows the ClusterRoleBinding refers to the system:discovery  ClusterRole,\\nas expected. It’s bound to two groups, system:authenticated  and system:unauthenti-\\ncated , which makes it bound to all users. Th is means absolutely everyone can access\\nthe URLs listed in the ClusterRole. \\nNOTE Groups are in the domain of th e authentication plugin. When a\\nrequest is received by the API server, it calls the authentication plugin to\\nobtain the list of groups the user belongs to. This information is then usedin authorization.\\nYou can confirm this by accessing the \\n/api  URL path from inside the pod (through\\nthe kubectl  proxy , which means you’ll be authenticated as the pod’s ServiceAccount)Listing 12.15 The default system:discovery  ClusterRoleBindingOnly the HTTP GET method \\nis allowed for these URLs.\\nThis ClusterRoleBinding references \\nthe system:discovery ClusterRole.\\nIt binds the ClusterRole \\nto all authenticated and unauthenticated users (that is, everyone).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 398}, page_content='367 Securing the cluster with role-based access control\\nand from your local machine, without specifying any authentication tokens (making\\nyou an unauthenticated user):\\n$ curl https://$(minikube ip):8443/api -k\\n{\\n  \"kind\": \"APIVersions\",\\n  \"versions\": [  ...\\nYou’ve now used ClusterRoles and ClusterRol eBindings to grant access to cluster-level\\nresources and non-resource URLs. Now let’ s look at how ClusterRoles can be used\\nwith namespaced RoleBindings to grant ac cess to namespaced resources in the Role-\\nBinding’s namespace.\\nUSING CLUSTER ROLES TO GRANT  ACCESS  TO RESOURCES  IN SPECIFIC  NAMESPACES\\nClusterRoles don’t always need to be bo und with cluster-level ClusterRoleBindings.\\nThey can also be bound with regular, namespaced RoleBindings. You’ve already\\nstarted looking at predefined ClusterRol es, so let’s look at another one called view ,\\nwhich is shown in the following listing.\\n$ kubectl get clusterrole view -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:  name: view\\n  ...\\nrules:- apiGroups:\\n  - \"\"\\n  resources:                           \\n  - configmaps                         \\n  - endpoints                            - persistentvolumeclaims             \\n  - pods                               \\n  - replicationcontrollers               - replicationcontrollers/scale       \\n  - serviceaccounts                    \\n  - services                             verbs:                \\n  - get                 \\n  - list                  - watch               \\n...\\nThis ClusterRole has many rules. Only the first one is shown in the listing. The rule\\nallows getting, listing, and watching reso urces like ConfigMaps, Endpoints, Persistent-\\nVolumeClaims, and so on. These are namespaced resources, even though you’re\\nlooking at a ClusterRole (not a regular, namespaced Role). What exactly does this\\nClusterRole do?Listing 12.16 The default view  ClusterRole\\nThis rule applies to \\nthese resources (note: they’re all namespaced \\nresources).\\nAs the ClusterRole’s name \\nsuggests, it only allows reading, not writing the \\nresources listed.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 399}, page_content='368 CHAPTER  12 Securing the Kubernetes API server\\n It depends whether it’s bound with a Cl usterRoleBinding or a RoleBinding (it can\\nbe bound with either). If you create a Cl usterRoleBinding and reference the Cluster-\\nRole in it, the subjects listed in the bind ing can view the specified resources across all\\nnamespaces. If, on the other hand, you create a RoleBinding, the subjects listed in the\\nbinding can only view resources in the name space of the RoleBind ing. You’ll try both\\noptions now.\\n You’ll see how the two options affect your te st pod’s ability to list pods. First, let’s\\nsee what happens before any bindings are in place:\\n/ # curl localhost:8001/api/v1/pods\\nUser \"system:serviceaccount:foo:default\" cannot list pods at the cluster \\nscope./ #\\n/ # curl localhost:8001/api/v1/namespaces/foo/pods\\nUser \"system:serviceaccount:foo:default\" cannot list pods in the namespace \\n\"foo\".\\nWith the first command, you’re trying to list pods across all namespaces. With the sec-\\nond, you’re trying to list pods in the foo namespace. The server doesn’t allow you to\\ndo either.\\n Now, let’s see what happens when you create a ClusterRoleBinding and bind it to\\nthe pod’s ServiceAccount:\\n$ kubectl create clusterrolebinding view-test --clusterrole=view \\n➥ --serviceaccount=foo:default\\nclusterrolebinding \"view-test\" created\\nCan the pod now list pods in the foo namespace?\\n/ # curl localhost:8001/api/v1/namespaces/foo/pods{\\n  \"kind\": \"PodList\",\\n  \"apiVersion\": \"v1\",\\n  ...\\nIt can! Because you created a ClusterRoleB inding, it applies across all namespaces.\\nThe pod in namespace foo can list pods in the bar namespace as well:\\n/ # curl localhost:8001/api/v1/namespaces/bar/pods\\n{\\n  \"kind\": \"PodList\",\\n  \"apiVersion\": \"v1\",  ...\\nOkay, the pod is allowed to list pods in a di fferent namespace. It can also retrieve pods\\nacross all namespaces by hitting the /api/v1/pods URL path:\\n/ # curl localhost:8001/api/v1/pods\\n{\\n  \"kind\": \"PodList\",  \"apiVersion\": \"v1\",\\n  ...'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 400}, page_content='369 Securing the cluster with role-based access control\\nAs expected, the pod can get a list of all th e pods in the cluster. To summarize, com-\\nbining a ClusterRoleBinding with a ClusterRole referring to namespaced resources\\nallows the pod to access namespaced resour ces in any namespace, as shown in fig-\\nure 12.9.\\nNow, let’s see what happens if you replace the ClusterRoleBinding with a RoleBinding.\\nFirst, delete the ClusterRoleBinding:\\n$ kubectl delete clusterrolebinding view-test\\nclusterrolebinding \"view-test\" deleted\\nNext create a RoleBinding instead. Beca use a RoleBinding is namespaced, you need\\nto specify the namespace you want to  create it in. Create it in the foo namespace:\\n$ kubectl create rolebinding view-test --clusterrole=view \\n➥ --serviceaccount=foo:default -n foo\\nrolebinding \"view-test\" created\\nYou now have a RoleBinding in the foo namespace, binding the default  Service-\\nAccount in that same namespace with the view  ClusterRole. What can your pod\\naccess now?\\n/ # curl localhost:8001/api/v1/namespaces/foo/pods\\n{  \"kind\": \"PodList\",\\n  \"apiVersion\": \"v1\",\\n  ...Namespace: foo\\nCluster-level\\nresourcesNamespace: bar\\nPods Pods\\nDefault\\nServiceAccountin foo namespaceis allowed toview pods inany namespace\\nClusterRole:\\nviewAllows getting,\\nlisting, watchingClusterRoleBinding:\\nview-testPods,\\nServices,\\nEndpoints,\\nConﬁgMaps,\\n…Service-\\nAccount:\\ndefault\\nFigure 12.9 A ClusterRoleBinding and ClusterRol e grants permission to resources across all \\nnamespaces.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 401}, page_content='370 CHAPTER  12 Securing the Kubernetes API server\\n/ # curl localhost:8001/api/v1/namespaces/bar/pods\\nUser \"system:serviceaccount:foo:default\" cannot list pods in the namespace \\n\"bar\".\\n/ # curl localhost:8001/api/v1/pods\\nUser \"system:serviceaccount:foo:default\" cannot list pods at the cluster \\nscope.\\nAs you can see, your pod can list pods in the foo namespace, but not in any other spe-\\ncific namespace or across all namespaces. This is visualized in figure 12.10.\\nSUMMARIZING  ROLE, CLUSTER ROLE, ROLEBINDING , AND CLUSTER ROLEBINDING  COMBINATIONS\\nWe’ve covered many different combinations and it may be hard for you to remember\\nwhen to use each one. Let’s see if we can make sense of all these combinations by cat-egorizing them per specific us e case. Refer to table 12.2.\\nTable 12.2 When to use specific combinations of role and binding types\\nFor accessing Role type to use Binding type to use\\nCluster-level resources (Node s, PersistentVolumes, ...) ClusterRole ClusterRoleBinding\\nNon-resource URLs ( /api , /healthz , ...) ClusterRole ClusterRoleBinding\\nNamespaced resources in any namespace (and \\nacross all namespaces)ClusterRole ClusterRoleBinding\\nNamespaced resources in a specific namespace (reus-\\ning the same ClusterR ole in multiple namespaces)ClusterRole RoleBinding\\nNamespaced resources in a specific namespace \\n(Role must be defined in each namespace)Role RoleBindingNamespace: foo\\nCluster-level resourcesNamespace: bar\\nPods Pods\\nClusterRole:\\nviewAllows getting,\\nlisting, watchingRoleBinding:\\nview-test\\nPods,\\nServices,\\nEndpoints,\\nConﬁgMaps,\\n…Default ServiceAccount in\\nfoo namespace is only allowedto view pods in namespace foo,despite using a ClusterRoleService-\\nAccount:\\ndefault\\nFigure 12.10 A RoleBinding referring to a Cluste rRole only grants access to resources inside the \\nRoleBinding’s namespace.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 402}, page_content='371 Securing the cluster with role-based access control\\nHopefully, the relationships between the four RBAC resources are much clearer\\nnow. Don’t worry if you still feel like you don’t yet grasp everything. Things may\\nclear up as we explore the pre-configured  ClusterRoles and ClusterRoleBindings in\\nthe next section.\\n12.2.5 Understanding default ClusterRoles and ClusterRoleBindings\\nKubernetes comes with a default set of Cl usterRoles and ClusterRoleBindings, which\\nare updated every time the API server star ts. This ensures all the default roles and\\nbindings are recreated if you mistakenly dele te them or if a newer version of Kuberne-\\ntes uses a different configuration of cluster roles and bindings.\\n You can see the default cluster roles and bindings in the following listing.\\n$ kubectl get clusterrolebindings\\nNAME                                           AGE\\ncluster-admin                                  1d\\nsystem:basic-user                              1dsystem:controller:attachdetach-controller      1d\\n...\\nsystem:controller:ttl-controller               1dsystem:discovery                               1d\\nsystem:kube-controller-manager                 1d\\nsystem:kube-dns                                1dsystem:kube-scheduler                          1d\\nsystem:node                                    1d\\nsystem:node-proxier                            1d\\n$ kubectl get clusterroles\\nNAME                                           AGE\\nadmin                                          1d\\ncluster-admin                                  1d\\nedit                                           1dsystem:auth-delegator                          1d\\nsystem:basic-user                              1d\\nsystem:controller:attachdetach-controller      1d...\\nsystem:controller:ttl-controller               1d\\nsystem:discovery                               1dsystem:heapster                                1d\\nsystem:kube-aggregator                         1d\\nsystem:kube-controller-manager                 1dsystem:kube-dns                                1d\\nsystem:kube-scheduler                          1d\\nsystem:node                                    1dsystem:node-bootstrapper                       1d\\nsystem:node-problem-detector                   1d\\nsystem:node-proxier                            1dsystem:persistent-volume-provisioner           1d\\nview                                           1dListing 12.17 Listing all ClusterRoleBinding s and ClusterRole s'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 403}, page_content='372 CHAPTER  12 Securing the Kubernetes API server\\nThe most important roles are the view , edit , admin , and cluster-admin  ClusterRoles.\\nThey’re meant to be bound to ServiceA ccounts used by user-defined pods.\\nALLOWING  READ-ONLY ACCESS  TO RESOURCES  WITH THE VIEW CLUSTER ROLE\\nYou already used the default view  ClusterRole in the previous example. It allows read-\\ning most resources in a namespace, except for Roles, RoleBindings, and Secrets. You’re\\nprobably wondering, why not Secrets? Beca use one of those Secrets might include an\\nauthentication token with greater pr ivileges than those defined in the view  Cluster-\\nRole and could allow the user to masquerade  as a different user to gain additional\\nprivileges (privilege escalation). \\nALLOWING  MODIFYING  RESOURCES  WITH THE EDIT CLUSTER ROLE\\nNext is the edit  ClusterRole, which allows you to modify resources in a namespace,\\nbut also allows both reading and modifying Se crets. It doesn’t, however, allow viewing\\nor modifying Roles or RoleBindings—again, this is to prevent privilege escalation.\\nGRANTING  FULL CONTROL  OF A NAMESPACE  WITH THE ADMIN  CLUSTER ROLE\\nComplete control of the resources in a namespace is granted in the admin  Cluster-\\nRole. Subjects with this ClusterRole can re ad and modify any resource in the name-\\nspace, except ResourceQuotas (we’ll lear n what those are in chapter 14) and the\\nNamespace resource itself. The main difference between the edit  and the admin  Cluster-\\nRoles is in the ability to view and modify  Roles and RoleBindings in the namespace.\\nNOTE To prevent privilege escalation, the AP I server only allows users to cre-\\nate and update Roles if they already have all the permissions listed in thatRole (and for the same scope). \\nALLOWING  COMPLETE  CONTROL  WITH THE CLUSTER -ADMIN  CLUSTER ROLE \\nComplete control of the Kubernetes cl uster can be given by assigning the cluster-\\nadmin  ClusterRole to a subject. As you’ve seen before, the admin  ClusterRole doesn’t\\nallow users to modify the namespace’s ResourceQuota objects or the Namespace\\nresource itself. If you want to allow a user to do that, you need to create a RoleBinding\\nthat references the cluster-admin  ClusterRole. This gives the user included in the\\nRoleBinding complete control over all aspe cts of the namespace in which the Role-\\nBinding is created.\\n If you’ve paid attention, you probably  already know how to give users complete\\ncontrol of all the namespaces in the cluster. Yes, by referencing the cluster-admin\\nClusterRole in a ClusterRoleBinding instead of a RoleBinding.\\nUNDERSTANDING  THE OTHER  DEFAULT  CLUSTER ROLES\\nThe list of default ClusterRoles includes a large number of other ClusterRoles, which\\nstart with the system:  prefix. These are meant to be used by the various Kubernetes\\ncomponents. Among them, you’ll find roles such as system:kube-scheduler , which\\nis obviously used by the Scheduler, system:node , which is used by the Kubelets, and\\nso on.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 404}, page_content='373 Summary\\n Although the Controller Manager runs as  a single pod, each  controller running\\ninside it can use a separate ClusterRole and ClusterRoleBinding (they’re prefixed\\nwith system: controller: ). \\n Each of these system ClusterRoles has a matching ClusterRoleBinding, which binds\\nit to the user the system component authenticates as. The system:kube-scheduler\\nClusterRoleBinding, for example, assigns the identically named ClusterRole to the\\nsystem:kube-scheduler  user, which is the username the scheduler Authenticates as. \\n12.2.6 Granting authorization permissions wisely\\nBy default, the default ServiceAccount in a namespace has no permissions other than\\nthose of an unauthenticated user (as yo u may remember from one of the previous\\nexamples, the system:discovery  ClusterRole and associated binding allow anyone to\\nmake GET requests on a few non-resource UR Ls). Therefore, pods, by default, can’t\\neven view cluster state. It’s up to you to grant them appropriate permissions to do that. \\n Obviously, giving all your ServiceAccounts the cluster-admin  ClusterRole is a\\nbad idea. As is always the case with security, it’s best to give everyone only the permis-sions they need to do their job and not a single permission more ( principle of least\\nprivilege ).\\nCREATING  SPECIFIC  SERVICE ACCOUNTS  FOR EACH POD\\nIt’s a good idea to create a specific Serv iceAccount for each pod (or a set of pod rep-\\nlicas) and then associate it with a tail or-made Role (or a ClusterRole) through a\\nRoleBinding (not a ClusterRoleBinding, be cause that would give the pod access to\\nresources in other namespaces, which is probably not what you want). \\n If one of your pods (the application ru nning within it) only needs to read pods,\\nwhile the other also needs to modify them , then create two different ServiceAccounts\\nand make those pods use them by specifying the serviceAccountName  property in the\\npod spec, as you learned in the first part of this chapter. Don’t add all the necessary\\npermissions required by both pods to the default ServiceAccount in the namespace. \\nEXPECTING  YOUR APPS TO BE COMPROMISED\\nYour aim is to reduce the possibility of an in truder getting hold of your cluster. Today’s\\ncomplex apps contain many vulnerabilities.  You should expect unwanted persons to\\neventually get their hands on the ServiceAcco unt’s authentication token, so you should\\nalways constrain the ServiceAccount to pr event them from doing any real damage.\\n12.3 Summary\\nThis chapter has given you a foundation on how to secure the Kubernetes API server.\\nYou learned the following:\\n\\uf0a1Clients of the API server include bot h human users and applications running\\nin pods.\\n\\uf0a1Applications in pods are associated with a ServiceAccount. \\n\\uf0a1Both users and ServiceAccounts are associated with groups.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 405}, page_content='374 CHAPTER  12 Securing the Kubernetes API server\\n\\uf0a1By default, pods run under the default ServiceAccount, which is created for\\neach namespace automatically.\\n\\uf0a1Additional ServiceAccounts can be created manually and associated with a pod.\\n\\uf0a1ServiceAccounts can be configured to allow mounting only a constrained list ofSecrets in a given pod.\\n\\uf0a1A ServiceAccount can also be used to atta ch image pull Secrets to pods, so you\\ndon’t need to specify the Secrets in every pod.\\n\\uf0a1Roles and ClusterRoles define what acti ons can be performed on which resources.\\n\\uf0a1RoleBindings and ClusterRoleBindings bind Roles and ClusterRoles to users,\\ngroups, and ServiceAccounts.\\n\\uf0a1Each cluster comes with default Cl usterRoles and ClusterRoleBindings.\\nIn the next chapter, you’ll learn how to protect the cluster nodes from pods and how\\nto isolate pods from each other by securing the network.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 406}, page_content='375Securing cluster nodes\\nand the network\\nIn the previous chapter, we talked abou t securing the API server. If an attacker\\ngets access to the API server, they can ru n whatever they like by packaging their\\ncode into a container image and running it in a pod. But can they do any real\\ndamage? Aren’t containers isolated from other containers and from the node\\nthey’re running on? \\n Not necessarily. In this chapter, you’ll learn how to allow pods to access the\\nresources of the node they’re running on . You’ll also learn how to configure the\\ncluster so users aren’t able to do what ever they want with their pods. Then, inThis chapter covers\\n\\uf0a1Using the node’s default Linux namespaces \\nin pods\\n\\uf0a1Running containers as different users\\n\\uf0a1Running privileged containers\\n\\uf0a1Adding or dropping a container’s kernel capabilities\\n\\uf0a1Defining security policies to limit what pods can do\\n\\uf0a1Securing the pod network'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 407}, page_content='376 CHAPTER  13 Securing cluster nodes and the network\\nthe last part of the chapter, you’ll also learn how to secure the network the pods use\\nto communicate.\\n13.1 Using the host node’s namespaces in a pod\\nContainers in a pod usually run under se parate Linux namespaces, which isolate\\ntheir processes from processes running in ot her containers or in the node’s default\\nnamespaces. \\n For example, we learned that each pod ge ts its own IP and port space, because it\\nuses its own network namespace. Likewise, each pod has its own process tree, because\\nit has its own PID namespace, and it also uses its own IPC namespace, allowing only\\nprocesses in the same pod to communicate with each other through the Inter-Process\\nCommunication mechanism (IPC).\\n13.1.1 Using the node’s network namespace in a pod\\nCertain pods (usually system pods) need to  operate in the host’s default namespaces,\\nallowing them to see and manipulate node-lev el resources and devices. For example, a\\npod may need to use the node’s network ad apters instead of its own virtual network\\nadapters. This can be achieved by setting the hostNetwork  property in the pod spec\\nto true .\\n In that case, the pod gets to use the node’s network interfaces instead of having its\\nown set, as shown in figure 13.1. This mean s the pod doesn’t get its own IP address and\\nif it runs a process that binds to a port, the process will be bound to the node’s port.\\nYou can try running such a pod. The next listing shows an example pod manifest.\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: pod-with-host-networkListing 13.1 A pod using the node’s netwo rk namespace: pod-with-host-network.yamlNode\\nPod APod’s own network\\nnamespace\\neth0 lo\\neth0 docker0 lo eth1Node’s default network\\nnamespace\\nPod B\\nhostNetwork: trueFigure 13.1 A pod \\nwith hostNetwork:  \\ntrue  uses the node’s \\nnetwork interfaces \\ninstead of its own.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 408}, page_content='377 Using the host node’s namespaces in a pod\\nspec:\\n  hostNetwork: true              \\n  containers:  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\nAfter you run the pod, you can use the following command to see that it’s indeed using\\nthe host’s network namespace (it sees all the host’s network adapters, for example).\\n$ kubectl exec pod-with-host-network ifconfig\\ndocker0   Link encap:Ethernet  HWaddr 02:42:14:08:23:47\\n          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0\\n          ...\\neth0      Link encap:Ethernet  HWaddr 08:00:27:F8:FA:4E\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\n          ...\\nlo        Link encap:Local Loopback\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\n          ...\\nveth1178d4f Link encap:Ethernet  HWaddr 1E:03:8D:D6:E1:2C\\n          inet6 addr: fe80::1c03:8dff:fed6:e12c/64 Scope:Link\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1...\\nWhen the Kubernetes Control Plane components are deployed as pods (such as when\\nyou deploy your cluster with kubeadm , as explained in appendix B), you’ll find that\\nthose pods use the hostNetwork  option, effectively making them behave as if they\\nweren’t running inside a pod.\\n13.1.2 Binding to a host port without using the host’s network \\nnamespace\\nA related feature allows pods to bind to a port in the node’s default namespace, but\\nstill have their own network namespace. This is done by using the hostPort  property\\nin one of the container’s ports defined in the spec.containers.ports  field.\\n Don’t confuse pods using hostPort  with pods exposed through a NodePort  service.\\nThey’re two different things, as explained in figure 13.2.\\n The first thing you’ll notice in the fi gure is that when a pod is using a hostPort , a\\nconnection to the node’s port is forwarded directly to the pod running on that node,\\nwhereas with a NodePort  service, a connection to the no de’s port is forwarded to a\\nrandomly selected pod (possibly on another node). The other difference is that with\\npods using a hostPort , the node’s port is only bound on nodes that run such pods,\\nwhereas NodePort  services bind the port on all nodes, even on those that don’t run\\nsuch a pod (as on node 3 in the figure).Listing 13.2 Network interfaces in a pod using the host’s network namespaceUsing the host node’s \\nnetwork namespace'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 409}, page_content='378 CHAPTER  13 Securing cluster nodes and the network\\nIt’s important to understand that if a po d is using a specific host port, only one\\ninstance of the pod can be scheduled to each node, because two processes can’t bind\\nto the same host port. The Scheduler takes this into account when scheduling pods, so\\nit doesn’t schedule multiple pods to the same node, as shown in figure 13.3. If you\\nhave three nodes and want to deploy four  pod replicas, only three will be scheduled\\n(one pod will remain Pending).Node 1\\nPod 1Two pods using\\nhostPort\\nPort\\n8080Port\\n9000Node 2\\nPod 2 Port\\n8080Port\\n9000Node 3\\nNode 1\\nPod 1Two pods under\\nthe same\\nNodePort\\nservice\\nPort\\n8080Node 2\\nPod 2 Port\\n8080Node 3\\nPort\\n88Port\\n88Port88\\nService\\n()iptablesService\\n()iptablesService\\n()iptables\\nFigure 13.2 Difference between pods using a hostPort  and pods behind a NodePort  service.\\nNode 1\\nPod 1 Port\\n8080Host\\nport9000Host\\nport9000\\nPod 2 Port\\n8080Node 2\\nPod 3 Port\\n8080Hostport9000Node 3\\nPod 4 Port\\n8080\\nCannot be scheduled to the same\\nnode, because the port is already boundOnly a single\\nreplica per node\\nFigure 13.3 If a host port is used, only a single pod instance can be scheduled to a node.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 410}, page_content='379 Using the host node’s namespaces in a pod\\nLet’s see how to define the hostPort  in a pod’s YAML definition. The following listing\\nshows the YAML to run your kubia  pod and bind it to the node’s port 9000.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: kubia-hostportspec:\\n  containers:\\n  - image: luksa/kubia    name: kubia\\n    ports:\\n    - containerPort: 8080          hostPort: 9000        \\n      protocol: TCP\\nAfter you create this pod, you can access it  through port 9000 of the node it’s sched-\\nuled to. If you have multiple nodes, you’ ll see you can’t access the pod through that\\nport on the other nodes. \\nNOTE If you’re trying this on GKE, you need to configure the firewall prop-\\nerly using gcloud  compute  firewall-rules , the way you did in chapter 5.\\nThe hostPort  feature is primarily used for exposing system services, which are\\ndeployed to every node using DaemonSets. Init ially, people also used it to ensure two\\nreplicas of the same pod were never schedu led to the same node, but now you have a\\nbetter way of achieving this—it’s explained in chapter 16.\\n13.1.3 Using the node’s PID and IPC namespaces\\nSimilar to the hostNetwork  option are the hostPID  and hostIPC  pod spec properties.\\nWhen you set them to true , the pod’s containers will use the node’s PID and IPC\\nnamespaces, allowing processes running in the containers to see all the other pro-\\ncesses on the node or communicate with th em through IPC, respectively. See the fol-\\nlowing listing for an example.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-host-pid-and-ipcspec:\\n  hostPID: true                    \\n  hostIPC: true                       containers:\\n  - name: main\\n    image: alpine    command: [\"/bin/sleep\", \"999999\"]Listing 13.3 Binding a pod to a port in the node’s port space: kubia-hostport.yaml\\nListing 13.4 Using the host’s PID and IPC namespaces: pod-with-host-pid-and-ipc.yamlThe container can be \\nreached on port 8080 \\nof the pod’s IP.\\nIt can also be reached \\non port 9000 of the \\nnode it’s deployed on.\\nYou want the pod to \\nuse the host’s PID \\nnamespace.\\nYou also want the \\npod to use the host’s \\nIPC namespace.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 411}, page_content='380 CHAPTER  13 Securing cluster nodes and the network\\nYou’ll remember that pods usually see only th eir own processes, but if you run this pod\\nand then list the processes from within its container, you’ll see all the processes run-\\nning on the host node, not only the ones running in the container, as shown in the\\nfollowing listing.\\n$ kubectl exec pod-with-host-pid-and-ipc ps aux\\nPID   USER     TIME   COMMAND    1 root       0:01 /usr/lib/systemd/systemd --switched-root --system ...\\n    2 root       0:00 [kthreadd]\\n    3 root       0:00 [ksoftirqd/0]    5 root       0:00 [kworker/0:0H]\\n    6 root       0:00 [kworker/u2:0]\\n    7 root       0:00 [migration/0]    8 root       0:00 [rcu_bh]\\n    9 root       0:00 [rcu_sched]\\n   10 root       0:00 [watchdog/0]...\\nBy setting the hostIPC  property to true , processes in the pod’s containers can also\\ncommunicate with all the other processes ru nning on the node, through Inter-Process\\nCommunication.\\n13.2 Configuring the container’s security context\\nBesides allowing the pod to use the host’s  Linux namespaces, other security-related\\nfeatures can also be configured on the pod and its container through the security-\\nContext  properties, which can be specified unde r the pod spec directly and inside the\\nspec of individual containers.\\nUNDERSTANDING  WHAT’S CONFIGURABLE  IN THE SECURITY  CONTEXT\\nConfiguring the security context allows you to do various things:\\n\\uf0a1Specify the user (the user’s ID) under which the process in the container will run.\\n\\uf0a1Prevent the container from running as ro ot (the default user a container runs\\nas is usually defined in the container im age itself, so you may want to prevent\\ncontainers from running as root).\\n\\uf0a1Run the container in privileged mode, giving  it full access to the node’s kernel.\\n\\uf0a1Configure fine-grained privileges, by ad ding or dropping capabilities—in con-\\ntrast to giving the container all possible permissions by running it in privi-\\nleged mode.\\n\\uf0a1Set SELinux (Security Enhanced Linux)  o p t i o n s  t o  s t r o n g l y  l o c k  d o w n  a\\ncontainer.\\n\\uf0a1Prevent the process from writing to the container’s filesystem.\\nWe’ll explore these options next. Listing 13.5 Processes visible in a pod with hostPID:  true'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 412}, page_content='381 Configuring the container’s security context\\nRUNNING  A POD WITHOUT  SPECIFYING  A SECURITY  CONTEXT\\nFirst, run a pod with the default security context options (by not specifying them at\\nall), so you can see how it behaves compared  to pods with a custom security context:\\n$ kubectl run pod-with-defaults --image alpine --restart Never \\n➥  -- /bin/sleep 999999\\npod \"pod-with-defaults\" created\\nLet’s see what user and group ID the cont ainer is running as, and which groups it\\nbelongs to. You can see this by running the id command inside the container:\\n$ kubectl exec pod-with-defaults id\\nuid=0(root) gid=0(root) groups=0(root), 1(bin), 2(daemon), 3(sys), 4(adm), \\n6(disk), 10(wheel), 11(floppy), 20(dialout), 26(tape), 27(video)\\nThe container is running as user ID ( uid) 0, which is root , and group ID ( gid) 0 (also\\nroot ). It’s also a member of multiple other groups. \\nNOTE What user the container runs as is specified in the container image. In\\na Dockerfile, this is done using the USER  directive. If omitted, the container\\nruns as root.\\nNow, you’ll run a pod where the cont ainer runs as a different user.\\n13.2.1 Running a container as a specific user\\nTo run a pod under a different user ID than  the one that’s baked into the container\\nimage, you’ll need to set the pod’s securityContext.runAsUser  property. You’ll\\nmake the container run as user guest , whose user ID in the alpine container image is\\n405, as shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: pod-as-user-guest\\nspec:\\n  containers:  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]    securityContext:\\n      runAsUser: 405      \\nNow, to see the effect of the runAsUser  property, run the id command in this new\\npod, the way you did before:\\n$ kubectl exec pod-as-user-guest id\\nuid=405(guest) gid=100(users)Listing 13.6 Running containers as a specific user: pod-as-user-guest.yaml\\nYou need to specify a user ID, not \\na username (id 405 corresponds \\nto the guest user).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 413}, page_content='382 CHAPTER  13 Securing cluster nodes and the network\\nAs requested, the container is running as the guest  user. \\n13.2.2 Preventing a container from running as root\\nWhat if you don’t care what user the contai ner runs as, but you still want to prevent it\\nfrom running as root? \\n Imagine having a pod deployed with a container image that was built with a USER\\ndaemon  directive in the Dockerfile, which makes the container run under the daemon\\nuser. What if an attacker gets access to your image registry and pushes a different\\nimage under the same tag? The attacker’s imag e is configured to run as the root user.\\nWhen Kubernetes schedules a new instance of your pod, the Kubelet will download\\nthe attacker’s image and run whatever code they put into it. \\n Although containers are mostly isolated  from the host system, running their pro-\\ncesses as root is still considered a bad pr actice. For example, when a host directory is\\nmounted into the container, if the process running in the container is running asroot, it has full access to the mounted direct ory, whereas if it’s running as non-root,\\nit won’t. \\n To prevent the attack scenario described previously, you can specify that the pod’s\\ncontainer needs to run as a non-root us er, as shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-run-as-non-rootspec:\\n  containers:\\n  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]    securityContext:                   \\n      runAsNonRoot: true               \\nIf you deploy this pod, it gets scheduled, but is not allowed to run:\\n$ kubectl get po pod-run-as-non-root\\nNAME                 READY  STATUS                                                  \\npod-run-as-non-root  0/1    co ntainer has runAsNonRoot and image will run \\n                            ➥  as root\\nNow, if anyone tampers with your container images, they won’t get far.\\n13.2.3 Running pods in privileged mode\\nSometimes pods need to do everything that  the node they’re running on can do, such\\nas use protected system devices or other kernel features, which aren’t accessible to\\nregular containers. Listing 13.7 Preventing containers from r unning as root: pod-run-as-non-root.yaml\\nThis container will only \\nbe allowed to run as a non-root user.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 414}, page_content='383 Configuring the container’s security context\\n An example of such a pod is the kube-proxy pod, which needs to modify the node’s\\niptables  rules to make services work, as was explained in chapter 11. If you follow the\\ninstructions in appendix B and deploy a cluster with kubeadm , you’ll see every cluster\\nnode runs a kube-proxy pod and you can exam ine its YAML specification to see all the\\nspecial features it’s using. \\n To get full access to the node’s kernel , the pod’s container runs in privileged\\nmode. This is achieved by setting the privileged  property in the container’s security-\\nContext  property to true . You’ll create a privileged pod from the YAML in the follow-\\ning listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: pod-privileged\\nspec:\\n  containers:  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]    securityContext:\\n      privileged: true     \\nGo ahead and deploy this pod, so you can compare it with the non-privileged pod you\\nran earlier. \\n If you’re familiar with Linux, you may kn ow it has a special file directory called /dev,\\nwhich contains device files for all the device s on the system. These aren’t regular files on\\ndisk, but are special files used  to communicate with devices. Let’s see what devices are\\nvisible in the non-privileged container you deployed earlier (the pod-with-defaults\\npod), by listing files in its /dev direct ory, as shown in the following listing.\\n$ kubectl exec -it pod-with-defaults ls /dev\\ncore             null             stderr           urandom\\nfd               ptmx             stdin            zerofull             pts              stdout\\nfuse             random           termination-log\\nmqueue           shm              tty\\nThe listing shows all the devices. The list is fairly short. Now, compare this with the fol-\\nlowing listing, which shows the device  files your privileged pod can see.\\n$ kubectl exec -it pod-privileged ls /dev\\nautofs              snd                 tty46bsg                 sr0                 tty47Listing 13.8 A pod with a privileged container: pod-privileged.yaml\\nListing 13.9 List of available devices in a non-privileged pod\\nListing 13.10 List of available devices in a privileged podThis container will \\nrun in privileged mode'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 415}, page_content='384 CHAPTER  13 Securing cluster nodes and the network\\nbtrfs-control       stderr              tty48\\ncore                stdin               tty49\\ncpu                 stdout              tty5cpu_dma_latency     termination-log     tty50\\nfd                  tty                 tty51\\nfull                tty0                tty52fuse                tty1                tty53\\nhpet                tty10               tty54\\nhwrng               tty11               tty55\\n...                 ...                 ...\\nI haven’t included the whole list, because it’s too long for the book, but it’s evident\\nthat the device list is much longer than before. In fact, the privileged container sees\\nall the host node’s devices. This means it can use any device freely. \\n For example, I had to use privileged mode like this when I wanted a pod running\\non a Raspberry Pi to control LEDs connected it.\\n13.2.4 Adding individual kernel capabilities to a container\\nIn the previous section, you saw one way of giving a container unlimited power. In the\\nold days, traditional UNIX implementations only distinguished between privilegedand unprivileged processes, but for many  years, Linux has supported a much more\\nfine-grained permission system through kernel capabilities .\\n Instead of making a container privileged and giving it unlimited permissions, a\\nmuch safer method (from a security perspectiv e) is to give it access only to the kernel\\nfeatures it really requires. Kubernetes allows  you to add capabilities to each container\\nor drop part of them, which allows you to  fine-tune the contai ner’s permissions and\\nlimit the impact of a potential intrusion by an attacker.\\n For example, a container usually isn’t allo wed to change the system time (the hard-\\nware clock’s time). You can confirm this by trying to set the time in your \\npod-with-\\ndefaults  pod:\\n$ kubectl exec -it pod-with-defaults -- date +%T -s \"12:00:00\"\\ndate: can\\'t set date: Operation not permitted\\nIf you want to allow the container to change the system time, you can add a capabil-\\nity called CAP_SYS_TIME  to the container’s capabilities  list, as shown in the follow-\\ning listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: pod-add-settime-capability\\nspec:\\n  containers:  - name: main\\n    image: alpineListing 13.11 Adding the CAP_SYS_TIME  capability: pod-add-settime-capability.yaml'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 416}, page_content='385 Configuring the container’s security context\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:                     \\n      capabilities:                              add:                  \\n        - SYS_TIME            \\nNOTE Linux kernel capabilities are usually prefixed with CAP_.  But when\\nspecifying them in a pod spec, you must leave out the prefix.\\nIf you run the same command in this new po d’s container, the system time is changed\\nsuccessfully:\\n$ kubectl exec -it pod-add-settime-capabil ity -- date +%T -s \"12:00:00\"\\n12:00:00\\n$ kubectl exec -it pod-add-settime-capability -- date\\nSun May  7 12:00:03 UTC 2017\\nWARNING If you try this yourself, be aware that it may cause your worker\\nnode to become unusable. In Minikube , although the system time was auto-\\nmatically reset back by the Network Time Protocol (NTP) daemon, I had to\\nreboot the VM to schedule new pods. \\nYou can confirm the node’s time has been changed by checking the time on the node\\nrunning the pod. In my case, I’m using Minikube, so I have only one node and I can\\nget its time like this:\\n$ minikube ssh date\\nSun May  7 12:00:07 UTC 2017\\nAdding capabilities like this is a much better  way than giving a container full privileges\\nwith privileged:  true . Admittedly, it does require yo u to know and understand what\\neach capability does.\\nTIP You’ll find the list of Linux kernel capabilities in the Linux man pages.\\n13.2.5 Dropping capabilities from a container\\nYou’ve seen how to add capabilities, but yo u can also drop capabilities that may oth-\\nerwise be available to the container. For ex ample, the default capabilities given to a\\ncontainer include the CAP_CHOWN  capability, which allows processes to change the\\nownership of files in the filesystem. \\n You can see that’s the case by changing the ownership of the /tmp directory in\\nyour pod-with-defaults  pod to the guest  user, for example:\\n$ kubectl exec pod-with-defaults chown guest /tmp\\n$ kubectl exec pod-with-defaults -- ls -la / | grep tmp\\ndrwxrwxrwt    2 guest    root             6 May 25 15:18 tmpCapabilities are added or dropped \\nunder the securityContext property.\\nYou’re adding the \\nSYS_TIME capability.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 417}, page_content='386 CHAPTER  13 Securing cluster nodes and the network\\nTo prevent the container from doing that, yo u need to drop the capability by listing it\\nunder the container’s securityContext.c apabilities.drop  property, as shown in\\nthe following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: pod-drop-chown-capability\\nspec:\\n  containers:  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]    securityContext:\\n      capabilities:\\n        drop:                           - CHOWN                 \\nBy dropping the CHOWN  capability, you’re not allowed to change the owner of the /tmp\\ndirectory in this pod:\\n$ kubectl exec pod-drop-chown-capability chown guest /tmp\\nchown: /tmp: Operation not permitted\\nYou’re almost done exploring the container’ s security context options. Let’s look at\\none more.\\n13.2.6 Preventing processes from writing to the container’s filesystem\\nYou may want to prevent the processes runni ng in the container from writing to the\\ncontainer’s filesystem, and only allow them to write to mounted volumes. You’d want\\nto do that mostly for security reasons. \\n Let’s imagine you’re running a PHP applic ation with a hidden vulnerability, allow-\\ning an attacker to write to the filesystem. The PHP files are added to the container\\nimage at build time and are served from th e container’s filesystem. Because of the vul-\\nnerability, the attacker can modify those f iles and inject them with malicious code. \\n These types of attacks can be thwarted by preventing the container from writing to\\nits filesystem, where the app’s executable code  is normally stored. This is done by set-\\nting the container’s securityContext.rea dOnlyRootFilesystem  property to true , as\\nshown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-readonly-filesystemListing 13.12 Dropping a capability from a container: pod-drop-chown-capability.yaml\\nListing 13.13 A container with a read-only f ilesystem: pod-with-readonly-filesystem.yamlYou’re not allowing this container \\nto change file ownership.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 418}, page_content='387 Configuring the container’s security context\\nspec:\\n  containers:\\n  - name: main    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:                            readOnlyRootFilesystem: true        \\n    volumeMounts:                      \\n    - name: my-volume                  \\n      mountPath: /volume               \\n      readOnly: false                    volumes:\\n  - name: my-volume\\n    emptyDir:\\nWhen you deploy this pod, the container is  running as root, which has write permis-\\nsions to the / directory, but trying to  write a file there fails:\\n$ kubectl exec -it pod-with-readonly-filesystem touch /new-file\\ntouch: /new-file: Read-only file system\\nOn the other hand, writing to the mounted volume is allowed:\\n$ kubectl exec -it pod-with-readonly-filesystem touch /volume/newfile\\n$ kubectl exec -it pod-with-re adonly-filesystem -- ls -la /volume/newfile\\n-rw-r--r--    1 root     root       0 May  7 19:11 /mountedVolume/newfile\\nAs shown in the example, when you make th e container’s filesystem read-only, you’ll\\nprobably want to mount a volume in every directory the application writes to (for\\nexample, logs, on-disk caches, and so on).\\nTIP To increase security, when running pods in production, set their con-\\ntainer’s readOnlyRootFilesystem  property to true .\\nSETTING  SECURITY  CONTEXT  OPTIONS  AT THE POD LEVEL\\nIn all these examples, you’ve set the securi ty context of an individual container. Sev-\\neral of these options can also be set at the pod level (through the pod.spec.security-\\nContext  property). They serve as a default for all the pod’s containers but can be\\noverridden at the container level. The pod-leve l security context also allows you to set\\nadditional properties, which we’ll explain next.\\n13.2.7 Sharing volumes when containers run as different users\\nIn chapter 6, we explained how volumes ar e used to share data between the pod’s\\ncontainers. You had no trouble writing fi les in one container and reading them in\\nthe other. \\n But this was only because both containers were running as root, giving them full\\naccess to all the files in the volume. Now imagine using the runAsUser  option we\\nexplained earlier. You may need to run the two containers as two different users (per-\\nhaps you’re using two third-party containe r images, where each one runs its processThis container’s filesystem \\ncan’t be written to...\\n...but writing to /volume is \\nallowed, becase a volume \\nis mounted there.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 419}, page_content='388 CHAPTER  13 Securing cluster nodes and the network\\nunder its own specific user). If those two co ntainers use a volume to share files, they\\nmay not necessarily be able to read  or write files of one another. \\n That’s why Kubernetes allows you to sp ecify supplemental groups for all the pods\\nrunning in the container, allowing them to  share files, regardless of the user IDs\\nthey’re running as. This is done using the following two properties:\\n\\uf0a1fsGroup\\n\\uf0a1supplementalGroups\\nWhat they do is best explained in an exam ple, so let’s see how to use them in a pod\\nand then see what their effect is. The next  listing describes a pod with two containers\\nsharing the same volume.\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: pod-with-shared-volume-fsgroup\\nspec:\\n  securityContext:                           fsGroup: 555                         \\n    supplementalGroups: [666, 777]       \\n  containers:  - name: first\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]    securityContext:                     \\n      runAsUser: 1111                    \\n    volumeMounts:                                   - name: shared-volume                       \\n      mountPath: /volume\\n      readOnly: false\\n  - name: second\\n    image: alpine    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:                     \\n      runAsUser: 2222                        volumeMounts:                               \\n    - name: shared-volume                       \\n      mountPath: /volume      readOnly: false\\n  volumes:                                      \\n  - name: shared-volume                             emptyDir:\\nAfter you create this pod, run a shell in it s first container and see what user and group\\nIDs the container is running as:\\n$ kubectl exec -it pod-with-shared-volume-fsgroup -c first sh\\n/ $ id\\nuid=1111 gid=0(root) groups=555,666,777Listing 13.14 fsGroup  & supplementalGroups : pod-with-shared-volume-fsgroup.yaml\\nThe fsGroup and supplementalGroups \\nare defined in the security context at the pod level.\\nThe first container \\nruns as user ID 1111 .\\nBoth containers \\nuse the same volumeThe second\\ncontainer\\nruns as user\\nID 2222.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 420}, page_content='389 Restricting the use of security-related features in pods\\nThe id command shows the container is running with user ID 1111 , as specified in the\\npod definition. The effective group ID is 0(root) , but group IDs 555, 666, and 777 are\\nalso associated with the user. \\n In the pod definition, you set fsGroup  to 555. Because of this, the mounted volume\\nwill be owned by group ID 555, as shown here:\\n/ $ ls -l / | grep volume\\ndrwxrwsrwx    2 root     555              6 May 29 12:23 volume\\nIf you create a file in the mounted volume’s  directory, the file is owned by user ID\\n1111  (that’s the user ID the containe r is running as) and by group ID 555:\\n/ $ echo foo > /volume/foo\\n/ $ ls -l /volumetotal 4\\n-rw-r--r--    1 1111     555               4 May 29 12:25 foo\\nThis is different from how ownership is ot herwise set up for newly created files. Usu-\\nally, the user’s effective group ID, which is 0 in your case, is used when a user creates\\nfiles. You can see this by creating a file in the container’s filesystem instead of in thevolume:\\n/ $ echo foo > /tmp/foo\\n/ $ ls -l /tmp\\ntotal 4-rw-r--r--    1 1111     root             4 May 29 12:41 foo\\nAs you can see, the fsGroup  security context property is used when the process cre-\\nates files in a volume (but this depends on the volume plugin used), whereas the\\nsupplementalGroups  property defines a list of additi onal group IDs the user is asso-\\nciated with. \\n This concludes this section about the conf iguration of the container’s security con-\\ntext. Next, we’ll see how a cluster administ rator can restrict users from doing so.\\n13.3 Restricting the use of security-related features in pods\\nThe examples in the previous sections have shown how a person deploying pods can\\ndo whatever they want on any cluster no de, by deploying a privileged pod to the\\nnode, for example. Obviously, a mechanis m must prevent users from doing part or\\nall of what’s been explained. The cluster admin can restrict the use of the previously\\ndescribed security-related features by creating one or more PodSecurityPolicy\\nresources.\\n13.3.1 Introducing the PodSecurityPolicy resource\\nPodSecurityPolicy is a cluster-level (non -namespaced) resource, which defines what\\nsecurity-related features users can or can’ t use in their pods. The job of upholding\\nthe policies configured in PodSecurit yPolicy resources is performed by the'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 421}, page_content='390 CHAPTER  13 Securing cluster nodes and the network\\nPodSecurityPolicy admission control plugin running in the API server (we explained\\nadmission control plugins in chapter 11).\\nNOTE The PodSecurityPolicy admission co ntrol plugin may not be enabled\\nin your cluster. Before running the following examples, ensure it’s enabled. Ifyou’re using Minikube, refer to the next sidebar.\\nWhen someone posts a pod resource to th e API server, the PodSecurityPolicy admis-\\nsion control plugin validates the pod definition against the configured PodSecurity-\\nPolicies. If the pod conforms to the cluste r’s policies, it’s accepted and stored into\\netcd; otherwise it’s rejected immediatel y. The plugin may also modify the pod\\nresource according to defaults configured in the policy.\\nUNDERSTANDING  WHAT A PODSECURITY POLICY CAN DO\\nA PodSecurityPolicy resource defines things like the following:\\n\\uf0a1Whether a pod can use the host’s IPC, PID, or Network namespaces\\n\\uf0a1Which host ports a pod can bind to\\n\\uf0a1What user IDs a container can run as\\n\\uf0a1Whether a pod with privileged containers can be createdEnabling RBAC and PodSecurityPolicy admission control in Minikube\\nI’m using Minikube version v0.19.0 to run these examples. That version doesn’t\\nenable either the PodSecurityPolicy admission control plugin or RBAC authorization,\\nwhich is required in part of the exercises.  One exercise also requires authenticating\\nas a different user, so you’ll also need to enable the basic authentication plugin\\nwhere users are defined in a file.\\nTo run Minikube with all these plugins enabl ed, you may need to use this (or a similar)\\ncommand, depending on the version you’re using: \\n$ minikube start --extra-confi g apiserver.Authentication.PasswordFile.\\n➥ BasicAuthFile=/etc/kubernetes/passwd --extra-config=apiserver.\\n➥ Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRun\\n➥ Options.AdmissionControl=NamespaceLifecycle,LimitRanger,Service\\n➥ Account,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,\\n➥ DefaultTolerationSeconds,PodSecurityPolicy\\nThe API server won’t start up until you create the password file you specified in the\\ncommand line options. This is how to create the file:\\n$ cat <<EOF | minikube ssh sudo tee /etc/kubernetes/passwd\\npassword,alice,1000,basic-userpassword,bob,2000,privileged-user\\nEOF\\nYou’ll find a shell script that runs both commands in the book’s code archive in\\nChapter13/min ikube-with-rbac-and-psp-enabled.sh.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 422}, page_content=\"391 Restricting the use of security-related features in pods\\n\\uf0a1Which kernel capabilities are allowed, wh ich are added by default and which are\\nalways dropped\\n\\uf0a1What SELinux labels a container can use\\n\\uf0a1Whether a container can use a writable root filesystem or not\\n\\uf0a1Which filesystem groups the container can run as\\n\\uf0a1Which volume types a pod can use\\nIf you’ve read this chapter up to this point,  everything but the last item in the previous\\nlist should be familiar. The last it em should also be fairly clear. \\nEXAMINING  A SAMPLE  PODSECURITY POLICY\\nThe following listing shows a sample PodS ecurityPolicy, which prevents pods from\\nusing the host’s IPC, PID, and Network na mespaces, and prevents running privileged\\ncontainers and the use of most host po rts (except ports from 10000-11000 and 13000-\\n14000). The policy doesn’t set any constrai nts on what users, groups, or SELinux\\ngroups the container can run as.\\napiVersion: extensions/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:  name: default\\nspec:\\n  hostIPC: false                   hostPID: false                 \\n  hostNetwork: false             \\n  hostPorts:                           - min: 10000                       \\n    max: 11000                       \\n  - min: 13000                       \\n    max: 14000                       \\n  privileged: false                readOnlyRootFilesystem: true   \\n  runAsUser:                      \\n    rule: RunAsAny                  fsGroup:                        \\n    rule: RunAsAny                \\n  supplementalGroups:                 rule: RunAsAny                \\n  seLinux:                      \\n    rule: RunAsAny                volumes:                  \\n  - '*'                     \\nMost of the options specified in the exampl e should be self-explanatory, especially if\\nyou’ve read the previous sections. After th is PodSecurityPolicy resource is posted toListing 13.15 An example PodSecurit yPolicy: pod-security-policy.yaml\\nContainers aren’t \\nallowed to use the \\nhost’s IPC, PID, or \\nnetwork namespace.\\nThey can only bind to host ports \\n10000 to 11000 (inclusive) or \\nhost ports 13000 to 14000.\\nContainers cannot run \\nin privileged mode.\\nContainers are forced to run \\nwith a read-only root filesystem.Containers can \\nrun as any user \\nand any group.\\nThey can also use any \\nSELinux groups they want.\\nAll volume types can \\nbe used in pods.\"),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 423}, page_content='392 CHAPTER  13 Securing cluster nodes and the network\\nthe cluster, the API server will no longer allow you to deploy the privileged pod used\\nearlier. For example\\n$ kubectl create -f pod-privileged.yaml\\nError from server (Forbidden): error when creating \"pod-privileged.yaml\":\\npods \"pod-privileged\" is forbidden: unable to validate against any pod \\nsecurity policy: [spec.containers[0].securityContext.privileged: Invalid \\nvalue: true: Privileged containers are not allowed]\\nLikewise, you can no longer deploy pods that  want to use the host’s PID, IPC, or Net-\\nwork namespace. Also, because you set readOnlyRootFilesystem  to true  in the pol-\\nicy, the container filesystems in all pods will be read-only (containers can only write\\nto volumes).\\n13.3.2 Understanding runAsUser, fsGroup, and supplementalGroups \\npolicies\\nThe policy in the previous example doesn’ t impose any limits on which users and\\ngroups containers can run as , because you’ve used the RunAsAny  rule for the runAs-\\nUser , fsGroup , and supplementalGroups  fields. If you want to constrain the list of\\nallowed user or group IDs, you change the rule to MustRunAs  and specify the range of\\nallowed IDs. \\nUSING THE MUSTRUNAS RULE\\nLet’s look at an example. To only allow containers to run as user ID 2 and constrain the\\ndefault filesystem group and supplemen tal group IDs to be anything from 2–10  or 20–\\n30 (all inclusive), you’d include the following snippet in the PodSecurityPolicy resource.\\n  runAsUser:\\n    rule: MustRunAs    ranges:\\n    - min: 2                \\n      max: 2                  fsGroup:\\n    rule: MustRunAs\\n    ranges:    - min: 2                \\n      max: 10               \\n    - min: 20                     max: 30               \\n  supplementalGroups:\\n    rule: MustRunAs    ranges:\\n    - min: 2                \\n      max: 10                   - min: 20               \\n      max: 30               Listing 13.16 Specifying IDs containers must run as: psp-must-run-as.yaml\\nAdd a single range with min equal \\nto max to set one specific ID.\\nMultiple ranges are \\nsupported—here, group IDs can be 2– 10 \\nor 20–30 (inclusive).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 424}, page_content='393 Restricting the use of security-related features in pods\\nIf the pod spec tries to set either of those fields to a value outside of these ranges, the\\npod will not be accepted by the API server. To  try this, delete the previous PodSecurity-\\nPolicy and create the new one from the psp-must-run-as.yaml file. \\nNOTE Changing the policy has no effect on  existing pods, because PodSecurity-\\nPolicies are enforced only when creating or updating pods.\\nDEPLOYING  A POD WITH RUNASUSER OUTSIDE  OF THE POLICY ’S RANGE\\nIf you try deploying the pod-as-user-guest.ya ml file from earlier, which says the con-\\ntainer should run as user ID 405, the API server rejects the pod:\\n$ kubectl create -f pod-as-user-guest.yaml\\nError from server (Forbidden): error when creating \"pod-as-user-guest.yaml\"\\n: pods \"pod-as-user-guest\" is forbidden: u nable to validate against any pod \\nsecurity policy: [securityContext.runAsUser: Invalid value: 405: UID on \\ncontainer main does not match required ran ge.  Found 405, allowed: [{2 2}]]\\nOkay, that was obvious. But what happens if you deploy a pod without setting the runAs-\\nUser  property, but the user ID is baked into the container image (using the USER  direc-\\ntive in the Dockerfile)?\\nDEPLOYING  A POD WITH A CONTAINER  IMAGE  WITH AN OUT-OF-RANGE  USER ID\\nI’ve created an alternative image for the Node.js app you’ve used throughout the\\nbook. The image is configured so that the co ntainer will run as user ID 5. The Docker-\\nfile for the image is shown in the following listing.\\nFROM node:7\\nADD app.js /app.js\\nUSER 5                         \\nENTRYPOINT [\"node\", \"app.js\"]\\nI pushed the image to Docker Hub as luksa/kubia-run-as-user-5 . If I deploy a pod\\nwith that image, the API server doesn’t reject it:\\n$ kubectl run run-as-5 --image  luksa/kubia-run-as-user-5 --restart Never\\npod \"run-as-5\" created\\nUnlike before, the API server accepted the pod and the Kubelet has run its container.\\nLet’s see what user ID the container is running as:\\n$ kubectl exec run-as-5 -- id\\nuid=2(bin) gid=2(bin) groups=2(bin)\\nAs you can see, the container is running as user ID 2, which is the ID you specified in\\nthe PodSecurityPolicy. The PodSecurityPolicy can be used to override the user IDhardcoded into a container image.Listing 13.17 Dockerfile with a USER di rective: kubia-run-as-user-5/Dockerfile\\nContainers run from \\nthis image will run \\nas user ID 5.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 425}, page_content='394 CHAPTER  13 Securing cluster nodes and the network\\nUSING THE MUSTRUNASNONROOT RULE IN THE RUNASUSER FIELD\\nFor the runAsUser  field an additional rule can be used: MustRunAsNonRoot . As the\\nname suggests, it prevents users from deploy ing containers that run as root. Either the\\ncontainer spec must specify a runAsUser  field, which can’t be zero (zero is the root\\nuser’s ID), or the container image itself mu st run as a non-zero user ID. We explained\\nwhy this is a good thing earlier.\\n13.3.3 Configuring allowed, default, and disallowed capabilities\\nAs you learned, containers can run in priv ileged mode or not, and you can define a\\nmore fine-grained permission configurat ion by adding or dropping Linux kernel\\ncapabilities in each container. Three fields  influence which capabilities containers can\\nor cannot use:\\n\\uf0a1allowedCapabilities\\n\\uf0a1defaultAddCapabilities\\n\\uf0a1requiredDropCapabilities\\nWe’ll look at an example first, and then disc uss what each of the three fields does. The\\nfollowing listing shows a snippet of a PodSec urityPolicy resource defining three fields\\nrelated to capabilities.\\napiVersion: extensions/v1beta1 \\nkind: PodSecurityPolicy\\nspec:\\n  allowedCapabilities:            - SYS_TIME                    \\n  defaultAddCapabilities:         \\n  - CHOWN                         \\n  requiredDropCapabilities:     \\n  - SYS_ADMIN                     - SYS_MODULE                  \\n  ...\\nNOTE The SYS_ADMIN  capability allows a range of  administrative operations,\\nand the SYS_MODULE  capability allows loading and unloading of Linux kernel\\nmodules.\\nSPECIFYING  WHICH  CAPABILITIES  CAN BE ADDED  TO A CONTAINER\\nThe allowedCapabilities  field is used to specify which capabilities pod authors can\\nadd in the securityContex t.capabilities  field in the container spec. In one of the\\nprevious examples, you added the SYS_TIME  capability to your container. If the Pod-\\nSecurityPolicy admission control plugin ha d been enabled, you wouldn’t have been\\nable to add that capability, unless it was specified in the PodSecurityPolicy as shown\\nin listing 13.18.Listing 13.18 Specifying capabilities in a PodSecurityPolicy: psp-capabilities.yaml\\nAllow containers to \\nadd the SYS_TIME \\ncapability.\\nAutomatically add the CHOWN \\ncapability to every container.\\nRequire containers to \\ndrop the SYS_ADMIN and SYS_MODULE capabilities.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 426}, page_content='395 Restricting the use of security-related features in pods\\nADDING  CAPABILITIES  TO ALL CONTAINERS\\nAll capabilities listed under the defaultAddCapabilities  field will be added to\\nevery deployed pod’s containers. If a user  doesn’t want certain containers to have\\nthose capabilities, they need to explicitly drop them in the specs of those containers.\\n The example in listing 13.18 enables the automatic addition of the CAP_CHOWN  capa-\\nbility to every container, thus allowing proc esses running in the container to change the\\nownership of files in the container (with the chown  command, for example).\\nDROPPING  CAPABILITIES  FROM A CONTAINER\\nThe final field in this example is requiredDropCapabilities . I must admit, this was a\\nsomewhat strange name for me at first, but it’s not that complicated. The capabilities\\nlisted in this field are dropped automatica lly from every container (the PodSecurity-\\nPolicy Admission Control plugin will  add them to every container’s security-\\nContext.capabilities.drop  field). \\n If a user tries to create a pod where they explicitly add one of the capabilities listed\\nin the policy’s requiredDropCapabilities  field, the pod is rejected:\\n$ kubectl create -f pod-add-sysadmin-capability.yaml\\nError from server (Forbidden): error when creating \"pod-add-sysadmin-\\ncapability.yaml\": pods \"pod-add-sysadmin-capability\" is forbidden: unable \\nto validate against any pod se curity policy: [capabilities.add: Invalid \\nvalue: \"SYS_ADMIN\": capability may not be added]\\n13.3.4 Constraining the types of volumes pods can use\\nThe last thing a PodSecurityPolicy resource can do is define which volume types users\\ncan add to their pods. At the minimum, a PodSecurityPolicy should allow using at\\nleast the emptyDir , configMap , secret , downwardAPI , and the persistentVolume-\\nClaim  volumes. The pertinent part of such a PodSecurityPolicy resource is shown in\\nthe following listing.\\nkind: PodSecurityPolicy\\nspec:\\n  volumes:\\n  - emptyDir  - configMap\\n  - secret\\n  - downwardAPI  - persistentVolumeClaim\\nIf multiple PodSecurityPolicy resources ar e in place, pods can use any volume type\\ndefined in any of the policies (the union of all volumes  lists is used).Listing 13.19 A PSP snippet allowing the use of only certain volume types: \\npsp-volumes.yaml'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 427}, page_content='396 CHAPTER  13 Securing cluster nodes and the network\\n13.3.5 Assigning different PodSecurityPolicies to different users \\nand groups\\nWe mentioned that a PodSecurityPolicy is  a cluster-level resource, which means it\\ncan’t be stored in and applied to a specif ic namespace. Does that mean it always\\napplies across all namespaces? No, because that would make them relatively unus-able. After all, system pods must often be allowed to do things that regular pods\\nshouldn’t.\\n Assigning different policies to differen t users is done through the RBAC mecha-\\nnism described in the previous chapter. The idea is to create as many policies as you\\nneed and make them available to individual users or groups by creating ClusterRole\\nresources and pointing them to the indi vidual policies by name. By binding those\\nClusterRoles to specific users or groups with ClusterRoleBindings, when the Pod-\\nSecurityPolicy Admission Control plugin need s to decide whether to admit a pod defi-\\nnition or not, it will only consider the po licies accessible to the user creating the pod. \\n You’ll see how to do this in the next exercise. You’ll start by creating an additional\\nPodSecurityPolicy.\\nCREATING  A PODSECURITY POLICY ALLOWING  PRIVILEGED  CONTAINERS  TO BE DEPLOYED\\nYou’ll create a special PodSecurityPolicy that will allow privileged users to create pods\\nwith privileged containers. The following  listing shows the policy’s definition.\\napiVersion: extensions/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:\\n  name: privileged          \\nspec:\\n  privileged: true          runAsUser:\\n    rule: RunAsAny\\n  fsGroup:    rule: RunAsAny\\n  supplementalGroups:\\n    rule: RunAsAny  seLinux:\\n    rule: RunAsAny\\n  volumes:  - \\'*\\'\\nAfter you post this policy to the API se rver, you have two policies in the cluster:\\n$ kubectl get psp\\nNAME         PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    ...  default      false   []     RunAsAny   RunAsAny    RunAsAny   ...\\nprivileged   true    []     RunAsAny   RunAsAny    RunAsAny   ...\\nNOTE The shorthand for PodSecurityPolicy  is psp.Listing 13.20 A PodSecurityPolicy for privileged users: psp-privileged.yaml\\nThe name of this \\npolicy is \"privileged.”\\nIt allows running \\nprivileged containers.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 428}, page_content='397 Restricting the use of security-related features in pods\\nAs you can see in the PRIV  column, the default  policy doesn’t allow running privi-\\nleged containers, whereas the privileged  policy does. Because you’re currently\\nlogged in as a cluster-admin, you can see all the policies. When creating pods, if any\\npolicy allows you to deploy a pod with ce rtain features, the API server will accept\\nyour pod.\\n Now imagine two additional users are usin g your cluster: Alice and Bob. You want\\nAlice to only deploy restricted (non-privi leged) pods, but you want to allow Bob to\\nalso deploy privileged pod s. You do this by making sure Alice can only use the default\\nPodSecurityPolicy, while allowing Bob to use both.\\nUSING RBAC TO ASSIGN  DIFFERENT  PODSECURITY POLICIES  TO DIFFERENT  USERS\\nIn the previous chapter, you used RBAC to grant users access to only certain resourcetypes, but I mentioned that access can be gran ted to specific resource instances by ref-\\nerencing them by name. That’s what you’ll use to make users use different Pod-SecurityPolicy resources.\\n First, you’ll create two ClusterRoles, each allowing the use of one of the policies.\\nYou’ll call the first one \\npsp-default  and in it allow the use of the default  Pod-\\nSecurityPolicy resource. You can use kubectl  create  clusterrole  to do that:\\n$ kubectl create clusterrole psp-default --verb=use \\n➥  --resource=podsecuritypolicies --resource-name=default\\nclusterrole \"psp-default\" created\\nNOTE You’re using the special verb use instead of get, list , watch , or similar.\\nAs you can see, you’re referring to a specific  instance of a PodSecurityPolicy resource by\\nusing the --resource-name  option. Now, create another ClusterRole called psp-\\nprivileged , pointing to the privileged  policy:\\n$ kubectl create clusterrole psp-privileged --verb=use\\n➥  --resource=podsecuritypolicies --resource-name=privileged\\nclusterrole \"psp-privileged\" created\\nNow, you need to bind these two policies to  users. As you may remember from the pre-\\nvious chapter, if you’re binding a Cluste rRole that grants access to cluster-level\\nresources (which is what PodSecurityPolicy resources are), you need to use a Cluster-\\nRoleBinding instead of a (namespaced) RoleBinding. \\n You’re going to bind the psp-default  ClusterRole to all authenticated users, not\\nonly to Alice. This is necessary becaus e otherwise no one could create any pods,\\nbecause the Admission Control plugin would complain that no policy is in place.\\nAuthenticated users all belong to the system:authenticated  group, so you’ll bind\\nthe ClusterRole to the group:\\n$ kubectl create clusterrolebinding psp-all-users \\n➥ --clusterrole=psp-default --group=system:authenticated\\nclusterrolebinding \"psp-all-users\" created'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 429}, page_content='398 CHAPTER  13 Securing cluster nodes and the network\\nYou’ll bind the psp-privileged  ClusterRole only to Bob:\\n$ kubectl create clusterrolebinding psp-bob \\n➥ --clusterrole=psp-privileged --user=bob\\nclusterrolebinding \"psp-bob\" created\\nAs an authenticated user, Alice should now have access to the default  PodSecurity-\\nPolicy, whereas Bob should have access to both the default  and the privileged  Pod-\\nSecurityPolicies. Alice shouldn’t be able to create privileged pods, whereas Bob\\nshould. Let’s see if that’s true.\\nCREATING  ADDITIONAL  USERS  FOR KUBECTL\\nBut how do you authenticate as Alice or Bo b instead of whatever you’re authenticated\\nas currently? The book’s appendix A explains how kubectl  can be used with multiple\\nclusters, but also with multiple contexts. A context includes the user credentials used\\nfor talking to a cluster. Turn to appendix A to find out more. Here we’ll show the bare\\ncommands enabling you to use kubectl  as Alice or Bob. \\n First, you’ll create two new users in kubectl ’s config with the following two\\ncommands:\\n$ kubectl config set-credentials alice --username=alice --password=password\\nUser \"alice\" set.\\n$ kubectl config set-credentials bob --username=bob --password=password\\nUser \"bob\" set.\\nIt should be obvious what the commands do. Because you’re setting username and\\npassword credentials, kubectl  will use basic HTTP authentication for these two users\\n(other authentication methods include to kens, client certificates, and so on).\\nCREATING  PODS AS A DIFFERENT  USER\\nYou can now try creating a privileged pod wh ile authenticating as Alice. You can tell\\nkubectl  which user credentials to use by using the --user  option:\\n$ kubectl --user alice create -f pod-privileged.yaml\\nError from server (Forbidden): error when creating \"pod-privileged.yaml\": \\npods \"pod-privileged\" is forbidden: unable to validate against any pod \\nsecurity policy: [sp ec.containers[0].securityContext.privileged: Invalid \\nvalue: true: Privileged containers are not allowed]\\nAs expected, the API server doesn’t allow Alic e to create privileged pods. Now, let’s see\\nif it allows Bob to do that:\\n$ kubectl --user bob create -f pod-privileged.yaml\\npod \"pod-privileged\" created'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 430}, page_content='399 Isolating the pod network\\nAnd there you go. You’ve successfully us ed RBAC to make the Admission Control\\nplugin use different PodSecurityPo licy resources for different users.\\n13.4 Isolating the pod network\\nUp to now in this chapter, we’ve explored many security-related configuration options\\nthat apply to individual pods and their cont ainers. In the remainder of this chapter,\\nwe’ll look at how the network between pods can be secured by limiting which pods cantalk to which pods.\\n Whether this is configurable or no t depends on which container networking\\nplugin is used in the cluster. If the netw orking plugin supports it, you can configure\\nnetwork isolation by creating NetworkPolicy resources. \\n A NetworkPolicy applies to pods that ma tch its label selector and specifies either\\nwhich sources can access the matched pods or which destinations can be accessed\\nfrom the matched pods. This is configured  through ingress and egress rules, respec-\\ntively. Both types of rules can match only  the pods that match a pod selector, all\\npods in a namespace whose labels matc h a namespace selector, or a network IP\\nblock specified using Classless Inter-Domain  Routing (CIDR) notation (for example,\\n192.168.1.0/24). \\n We’ll look at both ingress and egre ss rules and all three matching options.\\nNOTE Ingress rules in a NetworkPolicy have nothing to do with the Ingress\\nresource discussed in chapter 5.\\n13.4.1 Enabling network isolation in a namespace\\nBy default, pods in a given namespace can be accessed by anyone. First, you’ll need\\nto change that. You’ll create a default-deny  NetworkPolicy, which will prevent all\\nclients from connecting to any pod in yo ur namespace. The NetworkPolicy defini-\\ntion is shown in the following listing.\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:  name: default-deny\\nspec:\\n  podSelector:        \\nWhen you create this NetworkPolicy in a certain namespace, no one can connect to\\nany pod in that namespace. \\n \\n \\n Listing 13.21 A default-deny  NetworkPolicy: network-policy-default-deny.yaml\\nEmpty pod selector \\nmatches all pods in the \\nsame namespace'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 431}, page_content='400 CHAPTER  13 Securing cluster nodes and the network\\nNOTE The CNI plugin or other type of networking solution used in the clus-\\nter must support NetworkPolicy, or else  there will be no effect on inter-pod\\nconnectivity.\\n13.4.2 Allowing only some pods in the namespace to connect to \\na server pod\\nTo let clients connect to the pods in the namespace, you must now explicitly say who\\ncan connect to the pods. By who I mean which pods. Let’s explore how to do this\\nthrough an example. \\n Imagine having a PostgreSQL database pod running in namespace foo and a web-\\nserver pod that uses the database. Other pods are also in the namespace, and you\\ndon’t want to allow them to connect to th e database. To secure the network, you need\\nto create the NetworkPolicy resource shown in the following listing in the same name-\\nspace as the database pod.\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: postgres-netpolicyspec:\\n  podSelector:                     \\n    matchLabels:                         app: database                \\n  ingress:                           \\n  - from:                                - podSelector:                   \\n        matchLabels:                 \\n          app: webserver             \\n    ports:                     \\n    - port: 5432               \\nThe example NetworkPolicy allows pods with the app=webserver  label to connect to\\npods with the app=database  label, and only on port 5432. Other pods can’t connect to\\nthe database pods, and no one (not even the webserver pods) can connect to anything\\nother than port 5432 of the database pods. This is shown in figure 13.4.\\n Client pods usually connect to server pods through a Service instead of directly to\\nthe pod, but that doesn’t change anything . The NetworkPolicy is enforced when con-\\nnecting through a Service, as well.\\n \\n \\n \\n Listing 13.22 A NetworkPolicy for the Postgres pod: network-policy-postgres.yaml\\nThis policy secures \\naccess to pods with \\napp=database label.\\nIt allows incoming connections \\nonly from pods with the \\napp=webserver label.\\nConnections to this \\nport are allowed.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 432}, page_content='401 Isolating the pod network\\n13.4.3 Isolating the network between Kubernetes namespaces\\nNow let’s look at another example, where multiple tenants are using the same Kuber-\\nnetes cluster. Each tenant can use multiple namespaces, and each namespace has a\\nlabel specifying the tenant it belongs to. For example, one of those tenants is Man-\\nning. All their namespaces have been labeled with tenant:  manning . In one of their\\nnamespaces, they run a Shopping Cart micros ervice that needs to be available to all\\npods running in any of their namespaces. Obviously, they don’t want any other tenants\\nto access their microservice.\\n To secure their microservice, they create  the NetworkPolicy resource shown in the\\nfollowing listing.\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:  name: shoppingcart-netpolicy\\nspec:\\n  podSelector:                           matchLabels:                     \\n      app: shopping-cart             \\n  ingress:  - from:\\n    - namespaceSelector:            \\n        matchLabels:                          tenant: manning           \\n    ports:\\n    - port: 80Listing 13.23 NetworkPolicy for the shopping cart pod(s): network-policy-cart.yamlapp: database\\nPod:\\ndatabasePort\\n5432\\nPort\\n9876app: webserver\\nPod:\\nwebserverPod selector:\\napp=webserverPod selector:\\napp=database\\napp: webserver\\nPod:\\nwebserverOther podsNetworkPolicy: postgres-netpolicy\\nFigure 13.4 A NetworkPolicy allowing only some pods to access other pods and only on a specific \\nport\\nThis policy applies to pods \\nlabeled as microservice= shopping-cart.\\nOnly pods running in namespaces \\nlabeled as tenant=manning are \\nallowed to access the microservice.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 433}, page_content='402 CHAPTER  13 Securing cluster nodes and the network\\nThis NetworkPolicy ensures only pods  running in namespaces labeled as tenant:\\nmanning  can access their Shopping Cart micr oservice, as shown in figure 13.5.\\nIf the shopping cart provider also wants to give access to other tenants (perhaps to\\none of their partner companies), they can ei ther create an additional NetworkPolicy\\nresource or add an additional ingress rule to their existing NetworkPolicy.\\nNOTE In a multi-tenant Kubernetes cluster, tenants usually can’t add labels\\n(or annotations) to their namespaces them selves. If they could, they’d be able\\nto circumvent the namespaceSelector -based ingress rules.\\n13.4.4 Isolating using CIDR notation\\nInstead of specifying a pod- or namespace selector to define who can access the pods\\ntargeted in the NetworkPolicy, you can also specify an IP block in CIDR notation. For\\nexample, to allow the shopping-cart  pods from the previous section to only be acces-\\nsible from IPs in the 192.168.1.1 to .255 ra nge, you’d specify the ingress rule in the\\nnext listing.\\n  ingress:\\n  - from:\\n    - ipBlock:                            cidr: 192.168.1.0/24      Listing 13.24 Specifying an IP block in an ingress rule: network-policy-cidr.yamlapp: shopping-cart\\nPod:\\nshopping-cartPort\\n80Namespace selector:tenant=manning\\nPod selector:app=shopping-cart\\nOther podsPodsNetworkPolicy:\\nshoppingcart-netpolicyNamespace: manningANamespace: ecommerce-ltd\\nOther namespacestenant: manning\\nPodsNamespace: manningB\\ntenant: manning\\nFigure 13.5 A NetworkPolicy only allowi ng pods in namespaces matching a namespaceSelector  to access a \\nspecific pod.\\nThis ingress rule only allows traffic from \\nclients in the 192.168.1.0/24 IP block.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 434}, page_content='403 Summary\\n13.4.5 Limiting the outbound traffic of a set of pods\\nIn all previous examples, you’ve been lim iting the inbound traffic to the pods that\\nmatch the NetworkPolicy’s pod selector using ingress rules, but you can also limittheir outbound traffic throug h egress rules. An example is shown in the next listing.\\nspec:\\n  podSelector:               \\n    matchLabels:             \\n      app: webserver           egress:               \\n  - to:                       \\n    - podSelector:                    matchLabels:          \\n          app: database       \\nThe NetworkPolicy in the previous listing allows pods that have the app=webserver\\nlabel to only access pods that have the app=database  label and nothing else (neither\\nother pods, nor any other IP, regardless of whether it’s internal or external to thecluster).\\n13.5 Summary\\nIn this chapter, you learned about securi ng cluster nodes from pods and pods from\\nother pods. You learned that\\n\\uf0a1Pods can use the node’s Linux namespaces instead of using their own.\\n\\uf0a1Containers can be configured to run as  a different user and/or group than the\\none defined in the container image.\\n\\uf0a1Containers can also run in privileged mode, allowing them to access the node’s\\ndevices that are otherwise not exposed to pods.\\n\\uf0a1Containers can be run as read-only, pr eventing processes from writing to the\\ncontainer’s filesystem (and only allowi ng them to write to mounted volumes).\\n\\uf0a1Cluster-level PodSecurityPolicy resources can be created to prevent users from\\ncreating pods that could compromise a node.\\n\\uf0a1PodSecurityPolicy resources can be associated with specific users using RBAC’sClusterRoles and ClusterRoleBindings.\\n\\uf0a1NetworkPolicy resources are used to limit a pod’s inbound and/or outbound\\ntraffic.\\nIn the next chapter, you’ll learn how comp utational resources available to pods can be\\nconstrained and how a pod’s qua lity of service is configured.Listing 13.25 Using egress rules in a Ne tworkPolicy: network-policy-egress.yaml\\nThis policy applies to pods with \\nthe app=webserver label.\\nIt limits\\nthe pods’\\noutbound\\ntraffic.Webserver pods may only \\nconnect to pods with the app=database label.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 435}, page_content='404Managing pods’\\n computational resources\\nUp to now you’ve created pods withou t caring about how much CPU and memory\\nthey’re allowed to consume. But as you’ll see in this chapter, setting both how\\nmuch a pod is expected to consume and the maximum amount it’s allowed to con-\\nsume is a vital part of any pod definiti on. Setting these two sets of parameters\\nmakes sure that a pod takes only its fair share of the resources provided by the\\nKubernetes cluster and also affects ho w pods are scheduled across the cluster.This chapter covers\\n\\uf0a1Requesting CPU, memory, and other \\ncomputational resources for containers\\n\\uf0a1Setting a hard limit for CPU and memory\\n\\uf0a1Understanding Quality of Service guarantees for pods\\n\\uf0a1Setting default, min, and max resources for pods in a namespace\\n\\uf0a1Limiting the total amount of resources available in a namespace'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 436}, page_content='405 Requesting resources for a pod’s containers\\n14.1 Requesting resources for a pod’s containers\\nWhen creating a pod, you can specify the amount of CPU and memory that a con-\\ntainer needs (these are called requests ) and a hard limit on what it may consume\\n(known as limits ). They’re specified for each contai ner individually, not for the pod as\\na whole. The pod’s resource requests and li mits are the sum of the requests and lim-\\nits of all its containers. \\n14.1.1 Creating pods with resource requests\\nLet’s look at an example pod manifest, wh ich has the CPU and memory requests spec-\\nified for its single container, as  shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: requests-podspec:\\n  containers:\\n  - image: busybox    command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"]\\n    name: main              \\n    resources:                    requests:             \\n        cpu: 200m          \\n        memory: 10Mi    \\nIn the pod manifest, your single container requires one-fifth of a CPU core (200 mil-\\nlicores) to run properly. Five such pods/conta iners can run sufficiently fast on a single\\nCPU core. \\n When you don’t specify a request for CPU,  you’re saying you don’t care how much\\nCPU time the process running in your contai ner is allotted. In the worst case, it may\\nnot get any CPU time at all (this happens when a heavy demand by other processes\\nexists on the CPU). Although this may be fi ne for low-priority batch jobs, which aren’t\\ntime-critical, it obviously isn’t appropriat e for containers handling user requests.\\n In the pod spec, you’re also requesting 10 mebibytes of memory for the container.\\nBy doing that, you’re saying that you ex pect the processes running inside the con-\\ntainer to use at most 10 mebibytes of RAM. They might use less, but you’re not expect-\\ning them to use more than that in normal circumstances. Later in this chapter you’ll\\nsee what happens if they do.\\n Now you’ll run the pod. When the pod star ts, you can take a quick look at the pro-\\ncess’ CPU consumption by running the top command inside the container, as shown\\nin the following listing.Listing 14.1 A pod with reso urce requests: requests-pod.yaml\\nYou’re specifying resource \\nrequests for the main container.\\nThe container requests 200 \\nmillicores (that is, 1/5 of a \\nsingle CPU core’s time). The container also\\nrequests 10 mebibytes\\nof memory.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 437}, page_content='406 CHAPTER  14 Managing pods’ computational resources\\n$ kubectl exec -it requests-pod top\\nMem: 1288116K used, 760368K free, 9196K shrd, 25748K buff, 814840K cachedCPU:  9.1% usr 42.1% sys  0.0% nic 48.4% i dle  0.0% io  0.0% irq  0.2% sirq\\nLoad average: 0.79 0.52 0.29 2/481 10\\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND    1     0 root     R     1192  0.0   1 50.2 dd if /dev/zero of /dev/null\\n    7     0 root     R     1200  0.0   0  0.0 top\\nThe dd command you’re running in the contai ner consumes as much CPU as it can,\\nbut it only runs a single thread so it ca n only use a single core. The Minikube VM,\\nwhich is where this example is running, has two CPU cores allotted to it. That’s why\\nthe process is shown consuming 50% of the whole CPU. \\n Fifty percent of two cores is obviously one whole core, which means the container\\nis using more than the 200 millicores you re quested in the pod specification. This is\\nexpected, because requests don’t limit the amount of CPU a container can use. You’d\\nneed to specify a CPU limit to do that. You’ll try that later,  but first, let’s see how spec-\\nifying resource requests in a pod affects the scheduling of the pod.\\n14.1.2 Understanding how resource requests affect scheduling\\nBy specifying resource requests, you’re sp ecifying the minimum amount of resources\\nyour pod needs. This information is what the Scheduler uses when scheduling the pod\\nto a node. Each node has a certain amount  of CPU and memory it can allocate to\\npods. When scheduling a pod, the Scheduler will only consider nodes with enough\\nunallocated resources to meet the pod’s resource requirements. If the amount of\\nunallocated CPU or memory is less than wh at the pod requests, Kubernetes will not\\nschedule the pod to that node, because th e node can’t provide the minimum amount\\nrequired by the pod.\\nUNDERSTANDING  HOW THE SCHEDULER  DETERMINES  IF A POD CAN FIT ON A NODE\\nWhat’s important and somewhat surprising he re is that the Scheduler doesn’t look at\\nhow much of each individual resource is be ing used at the exact time of scheduling\\nbut at the sum of resources requested by the existing pods deployed on the node.\\nEven though existing pods may be using le ss than what they’ve requested, scheduling\\nanother pod based on actual resource co nsumption would break the guarantee given\\nto the already deployed pods.\\n This is visualized in figure 14.1. Thre e pods are deployed on the node. Together,\\nthey’ve requested 80% of the node’s CP U and 60% of the node’s memory. Pod D,\\nshown at the bottom right of the figure, ca nnot be scheduled onto the node because it\\nrequests 25% of the CPU, which is more than the 20% of unallocated CPU. The fact\\nthat the three pods are currently using only 70% of the CPU makes no difference.Listing 14.2 Examining CPU and memory usage from within a container'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 438}, page_content='407 Requesting resources for a pod’s containers\\nUNDERSTANDING  HOW THE SCHEDULER  USES PODS’ REQUESTS  WHEN  SELECTING  THE BEST NODE \\nFOR A POD\\nYou may remember from chapter 11 that the Scheduler first filters the list of nodes to\\nexclude those that the pod can’t fit on and then prioritizes the remaining nodes per the\\nconfigured prioritization functions. Among others, two prioritization functions rank\\nnodes based on the amount of resources requested: LeastRequestedPriority  and\\nMostRequestedPriority . The first one prefers nodes with fewer requested resources\\n(with a greater amount of unallocated resour ces), whereas the second one is the exact\\nopposite—it prefers nodes that have the most  requested resources (a smaller amount of\\nunallocated CPU and memory). But, as we’v e discussed, they both consider the amount\\nof requested resources, not the amou nt of resources actually consumed.\\n The Scheduler is configured to use only  one of those functions. You may wonder\\nwhy anyone would want to use the MostRequestedPriority  function. After all, if you\\nhave a set of nodes, you usually want to spread CPU load evenly across them. However,\\nthat’s not the case when running on cloud infrastructure, where you can add and\\nremove nodes whenever necessary. By configuring the Scheduler to use the Most-\\nRequestedPriority  function, you guarantee that Kubernetes will use the smallest pos-\\nsible number of nodes while still providing each pod with the amount of CPU/memory\\nit requests. By keeping pods tightly pack ed, certain nodes are left vacant and can be\\nremoved. Because you’re paying for in dividual nodes, this saves you money.\\nINSPECTING  A NODE’S CAPACITY\\nLet’s see the Scheduler in action. You’ll deploy another pod with four times the\\namount of requested resources as before. Bu t before you do that, let’s see your node’s\\ncapacity. Because the Scheduler needs to  know how much CPU and memory each\\nnode has, the Kubelet reports this data to the API server, making it available throughPod CNode\\nPod A Unallocated CPU requests Pod B\\nPod A Currently unused CPU usage Pod B Pod C\\n0% 100%Pod A Memory requests Pod B Pod C\\nPod A Memory usage Pod B Pod C\\nCPU requests\\nMemory requestsUnallocated\\nCurrently unused\\nPod DPod D cannot be scheduled; its CPU\\nrequests exceed unallocated CPU\\nFigure 14.1 The Scheduler only cares about requests, not actual usage.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 439}, page_content='408 CHAPTER  14 Managing pods’ computational resources\\nthe Node resource. You can see it by using the kubectl  describe  command as in the\\nfollowing listing.\\n$ kubectl describe nodes\\nName:       minikube\\n...\\nCapacity:                       \\n  cpu:           2                 memory:        2048484Ki       \\n  pods:          110             \\nAllocatable:                         cpu:           2                  \\n  memory:        1946084Ki          \\n  pods:          110                ...\\nThe output shows two sets of amounts relate d to the available resources on the node:\\nthe node’s capacity  and allocatable  resources. The capacity represents the total resources\\nof a node, which may not all be available to pods. Certain resources may be reserved\\nfor Kubernetes and/or system components. The Scheduler bases its decisions only on\\nthe allocatable resource amounts.\\n In the previous example, the node called minikube  runs in a VM with two cores\\nand has no CPU reserved, making the wh ole CPU allocatable to pods. Therefore,\\nthe Scheduler should have no proble m scheduling another pod requesting 800\\nmillicores. \\n Run the pod now. You can use the YAML f ile in the code archive, or run the pod\\nwith the kubectl  run command like this:\\n$ kubectl run requests-pod-2 --image=busybox --restart Never\\n➥ --requests=\\'cpu=800m,memory=20Mi\\' -- dd if=/dev/zero of=/dev/null\\npod \"requests-pod-2\" created\\nLet’s see if it was scheduled:\\n$ kubectl get po requests-pod-2\\nNAME             READY     STATUS    RESTARTS   AGE\\nrequests-pod-2   1/1       Running   0          3m\\nOkay, the pod has been scheduled and is running. \\nCREATING  A POD THAT DOESN ’T FIT ON ANY NODE\\nYou now have two pods deployed, which toge ther have requested a total of 1,000 mil-\\nlicores or exactly 1 core. You should theref ore have another 1,000 millicores available\\nfor additional pods, right? You can deploy another pod with a resource request of\\n1,000 millicores. Use a similar command as before:\\n$ kubectl run requests-pod-3 --image=busybox --restart Never\\n➥ --requests=\\'cpu=1,memory=20Mi\\' -- dd if=/dev/zero of=/dev/null\\npod \"requests-pod-2\" createdListing 14.3 A node’s capacity and allocatable resources\\nThe overall capacity \\nof the node\\nThe resources \\nallocatable to pods'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 440}, page_content='409 Requesting resources for a pod’s containers\\nNOTE This time you’re specifying the CPU request in whole cores ( cpu=1 )\\ninstead of millicores ( cpu=1000m ).\\nSo far, so good. The pod has been accepted  by the API server (you’ll remember from\\nthe previous chapter that the API server can reject pods if they’re invalid in any way).\\nNow, check if the pod is running:\\n$ kubectl get po requests-pod-3\\nNAME             READY     STATUS    RESTARTS   AGE\\nrequests-pod-3   0/1       Pending   0          4m\\nEven if you wait a while, the pod is still stuck at Pending. You can see more informa-\\ntion on why that’s the case by using the kubectl  describe  command, as shown in\\nthe following listing.\\n$ kubectl describe po requests-pod-3\\nName:       requests-pod-3\\nNamespace:  defaultNode:       /                    \\n...\\nConditions:  Type           Status\\n  PodScheduled   False           \\n...Events:\\n... Warning  FailedScheduling     No nodes are available      \\n                                 that match all of the                                        following predicates::      \\n                                 Insufficient cpu  (1).       \\nThe output shows that the pod hasn’t been scheduled because it can’t fit on any node\\ndue to insufficient CPU on your single node. But why is that? The sum of the CPU\\nrequests of all three pods equals 2,000 millic ores or exactly two cores, which is exactly\\nwhat your node can provide. What’s wrong?\\nDETERMINING  WHY A POD ISN’T BEING  SCHEDULED\\nYou can figure out why the pod isn’t being scheduled by inspecting the node resource.Use the \\nkubectl  describe  node  command again and examine the output more\\nclosely in the following listing.\\n$ kubectl describe node\\nName:                   minikube...\\nNon-terminated Pods:    (7 in total)\\n  Namespace    Name            CPU Requ.   CPU Lim.  Mem Req.    Mem Lim.\\n  ---------    ----            ----------  --------  ---------   --------\\n  default      requests-pod    200m (10%)  0 (0%)    10Mi (0%)   0 (0%)Listing 14.4 Examining why a pod is stuck at Pending with kubectl  describe  pod\\nListing 14.5 Inspecting allocated resources on a node with kubectl  describe  nodeNo node is \\nassociated with the pod.\\nThe pod hasn’t \\nbeen scheduled.\\nScheduling has \\nfailed because of insufficient CPU.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 441}, page_content='410 CHAPTER  14 Managing pods’ computational resources\\n  default      requests-pod-2  800m (40%)  0 (0%)    20Mi (1%)   0 (0%)\\n  kube-system  dflt-http-b...  10m (0%)    10m (0%)  20Mi (1%)   20Mi (1%)\\n  kube-system  kube-addon-...  5m (0%)     0 (0%)    50Mi (2%)   0 (0%)\\n  kube-system  kube-dns-26...  260m (13%)  0 (0%)    110Mi (5%)  170Mi (8%)\\n  kube-system  kubernetes-...  0 (0%)      0 (0%)    0 (0%)      0 (0%)\\n  kube-system  nginx-ingre...  0 (0%)      0 (0%)    0 (0%)      0 (0%)Allocated resources:\\n  (Total limits may be over 100 percent, i.e., overcommitted.)\\n  CPU Requests   CPU Limits      Memory Requests Memory Limits\\n  ------------  ----------      --------------- -------------\\n  1275m (63%)    10m (0%)        210Mi (11%)     190Mi (9%)\\nIf you look at the bottom left of the list ing, you’ll see a total of 1,275 millicores have\\nbeen requested by the running pods, whic h is 275 millicores more than what you\\nrequested for the first two pods you depl oyed. Something is eating up additional\\nCPU resources. \\n You can find the culprit in the list of pods  in the previous listing. Three pods in the\\nkube-system  namespace have explicitly requested CPU resources. Those pods plus\\nyour two pods leave only 725 millicores available for addition al pods. Because your\\nthird pod requested 1,000 millicores, the Sche duler won’t schedule it to this node, as\\nthat would make the node overcommitted. \\nFREEING  RESOURCES  TO GET THE POD SCHEDULED\\nThe pod will only be scheduled when an ad equate amount of CPU is freed (when one\\nof the first two pods is deleted, for exam ple). If you delete your second pod, the\\nScheduler will be notified of the deletion  (through the watch mechanism described in\\nchapter 11) and will schedule your third po d as soon as the second pod terminates.\\nThis is shown in the following listing.\\n$ kubectl delete po requests-pod-2\\npod \"requests-pod-2\" deleted \\n$ kubectl get po\\nNAME             READY     STATUS        RESTARTS   AGE\\nrequests-pod     1/1       Running       0          2h\\nrequests-pod-2   1/1       Terminating    0          1h\\nrequests-pod-3   0/1       Pending       0          1h\\n$ kubectl get po\\nNAME             READY     STATUS    RESTARTS   AGErequests-pod     1/1       Running   0          2h\\nrequests-pod-3   1/1       Running   0          1h\\nIn all these examples, you’ve specified a re quest for memory, but it hasn’t played any\\nrole in the scheduling because your node has more than enough allocatable memory to\\naccommodate all your pods’ requests. Both CPU and memory requests are treated the\\nsame way by the Scheduler, but in contrast  to memory requests, a pod’s CPU requests\\nalso play a role elsewhere—while the pod is running. You’ll learn about this next.Listing 14.6 Pod is scheduled after deleting another pod'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 442}, page_content='411 Requesting resources for a pod’s containers\\n14.1.3 Understanding how CPU requests affect CPU time sharing\\nYou now have two pods running in your cl uster (you can disregard the system pods\\nright now, because they’re mostly idle). One has requested 200 millicores and the\\nother one five times as much. At the beginning of the chapter, we said Kubernetes dis-tinguishes between resource requests and limits. You haven’t defined any limits yet, so\\nthe two pods are in no way limited when it comes to how much CPU they can each\\nconsume. If the process inside each pod consumes as much CPU time as it can, howmuch CPU time does each pod get? \\n The CPU requests don’t only affect scheduling—they also determine how the\\nremaining (unused) CPU time is distribu ted between pods. Because your first pod\\nrequested 200 millicores of CPU and the ot her one 1,000 millicores, any unused CPU\\nwill be split among the two pods in a 1 to 5 ratio, as shown in figure 14.2. If both pods\\nconsume as much CPU as they can, the fi rst pod will get one sixth or 16.7% of the\\nCPU time and the other one the remaining five sixths or 83.3%.\\nBut if one container wants to use up as much  CPU as it can, while the other one is sit-\\nting idle at a given moment, the first cont ainer will be allowed to use the whole CPU\\ntime (minus the small amount of time used by the second container, if any). After all,\\nit makes sense to use all the available CPU if no one else is using it, right? As soon as\\nthe second container needs CPU time, it will ge t it and the first container will be throt-\\ntled back.\\n14.1.4 Defining and requesting custom resources\\nKubernetes also allows you to add your ow n custom resources to a node and request\\nthem in the pod’s resource requests. Init ially these were known as Opaque Integer\\nResources, but were replaced with Extend ed Resources in Kubernetes version 1.8.Pod A:\\n200 mCPU\\nrequestsPod B: 1000 m 800 m available\\nCPU\\nusage\\n2000 m 1000 m 0 mPod A and B requests\\nare in 1:5 ratio.Available CPU time is\\ndistributed in same ratio.\\nPod B: 1667 m133 m\\n(1/6)667 m\\n(5/6)\\nPod A:\\n333 m\\nFigure 14.2 Unused CPU time is distributed to containers based on their CPU requests.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 443}, page_content='412 CHAPTER  14 Managing pods’ computational resources\\n First, you obviously need to make Kube rnetes aware of your custom resource by\\nadding it to the Node object’s capacity  field. This can be done by performing a\\nPATCH  HTTP request. The resource name can be anything, such as example.org/my-\\nresource , as long as it doesn’t start with the kubernetes.io  domain. The quantity\\nmust be an integer (for example, you can’t set it to 100 millis, because 0.1 isn’t an inte-\\nger; but you can set it to 1000m or 2000m or , simply, 1 or 2). The value will be copied\\nfrom the capacity  to the allocatable  field automatically.\\n Then, when creating pods, you specify the same resource name and the requested\\nquantity under the resources.requests  field in the container spec or with --requests\\nwhen using kubectl  run like you did in previous examples. The Scheduler will make\\nsure the pod is only deployed to a node that has the requested amount of the custom\\nresource available. Every deployed pod obviously reduces the number of allocatable\\nunits of the resource.\\n An example of a custom reso urce could be the number of GPU units available on the\\nnode. Pods requiring the use of a GPU specif y that in their reques ts. The Scheduler then\\nmakes sure the pod is only scheduled to no des with at least one GPU still unallocated.\\n14.2 Limiting resources available to a container\\nSetting resource requests for containers in a pod ensures each container gets the min-\\nimum amount of resources it needs. Now let’s see the other side of the coin—the\\nmaximum amount the container will be allowed to consume. \\n14.2.1 Setting a hard limit for the amount of resources a container can use\\nWe’ve seen how containers are allowed to us e up all the CPU if all the other processes\\nare sitting idle. But you may want to prev ent certain containers from using up more\\nthan a specific amount of CPU. And you’ll always want to limit the amount of memory\\na container can consume. \\n CPU is a compressible resource, which me ans the amount used by a container can\\nbe throttled without affecting the process ru nning in the container in an adverse way.\\nMemory is obviously different—it’s incompre ssible. Once a process is given a chunk of\\nmemory, that memory can’t be taken away fr om it until it’s released by the process\\nitself. That’s why you need to limit the maximum amount of me mory a container can\\nbe given. \\n Without limiting memory, a container (o r a pod) running on a worker node may\\neat up all the available memory and affect  all other pods on the node and any new\\npods scheduled to the node (remember that new pods are scheduled to the nodebased on the memory requests and not actu al memory usage). A single malfunction-\\ning or malicious pod can practically make the whole node unusable.\\nCREATING  A POD WITH RESOURCE  LIMITS\\nTo prevent this from happening, Kubernetes allows you to specify resource limits for\\nevery container (along with, and virtually in  the same way as, resource requests). The\\nfollowing listing shows an example po d manifest with resource limits.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 444}, page_content='413 Limiting resources available to a container\\napiVersion: v1\\nkind: Podmetadata:\\n  name: limited-pod\\nspec:  containers:\\n  - image: busybox\\n    command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"]\\n    name: main\\n    resources:                  limits:             \\n        cpu: 1             \\n        memory: 20Mi       \\nThis pod’s container has resource limits configured for both CPU and memory. The\\nprocess or processes running inside the co ntainer will not be allowed to consume\\nmore than 1 CPU core and 20 mebibytes of memory. \\nNOTE Because you haven’t specified any re source requests, they’ll be set to\\nthe same values as the resource limits.\\nOVERCOMMITTING  LIMITS\\nUnlike resource requests, resource limits ar en’t constrained by the node’s allocatable\\nresource amounts. The sum of all limits of all the pods on  a node is allowed to exceed\\n100% of the node’s capacity (figure 14.3). Restated, reso urce limits can be overcom-\\nmitted. This has an important consequence— when 100% of the node’s resources are\\nused up, certain containers will need to be killed.\\nYou’ll see how Kubernetes decides which contai ners to kill in section 14.3, but individ-\\nual containers can be killed even if they try to use more than their resource limits\\nspecify. You’ll learn more about this next.Listing 14.7 A pod with a hard limit  on CPU and memory: limited-pod.yaml\\nSpecifying resource \\nlimits for the container\\nThis container will be \\nallowed to use at most 1 CPU core. The container will be\\nallowed to use up to 20\\nmebibytes of memory.\\nNode\\n0% 136% 100%Pod A Memory requests Pod B Pod C\\nPod A Memory limits Pod BUnallocated\\nPod C\\nFigure 14.3 The sum of resource limits of all pods on a node can exceed 100% of the node’s \\ncapacity.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 445}, page_content='414 CHAPTER  14 Managing pods’ computational resources\\n14.2.2 Exceeding the limits\\nWhat happens when a process running in a container tries to use a greater amount of\\nresources than it’s allowed to? \\n You’ve already learned that CPU is a compressible resource, and it’s only natural\\nfor a process to want to consume all of the CPU time when not waiting for an I/O\\no p e r a t i o n .  A s  y o u ’ v e  l e a r n e d ,  a  p r o c e s s ’  C P U  u s a g e  i s  t h r o t t l e d ,  s o  w h e n  a  C P Ulimit is set for a container, the process is n’t given more CPU time than the config-\\nured limit. \\n With memory, it’s different. When a process tries to allocate memory over its\\nlimit, the process is killed (it’s said the container is \\nOOMKilled , where OOM stands\\nfor Out Of Memory). If the pod’s restart policy is set to Always  or OnFailure , the\\nprocess is restarted immediately, so you may not even notice it getting killed. But if it\\nkeeps going over the memory limit and getti ng killed, Kubernetes will begin restart-\\ning it with increasing delays between restarts. You’ll see a CrashLoopBackOff  status\\nin that case:\\n$ kubectl get po\\nNAME        READY     STATUS             RESTARTS   AGEmemoryhog   0/1       CrashLoopBackOff    3          1m\\nThe CrashLoopBackOff  status doesn’t mean the Kubelet has given up. It means that\\nafter each crash, the Kubelet is increasing the time period before restarting the con-\\ntainer. After the first crash, it restarts th e container immediately and then, if it crashes\\nagain, waits for 10 seconds before restarting  it again. On subsequent crashes, this\\ndelay is then increased exponentially to 20,  40, 80, and 160 seconds, and finally lim-\\nited to 300 seconds. Once the interval hi ts the 300-second limit, the Kubelet keeps\\nrestarting the container indefinitely ever y five minutes until the pod either stops\\ncrashing or is deleted. \\n To examine why the container crashed,  you can check the pod’s log and/or use\\nthe kubectl  describe  pod command, as shown in the following listing.\\n$ kubectl describe pod\\nName:       memoryhog\\n...\\nContainers:  main:\\n    ...\\n    State:          Terminated                Reason:       OOMKilled           \\n      Exit Code:    137\\n      Started:      Tue, 27 Dec 2016 14:55:53 +0100      Finished:     Tue, 27 Dec 2016 14:55:58 +0100\\n    Last State:     Terminated            \\n      Reason:       OOMKilled                   Exit Code:    137Listing 14.8 Inspecting why a container terminated with kubectl  describe  pod\\nThe current container was \\nkilled because it was out of memory (OOM).\\nThe previous container \\nwas also killed because it was  OOM'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 446}, page_content='415 Limiting resources available to a container\\n      Started:      Tue, 27 Dec 2016 14:55:37 +0100\\n      Finished:     Tue, 27 Dec 2016 14:55:50 +0100\\n    Ready:          False...\\nThe OOMKilled  status tells you that the container was killed because it was out of mem-\\nory. In the previous listing, the container went over its memory limit and was killed\\nimmediately. \\n It’s important not to set memory limits too low if you don’t want your container to\\nbe killed. But containers can get OOMKilled  even if they aren’t over their limit. You’ll\\nsee why in section 14.3.2, but first, let’s di scuss something that catches most users off-\\nguard the first time they start spec ifying limits for their containers.\\n14.2.3 Understanding how apps in containers see limits\\nIf you haven’t deployed the pod fr om listing 14.7, deploy it now:\\n$ kubectl create -f limited-pod.yaml\\npod \"limited-pod\" created\\nNow, run the top command in the container, the wa y you did at the beginning of the\\nchapter. The command’s output is shown in the following listing.\\n$ kubectl exec -it limited-pod top\\nMem: 1450980K used, 597504K free , 22012K shrd, 65876K buff, 857552K cached\\nCPU: 10.0% usr 40.0% sys   0.0% nic 50.0% idle  0.0% io  0.0% irq  0.0% sirq\\nLoad average: 0.17 1.19 2.47 4/503 10\\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\\n    1     0 root     R     1192  0.0   1 49.9 dd if /dev/zero of /dev/null\\n    5     0 root     R     1196  0.0   0  0.0 top\\nFirst, let me remind you that the pod’s CPU limit is set to 1 core and its memory limit\\nis set to 20 MiB. Now, examine the output of the top command closely. Is there any-\\nthing that strikes you as odd?\\n Look at the amount of used and free  memory. Those numbers are nowhere near\\nthe 20 MiB you set as the limit for the container. Similarly, you set the CPU limit to\\none core and it seems like the main process is using only 50% of the available CPUtime, even though the \\ndd command, when used like you’re using it, usually uses all the\\nCPU it has available. What’s going on?\\nUNDERSTANDING  THAT CONTAINERS  ALWAYS  SEE THE NODE’S MEMORY , NOT THE CONTAINER ’S\\nThe top command shows the memory amounts of the whole node the container is\\nrunning on. Even though you set a limit on how much memory is available to a con-tainer, the container will not be aware of this limit. Listing 14.9 Running the top command in a CPU- and memory-limited container'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 447}, page_content='416 CHAPTER  14 Managing pods’ computational resources\\n This has an unfortunate effect on any application that looks up the amount of\\nmemory available on the system and uses that information to decide how much mem-\\nory it wants to reserve. \\n The problem is visible when running Java apps, especially if you don’t specify the\\nmaximum heap size for the Java Virtual Machine with the -Xmx  option. In that case,\\nthe JVM will set the maximum heap size based on the host’s total memory instead of\\nthe memory available to the container. When  you run your containerized Java apps in\\na Kubernetes cluster on your laptop, the pr oblem doesn’t manifest itself, because the\\ndifference between the memory limits you se t for the pod and the total memory avail-\\nable on your laptop is not that great. \\n But when you deploy your pod onto a production system, where nodes have much\\nmore physical memory, the JVM may go over  the container’s memory limit you config-\\nured and will be OOMKilled . \\n And if you think setting the -Xmx  option properly solves the issue, you’re wrong,\\nunfortunately. The -Xmx  option only constrains the he ap size, but does nothing about\\nthe JVM’s off-heap memory. Luckily, new versions of Java alleviate that problem by tak-ing the configured container limits into account.\\nUNDERSTANDING  THAT CONTAINERS  ALSO SEE ALL THE NODE’S CPU CORES\\nExactly like with memory, containers will also see all the node’s CPUs, regardless of\\nthe CPU limits configured for the container.  Setting a CPU limit to one core doesn’t\\nmagically only expose only one CPU core to  the container. All the CPU limit does is\\nconstrain the amount of CPU time the container can use. \\n A container with a one-core CPU limit running on a 64-core CPU will get 1/64th\\nof the overall CPU time. And even though it s limit is set to one core, the container’s\\nprocesses will not run on only one core. At  different points in time, its code may be\\nexecuted on different cores.\\n Nothing is wrong with this, right? While that’s generally the case, at least one sce-\\nnario exists where this situation is catastrophic.\\n Certain applications look up the number of CPUs on the system to decide how\\nmany worker threads they should run. Agai n, such an app will run fine on a develop-\\nment laptop, but when deployed on a node with a much bigger number of cores, it’s\\ngoing to spin up too many threads, all competing for the (possibly) limited CPU time.\\nAlso, each thread requires additional memory, causing the apps memory usage to sky-rocket. \\n You may want to use the Downward API to pass the CPU limit to the container and\\nuse it instead of relying on the number of CPUs your app can see on the system. Youcan also tap into the cgroups system directly  to get the configured CPU limit by read-\\ning the following files:\\n\\uf0a1/sys/fs/cgroup/cpu/cpu.cfs_quota_us\\n\\uf0a1/sys/fs/cgroup/cpu/cpu.cfs_period_us'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 448}, page_content='417 Understanding pod QoS classes\\n14.3 Understanding pod QoS classes\\nWe’ve already mentioned that resource limits can be overcommitted and that a\\nnode can’t necessarily provide all its pods the amount of resources specified in their\\nresource limits. \\n Imagine having two pods, where pod A is using, let’s say, 90% of the node’s mem-\\nory and then pod B suddenly requires more memory than what it had been using up\\nto that point and the node can’t provide the required amount of memory. Which\\ncontainer should be killed? Should it be pod B, because its request for memory can’t\\nbe satisfied, or should pod A be killed to free up memory, so it can be provided to\\npod B? \\n Obviously, it depends. Kubernetes can’t make a proper decision on its own. You\\nneed a way to specify which po ds have priority in such cases. Kubernetes does this by\\ncategorizing pods into three Qu ality of Service (QoS) classes:\\n\\uf0a1BestEffort  (the lowest priority)\\n\\uf0a1Burstable\\n\\uf0a1Guaranteed  (the highest)\\n14.3.1 Defining the QoS class for a pod\\nYou might expect these classes to be assignable to pods through a separate field in the\\nmanifest, but they aren’t. The QoS class is  derived from the combination of resource\\nrequests and limits for the pod’s containers. Here’s how.\\nASSIGNING  A POD TO THE BESTEFFORT  CLASS\\nThe lowest priority QoS class is the BestEffort  class. It’s assigned to pods that don’t\\nhave any requests or limits set at all (in an y of their containers). This is the QoS class\\nthat has been assigned to all the pods yo u created in previous chapters. Containers\\nrunning in these pods have had no reso urce guarantees whatsoever. In the worst\\ncase, they may get almost no CPU time at  all and will be the first ones killed when\\nmemory needs to be freed for other pods. But because a BestEffort  pod has no\\nmemory limits set, its containers may use as much memory as they want, if enoughmemory is available.\\nASSIGNING  A POD TO THE GUARANTEED  CLASS\\nOn the other end of the spectrum is the Guaranteed  QoS class. This class is given to\\npods whose containers’ requests are equal to  the limits for all resources. For a pod’s\\nclass to be Guaranteed , three things need to be true:\\n\\uf0a1Requests and limits need to be set for both CPU and memory.\\n\\uf0a1They need to be set for each container.\\n\\uf0a1They need to be equal (the limit need s to match the request for each resource\\nin each container).\\nBecause a container’s resource requests, if not set explicitly, default to the limits,\\nspecifying the limits for all resources (for each container in the pod) is enough for'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 449}, page_content='418 CHAPTER  14 Managing pods’ computational resources\\nthe pod to be Guaranteed . Containers in those pods get the requested amount of\\nresources, but cannot consume additional on es (because their limits are no higher\\nthan their requests). \\nASSIGNING  THE BURSTABLE  QOS CLASS  TO A POD\\nIn between BestEffort  and Guaranteed  is the Burstable  QoS class. All other pods\\nfall into this class. This includes single-container pods where the container’s limits\\ndon’t match its requests and all pods wh ere at least one container has a resource\\nrequest specified, but not the limit. It also includes pods where one container’s\\nrequests match their limits, but another container has no requests or limits specified.\\nBurstable  pods get the amount of resources they request, but are allowed to use addi-\\ntional resources (up to the limit) if needed.\\nUNDERSTANDING  HOW THE RELATIONSHIP  BETWEEN  REQUESTS  AND LIMITS  DEFINES  THE QOS CLASS\\nAll three QoS classes and their relationships with requests and limits are shown in fig-\\nure 14.4.\\nThinking about what QoS class a pod has can make your head spin, because it involves\\nmultiple containers, multiple resources, and all the possible relationships between\\nrequests and limits. It’s easier if you start by thinking about QoS at the container level\\n(although QoS classes are a property of po ds, not containers) and then derive the\\npod’s QoS class from the QoS classes of containers. \\nFIGURING  OUT A CONTAINER ’S QOS CLASS\\nTable 14.1 shows the QoS class based on  how resource requests and limits are\\ndefined on a single container. For single -container pods, the QoS class applies to\\nthe pod as well.\\n BestEffort\\nQoSRequests Limits\\nBurstable\\nQoSRequestsLimits\\nGuaranteed\\nQoSRequests LimitsRequests and\\nlimits are not setRequests are\\nbelow limitsRequests\\nequal limits\\nFigure 14.4 Resource requests, limits and QoS classes'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 450}, page_content='419 Understanding pod QoS classes\\nNOTE If only requests are set, but not lim its, refer to the table rows where\\nrequests are less than the limits. If only  limits are set, requests default to the\\nlimits, so refer to the rows where requests equal limits.\\nFIGURING  OUT THE QOS CLASS  OF A POD WITH MULTIPLE  CONTAINERS\\nFor multi-container pods, if all the containers  have the same QoS class, that’s also the\\npod’s QoS class. If at least one container has a different class, the pod’s QoS class is\\nBurstable , regardless of what the container classes are. Table 14.2 shows how a two-\\ncontainer pod’s QoS class relates to the classes of its two containers. You can easily\\nextend this to pods with more than two containers.\\nNOTE A pod’s QoS class is shown when running kubectl  describe  pod and\\nin the pod’s YAML/JSON manifest in the status.qosClass  field.\\nWe’ve explained how QoS classes are determined , but we still need to look at how they\\ndetermine which container gets ki lled in an overcommitted system.Table 14.1 The QoS class of a single-container pod based on resource requests and limits\\nCPU requests vs. limits Memory requests vs. limits Container QoS class\\nNone set None set BestEffort\\nNone set Requests < Limits Burstable\\nNone set Requests = Limits Burstable\\nRequests < Limits None set Burstable\\nRequests < Limits Requests < Limits Burstable\\nRequests < Limits Requests = Limits Burstable\\nRequests = Limits Requests = Limits Guaranteed\\nTable 14.2 A Pod’s QoS class derived from the classes of its containers\\nContainer 1 QoS class Container 2 QoS class Pod’s QoS class\\nBestEffort BestEffort BestEffort\\nBestEffort Burstable Burstable\\nBestEffort Guaranteed Burstable\\nBurstable Burstable BurstableBurstable Guaranteed Burstable\\nGuaranteed Guaranteed Guaranteed'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 451}, page_content='420 CHAPTER  14 Managing pods’ computational resources\\n14.3.2 Understanding which process gets killed when memory is low\\nWhen the system is overcommitted, the QoS classes determine which container gets\\nkilled first so the freed resources can be give n to higher priority pods. First in line to\\nget killed are pods in the BestEffort  class, followed by Burstable  pods, and finally\\nGuaranteed  pods, which only get killed if system processes need memory.\\nUNDERSTANDING  HOW QOS CLASSES  LINE UP\\nLet’s look at the example shown in figure  14.5. Imagine having two single-container\\npods, where the first one has the BestEffort  QoS class, and the second one’s is\\nBurstable . When the node’s whole memory is already maxed out and one of the pro-\\ncesses on the node tries to allocate more me mory, the system will need to kill one of\\nthe processes (perhaps even the process tr ying to allocate additional memory) to\\nhonor the allocation request. In this case, the process running in the BestEffort  pod\\nwill always be killed before the one in the Burstable  pod.\\nObviously, a BestEffort  pod’s process will also be killed before any Guaranteed  pods’\\nprocesses are killed. Likewise, a Burstable  pod’s process will also be killed before that\\nof a Guaranteed  pod. But what happens if there are only two Burstable  pods? Clearly,\\nthe selection process needs to prefer one over the other.\\nUNDERSTANDING  HOW CONTAINERS  WITH THE SAME QOS CLASS  ARE HANDLED\\nEach running process has an OutOfMemor y (OOM) score. The system selects the\\nprocess to kill by comparing OOM scores of  all the running processes. When memory\\nneeds to be freed, the process wi th the highest score gets killed.\\n OOM scores are calculated from two thin gs: the percentage of the available mem-\\nory the process is consuming and a fixed OOM  score adjustment, which is based on the\\npod’s QoS class and the container’s requeste d memory. When two single-container pods\\nexist, both in the Burstable  class, the system will kill the one using more of its requestedBestEffort\\nQoS pod\\nPod A\\nFirst in line\\nto be killedActual usage Requests LimitsBurstable\\nQoS pod\\nPod B\\nSecond in line\\nto be killed90% used Requests LimitsBurstable\\nQoS pod\\nPod C\\nThird in line\\nto be killed70% used Requests LimitsGuaranteed\\nQoS pod\\nPod D\\nLast to\\nbe killed99% used Requests Limits\\nFigure 14.5 Which pods get killed first'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 452}, page_content='421 Setting default requests and limits for pods per namespace\\nmemory than the other, percentage-wise. Th at’s why in figure 14.5, pod B, using 90%\\nof its requested memory, gets killed before  pod C, which is only using 70%, even\\nthough it’s using more megabytes of memory than pod B. \\n This shows you need to be mindful of not only the relationship between requests\\nand limits, but also of requests and th e expected actual memory consumption. \\n14.4 Setting default requests and limits for pods per \\nnamespace\\nWe’ve looked at how resource requests and limits can be set for each individual con-\\ntainer. If you don’t set them, the container is  at the mercy of all other containers that\\ndo specify resource requests and limits. It’s  a good idea to set requests and limits on\\nevery container.\\n14.4.1 Introducing the LimitRange resource\\nInstead of having to do this for every container, you can also do it by creating a Limit-\\nRange resource. It allows you to specify (f or each namespace) not only the minimum\\nand maximum limit you can set on a containe r for each resource, but also the default\\nresource requests for containers that don’t specify requests explicitly, as depicted in\\nfigure 14.6.\\nAPI server\\nValidation\\nPod A\\nmanifest\\n- Requests\\n- LimitsPod A\\nmanifest\\n- Requests\\n- Limits\\nPod B\\nmanifest\\n- No\\nrequestsor limitsPod B\\nmanifest\\n- No\\nrequestsor limitsDefaultingRejected because\\nrequests and limits areoutside min/max values\\nDefaults\\nappliedNamespace XYZ\\nLimitRange\\nPod B\\nmanifest\\n- Default\\nrequests\\n- Default\\nlimitsPod B\\n- Default requests\\n- Default limits- Min/max CPU- Min/max memory\\n- Default requests- Default limits\\nFigure 14.6 A LimitRange is used for validation and defaulting pods.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 453}, page_content='422 CHAPTER  14 Managing pods’ computational resources\\nLimitRange resources are used by the Li mitRanger Admission Control plugin (we\\nexplained what those plugins are in chapter 11). When a pod manifest is posted to the\\nAPI server, the LimitRanger plugin validates th e pod spec. If validation fails, the mani-\\nfest is rejected immediately. Because of this, a great use-case for LimitRange objects isto prevent users from creating pods that are bigger than any node in the cluster. With-\\nout such a LimitRange, the API server will  gladly accept the pod, but then never\\nschedule it. \\n The limits specified in a LimitRange re source apply to each individual pod/con-\\ntainer or other kind of object created in the same namespace as the LimitRange\\nobject. They don’t limit the total amount of  resources available across all the pods in\\nthe namespace. This is specified through ResourceQuota objects, which are explained\\nin section 14.5. \\n14.4.2 Creating a LimitRange object\\nLet’s look at a full example of a LimitRange  and see what the individual properties do.\\nThe following listing shows the full de finition of a LimitRange resource.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:  name: example\\nspec:\\n  limits:  - type: Pod           \\n    min:                         \\n      cpu: 50m                   \\n      memory: 5Mi                \\n    max:                                cpu: 1                      \\n      memory: 1Gi                 \\n  - type: Container                 defaultRequest:             \\n      cpu: 100m                 \\n      memory: 10Mi                  default:                      \\n      cpu: 200m                   \\n      memory: 100Mi                   min:                         \\n      cpu: 50m                   \\n      memory: 5Mi                    max:                         \\n      cpu: 1                     \\n      memory: 1Gi                    maxLimitRequestRatio:         \\n      cpu: 4                      \\n      memory: 10                  Listing 14.10 A LimitRange resource: limits.yaml\\nSpecifies the \\nlimits for a pod \\nas a whole\\nMinimum CPU and memory all the \\npod’s containers can request in total\\nMaximum CPU and memory all the pod’s \\ncontainers can request (and limit)\\nThe\\ncontainer\\nlimits are\\nspecified\\nbelow this\\nline.Default requests for CPU and memory \\nthat will be applied to containers that \\ndon’t specify them explicitly\\nDefault limits for containers \\nthat don’t specify them\\nMinimum and maximum \\nrequests/limits that a container can have\\nMaximum ratio between \\nthe limit and request for each resource'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 454}, page_content='423 Setting default requests and limits for pods per namespace\\n  - type: PersistentVolumeClaim      \\n    min:                             \\n      storage: 1Gi                       max:                             \\n      storage: 10Gi                  \\nAs you can see from the previous example, the minimum and maximum limits for a\\nwhole pod can be configured. They apply to the sum of all the pod’s containers’requests and limits. \\n Lower down, at the container level, you can set not only the minimum and maxi-\\nmum, but also default resource requests (\\ndefaultRequest ) and default limits\\n(default ) that will be applied to each containe r that doesn’t specify them explicitly. \\n Beside the min, max, and default valu es, you can even set the maximum ratio of\\nlimits vs. requests. The previous listing sets the CPU maxLimitRequestRatio  to 4,\\nwhich means a container’s CPU limits will not be allowed to be more than four times\\ngreater than its CPU requests. A containe r requesting 200 millicores will not be\\naccepted if its CPU limit is  set to 801 millicores or hi gher. For memory, the maximum\\nratio is set to 10.\\n In chapter 6 we looked at PersistentVolu meClaims (PVC), which allow you to claim\\na certain amount of persistent storage sim ilarly to how a pod’s containers claim CPU\\nand memory. In the same way you’re limiting the minimum and maximum amount of\\nCPU a container can request, you should al so limit the amount of storage a single\\nPVC can request. A LimitRange object allows you to do that as well, as you can see at\\nthe bottom of the example.\\n The example shows a single LimitRange ob ject containing limits for everything,\\nbut you could also split them into multiple objects if you prefer to have them orga-\\nnized per type (one for pod limits, anothe r for container limits, and yet another for\\nPVCs, for example). Limits from multiple  LimitRange objects are all consolidated\\nwhen validating a pod or PVC.\\n Because the validation (and defaults) configured in a LimitRange object is per-\\nformed by the API server when it receives  a new pod or PVC manifest, if you modify\\nthe limits afterwards, existing pods and PV Cs will not be revalidated—the new limits\\nwill only apply to pods and PVCs created afterward. \\n14.4.3 Enforcing the limits\\nWith your limits in place, you can now try creating a pod that requests more CPU than\\nallowed by the LimitRange. You’ll find the YAML for the pod in the code archive. The\\nnext listing only shows the part  relevant to the discussion.\\n    resources:\\n      requests:\\n        cpu: 2Listing 14.11 A pod with CPU requests great er than the limit: limits-pod-too-big.yamlA LimitRange can also set \\nthe minimum and maximum amount of storage a PVC \\ncan request.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 455}, page_content='424 CHAPTER  14 Managing pods’ computational resources\\nThe pod’s single container is requesting two CPUs, which is more than the maximum\\nyou set in the LimitRange earlier. Creati ng the pod yields the following result:\\n$ kubectl create -f limits-pod-too-big.yaml \\nError from server (Forbidden): erro r when creating \"limits-pod-too-big.yaml\": \\npods \"too-big\" is forbidden: [\\n  maximum cpu usage per Pod is 1, but request is 2.,   maximum cpu usage per Container is 1, but request is 2.]\\nI’ve modified the output slightly to make it more legible. The nice thing about the\\nerror message from the server is that it lis ts all the reasons why the pod was rejected,\\nnot only the first one it encountered. As you can see, the pod was rejected for two rea-sons: you requested two CPUs for the container, but the maximum CPU limit for a\\ncontainer is one. Likewise, the pod as a whole requested two CPUs, but the maximum\\nis one CPU (if this was a multi-container pod, even if each individual container\\nrequested less than the maximum amount of  CPU, together they’d still need to\\nrequest less than two CPUs to pass the maximum CPU for pods). \\n14.4.4 Applying default resource requests and limits\\nNow let’s also see how default resource requ ests and limits are set on containers that\\ndon’t specify them. Deploy the kubia-manual  pod from chapter 3 again:\\n$ kubectl create -f ../Chapter03/kubia-manual.yaml\\npod \"kubia-manual\" created\\nBefore you set up your LimitRange object, all your pods were created without any\\nresource requests or limits, but now the de faults are applied automatically when creat-\\ning the pod. You can confirm this by describing the kubia-manual  pod, as shown in\\nthe following listing.\\n$ kubectl describe po kubia-manual\\nName:           kubia-manual\\n...\\nContainers:  kubia:\\n    Limits:\\n      cpu:      200m      memory:   100Mi\\n    Requests:\\n      cpu:      100m      memory:   10Mi\\nThe container’s requests and limits match the ones you specified in the LimitRange\\nobject. If you used a different LimitRange  specification in another namespace, pods\\ncreated in that namespace would obviously have different requests and limits. This\\nallows admins to configure default, min,  and max resources for pods per namespace.Listing 14.12 Inspecting limits that were applied to a pod automatically'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 456}, page_content='425 Limiting the total resources available in a namespace\\nIf namespaces are used to separate differ ent teams or to separa te development, QA,\\nstaging, and production pods running in th e same Kubernetes cluster, using a differ-\\nent LimitRange in each namespace ensures la rge pods can only be created in certain\\nnamespaces, whereas others are constrained to smaller pods.\\n But remember, the limits configured in a LimitRange only apply to each individual\\npod/container. It’s still possible to create many pods and eat up all the resources avail-\\nable in the cluster. LimitRanges don’t prov ide any protection fr om that. A Resource-\\nQuota object, on the other hand, do es. You’ll learn about them next.\\n14.5 Limiting the total resources available in a namespace\\nAs you’ve seen, LimitRanges only apply to  individual pods, but cluster admins also\\nneed a way to limit the total amount of re sources available in a namespace. This is\\nachieved by creating a ResourceQuota object. \\n14.5.1 Introducing the ResourceQuota object\\nIn chapter 10 we said that several Admissi on Control plugins running inside the API\\nserver verify whether the pod may be create d or not. In the previous section, I said\\nthat the LimitRanger plugin enforces the policies configured in LimitRange resources.\\nSimilarly, the ResourceQuota Admission Control plugin checks whether the pod\\nbeing created would cause the configured ResourceQuota to be exceeded. If that’s\\nthe case, the pod’s creation is rejected. Be cause resource quotas are enforced at pod\\ncreation time, a ResourceQuota object only  affects pods created after the Resource-\\nQuota object is created—creating it has no effect on existing pods.\\n A ResourceQuota limits the amount of co mputational resources the pods and the\\namount of storage PersistentVolumeClaims in a namespace can consume. It can also\\nlimit the number of pods, claims, and othe r API objects users are allowed to create\\ninside the namespace. Because you’ve mostly  dealt with CPU and memory so far, let’s\\nstart by looking at how to specify quotas for them.\\nCREATING  A RESOURCE QUOTA FOR CPU AND MEMORY\\nThe overall CPU and memory all the pods in a namespace are allowed to consume is\\ndefined by creating a ResourceQuota obje ct as shown in the following listing.\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:  name: cpu-and-mem\\nspec:\\n  hard:    requests.cpu: 400m\\n    requests.memory: 200Mi\\n    limits.cpu: 600m    limits.memory: 500MiListing 14.13 A ResourceQuota resource fo r CPU and memory: quota-cpu-memory.yaml'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 457}, page_content='426 CHAPTER  14 Managing pods’ computational resources\\nInstead of defining a single total for each  resource, you define separate totals for\\nrequests and limits for both CPU and memory. You’ll notice the structure is a bit dif-\\nferent, compared to that of a LimitRange. Here, both the requests and the limits for\\nall resources are defined in a single place. \\n This ResourceQuota sets the maximum amount of CPU pods in the namespace\\ncan request to 400 millicores. The maximum total CPU limits in the namespace are\\nset to 600 millicores. For memory, the maxi mum total requests are set to 200 MiB,\\nwhereas the limits are set to 500 MiB.\\n A ResourceQuota object applies to the namespace it’s created in, like a Limit-\\nRange, but it applies to all the pods’ reso urce requests and limits in total and not to\\neach individual pod or container se parately, as shown in figure 14.7.\\nINSPECTING  THE QUOTA  AND QUOTA  USAGE\\nAfter you post the ResourceQuota object  to the API server, you can use the kubectl\\ndescribe  command to see how much of the qu ota is already used up, as shown in\\nthe following listing.\\n$ kubectl describe quota\\nName:           cpu-and-mem\\nNamespace:      default\\nResource        Used   Hard\\n--------        ----   ----limits.cpu      200m   600m\\nlimits.memory   100Mi  500Mi\\nrequests.cpu    100m   400mrequests.memory 10Mi   200Mi\\nI only have the kubia-manual  pod running, so the Used  column matches its resource\\nrequests and limits. When I run additional pods, their requests and limits are added to\\nthe used amounts.Listing 14.14 Inspecting the ResourceQuota with kubectl  describe  quotaLimitRange ResourceQuotaNamespace: FOO\\nPod A Pod B Pod CLimitRange ResourceQuotaNamespace: BAR\\nPod D Pod E Pod F\\nFigure 14.7 LimitRanges apply to individual pods; ResourceQuotas apply to all pods in the \\nnamespace.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 458}, page_content='427 Limiting the total resources available in a namespace\\nCREATING  A LIMITRANGE ALONG  WITH A RESOURCE QUOTA\\nOne caveat when creating a ResourceQuota is that you will also want to create a Limit-\\nRange object alongside it. In your case, yo u have a LimitRange configured from the\\nprevious section, but if you didn’t have one, you couldn’t run the kubia-manual  pod,\\nbecause it doesn’t specify any resource requ ests or limits. Here’s what would happen\\nin that case:\\n$ kubectl create -f ../Chapter03/kubia-manual.yaml\\nError from server (Forbidden): error when creating \"../Chapter03/kubia-\\nmanual.yaml\": pods \"kubia-manual\" is forbidden: failed quota: cpu-and-\\nmem: must specify limits.cpu,limits.memory,requests.cpu,requests.memory\\nWhen a quota for a specific resource (CPU  o r  m e m o r y )  i s  c o n f i g u r e d  ( r e q u e s t  o r\\nlimit), pods need to have the request or limi t (respectively) set fo r that same resource;\\notherwise the API server will not accept the pod. That’s why having a LimitRange with\\ndefaults for those resources can make life  a bit easier for people creating pods.\\n14.5.2 Specifying a quota for persistent storage\\nA ResourceQuota object can also limit the amount of persistent storage that can be\\nclaimed in the namespace, as shown in the following listing.\\napiVersion: v1\\nkind: ResourceQuotametadata:\\n  name: storage\\nspec:  hard:\\n    requests.storage: 500Gi                               \\n    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi     \\n    standard.storageclass.storage.k8s.io/requests.storage: 1Ti\\nIn this example, the amount of storage all PersistentVolumeClaims in a namespace\\ncan request is limited to 500 GiB (by the requests.storage  entry in the Resource-\\nQuota object). But as you’ll remember from  chapter 6, PersistentVolumeClaims can\\nrequest a dynamically provisioned Persistent Volume of a specific StorageClass. That’s\\nwhy Kubernetes also makes it possible to de fine storage quotas for each StorageClass\\nindividually. The previous example limits the total amount of claimable SSD storage\\n(designated by the ssd StorageClass) to 300 GiB. Th e less-performant HDD storage\\n(StorageClass standard ) is limited to 1 TiB.\\n14.5.3 Limiting the number of objects that can be created\\nA ResourceQuota can also be configured to  limit the number of Pods, Replication-\\nControllers, Services, and other objects insi de a single namespace. This allows the\\ncluster admin to limit the number of objects users can create based on their paymentListing 14.15 A ResourceQuota for storage: quota-storage.yaml\\nThe amount of \\nstorage claimable overall\\nThe amount \\nof claimable \\nstorage in \\nStorageClass ssd'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 459}, page_content='428 CHAPTER  14 Managing pods’ computational resources\\nplan, for example, and can also limit the number of public IPs or node ports Ser-\\nvices can use. \\n The following listing shows what a Resour ceQuota object that limits the number of\\nobjects may look like.\\napiVersion: v1\\nkind: ResourceQuotametadata:\\n  name: objects\\nspec:  hard:\\n    pods: 10                        \\n    replicationcontrollers: 5           secrets: 10                     \\n    configmaps: 10                  \\n    persistentvolumeclaims: 4           services: 5                      \\n    services.loadbalancers: 1        \\n    services.nodeports: 2                ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2   \\nThe ResourceQuota in this listing allows user s to create at most 10 Pods in the name-\\nspace, regardless if they’re created manually  or by a ReplicationController, Replica-\\nSet, DaemonSet, Job, and so on. It also limits the number of ReplicationControllers to\\nfive. A maximum of five Services can be created, of which only one can be a LoadBal-\\nancer -type Service, and only two can be NodePort  Services. Similar to how the maxi-\\nmum amount of requested storage can be specified per StorageClass, the number of\\nPersistentVolumeClaims can also be limited per StorageClass.\\n Object count quotas can currently be set for the following objects: \\n\\uf0a1Pods\\n\\uf0a1ReplicationControllers \\n\\uf0a1Secrets\\n\\uf0a1ConfigMaps\\n\\uf0a1PersistentVolumeClaims\\n\\uf0a1Services (in general), and for two specific types of Services, such as Load-\\nBalancer  Services ( services.loadbalancers ) and NodePort  Services ( ser-\\nvices.nodeports ) \\nFinally, you can even set an object count quota for ResourceQuota objects themselves.\\nThe number of other objects, such as Rep licaSets, Jobs, Deployments, Ingresses, and\\nso on, cannot be limited yet (but this may have changed since the book was published,\\nso please check the documentation for up-to-date information).Listing 14.16 A ResourceQuota for max number of resources: quota-object-count.yaml\\nOnly 10 Pods, 5 ReplicationControllers, \\n10 Secrets, 10 ConfigMaps, and \\n4 PersistentVolumeClaims can be \\ncreated in the namespace.\\nFive Services overall can be created, \\nof which at most one can be a \\nLoadBalancer Service and at most \\ntwo can be NodePort Services.\\nOnly two PVCs can claim storage\\nwith the ssd StorageClass.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 460}, page_content='429 Limiting the total resources available in a namespace\\n14.5.4 Specifying quotas for specific pod states and/or QoS classes\\nThe quotas you’ve created so far have appl ied to all pods, regardless of their current\\nstate and QoS class. But quotas ca n also be limited to a set of quota scopes . Four scopes are\\ncurrently available: BestEffort , NotBestEffort , Terminating , and NotTerminating . \\n The BestEffort  and NotBestEffort  scopes determine whether the quota applies\\nto pods with the BestEffort  QoS class or with one of the other two classes (that is,\\nBurstable  and Guaranteed ). \\n The other two scopes ( Terminating  and NotTerminating ) don’t apply to pods\\nthat are (or aren’t) in the process of shu tting down, as the name might lead you to\\nbelieve. We haven’t talked about this, but you can specify how long each pod is\\nallowed to run before it’s terminated and marked as Failed . This is done by setting\\nthe activeDeadlineSeconds  field in the pod spec. This  property defines the number\\nof seconds a pod is allowed to be active on the node relative to its start time before it’s\\nmarked as Failed  and then terminated. The Terminating  quota scope applies to pods\\nthat have the activeDeadlineSeconds  set, whereas the NotTerminating  applies to\\nthose that don’t. \\n When creating a ResourceQuota, you can specify the scopes that it applies to. A\\npod must match all the specified scopes for the quota to apply to it. Additionally, what\\na quota can limit depends on the quota’s scope. BestEffort  scope can only limit the\\nnumber of pods, whereas the other three scopes can limit the number of pods,\\nCPU/memory requests, and CPU/memory limits. \\n If, for example, you want the quota to apply only to BestEffort , NotTerminating\\npods, you can create the ResourceQuota object shown in the following listing.\\napiVersion: v1\\nkind: ResourceQuotametadata:\\n  name: besteffort-notterminating-pods\\nspec:  scopes:                 \\n  - BestEffort            \\n  - NotTerminating          hard: \\n    pods: 4          \\nThis quota ensures that at mo st four pods exist with the BestEffort  QoS class,\\nwhich don’t have an active dead line. If the quota was targeting NotBestEffort  pods\\ninstead, you could also specify requests.cpu , requests.memory , limits.cpu , and\\nlimits.memory .\\nNOTE Before you move on to the next sect ion of this chapter, please delete\\nall the ResourceQuota and LimitRange  resources you created. You won’tListing 14.17 ResourceQuota for BestEffort/NotTerminating  pods: \\nquota-scoped.yaml\\nThis quota only applies to pods \\nthat have the BestEffort QoS and don’t have an active deadline set.\\nOnly four such \\npods can exist.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 461}, page_content='430 CHAPTER  14 Managing pods’ computational resources\\nneed them anymore and they may inte rfere with examples in the following\\nchapters.\\n14.6 Monitoring pod resource usage\\nProperly setting resource requests and limits  is crucial for getting the most out of your\\nKubernetes cluster. If requests are set to o high, your cluster nodes will be underuti-\\nlized and you’ll be throwing money away. If  you set them too low, your apps will be\\nCPU-starved or even killed by the OOM Kill er. How do you find the sweet spot for\\nrequests and limits?\\n You find it by monitoring the actual re source usage of your containers under the\\nexpected load levels. Once the application is exposed to the public, you should keep\\nmonitoring it and adjust the resource  requests and limits if required.\\n14.6.1 Collecting and retrieving actual resource usages\\nHow does one monitor apps running in Kubernetes? Luckily, the Kubelet itself\\nalready contains an agent called cAdvisor , which performs the basic collection of\\nresource consumption data for both indivi dual containers running on the node and\\nthe node as a whole. Gathering those statis tics centrally for the whole cluster requires\\nyou to run an additional component called Heapster. \\n Heapster runs as a pod on one of th e nodes and is exposed through a regular\\nKubernetes Service, making it accessible at  a stable IP address. It collects the data\\nfrom all cAdvisors in the cluster and exposes it in a single location. Figure 14.8shows the flow of the metrics data from the pods, through cAdvisor and finally into\\nHeapster.\\nKubeletcAdvisorNode 1\\nPod\\nPod\\nKubeletcAdvisorNode 2\\nPod\\nKubeletcAdvisorNode X\\nPod\\nHeapster\\nEach cAdvisor collects metrics from\\ncontainers running on its node.Heapster runs on one of the nodes as a\\npod and collects metrics from all nodes.\\nFigure 14.8 The flow of metrics data into Heapster'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 462}, page_content='431 Monitoring pod resource usage\\nThe arrows in the figure show how the metr ics data flows. They don’t show which com-\\nponent connects to which to get the da ta. The pods (or the containers running\\ntherein) don’t know anything about cAdvisor, and cAdvisor doesn’t know anything\\nabout Heapster. It’s Heapster that connects to  all the cAdvisors, and it’s the cAdvisors\\nthat collect the container and node usage data  without having to talk to the processes\\nrunning inside the pods’ containers.\\nENABLING  HEAPSTER\\nIf you’re running a cluster in Google Ku bernetes Engine, Heapster is enabled by\\ndefault. If you’re using Minikube, it’s ava ilable as an add-on and can be enabled with\\nthe following command:\\n$ minikube addons enable heapster\\nheapster was successfully enabled\\nTo run Heapster manually in other types of Kubernetes cluste rs, you can refer to\\ninstructions located at https:/ /github.com/kubernetes/heapster . \\n After enabling Heapster, you’ll need to wait  a few minutes for it to collect metrics\\nbefore you can see resource usage statisti cs for your cluster, so be patient. \\nDISPLAYING  CPU AND MEMORY  USAGE  FOR CLUSTER  NODES\\nRunning Heapster in your cluster makes it possible to obtain resource usages for\\nnodes and individual pods through the kubectl  top command. To see how much\\nCPU and memory is being used on your nodes, you can run the command shown in\\nthe following listing.\\n$ kubectl top node\\nNAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%\\nminikube   170m         8%        556Mi           27%\\nThis shows the actual, current CPU and memo ry usage of all the pods running on the\\nnode, unlike the kubectl  describe  node  command, which shows the amount of CPU\\nand memory requests and limits instead of actual runtime usage data. \\nDISPLAYING  CPU AND MEMORY  USAGE  FOR INDIVIDUAL  PODS\\nTo see how much each individual pod is using, you can use the kubectl  top pod com-\\nmand, as shown in the following listing.\\n$ kubectl top pod --all-namespaces\\nNAMESPACE      NAME                             CPU(cores)   MEMORY(bytes)\\nkube-system    influxdb-grafana-2r2w9           1m           32Mi\\nkube-system    heapster-40j6d                   0m           18MiListing 14.18 Actual CPU and memory usage of nodes\\nListing 14.19 Actual CPU and memory usages of pods'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 463}, page_content='432 CHAPTER  14 Managing pods’ computational resources\\ndefault        kubia-3773182134-63bmb           0m           9Mi\\nkube-system    kube-dns-v20-z0hq6               1m           11Mi\\nkube-system    kubernetes-dashboard-r53mc       0m           14Mikube-system    kube-addon-manager-minikube      7m           33Mi\\nThe outputs of both these commands are fair ly simple, so you probably don’t need me\\nto explain them, but I do need to wa rn you about one thing. Sometimes the top pod\\ncommand will refuse to show any metrics and instead print out an error like this:\\n$ kubectl top pod\\nW0312 22:12:58.021885   15126 top_pod.go:186] Metrics not available for pod \\ndefault/kubia-3773182134-63bmb, age: 1h24m19.021873823s\\nerror: Metrics not available f or pod default/kubia-3773182134-63bmb, age: \\n1h24m19.021873823s\\nIf this happens, don’t start looking for the cause of the error yet. Relax, wait a while,\\nand rerun the command—it may take a few minutes, but the metrics should appear\\neventually. The kubectl  top command gets the metrics from Heapster, which aggre-\\ngates the data over a few minutes an d doesn’t expose it immediately. \\nTIP To see resource usages across indivi dual containers instead of pods, you\\ncan use the --containers  option. \\n14.6.2 Storing and analyzing historical resource consumption statistics\\nThe top command only shows current resource usages—it doesn’t show you how\\nmuch CPU or memory your pods consumed throughout the last hour, yesterday, or aweek ago, for example. In fact, both cAdvis or and Heapster only hold resource usage\\ndata for a short window of time. If you wa nt to analyze your pods’ resource consump-\\ntion over longer time periods, yo u’ll need to run additional tools.\\n When using Google Kubernetes Engine, you can monitor your cluster with Google\\nCloud Monitoring, but when you’re running your own local Kubernetes cluster\\n(either through Minikube or other means), people usually use InfluxDB for storing\\nstatistics data and Grafana for visualizing and analyzing them. \\nINTRODUCING  INFLUX DB AND GRAFANA\\nInfluxDB is an open source time-series da tabase ideal for storing application metrics\\nand other monitoring data. Grafana, also open source, is an analytics and visualization\\nsuite with a nice-looking web console that allows you to visualize the data stored in\\nInfluxDB and discover how your applicatio n’s resource usage behaves over time (an\\nexample showing three Grafana char ts is shown in figure 14.9).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 464}, page_content='433 Monitoring pod resource usage\\nRUNNING  INFLUX DB AND GRAFANA  IN YOUR CLUSTER\\nBoth InfluxDB and Grafana can run as pods . Deploying them is straightforward. All\\nthe necessary manifests are available in the Heapster Git repository at http:/ /github\\n.com/kubernetes/heapster/tree/ma ster/deploy/kube-config/influxdb .\\n When using Minikube, you don’t even ne ed to deploy them manually, because\\nthey’re deployed along with Heapster when you enable the Heapster add-on.\\nANALYZING  RESOURCE  USAGE  WITH GRAFANA\\nTo discover how much of each resource your pod requires over time, open the\\nGrafana web console and explore the predefined dashboards. Generally, you can findout the URL of Grafana’s web console with \\nkubectl  cluster-info :\\n$ kubectl cluster-info\\n...\\nmonitoring-grafana  is running at \\nhttps://192.168.99.100:8443/api/v1/proxy/namespaces/kube-\\nsystem/services/monitoring-grafana\\nFigure 14.9 Grafana dashboard showin g CPU usage across the cluster'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 465}, page_content='434 CHAPTER  14 Managing pods’ computational resources\\nWhen using Minikube, Grafana’s web console is exposed through a NodePort  Service,\\nso you can open it in your br owser with the following command:\\n$ minikube service monitoring-grafana -n kube-system\\nOpening kubernetes service kub e-system/monitoring-grafana in default \\nbrowser...\\nA new browser window or tab will open an d show the Grafana Home screen. On the\\nright-hand side, you’ll see a list of  dashboards containing two entries:\\n\\uf0a1Cluster\\n\\uf0a1Pods\\nTo see the resource usage statistics of th e nodes, open the Cluster dashboard. There\\nyou’ll see several charts showing the overall cluster usage, usage by node, and the\\nindividual usage for CPU, memory, network,  and filesystem. The charts will not only\\nshow the actual usage, but also the requests and limits for those resources (where\\nthey apply).\\n If you then switch over to the Pods  dashboard, you can examine the resource\\nusages for each individual pod, again with both requests and limits shown alongside\\nthe actual usage. \\n Initially, the charts show the statistics for the last 30 minutes, but you can zoom out\\nand see the data for much longer time periods: days, months, or even years.\\nUSING THE INFORMATION  SHOWN  IN THE CHARTS\\nBy looking at the charts, you can quickly see if the resource requests or limits you’ve\\nset for your pods need to be raised or wh ether they can be lowered to allow more pods\\nto fit on your nodes. Let’s look at an example. Figure 14.10 shows the CPU and mem-\\nory charts for a pod.\\n At the far right of the top chart, you ca n see the pod is using more CPU than was\\nrequested in the pod’s manifest. Although th is isn’t problematic when this is the only\\npod running on the node, you should keep in mind that a pod is only guaranteed as\\nmuch of a resource as it requests throug h resource requests. Your pod may be running\\nfine now, but when other pods are deployed to the same node and start using theCPU, your pod’s CPU time may be throttled. Because of this, to ensure the pod can\\nuse as much CPU as it needs to at any time , you should raise the CPU resource request\\nfor the pod’s container.\\n The bottom chart shows the pod’s memory usage and request. Here the situation is\\nthe exact opposite. The amount of memory the pod is using is well below what was\\nrequested in the pod’s spec. The requested memory is reserved for the pod and won’tbe available to other pods. The unused memory is therefore wasted. You should\\ndecrease the pod’s memory request to make  the memory available to other pods run-\\nning on the node.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 466}, page_content='435 Summary\\n14.7 Summary\\nThis chapter has shown you that you need to  consider your pod’s resource usage and\\nconfigure both the resource requests and th e limits for your pod to keep everything\\nrunning smoothly. The key take aways from this chapter are\\n\\uf0a1Specifying resource requests helps Kubern etes schedule pods across the cluster.\\n\\uf0a1Specifying resource limits keeps pods from starving other pods of resources.\\n\\uf0a1Unused CPU time is allocated ba sed on containers’ CPU requests.\\n\\uf0a1Containers never get killed if they try to  use too much CPU, but they are killed\\nif they try to use too much memory.\\n\\uf0a1In an overcommitted system, containers al so get killed to free memory for more\\nimportant pods, based on the pods’ QoS classes and actual memory usage.\\nActual CPU usage is higher\\nthan what was requested.The application’s CPU timewill be throttled when otherapps demand more CPU.You should increase theCPU request.\\nActual memory usage is well\\nbelow requested memory.You’ve reserved too muchmemory for this app. You’rewasting memory, because itwon’t ever be used by thisapp and also can’t be usedby other apps. You shoulddecrease the memoryrequest.CPU requestCPU usage\\nMemory request\\nMemory usage\\nFigure 14.10 CPU and memory usage chart for a pod'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 467}, page_content='436 CHAPTER  14 Managing pods’ computational resources\\n\\uf0a1You can use LimitRange objects to defi ne the minimum, maximum, and default\\nresource requests and limits for individual pods.\\n\\uf0a1You can use ResourceQuota objects to li mit the amount of resources available\\nto all the pods in a namespace.\\n\\uf0a1To know how high to set a pod’s resource  requests and limits, you need to mon-\\nitor how the pod uses resources over a long-enough time period.\\nIn the next chapter, you’ll see how these me trics can be used by Kubernetes to auto-\\nmatically scale your pods.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 468}, page_content='437Automatic scaling\\nof pods and cluster nodes\\nApplications running in pods can be sc aled out manually by increasing the\\nreplicas  field in the ReplicationController,  ReplicaSet, Deployment, or other\\nscalable resource. Pods can also be scaled vertically by increasing their container’sresource requests and limits (though this  can currently only be done at pod cre-\\nation time, not while the pod is running).  Although manual scaling is okay for\\ntimes when you can anticipate load spikes in advance or when the load changesgradually over longer periods of time, requiring manual intervention to handle\\nsudden, unpredictable traffic increases isn’t ideal. This chapter covers\\n\\uf0a1Configuring automatic horizontal scaling of pods \\nbased on CPU utilization\\n\\uf0a1Configuring automatic horizontal scaling of pods based on custom metrics\\n\\uf0a1Understanding why vertical scaling of pods isn’t possible yet\\n\\uf0a1Understanding automatic horizontal scaling of cluster nodes'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 469}, page_content='438 CHAPTER  15 Automatic scaling of pods and cluster nodes\\n Luckily, Kubernetes can monitor your po ds and scale them up automatically as\\nsoon as it detects an increase in the CPU usage or some other metric. If running on a\\ncloud infrastructure, it can even spin up ad ditional nodes if the existing ones can’t\\naccept any more pods. This chapter will expl ain how to get Kubernetes to do both pod\\nand node autoscaling.\\n The autoscaling feature in Kubernetes was completely rewritten between the 1.6\\nand the 1.7 version, so be aware you may fi nd outdated information on this subject\\nonline.\\n15.1 Horizontal pod autoscaling\\nHorizontal pod autoscaling is the automatic scaling of the number of pod replicas man-\\naged by a controller. It’s performed by the Horizontal controller, which is enabled and\\nconfigured by creating a HorizontalPodAutoscaler (HPA) resource. The controllerperiodically checks pod metrics, calculates  the number of replicas required to meet\\nthe target metric value configured in the HorizontalPodAutoscaler resource, and\\nadjusts the \\nreplicas  field on the target resource (D eployment, ReplicaSet, Replication-\\nController, or StatefulSet). \\n15.1.1 Understanding the autoscaling process\\nThe autoscaling process can be split into three steps:\\n\\uf0a1Obtain metrics of all the pods mana ged by the scaled resource object.\\n\\uf0a1Calculate the number of pods required to  bring the metrics to  (or close to) the\\nspecified target value.\\n\\uf0a1Update the replicas  field of the scaled resource.\\nLet’s examine all three steps next.\\nOBTAINING  POD METRICS\\nThe Autoscaler doesn’t perform the gatherin g of the pod metrics itself. It gets the\\nmetrics from a different source. As we saw in the previous chapter, pod and node met-\\nrics are collected by an agent called cAdvisor , which runs in the Kubelet on each node,\\nand then aggregated by the cluster-wide component called Heapster. The horizontal\\npod autoscaler controller gets the metric s of all the pods by querying Heapster\\nthrough REST calls. The flow of metrics data  is shown in figure 15.1 (although all the\\nconnections are initiated in the opposite direction).\\nThis implies that Heapster must be runnin g in the cluster for autoscaling to work. If\\nyou’re using Minikube and were following along in the previous chapter, HeapsterPod(s) cAdvisor(s) Horizontal Pod Autoscaler(s) Heapster\\nFigure 15.1 Flow of metrics from the pod(s) to the HorizontalPodAutoscaler(s)'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 470}, page_content='439 Horizontal pod autoscaling\\nshould already be enabled in your cluster. If not, make sure to enable the Heapster\\nadd-on before trying out any autoscaling examples.\\n Although you don’t need to query Heapster directly, if you’re interested in doing\\nso, you’ll find both the Heapster Pod an d the Service it’s exposed through in the\\nkube-system  namespace. \\nCALCULATING  THE REQUIRED  NUMBER  OF PODS\\nOnce the Autoscaler has metrics for all the pods belonging to the resource the Auto-\\nscaler is scaling (the Deployment, ReplicaS et, ReplicationController, or StatefulSet\\nresource), it can use those metrics to figu re out the required number of replicas. It\\nneeds to find the number that will bring the average value of the metric across all\\nthose replicas as close to the configured targ et value as possible. The input to this cal-\\nculation is a set of pod metrics (possibly mu ltiple metrics per pod) and the output is a\\nsingle integer (the number of pod replicas). \\n When the Autoscaler is configured to cons ider only a single metric, calculating the\\nrequired replica count is simple. All it ta kes is summing up the metrics values of all\\nthe pods, dividing that by the target value set on the HorizontalPodAutoscaler\\nresource, and then rounding it up to the ne xt-larger integer. The actual calculation is\\na bit more involved than this, because it al so makes sure the Autoscaler doesn’t thrash\\naround when the metric value is  unstable and changes rapidly. \\n When autoscaling is based on multiple pod metrics (for example, both CPU usage\\nand Queries-Per-Second [QPS]), the calcul ation isn’t that much more complicated.\\nThe Autoscaler calculates the replica count for each metric individually and then\\ntakes the highest value (for example, if fo ur pods are required to achieve the target\\nCPU usage, and three pods are required to achieve the target QPS, the Autoscaler willscale to four pods). Figure 15.2 shows this example.A look at changes related to how the Autoscaler obtains metrics\\nPrior to Kubernetes version 1.6, the Hor izontalPodAutoscaler obtained the metrics\\nfrom Heapster directly. In version 1.8, t he Autoscaler can get the metrics through an\\naggregated version of the resource metrics API by starting the Controller Managerwith the \\n--horizontal-pod-a utoscaler-use-re st-clients=true  flag. From ver-\\nsion 1.9, this behavior will be enabled by default.\\nThe core API server will not expose the metrics itself. From version 1.7, Kubernetes\\nallows registering multiple API servers and making them appear as a single API\\nserver. This allows it to expose metrics through one of those underlying API servers.\\nWe’ll explain API server aggregation in the last chapter. \\nSelecting what metrics collector to use in th eir clusters will be up to cluster adminis-\\ntrators. A simple translation layer is usua lly required to expose the metrics in the\\nappropriate API paths and in the appropriate format.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 471}, page_content='440 CHAPTER  15 Automatic scaling of pods and cluster nodes\\nUPDATING  THE DESIRED  REPLICA  COUNT  ON THE SCALED  RESOURCE\\nThe final step of an autoscaling operation is updating the desired replica count field\\non the scaled resource object (a ReplicaSet , for example) and then letting the Replica-\\nSet controller take care of spinning up additional pods or deleting excess ones.\\n The Autoscaler controller modifies the replicas  field of the scaled resource\\nthrough the Scale sub-resource. It enables th e Autoscaler to do its work without know-\\ning any details of the resource it’s scaling,  except for what’s exposed through the Scale\\nsub-resource (see figure 15.3).\\nThis allows the Autoscaler to operate on any scalable resource, as long as the API\\nserver exposes the Scale sub-resource for it. Currently, it’s exposed for\\n\\uf0a1Deployments\\n\\uf0a1ReplicaSets\\n\\uf0a1ReplicationControllers\\n\\uf0a1StatefulSets\\nThese are currently the only objects you can attach an Autoscaler to.Pod 1\\nCPU\\nutilization\\nQPSPod 2 Pod 3\\nTarget\\nCPU utilization\\nTarget QPSReplicas: 4\\nReplicas: 3Replicas: 430 12 15\\n20(15 + 30 + 12) / 20 = 57 / 20(60 + 90 + 50) / 50 = 200 / 50\\nMax(4, 3)50%60% 90% 50%\\nFigure 15.2 Calculating the number of replicas from two metrics\\nAutoscaler adjusts replicas (++ or --)\\nHorizontal Pod AutoscalerDeployment, ReplicaSet,\\nStatefulSet, or\\nReplicationController\\nScale\\nsub-resource\\nFigure 15.3 The Horizontal Pod Autoscaler  modifies only on the Scale sub-resource.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 472}, page_content='441 Horizontal pod autoscaling\\nUNDERSTANDING  THE WHOLE  AUTOSCALING  PROCESS\\nYou now understand the three steps involved in autoscaling, so let’s visualize all the\\ncomponents involved in the autoscaling process. They’re shown in figure 15.4.\\nThe arrows leading from the pods to the cAdvisors, which continue on to Heapster\\nand finally to the Horizontal Pod Autoscaler, indicate the direction of the flow of met-\\nrics data. It’s important to be aware that each component gets the metrics from theother components periodically (that is, cAdv isor gets the metrics from the pods in a\\ncontinuous loop; the same is also true fo r Heapster and for the HPA controller). The\\nend effect is that it takes quite a while for the metrics data to be propagated and a res-\\ncaling action to be performed. It isn’t imme diate. Keep this in mind when you observe\\nthe Autoscaler in action next.\\n15.1.2 Scaling based on CPU utilization\\nPerhaps the most important metric you’ll want to base autoscaling on is the amount of\\nCPU consumed by the processes running insi de your pods. Imagine having a few pods\\nproviding a service. When their CPU usage reaches 100% it’s obvious they can’t cope\\nwith the demand anymore and need to be scaled either up (vertical scaling—increas-\\ning the amount of CPU the pods can use) or out (horizontal scaling—increasing the\\nnumber of pods). Because we’re talking ab out the horizontal pod autoscaler here,Autoscaler adjusts\\nreplicas (++ or --)\\nHeapster collects\\nmetrics from all nodes\\ncAdvisor collects metrics\\nfrom all containers on a nodeDeployment ReplicaSet\\nAutoscaler collects\\nmetrics from HeapsterKubeletcAdvisorNode 1\\nPod\\nPod\\nKubeletcAdvisorNode 2\\nPodNode X\\nHeapsterHorizontal Pod\\nAutoscaler\\nFigure 15.4 How the autoscaler obtains me trics and rescales the target deployment'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 473}, page_content='442 CHAPTER  15 Automatic scaling of pods and cluster nodes\\nwe’re only focusing on scaling out (increas ing the number of pods). By doing that,\\nthe average CPU usage should come down. \\n Because CPU usage is usually unstable, it makes sense to scale out even before the\\nCPU is completely swamped—perhaps when  the average CPU load across the pods\\nreaches or exceeds 80%. But 80% of what, exactly?\\nTIP Always set the target CPU usage well  below 100% (and definitely never\\nabove 90%) to leave enough room for handling sudden load spikes.\\nAs you may remember from the previous ch apter, the process ru nning inside a con-\\ntainer is guaranteed the amount of CP U requested through the resource requests\\nspecified for the container. But at times wh en no other processes need CPU, the pro-\\ncess may use all the available CPU on the node. When someone says a pod is consum-ing 80% of the CPU, it’s not clear if they  mean 80% of the node’s CPU, 80% of the\\npod’s guaranteed CPU (the resource reques t), or 80% of the hard limit configured\\nfor the pod through resource limits. \\n As far as the Autoscaler is concerned, only the pod’s guaranteed CPU amount (the\\nCPU requests) is important when determinin g the CPU utilization of a pod. The Auto-\\nscaler compares the pod’s actual CPU consumption and its CPU requests, which\\nmeans the pods you’re autoscaling need to have CPU requests set (either directly or\\nindirectly through a LimitRange object) fo r the Autoscaler to determine the CPU uti-\\nlization percentage.\\nCREATING  A HORIZONTAL PODAUTOSCALER  BASED  ON CPU USAGE\\nLet’s see how to create a HorizontalPodAut oscaler now and configure it to scale pods\\nbased on their CPU utilization. You’ll create  a Deployment similar to the one in chap-\\nter 9, but as we’ve discussed, you’ll need to make sure the pods created by the Deploy-\\nment all have the CPU resource requests specified in order to make autoscalingpossible. You’ll have to add a CPU resour ce request to the Deployment’s pod tem-\\nplate, as shown in the following listing.\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:  name: kubia\\nspec:\\n  replicas: 3                  template:\\n    metadata:\\n      name: kubia      labels:\\n        app: kubia\\n    spec:      containers:\\n      - image: luksa/kubia:v1     \\n        name: nodejsListing 15.1 Deployment with CPU requests set: deployment.yaml\\nManually setting the \\n(initial) desired number \\nof replicas to three\\nRunning the \\nkubia:v 1 image'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 474}, page_content='443 Horizontal pod autoscaling\\n        resources:              \\n          requests:             \\n            cpu: 100m           \\nThis is a regular Deployment object—it does n’t use autoscaling yet. It will run three\\ninstances of the kubia  NodeJS app, with each instance requesting 100 millicores\\nof CPU. \\n After creating the Deployment, to enable horizontal autoscaling of its pods, you\\nneed to create a HorizontalPodAutoscaler (HPA) object and poin t it to the Deploy-\\nment. You could prepare and post the YAML manifest for the HPA, but an easier way\\nexists—using the kubectl  autoscale  command:\\n$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5\\ndeployment \"kubia\" autoscaled\\nThis creates the HPA object for you and sets the Deployment called kubia  as the scal-\\ning target. You’re setting the target CPU ut ilization of the pods to 30% and specifying\\nthe minimum and maximum number of replic as. The Autoscaler will constantly keep\\nadjusting the number of replicas to keep th eir CPU utilization arou nd 30%, but it will\\nnever scale down to less than one or scale up to more than five replicas. \\nTIP A l w a y s  m a k e  s u r e  t o  a u t o s c a l e  D e ployments instead of the underlying\\nReplicaSets. This way, you ensure the desired replica count is preserved across\\napplication updates (remember that a Deployment creates a new ReplicaSetfor each version). The same rule ap plies to manual scaling, as well.\\nLet’s look at the definition of the Horizo ntalPodAutoscaler resource to gain a better\\nunderstanding of it. It’s shown in the following listing.\\n$ kubectl get hpa.v2beta1.autoscaling kubia -o yaml\\napiVersion: autoscaling/v2beta1            \\nkind: HorizontalPodAutoscaler              metadata:\\n  name: kubia               \\n  ...spec:\\n  maxReplicas: 5                   \\n  metrics:                                - resource:                           \\n      name: cpu                         \\n      targetAverageUtilization: 30          type: Resource                      \\n  minReplicas: 1                   \\n  scaleTargetRef:                              apiVersion: extensions/v1beta1         \\n    kind: Deployment                       \\n    name: kubia                            Listing 15.2 A HorizontalPodAutoscaler YAML definitionRequesting 100 millicores \\nof CPU per pod\\nHPA resources are in the \\nautoscaling API group.\\nEach HPA has a name (it doesn’t \\nneed to match the name of the Deployment as in this case).\\nThe\\nminimum\\nand\\nmaximum\\nnumber of\\nreplicas\\nyou\\nspecifiedYou’d like the Autoscaler to \\nadjust the number of pods \\nso they each utilize 30% of requested CPU.\\nThe target resource \\nwhich this Autoscaler will act upon'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 475}, page_content='444 CHAPTER  15 Automatic scaling of pods and cluster nodes\\nstatus:\\n  currentMetrics: []        \\n  currentReplicas: 3          desiredReplicas: 0        \\nNOTE Multiple versions of HPA resources exist: the new autoscaling/v2beta1\\nand the old autoscaling/v1 . You’re requesting the new version here.\\nSEEING THE FIRST AUTOMATIC  RESCALE  EVENT\\nIt takes a while for cAdvisor to get the CPU metrics and for Heapster to collect them\\nbefore the Autoscaler can take action. During  that time, if you display the HPA resource\\nwith kubectl  get, the TARGETS  column will show <unknown> :\\n$ kubectl get hpa\\nNAME      REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS\\nkubia     Deployment/kubia   <unknown> / 30%   1         5         0       \\nBecause you’re running three pods that are currently receiving no requests, which\\nmeans their CPU usage should be close to zero, you should expect the Autoscaler to\\nscale them down to a single pod, because ev en with a single pod, the CPU utilization\\nwill still be below the 30% target. \\n And sure enough, the autoscaler does ex actly that. It soon scales the Deployment\\ndown to a single replica:\\n$ kubectl get deployment\\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\nkubia     1         1         1            1           23m\\nRemember, the autoscaler only adjusts the desired replica count on the Deployment.\\nThe Deployment controller then takes care  of updating the desired replica count on\\nthe ReplicaSet object, which then causes the ReplicaSet controller to delete two excess\\npods, leaving one pod running.\\n You can use kubectl  describe  to see more information on the HorizontalPod-\\nAutoscaler and the operation of the underlying  controller, as the following listing shows.\\n$ kubectl describe hpa\\nName:                             kubia\\nNamespace:                        defaultLabels:                           <none>\\nAnnotations:                      <none>\\nCreationTimestamp:                Sat, 03 Jun 2017 12:59:57 +0200Reference:                        Deployment/kubia\\nMetrics:                          ( current / target )\\n  resource cpu on pods    (as a percentage of request):   0% (0) / 30%\\nMin replicas:                     1\\nMax replicas:                     5Listing 15.3 Inspecting a HorizontalPodAutoscaler with kubectl  describeThe current status \\nof the Autoscaler'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 476}, page_content='445 Horizontal pod autoscaling\\nEvents:\\nFrom                        Reason              Message\\n----                        ------              ---horizontal-pod-autoscaler   SuccessfulRescale   New size: 1; reason: All \\n                                                metrics below target\\nNOTE The output has been modified to make it more readable.\\nTurn your focus to the table of events at the bottom of the listing. You see the horizon-\\ntal pod autoscaler has successfully rescaled  to one replica, because all metrics were\\nbelow target. \\nTRIGGERING  A SCALE -UP\\nYou’ve already witnessed your first automati c rescale event (a scale-down). Now, you’ll\\nstart sending requests to your pod, thereby increasing its CPU usage, and you should\\nsee the autoscaler detect this and start up additional pods.\\n You’ll need to expose the pods through a Se rvice, so you can hit all of them through\\na single URL. You may remember that the easiest way to do that is with kubectl  expose :\\n$ kubectl expose deployment kubia --port=80 --target-port=8080\\nservice \"kubia\" exposed\\nBefore you start hitting your pod(s) with requests, you may want to run the follow-\\ning command in a separate terminal to keep an eye on what’s happening with theHorizontalPodAutoscaler and the Deployme nt, as shown in the following listing.\\n$ watch -n 1 kubectl get hpa,deployment\\nEvery 1.0s: kubectl get hpa,deployment                                                                                                                                                                  \\nNAME        REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS  AGE\\nhpa/kubia   Deployment/kubia   0% / 30%   1         5         1         45m\\nNAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\ndeploy/kubia   1         1         1            1           56m\\nTIP List multiple resource types with kubectl  get by delimiting them with\\na comma. \\nIf you’re using OSX, you’ll have to replace the watch  command with a loop, manually\\nrun kubectl  get periodically, or use kubectl ’s --watch  option. But although a plain\\nkubectl  get can show multiple types of resource s at once, that’s not the case when\\nusing the aforementioned --watch  option, so you’ll need to use two terminals if you\\nwant to watch both the HPA and the Deployment objects. \\n Keep an eye on the state of those two objects while you run a load-generating pod.\\nYou’ll run the following command in another terminal:\\n$ kubectl run -it --rm --resta rt=Never loadgenerator --image=busybox \\n➥ -- sh -c \"while true; do wget -O - -q http://kubia.default; done\"Listing 15.4 Watching multiple resources in parallel'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 477}, page_content='446 CHAPTER  15 Automatic scaling of pods and cluster nodes\\nThis will run a pod which repeatedly hits the kubia  Service. You’ve seen the -it\\noption a few times when running the kubectl  exec  command. As you can see, it can\\nalso be used with kubectl  run. It allows you to attach the console to the process,\\nwhich will not only show you the process’ output directly, but will also terminate theprocess as soon as you press CTRL+C. The \\n--rm  option causes the pod to be deleted\\nafterward, and the --restart=Never  option causes kubectl  run to create an unman-\\naged pod directly instead of through a Deployment object, which you don’t need.\\nThis combination of options is useful fo r running commands inside the cluster with-\\nout having to piggyback on an existing pod.  It not only behaves the same as if you\\nwere running the command locally, it even  cleans up everything when the command\\nterminates. \\nSEEING THE AUTOSCALER  SCALE  UP THE DEPLOYMENT\\nAs the load-generator pod runs, you’ll see it initially hitting the single pod. As before,\\nit takes time for the metrics to be updated, but when they are, you’ll see the autoscaler\\nincrease the number of replicas. In my case, the pod’s CPU utilization initially jumped\\nto 108%, which caused the autoscaler to in crease the number of pods to four. The\\nutilization on the individual pods then decreased to 74% and then stabilized at\\naround 26%. \\nNOTE If the CPU load in your case doesn’t exceed 30%, try running addi-\\ntional load-generators.\\nAgain, you can inspect autoscaler events with kubectl  describe  to see what the\\nautoscaler has done (only the most import ant information is shown in the following\\nlisting).\\nFrom    Reason              Message\\n----    ------              -------\\nh-p-a   SuccessfulRescale   New size: 1; reason: All metrics below target\\nh-p-a   SuccessfulRescale   New size: 4; reason: cpu resource utilization \\n                            (percentage of request) above target\\nDoes it strike you as odd that the initial average CPU utilization in my case, when I\\nonly had one pod, was 108%, which is mo re than 100%? Remember, a container’s\\nCPU utilization is the container’s actual CP U usage divided by its requested CPU. The\\nrequested CPU defines the minimum, not ma ximum amount of CPU available to the\\ncontainer, so a container may consume more than the requested CPU, bringing the\\npercentage over 100. \\n Before we go on, let’s do a little math  and see how the autoscaler concluded that\\nfour replicas are needed. Initially, there was one replica handling requests and itsCPU usage spiked to 108%. Dividing 108 by 30 (the target CPU utilization percent-\\nage) gives 3.6, which the autoscaler then ro unded up to 4. If you divide 108 by 4, youListing 15.5 Events of a HorizontalPodAutoscaler'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 478}, page_content='447 Horizontal pod autoscaling\\nget 27%. If the autoscaler scales up to four pods, their average CPU utilization is\\nexpected to be somewhere in the neighbor hood of 27%, which is close to the target\\nvalue of 30% and almost exactly what  the observed CPU utilization was.\\nUNDERSTANDING  THE MAXIMUM  RATE OF SCALING\\nIn my case, the CPU usage shot up to 108 %, but in general, the initial CPU usage\\ncould spike even higher. Even if the initial average CPU utilization was higher (say\\n150%), requiring five replicas to achieve the 30% target, the autoscaler would still\\nonly scale up to four pods in the first step , because it has a limit on how many repli-\\ncas can be added in a single scale-up oper ation. The autoscaler  will at most double\\nthe number of replicas in a single oper ation, if more than two current replicas\\nexist. If only one or two exist, it will scale up to a maximum of four replicas in a sin-\\ngle step. \\n Additionally, it has a limit on how soon a subsequent autoscale operation can\\noccur after the previous one. Currently, a scale-up will occur only if no rescaling\\nevent occurred in the last three minutes. A scale-down event is performed even less\\nfrequently—every five minutes. Keep this  in mind so you don’t wonder why the\\nautoscaler refuses to perform a rescale op eration even if the metrics clearly show\\nthat it should.\\nMODIFYING  THE TARGET  METRIC  VALUE  ON AN EXISTING  HPA OBJECT\\nTo wrap up this section, let’s do one last exercise. Maybe your initial CPU utilization\\ntarget of 30% was a bit too low, so increase it to 60%. You do this by editing the HPAresource with the \\nkubectl  edit  command. When the text editor opens, change the\\ntargetAverageUtilization  field to 60, as shown in the following listing.\\n...\\nspec:\\n  maxReplicas: 5\\n  metrics:  - resource:\\n      name: cpu\\n      targetAverageUtilization: 60        type: Resource\\n...\\nAs with most other resources, after you modify the resource, your changes will be\\ndetected by the autoscaler controller and acted upon. You could also delete the\\nresource and recreate it with different ta rget values, because by deleting the HPA\\nresource, you only disable autoscaling of th e target resource (a Deployment in this\\ncase) and leave it at the scale it is at that  time. The automatic scaling will resume after\\nyou create a new HPA reso urce for the Deployment.Listing 15.6 Increasing the target CPU utilization by editing the HPA resource\\nChange this \\nfrom 30 to 60.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 479}, page_content='448 CHAPTER  15 Automatic scaling of pods and cluster nodes\\n15.1.3 Scaling based on memory consumption\\nYou’ve seen how easily the horizontal Auto scaler can be configured to keep CPU uti-\\nlization at the target level. But what about autoscaling based on the pods’ memory\\nusage? \\n Memory-based autoscaling is much more  problematic than CPU-based autoscal-\\ning. The main reason is because after scaling up, the old pods would somehow need to\\nbe forced to release memory. This needs to be done by the app itself—it can’t be doneby the system. All the system could do is kill and restart the app, hoping it would use\\nless memory than before. But if the app th en uses the same amount as before, the\\nAutoscaler would scale it up again. And agai n, and again, until it reaches the maxi-\\nmum number of pods co nfigured on the HPA resource. Ob viously, this isn’t what any-\\none wants. Memory-based autoscaling was in troduced in Kubernetes version 1.8, and\\nis configured exactly like CPU-based autoscal ing. Exploring it is left up to the reader.\\n15.1.4 Scaling based on other and custom metrics\\nYou’ve seen how easy it is to scale pods base d on their CPU usage. Initially, this was the\\nonly autoscaling option that was usable in practice. To have the autoscaler use custom,\\napp-defined metrics to drive its autoscaling decisions was fairly complicated. The ini-\\ntial design of the autoscaler didn’t make it easy to move beyond simple CPU-based\\nscaling. This prompted the Kubernetes Au toscaling Special Interest Group (SIG) to\\nredesign the autoscaler completely. \\n If you’re interested in learning how comp licated it was to use the initial autoscaler\\nwith custom metrics, I invite you to read my blog post entitled “Kubernetes autoscal-\\ning based on custom metrics without using a host port,” which you’ll find online at\\nhttp:/ /medium.com/@marko.luksa . You’ll learn about all the other problems I\\nencountered when trying to set up autosc aling based on custom metrics. Luckily,\\nnewer versions of Kubernetes don’t have th ose problems. I’ll cover the subject in a\\nnew blog post. \\n Instead of going through a complete ex ample here, let’s quickly go over how to\\nconfigure the autoscaler to use different me trics sources. We’ll start by examining how\\nwe defined what metric to use in our previous example. The following listing shows\\nhow your previous HPA object was conf igured to use the CPU usage metric.\\n...\\nspec:\\n  maxReplicas: 5  metrics:\\n  - type: Resource      \\n    resource:      name: cpu                      \\n      targetAverageUtilization: 30    \\n...Listing 15.7 HorizontalPodAutoscaler definition for CPU-based autoscaling\\nDefines the type \\nof metric\\nThe resource, whose \\nutilization will be monitored\\nThe target utilization \\nof this resource'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 480}, page_content='449 Horizontal pod autoscaling\\nAs you can see, the metrics  field allows you to define more than one metric to use.\\nIn the listing, you’re using a single metric. Each entry defines the type  of metric—\\nin this case, a Resource  metric. You have three types of metrics you can use in an\\nHPA object:\\n\\uf0a1Resource\\n\\uf0a1Pods\\n\\uf0a1Object\\nUNDERSTANDING  THE RESOURCE  METRIC  TYPE\\nThe Resource  type makes the autoscaler base its autoscaling decisions on a resource\\nmetric, like the ones specified in a contai ner’s resource requests. We’ve already seen\\nhow to do that, so let’s focus on the other two types.\\nUNDERSTANDING  THE PODS METRIC  TYPE\\nThe Pods  type is used to refer to any other (i ncluding custom) metric related to the\\npod directly. An example of such a metric could be the already mentioned Queries-Per-Second (QPS) or the number of message s in a message broker’s queue (when the\\nmessage broker is running as a pod). To co nfigure the autoscaler to use the pod’s QPS\\nmetric, the HPA object would need to include the entry shown in the following listingunder its \\nmetrics  field.\\n...\\nspec:\\n  metrics:  - type: Pods              \\n    resource:\\n      metricName: qps             \\n      targetAverageValue: 100    \\n...\\nThe example in the listing configures the au toscaler to keep the average QPS of all\\nthe pods managed by the ReplicaSet (or other) controller targeted by this HPA\\nresource at 100. \\nUNDERSTANDING  THE OBJECT  METRIC  TYPE\\nThe Object  metric type is used when you want  to make the autoscaler scale pods\\nbased on a metric that doesn’t pertain dire ctly to those pods. For example, you may\\nwant to scale pods according to a metric of  another cluster object, such as an Ingress\\nobject. The metric could be QPS as in li sting 15.8, the average request latency, or\\nsomething else completely. \\n Unlike in the previous case, where the auto scaler needed to obtain the metric for\\nall targeted pods and then use the average of those values, when you use an Object\\nmetric type, the autoscaler obtains a single metric from the single object. In the HPAListing 15.8 Referring to a custom pod metric in the HPA\\nDefines a pod metric\\nThe name of \\nthe metric\\nThe target average value \\nacross all targeted pods'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 481}, page_content='450 CHAPTER  15 Automatic scaling of pods and cluster nodes\\ndefinition, you need to specify the target object and the target value. The following\\nlisting shows an example.\\n...\\nspec:\\n  metrics:\\n  - type: Object                   \\n    resource:\\n      metricName: latencyMillis                 target: \\n        apiVersion: extensions/v1beta1     \\n        kind: Ingress                              name: frontend                     \\n      targetValue: 20                   \\n  scaleTargetRef:                              apiVersion: extensions/v1beta1         \\n    kind: Deployment                       \\n    name: kubia                            ...\\nIn this example, the HPA is configured to use the latencyMillis  metric of the\\nfrontend  Ingress object. The target value for the metric is 20. The horizontal pod\\nautoscaler will monitor the Ingress’ metric an d if it rises too far above the target value,\\nthe autoscaler will scale the kubia  Deployment resource. \\n15.1.5 Determining which metrics are appropriate for autoscaling\\nYou need to understand that not all metr ics are appropriate for use as the basis of\\nautoscaling. As mentioned previously, the pods’ containers’ memory consumption isn’t\\na good metric for autoscaling. The autoscal er won’t function properly if increasing\\nthe number of replicas doesn’t result in a linear decrease of the average value of the\\nobserved metric (or at least close to linear). \\n For example, if you have only a single po d instance and the value of the metric is X\\nand the autoscaler scales up to two replicas, the metric needs to fall to somewhere\\nclose to X/2. An example of such a cust om metric is Queries per Second (QPS),\\nwhich in the case of web applications repo rts the number of requests the application\\nis receiving per second. Increasing the number  of replicas will always result in a pro-\\nportionate decrease of QPS, because a grea ter number of pods will be handling the\\nsame total number of requests. \\n Before you decide to base the autoscaler  on your app’s own custom metric, be sure\\nto think about how its value will behave when the number of pods increases or\\ndecreases.\\n15.1.6 Scaling down to zero replicas\\nThe horizontal pod autoscaler currently doesn’t allow setting the minReplicas  field\\nto 0, so the autoscaler will never scale do wn to zero, even if the pods aren’t doingListing 15.9 Referring to a metric of a different object in the HPA\\nUse metric of a \\nspecific object\\nThe name of \\nthe metric\\nThe specific object whose metric \\nthe autoscaler should obtainThe\\nAutoscaler\\nshould\\nscale so\\nthe value\\nof the\\nmetric\\nstays close\\nto this.\\nThe scalable resource the \\nautoscaler will scale'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 482}, page_content='451 Vertical pod autoscaling\\nanything. Allowing the number of pods to be scaled down to zero can dramatically\\nincrease the utilization of your hardware. Wh en you run services that get requests only\\nonce every few hours or even days, it doesn’t make sense to have them running all the\\ntime, eating up resources that could be used  by other pods. But you still want to have\\nthose services available immediatel y when a client request comes in. \\n This is known as idling and un-idling. It  allows pods that provide a certain service\\nto be scaled down to zero. When a new re quest comes in, the request is blocked until\\nthe pod is brought up and then the requ est is finally forwarded to the pod. \\n Kubernetes currently doesn’t provide this feature yet, but it will eventually. Check\\nthe documentation to see if idli ng has been implemented yet. \\n15.2 Vertical pod autoscaling\\nHorizontal scaling is great, but not every application can be scaled horizontally. For\\nsuch applications, the only option is to scale them vertically—give them more CPU\\nand/or memory. Because a node usually ha s more resources than a single pod\\nrequests, it should almost always be po ssible to scale a pod vertically, right? \\n Because a pod’s resource requests are configured through fields in the pod\\nmanifest, vertically scaling a pod would be performed by changing those fields. I\\nsay “would” because it’s currently not possi ble to change either resource requests\\nor limits of existing pods. Before I starte d writing the book (well over a year ago), I\\nwas sure that by the time I wrote this chapter, Kubernetes would already support\\nproper vertical pod autoscaling, so I include d it in my proposal for the table of con-\\ntents. Sadly, what seems lik e a lifetime later, vertical pod autoscaling is still not\\navailable yet. \\n15.2.1 Automatically configuring resource requests\\nAn experimental feature sets the CPU and memory requests on newly created pods, if\\ntheir containers don’t have them set explicit ly. The feature is provided by an Admission\\nControl plugin called InitialR esources. When a new pod wi thout resource requests is\\ncreated, the plugin looks at historical reso urce usage data of the pod’s containers (per\\nthe underlying container image and tag) and sets the requests accordingly. \\n You can deploy pods without specifying resource requests and rely on Kubernetes\\nto eventually figure out what each containe r’s resource needs are. Effectively, Kuber-\\nnetes is vertically scaling the pod. For ex ample, if a container keeps running out of\\nmemory, the next time a pod with that contai ner image is created, its resource request\\nfor memory will be set higher automatically.\\n15.2.2 Modifying resource requests while a pod is running\\nEventually, the same mechanism will be us ed to modify an existing pod’s resource\\nrequests, which means it will vertically scale the pod while it’s running. As I’m writing\\nthis, a new vertical pod autoscaling proposal  is being finalized. Please refer to the'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 483}, page_content='452 CHAPTER  15 Automatic scaling of pods and cluster nodes\\nKubernetes documentation to find out whet her vertical pod autoscaling is already\\nimplemented or not.\\n15.3 Horizontal scaling of cluster nodes\\nThe Horizontal Pod Autoscaler creates additional pod instances when the need for\\nthem arises. But what about when all your  nodes are at capacity and can’t run any\\nmore pods? Obviously, this problem isn’t li mited only to when new pod instances are\\ncreated by the Autoscaler. Even when creating pods manually, you may encounter the\\nproblem where none of the nodes can ac cept the new pods, because the node’s\\nresources are used up by existing pods. \\n In that case, you’d need to delete several of those existing pods, scale them down\\nvertically, or add additional nodes to your cluster. If your Kubernetes cluster is run-\\nning on premises, you’d need to physically add a new machine and make it part of the\\nKubernetes cluster. But if your cluster is running on a cloud infrastructure, adding\\nadditional nodes is usually a matter of a fe w clicks or an API call to the cloud infra-\\nstructure. This can be done automatically, right?\\n Kubernetes includes the feature to auto matically request additional nodes from\\nt h e  c l o u d  p r o v i d e r  a s  s o o n  a s  i t  d e t e c t s  a d d i t i o n a l  n o d e s  a r e  n e e d e d .  T h i s  i s  p e r -\\nformed by the Cluster Autoscaler.\\n15.3.1 Introducing the Cluster Autoscaler\\nThe Cluster Autoscaler takes care of automatically provisioning additional nodeswhen it notices a pod that can’t be schedul ed to existing nodes because of a lack of\\nresources on those nodes. It also de-provi sions nodes when they’re underutilized for\\nlonger periods of time. \\nREQUESTING  ADDITIONAL  NODES  FROM THE CLOUD  INFRASTRUCTURE\\nA new node will be provisioned if, after a new pod is created, the Scheduler can’t\\nschedule it to any of the existing nodes.  The Cluster Autoscaler looks out for such\\npods and asks the cloud provider to start up an additional node. But before doing\\nthat, it checks whether the new node can even accommodate the pod. After all, if\\nthat’s not the case, it makes no  sense to start up such a node.\\n Cloud providers usually gro up nodes into groups (or pools) of same-sized nodes\\n(or nodes having the same features). The Cluster Autoscaler thus can’t simply say\\n“Give me an additional node.” It ne eds to also specify the node type.\\n The Cluster Autoscaler does this by exam ining the available node groups to see if\\nat least one node type would be able to fit the unscheduled pod. If exactly one such\\nnode group exists, the Autoscaler can increase the size of the node group to have the\\ncloud provider add another node to the gro up. If more than one option is available,\\nthe Autoscaler must pick the best one. Th e exact meaning of “best” will obviously\\nneed to be configurable. In the worst case, it selects a random one. A simple overview\\nof how the cluster Autoscaler reacts to an  unschedulable pod is shown in figure 15.5.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 484}, page_content='453 Horizontal scaling of cluster nodes\\nWhen the new node starts up, the Kubelet on  that node contacts the API server and\\nregisters the node by creating  a Node resource. From then on, the node is part of the\\nKubernetes cluster and pods can be scheduled to it.\\n Simple, right? What about scaling down?\\nRELINQUISHING  NODES\\nThe Cluster Autoscaler also needs to sc ale down the number of nodes when they\\naren’t being utilized enough. The Autoscaler  does this by monitoring the requested\\nCPU and memory on all the nodes. If the CPU and memory requests of all the pods\\nrunning on a given node are below 50%, the node is considered unnecessary. \\n That’s not the only determining factor in  deciding whether to bring a node down.\\nThe Autoscaler also checks to see if any system pods are running (only) on that node\\n(apart from those that are run on every no de, because they’re deployed by a Daemon-\\nSet, for example). If a system pod is runnin g on a node, the node won’t be relinquished.\\nThe same is also true if an unmanaged pod or  a pod with local storage is running on the\\nnode, because that would cause disruption to the service the pod is providing. In other\\nwords, a node will only be returned to the cloud provider if the Cluster Autoscalerknows the pods running on the node will be rescheduled to other nodes.\\n When a node is selected to be shut down, the node is first marked as unschedula-\\nble and then all the pods running on the node are evicted. Because all those pods\\nbelong to ReplicaSets or other controllers , their replacements are created and sched-\\nuled to the remaining nodes (that’s why th e node that’s being shut down is first\\nmarked as unschedulable).Node group X\\nNode X1\\n1. Autoscaler notices a\\nPod can’t be scheduledto existing nodes3. Autoscaler scales up the\\nnode group selected inprevious step2. Autoscaler determines which node\\ntype (if any) would be able to ﬁt thepod. If multiple types could ﬁt thepod, it selects one of them.\\nCluster\\nAutoscalerPodsNode X2\\nPodsNode group Y\\nNode Y1\\nPods\\nUnschedulable\\npod\\nFigure 15.5 The Cluster Autoscaler scales up when it finds a pod that can’t be scheduled to \\nexisting nodes.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 485}, page_content='454 CHAPTER  15 Automatic scaling of pods and cluster nodes\\n15.3.2 Enabling the Cluster Autoscaler\\nCluster autoscaling is currently available on\\n\\uf0a1Google Kubernetes Engine (GKE)\\n\\uf0a1Google Compute Engine (GCE)\\n\\uf0a1Amazon Web Services (AWS)\\n\\uf0a1Microsoft Azure\\nHow you start the Autoscaler depends on wh ere your Kubernetes cluster is running.\\nFor your kubia  cluster running on GKE, you can enable the Cluster Autoscaler like\\nthis:\\n$ gcloud container clusters update kubia --enable-autoscaling \\\\\\n  --min-nodes=3 --max-nodes=5\\nIf your cluster is running on GCE, you ne ed to set three environment variables before\\nrunning kube-up.sh : \\n\\uf0a1KUBE_ENABLE_CLUSTER _AUTOSCALER=true\\n\\uf0a1KUBE_AUTOSCALE R_MIN_NODES=3\\n\\uf0a1KUBE_AUTOSCALE R_MAX_NODES=5\\nRefer to the Cluster Auto scaler GitHub repo at https:/ /github.com/kubernetes/auto-\\nscaler/tree/master/cluster-autoscaler  for information on how to enable it on other\\nplatforms. \\nNOTE The Cluster Autoscaler publishes its status to the cluster-autoscaler-\\nstatus  ConfigMap in the kube-system  namespace.\\n15.3.3 Limiting service disruption during cluster scale-down\\nWhen a node fails unexpectedly, nothing you can do will prevent its pods from becom-\\ning unavailable. But when a node is shut down voluntarily, either by the Cluster Auto-\\nscaler or by a human operator, you can ma ke sure the operation doesn’t disrupt the\\nservice provided by the pods running on that node through an additional feature.Manually cordoning and draining nodes\\nA node can also be marked as unschedulable  and drained manually. Without going\\ninto specifics, this is done with the following kubectl  commands:\\n\\uf0a1kubectl  cordon  <node>  marks the node as unschedulable (but doesn’t do\\nanything with pods running on that node).\\n\\uf0a1kubectl  drain  <node>  marks the node as unschedulable and then evicts all\\nthe pods from the node.\\nIn both cases, no new pods are schedu led to the node until you uncordon it again\\nwith kubectl  uncordon  <node> .'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 486}, page_content='455 Horizontal scaling of cluster nodes\\n Certain services require that a minimum number of pods always keeps running;\\nthis is especially true for quorum-based cl ustered applications. For this reason, Kuber-\\nnetes provides a way of specifying the mi nimum number of pods that need to keep\\nrunning while performing these types of oper ations. This is done by creating a Pod-\\nDisruptionBudget resource.\\n Even though the name of the resource sounds complex, it’s one of the simplest\\nKubernetes resources available. It contai ns only a pod label selector and a number\\nspecifying the minimum number of pods th at must always be available or, starting\\nfrom Kubernetes version 1.7, the maximum number of pods that can be unavailable.\\nWe’ll look at what a PodDisruptionBudget (PDB) resource manifest looks like, but\\ninstead of creating it from a YA ML file, you’ll create it with kubectl  create  pod-\\ndisruptionbudget  and then obtain and examine the YAML later.\\n If you want to ensure three instances of your kubia  pod are always running (they\\nhave the label app=kubia ), create the PodDisruptionBudget resource like this:\\n$ kubectl create pdb kubia-pdb  --selector=app=kubia --min-available=3\\npoddisruptionbudget \"kubia-pdb\" created\\nSimple, right? Now, retrieve the PDB’s YAML. It’s shown in the next listing.\\n$ kubectl get pdb kubia-pdb -o yaml\\napiVersion: policy/v1beta1\\nkind: PodDisruptionBudget\\nmetadata:  name: kubia-pdb\\nspec:\\n  minAvailable: 3         \\n  selector:                \\n    matchLabels:                 app: kubia           \\nstatus:\\n  ...\\nYou can also use a percentage instea d of an absolute number in the minAvailable\\nfield. For example, you could state that 60% of all pods with the app=kubia  label need\\nto be running at all times.\\nNOTE Starting with Kubernetes 1.7, the PodDisruptionBudget resource also\\nsupports the maxUnavailable  field, which you can use instead of min-\\nAvailable  if you want to block evictions wh en more than that many pods are\\nunavailable. \\nWe don’t have much more to say about this resource. As long as it exists, both the\\nCluster Autoscaler and the kubectl  drain  command will adhere to it and will never\\nevict a pod with the app=kubia  label if that would brin g  t h e  n u m b e r  o f  s u c h  p o d s\\nbelow three. Listing 15.10 A PodDisruptionBudget definition\\nHow many pods should \\nalways be available\\nThe label selector that \\ndetermines which pods this budget applies to'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 487}, page_content='456 CHAPTER  15 Automatic scaling of pods and cluster nodes\\n For example, if there were four pods altogether and minAvailable  was set to three\\nas in the example, the pod eviction process would evict pods one by one, waiting for\\nthe evicted pod to be replaced with a ne w one by the ReplicaSet controller, before\\nevicting another pod. \\n15.4 Summary\\nThis chapter has shown you how Kubernetes  can scale not only your pods, but also\\nyour nodes. You’ve learned that\\n\\uf0a1Configuring the automatic horizontal scalin g  o f  p o d s  i s  a s  e a s y  a s  c r e a t i n g  a\\nHorizontalPodAutoscaler object and pointi ng it to a Deployment, ReplicaSet,\\nor ReplicationController and specifying the target CPU utilization for the pods.\\n\\uf0a1Besides having the Horizontal Pod Autosc aler perform scaling operations based\\non the pods’ CPU utilization, you can al so configure it to scale based on your\\nown application-provided custom metric s or metrics related to other objects\\ndeployed in the cluster.\\n\\uf0a1Vertical pod autoscaling isn’t possible yet.\\n\\uf0a1Even cluster nodes can be scaled automa tically if your Kubernetes cluster runs\\non a supported cloud provider.\\n\\uf0a1You can run one-off processes in a pod and have the pod stopped and deletedautomatically as soon you press CTRL+C by using \\nkubectl  run with the -it and\\n--rm  options.\\nIn the next chapter, you’ll explore advanced  scheduling features, such as how to keep\\ncertain pods away from certain nodes and how to schedule pods either close together\\nor apart.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 488}, page_content='457Advanced scheduling\\nKubernetes allows you to affect where pods are scheduled. Initially, this was only\\ndone by specifying a node selector in th e pod specification, but additional mech-\\nanisms were later added that expanded th is functionality. Th ey’re covered in this\\nchapter.\\n16.1 Using taints and tolerations to repel pods from \\ncertain nodes\\nThe first two features related to advanced  scheduling that we’ll explore here are\\nthe node taints and pods’ tolerations of those taints. They’re used for restrictingThis chapter covers\\n\\uf0a1Using node taints and pod tolerations to keep \\npods away from certain nodes\\n\\uf0a1Defining node affinity rules as an alternative to node selectors\\n\\uf0a1Co-locating pods using pod affinity \\n\\uf0a1Keeping pods away from each other using pod anti-affinity'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 489}, page_content='458 CHAPTER  16 Advanced scheduling\\nwhich pods can use a certain node. A pod can only be scheduled to a node if it toler-\\nates the node’s taints.\\n This is somewhat different from using node selectors and node affinity, which\\nyou’ll learn about later in this chapter. Node selectors and node affinity rules make\\nit possible to select which nodes a pod ca n or can’t be scheduled to by specifically\\nadding that information to the pod, wher eas taints allow rejecting deployment of\\npods to certain nodes by only adding tain ts to the node without having to modify\\nexisting pods. Pods that you want deployed on a tainted node need to opt in to use\\nthe node, whereas with node selectors, po ds explicitly specify which node(s) they\\nwant to be deployed to.\\n16.1.1 Introducing taints and tolerations\\nThe best path to learn about node taints is to see an existing taint. Appendix B shows\\nhow to set up a multi-node cluster with the kubeadm  tool. By default, the master node\\nin such a cluster is tainted, so only Co ntrol Plane pods can be deployed on it. \\nDISPLAYING  A NODE’S TAINTS\\nYou can see the node’s taints using kubectl  describe  node , as shown in the follow-\\ning listing.\\n$ kubectl describe node master.k8s\\nName:         master.k8sRole:\\nLabels:       beta.kubernetes.io/arch=amd64\\n              beta.kubernetes.io/os=linux              kubernetes.io/hostname=master.k8s\\n              node-role.kubernetes.io/master=\\nAnnotations:  node.alpha.kubernetes.io/ttl=0\\n              volumes.kubernetes.io/controller-managed-attach-detach=true\\nTaints:       node-role.kubernetes.io/master:NoSchedule      ...\\nThe master node has a single taint. Taints have a key, value , and an effect, and are repre-\\nsented as <key>=<value>:<effect> . The master node’s taint shown in the previous\\nlisting has the key node-role.kuber netes.io/master , a null  value (not shown in the\\ntaint), and the effect of NoSchedule . \\n This taint prevents pods from being sche duled to the master node, unless those pods\\ntolerate this taint. The pods that tolerate it are usually system pods (see figure 16.1).\\n \\n \\n \\n Listing 16.1 Describing the master node in a cluster created with kubeadm\\nThe master node \\nhas one taint.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 490}, page_content='459 Using taints and tolerations to repel pods from certain nodes\\nDISPLAYING  A POD’S TOLERATIONS\\nIn a cluster installed with kubeadm , the kube-proxy cluster component runs as a pod\\non every node, including the master node, because master components that run aspods may also need to access Kubernetes Services. To make sure the kube-proxy pod\\nalso runs on the master node, it includes th e appropriate toleration. In total, the pod\\nhas three tolerations, which are shown in the following listing.\\n$ kubectl describe po kube-proxy-80wqm -n kube-system\\n...Tolerations:    node-role.kubernetes.io/master=:NoSchedule\\n                node.alpha.kubernetes.io/notReady=:Exists:NoExecute\\n                node.alpha.kubernetes.io/unreachable=:Exists:NoExecute\\n...\\nAs you can see, the first toleration matches th e master node’s taint, allowing this kube-\\nproxy pod to be scheduled to the master node. \\nNOTE Disregard the equal sign, which is shown in the pod’s tolerations, but\\nnot in the node’s taints. Kubectl apparently displays taints and tolerations dif-\\nferently when the taint’s/toleration’s value is null .\\nUNDERSTANDING  TAINT EFFECTS\\nThe two other tolerations on the kube-proxy  pod define how long the pod is allowed\\nto run on nodes that aren’t ready or are un reachable (the time in seconds isn’t shown,Listing 16.2 A pod’s tolerationsSystem pod may be\\nscheduled to masternode because itstoleration matchesthe node’s taint.\\nSystem podMaster node\\nTaint:\\nnode-role.kubernetes.io\\n/master:NoSchedule\\nToleration:\\nnode-role.kubernetes.io\\n/master:NoScheduleRegular podRegular node\\nNo taints\\nNo tolerations\\nPods with no tolerations\\nmay only be scheduledto nodes without taints.\\nFigure 16.1 A pod is only scheduled to a node if it tolerates the node’s taints.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 491}, page_content='460 CHAPTER  16 Advanced scheduling\\nbut can be seen in the pod’s YAML).  Those two tolerations refer to the NoExecute\\ninstead of the NoSchedule  effect. \\n Each taint has an effect associated  with it. Three possible effects exist:\\n\\uf0a1NoSchedule , which means pods won’t be scheduled  to the node if they don’t tol-\\nerate the taint.\\n\\uf0a1PreferNoSchedule  is a soft version of NoSchedule , meaning the scheduler will\\ntry to avoid scheduling the pod to the node, but will schedule it to the node if it\\ncan’t schedule it somewhere else. \\n\\uf0a1NoExecute , unlike NoSchedule  and PreferNoSchedule  that only affect schedul-\\ning, also affects pods already running on the node. If you add a NoExecute  taint\\nto a node, pods that are already running  on that node and don’t tolerate the\\nNoExecute  taint will be evicted from the node. \\n16.1.2 Adding custom taints to a node\\nImagine having a single Kubernetes cluste r where you run both production and non-\\nproduction workloads. It’s of the utmost importance that non-production pods never\\nrun on the production nodes. This can be ac hieved by adding a taint to your produc-\\ntion nodes. To add a taint, you use the kubectl  taint  command:\\n$ kubectl taint node node1.k8s node-type=production:NoSchedule\\nnode \"node1.k8s\" tainted\\nThis adds a taint with key node-type , value production  and the NoSchedule  effect. If\\nyou now deploy multiple replicas of a regula r pod, you’ll see none of them are sched-\\nuled to the node you tainted, as shown in the following listing.\\n$ kubectl run test --image busybox --replicas 5 -- sleep 99999\\ndeployment \"test\" created\\n$ kubectl get po -o wide\\nNAME                READY  STATUS    RESTARTS   AGE   IP          NODE\\ntest-196686-46ngl   1/1    Running   0          12s   10.47.0.1   node2.k8s\\ntest-196686-73p89   1/1    Running   0          12s   10.47.0.7   node2.k8s\\ntest-196686-77280   1/1    Running   0          12s   10.47.0.6   node2.k8s\\ntest-196686-h9m8f   1/1    Running   0          12s   10.47.0.5   node2.k8s\\ntest-196686-p85ll   1/1    Running   0          12s   10.47.0.4   node2.k8s\\nNow, no one can inadvertently deploy  pods onto the production nodes. \\n16.1.3 Adding tolerations to pods\\nTo deploy production pods to the production  nodes, they need to tolerate the taint\\nyou added to the nodes. The manifests of your production pods need to include the\\nYAML snippet shown in the following listing.\\n Listing 16.3 Deploying pods without a toleration'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 492}, page_content='461 Using taints and tolerations to repel pods from certain nodes\\napiVersion: extensions/v1beta1\\nkind: Deploymentmetadata:\\n  name: prod\\nspec:  replicas: 5\\n  template:\\n    spec:\\n      ...\\n      tolerations:      - key: node-type         \\n        Operator: Equal        \\n        value: production              effect: NoSchedule     \\nIf you deploy this Deployment, you’ll se e its pods get deployed to the production\\nnode, as shown in the next listing.\\n$ kubectl get po -o wide\\nNAME                READY  STATUS    RESTARTS   AGE   IP          NODE\\nprod-350605-1ph5h   0/1    Running   0          16s   10.44.0.3   node1.k8s\\nprod-350605-ctqcr   1/1    Running   0          16s   10.47.0.4   node2.k8s\\nprod-350605-f7pcc   0/1    Running   0          17s   10.44.0.6   node1.k8s\\nprod-350605-k7c8g   1/1    Running   0          17s   10.47.0.9   node2.k8s\\nprod-350605-rp1nv   0/1    Running   0          17s   10.44.0.4   node1.k8s\\nAs you can see in the listing, produc tion pods were also deployed to node2 , which isn’t\\na production node. To prevent that from happening, you’d also need to taint the non-\\nproduction nodes with a taint such as node-type=non-pro duction:NoSchedule . Then\\nyou’d also need to add the matching tole ration to all your non-production pods.\\n16.1.4 Understanding what taints and tolerations can be used for\\nNodes can have more than one taint and pods can have more than one toleration. As\\nyou’ve seen, taints can only have a key and an effect and don’t re quire a value. Tolera-\\ntions can tolerate a specific value by specifying the Equal  operator (that’s also the\\ndefault operator if you don’t specify one), or  they can tolerate any value for a specific\\ntaint key if you use the Exists  operator.\\nUSING  TAINTS  AND TOLERATIONS  DURING  SCHEDULING\\nTaints can be used to prevent scheduling of new pods ( NoSchedule  effect) and to\\ndefine unpreferred nodes ( PreferNoSchedule  effect) and even evict existing pods\\nfrom a node ( NoExecute ).\\n You can set up taints and tolerations any way you see fit. For example, you could\\npartition your cluster into multiple part itions, allowing your development teams to\\nschedule pods only to their respective nodes. You can also use taints and tolerationsListing 16.4 A production Deployment with  a toleration: production-deployment.yaml\\nListing 16.5 Pods with the toleration are deployed on production node1This toleration allows the \\npod to be scheduled to production nodes.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 493}, page_content='462 CHAPTER  16 Advanced scheduling\\nwhen several of your nodes provide special hardware and only part of your pods need\\nto use it.\\nCONFIGURING  HOW LONG AFTER  A NODE FAILURE  A POD IS RESCHEDULED\\nYou can also use a toleration to specify how long Kubernetes should wait before\\nrescheduling a pod to another node if the node the pod is running on becomes\\nunready or unreachable. If you look at the tolerations of one of your pods, you’ll see\\ntwo tolerations, which are shown in the following listing.\\n$ kubectl get po prod-350605-1ph5h -o yaml\\n...  tolerations:\\n  - effect: NoExecute                            \\n    key: node.alpha.kubernetes.io/notReady           operator: Exists                             \\n    tolerationSeconds: 300                       \\n  - effect: NoExecute                                  key: node.alpha.kubernetes.io/unreachable      \\n    operator: Exists                               \\n    tolerationSeconds: 300                         \\nThese two tolerations say that this pod tolerates a node being notReady  or unreach-\\nable  for 300 seconds. The Kubernetes Control Plane,  when it detects that a node is no\\nlonger ready or no longer reachable, will wait for 300 seconds before it deletes the\\npod and reschedules it to another node.\\n These two tolerations are automatically ad ded to pods that don’t define them. If\\nthat five-minute delay is too long for your pods, you can make the delay shorter by\\nadding those two tolerations to the pod’s spec.\\nNOTE This is currently an alpha feature, so it may change in future versions\\nof Kubernetes. Taint-based evictions also  aren’t enabled by default. You enable\\nthem by running the Controller Manager with the --feature-gates=Taint-\\nBasedEvictions=true  option.\\n16.2 Using node affinity to attract pods to certain nodes\\nAs you’ve learned, taints are used to keep  pods away from certain nodes. Now you’ll\\nlearn about a newer mechanism called node affinity , which allows you to tell Kuberne-\\ntes to schedule pods only to specific subsets of nodes.\\nCOMPARING  NODE AFFINITY  TO NODE SELECTORS\\nThe initial node affinity mechanism in early versions of Kubernetes was the node-\\nSelector  field in the pod specification. The no de had to include all the labels speci-\\nfied in that field to be eligible to become the target for the pod. \\n Node selectors get the job done and ar e simple, but they don’t offer everything\\nthat you may need. Because of that, a mo re powerful mechanism was introduced.Listing 16.6 Pod with default tolerations\\nThe pod tolerates the node being \\nnotReady for 300 seconds, before \\nit needs to be rescheduled.\\nThe same applies to the \\nnode being unreachable.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 494}, page_content='463 Using node affinity to attract pods to certain nodes\\nNode selectors will eventually be deprecat ed, so it’s important you understand the\\nnew node affinity rules.\\n Similar to node selectors, each pod can define its own node affinity rules. These\\nallow you to specify either hard requirements or preferences. By specifying a prefer-ence, you tell Kubernetes which nodes you prefer for a specific pod, and Kubernetes\\nwill try to schedule the pod to one of those no des. If that’s not possible, it will choose\\none of the other nodes. \\nEXAMINING  THE DEFAULT  NODE LABELS\\nNode affinity selects nodes based on their labels, the same way node selectors do.Before you see how to use node affinity, let’ s examine the labels of one of the nodes in\\na Google Kubernetes Engine cluster (GKE) to see what the default node labels are.\\nThey’re shown in the following listing.\\n$ kubectl describe node gke-kubia-default-pool-db274c5a-mjnf\\nName:     gke-kubia-default-pool-db274c5a-mjnf\\nRole:Labels:   beta.kubernetes.io/arch=amd64\\n          beta.kubernetes.io/fluentd-ds-ready=true\\n          beta.kubernetes.io/instance-type=f1-micro          beta.kubernetes.io/os=linux\\n          cloud.google.com/gke-nodepool=default-pool\\n          failure-domain.beta. kubernetes.io/region=europe-west1         \\n          failure-domain.beta. kubernetes.io/zone=europe-west1-d         \\n          kubernetes.io/hostname=gke-kubia-default-pool-db274c5a-mjnf   \\nThe node has many labels, but the last thre e are the most important when it comes to\\nnode affinity and pod affinity, which you’ll  learn about later. The meaning of those\\nthree labels is as follows:\\n\\uf0a1failure-domain.beta. kubernetes.io/region  specifies the geographical region\\nthe node is located in.\\n\\uf0a1failure-domain.beta .kubernetes.io/zone  specifies the availability zone the\\nnode is in.\\n\\uf0a1kubernetes.io/hostname  is obviously the node’s hostname.\\nThese and other labels can be used in pod affinity rules. In chapter 3, you already\\nlearned how you can add a custom label to no des and use it in a pod’s node selector.\\nYou used the custom label to deploy pods only  to nodes with that label by adding a node\\nselector to the pods. Now, you’ll see how to  do the same using node affinity rules.\\n16.2.1 Specifying hard node affinity rules\\nIn the example in chapter 3, you used the no de selector to deploy a pod that requires\\na GPU only to nodes that have a GPU. The pod spec included the nodeSelector  field\\nshown in the following listing.Listing 16.7 Default labels of a node in GKE\\nThese three\\nlabels are the\\nmost important\\nones related to\\nnode affinity.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 495}, page_content='464 CHAPTER  16 Advanced scheduling\\napiVersion: v1\\nkind: Podmetadata:\\n  name: kubia-gpu\\nspec:  nodeSelector:          \\n    gpu: \"true\"          \\n  ...\\nThe nodeSelector  field specifies that the pod should  only be deployed on nodes that\\ninclude the gpu=true  label. If you replace the node se lector with a node affinity rule,\\nthe pod definition will look like the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:  name: kubia-gpu\\nspec:\\n  affinity:    nodeAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:\\n        nodeSelectorTerms:        - matchExpressions:\\n          - key: gpu\\n            operator: In            values:\\n            - \"true\"\\nThe first thing you’ll notice is that this is much more complicated than a simple node\\nselector. But that’s because it’s much more expressive. Let’s examine the rule in detail. \\nMAKING  SENSE  OF THE LONG NODEAFFINITY  ATTRIBUTE  NAME\\nAs you can see, the pod’s spec section contains an affinity  field that contains a node-\\nAffinity  field, which contains a field with an  extremely long name, so let’s focus on\\nthat first.\\n Let’s break it down into two parts and examine what they mean:\\n\\uf0a1requiredDuring Scheduling...  means the rules defined under this field spec-\\nify the labels the node must have for the pod to be scheduled to the node.\\n\\uf0a1...IgnoredDuringExecution  means the rules defined under the field don’t\\naffect pods already executing on the node. \\nAt this point, let me make th ings easier for you by letting you know that affinity cur-\\nrently only affects pod scheduling and never causes a pod to be evicted from a node.\\nThat’s why all the rules right now always end with IgnoredDuringExecution . Eventu-\\nally, Kubernetes will also support RequiredDuringExecution , which means that if youListing 16.8 A pod using a node selector: kubia-gpu-nodeselector.yaml\\nListing 16.9 A pod using a nodeAffinity  rule: kubia-gpu-nodeaffinity.yamlThis pod is only scheduled \\nto nodes that have the gpu=true label.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 496}, page_content='465 Using node affinity to attract pods to certain nodes\\nr e m o v e  a  l a b e l  f r o m  a  n o d e ,  p o d s  t h a t  r e q u i r e  t h e  n o d e  t o  h a v e  t h a t  l a b e l  w i l l  b e\\nevicted from such a node. As I’ve said, that ’s not yet supported in Kubernetes, so let’s\\nnot concern ourselves with the second part of that long field any longer.\\nUNDERSTANDING  NODESELECTOR TERMS\\nBy keeping what was explained in the previous  section in mind, it’s easy to understand\\nthat the nodeSelectorTerms  field and the matchExpressions  field define which\\nexpressions the node’s labels must match for the pod to be scheduled to the node.\\nThe single expression in the example is si mple to understand. The node must have a\\ngpu label whose value is set to true . \\n This pod will therefore only be scheduled to nodes that have the gpu=true  label, as\\nshown in figure 16.2.\\nNow comes the more interesting part. Node also  affinity allows you to prioritize nodes\\nduring scheduling. We’ll look at that next.\\n16.2.2 Prioritizing nodes when scheduling a pod\\nThe biggest benefit of the newly introduced no de affinity feature is the ability to spec-\\nify which nodes the Scheduler should prefer when scheduling a specific pod. This is\\ndone through the preferredDuringSchedulin gIgnoredDuringExecution  field.\\n Imagine having multiple datacenters ac ross different countries. Each datacenter\\nrepresents a separate availability zone. In each zone, you have certain machines meant\\nonly for your own use and others that your partner companies can use. You now want\\nto deploy a few pods and you’d prefer them to be scheduled to zone1  and to theNode with a GPU\\nPod\\nNode afﬁnity\\nRequired label:\\ngpu=truePod\\nNo node afﬁnitygpu: true\\nNode with a GPU Node without a GPU Node without a GPUgpu: true\\nThis pod may be scheduled only\\nto nodes with gpu=true labelThis pod may be\\nscheduled to any node\\nFigure 16.2 A pod’s node affinity specifies which labels a node must have for the pod to be \\nscheduled to it.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 497}, page_content='466 CHAPTER  16 Advanced scheduling\\nmachines reserved for your company’s de ployments. If those machines don’t have\\nenough room for the pods or if other impo rtant reasons exist that prevent them from\\nbeing scheduled there, you’re okay with them being scheduled to the machines your\\npartners use and to the other zones. No de affinity allows you to do that.\\nLABELING  NODES\\nFirst, the nodes need to be labeled appropri ately. Each node needs to have a label that\\ndesignates the availability zone the node belongs to and a label marking it as either adedicated or a shared node.\\n Appendix B explains how to set up a th ree-node cluster (one master and two\\nworker nodes) in VMs running locally. In the following examples, I’ll use the two worker\\nnodes in that cluster, but you can also us e Google Kubernetes Engine or any other\\nmulti-node cluster. \\nNOTE Minikube isn’t the best choice for running these examples, because it\\nruns only one node.\\nFirst, label the nodes, as shown in the next listing.\\n$ kubectl label node node1.k8s availability-zone=zone1\\nnode \"node1.k8s\" labeled\\n$ kubectl label node node1.k8s share-type=dedicated\\nnode \"node1.k8s\" labeled$ kubectl label node node2.k8s availability-zone=zone2\\nnode \"node2.k8s\" labeled\\n$ kubectl label node node2.k8s share-type=sharednode \"node2.k8s\" labeled\\n$ kubectl get node -L availability-zone -L share-type\\nNAME         STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE\\nmaster.k8s   Ready     4d        v1.6.4    <none>              <none>\\nnode1.k8s    Ready     4d        v1.6.4    zone1               dedicated\\nnode2.k8s    Ready     4d        v1.6.4    zone2               shared\\nSPECIFYING  PREFERENTIAL  NODE AFFINITY  RULES\\nWith the node labels set up, you can now create a Deployment that prefers dedicated\\nnodes in zone1 . The following listing shows the Deployment manifest.\\napiVersion: extensions/v1beta1\\nkind: Deploymentmetadata:\\n  name: pref\\nspec:  template:\\n    ...\\n    spec:      affinity:\\n        nodeAffinity:Listing 16.10 Labeling nodes\\nListing 16.11 Deployment with preferred node affinity: preferred-deployment.yaml'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 498}, page_content='467 Using node affinity to attract pods to certain nodes\\n          preferredDuringSchedulingIgnoredDuringExecution:    \\n          - weight: 80                               \\n            preference:                                            matchExpressions:                      \\n              - key: availability-zone               \\n                operator: In                                         values:                              \\n                - zone1                              \\n          - weight: 20                     \\n            preference:                    \\n              matchExpressions:                          - key: share-type            \\n                operator: In               \\n                values:                                    - dedicated                \\n      ...\\nLet’s examine the listing closely. You’re defi ning a node affinity preference, instead of\\na hard requirement. You want the pods scheduled to nodes that include the labels\\navailability-zone=zone1  and share-type=dedicated . You’re saying that the first\\npreference rule is important by setting its weight  to 80, whereas the second one is\\nmuch less important ( weight  is set to 20).\\nUNDERSTANDING  HOW NODE PREFERENCES  WORK\\nIf your cluster had many nodes, when sche duling the pods of the Deployment in the\\nprevious listing, the nodes would be split in to four groups, as shown in figure 16.3.\\nNodes whose availability-zone  and share-type  labels match the pod’s node affin-\\nity are ranked the highest. Then, because of  how the weights in the pod’s node affinity\\nrules are configured, next come the shared  nodes in zone1 , then come the dedicated\\nnodes in the other zones, and at the lo west priority are all the other nodes.You’re\\nspecifying\\npreferences,\\nnot hard\\nrequirements.You prefer the pod to be \\nscheduled to zone1. This is your most important \\npreference.\\nYou also prefer that your \\npods be scheduled to dedicated nodes, but this is \\nfour times less important \\nthan your zone preference.\\nNode\\nTop priorityAvailability zone 1\\nPodPriority: 2 Priority: 3 Priority: 4\\nNode afﬁnity\\nPreferred labels:\\navail-zone: zone1 (weight 80)\\nshare: dedicated (weight 20)\\navail-zone: zone1\\nshare: dedicated\\nNode\\navail-zone: zone1\\nshare: shared\\nNodeAvailability zone 2\\navail-zone: zone2\\nshare: dedicated\\nNodeavail-zone: zone2\\nshare: shared\\nThis pod may be scheduled to\\nany node, but certain nodes arepreferred based on their labels.\\nFigure 16.3 Prioritizing nodes based on a pod’s node affinity preferences'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 499}, page_content='468 CHAPTER  16 Advanced scheduling\\nDEPLOYING  THE PODS IN THE TWO-NODE CLUSTER\\nIf you create this Deployment in your two- node cluster, you should see most (if not\\nall) of your pods deployed to node1 . Examine the following listing to see if that’s true.\\n$ kubectl get po -o wide\\nNAME                READY   STATUS    RESTARTS  AGE   IP          NODE\\npref-607515-1rnwv   1/1     Running   0         4m    10.47.0.1   node2.k8s\\npref-607515-27wp0   1/1     Running   0         4m    10.44.0.8   node1.k8s\\npref-607515-5xd0z   1/1     Running   0         4m    10.44.0.5   node1.k8s\\npref-607515-jx9wt   1/1     Running   0         4m    10.44.0.4   node1.k8s\\npref-607515-mlgqm   1/1     Running   0         4m    10.44.0.6   node1.k8s\\nOut of the five pods that were created, four of them landed on node1  and only one\\nlanded on node2 . Why did one of them land on node2  instead of node1 ? The reason is\\nthat besides the node affinity  prioritization function, the Scheduler also uses other pri-\\noritization functions to decide where to  schedule a pod. One of those is the Selector-\\nSpreadPriority  function, which makes sure pods be longing to the same ReplicaSet or\\nService are spread around different nodes so  a node failure won’t bring the whole ser-\\nvice down. That’s most likely what caused  one of the pods to be scheduled to node2 .\\n You can try scaling the Deployment up to 20 or more and you’ll see the majority of\\npods will be scheduled to node1 . In my test, only two out of the 20 were scheduled to\\nnode2 . If you hadn’t defined any node affinity  preferences, the pods would have been\\nspread around the two nodes evenly.\\n16.3 Co-locating pods with pod affinity and anti-affinity\\nYou’ve seen how node affinity rules are used  to influence which node a pod is scheduled\\nto. But these rules only affect the affinity  between a pod and a node, whereas sometimes\\nyou’d like to have the ability to specify the affinity between pods themselves. \\n For example, imagine having a frontend  and a backend pod. Having those pods\\ndeployed near to each other reduces late ncy and improves the performance of the\\napp. You could use node affinity rules to ensure both are deployed to the same node,\\nrack, or datacenter, but then you’d have to  specify exactly which node, rack, or data-\\ncenter to schedule them to, which is not the best solution. It’s better to let Kubernetes\\ndeploy your pods anywhere it sees fit, while keeping the frontend and backend pods\\nclose together. This can be achieved using pod affinity . Let’s learn more about it with\\nan example.\\n16.3.1 Using inter-pod affinity to deploy pods on the same node\\nYou’ll deploy a backend pod and five frontend pod replicas with pod affinity config-\\nured so that they’re all deployed on  the same node as the backend pod.\\n First, deploy the backend pod:\\n$ kubectl run backend -l app=backend --image busybox -- sleep 999999\\ndeployment \"backend\" createdListing 16.12 Seeing where pods were scheduled'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 500}, page_content='469 Co-locating pods with pod affinity and anti-affinity\\nThis Deployment is not special in any way.  The only thing you need to note is the\\napp=backend  label you added to the pod using the -l option. This label is what you’ll\\nuse in the frontend pod’s podAffinity  configuration. \\nSPECIFYING  POD AFFINITY  IN A POD DEFINITION\\nThe frontend pod’s definition is shown in the following listing.\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:  name: frontend\\nspec:\\n  replicas: 5  template:\\n    ...\\n    spec:      affinity:\\n        podAffinity:                                 \\n          requiredDuringSchedulingIgnoredDuringExecution:             - topologyKey: kubernetes.io/hostname           \\n            labelSelector:                                \\n              matchLabels:                                                app: backend                              \\n      ...\\nThe listing shows that this Deployment will create pods that have a hard requirement\\nto be deployed on the same node (specified by the topologyKey  field) as pods that\\nhave the app=backend  label (see figure 16.4).Listing 16.13 Pod using podAffinity : frontend-podaffinity-host.yaml\\nDefining \\npodAffinity rules\\nDefining a hard \\nrequirement, not \\na preference\\nThe pods of this Deployment \\nmust be deployed on the \\nsame node as the pods that \\nmatch the selector.\\nAll frontend pods will\\nbe scheduled only tothe node the backendpod was scheduled to.Some node Other nodes\\nFrontend podsBackend\\npod\\nPod afﬁnity\\nLabel selector: app=backend\\nTopology key: hostnameapp: backend\\nFigure 16.4 Pod affinity allows s cheduling pods to the node where other pods \\nwith a specific label are.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 501}, page_content='470 CHAPTER  16 Advanced scheduling\\nNOTE Instead of the simpler matchLabels  field, you could also use the more\\nexpressive matchExpressions  field.\\nDEPLOYING  A POD WITH POD AFFINITY\\nBefore you create this Deployment, let’s see which node the backend pod was sched-\\nuled to earlier:\\n$ kubectl get po -o wide\\nNAME                   READY  STATUS   RESTARTS  AGE  IP         NODEbackend-257820-qhqj6   1/1    Running  0         8m   10.47.0.1  node2.k8s\\nWhen you create the frontend pods, they should be deployed to node2  as well. You’re\\ngoing to create the Deployment and see wh ere the pods are deployed. This is shown\\nin the next listing.\\n$ kubectl create -f frontend-podaffinity-host.yaml\\ndeployment \"frontend\" created\\n$ kubectl get po -o wide\\nNAME                   READY  STATUS    RESTARTS  AGE  IP         NODE\\nbackend-257820-qhqj6   1/1    Running   0         8m   10.47.0.1  node2.k8s\\nfrontend-121895-2c1ts  1/1    Running   0         13s  10.47.0.6  node2.k8s\\nfrontend-121895-776m7  1/1    Running   0         13s  10.47.0.4  node2.k8s\\nfrontend-121895-7ffsm  1/1    Running   0         13s  10.47.0.8  node2.k8s\\nfrontend-121895-fpgm6  1/1    Running   0         13s  10.47.0.7  node2.k8s\\nfrontend-121895-vb9ll  1/1    Running   0         13s  10.47.0.5  node2.k8s\\nAll the frontend pods were indeed schedul ed to the same node as the backend pod.\\nWhen scheduling the frontend pod, the Sche duler first found all the pods that match\\nthe labelSelector  defined in the frontend pod’s podAffinity  configuration and\\nthen scheduled the frontend pod to the same node.\\nUNDERSTANDING  HOW THE SCHEDULER  USES POD AFFINITY  RULES\\nWhat’s interesting is that if you now dele te the backend pod, the Scheduler will sched-\\nule the pod to node2  even though it doesn’t define any pod affinity rules itself (the\\nrules are only on the frontend pods). This makes sense, because otherwise if the back-\\nend pod were to be deleted by accident and rescheduled to a different node, the fron-tend pods’ affinity rules would be broken. \\n You can confirm the Scheduler takes other pods’ pod affinity rules into account, if\\nyou increase the Scheduler’s logging level and then check its log. The following listingshows the relevant log lines.\\n... Attempting to schedule pod: default/backend-257820-qhqj6\\n... ...\\n... backend-qhqj6 -> node2.k8s: Taint Toleration Priority, Score: (10)Listing 16.14 Deploying frontend pods a nd seeing which node they’re scheduled to\\nListing 16.15 Scheduler log showing why the backend pod is scheduled to node2'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 502}, page_content='471 Co-locating pods with pod affinity and anti-affinity\\n... backend-qhqj6 -> node1.k8s: Taint Toleration Priority, Score: (10)\\n... backend-qhqj6 -> node2.k8s: InterPodAffinityPriority, Score: (10)\\n... backend-qhqj6 -> node1.k8s: InterPodAffinityPriority, Score: (0)\\n... backend-qhqj6 -> node2.k8s: SelectorSpreadPriority, Score: (10)\\n... backend-qhqj6 -> node1.k8s: SelectorSpreadPriority, Score: (10)\\n... backend-qhqj6 -> node2.k8s: NodeAffinityPriority, Score: (0)... backend-qhqj6 -> node1.k8s: NodeAffinityPriority, Score: (0)\\n... Host node2.k8s => Score 100030\\n... Host node1.k8s => Score 100022\\n... Attempting to bind backend-257820-qhqj6 to node2.k8s\\nIf you focus on the two lines in bold, you’ll  see that during the scheduling of the back-\\nend pod, node2  received a higher score than node1  because of inter-pod affinity. \\n16.3.2 Deploying pods in the same rack, availability zone, or \\ngeographic region\\nIn the previous example, you used podAffinity  to deploy frontend pods onto the\\nsame node as the backend pods. You probab ly don’t want all your frontend pods to\\nrun on the same machine, but you’d still like to keep them close to the backend\\npod—for example, run them in the same availability zone. \\nCO-LOCATING  PODS IN THE SAME AVAILABILITY  ZONE\\nThe cluster I’m using runs in three VMs on my local machine, so all the nodes are in\\nthe same availability zone, so to speak. But if the nodes were in different zones, all I’dneed to do to run the frontend pods in th e same zone as the backend pod would be to\\nchange the \\ntopologyKey  property to failure-domain.beta. kubernetes.io/zone . \\nCO-LOCATING  PODS IN THE SAME GEOGRAPHICAL  REGION\\nTo allow the pods to be deployed in the sa me region instead of the same zone (cloud\\nproviders usually have datacenters located in  different geographical regions and split\\ninto multiple availability zones in each region), the topologyKey  would be set to\\nfailure-domain.beta.k ubernetes.io/region .\\nUNDERSTANDING  HOW TOPOLOGY KEY WORKS\\nThe way topologyKey  works is simple. The three keys we’ve mentioned so far aren’t\\nspecial. If you want, you can easily use your own topologyKey , such as rack , to have\\nthe pods scheduled to the same server ra ck. The only prerequisite is to add a rack\\nlabel to your nodes. This scenario is shown in figure 16.5.\\n For example, if you had 20 nodes, with 10 in each rack, you’d label the first ten as\\nrack=rack1  and the others as rack=rack2 . Then, when defining a pod’s podAffinity ,\\nyou’d set the toplogyKey  to rack . \\n When the Scheduler is deciding where to deploy a pod, it checks the pod’s pod-\\nAffinity  config, finds the pods that match th e label selector, and looks up the nodes\\nthey’re running on. Specifically, it looks up the nodes’ label whose key matches the\\ntopologyKey  field specified in podAffinity . Then it selects all the nodes whose label'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 503}, page_content='472 CHAPTER  16 Advanced scheduling\\nmatches the values of the pods it found ea rlier. In figure 16.5, the label selector\\nmatched the backend pod, which runs on Node 12. The value of the rack  label on\\nthat node equals rack2 , so when scheduling a frontend pod, the Scheduler will only\\nselect among the nodes that have the rack=rack2  label.\\nNOTE By default, the label selector only matches pods in the same name-\\nspace as the pod that’s being scheduled. But you can also select pods fromother namespaces by adding a \\nnamespaces  field at the same level as label-\\nSelector .\\n16.3.3 Expressing pod affinity preferences instead of hard requirements\\nEarlier, when we talked about node affinity, you saw that nodeAffinity  can be used to\\nexpress a hard requirement, which means a po d is only scheduled to nodes that match\\nthe node affinity rules. It can also be used to specify node preferences, to instruct the\\nScheduler to schedule the pod to certain node s, while allowing it to schedule it any-\\nwhere else if those nodes can’t fit the pod for any reason.\\n The same also applies to podAffinity . You can tell the Scheduler you’d prefer to\\nhave your frontend pods scheduled onto the same node as your backend pod, but if\\nthat’s not possible, you’re okay with them  being scheduled elsewhere. An example of\\na Deployment using the preferredDuringSchedulin gIgnoredDuringExecution  pod\\naffinity rule is shown in the next listing.Frontend pods will be\\nscheduled to nodes inthe same rack as thebackend pod.\\nNode 1Rack 1\\nrack: rack1\\nNode 2rack: rack1\\nNode 3\\n...rack: rack1\\nNode 10rack: rack1Node 11Rack 2\\nrack: rack2\\nNode 12rack: rack2\\n...\\nNode 20rack: rack2Backend\\npodapp: backendFrontend pods\\nPod afﬁnity (required)\\nLabel selector: app=backend\\nTopology key: rack\\nFigure 16.5 The topologyKey  in podAffinity  determines the scope of where the pod \\nshould be scheduled to.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 504}, page_content='473 Co-locating pods with pod affinity and anti-affinity\\napiVersion: extensions/v1beta1\\nkind: Deploymentmetadata:\\n  name: frontend\\nspec:  replicas: 5\\n  template:\\n    ...\\n    spec:\\n      affinity:        podAffinity:\\n          preferredDuringSchedulingIgnoredDuringExecution:  \\n          - weight: 80                                                    podAffinityTerm:                                  \\n              topologyKey: kubernetes.io/hostname             \\n              labelSelector:                                                  matchLabels:                                  \\n                  app: backend                                \\n      containers: ...\\nAs in nodeAffinity  preference rules, you need to de fine a weight for each rule. You\\nalso need to specify the topologyKey  and labelSelector , as in the hard-requirement\\npodAffinity  rules. Figure 16.6 shows this scenario.\\nDeploying this pod, as with your nodeAffinity  example, deploys four pods on the same\\nnode as the backend pod, an d one pod on the other node  (see the following listing).Listing 16.16 Pod affinity preference\\nPreferred \\ninstead of Required\\nA weight and a \\npodAffinity term is specified as in the \\nprevious example\\nThe Scheduler will prefer\\nNode 2 for frontend pods,but may schedule podsto Node 1 as well.Node 1 Node 2\\nBackend\\npodapp: backend\\nFrontend pod\\nPod afﬁnity (preferred)\\nLabel selector: app=backend\\nTopology key: hostnamehostname: node2 hostname: node1\\nFigure 16.6 Pod affinity can be used to make the Scheduler prefer nodes where \\npods with a certain label are running.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 505}, page_content='474 CHAPTER  16 Advanced scheduling\\n$ kubectl get po -o wide\\nNAME                   READY  STATUS   RESTARTS  AGE  IP          NODE\\nbackend-257820-ssrgj   1/1    Running  0         1h   10.47.0.9   node2.k8s\\nfrontend-941083-3mff9  1/1    Running  0         8m   10.44.0.4   node1.k8s\\nfrontend-941083-7fp7d  1/1    Running  0         8m   10.47.0.6   node2.k8s\\nfrontend-941083-cq23b  1/1    Running  0         8m   10.47.0.1   node2.k8s\\nfrontend-941083-m70sw  1/1    Running  0         8m   10.47.0.5   node2.k8s\\nfrontend-941083-wsjv8  1/1    Running  0         8m   10.47.0.4   node2.k8s\\n16.3.4 Scheduling pods away from each other with pod anti-affinity\\nYou’ve seen how to tell the Scheduler to  co-locate pods, but sometimes you may want\\nthe exact opposite. You may want to keep po ds away from each other. This is called\\npod anti-affinity. It’s specified the same way as pod affinity, except that you use the\\npodAntiAffinity  property instead of podAffinity , which results in the Scheduler\\nnever choosing nodes where pods matching the podAntiAffinity ’s label selector are\\nrunning, as shown in figure 16.7.\\nAn example of why you’d want to use pod anti -affinity is when two sets of pods inter-\\nfere with each other’s performance if they run on the same node. In that case, you\\nwant to tell the Scheduler to never schedu le those pods on the same node. Another\\nexample would be to force the Scheduler to spread pods of the same group across dif-\\nferent availability zones or regions, so that  a failure of a whole zone (or region) never\\nbrings the service down completely. Listing 16.17 Pods deployed with podAffinity  preferences\\nThese pods will NOT be scheduled\\nto the same node(s) where podswith app=foo label are running.Some node Other nodes\\nPodsPod: foo\\nPod (required)anti-afﬁnity\\nLabel selector: app=foo\\nTopology key: hostnameapp: foo\\nFigure 16.7 Using pod anti-affinity to keep pods away from nodes that run pods \\nwith a certain label.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 506}, page_content='475 Co-locating pods with pod affinity and anti-affinity\\nUSING ANTI-AFFINITY  TO SPREAD  APART  PODS OF THE SAME DEPLOYMENT\\nLet’s see how to force your frontend pods to  be scheduled to different nodes. The fol-\\nlowing listing shows how the pods’ anti-affinity is configured.\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: frontend\\nspec:  replicas: 5\\n  template:\\n    metadata:      labels:                  \\n        app: frontend          \\n    spec:      affinity:\\n        podAntiAffinity:                                      \\n          requiredDuringSchedulingIgnoredDuringExecution:               - topologyKey: kubernetes.io/hostname            \\n            labelSelector:                                 \\n              matchLabels:                                                 app: frontend                              \\n      containers: ...\\nThis time, you’re defining podAntiAffinity  instead of podAffinity , and you’re mak-\\ning the labelSelector  match the same pods that the Deployment creates. Let’s see\\nwhat happens when you create this Deployment. The pods created by it are shown in\\nthe following listing.\\n$ kubectl get po -l app=frontend -o wide\\nNAME                    READY  STATUS   RESTARTS  AGE  IP         NODE\\nfrontend-286632-0lffz   0/1    Pending  0         1m   <none>frontend-286632-2rkcz   1/1    Running  0         1m   10.47.0.1  node2.k8s\\nfrontend-286632-4nwhp   0/1    Pending  0         1m   <none>\\nfrontend-286632-h4686   0/1    Pending  0         1m   <none>frontend-286632-st222   1/1    Running  0         1m   10.44.0.4  node1.k8s\\nAs you can see, only two po ds were scheduled—one to node1 , the other to node2 . The\\nthree remaining pods are all Pending , because the Scheduler isn’t allowed to schedule\\nthem to the same nodes.\\nUSING PREFERENTIAL  POD ANTI-AFFINITY\\nIn this case, you probably should have spec ified a soft requirement instead (using the\\npreferredDuringScheduli ngIgnoredDuringExecution  property). After all, it’s not\\nsuch a big problem if two frontend pods ru n on the same node. But in scenarios where\\nthat’s a problem, using requiredDuringScheduling  is appropriate. Listing 16.18 Pods with anti-affinity: frontend-podantiaffinity-host.yaml\\nListing 16.19 Pods created by the DeploymentThe frontend pods have \\nthe app=frontend label.\\nDefining hard-\\nrequirements for \\npod anti-affinity\\nA frontend pod must not \\nbe scheduled to the same \\nmachine as a pod with \\napp=frontend label.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 507}, page_content='476 CHAPTER  16 Advanced scheduling\\n As with pod affinity, the topologyKey  property determines the scope of where the\\npod shouldn’t be deployed to. You can use it to ensure pods aren’t deployed to the\\nsame rack, availability zone, region, or any custom scope you create using custom\\nnode labels.\\n16.4 Summary\\nIn this chapter, we looked at how to ensure pods aren’t scheduled to certain nodes orare only scheduled to specific nodes, either  because of the node’s labels or because of\\nthe pods running on them.\\n You learned that\\n\\uf0a1If you add a taint to a node, pods won’ t be scheduled to that node unless they\\ntolerate that taint.\\n\\uf0a1Three types of taints exist: NoSchedule  completely prevents scheduling, Prefer-\\nNoSchedule  isn’t as strict, and NoExecute  even evicts existing pods from a node.\\n\\uf0a1The NoExecute  taint is also used to specify how long the Control Plane should\\nwait before rescheduling the pod when the node it runs on becomes unreach-\\nable or unready.\\n\\uf0a1Node affinity allows you to specify which nodes a pod should be scheduled to. It\\ncan be used to specify a hard requiremen t or to only express a node preference.\\n\\uf0a1Pod affinity is used to make the Schedu ler deploy pods to the same node where\\nanother pod is running (based on the pod’s labels). \\n\\uf0a1Pod affinity’s topologyKey  specifies how close the pod should be deployed to\\nthe other pod (onto the same node or onto  a node in the same rack, availability\\nzone, or availability region).\\n\\uf0a1Pod anti-affinity can be used to keep certain pods away from each other. \\n\\uf0a1Both pod affinity and anti-affinity, like node affinity, can either specify hard\\nrequirements or preferences.\\nIn the next chapter, you’ll learn about best  practices for developing apps and how to\\nmake them run smoothly in a Kubernetes environment.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 508}, page_content='477Best practices\\nfor developing apps\\nWe’ve now covered most of what you need to know to run your apps in Kubernetes.\\nWe’ve explored what each individual resource does and how it’s used. Now we’ll see\\nhow to combine them in a typical applic ation running on Kubernetes. We’ll also\\nlook at how to make an application run sm oothly. After all, that’s the whole point\\nof using Kubernetes, isn’t it? \\n Hopefully, this chapter will help to clear up any misunderstandings and explain\\nthings that weren’t explained clearly yet. Along the way, we’ll also introduce a few\\nadditional concepts that haven’t been mentioned up to this point.This chapter covers\\n\\uf0a1Understanding which Kubernetes resources \\nappear in a typical application\\n\\uf0a1Adding post-start and pre-stop pod lifecycle hooks\\n\\uf0a1Properly terminating an app without breaking client requests\\n\\uf0a1Making apps easy to manage in Kubernetes\\n\\uf0a1Using init containers in a pod\\n\\uf0a1Developing locally with Minikube'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 509}, page_content='478 CHAPTER  17 Best practices for developing apps\\n17.1 Bringing everything together\\nLet’s start by looking at what an actual app lication consists of. This will also give you a\\nchance to see if you remember everything you’ve learned so far and look at the big\\npicture. Figure 17.1 shows the Kubernetes components used in a typical application.\\nA typical application manifest contains on e or more Deployment and/or StatefulSet\\nobjects. Those include a pod template containing one or more containers, with a live-\\nness probe for each of them and a readine ss probe for the service(s) the container\\nprovides (if any). Pods that provide serv ices to others are exposed through one or\\nmore Services. When they need to be reac hable from outside the cluster, the Services\\nare either configured to be LoadBalancer  or NodePort -type Services, or exposed\\nthrough an Ingress resource. \\n The pod templates (and the pods created from them) usually reference two types\\nof Secrets—those for pulling container images  from private image registries and those\\nused directly by the process running insi de the pods. The Secrets themselves are\\nusually not part of the app lication manifest, because they aren’t configured by the\\napplication developers but by the operatio ns team. Secrets are usually assigned to\\nServiceAccounts, which are assigned to individual pods. Deﬁned in the app manifest by the developer\\nPod templateDeployment\\nlabels\\nPod(s)Label selector\\nlabelsCreated automatically at runtimeCreated by a cluster admin beforehand\\nContainer(s)\\nVolume(s)\\nReplicaSet(s)\\nEndpoints\\x81 Health probes\\n\\x81 Environment variables\\x81 Volume mounts\\x81 Resource reqs/limitsHorizontal\\nPodAutoscaler\\nStatefulSet\\nDaemonSet\\nJob\\nCronJobPersistent\\nVolume\\nConﬁgMap\\nServicePersistent\\nVolume\\nClaimSecret(s)Service\\naccount\\nStorage\\nClassLimitRange\\nResourceQuota\\nIngressimagePullSecret\\nFigure 17.1 Resources in a typical application'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 510}, page_content='479 Understanding the pod’s lifecycle\\n The application also contains one or more ConfigMaps, which are either used to\\ninitialize environment variables or mounted as a configMap  volume in the pod. Cer-\\ntain pods use additional volumes, such as an emptyDir  or a gitRepo  volume, whereas\\npods requiring persistent storage use persistentVolumeClaim  volumes. The Persistent-\\nVolumeClaims are also part of the application manifest, whereas StorageClasses refer-\\nenced by them are created by system administrators upfront. \\n In certain cases, an application also requ ires the use of Jobs or CronJobs. Daemon-\\nSets aren’t normally part of  application deployments, but are usually created by sysad-\\nmins to run system services on all or a subset of nodes. HorizontalPodAutoscalers\\nare either included in the manifest by the developers or added to the system later bythe ops team. The cluster administrator al so creates LimitRange and ResourceQuota\\nobjects to keep compute resource usage of individual pods and all the pods (as a\\nwhole) under control.\\n After the application is deployed, additi onal objects are created automatically by\\nthe various Kubernetes controllers. These include service Endpoints objects created\\nby the Endpoints controller, ReplicaSets cr eated by the Deployment controller, and\\nthe actual pods created by the ReplicaSet (o r Job, CronJob, StatefulSet, or DaemonSet)\\ncontrollers.\\n Resources are often labeled with one or more labels to keep them organized. This\\ndoesn’t apply only to pods but to all other re sources as well. In addition to labels, most\\nresources also contain annotations that desc ribe each resource, list the contact infor-\\nmation of the person or team responsible for it, or provide additional metadata for\\nmanagement and other tools. \\n At the center of all this is the Pod, wh ich arguably is the most important Kuberne-\\ntes resource. After all, each of  your applications runs inside it. To make sure you know\\nhow to develop apps that make the most out of their environment, let’s take one last\\nclose look at pods—this time from  the application’s perspective. \\n17.2 Understanding the pod’s lifecycle\\nWe’ve said that pods can be compared to  VMs dedicated to running only a single\\napplication. Although an application running inside a pod is not unlike an applicationrunning in a VM, significant differences do exist. One example is that apps running in\\na pod can be killed any time, because Ku bernetes needs to relocate the pod to\\nanother node for a reason or because of a scale-down request. We’ll explore this\\naspect next.\\n17.2.1 Applications must expect to be killed and relocated\\nOutside Kubernetes, apps running in VM s are seldom moved from one machine to\\nanother. When an operator moves the app,  they can also reconfigure the app and\\nmanually check that the app is running fi ne in the new location. With Kubernetes,\\napps are relocated much more frequently and automatically—no human operator'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 511}, page_content='480 CHAPTER  17 Best practices for developing apps\\nreconfigures them and makes sure they st ill run properly after the move. This means\\napplication developers need to make sure their apps allow being moved relatively\\noften. \\nEXPECTING  THE LOCAL  IP AND HOSTNAME  TO CHANGE\\nWhen a pod is killed and run elsewhere (tec hnically, it’s a new pod instance replac-\\ning the old one; the pod isn’t relocated), it not only has a new IP address but also a\\nnew name and hostname. Most stateless ap ps can usually handle this without any\\nadverse effects, but stateful apps usually ca n’t. We’ve learned that stateful apps can\\nbe run through a StatefulSet, which ensure s that when the app starts up on a new\\nnode after being rescheduled, it will still s ee the same host name and persistent state\\nas before. The pod’s IP will change nevertheless. Apps need to be prepared for that\\nto happen. The application developer therefore should never base membership in a\\nclustered app on the member’s IP address, and if basing it on the hostname, should\\nalways use a StatefulSet.\\nEXPECTING  THE DATA WRITTEN  TO DISK TO DISAPPEAR\\nAnother thing to keep in mind is that if the app writes data to disk, that data may not be\\navailable after the app is started inside a ne w pod, unless you mount persistent storage at\\nthe location the app is writing to. It shou ld be clear this happens when the pod is\\nrescheduled, but files written to disk will disappear even in scenarios that don’t involve\\nany rescheduling. Even during the lifetime of a single pod, the files written to disk by\\nthe app running in the pod may disappear. Let me explain this with an example.\\n Imagine an app that has a long and comp utationally intensive initial startup proce-\\ndure. To help the app come up faster on subsequent startups, the developers make\\nthe app cache the results of the initial star tup on disk (an example of this would be\\nthe scanning of all Java classes for annotations at startup and then writing the results\\nto an index file). Because apps in Kubernetes  run in containers by default, these files\\nare written to the container’s filesystem. If the container is then restarted, they’re all\\nlost, because the new container starts off with  a completely new writable layer (see fig-\\nure 17.2).\\n Don’t forget that individual containers may be restarted for several reasons, such\\nas because the process crashes, because the liveness probe returned a failure, or\\nbecause the node started running out of me mory and the process was killed by the\\nOOMKiller. When this happens, the pod is st ill the same, but the container itself is\\ncompletely new. The Kubelet doesn’t run the same container again; it always creates a\\nnew container. \\nUSING VOLUMES  TO PRESERVE  DATA ACROSS  CONTAINER  RESTARTS\\nWhen its container is restarted, the app in the example will need to perform the\\nintensive startup procedure again. This may or may not be desired. To make sure data\\nlike this isn’t lost, you need to use at le ast a pod-scoped volume. Because volumes live\\nand die together with the pod, the new contai ner will be able to reuse the data written\\nto the volume by the previous container (figure 17.3).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 512}, page_content='481 Understanding the pod’s lifecycle\\nContainer\\nProcess\\nWrites toFilesystem\\nWritable layer\\nRead-only layer\\nRead-only layer\\nImage layersContainer crashes\\nor is killed\\nPodNew container\\nNew process\\nFilesystem\\nNew writable layer\\nRead-only layer\\nRead-only layer\\nImage layersNew container started\\n(part of the same pod)\\nNew containerstarts with newwriteable layer:all ﬁles are lost\\nFigure 17.2 Files written to the container’s file system are lost when the container is restarted.\\nContainer\\nProcess\\nWrites toCan read\\nthe same ﬁles Filesystem\\nvolumeMountContainer crashes\\nor is killed\\nPodNew container\\nNew process\\nFilesystem\\nvolumeMountNew container started\\n(part of the same pod)\\nNew process canuse data preservedin the volumeVolume\\nFigure 17.3 Using a volume to persist data across container restarts'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 513}, page_content='482 CHAPTER  17 Best practices for developing apps\\nUsing a volume to preserve files across container restarts is a great idea sometimes,\\nbut not always. What if the data gets corr upted and causes the newly created process\\nto crash again? This will result in a cont i n u o u s  c r a s h  l o o p  ( t h e  p o d  w i l l  s h o w  t h e\\nCrashLoopBackOff  status). If you hadn’t used a volume, the new container would start\\nfrom scratch and most likely not crash. Using volumes to preserve files across con-\\ntainer restarts like this is a double-edged  sword. You need to think carefully about\\nwhether to use them or not.\\n17.2.2 Rescheduling of dead or partially dead pods\\nIf a pod’s container keeps crashing, the Kube let will keep restarting it indefinitely.\\nThe time between restarts will be increased exponentially until it reaches five minutes.\\nDuring those five minute intervals, the pod is essentially dead, because its container’s\\nprocess isn’t running. To be fair, if it’s a multi-container pod, certain containers may\\nbe running normally, so the pod is only partially dead. But if a pod contains only a sin-gle container, the pod is effectively dead an d completely useless, because no process is\\nrunning in it anymore.\\n You may find it surprising to learn that  such pods aren’t automatically removed\\nand rescheduled, even if they’re part of a Re plicaSet or similar controller. If you cre-\\nate a ReplicaSet with a desired replica coun t of three, and then one of the containers\\nin one of those pods starts crashing, Kubernetes will not delete and replace the pod.\\nThe end result is a ReplicaSet with only tw o properly running replicas instead of the\\ndesired three (figure 17.4).\\nYou’d probably expect the pod to be deleted and replaced with another pod instance\\nthat might run successfully on another node . After all, the container may be crashing\\nbecause of a node-related problem that does n’t manifest itself on other nodes. Sadly,\\nthat isn’t the case. The ReplicaSet controlle r doesn’t care if the pods are dead—all itReplicaSet\\nDesired replicas: 3\\nActual replicas: 3Only two pods are actually\\nperforming their jobsThird pod’s status is Running,\\nbut its container keeps crashing,with signiﬁcant delays betweenrestarts (CrashLoopBackOff)\\nWe wantthree pods\\nPodRunning\\ncontainer\\nPodRunning\\ncontainer\\nPodDead\\ncontainer\\nFigure 17.4 A ReplicaSet controller doesn’t reschedule dead pods.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 514}, page_content='483 Understanding the pod’s lifecycle\\ncares about is that the number of pods matches the desired replica count, which in\\nthis case, it does.\\n If you’d like to see for yourself, I’ve included a YAML manifest for a ReplicaSet\\nwhose pods will keep crashing (see file replicaset-crashingpods.yaml in the code\\narchive). If you create the ReplicaSet and inspect the pods that are created, the follow-\\ning listing is what you’ll see.\\n$ kubectl get po\\nNAME                  READY     STATUS             RESTARTS   AGE\\ncrashing-pods-f1tcd   0/1       CrashLoopBackOff    5          6m     \\ncrashing-pods-k7l6k   0/1       CrashLoopBackOff   5          6m\\ncrashing-pods-z7l3v   0/1       CrashLoopBackOff   5          6m\\n$ kubectl describe rs crashing-pods\\nName:           crashing-pods\\nReplicas:       3 current / 3 desired                       \\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed      \\n$ kubectl describe po crashing-pods-f1tcd\\nName:           crashing-pods-f1tcd\\nNamespace:      defaultNode:           minikube/192.168.99.102\\nStart Time:     Thu, 02 Mar 2017 14:02:23 +0100\\nLabels:         app=crashing-podsStatus:         Running                      \\nIn a way, it’s understandable that Kubernet es behaves this way. The container will be\\nrestarted every five minutes in the hope th at the underlying cause of the crash will be\\nresolved. The rationale is that reschedu ling the pod to another node most likely\\nwouldn’t fix the problem anyway, because th e app is running inside a container and\\nall the nodes should be mostly equivalent. Th at’s not always the case, but it is most of\\nthe time. \\n17.2.3 Starting pods in a specific order\\nOne other difference between apps runnin g in pods and those managed manually is\\nthat the ops person deploying those apps  knows about the dependencies between\\nthem. This allows them to start the apps in order. \\nUNDERSTANDING  HOW PODS ARE STARTED\\nWhen you use Kubernetes to run your multi-pod applications, you don’t have a built-\\nin way to tell Kubernetes to run certain pods  first and the rest only when the first pods\\nare already up and ready to serve. Sure, you could post the manifest for the first app\\nand then wait for the pod(s) to be ready be fore you post the second manifest, but yourListing 17.1 ReplicaSet and pods that keep crashing\\nThe pod’s status shows the Kubelet is\\ndelaying the restart because the\\ncontainer keeps crashing.\\nNo action taken \\nby the controller, \\nbecause current replicas match \\ndesired replicas\\nThree \\nreplicas are \\nshown as \\nrunning.\\nkubectl describe \\nalso shows pod’s status as running'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 515}, page_content='484 CHAPTER  17 Best practices for developing apps\\nwhole system is usually defined in a single YAML or JSON containing multiple Pods,\\nServices, and other objects. \\n The Kubernetes API server does proces s the objects in the YAML/JSON in the\\norder they’re listed, but this only means they ’re written to etcd in that order. You have\\nno guarantee that pods will also  be started in that order. \\n But you can prevent a pod’s main container from starting until a precondition is\\nmet. This is done by including an init containers in the pod. \\nINTRODUCING  INIT CONTAINERS\\nIn addition to regular containers, pods ca n also include init containers. As the name\\nsuggests, they can be used to initialize the pod—this often means writing data to thepod’s volumes, which are then mounted into the pod’s main container(s).\\n A pod may have any number of init cont ainers. They’re executed sequentially and\\nonly after the last one completes are the po d’s main containers started. This means\\ninit containers can also be used to delay the start of the pod’s main container(s)—for\\nexample, until a certain precondition is met.  An init container could wait for a service\\nrequired by the pod’s main container to be up and ready. When it is, the init containerterminates and allows the main container(s)  to be started. This way, the main con-\\ntainer wouldn’t use the service before it’s ready.\\n Let’s look at an example of a pod using an  init container to delay the start of the\\nmain container. Remember the \\nfortune  pod you created in chapter 7? It’s a web\\nserver that returns a fortune quote as a resp onse to client requests. Now, let’s imagine\\nyou have a fortune-client  pod that requires the fortune  Service to be up and run-\\nning before its main container starts. You can add an init container, which checks\\nwhether the Service is responding to requests . Until that’s the case, the init container\\nkeeps retrying. Once it gets a response, the init container terminates and lets the main\\ncontainer start.\\nADDING  AN INIT CONTAINER  TO A POD\\nInit containers can be defined in the pod spec like main containers but through the\\nspec.initContainers  field. You’ll find the complete YAML for the fortune-client pod\\nin the book’s code archive. The following listing shows the part where the init con-tainer is defined.\\nspec:\\n  initContainers:      \\n  - name: init\\n    image: busybox    command:\\n    - sh\\n    - -c    - \\'while true; do echo \"Waiting for fortune service to come up...\";  \\n    \\n➥ wget http://fortune -q -T 1 -O /dev/null >/dev/null 2>/dev/null   \\n    ➥ && break; sleep 1; done; echo \"Service is up! Starting main       \\n    ➥ container.\"\\'Listing 17.2 An init container defi ned in a pod: fortune-client.yaml\\nYou’re defining \\nan init container, \\nnot a regular \\ncontainer.The init container runs a\\nloop that runs until the\\nfortune Service is up.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 516}, page_content='485 Understanding the pod’s lifecycle\\nWhen you deploy this pod, only its init container is started. This is shown in the pod’s\\nstatus when you list pods with kubectl  get:\\n$ kubectl get po\\nNAME             READY     STATUS     RESTARTS   AGE\\nfortune-client   0/1       Init:0/1    0          1m\\nThe STATUS  column shows that zero of one init containers have finished. You can see\\nthe log of the init container with kubectl  logs :\\n$ kubectl logs fortune-client -c init\\nWaiting for fortune service to come up...\\nWhen running the kubectl  logs  command, you need to specify the name of the init\\ncontainer with the -c switch (in the example, the name  of the pod’s init container is\\ninit , as you can see in listing 17.2).\\n The main container won’t run until you deploy the fortune  Service and the\\nfortune-server  pod. You’ll find them in the fortune-server.yaml file. \\nBEST PRACTICES  FOR HANDLING  INTER-POD DEPENDENCIES\\nYou’ve seen how an init container can be used to delay starting the pod’s main con-\\ntainer(s) until a precondition is met (mak ing sure the Service the pod depends on is\\nready, for example), but it’s much better to  write apps that don’t require every service\\nthey rely on to be ready before the app starts up. After all, the service may also go\\noffline later, while the app is already running.\\n The application needs to handle internal ly the possibility that its dependencies\\naren’t ready. And don’t forget readiness probe s. If an app can’t do its job because one\\nof its dependencies is missing, it should signal that through its readiness probe, so\\nKubernetes knows it, too, isn’t ready. You’ll want to do this not only because it pre-vents the app from being added as a service endpoint, but also because the app’s read-\\niness is also used by the Deployment co ntroller when performing a rolling update,\\nthereby preventing a roll out of a bad version. \\n17.2.4 Adding lifecycle hooks\\nWe’ve talked about how init containers can be used to hook into the startup of the\\npod, but pods also allow you to define two lifecycle hooks:\\n\\uf0a1Post-start  hooks\\n\\uf0a1Pre-stop  hooks\\nThese lifecycle hooks are specified per contai ner, unlike init containers, which apply\\nto the whole pod. As their names suggest, they’re executed when the container starts\\nand before it stops. \\n Lifecycle hooks are similar to liveness and readiness probes in that they can either\\n\\uf0a1Execute a command inside the container\\n\\uf0a1Perform an HTTP GET request against a URL'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 517}, page_content='486 CHAPTER  17 Best practices for developing apps\\nLet’s look at the two hooks individually to see what effect they have on the container\\nlifecycle.\\nUSING A POST-START  CONTAINER  LIFECYCLE  HOOK\\nA post-start hook is executed immediately af ter the container’s main process is started.\\nYou use it to perform additional operations when the application starts. Sure, if you’re\\nthe author of the application running in th e container, you can always perform those\\noperations inside the applicat ion code itself. But when you’re running an application\\ndeveloped by someone else, you mostly don’ t want to (or can’t) modify its source\\ncode. Post-start hooks allow you to run addi tional commands without having to touch\\nthe app. These may signal to an external lis tener that the app is starting, or they may\\ninitialize the application so it can start doing its job.\\n The hook is run in parallel with the main process. The name might be somewhat\\nmisleading, because it doesn’t wait for the ma in process to start up fully (if the process\\nhas an initialization procedure, the Kubele t obviously can’t wait for the procedure to\\ncomplete, because it has no way of knowing when that is). \\n But even though the hook runs asynchrono usly, it does affect the container in two\\nways. Until the hook completes, the container will stay in the Waiting  state with the\\nreason ContainerCreating . Because of this, the pod’s status will be Pending  instead of\\nRunning . If the hook fails to run or returns a non-zero exit code, the main container\\nwill be killed. \\n A pod manifest containing a post-start  hook looks like the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-poststart-hook\\nspec:  containers:\\n  - image: luksa/kubia\\n    name: kubia    lifecycle:          \\n      postStart:        \\n        exec:                                                               \\n          command:                                                          \\n          - sh                                                              \\n          - -c                                                              \\n          - \"echo \\'hook will f ail with exit code 15\\'; sleep 5; exit 15\"     \\nIn the example, the echo , sleep , and exit  commands are executed along with the\\ncontainer’s main process as soon as the co ntainer is created. Rather than run a com-\\nmand like this, you’d typically run a shell scri pt or a binary executable file stored in\\nthe container image. \\n Sadly, if the process started by the hook logs to the standard output, you can’t see\\nthe output anywhere. This makes debugging lifecycle hooks painful. If the hook fails,Listing 17.3 A pod with a post-start  lifecycle hook: post-start-hook.yaml\\nThe hook is executed as \\nthe container starts.\\nIt executes the\\npostStart.sh\\nscript in the /bin\\ndirectory inside\\nthe container.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 518}, page_content='487 Understanding the pod’s lifecycle\\nyou’ll only see a FailedPostStartHook  warning among the pod’s events (you can see\\nthem using kubectl  describe  pod). A while later, you’ll see more information on why\\nthe hook failed, as shown in the following listing.\\nFailedSync    Error syncing pod, skipping: failed to \"StartContainer\" for \\n             \"kubi a\" with PostStart handler: command \\'sh -c echo \\'hook \\n             will fail with exit code 15\\'; sleep 5 ; exit 15\\' exited \\n             with 15: : \"PostStart Hook Failed\" \\nThe number 15 in the last line is the exit code of the command. When using an HTTP\\nGET hook handler, the reason may look like  the following listing (you can try this by\\ndeploying the post-start-hook-httpget.yaml  file from the book’s code archive).\\nFailedSync    Error syncing pod, skipping: failed to \"StartContainer\" for \\n             \"kubia\" with PostStart handler: Get \\n             http://10.32.0.2:9090/postStart: dial tcp 10.32.0.2:9090: \\n             getsockopt: connection refused : \"PostStart Hook Failed\" \\nNOTE The post-start hook is intentiona lly misconfigured to use port 9090\\ninstead of the correct port 8080, to sh ow what happens when the hook fails.\\nThe standard and error outputs of command- based post-start hooks aren’t logged any-\\nwhere, so you may want to have the process the hook invokes log to a file in the con-\\ntainer’s filesystem, which w ill allow you to examine the contents of the file with\\nsomething like this:\\n$ kubectl exec my-pod cat logfile.txt \\nIf the container gets restarted for whatever reason (including because the hook failed),\\nthe file may be gone before  you can examine it. You can work around that by mount-\\ning an emptyDir  volume into the container and having the hook write to it.\\nUSING A PRE-STOP CONTAINER  LIFECYCLE  HOOK\\nA pre-stop hook is executed immediately before a container is terminated. When a\\ncontainer needs to be terminated, the Kubele t will run the pre-stop hook, if config-\\nured, and only then send a SIGTERM  to the process (and late r kill the process if it\\ndoesn’t terminate gracefully). \\n A pre-stop hook can be used to initiate a graceful shutdown of the container, if it\\ndoesn’t shut down gracefully upon receipt of a SIGTERM  signal. They can also be used\\nto perform arbitrary operations before shutdown without having to implement those\\noperations in the application itself (this is  useful when you’re running a third-party\\napp, whose source code you don’t ha ve access to and/or can’t modify). \\n Configuring a pre-stop hook in a pod mani fest isn’t very different from adding a\\npost-start hook. The previous example showed  a post-start hook that executes a com-Listing 17.4 Pod’s events showing the exit code of the failed command-based hook\\nListing 17.5 Pod’s events showing the reason why an HTTP GET hook failed'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 519}, page_content='488 CHAPTER  17 Best practices for developing apps\\nmand, so we’ll look at a pre-stop hook th at performs an HTTP GET request now. The\\nfollowing listing shows how to define a pre-stop HTTP GET hook in a pod.\\n    lifecycle:\\n      preStop:            \\n        httpGet:          \\n          port: 8080          \\n          path: shutdown      \\nThe pre-stop hook defined in this list ing performs an HTTP GET request to http:/ /\\nPOD_IP:8080/shutdown  as soon as the Kubelet starts terminating the container.\\nApart from the port  and path  shown in the listing, you can also set the fields scheme\\n(HTTP or HTTPS) and host , as well as httpHeaders  that should be sent in the\\nrequest. The host  field defaults to the pod IP. Be sure not to set it to localhost,\\nbecause localhost would refer to the node, not the pod.\\n In contrast to the post-start hook, the container will be terminated regardless of\\nthe result of the hook—an error HTTP resp onse code or a non-zero exit code when\\nusing a command-based hook will not prev ent the container from being terminated.\\nIf the pre-stop hook fails, you’ll see a FailedPreStopHook  warning event among the\\npod’s events, but because the pod is deleted soon afterward (after all, the pod’s dele-\\ntion is what triggered the pre-stop hook in the first place), you may not even notice\\nthat the pre-stop hook failed to run properly. \\nTIP If the successful completion of the pr e-stop hook is critical to the proper\\noperation of your system, verify whether i t’s  b ei n g  ex ec u te d a t a l l . I ’v e  w it -\\nnessed situations where the pre-stop hook didn’t run and the developerwasn’t even aware of that.\\nUSING A PRE-STOP HOOK BECAUSE  YOUR APP DOESN ’T RECEIVE  THE SIGTERM SIGNAL\\nMany developers make the mistake of defi ning a pre-stop hook solely to send a SIGTERM\\nsignal to their apps in the pr e-stop hook. They do this because they don’t see their appli-\\ncation receive the SIGTERM  signal sent by the Kubelet. The reason why the signal isn’t\\nreceived by the application isn’t because Kube rnetes isn’t sending it, but because the sig-\\nnal isn’t being passed to the app process inside the container itself. If your container\\nimage is configured to run a shell, which in turn runs the app process, the signal may be\\neaten up by the shell itself, instead of being passed down to the child process.\\n In such cases, instead of adding a pre-stop  hook to send the signal directly to your\\napp, the proper fix is to make sure the sh ell passes the signal to the app. This can be\\nachieved by handling the signal in the she ll script running as the main container pro-\\ncess and then passing it on to the app. Or  you could not configure the container image\\nto run a shell at all and instead run the applic ation binary directly. You do this by using\\nthe exec form of ENTRYPOINT  or CMD in the Dockerfile: ENTRYPOINT  [\"/mybinary\"]\\ninstead of ENTRYPOINT  /mybinary .Listing 17.6 A pre-stop hook YAML snippet: pre-stop-hook-httpget.yaml\\nThis is a pre-stop hook that \\nperforms an HTTP GET request.\\nThe request is sent to \\nhttp://POD_IP:8080/shutdown .'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 520}, page_content='489 Understanding the pod’s lifecycle\\n A container using the first form runs the mybinary  executable as its main process,\\nwhereas the second form runs a shell as the main process with the mybinary  process\\nexecuted as a child of the shell process.\\nUNDERSTANDING  THAT LIFECYCLE  HOOKS  TARGET  CONTAINERS , NOT PODS\\nAs a final thought on post-start and pre-stop hooks, let me emphasize that these lifecy-cle hooks relate to containers, not pods. You shouldn’t use a pre-stop hook for run-\\nning actions that need to be performed wh en the pod is terminating. The reason is\\nthat the pre-stop hook gets called when the container is being terminated (most likely\\nbecause of a failed liveness probe). This may happen multiple times in the pod’s life-\\ntime, not only when the pod is in the process of being shut down. \\n17.2.5 Understanding pod shutdown\\nWe’ve touched on the subject of pod termination, so let’s explore this subject in moredetail and go over exactly what happens during pod shutdown. This is important for\\nunderstanding how to cleanly shut down an application running in a pod.\\n Let’s start at the beginning. A pod’s shut-d own is triggered by the deletion of the\\nPod object through the API server. Upon receiving an HTTP DELETE request, the\\nAPI server doesn’t delete the object yet, but only sets a \\ndeletionTimestamp  field in it.\\nPods that have the deletionTimestamp  field set are terminating. \\n Once the Kubelet notices th e pod needs to be terminat ed, it starts terminating\\neach of the pod’s containers. It gives each container time to shut down gracefully, but\\nthe time is limited. That time is called the termination grace period and is configu-\\nrable per pod. The timer starts as soon as the termination process starts. Then the fol-\\nlowing sequence of events is performed:\\n1Run the pre-stop hook, if one is conf igured, and wait for it to finish.\\n2Send the SIGTERM  signal to the main process of the container.\\n3Wait until the container shuts down cleanly or until the termination grace\\nperiod runs out.\\n4Forcibly kill the process with SIGKILL , if it hasn’t terminated gracefully yet.\\nThe sequence of events is illustrated in figure 17.5.\\nPre-stop hook processTermination grace period\\nMain container process\\nContainer shutdown\\ninitiatedContainer killed\\nif still runningTimeSIGTERM SIGKILL\\nFigure 17.5 The container termination sequence'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 521}, page_content='490 CHAPTER  17 Best practices for developing apps\\nSPECIFYING  THE TERMINATION  GRACE  PERIOD\\nThe termination grace period can be configured in the pod spec by setting the spec.\\nterminationGra cePeriodSeconds  field. It defaults to 30, which means the pod’s con-\\ntainers will be given 30 seconds to terminate gracefully before they’re killed forcibly. \\nTIP You should set the grace period to lo ng enough so your process can fin-\\nish cleaning up in that time. \\nThe grace period specified in the pod spec can also be overridden when deleting the\\npod like this:\\n$ kubectl delete po mypod --grace-period=5\\nThis will make the Kubelet wait five second s for the pod to shut down cleanly. When\\nall the pod’s containers stop, the Kubelet no tifies the API server and the Pod resource\\nis finally deleted. You can force the API se rver to delete the resource immediately,\\nwithout waiting for confirmation, by setting the grace period to zero and adding the\\n--force  option like this:\\n$ kubectl delete po mypod --grace-period=0 --force\\nBe careful when using this option, especially  with pods of a StatefulSet. The Stateful-\\nSet controller takes great care to never run two instances of the same pod at the same\\ntime (two pods with the same ordinal index and name and attached to the same\\nPersistentVolume). By force-deleting a pod, you’ll cause the controller to create areplacement pod without waiting for the containers of the deleted pod to shut\\ndown. In other words, two instances of th e same pod might be running at the same\\ntime, which may cause your stateful cluster to malfunction. Only delete stateful pods\\nforcibly when you’re absolutely sure the pod isn’t running anymore or can’t talk to\\nthe other members of the cluster (you can be  sure of this when you confirm that the\\nnode that hosted the pod has failed or ha s been disconnected from the network and\\ncan’t reconnect). \\n Now that you understand how containers ar e shut down, let’s look at it from the\\napplication’s perspective and go over how applications should handle the shutdown\\nprocedure.\\nIMPLEMENTING  THE PROPER  SHUTDOWN  HANDLER  IN YOUR APPLICATION\\nApplications should react to a SIGTERM  signal by starting their shut-down procedure\\nand terminating when it finishes. Instead of handling the SIGTERM  signal, the applica-\\ntion can be notified to shut down through a pre-stop hook. In both cases, the app\\nthen only has a fixed amount of time to terminate cleanly. \\n But what if you can’t predict how long the app will take to shut down cleanly? For\\nexample, imagine your app is a distributed data store. On scale-down, one of the pod\\ninstances will be deleted and therefore shut down. In the shut-down procedure, the'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 522}, page_content='491 Understanding the pod’s lifecycle\\npod needs to migrate all its data to the re maining pods to make sure it’s not lost.\\nShould the pod start migrating the data upo n receiving a termination signal (through\\neither the SIGTERM  signal or through a pre-stop hook)? \\n Absolutely not! This is not recommende d for at least the following two reasons:\\n\\uf0a1A container terminating doesn’t necessarily mean the whole pod is being\\nterminated.\\n\\uf0a1You have no guarantee the shut-down proc edure will finish before the process\\nis killed.\\nThis second scenario doesn’t happen only wh en the grace period runs out before the\\napplication has finished shutting down gr acefully, but also when the node running\\nthe pod fails in the middle of the contai ner shut-down sequence. Even if the node\\nthen starts up again, the Kubelet will no t restart the shut-down procedure (it won’t\\neven start up the container again). There are absolutely no guarantees that the pod\\nwill be allowed to complete its whole shut-down procedure.\\nREPLACING  CRITICAL  SHUT-DOWN  PROCEDURES  WITH DEDICATED  SHUT-DOWN  PROCEDURE  PODS\\nHow do you ensure that a critical shut-down procedure that absolutely must run to\\ncompletion does run to completion (for example, to ensure that a pod’s data is\\nmigrated to other pods)?\\n One solution is for the app (upon receipt of a termination signal) to create a new\\nJob resource that would run a new pod, whos e sole job is to migrate the deleted pod’s\\ndata to the remaining pods. But if you’ve be en paying attention, you’ll know that you\\nhave no guarantee the app will indeed mana ge to create the Job object every single\\ntime. What if the node fails exactl y when the app tries to do that? \\n The proper way to handle this problem is  by having a dedicated, constantly run-\\nning pod that keeps checking for the existe nce of orphaned data. When this pod finds\\nthe orphaned data, it can migrate it to th e remaining pods. Rather than a constantly\\nrunning pod, you can also use a CronJob resource and run the pod periodically. \\n You may think StatefulSets could help here, but they don’t. As you’ll remember,\\nscaling down a StatefulSet leaves Persiste ntVolumeClaims orphaned, leaving the data\\nstored on the PersistentVolume stranded. Yes,  upon a subsequent scale-up, the Persistent-\\nVolume will be reattached to the new pod instance, but what if that scale-up never\\nhappens (or happens after a long time)? For this reason, you may want to run a\\ndata-migrating pod also when using StatefulSe ts (this scenario is shown in figure 17.6).\\nTo prevent the migration from occurring during an application upgrade, the data-\\nmigrating pod could be configured to wait a while to give the stateful pod time to\\ncome up again before pe rforming the migration.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 523}, page_content='492 CHAPTER  17 Best practices for developing apps\\n17.3 Ensuring all client requests are handled properly\\nYou now have a good sense of how to make po ds shut down cleanly. Now, we’ll look at\\nthe pod’s lifecycle from the perspective of the pod’s clients (clients consuming the ser-\\nvice the pod is providing). This is importan t to understand if you don’t want clients to\\nrun into problems when you scale pods up or down.\\n It goes without saying that you want all client requests to be handled properly. You\\nobviously don’t want to see broken connections when pods  are starting up or shutting\\ndown. By itself, Kubernetes doesn’t preven t this from happening. Your app needs to\\nfollow a few rules to prevent broken connectio ns. First, let’s focus on making sure all\\nconnections are handled proper ly when the pod starts up.\\n17.3.1 Preventing broken client connections when a pod is starting up\\nEnsuring each connection is handled properly at pod startup is simple if you under-\\nstand how Services and service Endpoints work . When a pod is started, it’s added as an\\nendpoint to all the Services, whose label se lector matches the pod’s labels. As you may\\nremember from chapter 5, the pod also needs to signal to Kubernetes that it’s ready.\\nUntil it is, it won’t become a service endpo int and therefore won’t receive any requests\\nfrom clients. \\n If you don’t specify a readiness probe in your pod spec, the pod is always considered\\nready. It will start receiving requests almost  immediately—as soon as the first kube-proxy\\nupdates the iptables  rules on its node and the first c lient pod tries to connect to the\\nservice. If your app isn’t ready to accept connections by then, clients will see “connec-\\ntion refused” types of errors.\\n All you need to do is make sure that your readiness probe returns success only\\nwhen your app is ready to properly handle incoming requests. A good first step is to\\nadd an HTTP GET readiness probe and point it to the base URL of your app. In manyPod\\nA-0Pod\\nA-1\\nStatefulSet A\\nReplicas: 2Scale\\ndownPVC\\nA-0PV\\nPVC\\nA-1PV\\nPod\\nA-0\\nStatefulSet A\\nReplicas: 1Transfers data to\\nremaining pod(s)Connects to\\norphaned PVC\\nData-migrating\\nPod\\nJobPVC\\nA-0PV\\nPVC\\nA-1PV\\nFigure 17.6 Using a dedicated pod to migrate data'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 524}, page_content='493 Ensuring all client requests are handled properly\\ncases that gets you far enough and saves yo u from having to implement a special read-\\niness endpoint in your app. \\n17.3.2 Preventing broken connections during pod shut-down\\nNow let’s see what happens at the other end of a pod’s life—when the pod is deleted and\\nits containers are terminated. We’ve alre ady talked about how the pod’s containers\\nshould start shutting down cleanly as soon they receive the SIGTERM  signal (or when its\\npre-stop hook is executed). But does that en sure all client requests are handled properly? \\n How should the app behave when it rece ives a termination signal? Should it con-\\ntinue to accept requests? What about reques ts that have already been received but\\nhaven’t completed yet? What about persis tent HTTP connections, which may be in\\nbetween requests, but are open (when no active request exists on the connection)?\\nBefore we can answer those questions, we ne ed to take a detailed look at the chain of\\nevents that unfolds across the cl uster when a Pod is deleted. \\nUNDERSTANDING  THE SEQUENCE  OF EVENTS  OCCURRING  AT POD DELETION\\nIn chapter 11 we took an in-depth look at what components make up a Kubernetes clus-\\nter. You need to always keep in mind that those components run as separate processes on\\nmultiple machines. They aren’t all part of a single big monolithic process. It takes time\\nfor all the components to be on the same pa ge regarding the state of the cluster. Let’s\\nexplore this fact by looking at what happe ns across the cluster when a Pod is deleted.\\n When a request for a pod deletion is received by the API server, it first modifies the\\nstate in etcd and then notifi es its watchers of the deletion. Among those watchers are\\nthe Kubelet and the Endpoints controller. The two sequences of events, which happen\\nin parallel (marked with either A or B), are shown in figure 17.7.\\nA2. Stop\\ncontainers\\nAPI server kube-proxyKubeletWorker node\\nEndpoints\\ncontrollerkube-proxyPod\\n(containers)\\nClientDelete\\npod\\nB1. Pod deletion\\nnotiﬁcationB2. Remove pod\\nas endpointA1. Pod deletion\\nnotiﬁcation\\nB3. Endpoint\\nmodiﬁcationnotiﬁcationB4. Remove pod\\nfromiptables\\nB4. Remove pod\\nfromiptables\\niptablesiptables\\nWorker node\\nFigure 17.7 Sequence of events that occurs when a Pod is deleted'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 525}, page_content='494 CHAPTER  17 Best practices for developing apps\\nIn the A sequence of events, you’ll see that as  soon as the Kubelet receives the notifica-\\ntion that the pod should be terminated, it initiates the shutdown sequence as explained\\nin section 17.2.5 (run the pre-stop hook, send SIGTERM , wait for a period of time, and\\nthen forcibly kill the container if it hasn ’t yet terminated on its own). If the app\\nresponds to the SIGTERM  by immediately ceasing to receive client requests, any client\\ntrying to connect to it will receive a Conne ction Refused error. Th e time it takes for\\nthis to happen from the time the pod is dele ted is relatively short because of the direct\\npath from the API server to the Kubelet.\\n Now, let’s look at what happens in the other sequence of events—the one leading\\nup to the pod being removed from the iptables  rules (sequence B in the figure).\\nWhen the Endpoints controller (which runs  in the Controller Manager in the Kuber-\\nnetes Control Plane) receives the notificati on of the Pod being deleted, it removes\\nthe pod as an endpoint in all services that th e pod is a part of. It does this by modify-\\ning the Endpoints API object by sending a REST request to the API server. The API\\nserver then notifies all clients watching  the Endpoints object. Among those watchers\\nare all the kube-proxies running on the wo rker nodes. Each of these proxies then\\nupdates the iptables  rules on its node, which is what prevents new connections\\nfrom being forwarded to the terminating pod. An important detail here is that\\nremoving the iptables  rules has no effect on existing connections—clients who are\\nalready connected to the pod will still send additional requests to the pod through\\nthose existing connections.\\n Both of these sequences of events happen in  parallel. Most likely, the time it takes\\nto shut down the app’s process in the pod is slightly shorter than the time required for\\nthe iptables  rules to be updated. The chai n of events that leads to iptables  rules\\nbeing updated is considerably longer (see figure 17.8), because the event must first\\nreach the Endpoints controller, which then sends a new request to the API server, and\\nA2. Send\\nSIGTERM\\nAPI server\\nAPI serverKubelet\\nEndpoints\\ncontrollerContainer(s)A1. Watch\\nnotiﬁcation(pod modiﬁed)\\nB1. Watch\\nnotiﬁcation(pod modiﬁed)B2. Remove pod’s IP\\nfrom endpointskube-proxy\\nB4. Update\\niptables\\nrulesiptables\\nkube-proxy iptables\\nTimeB3. Watch notiﬁcation\\n(endpoints changed)\\nFigure 17.8 Timeline of events when pod is deleted'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 526}, page_content='495 Ensuring all client requests are handled properly\\nthen the API server must notify the kube-p roxy before the proxy finally modifies the\\niptables  rules. A high probability exists that the SIGTERM  signal will be sent well\\nbefore the iptables  rules are updated on all nodes.\\n The end result is that the pod may still re ceive client requests after it was sent the\\ntermination signal. If the app closes the se rver socket and stops accepting connections\\nimmediately, this will cause clients to re ceive “Connection Refused” types of errors\\n(similar to what happens at pod startup if  your app isn’t capable of accepting connec-\\ntions immediately and you don’t define a readiness probe for it). \\nSOLVING  THE PROBLEM\\nGoogling solutions to this problem makes it seem as though adding a readiness probe\\nto your pod will solve the problem. Supposedly, all you need to do is make the readi-\\nness probe start failing as soon as the pod receives the SIGTERM . This is supposed to\\ncause the pod to be removed as the endpoi nt of the service. But the removal would\\nhappen only after the readiness probe fails fo r a few consecutive times (this is configu-\\nrable in the readiness probe spec). And, obviously, the removal then still needs to\\nreach the kube-proxy before the pod is removed from iptables  rules. \\n In reality, the readiness probe has absolutely no bearing on the whole process at\\nall. The Endpoints controller removes the po d from the service Endpoints as soon as\\nit receives notice of the pod being deleted (when the deletionTimestamp  field in the\\npod’s spec is no longer null ). From that point on, the re sult of the readiness probe\\nis irrelevant.\\n What’s the proper solution to the pr oblem? How can you make sure all requests\\nare handled fully?\\n It’s clear the pod needs to keep accepting connections even after it receives the ter-\\nmination signal up until all the kube-p roxies have finished updating the iptables\\nrules. Well, it’s not only the kube-proxies. There may also be Ingress controllers or\\nload balancers forwarding connections to the pod directly, without going through the\\nService (iptables ). This also includes clients using client-side load-balancing. To\\nensure none of the clients experience broken  connections, you’d have to wait until all\\nof them somehow notify you they’ll no longer forward connections to the pod. \\n That’s impossible, because all those co mponents are distributed across many dif-\\nferent computers. Even if you knew the lo cation of every one of them and could wait\\nuntil all of them say it’s okay to shut down the pod, what do you do if one of themdoesn’t respond? How long do you wait  for the response? Remember, during that\\ntime, you’re holding up the shut-down process. \\n The only reasonable thing you can do is wait for a long-enough time to ensure all\\nthe proxies have done their job. But how long is long enough? A few seconds should\\nbe enough in most situations, but there’s no guarantee it will suffice every time. When\\nthe API server or the Endpoints controller is overloaded, it may take longer for the\\nnotification to reach the kube-proxy. It’s important to understand that you can’t solve\\nthe problem perfectly, but even adding a 5- or 10-second delay should improve theuser experience considerably. You can use a longer delay, but don’t go overboard,'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 527}, page_content='496 CHAPTER  17 Best practices for developing apps\\nbecause the delay will prevent the containe r from shutting down promptly and will\\ncause the pod to be shown in lists long after it has been deleted, which is always frus-\\ntrating to the user deleting the pod.\\nWRAPPING  UP THIS SECTION\\nTo recap—properly shutting down an application includes these steps:\\n\\uf0a1Wait for a few seconds, then stop accepting new connections. \\n\\uf0a1Close all keep-alive connections not in the middle of a request.\\n\\uf0a1Wait for all active requests to finish.\\n\\uf0a1Then shut down completely.\\nTo understand what’s happening with the connections and requests during this pro-\\ncess, examine figure 17.9 carefully.\\nNot as simple as exiting the process imme diately upon receiving the termination sig-\\nnal, right? Is it worth going through all this ? That’s for you to decide. But the least you\\ncan do is add a pre-stop hook that waits a few seconds, like the one in the following\\nlisting, perhaps.\\n    lifecycle:                    \\n      preStop:                    \\n        exec:                               command:                \\n          - sh\\n          - -c          - \"sleep 5\"Listing 17.7 A pre-stop hook for preventing broken connectionsDelay (few seconds)\\nKey:\\nConnection\\nRequestiptables rules\\nupdated on all nodes\\n(no new connections\\nafter this point)Stop\\naccepting new\\nconnectionsClose inactive\\nkeep-alive\\nconnections\\nand wait for\\nactive requests\\nto ﬁnishWhen last\\nactive request\\ncompletes,\\nshut down\\ncompletelyTime\\nSIGTERM\\nFigure 17.9 Properly handling existing and new connections after receiving a termination signal'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 528}, page_content='497 Making your apps easy to run and manage in Kubernetes\\nThis way, you don’t need to modify the code  of your app at all. If your app already\\nensures all in-flight requests are processed completely, this pre-stop delay may be all\\nyou need.\\n17.4 Making your apps easy to run and manage in Kubernetes\\nI hope you now have a better sense of how to make your apps handle clients nicely.\\nNow we’ll look at other aspects of how an app should be built to make it easier to man-\\nage in Kubernetes.\\n17.4.1 Making manageable container images\\nWhen you package your app into an imag e, you can choose to include the app’s\\nbinary executable and any additional librari es it needs, or you can package up a whole\\nOS filesystem along with the app. Way too ma ny people do this, even though it’s usu-\\nally unnecessary.\\n Do you need every single file from an OS distribution in your image? Probably not.\\nMost of the files will never be used and wi ll make your image larger than it needs to\\nbe. Sure, the layering of images makes sure  each individual layer is downloaded only\\nonce, but even having to wait longer than  necessary the first time a pod is scheduled\\nto a node is undesirable.\\n Deploying new pods and scaling them shou ld be fast. This demands having small\\nimages without unnecessary cruft. If you’re  building apps using the Go language, your\\nimages don’t need to include anything else apart from the app’s single binary execut-\\nable file. This makes Go-based container images extremely small and perfect for\\nKubernetes.\\nTIP Use the FROM  scratch  directive in the Dockerfile for these images.\\nBut in practice, you’ll soon see these minima l images are extremely difficult to debug.\\nThe first time you need to run a tool such as ping , dig, curl , or something similar\\ninside the container, you’ll realize how impo rtant it is for container images to also\\ninclude at least a limited set of these tools.  I can’t tell you what to include and what\\nnot to include in your images, because it  depends on how you do things, so you’ll\\nneed to find the sw eet spot yourself.\\n17.4.2 Properly tagging your images and using imagePullPolicy wisely\\nYou’ll also soon learn that referring to the latest  image tag in your pod manifests will\\ncause problems, because you can’t tell which version of the image each individual pod\\nreplica is running. Even if initially all your  pod replicas run the same image version, if\\nyou push a new version of the image under the latest  tag, and then pods are resched-\\nuled (or you scale up your Deployment),  the new pods will run the new version,\\nwhereas the old ones will still be running the old one. Also, using the latest  tag\\nmakes it impossible to roll back to a prev ious version (unless you push the old version\\nof the image again).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 529}, page_content='498 CHAPTER  17 Best practices for developing apps\\n It’s almost mandatory to use tags contai ning a proper version designator instead\\nof latest , except maybe in development. Keep in mind that if you use mutable tags\\n(you push changes to the same tag), you’ll need to set the imagePullPolicy  field in\\nthe pod spec to Always . But if you use that in produc tion pods, be aware of the big\\ncaveat associated with it. If the image pull policy is set to Always , the container run-\\ntime will contact the image registry ever y time a new pod is deployed. This slows\\ndown pod startup a bit, because the node needs to check if the image has been mod-\\nified. Worse yet, this policy prevents the pod from starting up when the registry can-\\nnot be contacted.\\n17.4.3 Using multi-dimensional instead of single-dimensional labels\\nDon’t forget to label all your resources, not only Pods. Make sure you add multiple\\nlabels to each resource, so they can be se lected across each individual dimension. You\\n(or the ops team) will be grateful you did it when the number of resources increases.\\n Labels may include things like\\n\\uf0a1The name of the application (or perhap s microservice) the resource belongs to\\n\\uf0a1Application tier (front-end, back-end, and so on)\\n\\uf0a1Environment (development, QA, staging, production, and so on)\\n\\uf0a1Version\\n\\uf0a1Type of release (stable, canary, green or  blue for green/blue deployments, and\\nso on)\\n\\uf0a1Tenant (if you’re running separate pods for each tenant instead of using name-\\nspaces)\\n\\uf0a1Shard for sharded systems\\nThis will allow you to manage resources in groups instead of individually and make it\\neasy to see where each resource belongs.\\n17.4.4 Describing each resource through annotations\\nTo add additional information to your resources use annotations. At the least,\\nresources should contain an annotation de scribing the resource and an annotation\\nwith contact information of th e person responsible for it. \\n In a microservices architecture, pods co uld contain an annotation that lists the\\nnames of the other services the pod is using. This makes it possible to show dependen-cies between pods. Other annotations coul d include build and version information\\nand metadata used by tooling or graphical user interfaces (icon names, and so on).\\n Both labels and annotations make managi ng running applications much easier, but\\nnothing is worse than when an applicatio n starts crashing and you don’t know why.\\n17.4.5 Providing information on why the process terminated\\nNothing is more frustrating than having to figure out why a container terminated\\n(or is even terminating continuously), espe cially if it happens at the worst possible'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 530}, page_content='499 Making your apps easy to run and manage in Kubernetes\\nmoment. Be nice to the ops people and make their lives easier by including all the\\nnecessary debug information in your log files. \\n But to make triage even easier, you can use one other Kubernetes feature that\\nmakes it possible to show the reason why a container terminated in the pod’s status.\\nYou do this by having the process write a termination message to a specific file in the\\ncontainer’s filesystem. The contents of this file are read by the Kubelet when the con-\\ntainer terminates and are shown in the output of kubectl  describe  pod. If an applica-\\ntion uses this mechanism, an operator ca n quickly see why the app terminated without\\neven having to look at the container logs. \\n The default file the process needs to wr ite the message to is /dev/termination-log,\\nbut it can be changed by setting the terminationMessagePath  field in the container\\ndefinition in the pod spec. \\n You can see this in action by running a pod whose container dies immediately, as\\nshown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-termination-messagespec:\\n  containers:\\n  - image: busybox    name: main\\n    terminationMessagePath: /var/termination-reason         \\n    command:    - sh\\n    - -c\\n    - \\'echo \"I\\'\\'ve had enough\" > /var/termination-reason ; exit 1\\'   \\nWhen running this pod, you’ll soon see the pod’s status shown as CrashLoopBackOff .\\nIf you then use kubectl  describe , you can see why the container died, without having\\nto dig down into its logs, as shown in the following listing.\\n$ kubectl describe po\\nName:           pod-with-termination-message...\\nContainers:\\n...    State:      Waiting\\n      Reason:   CrashLoopBackOff\\n    Last State: Terminated      Reason:   Error\\n      Message:  I\\'ve had enough           \\n      Exit Code:        1      Started:          Tue, 21 Feb 2017 21:38:31 +0100\\n      Finished:         Tue, 21 Feb 2017 21:38:31 +0100Listing 17.8 Pod writing a terminati on message: termination-message.yaml\\nListing 17.9 Seeing the container’s termination message with kubectl  describeYou’re overriding the \\ndefault path of the termination message file.\\nThe container\\nwill write the\\nmessage to\\nthe file just\\nbefore exiting.\\nYou can see the reason \\nwhy the container died \\nwithout having to inspect its logs.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 531}, page_content='500 CHAPTER  17 Best practices for developing apps\\n    Ready:              False\\n    Restart Count:      6\\nAs you can see, the “ I’ve  had enough”  message the process wrote to the file /var/ter-\\nmination-reason is shown in the container’s Last  State  section. Note that this mecha-\\nnism isn’t limited only to containers that cras h. It can also be used in pods that run a\\ncompletable task and terminate successfully (you’ll find an example in the file termi-\\nnation-message-success.yaml). \\n This mechanism is great for terminated containers, but you’ll probably agree that\\na similar mechanism would also be useful for showing app-specific status messages of\\nrunning, not only terminated, containers. Kubernetes currently doesn’t provide any\\nsuch functionality and I’m not awar e of any plans to introduce it.\\nNOTE If the container doesn’t write the me ssage to any file, you can set the\\nterminationMessagePolicy  field to FallbackToLogsOnError . In that case,\\nthe last few lines of the container’s log are used as its termination message\\n(but only when the container terminates unsuccessfully).\\n17.4.6 Handling application logs\\nWhile we’re on the subject of application loggi ng, let’s reiterate that apps should write\\nto the standard output instead of files. Th is makes it easy to view logs with the kubectl\\nlogs  command. \\nTIP If a container crashes and is replaced  with a new one, you’ll see the new\\ncontainer’s log. To see the prev ious container’s logs, use the --previous\\noption with kubectl  logs .\\nIf the application logs to a file instead of  the standard output, you can display the log\\nfile using an alternative approach: \\n$ kubectl exec <pod> cat <logfile>\\nThis executes the cat command inside the container and streams the logs back to\\nkubectl, which prints them out in your terminal. \\nCOPYING  LOG AND OTHER  FILES TO AND FROM A CONTAINER\\nYou can also copy the log file to your local machine using the kubectl  cp command,\\nwhich we haven’t looked at yet. It allows you to copy files from and into a container. For\\nexample, if a pod called foo-pod  and its single containe r contains a file at /var/log/\\nfoo.log , you can transfer it to your loca l machine with the following command:\\n$ kubectl cp foo-pod:/var/log/foo.log foo.log\\nTo copy a file from your local machine into  the pod, specify the pod’s name in the sec-\\nond argument:\\n$ kubectl cp localfile foo-pod:/etc/remotefile'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 532}, page_content='501 Making your apps easy to run and manage in Kubernetes\\nThis copies the file localfile to /etc/remotef ile inside the pod’s container. If the pod has\\nmore than one container, you specify the container using the -c containerName  option.\\nUSING CENTRALIZED  LOGGING\\nIn a production system, you’ll want to use a centralized, cluster-wide logging solution,\\nso all your logs are collected and (perma nently) stored in a central location. This\\nallows you to examine historical logs an d analyze trends. With out such a system, a\\npod’s logs are only available while the pod exis ts. As soon as it’s deleted, its logs are\\ndeleted also. \\n Kubernetes by itself doesn’t provide an y kind of centralized logging. The compo-\\nnents necessary for providing a centralized storage and analysis of all the container\\nlogs must be provided by additional components, which usually run as regular pods in\\nthe cluster. \\n Deploying centralized logging solutions is easy. All you need to do is deploy a few\\nYAML/JSON manifests and you’re good to go. On Google Kubernetes Engine, it’s\\neven easier. Check the Enable Stackdriver Logging checkbox when setting up the clus-\\nter. Setting up centralized logging on an on -premises Kubernetes cluster is beyond the\\nscope of this book, but I’ll give you a quick overview of how it’s usually done.\\n You may have already heard of the ELK st ack composed of ElasticSearch, Logstash,\\nand Kibana. A slightly modified variation is  the EFK stack, where Logstash is replaced\\nwith FluentD. \\n When using the EFK stack for centralize d logging, each Kubernetes cluster node\\nruns a FluentD agent (usually as a pod deployed through a DaemonSet), which is\\nresponsible for gathering the logs from the containers, tagging them with pod-specific\\ninformation, and delivering them to Elasti cSearch, which stores them persistently.\\nElasticSearch is also deployed as a pod somewhere in the cluster. The logs can then be\\nviewed and analyzed in a web browser through Kibana, which is a web tool for visualiz-\\ning ElasticSearch data. It also usually runs  as a pod and is exposed through a Service.\\nThe three components of the EFK stac k are shown in the following figure.\\nNOTE In the next chapter, you’ll learn about Helm charts. You can use charts\\ncreated by the Kubernetes community to deploy the EFK stack instead of cre-ating your own YAML manifests. Node 1Container logs KibanaWeb\\nbrowserFluentD\\nNode 2Container logsFluentD\\nNode 3Container logsFluentD ElasticSearch\\nFigure 17.10 Centralized logging with FluentD, ElasticSearch, and Kibana'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 533}, page_content='502 CHAPTER  17 Best practices for developing apps\\nHANDLING  MULTI -LINE LOG STATEMENTS\\nThe FluentD agent stores each line of the lo g file as an entry in the ElasticSearch\\ndata store. There’s one problem with that . Log statements spanning multiple lines,\\nsuch as exception stack traces in Java, ap pear as separate entries in the centralized\\nlogging system. \\n To solve this problem, you can have the apps output JSON instead of plain text.\\nThis way, a multiline log statement can be stored and shown in Kibana as a single\\nentry. But that makes viewing logs with kubectl  logs  much less human-friendly. \\n The solution may be to keep outputting human-readable logs to standard output,\\nwhile writing JSON logs to a file and having them processed by FluentD. This requires\\nconfiguring the node-level FluentD agent a ppropriately or adding a logging sidecar\\ncontainer to every pod. \\n17.5 Best practices for development and testing\\nWe’ve talked about what to be mindful of  when developing apps, but we haven’t\\ntalked about the development and testing workflows that will help you streamline\\nthose processes. I don’t want to go into to o much detail here, because everyone needs\\nto find what works best for them, but here are a few starting points.\\n17.5.1 Running apps outside of Kubernetes during development\\nWhen you’re developing an app that will ru n in a production Kubernetes cluster, does\\nthat mean you also need to run it in Kubernetes during development? Not really. Hav-\\ning to build the app after each minor chan ge, then build the container image, push it\\nto a registry, and then re-deploy the pods  would make development slow and painful.\\nLuckily, you don’t need to go through all that trouble.\\n You can always develop and run apps on your local machine, the way you’re used\\nto. After all, an app running in Kubernetes  is a regular (although isolated) process\\nrunning on one of the cluster nodes. If the app depends on certain features the\\nKubernetes environment provides, you can ea sily replicate that environment on your\\ndevelopment machine.\\n I’m not even talking about running the a pp in a container. Most of the time, you\\ndon’t need that—you can usually run the app directly from your IDE. \\nCONNECTING  TO BACKEND  SERVICES\\nIn production, if the app connects to a backend Service and uses the BACKEND_SERVICE\\n_HOST  and BACKEND_SERVICE_PORT  environment variables to find the Service’s coordi-\\nnates, you can obviously set those environm ent variables on your local machine manu-\\nally and point them to the backend Service, regardless of if it’s running outside or\\ninside a Kubernetes cluster. If it’s running inside Kubernetes, you can always (at least\\ntemporarily) make the Service accessible externally by changing it to a NodePort  or a\\nLoadBalancer -type Service.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 534}, page_content='503 Best practices for development and testing\\nCONNECTING  TO THE API SERVER\\nSimilarly, if your app requires access to  the Kubernetes API server when running\\ninside a Kubernetes cluster, it can easily ta lk to the API server from outside the cluster\\nduring development. If it uses the ServiceA ccount’s token to authenticate itself, you\\ncan always copy the ServiceAccount’s Secret’s files to your local machine with kubectl\\ncp. The API server doesn’t care if  the client accessing it is in side or outside the cluster. \\n If the app uses an ambassador container like the one described in chapter 8, you\\ndon’t even need those Secret files. Run kubectl  proxy  on your local machine, run\\nyour app locally, and it should be ready to talk to your local kubectl  proxy  (as long as\\nit and the ambassador container bi nd the proxy to the same port).\\n In this case, you’ll need to make  sure the user account your local kubectl  is using\\nhas the same privileges as the Se rviceAccount the app will run under.\\nRUNNING  INSIDE  A CONTAINER  EVEN DURING  DEVELOPMENT\\nWhen during development you absolutely have  to run the app in a container for what-\\never reason, there is a way of avoiding havi ng to build the container image every time.\\nInstead of baking the binaries into the imag e, you can always mount your local filesys-\\ntem into the container through Docker volu mes, for example. This way, after you\\nbuild a new version of the app’s binaries, all you need to do is restart the container (or\\nnot even that, if hot-redeploy is supp orted). No need to rebuild the image.\\n17.5.2 Using Minikube in development\\nAs you can see, nothing forces you to run your app inside Kubernetes during develop-\\nment. But you may do that anyway to see how the app behaves in a true Kubernetes\\nenvironment.\\n You may have used Minikube to run exam ples in this book. Although a Minikube\\ncluster runs only a single wo rker node, it’s nevertheless a valuable method of trying\\nout your app in Kubernetes (and, of course , developing all the resource manifests that\\nmake up your complete application). Miniku be doesn’t offer everything that a proper\\nmulti-node Kubernetes cluster usually provides , but in most cases, that doesn’t matter.\\nMOUNTING  LOCAL  FILES INTO THE MINIKUBE  VM AND THEN INTO YOUR CONTAINERS\\nWhen you’re developing with Minikube and you’d like to try out every change to your\\napp in your Kubernetes cluster, you can mo unt your local filesystem into the Minikube\\nVM using the minikube  mount  command and then mount it into your containers\\nthrough a hostPath  volume. You’ll find additional instructions on how to do that\\nin the Minikube documentation at https:/ /github.com/kubernetes/minikube/tree/\\nmaster/docs .\\nUSING THE DOCKER  DAEMON  INSIDE  THE MINIKUBE  VM TO BUILD YOUR IMAGES\\nIf you’re developing your app with Mini kube and planning to build the container\\nimage after every change, you can use the Do cker daemon inside the Minikube VM to\\ndo the building, instead of having to buil d the image through your local Docker dae-\\nmon, push it to a registry, and then have it pulled by the daemon in the VM. To use'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 535}, page_content='504 CHAPTER  17 Best practices for developing apps\\nMinikube’s Docker daemon, all you need to do is point your DOCKER_HOST  environ-\\nment variable to it. Luckily, this is much easier than it sounds. All you need to do is\\nrun the following command on your local machine:\\n$ eval $(minikube docker-env)\\nThis will set all the required environmen t variables for you. You then build your\\nimages the same way as if the Docker daemon was running on your local machine.\\nAfter you build the image, you don’t need to push it anywhere, because it’s already\\nstored locally on the Minikube VM, which means new pods can use the image immedi-\\nately. If your pods are already running, you either need to delete them or kill their\\ncontainers so they’re restarted.\\nBUILDING  IMAGES  LOCALLY  AND COPYING  THEM OVER TO THE MINIKUBE  VM DIRECTLY\\nIf you can’t use the daemon inside the VM to build the images, you still have a way toavoid having to push the image to a regi stry and have the Kubelet running in the\\nMinikube VM pull it. If you build the imag e on your local machine, you can copy it\\nover to the Minikube VM with the following command:\\n$ docker save <image> | (eval $(minikube docker-env) && docker load)\\nAs before, the image is immediately ready to  be  u sed  i n  a p o d . B u t m ak e s ur e th e\\nimagePullPolicy  in your pod spec isn’t set to Always , because that would cause the\\nimage to be pulled from the external regist ry again and you’d lose the changes you’ve\\ncopied over.\\nCOMBINING  MINIKUBE  WITH A PROPER  KUBERNETES  CLUSTER\\nYou have virtually no limit when developi ng apps with Minikube. You can even com-\\nbine a Minikube cluster with a proper Kube rnetes cluster. I sometimes run my devel-\\nopment workloads in my local Minikube cl uster and have them talk to my other\\nworkloads that are deployed in a remote multi-node Kubernetes cluster thousands of\\nmiles away. \\n Once I’m finished with development, I can move my local workloads to the remote\\ncluster with no modifications and with absolutely no problems thanks to how Kuber-\\nnetes abstracts away the underlying infrastructure from the app.\\n17.5.3 Versioning and auto-deploying resource manifests\\nBecause Kubernetes uses a declarative model,  you never have to figure out the current\\nstate of your deployed resources and issue imperative commands to bring that state to\\nwhat you desire. All you need to do is tell Ku bernetes your desired state and it will take\\nall the necessary actions to reconcile th e cluster state with the desired state.\\n You can store your collection of resour ce manifests in a Version Control System,\\nenabling you to perform code reviews, keep  an audit trail, and roll back changes\\nwhenever necessary. After each commit, you can run the kubectl  apply  command to\\nhave your changes reflected in your deployed resources.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 536}, page_content='505 Best practices for development and testing\\n If you run an agent that periodically (o r when it detects a new commit) checks out\\nyour manifests from the Version Control System (VCS), and then runs the apply  com-\\nmand, you can manage your running apps si mply by committing changes to the VCS\\nwithout having to manually talk to the Ku bernetes API server. Luckily, the people at\\nBox (which coincidently was used to host this book’s manuscript and other materials)\\ndeveloped and released a tool called kube-applier , which does exactly what I described.\\nYou’ll find the tool’s source code at https:/ /github.com/box/kube-applier .\\n You can use multiple branches to deploy the manifests to a development, QA, stag-\\ning, and production cluster (or in diff erent namespaces in the same cluster).\\n17.5.4 Introducing Ksonnet as an alternative to writing YAML/JSON \\nmanifests\\nWe’ve seen a number of YAML manifests throughout the book. I don’t see writing\\nYAML as too big of a problem, especially once you learn how to use kubectl  explain\\nto see the available options, but some people do. \\n Just as I was finalizing the manuscript fo r this book, a new tool called Ksonnet was\\nannounced. It’s a library built on top of Jsonnet, which is a data templating language\\nfor building JSON data structures. Instead of writing the complete JSON by hand, it\\nlets you define parameterized JSON fragme nts, give them a name, and then build a\\nfull JSON manifest by referencing those fr agments by name, instead of repeating the\\nsame JSON code in multiple locations—much  like you use functions or methods in a\\nprogramming language. \\n Ksonnet defines the fragments you’d find in Kubernetes resource manifests, allow-\\ning you to quickly build a complete Kubern etes resource JSON manifest with much\\nless code. The following listing shows an example.\\nlocal k = import \"../ksonnet-lib/ksonnet.beta.1/k.libsonnet\";\\nlocal container = k.core.v1.container;\\nlocal deployment = k.apps.v1beta1.deployment;\\nlocal kubiaContainer =                              \\n  container.default(\"kubia\", \"luksa/kubia:v1\") +    \\n  container.helpers.namedPort(\"http\", 8080);        \\ndeployment.default(\"kubia\", kubiaContainer) +    \\ndeployment.mixin.spec.replicas(3)                \\nThe kubia.ksonnet file shown in the listin g is converted to a full JSON Deployment\\nmanifest when you run the following command:\\n$ jsonnet kubia.ksonnetListing 17.10 The kubia  Deployment written with Ksonnet: kubia.ksonnet\\nThis defines a container called kubia, \\nwhich uses the luksa/kubia:v1 image \\nand includes a port called http.\\nThis will be expanded into a full \\nDeployment resource. The kubiaContainer \\ndefined here will be included in the Deployment’s pod template.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 537}, page_content='506 CHAPTER  17 Best practices for developing apps\\nThe power of Ksonnet and Jsonnet becomes apparent when you realize you can define\\nyour own higher-level fragments and make all your manifests consistent and duplica-\\ntion-free. You’ll find more information on using and installing Ksonnet and Jsonnet at\\nhttps:/ /github.com/ksonnet/ksonnet-lib .\\n17.5.5 Employing Continuous Integration and Continuous Delivery \\n(CI/CD)\\nWe’ve touched on automating the deployme nt of Kubernetes resources two sections\\nback, but you may want to set up a complete  CI/CD pipeline for building your appli-\\ncation binaries, container images, and reso urce manifests and then deploying them in\\none or more Kubernetes clusters.\\n You’ll find many online resources talking about this subject. Here, I’d like to point\\nyou specifically to the Fabric8 project ( http:/ /fabric8.io ), which is an integrated\\ndevelopment platform for Kubernetes. It includes Jenkins, the well-known, open-\\nsource automation system, and various othe r tools to deliver a full CI/CD pipeline\\nfor DevOps-style development, deployment, and management of microservices onKubernetes.\\n If you’d like to build your own solution, I also suggest looking at one of the Google\\nCloud Platform’s online labs that talks about this subject. It’s available at https:/ /\\ngithub.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes .\\n17.6 Summary\\nHopefully, the information in this chapter has given you an even deeper insight into\\nhow Kubernetes works and will help you bu i l d  a p p s  t h a t  f e e l  r i g h t  a t  h o m e  w h e n\\ndeployed to a Kubernetes cluster.  The aim of this chapter was to\\n\\uf0a1Show you how all the resources covered in  this book come together to repre-\\nsent a typical application running in Kubernetes.\\n\\uf0a1Make you think about the difference between apps that are rarely movedbetween machines and apps running as pods, which are relocated much more\\nfrequently.\\n\\uf0a1Help you understand that your multi-co mponent apps (or microservices, if you\\nwill) shouldn’t rely on a specific start-up order.\\n\\uf0a1Introduce init containers, which can be used  to initialize a pod or delay the start\\nof the pod’s main containers until a precondition is met.\\n\\uf0a1Teach you about container lifecycle  hooks and when to use them.\\n\\uf0a1Gain a deeper insight into the consequences of the distributed nature of\\nKubernetes components and its eventual consistency model.\\n\\uf0a1Learn how to make your apps shut do wn properly without breaking client\\nconnections.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 538}, page_content='507 Summary\\n\\uf0a1Give you a few small tips on how to make your apps easier to manage by keep-\\ning image sizes small, adding annotation s and multi-dimensional labels to all\\nyour resources, and making it easier to see why an application terminated.\\n\\uf0a1Teach you how to develop Kubernetes apps and run them locally or in Mini-\\nkube before deploying them on a proper multi-node cluster.\\nIn the next and final chapter, we’ll lear n how you can extend Kubernetes with your\\nown custom API objects and controllers and how others have done it to create com-\\nplete Platform-as-a-Service solutions on top of Kubernetes.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 539}, page_content='508Extending Kubernetes\\nYou’re almost done. To wrap up, we’ll lo ok at how you can define your own API\\nobjects and create controllers for those objects. We’ll also look at how others have\\nextended Kubernetes and built Platfor m-as-a-Service solutions on top of it.\\n18.1 Defining custom API objects\\nThroughout the book, you’ve learned abou t the API objects that Kubernetes pro-\\nvides and how they’re used to build application systems. Currently, Kubernetes\\nusers mostly use only these objects even though they represent relatively low-level,\\ngeneric concepts. This chapter covers\\n\\uf0a1Adding custom objects to Kubernetes\\n\\uf0a1Creating a controller for the custom object\\n\\uf0a1Adding custom API servers\\n\\uf0a1Self-provisioning of services with the Kubernetes \\nService Catalog\\n\\uf0a1Red Hat’s OpenShift Container Platform\\n\\uf0a1Deis Workflow and Helm'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 540}, page_content='509 Defining custom API objects\\n As the Kubernetes ecosystem evolves, you’ ll see more and more high-level objects,\\nwhich will be much more specialized than  the resources Kubernetes supports today.\\nInstead of dealing with Deployments, Servic es, ConfigMaps, and the like, you’ll create\\nand manage objects that represent whole applications or software services. A customcontroller will observe those high-level ob jects and create low-level objects based on\\nthem. For example, to run a messaging broker  inside a Kubernetes cluster, all you’ll\\nneed to do is create an instance of a Queue resource and all the necessary Secrets,\\nDeployments, and Services will be create d by a custom Queue controller. Kubernetes\\nalready provides ways of adding  custom resources like this. \\n18.1.1 Introducing CustomResourceDefinitions\\nTo define a new resource type, all you need  to do is post a CustomResourceDefinition\\nobject (CRD) to the Kubernetes API server . The CustomResourceDefinition object is\\nthe description of the custom resource ty pe. Once the CRD is posted, users can then\\ncreate instances of the custom resource by  posting JSON or YAML manifests to the\\nAPI server, the same as with any other Kubernetes resource.\\nNOTE Prior to Kubernetes 1.7, custom resources were defined through Third-\\nPartyResource objects, which were similar to CustomResourceDefinitions, butwere removed in version 1.8.\\nCreating a CRD so that users can create obje cts of the new type isn’t a useful feature if\\nthose objects don’t make something tangible  happen in the cluster. Each CRD will\\nusually also have an associated controll er (an active component doing something\\nbased on the custom objects), the same wa y that all the core Kubernetes resources\\nhave an associated controller, as was explai ned in chapter 11. For this reason, to prop-\\nerly show what CustomResourceDefiniti ons allow you to do other than adding\\ninstances of a custom object, a controller must  be deployed as well. You’ll do that in\\nthe next example.\\nINTRODUCING  THE EXAMPLE  CUSTOM RESOURCE DEFINITION\\nLet’s imagine you want to allow users of your  Kubernetes cluster to run static websites\\nas easily as possible, without having to de al with Pods, Services, and other Kubernetes\\nresources. What you want to achieve is for users to create objects of type Website that\\ncontain nothing more than the website’s name and the source from which the web-\\nsite’s files (HTML, CSS, PNG, and others) sh ould be obtained. You’ll use a Git reposi-\\ntory as the source of those files. When a user creates an instance of the Website\\nresource, you want Kubernetes to spin up a new web server pod and expose it through\\na Service, as shown in figure 18.1.\\n To create the Website resource, you want users to post manifests along the lines of\\nthe one shown in the following listing.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 541}, page_content='510 CHAPTER  18 Extending Kubernetes\\nkind: Website        \\nmetadata:\\n  name: kubia             spec:\\n  gitRepo: https://github.com/luksa/kubia-website-example.git   \\nLike all other resources, your resource contains a kind  and a metadata.name  field,\\nand like most resources,  it also contains a spec  section. It contains a single field called\\ngitRepo  (you can choose the name)—it specifies the Git repository containing the\\nwebsite’s files. You’ll also need to include an apiVersion  field, but you don’t know yet\\nwhat its value must be  for custom resources.\\n If you try posting this resource to Ku bernetes, you’ll receive an error because\\nKubernetes doesn’t know what a Website object is yet:\\n$ kubectl create -f imaginary-kubia-website.yaml\\nerror: unable to recognize \"imaginary-kubi a-website.yaml\": no matches for \\n➥ /, Kind=Website\\nBefore you can create instances of your cu stom object, you need to make Kubernetes\\nrecognize them.\\nCREATING  A CUSTOM RESOURCE DEFINITION  OBJECT\\nTo make Kubernetes accept your custom We bsite resource instances, you need to post\\nthe CustomResourceDefinition shown in th e following listing to the API server.\\napiVersion: apiextensions.k8s.io/v1beta1       \\nkind: CustomResourceDefinition                 \\nmetadata:\\n  name: websites.extensions.example.com      spec:\\n  scope: Namespaced                          Listing 18.1 An imaginary custom reso urce: imaginary-kubia-website.yaml\\nListing 18.2 A CustomResourceDefinition manifest: website-crd.yamlWebsite\\nkind: Website\\nmetadata:\\nname: kubia\\nspec:\\ngitRepo:\\ngithub.com/.../kubia.gitPod:\\nkubia-websiteService:\\nkubia-website\\nFigure 18.1 Each Website object should result in the creation of a Service and an HTTP \\nserver Pod.\\nA custom \\nobject kind The name of the website \\n(used for naming the resulting Service and Pod) The Git \\nrepository \\nholding the website’s files\\nCustomResourceDefinitions belong \\nto this API group and version. The full\\nname of\\nyour\\ncustom\\nobjectYou want Website resources \\nto be namespaced.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 542}, page_content='511 Defining custom API objects\\n  group: extensions.example.com                \\n  version: v1                                  \\n  names:                                        kind: Website                           \\n    singular: website                       \\n    plural: websites                        \\nAfter you post the descriptor to Kubernetes, it will allow you to create any number of\\ninstances of the custom Website resource. \\n You can create the CRD from the website-cr d.yaml file available in the code archive:\\n$ kubectl create -f website-crd-definition.yaml\\ncustomresourcedefinition \"websites.extensions.example.com\" created\\nI’m sure you’re wondering about the long na me of the CRD. Why not call it Website?\\nThe reason is to prevent name clashes. By adding a suffix to the name of the CRD(which will usually include the name of th e organization that created the CRD), you\\nkeep CRD names unique. Luckily, the long na me doesn’t mean you’ll need to create\\nyour Website resources with \\nkind:  websites.extensio ns.example.com , but as kind:\\nWebsite , as specified in the names.kind  property of the CRD. The extensions.exam-\\nple.com  part is the API group of your resource. \\n You’ve seen how creating Deploy ment objects requires you to set apiVersion  to\\napps/v1beta1  instead of v1. The part before the slash is the API group (Deployments\\nbelong to the apps  API group), and the part after it is the version name ( v1beta1  in\\nthe case of Deployments). When creating instances of the custom Website resource,\\nthe apiVersion  property will need to be set to extensions.example.com/v1 .\\nCREATING  AN INSTANCE  OF A CUSTOM  RESOURCE\\nConsidering what you learned, you’ll now create a proper YAML for your Website\\nresource instance. The YAML manifest  is shown in the following listing.\\napiVersion: extensions.example.com/v1       \\nkind: Website                               \\nmetadata:\\n  name: kubia                                spec:\\n  gitRepo: https://github.com/luksa/kubia-website-example.git\\nThe kind  of your resource is Website, and the apiVersion  is composed of the API\\ngroup and the version number you defi ned in the CustomResourceDefinition.\\n Create your Website object now:\\n$ kubectl create -f kubia-website.yaml\\nwebsite \"kubia\" createdListing 18.3 A custom Website resource: kubia-website.yamlDefine an API group and version \\nof the Website resource.\\nYou need to specify the various \\nforms of the custom object’s name.\\nYour custom API\\ngroup and versionThis manifest \\ndescribes a Website resource instance.\\nThe name of the \\nWebsite instance'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 543}, page_content='512 CHAPTER  18 Extending Kubernetes\\nThe response tells you that the API serv er has accepted and stored your custom\\nWebsite object. Let’s see if  you can now retrieve it. \\nRETRIEVING  INSTANCES  OF A CUSTOM  RESOURCE\\nList all the websites in your cluster:\\n$ kubectl get websites\\nNAME      KIND\\nkubia     Website.v1.extensions.example.com\\nAs with existing Kubernetes resources, you can create and then list instances of cus-\\ntom resources. You can also use kubectl  describe  to see the details of your custom\\nobject, or retrieve the whole YAML with kubectl  get, as in the following listing.\\n$ kubectl get website kubia -o yaml\\napiVersion: extensions.example.com/v1\\nkind: Website\\nmetadata:  creationTimestamp: 2017-02-26T15:53:21Z\\n  name: kubia\\n  namespace: default  resourceVersion: \"57047\"\\n  selfLink: /apis /extensions.example.com/v1/.../default/websites/kubia\\n  uid: b2eb6d99-fc3b-11e6-bd71-0800270a1c50spec:\\n  gitRepo: https://github.com/luksa/kubia-website-example.git\\nNote that the resource includes everything that was in the original YAML definition,\\nand that Kubernetes has initialized additional  metadata fields the way it does with all\\nother resources. \\nDELETING  AN INSTANCE  OF A CUSTOM  OBJECT\\nObviously, in addition to creating and retr ieving custom object instances, you can also\\ndelete them:\\n$ kubectl delete website kubia\\nwebsite \"kubia\" deleted\\nNOTE You’re deleting an instance of  a Website, not the Website CRD\\nresource. You could also delete the CRD ob ject itself, but let’s hold off on that\\nfor a while, because you’ll be creating  additional Website instances in the\\nnext section. \\nLet’s go over everything you’ve done. By creating a CustomResourceDefinition object,\\nyou can now store, retrieve, and delete cu stom objects through the Kubernetes API\\nserver. These objects don’t do anything yet. You’ll need to create a controller to make\\nthem do something. Listing 18.4 Full Website resource defin ition retrieved from the API server'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 544}, page_content='513 Defining custom API objects\\n In general, the point of creating custom ob jects like this isn’t always to make some-\\nthing happen when the object is created. Ce rtain custom objects are used to store data\\ninstead of using a more generic mechanism such as a ConfigMap. Applications run-\\nning inside pods can query the API server for those objects and read whatever isstored in them. \\n But in this case, we said you wanted the existence of a Website object to result in\\nthe spinning up of a web server serving the contents of the Git repository referenced\\nin the object. We’ll look at how to do that next.\\n18.1.2 Automating custom resources with custom controllers\\nTo make your Website objects run a web se rver pod exposed through a Service, you’ll\\nneed to build and deploy a Website controlle r, which will watch the API server for the\\ncreation of Website objects and then crea te the Service and the web server Pod for\\neach of them. \\n To make sure the Pod is managed and su rvives node failures, the controller will\\ncreate a Deployment resource instead of an unmanaged Pod directly. The controller’s\\noperation is summarized in figure 18.2.\\nI’ve written a simple initial version of the controller, which works well enough to\\nshow CRDs and the controller in action, bu t it’s far from being production-ready,\\nbecause it’s overly simplified. The container image is available at docker.io/luksa/\\nwebsite-controller:latest , and the source code is at https:/ /github.com/luksa/k8s-\\nwebsite-controller . Instead of going through its sour ce code, I’ll explain what the con-\\ntroller does.API server\\nWebsites\\nWebsite:\\nkubia\\nDeployments\\nDeployment:\\nkubia-website\\nServices\\nService:\\nkubia-websiteWebsite\\ncontrollerWatches\\nCreates\\nFigure 18.2 The Website controller \\nwatches for Website objects and \\ncreates a Deployment and a Service.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 545}, page_content='514 CHAPTER  18 Extending Kubernetes\\nUNDERSTANDING  WHAT THE WEBSITE  CONTROLLER  DOES\\nImmediately upon startup, the controller star ts to watch Website objects by requesting\\nthe following URL:\\nhttp://localhost:8001/apis/extensions.exam ple.com/v1/websites?watch=true\\nYou may recognize the hostname and port—the controller isn’t connecting to the\\nAPI server directly, but is instead connecting to the kubectl  proxy  process, which\\nruns in a sidecar container in the same pod and acts as the ambassador to the API\\nserver (we examined the ambassador pattern  in chapter 8). The proxy forwards the\\nrequest to the API server, taking care of both TLS encryption and authentication\\n(see figure 18.3).\\nThrough the connection opened by this H TTP GET request, the API server will send\\nwatch events for every change to any Website object.\\n The API server sends the ADDED  watch event every time a new Website object is cre-\\nated. When the controller receives such an event, it extracts the Website’s name and\\nthe URL of the Git repository from the Webs ite object it received in the watch event\\nand creates a Deployment and a Service object  by posting their JSON manifests to the\\nAPI server. \\n The Deployment resource contains a template for a pod with two containers\\n(shown in figure 18.4): one running an ng inx server and another one running a git-\\nsync process, which keeps a local directory synced with the contents of a Git repo.\\nThe local directory is shared with the nginx container through an emptyDir  volume\\n(you did something similar to that in ch apter 6, but instead of keeping the local\\ndirectory synced with a Git repo, you used a gitRepo  volume to download the Git\\nrepo’s contents at pod startup; the volume’s  contents weren’t kept in sync with the\\nGit repo afterward). The Service is a NodePort  Service, which exposes your web\\nserver pod through a random port on each  node (the same port is used on all\\nnodes). When a pod is created by the Depl oyment object, clients can access the web-\\nsite through the node port.Pod: website-controller\\nContainer: main\\nWebsite controller\\nGET http://localhost:8001/apis/extensions.\\nexample.com/v1/websites?watch=trueGET https://kubernetes:443/apis/extensions.\\nexample.com/v1/websites?watch=true\\nAuthorization: Bearer <token>Container: proxy\\nkubectl proxy API server\\nFigure 18.3 The Website controller talks to the API server through a proxy (in the ambassador container).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 546}, page_content='515 Defining custom API objects\\nThe API server also sends a DELETED  watch event when a Website resource instance is\\ndeleted. Upon receiving the event, the co ntroller deletes the Deployment and the Ser-\\nvice resources it created earlier. As soon as a user deletes the Website instance, the\\ncontroller will shut down and remove the web server serving that website.\\nNOTE My oversimplified controller isn’t implemented properly. The way it\\nwatches the API objects doesn’t guaran tee it won’t miss individual watch\\nevents. The proper way to watch objects through the API server is to not onlywatch them, but also periodically re-lis t all objects in case any watch events\\nwere missed. \\nRUNNING  THE CONTROLLER  AS A POD\\nDuring development, I ran the controller on  my local development laptop and used a\\nlocally running kubectl  proxy  process (not running as a pod) as the ambassador to\\nthe Kubernetes API server. This allowed me to develop quickly, because I didn’t need\\nto build a container image after every chan ge to the source code and then run it\\ninside Kubernetes. \\n When I’m ready to deploy the controller in to production, the best way is to run the\\ncontroller inside Kubernetes itself, the way you do with all the other core controllers.\\nTo run the controller in Kubernetes, you can deploy it through a Deployment resource.\\nThe following listing shows an example of such a Deployment.\\napiVersion: apps/v1beta1\\nkind: Deploymentmetadata:\\n  name: website-controller\\nspec:  replicas: 1                      \\n  template:Listing 18.5 A Website controller Deployment: website-controller.yamlPod\\nWebserver\\ncontainerWeb client\\ngit-sync\\ncontainerServes website to\\nweb client througha random port\\nClones Git repo\\ninto volume andkeeps it syncedemptyDir\\nvolume\\nFigure 18.4 The pod serving \\nthe website specified in the Website object\\nYou’ll run a single \\nreplica of the \\ncontroller.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 547}, page_content='516 CHAPTER  18 Extending Kubernetes\\n    metadata:\\n      name: website-controller\\n      labels:        app: website-controller\\n    spec:\\n      serviceAccountName: website-controller          containers:                                    \\n      - name: main                                   \\n        image: luksa/website-controller              \\n      - name: proxy                                  \\n        image: luksa/kubectl-proxy:1.6.2             \\nAs you can see, the Deployment deploys a si ngle replica of a two-container pod. One\\ncontainer runs your controller, whereas th e other one is the ambassador container\\nused for simpler communication with the API server. The pod runs under its own spe-\\ncial ServiceAccount, so you’ll need to cr eate it before you deploy the controller:\\n$ kubectl create serviceaccount website-controller\\nserviceaccount \"website-controller\" created\\nIf Role Based Access Control (RBAC) is en abled in your cluster, Kubernetes will not\\nallow the controller to watch Website resour ces or create Deployments or Services. To\\nallow it to do that, you’ll need to bind the website-controller  ServiceAccount to the\\ncluster-admin  ClusterRole, by creating a ClusterRoleBinding like this:\\n$ kubectl create clusterrolebinding website-controller \\n➥ --clusterrole=cluster-admin \\n➥ --serviceaccount=default:website-controller\\nclusterrolebinding \"website-controller\" created\\nOnce you have the ServiceAccount and ClusterRoleBinding in place, you can deploy\\nthe controller’s Deployment. \\nSEEING THE CONTROLLER  IN ACTION\\nWith the controller now running, create the kubia  Website resource again:\\n$ kubectl create -f kubia-website.yaml\\nwebsite \"kubia\" created\\nNow, let’s check the controller’s logs (shown in the following listing) to see if it has\\nreceived the watch event.\\n$ kubectl logs website-controller-2429717411-q43zs -c main\\n2017/02/26 16:54:41 website-controller started.2017/02/26 16:54:47 Received watch event: ADDED: kubia: https://github.c...\\n2017/02/26 16:54:47 Creating services with  name kubia-website in namespa... \\n2017/02/26 16:54:47 Response status: 201 Created2017/02/26 16:54:47 Creating deployments w ith name kubia-website in name... \\n2017/02/26 16:54:47 Response status: 201 CreatedListing 18.6 Displaying logs of the Website controllerIt will run \\nunder a special \\nServiceAccount.\\nTwo containers: the \\nmain container and \\nthe proxy sidecar'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 548}, page_content='517 Defining custom API objects\\nThe logs show that the controller received the ADDED  event and that it created a Service\\nand a Deployment for the kubia-website  Website. The API server responded with a\\n201 Created  response, which means the two resources should now exist. Let’s verify\\nthat the Deployment, Service and the result ing Pod were created. The following list-\\ning lists all Deployments, Services and Pods.\\n$ kubectl get deploy,svc,po\\nNAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE  AGE\\ndeploy/kubia-website         1         1         1            1          4s\\ndeploy/website-controller   1         1         1            1          5m\\nNAME                CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\\nsvc/kubernetes      10.96.0.1      <none>        443/TCP        38dsvc/kubia-website    10.101.48.23   <nodes>       80:32589/TCP   4s\\nNAME                                     READY     STATUS    RESTARTS   AGE\\npo/kubia-website-1029415133-rs715         2/2       Running   0          4s\\npo/website-controller-1571685839-qzmg6   2/2       Run ning   1          5m\\nThere they are. The kubia-website  Service, through which you can access your web-\\nsite, is available on port 32589  on all cluster nodes. You can access it with your browser.\\nAwesome, right? \\n Users of your Kubernetes cluster can now deploy static websites in seconds, with-\\nout knowing anything about Pods, Services, or any other Kubernetes resources, except\\nyour custom Website resource. \\n Obviously, you still have room for improvement. The co ntroller could, for exam-\\nple, watch for Service objects and as soon as  the node port is assigned, write the URL\\nthe website is accessible at into the status  section of the Website resource instance\\nitself. Or it could also create an Ingress ob ject for each website. I’ll leave the imple-\\nmentation of these additional fe atures to you as an exercise.\\n18.1.3 Validating custom objects\\nYou may have noticed that you didn’t specif y any kind of validation schema in the Web-\\nsite CustomResourceDefinition. Users can include any field they want in the YAML of\\ntheir Website object. The API server doesn’t validate the contents of the YAML (except\\nthe usual fields like apiVersion , kind , and metadata ), so users can create invalid\\nWebsite objects (without a gitRepo  field, for example). \\n Is it possible to add validation to the controller and prevent invalid objects from\\nbeing accepted by the API server? It isn’t, be cause the API server first stores the object,\\nthen returns a success resp onse to the client ( kubectl ), and only then notifies all the\\nwatchers (the controller is one of them). All the controller can really do is validate\\nthe object when it receives it in a watch ev ent, and if the object is invalid, write the\\nerror message to the Website object (by updating the object through a new request tothe API server). The user wouldn’t be notifi ed of the error automatically. They’d haveListing 18.7 The Deployment, Serv ice, and Pod created for the kubia-website'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 549}, page_content='518 CHAPTER  18 Extending Kubernetes\\nto notice the error message by querying th e API server for the Website object. Unless\\nthe user does this, they have no way of knowing whether the object is valid or not.\\n This obviously isn’t ideal. You’d want the API server to validate the object and\\nreject invalid objects immediately. Validat ion of custom objects was introduced in\\nKubernetes version 1.8 as an alpha feature. To have the API server validate your cus-\\ntom objects, you need to enable the CustomResourceValidation  feature gate in the\\nAPI server and specify a JSON schema in the CRD.\\n18.1.4 Providing a custom API server for your custom objects\\nA better way of adding support  for custom objects in Kube rnetes is to implement your\\nown API server and have the clients talk directly to it. \\nINTRODUCING  API SERVER  AGGREGATION\\nIn Kubernetes version 1.7, you can integr ate your custom API server with the main\\nKubernetes API server, through API server ag gregation. Initially, the Kubernetes API\\nserver was a single monolithic component.  From Kubernetes version 1.7, multiple\\naggregated API servers will be exposed at a single location. Clients can connect to the\\naggregated API and have their requests tr ansparently forwarded to the appropriate\\nAPI server. This way, the client wouldn’t ev en be aware that multiple API servers han-\\ndle different objects behind the scenes. Ev en the core Kubernetes API server may\\neventually end up being split into multiple smaller API servers and exposed as a single\\nserver through the aggregator, as shown in figure 18.5.\\nIn your case, you could create an API se rver responsible for handling your Website\\nobjects. It could validate those objects the way the core Kubernetes API server validates\\nthem. You’d no longer need to create a CR D to represent those objects, because you’d\\nimplement the Website object type in to the custom API server directly. \\n Generally, each API server is responsibl e for storing their own resources. As shown\\nin figure 18.5, it can either run its own inst ance of etcd (or a whole etcd cluster), or itMain\\nAPI server\\nCustom\\nAPI server YCustom\\nAPI server X\\nkubectl\\nUses its own etcd instance\\nfor storing its resourcesUses CustomResourceDeﬁnitions\\nin main API server as storagemechanism\\netcdetcd\\nFigure 18.5 API server aggregation'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 550}, page_content='519 Extending Kubernetes with the Kubernetes Service Catalog\\ncan store its resources in the core API server ’s etcd store by creating CRD instances in\\nthe core API server. In that case, it needs to  create a CRD object first, before creating\\ninstances of the CRD, the way you did in the example.\\nREGISTERING  A CUSTOM  API SERVER\\nTo add a custom API server to your cluster, you’d deploy it as a pod and expose it\\nthrough a Service. Then, to integrate it in to the main API server, you’d deploy a YAML\\nmanifest describing an APIService resource like the one in the following listing.\\napiVersion: apiregistration.k8s.io/v1beta1   \\nkind: APIService                             \\nmetadata:\\n  name: v1alpha1.extensions.example.comspec:\\n  group: extensions.example.com           \\n  version: v1alpha1                        priority: 150\\n  service:                    \\n    name: website-api             namespace: default        \\nAfter creating the APIService resource from the previous listing, client requests sent\\nto the main API server that contain any resource from the extensions.example.com\\nAPI group and version v1alpha1  would be forwarded to the custom API server pod(s)\\nexposed through the website-api  Service. \\nCREATING  CUSTOM  CLIENTS\\nWhile you can create custom resources from YAML files using the regular kubectl  cli-\\nent, to make deployment of custom objects even easier, in addition to providing a cus-\\ntom API server, you can also build a custom  CLI tool. This will allow you to add\\ndedicated commands for manipulati ng those objects, similar to how kubectl  allows\\ncreating Secrets, Deployments, and othe r resources through resource-specific com-\\nmands like kubectl  create  secret  or kubectl  create  deployment .\\n As I’ve already mentioned, custom API se rvers, API server aggregation, and other\\nfeatures related to extending Kubernetes ar e currently being worked on intensively, so\\nthey may change after the book is publishe d. To get up-to-date information on the\\nsubject, refer to the Kubernetes GitHub repos at http:/ /github.com/kubernetes .\\n18.2 Extending Kubernetes with the Kubernetes Service \\nCatalog\\nOne of the first additional API servers that  will be added to Kubernetes through API\\nserver aggregation is the Service Catalog AP I server. The Service Catalog is a hot topic\\nin the Kubernetes community, so you may want to know about it. \\n Currently, for a pod to consume a servic e (here I use the term generally, not in\\nrelation to Service resources; for exampl e, a database service includes everythingListing 18.8 An APIService  YAML definition \\nThis is an APIService \\nresource.\\nThe API group this API \\nserver is responsible for\\nThe supported API version\\nThe Service the custom API \\nserver is exposed through'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 551}, page_content='520 CHAPTER  18 Extending Kubernetes\\nrequired to allow users to use a database in their app), someone needs to deploy the\\npods providing the service, a Service resource , and possibly a Secret so the client pod\\ncan use it to authenticate with the service. That someone is usually the same user\\ndeploying the client pod or, if a team is de dicated to deploying these types of general\\nservices, the user needs to fi le a ticket and wait for the team to provision the service.\\nThis means the user needs to either create the manifests for all the components of the\\nservice, know where to find an existing set of manifests, know how to configure it\\nproperly, and deploy it manually, or wait for the other team to do it. \\n But Kubernetes is supposed to be an easy -to-use, self-service system. Ideally, users\\nwhose apps require a certain service (for ex ample, a web application requiring a back-\\nend database), should be able to say to Kubernetes. “Hey, I need a PostgreSQL data-\\nbase. Please provision one and tell me wher e and how I can connect to it.” This will\\nsoon be possible through the Kubernetes Service Catalog. \\n18.2.1 Introducing the Service Catalog\\nAs the name suggests, the Service Catalog is a catalog of services. Users can browse\\nthrough the catalog and provision instances of the services listed in the catalog by\\nthemselves without having to deal with Po ds, Services, ConfigMaps, and other resources\\nrequired for the service to run. You’ll recogn ize that this is similar to what you did\\nwith the Website custom resource.\\n Instead of adding custom resources to the API server for each type of service, the\\nService Catalog introduces the following four generic API resources:\\n\\uf0a1A ClusterServiceBroker, which describes an  (external) system that can provision\\nservices\\n\\uf0a1A ClusterServiceClass, which describes a ty pe of service that can be provisioned\\n\\uf0a1A ServiceInstance, which is one instance of a service that has been provisioned\\n\\uf0a1A ServiceBinding, which represents a binding between a set of clients (pods)\\nand a ServiceInstance\\nThe relationships between those four resources are shown in the figure 18.6 and\\nexplained in the following paragraphs.\\nIn a nutshell, a cluster admin creates a Clus terServiceBroker resource for each service\\nbroker whose services they’d like to make av ailable in the cluster. Kubernetes then asks\\nthe broker for a list of services that it can provide and creates a ClusterServiceClass\\nresource for each of them. When a user requir es a service to be provisioned, they create\\nan ServiceInstance resource and then a Serv iceBinding to bind that ServiceInstance toClient pods ServiceBinding ServiceInstance ClusterServiceClass(es) ClusterServiceBroker\\nFigure 18.6 The relationships between Service Catalog API resources.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 552}, page_content='521 Extending Kubernetes with the Kubernetes Service Catalog\\ntheir pods. Those pods are then injected with  a Secret that holds all the necessary cre-\\ndentials and other data required to co nnect to the provisioned ServiceInstance.\\n The Service Catalog system archit ecture is shown in figure 18.7.\\nThe components shown in the figure ar e explained in the following sections.\\n18.2.2 Introducing the Service Catalog API server and Controller \\nManager\\nSimilar to core Kubernetes, the Service Catalog is a distributed system composed of\\nthree components:\\n\\uf0a1Service Catalog API Server\\n\\uf0a1etcd as the storage\\n\\uf0a1Controller Manager, where all the controllers run\\nThe four Service Catalog–related resources we  introduced earlier are created by post-\\ning YAML/JSON manifests to the API server. It then stores them into its own etcd\\ninstance or uses CustomResourceDefinitions in the main API server as an alternative\\nstorage mechanism (in that case, no additional etcd instance is required). \\n The controllers running in the Contro ller Manager are the ones doing some-\\nthing with those resources. They obviously talk to the Service Catalog API server, the\\nway other core Kubernetes controllers talk  to the core API server. Those controllers\\ndon’t provision the requested services th emselves. They leave that up to external\\nservice brokers, which are registered by cr eating ServiceBroker resources in the Ser-\\nvice Catalog API.Kubernetes cluster External system(s)\\nKubernetes Service Catalog\\nClient podsProvisioned\\nservicesBroker A\\nBroker B\\netcdService\\nCatalog\\nAPI serverController\\nManager\\nkubectlProvisioned\\nservices\\nClient pods use the\\nprovisioned services\\nFigure 18.7 The architecture of the Service Catalog'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 553}, page_content='522 CHAPTER  18 Extending Kubernetes\\n18.2.3 Introducing Service Brokers and the OpenServiceBroker API\\nA cluster administrator can register one or more external ServiceBrokers in the Ser-\\nvice Catalog. Every broker must implement the OpenServiceBroker API.\\nINTRODUCING  THE OPENSERVICE BROKER  API\\nThe Service Catalog talks to the broker throug h that API. The API is relatively simple.\\nIt’s a REST API providing the following operations:\\n\\uf0a1Retrieving the list of services with GET /v2/catalog\\n\\uf0a1Provisioning a service instance ( PUT /v2/service_instances/:id )\\n\\uf0a1Updating a service instance ( PATCH  /v2/service_instances/:id )\\n\\uf0a1Binding a service instance ( PUT /v2/service_instanc es/:id/service_bind-\\nings/:binding_id )\\n\\uf0a1Unbinding an instance ( DELETE  /v2/service_instanc es/:id/service_bind-\\nings/:binding_id )\\n\\uf0a1Deprovisioning a service instance ( DELETE  /v2/service_instances/:id )\\nYou’ll find the OpenServiceBroker API spec at https:/ /github.com/openservicebro-\\nkerapi/servicebroker .\\nREGISTERING  BROKERS  IN THE SERVICE  CATALOG\\nThe cluster administrator registers a broker  by posting a ServiceBroker resource man-\\nifest to the Service Catalog API, like the one shown in the following listing.\\napiVersion: servicecatalog.k8s.io/v1alpha1    \\nkind: ClusterServiceBroker                                  \\nmetadata:\\n  name: database-broker                          \\nspec:  url: http://database-osbapi.myorganization.org  \\nThe listing describes an imaginary broker that can provision databases of different\\ntypes. After the administrator creates the ClusterServiceBroker resource, a controller\\nin the Service Catalog Controller Manage r connects to the URL specified in the\\nresource to retrieve the list of services this broker can provision.\\n After the Service Catalog retrieves the list  of services, it creates a ClusterService-\\nClass resource for each of them. Each Cl usterServiceClass resource describes a sin-\\ngle type of service that can be provisioned (an example of a ClusterServiceClass is\\n“PostgreSQL database”). Each ClusterService Class has one or more service plans asso-\\nciated with it. These allow the user to choose  the level of service they need (for exam-\\nple, a database ClusterServiceClass could pr ovide a “Free” plan, where the size of theListing 18.9 A ClusterServiceBroker manifest: database-broker.yaml\\nThe resource kind and \\nthe API group and version\\nThe name of this broker\\nWhere the Service Catalog\\ncan contact the broker\\n(its OpenServiceBroker [OSB] API URL)'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 554}, page_content='523 Extending Kubernetes with the Kubernetes Service Catalog\\ndatabase is limited and the underlying storage is a spinning disk, and a “Premium”\\nplan, with unlimited size and SSD storage). \\nLISTING  THE AVAILABLE  SERVICES  IN A CLUSTER\\nUsers of the Kubernetes cluster can retrieve a list of all services that can be provi-sioned in the cluster with \\nkubectl  get serviceclasses , as shown in the following\\nlisting.\\n$ kubectl get clusterserviceclasses\\nNAME                KIND\\npostgres-database   ClusterServiceClass.v1alpha1.servicecatalog.k8s.io\\nmysql-database      ServiceClass.v1alpha1.servicecatalog.k8s.iomongodb-database    ServiceClass.v1alpha1.servicecatalog.k8s.io\\nThe listing shows ClusterServiceClasses for services that your imaginary database bro-\\nker could provide. You can co mpare ClusterServiceClasses to StorageClasses, which we\\ndiscussed in chapter 6. StorageClasses allow yo u to select the type of storage you’d like\\nto use in your pods, while ClusterServiceCla sses allow you to select the type of service.\\n You can see details of one of the Cluste rServiceClasses by retrieving its YAML. An\\nexample is shown in the following listing.\\n$ kubectl get serviceclass postgres-database -o yaml\\napiVersion: servicecatalog.k8s.io/v1alpha1bindable: true\\nbrokerName: database-broker                     \\ndescription: A PostgreSQL database\\nkind: ClusterServiceClass\\nmetadata:  name: postgres-database\\n  ...\\nplanUpdatable: falseplans:\\n- description: A free (but slow) PostgreSQL instance        \\n  name: free                                                  osbFree: true                                             \\n  ...\\n- description: A paid (very fast) PostgreSQL instance        name: premium                                            \\n  osbFree: false                                           \\n  ...\\nThe ClusterServiceClass in the listing contains two plans—a free  plan, and a premium\\nplan. You can see that this ClusterServiceClass is provided by the database-broker\\nbroker.Listing 18.10 List of ClusterServiceClasses in a cluster\\nListing 18.11 A ClusterServiceClass definition\\nThis ClusterServiceClass \\nis provided by the \\ndatabase-broker.\\nA free plan for \\nthis service\\nA paid plan'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 555}, page_content='524 CHAPTER  18 Extending Kubernetes\\n18.2.4 Provisioning and using a service\\nLet’s imagine the pods you’re deploying need  to use a database. You’ve inspected the\\nlist of available ClusterServiceClasses and have chosen to use the free  p l a n  o f  t h e\\npostgres-database  ClusterServiceClass. \\nPROVISIONING  A SERVICE INSTANCE\\nTo have the database provisioned for you, all you need to do is create a Service-\\nInstance resource, as shown in the following listing.\\napiVersion: servicecatalog.k8s.io/v1alpha1\\nkind: ServiceInstance\\nmetadata:\\n  name: my-postgres-db                     spec:\\n  clusterServiceClassName: postgres-database        \\n  clusterServicePlanName: free                               parameters:\\n    init-db-args: --data-checksums         \\nYou created a ServiceInstance called my-postgres-db  (that will be the name of the\\nresource you’re deploying) and specifie d the ClusterServiceClass and the chosen\\nplan. You’re also specifying a parameter, wh ich is specific for each broker and Cluster-\\nServiceClass. Let’s imagine you looked up th e possible parameters in the broker’s doc-\\numentation.\\n As soon as you create this resource, the Service Catalog will contact the broker the\\nClusterServiceClass belongs to and ask it to provision the service. It will pass on the\\nchosen ClusterServiceClass and plan names, as well as all the parameters you specified.\\n It’s then completely up to the broker to know what to do with this information. In\\nyour case, your database broker will probab ly spin up a new instance of a PostgreSQL\\ndatabase somewhere—not necessarily in the same Kubernetes cluster or even in\\nKubernetes at all. It could run a Virtual Machine and run the database in there. The\\nService Catalog doesn’t care, and neither does the user requesting the service. \\n You can check if the service has been pr ovisioned successfully by inspecting the\\nstatus  section of the my-postgres-db ServiceI nstance you created, as shown in the\\nfollowing listing.\\n$ kubectl get instance my-postgres-db -o yaml\\napiVersion: servicecatalog.k8s.io/v1alpha1kind: ServiceInstance\\n...\\nstatus:  asyncOpInProgress: false\\n  conditions:Listing 18.12 A ServiceInstance manifest: database-instance.yaml\\nListing 18.13 Inspecting the status of a ServiceInstanceYou’re giving this \\nInstance a name.\\nThe ServiceClass \\nand Plan you want\\nAdditional parameters \\npassed to the broker'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 556}, page_content='525 Extending Kubernetes with the Kubernetes Service Catalog\\n  - lastTransitionTime: 2017-05-17T13:57:22Z\\n    message: The instance was provisioned successfully    \\n    reason: ProvisionedSuccessfully                           status: \"True\"\\n    type: Ready                   \\nA database instance is now running somewher e, but how do you use it in your pods?\\nTo do that, you need to bind it.\\nBINDING  A SERVICE INSTANCE\\nTo use a provisioned ServiceInstance in your pods, you create a ServiceBindingresource, as shown in the following listing.\\napiVersion: servicecatalog.k8s.io/v1alpha1\\nkind: ServiceBinding\\nmetadata:  name: my-postgres-db-binding\\nspec:\\n  instanceRef:                              name: my-postgres-db                \\n  secretName: postgres-secret           \\nThe listing shows that you’re defining a ServiceBinding resource called my-postgres-\\ndb-binding , in which you’re referencing the my-postgres-db  service instance you\\ncreated earlier. You’re also specifying a na me of a Secret. You want the Service Catalog\\nto put all the necessary credentials for ac cessing the service instance into a Secret\\ncalled postgres-secret . But where are you binding the ServiceInstance to your pods?\\nNowhere, actually.\\n Currently, the Service Catalog doesn’t yet make it possible to inject pods with the\\nServiceInstance’s credentials. This will be possible when a new Kubernetes feature\\ncalled PodPresets is available. Until then, you can choose a name for the Secret\\nwhere you want the credentials to be stored in and mount that Secret into your pods\\nmanually.\\n When you submit the ServiceBinding resour ce from the previous listing to the Ser-\\nvice Catalog API server, the controller will  contact the Database broker once again\\nand create a binding for the ServiceInsta nce you provisioned earlier. The broker\\nresponds with a list of credentials and othe r data necessary for connecting to the data-\\nbase. The Service Catalog creates a new Secr et with the name you specified in the\\nServiceBinding resource and stores  all that data in the Secret. \\nUSING THE NEWLY  CREATED  SECRET  IN CLIENT  PODS\\nThe Secret created by the Service Catalog system can be mounted into pods, so they\\ncan read its contents and use them to conne ct to the provisioned service instance (a\\nPostgreSQL database in the example). The Se cret could look like the one in the fol-\\nlowing listing.Listing 18.14 A ServiceBinding: my-postgres-db-binding.yamlThe database was \\nprovisioned successfully.\\nIt’s ready to be used.\\nYou’re referencing the \\ninstance you created \\nearlier.\\nYou’d like the credentials \\nfor accessing the service stored in this Secret.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 557}, page_content='526 CHAPTER  18 Extending Kubernetes\\n$ kubectl get secret postgres-secret -o yaml\\napiVersion: v1data:\\n  host: <base64-encoded hostname of the database>     \\n  username: <base64-encoded username>                   password: <base64-encoded password>                 \\nkind: Secret\\nmetadata:\\n  name: postgres-secret\\n  namespace: default  ...\\ntype: Opaque\\nBecause you can choose the name of the Se cret yourself, you can deploy pods before\\nprovisioning or binding the service. As yo u learned in chapter 7, the pods won’t be\\nstarted until such a Secret exists. \\n If necessary, multiple bindings can be created for different pods. The service bro-\\nker can choose to use the same set of cred entials in every binding, but it’s better to\\ncreate a new set of credentials for every bi nding instance. This way, pods can be pre-\\nvented from using the service by de leting the ServiceBinding resource.\\n18.2.5 Unbinding and deprovisioning\\nOnce you no longer need a ServiceBinding, you can delete it the way you delete other\\nresources:\\n$ kubectl delete servicebinding my-postgres-db-binding\\nservicebinding \"my-postgres-db-binding\" deleted\\nWhen you do this, the Service Catalog controller will delete the Secret and call the bro-\\nker to perform an unbinding op eration. The service instance (in your case a PostgreSQL\\ndatabase) is still running. You can therefore create a new ServiceBinding if you want.\\n But if you don’t need the database instan ce anymore, you should delete the Service-\\nInstance resource also:\\n$ kubectl delete serviceinstance my-postgres-db\\nserviceinstance \"my-postgres-db \" deleted\\nDeleting the ServiceInstance resource caus es the Service Catalog to perform a depro-\\nvisioning operation on the service broker. Again, exactly what that means is up to the\\nservice broker, but in your case, the broker  should shut down the PostgreSQL data-\\nbase instance that it created when we provisioned the service instance.\\n18.2.6 Understanding what the Service Catalog brings\\nAs you’ve learned, the Service Catalog en ables service providers make it possible to\\nexpose those services in any Kubernetes cluste r by registering the broker in that cluster.Listing 18.15 A Secret holding the credentia ls for connecting to the service instance\\nThis is what the pod \\nshould use to connect to \\nthe database service.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 558}, page_content='527 Platforms built on top of Kubernetes\\nF o r  e x a m p l e ,  I ’ v e  b e e n  i n v o l v e d  w i t h  t h e Service Catalog since early on and have\\nimplemented a broker, which makes it triv ial to provision messaging systems and\\nexpose them to pods in a Kubernetes clus ter. Another team has implemented a broker\\nthat makes it easy to provision Amazon Web Services. \\n In general, service brokers allow easy provisioning and exposing of services in\\nKubernetes and will make Kubernetes an even more awesome platform for deploying\\nyour applications. \\n18.3 Platforms built on top of Kubernetes\\nI’m sure you’ll agree that Kubernetes is a gr eat system by itself. Given that it’s easily\\nextensible across all its components, it’s no wonder companies that had previously\\ndeveloped their own custom platforms are now re-implementing them on top of\\nKubernetes. Kubernetes is, in fact, becomi ng a widely accepted foundation for the\\nnew generation of Platform-as-a-Service offerings.\\n Among the best-known PaaS systems built on Kubernetes are Deis Workflow and\\nRed Hat’s OpenShift. We’ll do a quick overvi ew of both systems to give you a sense of\\nwhat they offer on top of all the aw esome stuff Kubernetes already offers.\\n18.3.1 Red Hat OpenShift Container Platform\\nRed Hat OpenShift is a Platform-as-a-Servic e and as such, it has a strong focus on\\ndeveloper experience. Among its goals are enabling rapid development of applica-\\ntions, as well as easy deployment, scalin g, and long-term maintenance of those apps.\\nOpenShift has been around much longer than Kubernetes. Versions 1 and 2 were\\nbuilt from the ground up and had nothing to  do with Kubernetes, but when Kuberne-\\ntes was announced, Red Hat decided to re build OpenShift version 3 from scratch—\\nthis time on top of Kubernetes. When a company such as Red Hat decides to throw\\naway an old version of their software and bu ild a new one on top of  an existing tech-\\nnology like Kubernetes, it should be cl ear to everyone how great Kubernetes is.\\n Kubernetes automates rollouts and applicat ion scaling, whereas OpenShift also auto-\\nmates the actual building of application im ages and their automatic deployment with-\\nout requiring you to integrate a Continuous Integration solution into your cluster. \\n OpenShift also provides user and gro up management, which allows you to run a\\nproperly secured multi-tenant Kubernetes cluster, where individual users are only\\nallowed to access their own Kubernetes na mespaces and the apps running in those\\nnamespaces are also fully network-isol ated from each other by default. \\nINTRODUCING  ADDITIONAL  RESOURCES  AVAILABLE  IN OPENSHIFT\\nOpenShift provides some additional API obje cts in addition to all those available in\\nKubernetes. We’ll explain them in the next few paragraphs to give you a good over-\\nview of what OpenShift does and what it provides.\\n The additional resources include\\n\\uf0a1Users & Groups\\n\\uf0a1Projects'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 559}, page_content='528 CHAPTER  18 Extending Kubernetes\\n\\uf0a1Templates\\n\\uf0a1BuildConfigs\\n\\uf0a1DeploymentConfigs\\n\\uf0a1ImageStreams\\n\\uf0a1Routes\\n\\uf0a1And others\\nUNDERSTANDING  USERS, GROUPS , AND PROJECTS\\nWe’ve said that OpenShift provides a prop er multi-tenant environment to its users.\\nUnlike Kubernetes, which doesn’t have an AP I object for representing an individual\\nuser of the cluster (but does have Servic eAccounts that represent services running in\\nit), OpenShift provides powerful user mana gement features, which make it possible to\\nspecify what each user can do and what they  cannot. These features pre-date the Role-\\nBased Access Control, which is now th e standard in vanilla Kubernetes.\\n Each user has access to certain Projects, which are nothing more than Kubernetes\\nNamespaces with additional annotations. Us ers can only act on resources that reside\\nin the projects the user has access to. Acce ss to the project is granted by a cluster\\nadministrator. \\nINTRODUCING  APPLICATION  TEMPLATES\\nKubernetes makes it possible to deploy a set of resources through a single JSON or\\nYAML manifest. OpenShift takes this a step further by allowing that manifest to be\\nparameterizable. A parameterizable list in OpenShift is called a Template ; it’s a list of\\nobjects whose definitions can include placeh olders that get replaced with parameter\\nvalues when you process and then instantiate a template (see figure 18.8).\\nThe template itself is a JSON or YAML file containing a list of parameters that are ref-\\nerenced in resources defined in that same  JSON/YAML. The template can be stored\\nin the API server like any other object. Befo re a template can be instantiated, it needsTemplate\\nParameters\\nAPP_NAME=\"kubia\"\\nVOL_CAPACITY=\"5 Gi\"...Pod\\nname: $(APP_NAME)\\nService\\nname: $(APP_NAME)Template\\nPod\\nname: kubia\\nService\\nname: kubiaPod\\nname: kubia\\nService\\nname: kubiaProcess Create\\nFigure 18.8 OpenShift templates'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 560}, page_content='529 Platforms built on top of Kubernetes\\nto be processed. To process a template, you supply the values for the template’s\\nparameters and then OpenShift replaces the references to the parameters with those\\nvalues. The result is a processed template, which is exactly like a Kubernetes resource\\nlist that can then be created with a single POST request.\\n OpenShift provides a long list of pre- fabricated templates that allow users to\\nquickly run complex applications by specifying  a few arguments (or none at all, if the\\ntemplate provides good defaults for thos e arguments). For example, a template can\\nenable the creation of all the Kubernetes resources necessary to run a Java EE appli-\\ncation inside an Application Server, whic h connects to a back-end database, also\\ndeployed as part of that same template. A ll those components can be deployed with a\\nsingle command.\\nBUILDING  IMAGES  FROM SOURCE  USING  BUILDCONFIGS\\nOne of the best features of OpenShift is the ability to have OpenShift build and imme-\\ndiately deploy an application in the OpenShift cluster by pointing it to a Git repositoryholding the application’s source code. You don’t need to build the container image at\\na l l —O p en Sh i f t do es  t h a t fo r y o u . T h i s  is  d on e  by  c re a ti n g  a  re s ou r ce  ca l l ed  B ui l d -\\nConfig, which can be configured to trigge r builds of container images immediately\\nafter a change is committed to  the source Git repository. \\n Although OpenShift doesn’t monitor the Gi t repository itself, a hook in the repos-\\nitory can notify OpenShift of the new comm it. OpenShift will then pull the changes\\nfrom the Git repository and start the bu ild process. A build mechanism called Source\\nTo Image  can detect what type of application is in the Git repository and run the\\nproper build procedure for it. For example, if it detects a pom.xml file, which is used\\nin Java Maven-formatted projects, it runs a Maven build. The re sulting artifacts are\\npackaged into an appropriate container im age, and are then pushed to an internal\\ncontainer registry (provided by OpenShift) . From there, they can be pulled and run\\nin the cluster immediately. \\n By creating a BuildConfig object, developers can thus point to a Git repo and not\\nworry about building container images. De velopers have almost no need to know\\nanything about containers. Once the ops team deploys an OpenShift cluster and\\ngives developers access to it, those develo pers can develop their code, commit, and\\npush it to a Git repo, the same way they used  to before we started packaging apps into\\ncontainers. Then OpenShift takes care of building, deploying, and managing apps\\nfrom that code.\\nAUTOMATICALLY  DEPLOYING  NEWLY  BUILT IMAGES  WITH DEPLOYMENT CONFIGS\\nOnce a new container image is built, it can also automatically be deployed in the clus-\\nter. This is enabled by creating a Deploy mentConfig object and pointing it to an\\nImageStream. As the name suggests, an Im ageStream is a stream of images. When an\\nimage is built, it’s added to the ImageStr eam. This enables the DeploymentConfig to\\nnotice the newly built image and allows it to  take action and initiate a rollout of the\\nnew image (see figure 18.9).'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 561}, page_content='530 CHAPTER  18 Extending Kubernetes\\nA DeploymentConfig is almost identical to the Deployment object in Kubernetes, but\\nit pre-dates it. Like a Deployment object, it has a configurable strategy for transition-\\ning between Deployments. It contains a pod template used to create the actual pods,\\nbut it also allows you to configure pre- an d post-deployment hooks. In contrast to a\\nKubernetes Deployment, it creates Replicat ionControllers instead of ReplicaSets and\\nprovides a few additional features.\\nEXPOSING  SERVICES  EXTERNALLY  USING  ROUTES\\nEarly on, Kubernetes didn’t provide Ingress objects. To expose Services to the outsideworld, you needed to use \\nNodePort  or LoadBalancer -type Services. But at that time,\\nOpenShift already provided a better option through a Route resource. A Route is sim-\\nilar to an Ingress, but it provides addition al configuration related to TLS termination\\nand traffic splitting. \\n Similar to an Ingress controller, a Route needs a Router, which is a controller that\\nprovides the load balancer or proxy. In cont rast to Kubernetes, the Router is available\\nout of the box in OpenShift. \\nTRYING  OUT OPENSHIFT\\nIf you’re interested in trying out OpenShif t, you can start by using Minishift, which is\\nthe OpenShift equivalent of Minikube, or  you can try OpenShift Online Starter at\\nhttps:/ /manage.openshift.com , which is a free multi-tenant, hosted solution provided\\nto get you started with OpenShift. \\n18.3.2 Deis Workflow and Helm\\nA company called Deis, which has recently been acquired by Microsoft, also provides aPaaS called Workflow, which is also built on top of Kubernetes. Besides Workflow,Pods\\nBuilder podReplication\\nControllerBuildConﬁgGit repo\\nDeploymentConﬁgImageStream\\nBuild trigger\\nClones Git repo, builds new\\nimage from source, and addsit to the ImageStreamWatches for new images in ImageStream\\nand rolls out new version (similarly to aDeployment)\\nFigure 18.9 BuildConfigs and DeploymentConfigs in OpenShift'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 562}, page_content='531 Platforms built on top of Kubernetes\\nthey’ve also developed a tool  called Helm, which is gaining traction in the Kubernetes\\ncommunity as a standard way of deploying ex isting apps in Kubernetes. We’ll take a\\nbrief look at both.\\nINTRODUCING  DEIS WORKFLOW\\nYou can deploy Deis Workflow to any existing Kubernetes cluster (unlike OpenShift,which is a complete cluster with a modified API server and other Kubernetes compo-\\nnents). When you run Workflow, it creates a set of Services and ReplicationControllers,\\nwhich then provide developers with a si mple, developer-friendly environment. \\n Deploying new versions of your app is triggered by pushing your changes with \\ngit\\npush  deis  master  and letting Workflow take care of the rest. Similar to OpenShift,\\nWorkflow also provides a source to image mechanism, application rollouts and roll-\\nbacks, edge routing, and also log aggreg ation, metrics, and alerting, which aren’t\\navailable in core Kubernetes. \\n To run Workflow in your Kubernetes cluste r, you first need to install the Deis Work-\\nflow and Helm CLI tools and then install Workflow into your cluster. We won’t go into\\nhow to do that here, but if you’d lik e to learn more, visit the website at https:/ /deis\\n.com/workflow . What we’ll explore here is the He lm tool, which can be used without\\nWorkflow and has gained po pularity in the community.\\nDEPLOYING  RESOURCES  THROUGH  HELM\\nHelm is a package manager for Kubernetes (similar to OS package managers like yum\\nor apt in Linux or homebrew  in MacOS). \\n Helm is comprised of two things:\\n\\uf0a1A helm  CLI tool (the client).\\n\\uf0a1Tiller, a server component running as a Pod inside the Kubernetes cluster.\\nThose two components are used to deploy and manage application packages in a\\nKubernetes cluster. Helm application pack ages are called Charts. They’re combined\\nwith a Config, which contains configuratio n information and is merged into a Chart\\nto create a Release, which is a running instance of an application (a combined Chartand Config). You deploy and manage Releases using the \\nhelm  CLI tool, which talks to\\nthe Tiller server, which is the component that creates all the necessary Kubernetes\\nresources defined in the Chart, as shown in figure 18.10.\\n You can create charts yourself and keep them on your local disk, or you can use\\nany existing chart, which is available in the growing list of helm charts maintained by\\nthe community at https:/ /github.com/kubernetes/charts . The list includes charts for\\napplications such as PostgreSQL, MySQL, MariaDB, Magento, Memcached, MongoDB,\\nOpenVPN, PHPBB, RabbitMQ, Redis, WordPress, and others.\\n Similar to how you don’t build and insta ll apps developed by other people to your\\nLinux system manually, you probably don’t want to build and manage your own\\nKubernetes manifests for such applications, right? That’s why you’ll want to use Helm\\nand the charts available in the GitHub repository I mentioned.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 563}, page_content='532 CHAPTER  18 Extending Kubernetes\\nWhen you want to run a PostgreSQL or a MySQL database in your Kubernetes cluster,\\ndon’t start writing manifests for them. Instea d, check if someone else has already gone\\nthrough the trouble and prepared a Helm chart for it. \\n Once someone prepares a Helm chart for a specific application and adds it to the\\nHelm chart GitHub repo, installing the whol e application takes a single one-line com-\\nmand. For example, to run My SQL in your Kubernetes clus ter, all you need to do is\\nclone the charts Git repo to your local machine and run the following command (pro-\\nvided you have Helm’s CLI tool and Tiller running in your cluster):\\n$ helm install --name my-database stable/mysql\\nThis will create all the necessary Deployment s, Services, Secrets, and PersistentVolu-\\nmeClaims needed to run MySQL in your cl uster. You don’t need to concern yourself\\nwith what components you need and how to  configure them to run MySQL properly.\\nI’m sure you’ll agree this is awesome.\\nTIP One of the most interesting charts available in the repo is an OpenVPN\\nchart, which runs an OpenVPN server inside your Kubernetes cluster and\\nallows you to enter the pod network through VPN and access Services as ifyour local machine was a pod in the cluste r. This is useful when you’re devel-\\noping apps and running them locally.\\nThese were several examples of how Kubern etes can be extended and how companies\\nlike Red Hat and Deis (now Microsoft) have extended it. Now go and start riding the\\nKubernetes wave yourself!Kubernetes clusterChart\\nand\\nConﬁg\\nHelm\\nCharts\\n(ﬁles on\\nlocal disk)Tiller\\n(pod)Deployments,\\nServices, and\\nother objectshelm\\nCLI toolManages\\nchartsCombines Chart andConﬁg into a ReleaseCreates Kubernetes objects\\ndeﬁned in the Release\\nFigure 18.10 Overview of Helm'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 564}, page_content='533 Summary\\n18.4 Summary\\nThis final chapter has shown you how you ca n go beyond the existing functionalities\\nKubernetes provides and how companies lik e Dies and Red Hat have done it. You’ve\\nlearned how\\n\\uf0a1Custom resources can be registered in  the API server by creating a Custom-\\nResourceDefinition object.\\n\\uf0a1Instances of custom objects can be stor ed, retrieved, updated, and deleted with-\\nout having to change the API server code.\\n\\uf0a1A custom controller can be implemen ted to bring those objects to life.\\n\\uf0a1Kubernetes can be extended with cust om API servers through API aggregation.\\n\\uf0a1Kubernetes Service Catalog makes it possib le to self-provision external services\\nand expose them to pods running in the Kubernetes cluster.\\n\\uf0a1Platforms-as-a-Service built on top of Ku bernetes make it easy to build contain-\\nerized applications inside the same Ku bernetes cluster that then runs them. \\n\\uf0a1A package manager called Helm makes de ploying existing apps without requir-\\ning you to build resource manifests for them.\\nThank you for taking the time to read thro ugh this long book. I hope you’ve learned\\nas much from reading it as I have from writing it.'),\n",
       "  Document(metadata={'source': '../pdf_parsing/pdf_source/Kubernetes_in_Action.pdf', 'page': 565}, page_content='534appendix A\\nUsing kubectl\\nwith multiple clusters\\nA.1 Switching between Minikube and Google Kubernetes \\nEngine\\nThe examples in this book can either be run in a cluster created with Minikube, or\\none created with Google Kubernetes Engine (GKE). If you plan on using both, youneed to know how to switch between them . A detailed explanation of how to use\\nkubectl  with multiple clusters is described in the next section. Here we look at how\\nto switch between Minikube and GKE.\\nSWITCHING  TO MINIKUBE\\nLuckily, every time you start up a Minikube cluster with minikube  start , it also\\nreconfigures kubectl  to use it:\\n$ minikube start\\nStarting local Kubernetes cluster......\\nSetting up kubeconfig...                            \\nKubectl is now configured to use the cluster.       \\nAfter switching from Minikube to GKE, yo u can switch back by stopping Minikube\\nand starting it up again. kubectl  will then be re-configured to use the Minikube clus-\\nter again.\\nSWITCHING  TO GKE\\nTo switch to using the GKE cluster,  you can use the following command:\\n$ gcloud container clusters get-credentials my-gke-cluster\\nThis will configure kubectl  to use the GKE cluster called my-gke-cluster .Minikube sets up kubectl every \\ntime you start the cluster.')],\n",
       " 'intermediate_steps': ['Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* In Kubernetes, a pod is a co-located group of containers that represents the basic building block.\\n* Pods can contain one or more containers, but all containers in a pod run on the same worker node.\\n* Running multiple containers together in a single pod is necessary for certain applications.\\n\\n**Important Details:**\\n\\n* Containers are designed to run only a single process per container (unless the process spawns child processes).\\n* Running multiple unrelated processes in a single container requires manual management, including restarting individual processes if they crash and logging output from each process.\\n* Pods provide a convenient way to manage groups of containers as a single unit, making it easier to deploy and operate applications that require multiple processes.\\n\\n**Key Takeaways:**\\n\\n* Pods are the basic building block of Kubernetes, and running multiple containers together in a pod is necessary for certain types of applications.\\n* Containers should be designed to run only one process per container, unless the process spawns child processes.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Because processes cannot be grouped into a single container, Kubernetes uses \"pods\" as a higher-level construct to bind multiple containers together.\\n2. A pod allows related processes to run together with shared resources (almost) identical to running in a single container, while maintaining some isolation between them.\\n\\n**Important Details:**\\n\\n1. **Partial Isolation**: Containers within the same pod share certain resources (e.g., network interfaces, hostname), but not all resources (e.g., filesystems).\\n2. **Shared Resources**: Containers in the same pod share:\\n\\t* Network and UTS namespaces\\n\\t* IPC namespace\\n\\t* PID namespace (optional)\\n3. **Filesystem Isolation**: By default, each container\\'s filesystem is fully isolated from others, but can be shared using Kubernetes Volumes.\\n4. **IP and Port Space Sharing**: Containers in the same pod share the same IP address and port space, requiring processes to avoid binding to the same port numbers to prevent conflicts.\\n5. **Port Conflicts**: Containers of different pods cannot run into port conflicts.\\n\\nThese points summarize the key concepts related to Kubernetes pods and their resource sharing capabilities.',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* In a Kubernetes cluster, all pods reside in a single flat network-address space, where every pod can access every other pod at its IP address.\\n* Communication between pods is simple and doesn't require NAT gateways or complex routing.\\n* Each pod gets its own IP address and behaves like a physical host or VM.\\n\\n**Important Details:**\\n\\n* The flat network allows containers within the same pod to communicate with each other, just like processes on the same machine.\\n* Pods are organized into separate machines, each hosting only a specific app or tightly related components.\\n* This approach is more efficient than cramming multiple apps onto a single host, as pods are relatively lightweight and don't incur significant overhead.\\n* Each pod has its own IP address, which can be accessed by other pods in the cluster.\\n\\n**Key Takeaways:**\\n\\n* Pods provide a simple and efficient way to organize containers across a Kubernetes cluster.\\n* The flat network allows for easy communication between containers within the same pod or across different pods.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. A multi-tier application (frontend server and backend database) should not be configured as a single pod in Kubernetes.\\n2. Running both components in a single pod can lead to underutilization of computational resources and make it difficult to scale individual containers.\\n\\n**Important Details:**\\n\\n* Containers within the same pod always run co-located, which means they will always be on the same machine.\\n* Splitting a multi-tier application into multiple pods allows Kubernetes to schedule each component on a separate node, improving resource utilization.\\n* Scaling a pod with both frontend and backend containers can lead to unnecessary scaling of the backend container, as it may not require the same level of scaling as the frontend.\\n* A single pod should be used when an application consists of one main process and one or more complementary processes that are tightly coupled.\\n\\n**Key Takeaways:**\\n\\n* Use separate pods for each component in a multi-tier application to enable individual scaling and improve resource utilization.\\n* Consider using multiple containers within a single pod only when the components are tightly coupled and form a single, cohesive process.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Pods in Kubernetes can contain multiple containers, which can be useful for running related tasks together.\\n2. Examples of sidecar containers include log rotators, data processors, and communication adapters that work alongside a main container.\\n3. When deciding whether to run multiple containers in the same pod or separate pods, consider:\\n\\t* Whether they need to run on the same host\\n\\t* Whether they represent a single whole or are independent components\\n\\t* Whether they must be scaled together or individually\\n\\n**Important Details:**\\n\\n1. Pods should only contain one container unless there is a specific reason for them to be in the same pod.\\n2. This chapter will focus on single-container pods, but multiple containers in the same pod will be covered later (in Chapter 6).\\n3. A figure (Figure 3.4) illustrates the concept that a container should not run multiple processes and a pod should not contain multiple containers unless they need to run on the same machine.\\n\\n**Key Takeaways:**\\n\\n* Pods can be used to group related containers together\\n* Considerations for running multiple containers in the same pod include host requirements, component relationships, and scaling needs\\n* Single-container pods are the focus of this chapter, but multiple containers will be covered later.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes resources can be created by posting a JSON or YAML manifest to the Kubernetes REST API endpoint.\\n2. Defining all Kubernetes objects from YAML files allows for version control, which has several benefits.\\n3. To configure all aspects of each resource type, you need to understand the Kubernetes API object definitions.\\n\\n**Important Details:**\\n\\n* The `kubectl get` command with the `-o yaml` option can be used to retrieve a YAML definition of an existing pod.\\n* A YAML descriptor for a pod includes metadata (name, labels, annotations, etc.) and specification (list of containers, volumes, etc.).\\n* Understanding the Kubernetes API object definitions is necessary to configure all aspects of each resource type.\\n\\n**Key Concepts:**\\n\\n* YAML descriptors\\n* Kubernetes API object definitions\\n* Version control using YAML files',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The text describes the structure of a Kubernetes Pod definition in YAML format.\\n2. A Pod definition consists of several parts, including metadata, spec, and status.\\n3. The spec section contains information about the pod's containers, volumes, and other data.\\n\\n**Important Details:**\\n\\n* The Pod definition includes fields such as `terminationMessagePath`, `dnsPolicy`, `restartPolicy`, and `serviceAccount`.\\n* The `spec` section has several important sub-sections, including:\\n\\t+ `containers`: lists of containers running in the pod.\\n\\t+ `volumes`: list of volumes attached to the pod.\\n\\t+ `volumeMounts`: list of volume mounts for each container.\\n* The `status` section provides detailed information about the pod's status and its containers.\\n\\n**Key Takeaways:**\\n\\n* Understanding the structure of a Pod definition is crucial for creating new pods in Kubernetes.\\n* The YAML required to create a new pod is often shorter than what is shown in this example.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Creating Pods from YAML or JSON Descriptors**\\n\\n* A Kubernetes pod's status contains runtime data, but this part is not needed when creating a new pod.\\n* The typical structure of a Kubernetes API object consists of three parts: metadata, spec, and status.\\n\\n**Basic YAML Descriptor for a Pod**\\n\\n* To create a simple YAML descriptor for a pod, you can use the following basic YAML:\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: kubia-manual\\nspec:\\n  containers:\\n  - image: luksa/kubia\\n    name: kubia\\n    ports:\\n    - containerPort: 8080\\n      protocol: TCP\\n```\\n* This YAML descriptor conforms to the v1 version of the Kubernetes API and describes a pod with a single container based on the `luksa/kubia` image.\\n* The pod has a name (`kubia-manual`) and the container is listening on port 8080.\\n\\n**Important Details**\\n\\n* Specifying ports in the pod definition is purely informational and does not affect whether clients can connect to the pod through that port or not.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When defining ports for pods in Kubernetes, it's good practice to explicitly list them so that others using the cluster can easily see what ports each pod exposes.\\n2. Using `kubectl explain` can help discover possible API object fields when preparing a manifest.\\n\\n**Important Details:**\\n\\n* Pods can accept connections through any port bound to the `0.0.0.0` address, even if not explicitly listed in the pod spec.\\n* Explicitly defining ports allows assigning names to each port, which can be useful later on.\\n* The `kubectl explain` command prints out an explanation of API objects and lists their attributes.\\n* You can drill deeper into specific attributes using `kubectl explain` (e.g., `$ kubectl explain pod.spec`).\\n* Attributes include fields like `kind`, `metadata`, `spec`, and `status`, which describe the desired behavior, metadata, and status of a pod.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Creating Pods from YAML or JSON Descriptors**\\n\\n* To create a pod from a YAML file, use `kubectl create -f kubia-manual.yaml`\\n* The `-f` flag can be used to create any resource (not just pods) from a YAML or JSON file\\n* After creating the pod, you can retrieve its full YAML definition using `kubectl get po <pod_name> -o yaml`\\n\\n**Retrieving Pod Definition and Listing Pods**\\n\\n* You can use `kubectl get po <pod_name> -o json` to retrieve the pod's definition in JSON format\\n* To list all pods, including their statuses, use `kubectl get pods`\\n* The output will show the pod's name, readiness status, restart count, and age\\n\\n**Viewing Application Logs**\\n\\n* Containerized applications log to standard output and standard error streams\\n* You can view the application logs using a command (not specified in this text snippet)\",\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Retrieving Pod Logs**\\n\\n* Kubernetes provides an easy way to retrieve logs from pods using `kubectl logs`\\n* To get logs, run `$ kubectl logs <pod name>` on your local machine (no need to SSH)\\n* If the pod contains multiple containers, specify the container name with `-c <container name>`, e.g. `$ kubectl logs kubia-manual -c kubia`\\n\\n**Important Details**\\n\\n* Container logs are automatically rotated daily and every time the log file reaches 10MB in size\\n* The `kubectl logs` command only shows log entries from the last rotation\\n* Pod logs are deleted when the pod is deleted; to keep logs after a pod is deleted, set up centralized logging (Chapter 17)\\n\\n**Additional Topics**\\n\\n* Port forwarding is another way to connect to a pod for testing and debugging purposes\",\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes allows you to configure port forwarding to a specific pod for debugging or other reasons.\\n2. You can use the `kubectl port-forward` command to forward your machine's local port to a port in the pod.\\n3. Port forwarding is an effective way to test an individual pod.\\n\\n**Important Details:**\\n\\n* To set up port forwarding, you use the `kubectl port-forward` command with the pod name and the local port number (e.g., `$ kubectl port-forward kubia-manual 8888:8080`).\\n* Once port forwarding is set up, you can connect to the pod through the local port using tools like `curl`.\\n* Port forwarding allows you to bypass services and talk directly to a specific pod.\\n* As the number of pods in your cluster increases, categorizing them into subsets becomes more important (e.g., with microservices architectures).\\n* Labels are used to organize pods into subsets.\\n\\n**Key Concepts:**\\n\\n* Port forwarding\\n* `kubectl port-forward` command\\n* Labels for organizing pods\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* In Kubernetes, multiple copies of the same component (replicas) and different versions/releases can run concurrently, leading to hundreds of pods.\\n* Without organization, this can result in a complex and difficult-to-manage system.\\n* To address this, Kubernetes uses **labels** to organize pods and other resources into smaller groups based on arbitrary criteria.\\n\\n**Important Details:**\\n\\n* Labels are key-value pairs attached to resources (such as pods) that allow for filtering and selection of resources using label selectors.\\n* A resource can have multiple labels with unique keys.\\n* Labels can be added or modified at any time without recreating the resource.\\n* This feature allows developers and system administrators to easily identify which pod is which and perform actions on groups of related pods.',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Adding labels to pods in a microservices system can improve organization and make it easier for developers and ops personnel to understand the system\\'s structure.\\n2. Two types of labels are used: \"app\" (which specifies which app or component the pod belongs to) and \"rel\" (which indicates whether the application is stable, beta, or a canary release).\\n3. These labels allow pods to be organized into two dimensions: horizontally by app and vertically by release.\\n\\n**Important Details:**\\n\\n* A canary release is when a new version of an application is deployed next to the stable version, with only a small fraction of users accessing the new version.\\n* Labels are used to specify which app or component a pod belongs to (e.g. \"app: ui\") and whether it\\'s a stable, beta, or canary release (e.g. \"rel: stable\").\\n* The example shows how labels can be used to organize pods into a structured system, making it easier for developers and ops personnel to understand the system.\\n\\n**Key Terms:**\\n\\n* Canary release\\n* Labels in Kubernetes\\n* Microservices architecture',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Labels can be added to pods in Kubernetes using the `labels` section in a YAML file.\\n2. The `kubectl create` command can be used to create a pod from a YAML file that includes labels.\\n3. Labels can be viewed using the `kubectl get pods` command with the `--show-labels` switch.\\n4. Labels can be filtered and displayed in separate columns using the `-L` switch.\\n\\n**Important Details:**\\n\\n* To add or modify labels on an existing pod, use the `kubectl label` command.\\n* When modifying existing labels, use the `--overwrite` option to avoid errors.\\n* The `creation_method=manual` and `env=data.labels` sections are used in this example to demonstrate labeling pods.\\n\\n**Key Commands:**\\n\\n* `kubectl create -f kubia-manual-with-labels.yaml`: Create a pod from a YAML file that includes labels.\\n* `kubectl get po --show-labels`: View all labels for all pods.\\n* `kubectl get po -L creation_method,env`: Filter and display specific labels in separate columns.\\n* `kubectl label po <pod-name> <label-key>=<label-value>`: Add or modify a label on an existing pod.',\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n* Labels can be attached to resources (such as pods) and updated on existing resources.\\n* Label selectors can be used to select subsets of resources based on certain labels and values.\\n* Label selectors can filter resources based on whether they contain a label with a specific key, value, or neither.\\n\\n**Important Details:**\\n\\n* To list pods using a label selector, use the `kubectl get` command with the `-l` flag followed by the label selector criteria (e.g. `-l creation_method=manual`).\\n* Label selectors can be used to select resources based on:\\n\\t+ Whether they contain a label with a certain key\\n\\t+ Whether they contain a label with a certain key and value\\n\\t+ Whether they contain a label with a certain key, but not the specified value\\n* Examples of using label selectors include:\\n\\t+ Listing all pods created manually (`kubectl get po -l creation_method=manual`)\\n\\t+ Listing all pods that have an `env` label (`kubectl get po -l env`)\\n\\t+ Listing all pods that do not have an `env` label (`kubectl get po -l '!env'`)\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Label Selectors in Kubernetes**\\n\\n* Label selectors can be used to select specific resources (e.g., pods) based on their labels.\\n* The syntax for label selectors includes single quotes around !env, e.g. \\'!env\\'.\\n* Multiple conditions can be included in a label selector using comma-separated criteria, e.g. \\'app=pc,rel=beta\\'.\\n\\n**Examples of Label Selectors**\\n\\n* Selecting all pods with the creation_method label set to any value other than \"manual\": \\'creation_method!=manual\\'\\n* Selecting pods with the env label set to either \"prod\" or \"devel\": \\'env in (prod,devel)\\'\\n* Selecting pods with the env label set to any value other than \"prod\" or \"devel\": \\'env notin (prod,devel)\\'\\n\\n**Using Label Selectors**\\n\\n* Label selectors can be used to select specific resources for listing, deleting, or performing other actions.\\n* They are also used internally by Kubernetes and can be used with the kubectl command.\\n\\nThe text includes a few examples of label selectors in use, including selecting pods based on their app and rel labels.',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* In a Kubernetes cluster, pods are usually scheduled randomly across worker nodes.\\n* However, there may be cases where you want to have some control over where a pod is scheduled, such as when hardware infrastructure is not homogenous or specific resources (e.g. GPU acceleration) are required.\\n\\n**Important Details:**\\n\\n* You should never specify an exact node for a pod to be scheduled on, as this would couple the application to the infrastructure and go against the principles of Kubernetes.\\n* Instead, you can describe the node requirements using node labels and selectors, which allows Kubernetes to select a node that matches those requirements.\\n* Node labels are used to assign metadata to nodes in the cluster, while label selectors are used to match pods with specific node requirements.\\n\\n**Key Concepts:**\\n\\n* Node labels\\n* Label selectors\\n* Pod scheduling constraints\\n\\nOverall, this text highlights the importance of flexibility and scalability in Kubernetes deployments, and provides a way for users to exert some control over where their pods are scheduled without compromising the principles of the platform.',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. Labels can be attached to any Kubernetes object, including nodes, to categorize them for scheduling purposes.\\n2. A node selector can be used in a pod\\'s YAML file to schedule the pod to specific nodes that match certain labels.\\n\\n**Important Details:**\\n\\n* To add a label to a node, use `kubectl label` command (e.g., `$ kubectl label node gke-kubia-85f6-node-0rrx gpu=true`).\\n* To list nodes with a specific label, use `kubectl get nodes -l <label_name>` (e.g., `$ kubectl get nodes -l gpu=true`).\\n* To schedule a pod to specific nodes that match certain labels, add a node selector to the pod\\'s YAML file using `nodeSelector: { <label_name>: \"true\" }`.\\n* In this example, a GPU-enabled node is labeled with `gpu=true`, and a pod is scheduled to run on that node by adding a node selector to its YAML file.\\n\\nOverall, these points highlight how labels can be used to categorize nodes in a Kubernetes cluster and schedule pods to specific nodes based on those labels.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. **Label Selectors**: Labels can be used to select nodes for pod scheduling. A `nodeSelector` field under the `spec` section can be used to schedule a pod to a specific node or group of nodes.\\n2. **Annotating Pods**: In addition to labels, pods and other objects can contain annotations, which are key-value pairs that hold larger pieces of information.\\n3. **Annotations vs Labels**: Annotations are not meant for identifying information like labels, but rather for holding additional data that can be used by tools.\\n\\n**Important Details:**\\n\\n* Label selectors can be used to schedule a pod to an exact node using the `kubernetes.io/hostname` label.\\n* Using a specific hostname label may lead to the pod being unschedulable if the node is offline.\\n* Annotations are automatically added by Kubernetes for certain objects, but others are added manually.\\n* Annotations are commonly used when introducing new features to Kubernetes and can be used to add descriptions or other information about pods or API objects.\\n\\n**Additional Information:**\\n\\n* The importance of label selectors will become more evident in the next two chapters on Replication-Controllers and Services.\\n* Additional ways of influencing which node a pod is scheduled to are covered in chapter 16.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Kubernetes pods can have labels and annotations.\\n2. Labels are short strings used to categorize objects, while annotations can contain larger amounts of data (up to 256 KB).\\n3. Annotations can be added or modified on existing pods using the `kubectl annotate` command.\\n\\n**Important Details:**\\n\\n* The `kubernetes.io/created-by` annotation was deprecated in Kubernetes version 1.8 and will be removed in 1.9.\\n* When adding annotations, it's a good idea to use unique prefixes (e.g., `mycompany.com/someannotation`) to prevent key collisions.\\n* Using the `kubectl describe` command can be used to view annotations added to pods.\\n\\n**Context:**\\n\\nThe text is part of a chapter on Kubernetes pods and containers. The discussion on labels and annotations is likely intended to help users understand how to organize and manage resources in a Kubernetes cluster.\",\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* Kubernetes groups objects into namespaces to provide a scope for object names.\\n* Namespaces allow you to split complex systems into smaller distinct groups or separate resources in a multi-tenant environment.\\n* Resource names only need to be unique within a namespace.\\n\\n**Important Details:**\\n\\n* Not all types of resources are namespaced, with Node being one example that is global and not tied to a single namespace.\\n* The default namespace is the one used when listing resources without specifying a namespace explicitly.\\n* You can list all namespaces in your cluster using `kubectl get ns`.\\n* You can specify a namespace when listing resources using `kubectl get <resource> --namespace <namespace>` (or `-n` instead of `--namespace`).\\n\\n**Key Takeaways:**\\n\\n* Namespaces provide a way to organize and separate resources within a Kubernetes cluster.\\n* Understanding namespaces is important for managing complex systems or multi-tenant environments.',\n",
       "  'Here\\'s a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes has a separate namespace called \"default\" that contains system resources, which helps keep everything organized.\\n2. Namespaces enable users to separate resources into non-overlapping groups, preventing accidental modification or deletion of other users\\' resources.\\n3. Namespaces also allow for access control and resource limiting.\\n\\n**Important Details:**\\n\\n1. To create a namespace, you can post a YAML file to the Kubernetes API server using `kubectl create -f <file>`.\\n2. The YAML file should contain the following structure:\\n\\t* `apiVersion`: v1\\n\\t* `kind`: Namespace\\n\\t* `metadata`:\\n\\t\\t+ `name`: the name of the namespace\\n3. Alternatively, you can use the `kubectl create namespace` command to create a namespace.\\n4. Namespaces are Kubernetes resources that can be created and managed like other resources.\\n\\n**Key Concepts:**\\n\\n1. Namespaces: separate resources into non-overlapping groups for organization and access control.\\n2. Access control: allow only certain users to access particular resources.\\n3. Resource limiting: limit the amount of computational resources available to individual users.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Namespaces are used to group resources and isolate them from others.\\n2. A namespace can be created using `kubectl create namespace` command.\\n3. Resources can be created, updated, or deleted in a specific namespace by specifying the namespace name with the `-n` flag.\\n\\n**Important Details:**\\n\\n* Namespaces have their own set of rules for naming conventions (e.g., no dots allowed).\\n* When creating resources in another namespace, you need to specify the namespace using `kubectl create -f <file> -n <namespace>` or by adding a `metadata.namespace` entry to the YAML manifest.\\n* To switch between namespaces quickly, an alias can be set up: `alias kcd='kubectl config set-context $(kubectl config current-context) --namespace'`.\\n* Namespaces do not provide isolation of running objects (e.g., pods in different namespaces can still communicate with each other if the networking solution does not provide inter-namespace network isolation).\\n\\n**Key Takeaways:**\\n\\n* Namespaces are a way to group resources and isolate them from others.\\n* Resources can be created, updated, or deleted in specific namespaces using `kubectl` commands.\\n* Networking solutions may affect whether pods in different namespaces can communicate with each other.\",\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Pods can communicate with each other within the same namespace.\\n2. Stopping and removing pods is necessary when they are no longer needed.\\n3. There are three ways to delete pods: by name, using label selectors, or deleting the whole namespace.\\n\\n**Important Details:**\\n\\n* When deleting a pod, Kubernetes sends a SIGTERM signal to the process and waits 30 seconds for it to shut down gracefully. If it doesn't shut down in time, the process is killed through SIGKILL.\\n* To delete multiple pods by name, specify their names separated by spaces (e.g., `kubectl delete po pod1 pod2`).\\n* Label selectors can be used to delete pods based on specific labels they have been assigned.\\n* Deleting a namespace will also delete all its associated pods and resources.\\n\\n**Key Terms:**\\n\\n* SIGTERM signal\\n* SIGKILL signal\\n* Label selector\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Deleting an entire namespace will delete all resources, including pods.\\n2. To delete all pods in a namespace while keeping the namespace intact, use the `--all` option with the `kubectl delete` command.\\n\\n**Important Details:**\\n\\n* The `kubectl delete ns <namespace>` command deletes the entire namespace and all its resources, including pods.\\n* Using `kubectl delete po --all` deletes all pods in the current namespace, but keeps the namespace intact.\\n* After deleting all pods, verify that no pods are still running using `kubectl get pods`.\\n* This process can also be done through the UI by selecting and deleting all pods with a specific label selector (e.g. `rel=canary`).',\n",
       "  \"Here's a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. Pods in Kubernetes can automatically recreate themselves if deleted, due to ReplicationControllers.\\n2. To delete all resources (including pods, ReplicationControllers, and Services) in a namespace, use the command `kubectl delete all --all`.\\n3. This command deletes most resources, but not all (e.g., Secrets).\\n\\n**Important Details:**\\n\\n1. When deleting a pod created by a ReplicationController, you must also delete the ReplicationController itself.\\n2. The `kubectl delete all --all` command will print the name of every resource it deletes.\\n3. Certain resources, like Secrets, are preserved and need to be deleted explicitly.\\n4. The `kubernetes Service` may be recreated automatically after being deleted by the `kubectl delete all --all` command.\\n\\n**Key Concepts:**\\n\\n1. Pods\\n2. ReplicationControllers\\n3. Services\\n4. Secrets\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Key Concepts in Pods**\\n\\n* A pod can run multiple processes and behaves similarly to physical hosts.\\n* YAML or JSON descriptors can be used to create, examine, and specify the configuration of a pod.\\n* Labels and label selectors are essential for organizing and performing operations on multiple pods at once.\\n* Node labels and selectors enable scheduling pods based on specific features.\\n* Annotations allow attaching larger data blobs to pods.\\n\\n**Cluster Management**\\n\\n* Namespaces enable different teams to use the same cluster as if they were using separate Kubernetes clusters.\\n* The `kubectl explain` command provides quick access to information about any Kubernetes resource.\\n\\n**Next Chapter**\\n\\n* The next chapter will cover ReplicationControllers and other resources that manage pods.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* In Kubernetes, you typically don't create pods directly, but rather use other resources like Replication Controllers or Deployments to manage them.\\n* These resources automatically create and manage pods, keeping them up and running without manual intervention.\\n* This chapter will cover various ways to manage pods, including:\\n\\t+ Keeping pods healthy\\n\\t+ Running multiple instances of the same pod\\n\\t+ Automatically rescheduling pods after a node fails\\n\\t+ Scaling pods horizontally\\n\\t+ Running system-level pods on each cluster node\\n\\t+ Running batch jobs and scheduling them to run periodically or once in the future\\n\\n**Important Details:**\\n\\n* Unmanaged pods are created directly, but this is not recommended as it doesn't provide automatic management.\\n* Replication Controllers and Deployments are used to create and manage managed pods, which stay up and running automatically.\",\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes can automatically restart containers if they fail, but not pods that are running on a failed node.\\n2. To ensure pod health, Kubernetes uses liveness probes to check if a container is still alive and restarts it if necessary.\\n\\n**Important Details:**\\n\\n* A pod's containers will be restarted by the Kubelet if they crash or exit unexpectedly.\\n* However, apps can stop working without crashing, such as due to memory leaks or infinite loops.\\n* To address this, Kubernetes uses liveness probes to periodically check a container's health and restart it if necessary.\\n* Liveness probes can be specified for each container in the pod's specification and can use one of three mechanisms:\\n\\t+ HTTP GET probe: performs an HTTP GET request on the container's IP address and port.\\n\\t+ Other mechanisms (not specified in this text).\\n* Readiness probes are also supported by Kubernetes, but will be covered in a separate chapter.\\n\\n**Key Takeaways:**\\n\\n* Kubernetes can automatically restart containers if they fail, but not pods that are running on a failed node.\\n* Liveness probes are used to ensure pod health and can be specified for each container in the pod's specification.\",\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. A liveness probe is a mechanism used by Kubernetes to determine whether a container is running correctly.\\n2. If the HTTP response code from a liveness probe is 2xx or 3xx, it is considered successful. Otherwise, the container will be restarted.\\n\\n**Types of Liveness Probes:**\\n\\n1. TCP Socket probe: attempts to open a TCP connection to a specified port in the container.\\n2. Exec probe: executes an arbitrary command inside the container and checks its exit status code.\\n3. HTTP-based liveness probe: sends an HTTP GET request to a specified path and port in the container.\\n\\n**Example: Creating an HTTP-Based Liveness Probe**\\n\\n1. A Node.js app is modified to return a 500 Internal Server Error HTTP status code after the fifth client request.\\n2. A new pod is created with an HTTP GET liveness probe that sends requests to the modified app's web server.\\n3. The YAML file for the pod is shown, specifying the image name, container name, and liveness probe configuration.\\n\\n**Important Details:**\\n\\n* The liveness probe will restart the container if it fails or returns an error response code.\\n* The example uses a Node.js app that is modified to fail artificially after five client requests.\",\n",
       "  \"Here's a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes uses liveness probes to periodically check if a container is still healthy.\\n2. The probe sends HTTP GET requests on port 8080 to determine if the container is running correctly.\\n3. If the container fails to respond or returns an error (in this case, a 500 status code), Kubernetes restarts it.\\n\\n**Important Details:**\\n\\n* Liveness probes are defined in the pod descriptor and start as soon as the container is run.\\n* The probe sends five requests before considering the container unhealthy and restarting it.\\n* You can see the liveness probe in action by creating a pod with a liveness probe, which will restart indefinitely.\\n* To view logs from a previously terminated container, use `kubectl logs` with the `--previous` option.\\n\\n**Key Takeaways:**\\n\\n* Liveness probes are an essential feature of Kubernetes that ensure containers are running correctly.\\n* You can troubleshoot issues by viewing logs from previous container instances using `kubectl logs --previous`.\",\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Pod Status**\\n\\n* The pod \"kubia-liveness\" has terminated with an error (Reason: Error, Exit Code: 137)\\n* It was restarted once\\n* The container is currently running\\n\\n**Liveness Probe**\\n\\n* The liveness probe detected the container as unhealthy and killed it\\n* A new container was created to replace the old one\\n* The exit code 137 indicates that the process was terminated by an external signal (SIGKILL)\\n* The events listed show why the container was killed: Kubernetes detected the container was unhealthy\\n\\n**Configuring Liveness Probe**\\n\\n* Additional properties of the liveness probe can be customized, such as:\\n\\t+ delay: time to wait before starting the probe\\n\\t+ timeout: time allowed for a response from the container\\n\\t+ period: frequency at which the probe is run\\n\\t+ #success and #failure: number of successful or failed probes required to restart the container\\n\\n**Example**\\n\\n* An example YAML file (kubia-liveness-probe-initial-delay.yaml) shows how to set an initial delay for the liveness probe using the `initialDelaySeconds` property.',\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. **Initial Delay**: When running pods, it's essential to set an initial delay (initialDelaySeconds) to account for app startup time. This prevents probes from failing due to the app not being ready.\\n2. **Liveness Probes**: For production pods, define a liveness probe to ensure Kubernetes knows whether your app is alive or not. Without one, Kubernetes will consider the container healthy as long as the process is running.\\n3. **Effective Liveness Probes**: A good liveness probe should check for specific URL paths (e.g., /health) and perform internal status checks to ensure vital components are functioning correctly.\\n\\n**Important Details:**\\n\\n* If an initial delay isn't set, probes may fail, leading to container restarts before the app is ready.\\n* Exit codes 137 and 143 indicate external termination of a process (SIGKILL or SIGTERM).\\n* Liveness probes should not require authentication to prevent infinite restarts.\\n* Probes should only check internal app components and not external factors that can cause failures.\",\n",
       "  \"Here's a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n* Kubernetes will restart containers that crash or fail their liveness probes.\\n* Liveness probes should be lightweight and not consume too many computational resources.\\n* Implementing retry loops in liveness probes is unnecessary, as Kubernetes will retry the probe multiple times before considering it a failed attempt.\\n* To ensure an application is restarted on another node if the original node fails, you need to use a ReplicationController or similar mechanism.\\n\\n**Important Details:**\\n\\n* Liveness probes should complete within 1 second and not consume too much CPU time.\\n* For Java applications, consider using an HTTP GET liveness probe instead of an Exec probe to avoid spinning up a new JVM.\\n* If the node hosting a pod crashes, the Kubernetes Control Plane will create replacements for managed pods (those created by a ReplicationController), but not unmanaged pods (those created directly).\\n* A ReplicationController ensures that its managed pods are always running and creates replacement pods if any disappear.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* A ReplicationController (RC) ensures that a specified number of identical pods are running at all times.\\n* If a pod fails, the RC creates a new one to replace it, but if there's no RC, the pod is not recreated.\\n* The RC constantly monitors the list of running pods and adjusts the number as needed by creating or removing replicas.\\n\\n**Important Details:**\\n\\n* A ReplicationController can manage multiple copies (replicas) of a pod, hence its name.\\n* If too few pods are running, the RC creates new ones from a pod template. If too many are running, it removes excess replicas.\\n* The RC only recreates pods that were created by it; if a pod was created manually or without an RC, it will not be recreated if it fails.\\n\\n**Key Concept:**\\n\\n* ReplicationControllers ensure that the desired number of identical pods is maintained at all times, and recreate lost pods to maintain this balance.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. ReplicationControllers don\\'t operate on \"types\" of pods, but rather on sets of pods that match a certain label selector.\\n2. A ReplicationController\\'s job is to ensure an exact number of pods always matches its label selector by taking action if the actual number doesn\\'t match the desired number.\\n\\n**Important Details:**\\n\\n1. **Reconciliation Loop**: The operation of a ReplicationController involves a reconciliation loop, where it checks if the actual number of pods matches the desired number and takes action accordingly.\\n2. **Three Essential Parts**: A ReplicationController has three essential parts:\\n\\t* **Label Selector**: determines what pods are in its scope\\n\\t* **Replica Count**: specifies the desired number of pods that should be running\\n\\t* **Pod Template**: used when creating new pod replicas\\n\\n**Key Concepts:**\\n\\n1. Label selector: a way to select specific pods based on labels\\n2. Replica count: the desired number of pods that should be running\\n3. Pod template: a template used to create new pod replicas',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. A ReplicationController (RC) can be modified at any time, but only changes to its replica count affect existing pods.\\n2. Changes to the label selector or pod template have no effect on existing pods and only apply to new pods created by the RC.\\n3. A ReplicationController provides features such as:\\n\\t* Ensuring a pod is always running by starting a new one when an existing one goes missing.\\n\\t* Creating replacement replicas for pods that were running on a failed node.\\n\\t* Enabling easy horizontal scaling of pods.\\n\\n**Important Details:**\\n\\n1. When a cluster node fails, the ReplicationController creates new pod instances to replace those that were running on the failed node.\\n2. A pod instance is never relocated to another node; instead, a completely new pod instance is created.\\n3. To create a ReplicationController, you can post a JSON or YAML descriptor to the Kubernetes API server.\\n4. The example provided shows how to create a YAML file called kubia-rc.yaml for a ReplicationController with 3 replicas and a selector that matches pods with the label \"app=kubia\".\\n\\nOverall, this text provides an introduction to ReplicationControllers in Kubernetes, highlighting their key features and benefits, as well as providing an example of how to create one.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* A ReplicationController named \"kubia\" was created from a YAML file using `kubectl create -f kubia-rc.yaml`.\\n* The ReplicationController ensures that three pod instances are always running, matching the label selector `app=kubia`.\\n\\n**Important Details:**\\n\\n* The pod template used to create new pods must have labels that match the label selector of the ReplicationController.\\n* If no selector is specified in the ReplicationController definition, it will be automatically configured from the labels in the pod template.\\n* To prevent infinite creation of new pods, the API server verifies the ReplicationController definition and will not accept it if misconfigured.\\n\\n**Key Actions:**\\n\\n* Create a ReplicationController using `kubectl create -f kubia-rc.yaml`.\\n* List the pods to see the ReplicationController in action (`kubectl get pods`).\\n\\nOverall, this text explains how to use a ReplicationController to manage a set of pods, ensuring that a specified number of instances are always running.',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. The Kubernetes system has successfully created three pods as desired by a ReplicationController.\\n2. When one of the pods is deleted manually, the ReplicationController responds by creating a new pod to replace it.\\n3. The `kubectl get` command can be used to view information about ReplicationControllers, including the desired and current number of pods.\\n\\n**Important Details:**\\n\\n* A ReplicationController ensures that a specified number of identical pods are running at all times.\\n* When a pod is deleted, the ReplicationController creates a new one to replace it, maintaining the desired number of pods.\\n* The `kubectl get` command displays three columns for ReplicationControllers: Desired (the number of pods desired), Current (the actual number of pods), and Ready (the number of pods that are ready).\\n* The `kubectl describe` command can be used to view additional information about a ReplicationController, including its name, namespace, selector, labels, and pod template.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* A Replication Controller has created four running pods for an application named \"kubia\".\\n* The current number of replicas matches the desired number, but this is because one of the pods was terminating and still counted as running.\\n* The Replication Controller creates a new replacement pod when it detects that there are too few pods to meet the desired replica count.\\n\\n**Important Details:**\\n\\n* The Replication Controller is notified about changes to resources and resource lists, including the deletion of a pod.\\n* However, it\\'s not the notification itself that causes the controller to create a new pod, but rather the resulting state - specifically, when there are too few pods to meet the desired replica count.\\n* The events related to this Replication Controller show that it creates a new replacement pod after deleting an existing one.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The text demonstrates how Kubernetes responds to a node failure using ReplicationControllers.\\n* A three-node cluster is simulated by disconnecting one node from the network, mimicking a node failure.\\n\\n**Important Details:**\\n\\n* To simulate a node failure on Google Kubernetes Engine (GKE), one of the nodes\\' network interfaces is shut down using `sudo ifconfig eth0 down`.\\n* The ReplicationController detects that its pods are down and spins up new pods to replace them.\\n* Initially, the status of the disconnected node is shown as \"NotReady\" for a minute or so before being rescheduled.\\n* If the node stays unreachable for several minutes, the status of the pods scheduled to that node changes to \"Unknown\", triggering the ReplicationController to spin up a new pod.\\n\\n**Key Takeaways:**\\n\\n* Kubernetes automatically migrates applications running on a failed node to other machines, eliminating the need for manual intervention.\\n* The ReplicationController plays a crucial role in responding to node failures and ensuring application availability.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. A ReplicationController can automatically bring the actual state of a system to the desired state by creating or deleting pods as needed.\\n2. If a node fails, the system can heal itself automatically without human intervention.\\n3. Pods created by a ReplicationController are not tied to it and can be removed from or added to its scope by changing their labels.\\n\\n**Important Details:**\\n\\n* A ReplicationController manages pods that match its label selector.\\n* Changing a pod's labels can move it out of the scope of a ReplicationController, making it no longer managed by the controller.\\n* If a node running a pod fails, the pod is not rescheduled if it was previously managed by a ReplicationController.\\n* However, if a pod is removed from a ReplicationController's scope, the controller will notice and create a new pod to replace it.\\n\\n**Key Commands:**\\n\\n* `kubectl get pods` to view the status of pods in the system\\n* `gcloud compute instances reset` to reset a failed node\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. A ReplicationController does not care if additional labels are added to its managed pods.\\n2. Changing the labels of a managed pod can cause the ReplicationController to create a new pod to maintain the desired replica count.\\n\\n**Important Details:**\\n\\n* To add or change labels on a pod, use the `kubectl label` command with the `--overwrite` argument to prevent warnings and ensure changes are made.\\n* The `-L app` option can be used with `kubectl get pods` to display the `app` label in a column.\\n* When a managed pod's labels no longer match the ReplicationController's label selector, the controller will create a new pod to maintain the desired replica count.\\n\\n**Key Takeaways:**\\n\\n* A ReplicationController is not concerned with additional labels on its managed pods.\\n* Changing a managed pod's labels can trigger the creation of a new pod by the ReplicationController.\",\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. A ReplicationController can be used to spin up new pods to replace ones that have been removed.\\n2. Pods can be removed from the scope of a ReplicationController by changing their labels or modifying the controller's label selector.\\n3. Changing the label selector of a ReplicationController will cause it to recreate all pods, not just the one(s) that were removed.\\n\\n**Important Details:**\\n\\n* A ReplicationController can spin up new pods to replace ones that have been removed, maintaining a specified number of replicas (in this case, 3).\\n* Pods can be removed from the scope of a ReplicationController by changing their labels or modifying the controller's label selector.\\n* Changing the label selector of a ReplicationController will cause it to recreate all pods, not just the one(s) that were removed.\\n\\n**Key Concepts:**\\n\\n* ReplicationControllers\\n* Label selectors\\n* Pod management\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. A ReplicationController's pod template can be modified at any time, but changing its label selector is not possible.\\n2. Modifying the pod template will only affect new pods created after the change, while existing pods remain unchanged.\\n\\n**Important Details:**\\n\\n* To modify a ReplicationController's pod template, you can use the `kubectl edit` command, which opens the YAML definition of the controller in your default text editor.\\n* You can add or remove labels from the pod template by editing its YAML definition.\\n* Changes to the pod template will only be applied to new pods created after the change, while existing pods must be deleted and replaced with new ones based on the updated template.\\n\\n**Key Takeaways:**\\n\\n* Modifying a ReplicationController's pod template is useful for upgrading pods or making changes that don't require immediate action.\\n* However, as mentioned in chapter 9, there are better ways to upgrade pods, which will be discussed later.\",\n",
       "  \"Here is a summary of the main points and important details from the chunk of text:\\n\\n**Main Points:**\\n\\n1. ReplicationControllers can be used to horizontally scale pods, making it easy to change the number of pod instances.\\n2. Scaling up or down involves changing the `replicas` field in the ReplicationController resource.\\n3. There are two ways to scale a ReplicationController: using the `kubectl scale` command or editing its definition directly.\\n\\n**Important Details:**\\n\\n1. To edit a ReplicationController's definition, use the `kubectl edit` command, which will open a text editor for you to modify the resource.\\n2. You can specify a different text editor to use with `kubectl edit` by setting the `KUBE_EDITOR` environment variable.\\n3. If no text editor is specified, `kubectl edit` will fall back to using the default editor configured through the `EDITOR` environment variable.\\n\\n**Example:**\\n\\n* To scale up a ReplicationController from 3 to 10 instances, you can use either:\\n\\t+ `$ kubectl scale rc kubia --replicas=10`\\n\\t+ `$ kubectl edit rc kubia`, then change the `spec.replicas` field to 10.\",\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. ReplicationControllers are used to manage multiple replicas (copies) of a pod.\\n2. The `kubectl scale` command can be used to change the number of replicas, but it only modifies the desired state of the ReplicationController.\\n3. Deleting a ReplicationController also deletes the associated pods.\\n\\n**Important Details:**\\n\\n* When using `kubectl scale`, you are making a declarative change to the desired state of the ReplicationController, rather than telling Kubernetes what to do.\\n* The `spec.replicas` field in the ReplicationController's definition is modified when scaling up or down.\\n* Deleting a ReplicationController will delete all associated pods, but this can be useful if you want to replace the ReplicationController with another resource (such as a ReplicaSet).\\n* The declarative approach to scaling makes interacting with a Kubernetes cluster easy and reduces the risk of errors.\\n\\n**Key Commands:**\\n\\n* `kubectl scale rc kubia --replicas=3` scales down the number of replicas to 3.\\n* `kubectl delete rc kubia` deletes the ReplicationController and associated pods.\",\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. When deleting a ReplicationController (RC) using `kubectl delete`, you can keep its managed pods running by passing the `--cascade=false` option.\\n2. Deleting an RC with this option leaves the pods unmanaged, but they can be made managed again by creating a new RC with the proper label selector.\\n3. ReplicaSets are a newer and recommended replacement for ReplicationControllers in Kubernetes.\\n\\n**Important Details:**\\n\\n* ReplicationControllers will eventually be deprecated in favor of ReplicaSets.\\n* When deleting an RC, its pods remain running and unmanaged until a new RC is created to manage them again.\\n* It's recommended to create ReplicaSets instead of ReplicationControllers from now on, as they are almost identical and provide the same functionality.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* ReplicaSets are used instead of ReplicationControllers for creating and managing pods.\\n* A ReplicaSet behaves exactly like a ReplicationController, but with more expressive pod selectors.\\n\\n**Important Details:**\\n\\n* A ReplicaSet can match pods based on multiple label conditions (e.g. env=production and env=devel at the same time), whereas a ReplicationController can only match one condition at a time.\\n* A ReplicaSet can also match pods based on the presence of a label key, regardless of its value (e.g. all pods with a label key \"env\" regardless of its value).\\n* To create a ReplicaSet, you need to define it in a YAML file using the `apiVersion: apps/v1beta2` and `kind: ReplicaSet` keywords.\\n* The template section of a ReplicaSet is the same as in a ReplicationController.\\n\\n**Key Takeaways:**\\n\\n* Understanding ReplicaSets is important for Kubernetes deployment management.\\n* ReplicaSets can be used to adopt orphaned pods created by a ReplicationController.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When creating a ReplicaSet, you need to specify the correct `apiVersion` attribute, which includes the API group (`apps`) and version (`v1beta2`).\\n2. A ReplicaSet can be created using a YAML file with the `kubectl create` command.\\n3. The `selector.matchLabels` property is used in a ReplicaSet to define label selectors, which is simpler than listing labels directly under the selector property.\\n\\n**Important Details:**\\n\\n* The `apiVersion` attribute specifies both the API group and version of the resource being created.\\n* A ReplicaSet can take existing pods under its control, so creating a new ReplicaSet will not cause new pods to be created if there are already matching pods running.\\n* The `kubectl get` and `kubectl describe` commands can be used to examine a ReplicaSet.\\n* The `rs` shorthand stands for \"replicaset\" and can be used in place of the full command.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. ReplicaSets are similar to ReplicationControllers, but have more expressive label selectors.\\n2. The main improvement of ReplicaSets over ReplicationControllers is their ability to use more complex label selectors.\\n\\n**Important Details:**\\n\\n* ReplicaSets can use \"matchExpressions\" instead of \"matchLabels\" for more complex label selectors.\\n* The \"matchExpressions\" property allows for multiple expressions, each with a key, operator, and values (if applicable).\\n* There are four valid operators:\\n\\t+ In: matches one of the specified values\\n\\t+ NotIn: does not match any of the specified values\\n\\t+ Exists: checks if a label exists with the specified key (value is not important)\\n\\t+ DoesNotExist: checks if a label does not exist with the specified key\\n* If multiple expressions are used, all must evaluate to true for the selector to match a pod.\\n* A \"matchExpressions\" selector can be combined with a \"matchLabels\" selector, but both must match and all expressions must evaluate to true.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. **ReplicaSets**: Introduced as an alternative to ReplicationControllers, and recommended for use instead. A quick example was provided to demonstrate how to delete a ReplicaSet.\\n2. **DaemonSets**: Used to run exactly one pod on each node in a Kubernetes cluster, with no replicas scattered around the cluster.\\n\\n**Important Details:**\\n\\n* DaemonSets are used for infrastructure-related pods that perform system-level operations, such as log collection and resource monitoring.\\n* Each node needs to run only one instance of the pod.\\n* Examples include Kubernetes' own kube-proxy process and other system-level processes.\\n* A diagram (Figure 4.8) was provided to illustrate the difference between ReplicaSets and DaemonSets.\\n\\n**Key Takeaways:**\\n\\n* Use ReplicaSets instead of ReplicationControllers for running a specific number of pods in a cluster.\\n* Use DaemonSets to run exactly one pod on each node in a cluster, with no replicas scattered around the cluster.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. In Kubernetes, processes can be run on each node using a DaemonSet, which is similar to a ReplicationController or ReplicaSet but with some key differences.\\n2. A DaemonSet ensures that a pod matching its selector is running on each node in the cluster.\\n3. If a node goes down, the DaemonSet doesn\\'t recreate the pod elsewhere, but if a new node is added, it deploys a new pod instance to it.\\n\\n**Important Details:**\\n\\n* A DaemonSet creates as many pods as there are nodes and deploys each one on its own node.\\n* Unlike ReplicaSets, DaemonSets don\\'t have a desired replica count and instead focus on ensuring that a pod matching its selector is running on each node.\\n* Node selectors can be used to specify which nodes a DaemonSet should deploy pods to, allowing for more targeted deployment.\\n* DaemonSets will even deploy pods to unschedulable nodes, as they bypass the Scheduler completely.\\n\\n**Example:**\\n\\nThe text uses an example of creating a DaemonSet that runs a daemon called \"ssd-monitor\" on all nodes with a solid-state drive (SSD). This is achieved by creating a DaemonSet with a node selector that only selects nodes with the label \"disk=ssd\".',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* The chapter discusses using DaemonSets to deploy managed pods in Kubernetes.\\n* A DaemonSet is created to run a mock SSD-monitor process on nodes labeled as \"disk:ssd\".\\n\\n**Important Details:**\\n\\n* The YAML definition for the DaemonSet is provided, specifying:\\n\\t+ API group and version (apps/v1beta2)\\n\\t+ Name of the DaemonSet (ssd-monitor)\\n\\t+ Node selector that matches nodes with the label \"disk:ssd\"\\n\\t+ Container image (luksa/ssd-monitor) and name (main)\\n* The DaemonSet will create a pod on each node labeled as \"disk:ssd\", running the specified container.\\n* An example is shown in Figure 4.9, illustrating how the DaemonSet deploys pods only on nodes with the desired label.\\n\\n**Key Takeaways:**\\n\\n* DaemonSets are used to deploy system pods that run on specific nodes.\\n* Node selectors can be used to target nodes with specific labels.\\n* The provided YAML definition demonstrates how to create a DaemonSet for deploying managed pods.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Creating a DaemonSet**\\n\\n* Create a DaemonSet using a YAML file: `kubectl create -f ssd-monitor-daemonset.yaml`\\n* The DaemonSet should deploy one pod per node, but initially it doesn't because the nodes are not labeled correctly.\\n\\n**Labeling Nodes**\\n\\n* List the nodes in the cluster: `kubectl get node`\\n* Add the required label (`disk=ssd`) to each node that should run a pod: `kubectl label node <node_name> disk=ssd`\\n* The DaemonSet will detect the new label and deploy one pod per labeled node.\\n\\n**Example**\\n\\n* In this example, the author labels a Minikube node with `disk=ssd` and sees a pod created by the DaemonSet.\\n* If multiple nodes are labeled with the same label, the DaemonSet will spin up pods on each of them.\\n\\n**Removing Labels**\\n\\n* To remove a label from a node, use: `kubectl label node <node_name> disk=hdd --overwrite`\\n* This can be used to correct mistakes or change the behavior of the DaemonSet.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The chapter discusses replication and other controllers, specifically focusing on running pods that perform a single completable task.\\n2. It introduces the Job resource, which allows you to run a pod whose container isn't restarted when the process finishes successfully.\\n3. Jobs are useful for ad hoc tasks where it's crucial that the task finishes properly.\\n\\n**Important Details:**\\n\\n1. The chapter shows an example of running a pod with a DaemonSet and deleting it, demonstrating how deleting a DaemonSet deletes all associated pods.\\n2. ReplicationControllers, ReplicaSets, and DaemonSets run continuous tasks that are never considered completed, whereas Jobs allow for completable tasks.\\n3. In the event of a node failure, Job-managed pods are rescheduled to other nodes like ReplicaSet pods.\\n4. Jobs can be configured to restart or not restart containers in case of process failure.\\n5. An example use case is running a task that transforms and exports data, which can be emulated by running a container image built on top of the busybox image.\\n\\n**Key Concepts:**\\n\\n1. Job resource\\n2. Completable tasks\\n3. ReplicationControllers, ReplicaSets, and DaemonSets\\n4. Node failure and rescheduling of pods\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Jobs are a type of resource in Kubernetes that run a single task until completion.\\n2. A Job is defined using a YAML manifest file.\\n3. The restart policy for a Job can be set to OnFailure, which means the pod will not be restarted if it fails.\\n\\n**Important Details:**\\n\\n* Jobs are part of the batch API group and use version v1.\\n* When a Job finishes running, Kubernetes will reschedule the pod until it completes successfully (see Figure 4.10).\\n* The restart policy for a Job cannot be set to Always, which is the default for pods.\\n* A Job does not require a pod selector, as one will be created based on the labels in the pod template.\\n\\n**Key Concepts:**\\n\\n* Jobs are used to run single tasks until completion.\\n* Restart policy can be set to OnFailure or other custom policies.\\n* Kubernetes will reschedule the pod if it fails until it completes successfully.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When creating a Job, you must explicitly set the restart policy to either OnFailure or Never using the pod spec property.\\n2. A Job can run multiple pods in parallel or sequentially by setting the completions and parallelism properties in the Job spec.\\n3. After a Job completes, its associated pod is not deleted by default, allowing for examination of logs.\\n\\n**Important Details:**\\n\\n* The restart policy must be set to either OnFailure or Never when creating a Job.\\n* Completed pods are not shown in the list by default, but can be displayed using the --show-all switch.\\n* The Job resource displays completion status as a number (1) instead of \"yes\" or \"true\".\\n* The DESIRED column in the Job resource indicates the number of pod instances desired to run.\\n\\n**Key Concepts:**\\n\\n* Replication and other controllers\\n* Managed pods\\n* Restart policy\\n* Completions and parallelism properties in Job spec',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. A Job can be configured to run multiple pods sequentially or in parallel.\\n2. To run pods sequentially, set `completions` to the desired number of runs.\\n3. To run pods in parallel, set `parallelism` to the desired number of concurrent runs.\\n\\n**Important Details:**\\n\\n* When running pods sequentially, if one pod fails, a new pod will be created to replace it.\\n* When running pods in parallel, multiple pods can run at the same time until the specified number is reached.\\n* The Job will continue to create new pods until all desired completions are met.\\n\\n**Example Configurations:**\\n\\n1. Running 5 pods sequentially:\\n```yaml\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: multi-completion-batch-job\\nspec:\\n  completions: 5\\n  template:\\n    # same as in listing 4.11\\n```\\n2. Running 5 pods in parallel with up to 2 concurrent runs:\\n```yaml\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: multi-completion-parallel-batch-job\\nspec:\\n  completions: 5\\n  parallelism: 2\\n  template:\\n    # same as in listing 4.11\\n```\\nOverall, this text provides guidance on how to configure a Job to run multiple pods with desired completion requirements, either sequentially or in parallel.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. **Scaling a Job**: You can increase or decrease the number of pods running in a Job by using the `kubectl scale` command.\\n2. **Limiting Pod Time**: The `activeDeadlineSeconds` property can be used to limit the time allowed for a pod to complete, and if it exceeds this time, the system will try to terminate it and mark the Job as failed.\\n3. **Configuring Retry Limit**: You can configure how many times a Job can be retried before it is marked as failed by specifying the `spec.backoffLimit` field in the Job manifest (defaulting to 6 retries).\\n4. **Scheduling Jobs Periodically or Once**: Kubernetes supports running jobs periodically or once in the future using CronJobs, which are configured with a schedule in the well-known cron format.\\n5. **Creating a CronJob**: To create a CronJob, you can specify a Job template and a schedule for running the job.\\n\\n**Important Details:**\\n\\n* The `kubectl scale` command can be used to increase or decrease the number of pods running in a Job.\\n* The `activeDeadlineSeconds` property can be used to limit the time allowed for a pod to complete.\\n* The `spec.backoffLimit` field can be used to configure how many times a Job can be retried before it is marked as failed (defaulting to 6 retries).\\n* CronJobs are configured with a schedule in the well-known cron format.',\n",
       "  'Here\\'s a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. The text describes how to schedule jobs to run periodically or once in the future using a CronJob resource in Kubernetes.\\n2. A CronJob creates Job resources from a template at scheduled times.\\n\\n**Important Details:**\\n\\n* **Cron Schedule Format:** The cron schedule format consists of five entries: minute, hour, day of month, month, and day of week (from left to right).\\n* **Configuring the Schedule:** To run a job every 15 minutes, for example, the schedule should be set to \"0,15,30,45 * * * *\" (at the 0, 15, 30, and 45 minute mark of every hour, day, month, and week).\\n* **Configuring the Job Template:** The CronJob creates Job resources from a template specified in the jobTemplate property. Refer to section 4.5 for more information on configuring the job template.\\n* **How Scheduled Jobs are Run:** Job resources are created from the CronJob resource at approximately the scheduled time, and then create pods.\\n\\n**Key Takeaways:**\\n\\n* To schedule a job to run periodically or once in the future, use a CronJob resource with a specified schedule and job template.\\n* Understand the cron schedule format and how to configure it for different scheduling needs.',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. When using a CronJob or Job in Kubernetes, you can specify a deadline for when the job should start running by setting the `startingDeadlineSeconds` field.\\n2. If the job does not start within the specified deadline, it will be marked as Failed.\\n3. To prevent issues with multiple jobs being created at the same time or none being created, ensure that your jobs are idempotent (running them multiple times has no negative effects) and that subsequent runs perform any work missed by previous runs.\\n\\n**Important Details:**\\n\\n* A CronJob creates a single Job for each execution in its schedule.\\n* ReplicationControllers keep the desired number of pod replicas running and can be used to scale pods horizontally.\\n* Pods are not owned by ReplicationControllers and can be moved between them if necessary.\\n* Changing the template of a ReplicationController has no effect on existing pods.\\n\\n**Key Concepts:**\\n\\n* Liveness probes: Kubernetes restarts containers when they become unhealthy.\\n* ReplicationControllers: manage the desired number of pod replicas running.\\n* CronJobs: schedule jobs to run at specific times and can be used with deadlines.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Key Points:**\\n\\n1. **Replace ReplicationControllers**: Use ReplicaSets and Deployments instead, which offer additional features.\\n2. **Pod Scheduling**: Understand how different resources schedule pods to cluster nodes:\\n\\t* ReplicationControllers and ReplicaSets: random node selection\\n\\t* DaemonSets: one instance of a pod per node\\n3. **Batch Tasks**: Create pods for batch tasks using the Kubernetes Job resource, not directly or through other objects.\\n4. **Scheduled Jobs**: Use CronJob resources to run jobs at a future time.\\n\\n**Important Details:**\\n\\n* The text highlights the importance of understanding how different Kubernetes resources schedule pods and manage workloads.\\n* It emphasizes the need to use the correct resource for specific use cases (e.g., using Job for batch tasks, CronJob for scheduled jobs).',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Pods need a way to discover and communicate with each other, especially in microservices architecture.\\n2. Kubernetes provides a Service resource that enables clients to find and talk to pods.\\n\\n**Important Details:**\\n\\n* In Kubernetes, Services are used to expose a group of pods at a single address.\\n* This allows pods to discover services in the cluster and connect to external services from inside the cluster.\\n* Services can be controlled to determine whether a pod is ready to be part of the service or not.\\n* Troubleshooting services is also an important aspect of managing Kubernetes clusters.\\n\\n**Key Concepts:**\\n\\n* Pods\\n* ReplicaSets\\n* Microservices architecture\\n* Service resources in Kubernetes',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. In Kubernetes, client applications cannot directly access pods providing a service because:\\n\\t* Pods are ephemeral and may come and go at any time.\\n\\t* Each pod has its own IP address, making it difficult for clients to know which IP to use.\\n2. To solve this problem, Kubernetes provides a resource type called \"Services\".\\n3. A Service is a single, constant point of entry to a group of pods providing the same service.\\n\\n**Important Details:**\\n\\n1. Services provide an IP address and port that never change while the service exists.\\n2. Clients can connect to the Service\\'s IP and port, which are then routed to one of the backing pods.\\n3. This allows clients to access the service without knowing the location of individual pods.\\n4. Services can be used to expose a single, constant IP address for external clients to connect to.\\n5. They can also be used to create a stable address for pods that may move around the cluster.\\n\\n**Example:**\\n\\nThe text uses an example of a frontend web server and a backend database server to illustrate how Services work:\\n\\n* A Service is created for the frontend pods, exposing a single IP address through which external clients can connect.\\n* A Service is also created for the backend pod, providing a stable address that doesn\\'t change even if the pod moves around the cluster.\\n\\nOverall, Kubernetes Services provide a way to make a group of pods providing the same service accessible through a single, constant point of entry.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Services in Kubernetes allow clients to discover and communicate with pods, even if the pod's IP address changes.\\n* Services enable easy communication between components by providing a stable name for the service through environment variables or DNS.\\n\\n**Important Details:**\\n\\n* A service can be backed by multiple pods, and connections to the service are load-balanced across all backing pods.\\n* To define which pods belong to a service, label selectors are used in the same way as in ReplicationControllers and other pod controllers.\\n* Creating a service involves specifying which pods belong to the service using label selectors.\\n\\n**Key Concepts:**\\n\\n* Services\\n* Label selectors\\n* ReplicationControllers\\n* Load-balancing\\n\\nOverall, this text provides an introduction to services in Kubernetes and explains how they enable communication between components, even if the underlying pod's IP address changes.\",\n",
       "  'Here is a summary of the text:\\n\\n**Creating a Service in Kubernetes**\\n\\nThere are two ways to create a service in Kubernetes: using `kubectl expose` or creating a YAML descriptor and posting it to the API server.\\n\\n**Method 1: Using kubectl expose**\\n\\n* This method creates a service with the same pod selector as a ReplicationController.\\n* It exposes all pods through a single IP address and port.\\n\\n**Method 2: Creating a YAML Descriptor**\\n\\n* Create a file called `kubia-svc.yaml` with the following contents:\\n\\t+ `apiVersion`: v1\\n\\t+ `kind`: Service\\n\\t+ `metadata`: name of the service (e.g. kubia)\\n\\t+ `spec`:\\n\\t\\t- `ports`: port number and targetPort for routing connections to pods\\n\\t\\t- `selector`: label selector for identifying which pods belong to the service\\n* Post the YAML file using `kubectl create`\\n* The resulting service will accept connections on a specified port and route them to a corresponding port on one of the matching pods.\\n\\n**Important Details**\\n\\n* A service can be created manually by posting a YAML descriptor, allowing for more control over its configuration.\\n* Label selectors are used to determine which pods belong to a service.\\n* The `selector` field in the YAML descriptor specifies the label selector for identifying matching pods.',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. After creating a service in Kubernetes, you can list all services in your namespace using `kubectl get svc`.\\n2. The service has an internal cluster IP address that is only accessible from within the cluster.\\n3. You can test your service by sending requests to it from within the cluster using various methods.\\n\\n**Important Details:**\\n\\n1. The primary purpose of services is to expose groups of pods to other pods in the cluster.\\n2. To access a service externally, you will need to use an external IP address or a load balancer (not covered in this text).\\n3. You can remotely execute commands inside running containers using `kubectl exec`.\\n4. This command allows you to examine the contents, state, and environment of a container.\\n\\n**Key Commands:**\\n\\n1. `kubectl get svc` - lists all services in your namespace.\\n2. `kubectl exec <pod-name> -- curl -s http://<service-IP>` - sends a request to a service from within a pod using `kubectl exec`.',\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. When you run `kubectl exec` with a command, Kubernetes executes that command inside a container within a pod.\\n2. The `--` double dash in the command signals the end of options for `kubectl` and indicates what command to execute inside the pod.\\n\\n**Important Details:**\\n\\n* When you use `kubectl exec`, it sends an HTTP request from your local machine to the service IP, which is backed by multiple pods.\\n* The Kubernetes service proxy intercepts the connection and selects a random pod among the three pods to forward the request to.\\n* Node.js running inside that pod handles the request and returns an HTTP response containing the pod's name.\\n* The output of the command is sent back to `kubectl`, which prints it to its standard output on your local machine.\\n\\n**Key Concepts:**\\n\\n* `kubectl exec` allows you to execute a command inside a container within a pod.\\n* The double dash (`--`) in the command separates options for `kubectl` from the command to be executed inside the pod.\\n* Kubernetes service proxy is used to route traffic to multiple pods behind a single IP address.\",\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes services can enable clients to discover and talk to pods.\\n2. By default, services forward each connection to a randomly selected backing pod.\\n3. To redirect all requests from a client to the same pod, set the service's `sessionAffinity` property to `ClientIP`.\\n4. Services support only two types of session affinity: `None` and `ClientIP`.\\n\\n**Important Details:**\\n\\n* Setting `sessionAffinity` to `ClientIP` makes the service proxy redirect all requests from the same client IP to the same pod.\\n* Kubernetes services operate at the TCP/UDP level, not HTTP, which is why cookie-based session affinity is not supported.\\n* Services can expose multiple ports, allowing a single service to forward traffic to different pods on different ports.\\n* When creating a multi-port service, each port must be specified with a unique name.\\n\\n**Key Takeaways:**\\n\\n* Understanding how services interact with clients and pods is crucial for designing effective Kubernetes deployments.\\n* Configuring session affinity can help ensure that requests from the same client are directed to the same pod.\\n* Services can support multiple ports, simplifying the management of complex applications.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. When creating a service in Kubernetes, you can specify multiple ports using the `ports` field in the service spec.\\n2. Each port must have a unique number and targetPort (the port on which the pod listens).\\n3. The label selector applies to the entire service, not individual ports.\\n\\n**Important Details:**\\n\\n1. If you want different ports to map to different subsets of pods, you need to create two separate services.\\n2. You can give names to each port in the pod definition and refer to them by name in the service spec.\\n3. When using named ports, you can specify the targetPort as the name of the port (e.g., `http` or `https`) instead of its number.\\n\\n**Key Takeaways:**\\n\\n* Port 80 is mapped to the pods\\' port 8080 and is called \"http\".\\n* Port 443 is mapped to the pods\\' port 8443 and is called \"https\".',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Naming ports in Kubernetes allows for easy changes to port numbers without modifying service specs.\\n2. Services provide a single, stable IP address and port that can be used to access pods, even if pod IPs change or their number increases/decreases.\\n3. Client pods can discover the IP and port of a service through environment variables.\\n\\n**Important Details:**\\n\\n1. To use named ports, you only need to update the port number in the pod spec, while keeping the port name unchanged.\\n2. When using named ports, client connections will be forwarded to the appropriate port numbers based on the pod receiving the connection.\\n3. Environment variables are initialized by Kubernetes when a pod is started and point to each service that exists at that moment.\\n4. To view environment variables for a service, you need to delete all existing pods and let the ReplicationController create new ones.\\n5. You can list environment variables inside a container using the `env` command.\\n\\nOverall, this chunk of text explains how services enable clients to discover and talk to pods in Kubernetes, and provides details on how named ports work and how client pods can discover service IP addresses and ports through environment variables.',\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Two services are defined in the cluster: `kubernetes` and `kubia`.\\n2. Each service has its own set of environment variables that can be used to access it, such as `KUBIA_SERVICE_HOST` and `KUBIA_SERVICE_PORT`.\\n3. Environment variables can be used to look up the IP address and port of a service.\\n4. Kubernetes also provides a DNS server (called `kube-dns`) that allows client pods to access services through their fully qualified domain name (FQDN).\\n\\n**Important Details:**\\n\\n1. The `kube-dns` pod runs a DNS server that knows all the services running in the system and can be used by client pods.\\n2. Each service gets a DNS entry in the internal DNS server, allowing client pods to access it through its FQDN.\\n3. Environment variables are one way of looking up the IP address and port of a service, but using DNS is also an option.\\n4. The `dnsPolicy` property in each pod's spec can be used to configure whether a pod uses the internal DNS server or not.\\n\\n**Key Concepts:**\\n\\n1. Services\\n2. Environment variables\\n3. DNS server (kube-dns)\\n4. Fully qualified domain name (FQDN)\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To connect to a service in Kubernetes, you can use its Fully Qualified Domain Name (FQDN) instead of its IP address.\\n2. The FQDN format is `<service-name>.<namespace>.svc.cluster.local>`.\\n3. You can omit the `.svc.cluster.local` suffix and namespace if the client pod is in the same namespace as the service.\\n\\n**Important Details:**\\n\\n* To access a service, you need to know its port number (unless it's using a standard port like 80 for HTTP).\\n* If you're running `kubectl exec` inside an existing pod, you can run a shell (like bash) instead of a single command.\\n* To use the shell properly, pass the `-it` option to `kubectl exec`.\\n* The shell's binary executable must be available in the container image for this to work.\\n\\n**Example:**\\n\\n* Accessing the `kubia` service through its FQDN using `curl`: `http://kubia.default.svc.cluster.local` or simply `http://kubia.default`.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Connecting to Services Outside the Cluster**\\n\\n* To access services living outside the cluster, use the service's name as the hostname in the requested URL (e.g. `http://kubia`).\\n* The DNS resolver inside each pod's container is configured to omit the namespace and `.svc.cluster.local` suffix.\\n* You can't ping a service IP because it's a virtual IP that only has meaning when combined with the service port.\\n\\n**Understanding Service Endpoints**\\n\\n* Services don't link directly to pods, but rather through an intermediate resource called Endpoints.\\n* The Endpoints resource is used by services to redirect connections to external IP(s) and port(s).\\n\\n**Connecting to External Services**\\n\\n* You can expose external services through Kubernetes services, allowing client pods in the cluster to connect to them like internal services.\\n* This allows you to take advantage of service load balancing and discovery.\\n\\nNote: The text also mentions that the details of how services work will be explained in Chapter 11.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes services can be configured to use manually managed endpoints, which allows for more flexibility in managing traffic to pods.\\n2. The Endpoints resource is a list of IP addresses and ports exposing a service, and it's used by the service proxy to redirect incoming connections.\\n3. To create a service with manually managed endpoints, you need to create both a Service and an Endpoints resource.\\n\\n**Important Details:**\\n\\n* When creating a service without a pod selector, Kubernetes won't automatically create the Endpoints resource.\\n* You must create the Endpoints resource manually to specify the list of endpoints for the service.\\n* The name of the Service and Endpoints resources must match in order for them to be associated with each other.\\n\\n**Key Concepts:**\\n\\n* ClusterIP: a type of service that provides an IP address for pods to communicate with each other.\\n* Session Affinity: not used in this example, but refers to the ability of a service to maintain session information across multiple requests.\\n* Endpoints resource: a list of IP addresses and ports exposing a service.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. A service called \"external-service\" was created without a pod selector, which means it doesn\\'t automatically create an Endpoints resource.\\n2. An Endpoints resource must be manually created to specify the target IP addresses and ports for the service.\\n3. The Endpoints object needs to have the same name as the service and list the target IP addresses and ports.\\n\\n**Important Details:**\\n\\n* Without a pod selector, the service can still be used by containers that are created after it is created.\\n* Containers will include environment variables for the service, and connections to its IP:port pair will be load balanced between the service\\'s endpoints.\\n* If the external service is later migrated to pods running inside Kubernetes, a selector can be added to the service to make its Endpoints managed automatically.\\n\\n**Key Takeaways:**\\n\\n* Creating an Endpoints resource is necessary for services without a pod selector.\\n* The name of the Endpoints object must match the name of the service.\\n* Manually creating an Endpoints resource allows for flexibility in managing external services.',\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes allows services to be updated independently of their Endpoints, ensuring a constant IP address.\\n2. A simpler method exists to expose external services by creating an alias using a fully qualified domain name (FQDN).\\n3. ExternalName-type services are implemented solely at the DNS level and don't require a cluster IP.\\n\\n**Important Details:**\\n\\n* To create an ExternalName service, set the `type` field to `ExternalName` in the Service resource.\\n* The `externalName` attribute specifies the FQDN of the actual external service.\\n* Clients can connect to the external service directly, bypassing the service proxy.\\n* ExternalName services don't get a cluster IP and are implemented solely at the DNS level using a CNAME record.\\n* This allows for easy modification of the service definition by changing the `externalName` attribute or switching back to ClusterIP.\",\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Exposing Services to External Clients**\\n\\nThere are three ways to make a service accessible externally in a Kubernetes cluster:\\n\\n1. **NodePort**: Creates a dedicated port on each node that redirects traffic to the underlying service. This allows external clients to access the service through any node\\'s IP address.\\n2. **LoadBalancer**: Uses a cloud-provisioned load balancer to redirect traffic to the NodePort across all nodes. Clients connect to the service through the load balancer\\'s IP address.\\n3. **Ingress**: A more advanced mechanism that operates at the HTTP level (layer 7) and can expose multiple services through a single IP address.\\n\\n**Using a NodePort Service**\\n\\nA NodePort service is similar to a regular ClusterIP service, but it can be accessed not only through its internal cluster IP, but also through any node\\'s IP and the reserved node port. This allows external clients to access the service through any node in the cluster.\\n\\n**Creating a NodePort Service**\\n\\nTo create a NodePort service, you set its type to \"NodePort\" in the YAML configuration file. This will reserve a port on all nodes and forward incoming connections to the pods that are part of the service.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Creating a NodePort service in Kubernetes allows clients to discover and communicate with pods across all cluster nodes.\\n2. The `type` field is set to `NodePort`, which specifies that the service should be bound to a port on each node in the cluster.\\n\\n**Important Details:**\\n\\n* When creating a NodePort service, you can specify a port number, but it's not mandatory; Kubernetes will choose a random port if omitted.\\n* The service is accessible through the IP address of any cluster node and the specified node port (in this case, 30123).\\n* The service is also accessible at the internal cluster IP address (e.g., `10.111.254.223:80`) and the node port on each node's IP address.\\n* Incoming connections to the node port are redirected to a randomly selected pod, which may or may not be running on the same node.\\n\\n**Key Takeaways:**\\n\\n* NodePort services allow clients to access pods across all cluster nodes.\\n* The service is accessible through multiple addresses (cluster IP, node ports, and each node's IP address).\\n* Kubernetes will choose a random port if one is not specified.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* A connection received on port 30123 of a node can be forwarded to either a pod running on that node or one of the pods running on another node.\\n* To access a NodePort service from an external client, you need to configure Google Cloud Platform's firewalls to allow external connections to your nodes on that port.\\n\\n**Important Details:**\\n\\n* The command `gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123` is used to create a new firewall rule allowing external connections to the node on port 30123.\\n* After creating the firewall rule, you can access your service through port 30123 of one of the node's IPs.\\n* To find the IP address of a node, refer to the sidebar for instructions.\\n\\n**Key Concepts:**\\n\\n* NodePort service\\n* External client\\n* Kubernetes cluster\\n* Pods and nodes in a cluster\",\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. Once you know the IPs of your nodes, you can access your service through them using `curl`.\\n2. Services are now accessible to the whole internet through port 30123 on any node.\\n3. However, this approach has a limitation: if one node fails, clients will not be able to access the service anymore.\\n4. To address this issue, a load balancer should be placed in front of the nodes to distribute requests across all healthy nodes.\\n\\n**Important Details:**\\n\\n1. Kubernetes clusters running on cloud providers can automatically provision a load balancer using a `LoadBalancer` service instead of a `NodePort` service.\\n2. You can use JSONPath to get the IPs of all your nodes using `kubectl`.\\n3. The example shows how to use JSONPath to filter and print only the ExternalIP addresses of the nodes.\\n\\n**Key Concepts:**\\n\\n1. NodePort services\\n2. Load balancers\\n3. JSONPath\\n4. Kubernetes clusters on cloud providers',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Exposing services to external clients can be done using LoadBalancer services, which provide a publicly accessible IP address and redirect connections to the service.\\n2. If Kubernetes is running in an environment that doesn\\'t support LoadBalancer services, it will behave like a NodePort service instead.\\n3. To create a LoadBalancer service, you need to specify the type as \"LoadBalancer\" in the YAML manifest.\\n\\n**Important Details:**\\n\\n1. The load balancer will have its own unique IP address and will redirect all connections to the service.\\n2. You can access the service through the load balancer\\'s IP address using tools like curl.\\n3. Creating a LoadBalancer service takes time for the cloud infrastructure to create the load balancer and write its IP address into the Service object.\\n4. The external IP address of the service will be listed in the output of `kubectl get svc`.\\n5. This type of service is supported on Google Kubernetes Engine (GKE) but not on Minikube.\\n\\n**Key Takeaways:**\\n\\n* LoadBalancer services provide a publicly accessible IP address and redirect connections to the service.\\n* Creating a LoadBalancer service takes time for the cloud infrastructure to create the load balancer.\\n* You can access the service through the load balancer\\'s IP address using tools like curl.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When an external client (e.g., `curl`) connects to a Kubernetes Service, it gets routed to one of the pods behind the Service.\\n2. The pod that receives the connection is chosen randomly when the first packet is sent.\\n3. All subsequent packets from the same client are sent to the same pod until the connection is closed.\\n\\n**Important Details:**\\n\\n* Even if the Service's session affinity is set to `None`, users will always hit the same pod (until the connection is closed).\\n* This behavior occurs because services work at the connection level, not at the request level.\\n* Web browsers use keep-alive connections and send all requests through a single connection, whereas tools like `curl` open a new connection every time.\\n\\n**Key Takeaways:**\\n\\n* When accessing a Service with a web browser, you may always hit the same pod due to keep-alive connections.\\n* This behavior is different from using tools like `curl`, which opens a new connection for each request.\",\n",
       "  'Here is a summary of the text with main points and important details:\\n\\n**Main Points:**\\n\\n1. Exposing services to external clients using LoadBalancer-type services involves an additional infrastructure-provided load balancer and a node port.\\n2. The service connection may be forwarded to a pod instance on any node, not just the one that received the connection.\\n3. To prevent unnecessary network hops, you can configure the service to redirect external traffic only to pods running on the same node using the `externalTrafficPolicy` field.\\n\\n**Important Details:**\\n\\n* When an external client connects through the node port, the pod may or may not be running on the same node, requiring an additional network hop.\\n* Setting `externalTrafficPolicy` to `Local` ensures that connections are forwarded only to locally running pods, but this can lead to uneven distribution of connections across nodes.\\n* Using this annotation also means that if no local pods exist, the connection will hang and not be forwarded to a random global pod.\\n\\n**Key Takeaways:**\\n\\n* LoadBalancer-type services involve an additional infrastructure-provided load balancer and a node port.\\n* Configuring `externalTrafficPolicy` to `Local` can prevent unnecessary network hops but may lead to uneven distribution of connections across nodes.',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. When clients inside a Kubernetes cluster connect to a service through a node port, the client's IP address is not preserved due to Source Network Address Translation (SNAT).\\n2. This can be a problem for applications that need to know the client's IP address.\\n3. The Local external traffic policy does not perform SNAT and preserves the client's IP address.\\n\\n**Important Details:**\\n\\n* When using a LoadBalancer service, each service requires its own load balancer with its own public IP address.\\n* An Ingress resource only requires one public IP address, even when providing access to multiple services.\\n* An Ingress determines which service an HTTP request is forwarded to based on the host and path in the request.\\n\\n**Key Concepts:**\\n\\n* SNAT (Source Network Address Translation)\\n* Local external traffic policy\\n* LoadBalancer service\\n* Ingress resource\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Ingress resources in Kubernetes allow exposing services externally, providing features like cookie-based session affinity that services cannot provide.\\n2. An Ingress controller is required to make Ingress resources work, but different Kubernetes environments may not have a default controller or use alternative implementations.\\n\\n**Important Details:**\\n\\n* To enable the Ingress add-on in Minikube, run `minikube addons list` and then `minikube addons enable ingress`.\\n* The Ingress add-on is disabled by default in Minikube.\\n* Multiple services can be exposed through a single Ingress resource (Figure 5.9).\\n* An example of an Ingress object exposing services at kubia.example.com, foo.example.com, and bar.example.com is provided.\\n\\n**Key Takeaways:**\\n\\n* Understanding the requirements for using Ingress resources in Kubernetes.\\n* Knowing how to enable the Ingress add-on in Minikube.\\n* Recognizing that multiple services can be exposed through a single Ingress resource.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Creating an Ingress resource allows clients to discover and talk to pods in a Kubernetes cluster.\\n2. An Ingress controller must be running in the cluster before creating an Ingress resource.\\n\\n**Important Details:**\\n\\n* The `--all-namespaces` option can be used with `kubectl get po` to list all running pods across all namespaces, which is useful when trying to find the Ingress controller pod.\\n* To create an Ingress resource, a YAML manifest file (e.g. `kubia-ingress.yaml`) must be created with the following format:\\n\\t+ `apiVersion: extensions/v1beta1`\\n\\t+ `kind: Ingress`\\n\\t+ `metadata: name: kubia`\\n\\t+ `spec: rules: - host: kubia.example.com ...`\\n* The Ingress resource defines a single rule that sends all HTTP requests to the specified service (e.g. `kubia-nodeport`) on port 80.\\n* The Ingress controller pod will be listed in the output of `kubectl get po --all-namespaces`, and its name may suggest which software is being used to provide the Ingress functionality (in this case, Nginx).',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Exposing services externally through an Ingress resource allows access from outside the cluster.\\n2. To access a service through an Ingress, you need to configure your DNS or hosts file to point to the IP address of the Ingress controller.\\n\\n**Important Details:**\\n\\n* On cloud providers (e.g., GKE), Ingress controllers require the Ingress to point to a NodePort service.\\n* To find the IP address of the Ingress, use `kubectl get ingresses`.\\n* The IP address may take time to appear on cloud providers due to provisioning a load balancer behind the scenes.\\n* Once you have the IP address, configure your DNS or hosts file to point to it.\\n* Accessing the service through an Ingress involves:\\n\\t+ Client performs a DNS lookup of the domain name (e.g., kubia.example.com).\\n\\t+ DNS server returns the IP address of the Ingress controller.\\n\\t+ Client sends an HTTP request to the Ingress controller with the Host header set to the domain name.\\n\\t+ The Ingress controller selects a pod associated with the service and forwards the client's request to it.\\n\\n**Key Takeaways:**\\n\\n* Exposing services externally through an Ingress resource allows access from outside the cluster.\\n* Configuring your DNS or hosts file is necessary to point to the IP address of the Ingress controller.\\n* Understanding how Ingresses work involves recognizing that they select a pod associated with the service, rather than forwarding requests directly to the service.\",\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n* An Ingress can expose multiple services through the same host (IP address) by mapping different paths to different services.\\n* This allows clients to reach two or more different services through a single IP address.\\n\\n**Important Details:**\\n\\n* The Ingress spec uses arrays for rules and paths, allowing multiple items to be specified.\\n* Multiple paths on the same host can be mapped to different services, as shown in the example listing.\\n* Requests are routed to different services based on the path in the requested URL.\\n* This approach enables clients to access two or more services through a single IP address, without needing to know the specific service names.\\n\\n**Key Example:**\\n\\n* The example shows an Ingress mapping multiple paths (e.g. /kubia and /foo) on the same host (kubia.example.com) to different services (kubia and bar).\\n* Requests to kubia.example.com/kubia are routed to the kubia service, while requests to kubia.example.com/bar are routed to the bar service.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. An Ingress resource can be used to expose multiple services externally, mapping different hosts in HTTP requests to different services.\\n2. This can be achieved by specifying multiple \"rules\" in the Ingress configuration, each with its own host and service backend.\\n3. The Ingress controller will forward requests to either service based on the Host header in the request.\\n\\n**Important Details:**\\n\\n* To use this feature, DNS needs to point both domain names (e.g. foo.example.com and bar.example.com) to the Ingress controller\\'s IP address.\\n* This approach allows for multiple services to be exposed externally, each with its own host and service backend.\\n* The application running in the pod does not need to support TLS, as the Ingress controller will handle all TLS-related tasks.\\n\\n**TLS Configuration:**\\n\\n1. To enable TLS support on an Ingress, a certificate and private key must be attached to the Ingress resource.\\n2. This requires creating a Kubernetes Secret resource to store the certificate and private key.\\n3. The Ingress configuration can then reference this Secret to enable TLS support.\\n4. A sample command is provided to generate a self-signed certificate using OpenSSL.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Creating a Secret to store a TLS certificate and private key.\\n2. Updating an Ingress object to accept HTTPS requests for a specific hostname.\\n3. Using a CertificateSigningRequest resource to sign certificates automatically.\\n\\n**Important Details:**\\n\\n* To create a Secret, use the command `kubectl create secret tls <name> --cert=<file> --key=<file>` (e.g., `$ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key`).\\n* The Ingress object must be updated to include the `tls` attribute and reference the Secret created above.\\n* To sign certificates automatically, use a CertificateSigningRequest resource and approve it with `kubectl certificate approve <name of the CSR>`.\\n* A certificate signer component must be running in the cluster for this process to work.\\n\\n**Key Terms:**\\n\\n* TLS (Transport Layer Security)\\n* Ingress\\n* Secret\\n* CertificateSigningRequest',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. You can use HTTPS to access your service through an Ingress, which allows you to configure server certificates.\\n2. The readiness probe feature in Kubernetes allows you to signal when a pod is ready to accept connections.\\n3. Readiness probes are similar to liveness probes, but determine whether a container should receive client requests or not.\\n\\n**Important Details:**\\n\\n* Support for Ingress features varies between different Ingress controller implementations.\\n* Ingresses currently support only L7 (HTTP/HTTPS) load balancing, with planned support for L4 load balancing.\\n* Readiness probes are specific to each container and can be used to prevent pods from receiving requests until they are fully ready.\\n* The readiness probe is invoked periodically and returns success when the container is ready to accept requests.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes allows developers to create \"readiness probes\" to determine if a container (pod) is ready to serve requests.\\n2. There are three types of readiness probes: Exec, HTTP GET, and TCP Socket probes.\\n3. Readiness probes can be used to ensure that only healthy containers receive traffic from clients.\\n\\n**Important Details:**\\n\\n* A readiness probe checks the health of a container periodically after it has started.\\n* If a pod fails its readiness check, it is removed from the service\\'s Endpoints object and clients are not redirected to it.\\n* Unlike liveness probes, failing a readiness check does not kill or restart the container.\\n* Readiness probes are useful during container startup, but also after the container has been running for a while.\\n\\n**Key Concepts:**\\n\\n* Readiness probe: a mechanism to determine if a container is ready to serve requests\\n* Endpoints object: a Kubernetes resource that keeps track of pods that are serving traffic from a service.\\n* Liveness probes: a separate mechanism to ensure containers remain healthy and are replaced if they fail.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Readiness probes are important for ensuring that clients only interact with healthy pods, and not ones experiencing issues.\\n2. A readiness probe can be added to a pod to signal when it's ready to accept connections.\\n\\n**Important Details:**\\n\\n* To add a readiness probe, modify the ReplicationController's pod template using `kubectl edit`.\\n* The readiness probe definition should be added under `spec.template.spec.containers` in the YAML file.\\n* A simple example of a readiness probe is using the `ls` command to check if a file `/var/ready` exists. If it does, the probe succeeds; otherwise, it fails.\\n\\n**Key Takeaways:**\\n\\n* Readiness probes help ensure that clients only interact with healthy pods.\\n* Adding a readiness probe can be done by modifying the ReplicationController's pod template using `kubectl edit`.\\n* A readiness probe can be defined for each container in the pod.\",\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. A readiness probe is used to determine if a pod is ready to receive traffic.\\n2. The readiness probe can be toggled by creating or removing a file in the pod.\\n3. Existing pods do not have their readiness probes updated when a ReplicationController's template changes.\\n\\n**Important Details:**\\n\\n* Changing a ReplicationController's pod template does not affect existing pods, so they still report as not ready.\\n* To update the readiness probe on existing pods, you must delete them and have them re-created by the ReplicationController.\\n* The readiness probe is checked periodically (every 10 seconds by default) to determine if the pod is ready.\\n* Creating a file in the pod's container can make the readiness probe succeed, making the pod report as ready.\\n\\n**Key Commands:**\\n\\n* `kubectl get pods` lists all pods and their readiness status\\n* `kubectl exec <pod> -- touch /var/ready` creates a file in the pod's container to toggle the readiness probe\\n* `kubectl describe <pod>` shows detailed information about the pod, including its readiness probe configuration\",\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. A readiness probe is used to determine when a pod is ready to accept connections.\\n2. If a pod doesn't have a readiness probe, it will become a service endpoint immediately, potentially causing client requests to be forwarded to an unready pod.\\n3. Readiness probes should return success or failure depending on whether the app can receive client requests.\\n\\n**Important Details:**\\n\\n1. In this example, only one pod was reporting as ready and receiving requests, even though three pods were running.\\n2. Manually removing a pod from a service should be done by deleting the pod or changing its labels, not by modifying the readiness probe.\\n3. It's recommended to always define a readiness probe, even if it's simple, to prevent client connection errors.\\n4. Readiness probes should not include shutdown logic, as Kubernetes will automatically remove the pod from services when it's deleted.\\n\\nOverall, this text emphasizes the importance of using readiness probes to ensure that pods are only receiving requests when they're ready and able to handle them.\",\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* Services in Kubernetes can be used to provide a stable IP address for clients to connect to pods or other endpoints.\\n* However, if a client needs to connect to all individual pods, using a service is not the best approach.\\n* Instead, Kubernetes allows clients to discover pod IPs through DNS lookups.\\n\\n**Important Details:**\\n\\n* To enable DNS lookups for individual pods, you can create a \"headless\" service by setting the `clusterIP` field to `None` in the service specification.\\n* A headless service does not have a cluster IP and is used only for DNS lookups.\\n* When creating a headless service, clients can perform a simple DNS lookup to get the IPs of all pods that are part of the service.\\n\\n**Key Terms:**\\n\\n* Headless service: a service that does not have a cluster IP and is used only for DNS lookups.\\n* ClusterIP: the IP address assigned to a service by Kubernetes.\\n* Selector: a field in the service specification that selects which pods to include in the service.',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Using a headless service to discover individual pods based on their pod selector.\\n2. Performing DNS lookups from within a pod to see if actual pod IPs are returned.\\n\\n**Important Details:**\\n\\n* A readiness probe is used by the pods, so only ready pods will be listed as endpoints of the service.\\n* To perform DNS-related actions, a new pod can be created using the `tutum/dnsutils` container image, which contains binaries for `nslookup` and `dig`.\\n* The `kubectl run` command can be used to create a pod without writing a YAML manifest by specifying the `--generator=run-pod/v1` option.\\n* When performing a DNS lookup using `nslookup`, two different IPs are returned for the headless service, corresponding to the IPs of the two pods that are reporting being ready.\\n\\n**Key Takeaways:**\\n\\n* Headless services can be used to discover individual pods based on their pod selector.\\n* DNS lookups from within a pod can be performed using the `tutum/dnsutils` container image.\\n* The `kubectl run` command can be used to create a pod without writing a YAML manifest.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Headless Services**\\n\\n* Headless services are different from regular services in that they don\\'t have a cluster IP address.\\n* Clients can still connect to pods behind a headless service using its DNS name, but instead of going through the service proxy, they connect directly to the pods.\\n* Load balancing is still provided, but through DNS round-robin mechanism instead of the service proxy.\\n\\n**Discovering All Pods**\\n\\n* By default, only ready pods become endpoints of services.\\n* To discover all pods matching a service\\'s label selector, including unready ones, add the `service.alpha.kubernetes.io/tolerate-unready-endpoints: \"true\"` annotation to the service.\\n* Note that this is an alpha feature and may be replaced by the `publishNotReadyAddresses` field in future versions of Kubernetes.\\n\\n**Troubleshooting Services**\\n\\n* When unable to access pods through a service, check the following:\\n\\t+ Service exists and is running\\n\\t+ Pod is running and has the correct label selector\\n\\t+ DNS resolution is working correctly\\n\\t+ Network connectivity between client and pod is established',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Troubleshooting Kubernetes Services**\\n\\nTo troubleshoot service-related problems in Kubernetes, follow these steps:\\n\\n1. Ensure you're connecting to the service's cluster IP from within the cluster.\\n2. Don't rely on pinging the service IP; instead, check if a readiness probe is succeeding.\\n3. Verify that a pod is part of the service by examining the corresponding Endpoints object with `kubectl get endpoints`.\\n4. Check if you're accessing the service through its FQDN or cluster IP correctly.\\n5. Ensure you're connecting to the correct port exposed by the service, not the target port.\\n6. Try connecting to the pod's IP directly to confirm it's accepting connections on the correct port.\\n\\n**Key Points about Kubernetes Services**\\n\\nKubernetes services provide a stable IP address and port for multiple pods that match a certain label selector. Key features include:\\n\\n1. Exposing multiple pods under a single, stable IP address and port.\\n2. Making services accessible from inside the cluster by default, with options to make them accessible from outside the cluster using NodePort or LoadBalancer types.\\n3. Enabling pods to discover services through environment variables.\\n4. Allowing discovery of external services by creating a Service resource without specifying a selector.\\n5. Providing a DNS CNAME alias for external services using the ExternalName service type.\\n6. Exposing multiple HTTP services through a single Ingress (consuming a single IP).\",\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* Services in Kubernetes enable clients to discover and interact with pods.\\n* A service's readiness probe determines whether a pod should be included as an endpoint.\\n* Headless services allow discovery of pod IPs through DNS.\\n\\n**Important Details:**\\n\\n* The chapter covers troubleshooting, modifying firewall rules, executing commands in pod containers, running bash shells, modifying Kubernetes resources, and creating unmanaged ad hoc pods using `kubectl`.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Summary**\\n\\nThis chapter focuses on attaching disk storage to containers within Kubernetes pods. It covers various ways to share external disk storage between containers, including creating volumes, using Git repositories, and attaching persistent storage.\\n\\n**Main Points:**\\n\\n1. Containers in a pod have their own isolated filesystems.\\n2. This chapter explores how to access external disk storage from within a pod.\\n3. Topics covered include:\\n\\t* Creating multi-container pods\\n\\t* Sharing disk storage between containers using volumes\\n\\t* Using Git repositories inside a pod\\n\\t* Attaching persistent storage (e.g., GCE Persistent Disk) to pods\\n\\t* Pre-provisioned and dynamic provisioning of persistent storage\\n\\n**Important Details:**\\n\\n* Each container in a pod has its own filesystem, which comes from the container's image.\\n* This chapter is focused on accessing external disk storage from within a pod.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Containers in a pod do not share files or data between restarts.\\n2. Kubernetes provides storage volumes to persist data across container restarts.\\n3. Volumes are defined as part of a pod and have the same lifecycle as the pod.\\n\\n**Important Details:**\\n\\n* Volumes can be used by all containers in a pod, but must be mounted in each container that needs to access it.\\n* A volume's contents will persist across container restarts.\\n* Volumes can be created or deleted only when creating or deleting a pod.\\n* Volumes are not standalone Kubernetes objects and cannot be managed independently.\\n\\n**Example:**\\n\\nThe text uses an example of a pod with three containers:\\n\\n1. A web server that serves HTML pages from /var/htdocs and stores access logs in /var/logs.\\n2. An agent that creates HTML files and stores them in /var/html.\\n3. A log rotator that processes logs found in /var/logs.\\n\\nWithout volumes, each container would not be able to share data with the others, making it useless. However, by adding two volumes (one for HTML files and one for logs) and mounting them at appropriate paths inside each container, the pod can function as a cohesive system.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* Two scenarios are presented, one without shared storage and another with shared storage.\\n* In the first scenario (Figure 6.1), three containers within the same pod do not share storage and each has its own filesystem.\\n\\n**Important Details:**\\n\\n* The second scenario (Figure 6.2) introduces volumes, which allow multiple containers to share storage.\\n* Two volumes are created: \"publicHtml\" and \"logVol\".\\n* Each container can mount these shared volumes at different paths, allowing them to access the same data.\\n* This setup enables collaboration between containers within a pod without requiring each one to have its own separate filesystem.',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Linux allows you to mount filesystems at arbitrary locations in the file tree, making it possible for multiple containers to access the same files.\\n2. In Kubernetes, volumes can be mounted into containers to enable collaboration between them.\\n3. Volumes are bound to the lifecycle of a pod and will stay in existence only while the pod exists.\\n\\n**Important Details:**\\n\\n1. Two types of volumes are used in the example:\\n\\t* `publicHtml`: a volume shared by two containers (`WebServer` and `ContentAgent`) at different locations.\\n\\t* `logVol`: a volume shared by two containers (`WebServer` and `LogRotator`) at the same location.\\n2. Volumes can be of various types, including:\\n\\t* `emptyDir`: an empty directory used for storing transient data.\\n\\t* `hostPath`: a directory mounted from the worker node's filesystem into the pod.\\n\\t* `gitRepo`: a volume initialized by checking out the contents of a Git repository.\\n\\t* `nfs`: an NFS share mounted into the pod.\\n\\t* Cloud provider-specific storage types (e.g., `gcePersistentDisk`, `awsElasticBlockStore`, `azureDisk`).\\n3. To access a volume from within a container, you need to define a `VolumeMount` inside the container's spec.\\n\\n**Key Takeaways:**\\n\\n1. Volumes enable collaboration between containers by allowing them to share files.\\n2. Different types of volumes are available for various use cases and storage technologies.\\n3. Volumes are bound to the lifecycle of a pod, but their contents may persist even after the pod disappears.\",\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes provides various types of volumes for sharing data between containers.\\n2. These volume types include Cinder, CephFS, ISCSI, Flocker, GlusterFS, Quobyte, RBD, FlexVolume, vSphere-Volume, PhotonPersistentDisk, ScaleIO, and more.\\n3. Special types of volumes (ConfigMap, Secret, DownwardAPI) are used to expose Kubernetes resources and cluster information to pods.\\n\\n**Important Details:**\\n\\n1. A single pod can use multiple volumes of different types at the same time.\\n2. Each container in a pod can either have a volume mounted or not.\\n3. Volumes can be used for sharing data between containers, as well as for temporary storage by a single container.\\n4. The `emptyDir` volume is a simple type of volume that starts out empty and can be written to by an app running inside the pod.\\n\\n**Key Concepts:**\\n\\n1. Sharing data between containers using volumes\\n2. Types of volumes available in Kubernetes\\n3. Using `emptyDir` volumes for temporary storage or sharing files between containers',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* The chapter discusses creating a pod in Kubernetes that attaches disk storage to containers.\\n* Two images are required for this example: `fortune` and `nginx`.\\n* The `fortune` image is built from scratch using a Dockerfile.\\n\\n**Important Details:**\\n\\n* To build the `fortune` image, create a new directory called `fortune` and add two files:\\n\\t+ A shell script (`fortuneloop.sh`) that writes fortunes to an HTML file every 10 seconds.\\n\\t+ A Dockerfile that uses the Ubuntu image, installs the `fortune` binary, adds the shell script, and sets it as the entrypoint.\\n* Build and upload the `fortune` image to Docker Hub using the commands `$ docker build -t luksa/fortune .` and `$ docker push luksa/fortune`.\\n* Create a pod manifest file (`fortune-pod.yaml`) that defines a pod with two containers: one running the `fortune` image and one running an `nginx` image.\\n* The pod uses a shared volume to store the fortunes written by the `fortune` container.\\n\\nOverall, this chapter provides a step-by-step guide to creating a Kubernetes pod that attaches disk storage to multiple containers.',\n",
       "  \"Here's a summary of the text:\\n\\n**Main Points:**\\n\\n1. A pod contains two containers, `html-generator` and `web-server`, which share data using a shared volume called `html`.\\n2. The `html-generator` container writes HTML files to the `/var/htdocs` directory on the shared volume every 10 seconds.\\n3. The `web-server` container serves HTML files from the `/usr/share/nginx/html` directory, which is also mounted as read-only from the shared volume.\\n4. When a client sends an HTTP request to the pod on port 80, it receives the current fortune message written by the `html-generator` container.\\n\\n**Important Details:**\\n\\n* The shared volume is created using an `emptyDir` type, which allows data to be written to it without persisting across restarts.\\n* The `web-server` container serves files from the shared volume in read-only mode, ensuring that changes made by the `html-generator` container are not overwritten.\\n* To access the pod, a port is forwarded from the local machine to the pod using `kubectl port-forward`.\\n* An example command is provided to use `curl` to send an HTTP request to the pod and retrieve the current fortune message.\",\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Kubernetes allows you to attach disk storage (volumes) to containers, which can improve performance and functionality.\\n2. The `emptyDir` volume type creates an empty directory on the node's disk, but this can be slow if the node has a slow disk.\\n3. To create an `emptyDir` on a tmpfs filesystem (in memory), set the `medium` to `Memory`.\\n4. Another volume type is `gitRepo`, which clones a Git repository and checks out a specific revision when the pod starts up.\\n\\n**Important Details:**\\n\\n* An `emptyDir` volume is not kept in sync with the underlying Git repository, so changes made to the repo will not be reflected in the volume.\\n* However, if the pod is managed by a ReplicationController, deleting the pod will result in a new pod being created with the latest commits from the Git repository.\\n* A `gitRepo` volume can be used to store static HTML files or other data that needs to be pulled from a Git repository.\\n\\n**Key Concepts:**\\n\\n* Volumes (disk storage) attached to containers\\n* `emptyDir` and `gitRepo` volume types\\n* tmpfs filesystem (in-memory storage)\\n* ReplicationController (manages pod creation and updates)\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Sharing Data between Containers using Volumes**\\n\\n* The approach involves creating a pod that uses a Git repository as a volume, allowing containers to share data.\\n* However, this method requires deleting the pod every time changes are made to the Git repository and a new version is desired.\\n\\n**Creating a Pod with a Git Repository Volume**\\n\\n* Create a fork of a GitHub repository (e.g., https://github.com/luksa/kubia-website-example.git) to make changes.\\n* Create a pod using a YAML file (gitrepo-volume-pod.yaml) that specifies:\\n\\t+ A single Nginx container\\n\\t+ A single gitRepo volume, pointing to the forked repository and checking out the master branch\\n\\t+ The volume is mounted at /usr/share/nginx/html in the container\\n* When creating the pod, the Git repository is cloned into the root directory of the volume.\\n\\n**Key Details**\\n\\n* The gitRepo volume is initialized as an empty directory and then cloned with the specified repository.\\n* The revision checked out is determined by the master branch at the time the volume is created.\\n* This approach allows containers to share data from a Git repository, but requires deleting the pod to update the version being served.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Changes made to an HTML file in a GitHub repository will not be reflected on a Nginx web server unless the pod is deleted and recreated.\\n2. To keep the files in sync with the Git repository, an additional process (Git sync) can be run, which can be done using a sidecar container.\\n3. A sidecar container is a separate container that runs alongside the main container to perform additional tasks.\\n\\n**Important Details:**\\n\\n1. The `gitRepo` volume is not kept in sync with the Git repository by default.\\n2. To use a `gitRepo` volume with a private Git repository, you should use a Git sync sidecar or a similar method instead of relying on the `gitRepo` volume.\\n3. An example of using a Git sync container can be found in Chapter 18.\\n\\n**Key Takeaways:**\\n\\n* Using a sidecar container to run a Git sync process is a good way to keep files in sync with the Git repository.\\n* A `gitRepo` volume should not be used with private Git repositories without additional configuration.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. GitRepo and emptyDir volumes are dedicated directories created for each pod, and their contents are deleted when the pod is deleted.\\n2. HostPath volumes allow pods to access files on the worker node's filesystem, making them useful for system-level pods that need to read or write to the host's files.\\n\\n**Important Details:**\\n\\n* HostPath volumes point to a specific file or directory on the node's filesystem and are persistent across pod deletions.\\n* If a pod is deleted and a new one uses the same hostPath volume, it will see whatever was left behind by the previous pod if scheduled to the same node.\\n* This type of storage is useful for system-level pods managed by DaemonSets that need to access or write to the host's files.\\n\\n**Key Concepts:**\\n\\n* GitRepo and emptyDir volumes\\n* HostPath volumes\\n* Persistent storage across pod deletions\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Using `hostPath` volumes for storing database data or other sensitive data is not recommended, as it makes pods sensitive to node scheduling.\\n* System-wide pods in the `kube-system` namespace often use `hostPath` volumes to access node-level directories such as `/var/log`, `/var/lib/docker/containers`, and others.\\n\\n**Important Details:**\\n\\n* The text highlights that system-wide pods are using `hostPath` volumes for accessing node data, not storing their own data.\\n* This suggests that `hostPath` volumes should be used with caution and only for accessing node-level directories, rather than storing persistent data.\\n* The chapter will discuss other types of volumes that can be used for storing persistent data in multi-node clusters.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Persistent storage is needed for applications that require data to be stored on disk and accessible even when the pod is rescheduled.\\n* HostPath volumes should not be used to persist data across pods, but only to read or write system files on a node.\\n\\n**Using Persistent Storage:**\\n\\n* To use persistent storage, you need to store data on network-attached storage (NAS) that can be accessed from any cluster node.\\n* A pod running the MongoDB database requires a volume to persist data.\\n\\n**Using a GCE Persistent Disk:**\\n\\n* If using Google Kubernetes Engine (GKE), a GCE Persistent Disk is used as underlying storage.\\n* In earlier versions of Kubernetes, provisioning the underlying storage was manual; now it can be done automatically, but will be covered later in the chapter.\\n* To create a GCE Persistent Disk:\\n\\t+ Create the disk manually using the `gcloud` command.\\n\\t+ Specify the size and zone (e.g., `--size=1GiB --zone=europe-west1-b mongodb`).\\n\\t+ Verify that the disk is created successfully.\\n\\n**Important Details:**\\n\\n* The warning about selecting a disk size of under 200GB, which may result in poor I/O performance.\\n* The importance of creating the GCE Persistent Disk in the same zone as the Kubernetes cluster.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Creating a 1 GiB Google Cloud Engine (GCE) persistent disk called \"mongodb\" for use in a container.\\n2. Using the GCE persistent disk as a volume inside a MongoDB pod.\\n3. Configuring a YAML file to create a pod with a single container and a single volume backed by the GCE Persistent Disk.\\n\\n**Important Details:**\\n\\n* The GCE Persistent Disk is used to store data for the MongoDB container, which stores its data at /data/db.\\n* The filesystem type of the volume is EXT4 (a Linux filesystem).\\n* The name of the persistent disk must match the actual PD created earlier.\\n* If using Minikube, a hostPath volume can be used instead of a GCE Persistent Disk.\\n\\n**Key Files and Configurations:**\\n\\n* `mongodb-pod-gcepd.yaml`: A YAML file that configures a pod with a single container and a single volume backed by the GCE Persistent Disk.\\n* `gcePersistentDisk` configuration: Specifies the name, filesystem type, and mount path of the volume.',\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. The text describes how to write data to persistent storage using MongoDB in a Kubernetes environment.\\n2. It shows how to create a pod, run the MongoDB shell inside it, and store JSON documents persistently.\\n3. The text then demonstrates how to re-create the pod and verify that the persisted data can be read by the new pod.\\n\\n**Important Details:**\\n\\n* To write data to persistent storage, you need to run the MongoDB shell inside the container using `kubectl exec`.\\n* You can insert a new JSON document into the database using the `insert()` command.\\n* The `find()` command is used to retrieve and verify that the inserted document has been stored persistently.\\n* When re-creating the pod, it uses the same GCE persistent disk as the previous pod, so the data remains accessible.\\n* You can use `kubectl get po -o wide` to see which node a pod is scheduled to.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. You can persist data across multiple pod instances by using a Kubernetes persistent disk volume.\\n2. The type of volume used depends on the underlying infrastructure, such as Google Cloud Platform (GCE), Amazon Web Services (AWS), or Microsoft Azure.\\n\\n**Important Details:**\\n\\n* To use a GCE Persistent Disk volume, you create the actual storage and then set the properties in the volume definition.\\n* For AWS EC2, you can use an `awsElasticBlockStore` volume instead of `gcePersistentDisk`.\\n* To switch to an `awsElasticBlockStore` volume, you only need to change the `volume` section in the pod YAML file (see Listing 6.7).\\n* The process is similar for using other types of volumes, such as Azure File or Disk.\\n\\n**Key Takeaways:**\\n\\n* Persistent storage can be used across multiple pod instances.\\n* The type of volume used depends on the underlying infrastructure.\\n* Changing the volume type requires modifying the pod YAML file accordingly.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes supports various storage technologies, including persistent volumes (PVs), which can be used to attach storage to pods.\\n2. Persistent volumes can be created using different types, such as NFS, ISCSI, GlusterFS, RBD, FlexVolume, Cinder, CephFS, Flocker, and FC.\\n\\n**Important Details:**\\n\\n1. To use an NFS volume, you need to specify the NFS server IP and path exported by the server.\\n2. Other storage technologies have their own properties that need to be set, which can be found in the Kubernetes API reference or using `kubectl explain`.\\n3. Using infrastructure-related storage details in pod definitions is not recommended, as it makes the pod definition specific to a particular cluster and cannot be reused elsewhere.\\n4. Persistent volumes are a better way to attach persistent storage to pods, and will be covered in more detail in the next section.\\n\\n**Key Takeaways:**\\n\\n* Kubernetes supports various storage technologies for persistent volumes.\\n* Using infrastructure-related details in pod definitions is not recommended.\\n* Persistent volumes are a better approach to attaching persistent storage to pods.',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* Kubernetes aims to hide underlying infrastructure from developers, making applications portable across different environments.\\n* Developers should not need to know about storage technology used underneath their application.\\n* Cluster administrators should handle infrastructure-related dealings.\\n\\n**Important Details:**\\n\\n* To enable apps to request storage without dealing with infrastructure specifics, two new resources were introduced: PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs).\\n* PVs are provisioned by cluster admins and represent a type of network storage (e.g. NFS export).\\n* PVCs are created by users to request a specific amount of persistent storage.\\n* Kubernetes matches a PV with a PVC based on size and access mode, and binds the two together.\\n* Users can then create pods that reference the PVC, which provides the requested storage.\\n\\n**Key Concepts:**\\n\\n* PersistentVolumes (PVs): provisioned by cluster admins to represent network storage\\n* PersistentVolumeClaims (PVCs): created by users to request specific persistent storage\\n* Pod: a container that references a PVC to access the requested storage.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Decoupling Pods from Underlying Storage Technology**\\n\\n* Instead of developers adding technology-specific volumes to their pods, cluster administrators set up underlying storage and register it in Kubernetes using a `PersistentVolume` resource.\\n* The administrator specifies the size and access modes supported by the volume.\\n\\n**Using Persistent Volumes**\\n\\n* Users create a `PersistentVolumeClaim` manifest specifying the minimum size and access mode required.\\n* Kubernetes binds the claim to an available `PersistentVolume`.\\n* The `PersistentVolumeClaim` can be used as a volume inside a pod, but other users cannot use the same volume until it is released by deleting the bound claim.\\n\\n**Creating a Persistent Volume**\\n\\n* Cluster administrators create a `PersistentVolume` resource in Kubernetes using a manifest (e.g. `mongodb-pv-gcepd.yaml`).\\n* The manifest specifies the size and access modes supported by the volume.\\n* The `PersistentVolume` is backed by an underlying storage technology, such as a GCE Persistent Disk.\\n\\n**Important Details**\\n\\n* The `persistentVolumeReclaimPolicy` should be set to \"Retain\" to prevent the volume from being erased or deleted after it is released.\\n* The `fsType` should be specified (e.g. ext4) to ensure compatibility with the underlying storage technology.',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. Creating a PersistentVolume (PV) requires specifying its capacity, access modes, and release policy.\\n2. The administrator must also specify the type, location, and properties of the actual storage backing the PV.\\n3. A PV can be claimed by a pod or other resources using a PersistentVolumeClaim.\\n\\n**Important Details:**\\n\\n* When creating a PV, the admin needs to specify its capacity, access modes (RWO, ROX), and release policy (e.g., Retain).\\n* The PV\\'s storage properties are specified in the same way as when referencing a GCE Persistent Disk directly.\\n* A PV is a cluster-level resource, not belonging to any namespace.\\n* After creating a PV, it can be listed using `kubectl get pv` and will show its status as \"Available\" until a claim is made.\\n\\n**Key Commands:**\\n\\n* `kubectl create` command to create a PV\\n* `kubectl get pv` command to list all PVs',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. To deploy a pod that requires persistent storage, you need to \"claim\" a PersistentVolume first.\\n2. This process is separate from creating a pod.\\n\\n**Important Details:**\\n\\n* A PersistentVolumeClaim (PVC) is created separately from a pod.\\n* The PVC stays available even if the pod is rescheduled.\\n* To create a PVC, you prepare a manifest file like Listing 6.11 and post it to the Kubernetes API using `kubectl create`.\\n* The name of the PVC is important, as it will be used later when referencing the claim in a pod\\'s volume.\\n\\n**Key Concepts:**\\n\\n* PersistentVolume (PV)\\n* PersistentVolumeClaim (PVC)\\n* Rescheduling (pod deletion and recreation)',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* A PersistentVolumeClaim (PVC) is created to request storage for a container, in this case 1 GiB of storage with ReadWriteOnce access mode.\\n* The PVC is bound to a PersistentVolume (PV) that matches the requested capacity and access modes.\\n* The PV\\'s status changes from \"Available\" to \"Bound\" after being attached to the PVC.\\n\\n**Important Details:**\\n\\n* The PVC is created with `resources.requests.storage` set to 1Gi, which means it requests 1 GiB of storage.\\n* The PVC requires a ReadWriteOnce access mode, meaning only one node can mount the volume for reading and writing.\\n* The PV\\'s capacity must be large enough to accommodate the requested storage (in this case, 1 GiB).\\n* The PV\\'s access modes must include the required access mode (ReadWriteOnce) for the PVC.\\n* The PVC is shown as \"Bound\" to the PV in the `kubectl get pvc` command output.\\n* The PV is also updated to show it is bound to the PVC, with its status changing from \"Available\" to \"Bound\".\\n\\n**Additional Information:**\\n\\n* The access modes used are:\\n\\t+ RWO (ReadWriteOnce): only one node can mount the volume for reading and writing.\\n\\t+ ROX (ReadOnlyMany): multiple nodes can mount the volume for reading.\\n\\t+ RWX (ReadWriteMany): multiple nodes can mount the volume for both reading and writing.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Persistent Volumes (PVs) are cluster-scoped resources that cannot be created in a specific namespace.\\n2. Persistent Volume Claims (PVCs), on the other hand, can only be created in a specific namespace and can be used by pods in the same namespace.\\n3. To use a PV inside a pod, you need to reference the PVC by name in the pod's volume configuration.\\n\\n**Important Details:**\\n\\n* A PV is tied to a specific underlying storage technology (e.g., GCE PD) and cannot be claimed by another pod until it is released.\\n* When referencing a PVC in a pod's volume configuration, use the `claimName` field to specify the name of the PVC.\\n* To verify that the pod is using the correct PV and its underlying storage, check the pod's logs or run a command like `kubectl exec -it <pod_name> mongo` to access the MongoDB shell.\\n\\n**Key Concepts:**\\n\\n* Persistent Volumes (PVs)\\n* Persistent Volume Claims (PVCs)\\n* Decoupling pods from underlying storage technology\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* Kubernetes provides two ways for pods to access storage: directly attaching to a disk or using a PersistentVolume (PV) and claim.\\n* Using a PV and claim is simpler for developers, as it abstracts away the underlying storage technology.\\n* This approach also makes it easier to deploy the same pod and claim manifests across multiple Kubernetes clusters.\\n\\n**Important Details:**\\n\\n* A PersistentVolumeClaim (PVC) specifies the amount of storage needed and access permissions (e.g., read/write).\\n* A Pod references a PVC by name in its volumeMounts section.\\n* A PV is created separately, specifying the underlying storage technology (in this case, a GCE Persistent Disk).\\n* Using a PV and claim decouples the pod from infrastructure-specific details, making it easier to deploy across different environments.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When a PersistentVolumeClaim (PVC) is deleted, its associated PersistentVolume (PV) remains in a \"Released\" state.\\n2. If a new PVC is created, it will not be bound to the existing PV unless the cluster admin manually recycles or deletes the PV.\\n3. The default reclaim policy for PVs is \"Retain\", which means Kubernetes retains the volume and its contents after it\\'s released from its claim.\\n\\n**Important Details:**\\n\\n* When a PVC is deleted, its associated PV remains in a \"Released\" state to prevent data leakage between different pods or namespaces.\\n* To manually recycle a PV, you must delete and recreate the PersistentVolume resource, which allows you to decide what to do with the files on the underlying storage.\\n* There are three possible reclaim policies for PVs: Retain (default), Recycle, and Delete. The Recycle policy deletes the volume\\'s contents and makes it available again, while the Delete policy deletes the underlying storage.\\n\\n**Key Takeaways:**\\n\\n* Be aware of the reclaim policy when creating a PVC to ensure data security and consistency.\\n* Understand how to manually recycle or delete PVs to manage their lifecycle and prevent data leakage.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* Persistent Volumes (PVs) have two reclaim policies: Retain or Delete.\\n* Other PV types may support different reclaim policies, so check before creating your own PV.\\n* You can change the reclaim policy on an existing PV from Delete to Retain to prevent data loss.\\n\\n**Dynamic Provisioning of Persistent Volumes:**\\n\\n* Kubernetes can automatically provision persistent storage through dynamic provisioning of PVs.\\n* A cluster administrator deploys a provisioner and defines one or more StorageClass objects, allowing users to choose the type of PV they want.\\n* Users refer to the StorageClass in their PersistentVolumeClaims, and the provisioner takes that into account when provisioning storage.\\n\\n**Important Details:**\\n\\n* StorageClass resources are not namespaced, like Persistent Volumes.\\n* Kubernetes includes provisioners for popular cloud providers, but a custom provisioner may be needed if deployed on-premises.\\n* The lifespan of PVs, PersistentVolumeClaims, and pods using them is illustrated in Figure 6.9.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Dynamic Provisioning of Persistent Volumes**\\n\\n* Instead of pre-provisioning multiple Persistent Volumes, administrators can define one or more Storage Classes to dynamically create new Persistent Volumes as needed.\\n* This approach prevents running out of Persistent Volumes, but not storage space.\\n\\n**Defining Storage Classes**\\n\\n* Administrators must create a Storage Class resource before users can request a Persistent Volume Claim (PVC).\\n* A Storage Class defines the provisioner to use and passes parameters specific to each provisioner plugin.\\n* The example provided uses the Google Compute Engine (GCE) Persistent Disk (PD) provisioner.\\n\\n**Requesting a Specific Storage Class in a PVC**\\n\\n* Users can refer to the storage class by name in their PVC definition.\\n* An updated YAML definition of a PVC is shown, requesting dynamic provisioning using a specific storage class.',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n* A PersistentVolumeClaim (PVC) was created with a specified size, access modes, and storage class (\"fast\").\\n* The PVC was created using a provisioner referenced in the \"fast\" StorageClass resource.\\n* When creating the PVC, an existing manually provisioned PersistentVolume (PV) that matches the claim is ignored.\\n\\n**Important Details:**\\n\\n* If a non-existent storage class is referenced in a PVC, provisioning of the PV will fail.\\n* The PVC was created with a capacity of 1Gi and access modes of RWO (Read-Write-Once).\\n* A new PersistentVolume (PV) named \"pvc-1e6bc048\" was dynamically provisioned to match the PVC.\\n* The PV has a capacity of 1Gi, access modes of RWO, and a reclaim policy of Delete.\\n* The provisioner used is kubernetes.io/gce-pd, which provisions GCE Persistent Disks.\\n* A new disk was created with a size of 1GB in the europe-west1-d zone.\\n\\n**Key Takeaways:**\\n\\n* The PVC was successfully bound to a dynamically provisioned PV using the \"fast\" StorageClass resource.\\n* The PV has the requested capacity and access modes, and will be deleted when the PVC is deleted.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Dynamic provisioning of PersistentVolumes (PVs) allows for flexible storage allocation.\\n2. StorageClasses enable multiple storage options with different characteristics to be created by cluster admins.\\n3. Developers can choose which StorageClass to use for each PersistentVolume Claim (PVC).\\n4. PVC definitions are portable across clusters as long as the StorageClass names are the same.\\n\\n**Important Details:**\\n\\n1. The `fast` StorageClass was created manually and is an SSD, as specified in the storageclass.yaml file.\\n2. There are multiple storage classes available in GKE, including a default \"standard\" class.\\n3. In Minikube, there are also multiple storage classes available, including a default \"standard\" class.\\n4. The `kubectl get sc` command can be used to list all storage classes in a cluster.\\n\\n**Key Concepts:**\\n\\n1. Dynamic provisioning of PVs\\n2. StorageClasses and their characteristics\\n3. PVC portability across clusters\\n4. Default storage classes (e.g. \"standard\")',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. The default storage class in a GKE cluster can be examined using `kubectl get sc standard -o yaml`.\\n2. The default storage class uses the kubernetes.io/gce-pd provisioner to create Persistent Volumes (PVs) of type pd-standard.\\n3. A Persistent Volume Claim (PVC) can be created without specifying a storageClassName attribute, and a GCE Persistent Disk of type pd-standard will be provisioned automatically.\\n\\n**Important Details:**\\n\\n* The default storage class is marked with an annotation `storageclass.beta.kubernetes.io/is-default-class: \"true\"` to indicate that it is the default.\\n* The type parameter in the storage class definition specifies the type of GCE PD to create (in this case, pd-standard).\\n* The provisioner used by the default storage class is kubernetes.io/gce-pd.\\n* When creating a PVC without specifying a storageClassName attribute, a GCE Persistent Disk of type pd-standard will be automatically provisioned.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Dynamic provisioning of Persistent Volumes (PVs) allows for automatic creation of PVs based on a Persistent Volume Claim (PVC).\\n2. A PVC can be created without specifying a storage class, in which case it will use the default storage class.\\n3. To bind a PVC to a pre-provisioned PV, specify an empty string as the storage class name.\\n\\n**Important Details:**\\n\\n* When creating a PVC without specifying a storage class, the dynamic volume provisioner will automatically create a new PV.\\n* If you want the PVC to use a pre-provisioned PV instead of dynamically provisioning one, explicitly set the `storageClassName` attribute to an empty string (`\"\"`).\\n* This ensures that the PVC binds to a manually pre-provisioned PV rather than creating a new one.\\n\\n**Key Takeaways:**\\n\\n* Dynamic provisioning of PVs is convenient but may not always be desirable.\\n* Specifying an empty string as the storage class name can help you control which PV is used by the PVC.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Volumes are used in Kubernetes to provide storage to containers.\\n2. There are different types of volumes, including emptyDir, gitRepo, hostPath, and PersistentVolumes (PVs).\\n3. PVs can be dynamically provisioned using a provisioner.\\n\\n**Key Details:**\\n\\n* To use a volume, you need to create a pod with a volume reference.\\n* The `emptyDir` volume is used for temporary storage that is deleted when the pod is deleted.\\n* The `gitRepo` volume allows you to populate a directory with data from a Git repository at pod startup.\\n* The `hostPath` volume allows you to access files on the host node.\\n* PersistentVolumes (PVs) can be dynamically provisioned using a provisioner, which creates actual storage and binds it to a PersistentVolumeClaim (PVC).\\n* To use dynamic provisioning, you need to:\\n\\t1. Set up a PersistentVolume provisioner.\\n\\t2. Create one or more StorageClasses and mark one as the default.\\n\\t3. Create a PVC referencing one of the StorageClasses.\\n\\t4. Create a pod with a volume referencing the PVC by name.\\n\\n**Important Concepts:**\\n\\n* PersistentVolumes (PVs)\\n* PersistentVolumeClaims (PVCs)\\n* StorageClasses\\n* Provisioners',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes applications require configuration data, which should not be baked into the built app itself.\\n2. This chapter covers how to pass configuration options to apps running in Kubernetes.\\n\\n**Important Details:**\\n\\n* There are four ways to configure containerized applications:\\n\\t+ Passing command-line options to the app\\n\\t+ Setting environment variables exposed to the app\\n\\t+ Configuring apps through ConfigMaps (a new topic covered in this chapter)\\n\\t+ Passing sensitive information through Secrets (also a new topic covered in this chapter)\\n\\n**Key Concepts:**\\n\\n* ConfigMaps and Secrets are used to pass configuration data to Kubernetes applications.\\n* These tools allow for secure and flexible management of application settings.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Configuring applications in containers can be done through various methods, including command-line arguments, environment variables, configuration files, and volumes.\\n2. Environment variables are a popular method for passing configuration options to containerized applications due to their ease of use and flexibility.\\n3. However, using configuration files inside Docker containers can be tricky and may require rebuilding the image or exposing sensitive information.\\n\\n**Important Details:**\\n\\n1. Kubernetes offers a resource called ConfigMap that allows storing configuration data in a top-level Kubernetes resource, making it easier to manage and secure.\\n2. There are three ways to configure applications:\\n\\t* Passing command-line arguments to containers\\n\\t* Setting custom environment variables for each container\\n\\t* Mounting configuration files into containers through a special type of volume\\n3. ConfigMaps can be used to store sensitive information such as credentials, private encryption keys, and similar data that needs to be kept secure.\\n4. Kubernetes also offers another resource called Secret that is specifically designed to handle sensitive information with care.\\n\\n**Key Takeaways:**\\n\\n1. Understanding the different methods for configuring applications in containers is crucial for effective deployment and management.\\n2. ConfigMaps and Secrets are essential resources in Kubernetes that provide a secure way to store and manage configuration data and sensitive information.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Passing Command-Line Arguments to Containers**\\n\\nWhen running a Docker container, you can pass command-line arguments to override those specified in the image. This section explains how to do that.\\n\\n**Defining Command and Arguments in Docker**\\n\\nIn a Dockerfile, two instructions define the command and arguments:\\n\\n* `ENTRYPOINT` defines the executable invoked when the container is started.\\n* `CMD` specifies the default arguments passed to `ENTRYPOINT`.\\n\\nThe correct way to specify the command is through `ENTRYPOINT`, and only use `CMD` if you want to define default arguments.\\n\\n**Understanding ENTRYPOINT and CMD**\\n\\nThere are two forms of these instructions:\\n\\n* `shell` form: e.g. `ENTRYPOINT node app.js`\\n* `exec` form: e.g. `ENTRYPOINT [\"node\", \"app.js\"]`\\n\\nThe difference is whether the command is invoked inside a shell or not.\\n\\n**Example in the Kubia Image**\\n\\nIn the kubia image, the `ENTRYPOINT` instruction uses the `exec` form:\\n```bash\\nENTRYPOINT [\"node\", \"app.js\"]\\n```\\nThis runs the node process directly, without invoking it inside a shell. If you used the `shell` form instead, the container would run `/bin/sh -c node app.js`, which is different.\\n\\n**Passing Arguments to Containers**\\n\\nYou can pass arguments to containers by running:\\n```bash\\ndocker run <image> <arguments>\\n```\\nThese arguments will override any default arguments specified in the image.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. When using the `ENTRYPOINT` instruction in a Dockerfile, it's recommended to use the `exec` form instead of running a shell process.\\n2. The example shows how to modify a script (fortune) and its Docker image to make the delay interval configurable through a command-line argument.\\n3. A variable `INTERVAL` is added to the script to store the value of the first command-line argument.\\n\\n**Important Details:**\\n\\n* In the modified script, the `sleep` command uses the `$INTERVAL` variable to pause for the specified number of seconds between generating fortunes.\\n* The Dockerfile uses the `exec` form of the `ENTRYPOINT` instruction and sets a default interval of 10 seconds using the `CMD` instruction.\\n* The image is built and pushed to Docker Hub with the tag `args`, allowing users to specify their own interval value when running the container.\\n\\n**Key Takeaways:**\\n\\n* Using the `exec` form of the `ENTRYPOINT` instruction can improve performance by avoiding unnecessary shell processes.\\n* Configuring applications through command-line arguments can make them more flexible and user-friendly.\",\n",
       "  'Here is a summary of the text:\\n\\n**Passing Command-Line Arguments to Containers**\\n\\nThe text discusses how to pass command-line arguments to containers, both in Docker and Kubernetes. The main points are:\\n\\n* In Docker, you can override the default sleep interval by passing it as an argument: `$ docker run -it docker.io/luksa/fortune:args 15`\\n* In Kubernetes, you can override both the ENTRYPOINT and CMD commands by setting the `command` and `args` fields in the container specification.\\n* The `command` field specifies the executable to be executed inside the container, while the `args` field specifies the arguments passed to it.\\n* You can only set custom arguments and rarely override the command (except for general-purpose images like busybox).\\n* The `command` and `args` fields cannot be updated after the pod is created.\\n\\n**Running a Pod with a Custom Interval**\\n\\nThe text also provides an example of running a pod with a custom delay interval using a YAML file. To do this:\\n\\n* Copy the original YAML file (`fortune-pod.yaml`) to a new file called `fortune-pod-args.yaml`.\\n* Modify the new file by adding a `command` and `args` field, as shown in Listing 7.4.\\n* Run the pod with the custom interval using the modified YAML file: `$ kubectl apply -f fortune-pod-args.yaml`.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* Configuring applications in Kubernetes involves using ConfigMaps and Secrets to pass configuration options to containers.\\n* In this chapter, we are exploring how to configure applications through command-line arguments and environment variables.\\n\\n**Important Details:**\\n\\n* To pass command-line arguments to a container, use the `args` array in the container definition. For example: `args: [\"2\"]`.\\n* If you have multiple arguments, you can use either the array notation or the following notation: `args: - foo - bar - \"15\"`.\\n* Environment variables can be specified for each container of a pod, but not at the pod level.\\n* Environment variables cannot be updated after the pod is created.\\n* Using environment variables allows containers to inherit configuration options from their parent pod.\\n\\n**Key Concepts:**\\n\\n* ConfigMaps and Secrets\\n* Command-line arguments\\n* Environment variables',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. To make a container's configuration configurable through an environment variable, you can remove the initialization of the variable from your script.\\n2. In a bash script, this means simply removing the line where the variable is initialized.\\n3. If your app was written in Java, Node.js, or Python, you would use a different method to access the environment variable.\\n\\n**Important Details:**\\n\\n* To specify an environment variable in a container definition, you add it to the `env` section of the container specification.\\n* In Kubernetes, environment variables are automatically exposed for each service in the same namespace.\\n* You set environment variables at the container level, not at the pod level.\\n* The example shows how to create a new image with an environment variable and run it as a pod.\\n\\n**Key Terms:**\\n\\n* Environment variables\\n* Container definition\\n* Kubernetes\\n* Pod\\n* Namespace\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Environment variables can reference previously defined environment variables using the $(VAR) syntax.\\n2. ConfigMaps are used to separate configuration options from pod definitions, allowing for reuse across multiple environments.\\n3. Using ConfigMaps decouples configuration from application source code.\\n\\n**Important Details:**\\n\\n* Environment variables can be referenced within each other's values using the $(VAR) syntax (e.g., `SECOND_VAR` referencing `FIRST_VAR`).\\n* Hardcoding environment variables in pod definitions requires separate definitions for production and development environments.\\n* ConfigMaps are a Kubernetes resource that contains key-value pairs, allowing configuration options to be separated from pod definitions.\\n* ConfigMaps can pass values to containers as either environment variables or files in a volume.\\n* Using ConfigMaps enables reuse of the same pod definition across multiple environments.\",\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Kubernetes allows you to pass ConfigMap entries as command-line arguments to processes using the $(ENV_VAR) syntax.\\n2. Applications can also read ConfigMap contents directly through the Kubernetes REST API endpoint, but it's recommended to keep apps Kubernetes-agnostic unless necessary.\\n3. Using a separate ConfigMap object allows for multiple manifests with the same name, each for a different environment (e.g., dev, test, prod).\\n4. Pods reference ConfigMaps by name, enabling the use of different configs in each environment while using the same pod specification across all environments.\\n\\n**Important Details:**\\n\\n* ConfigMaps can be used to keep multiple configurations separate and organized.\\n* Using ConfigMaps allows for a single pod specification to be used across different environments (e.g., dev, test, prod).\\n* The figures (7.2 and 7.3) illustrate how pods use ConfigMaps through environment variables and configMap volumes, and how multiple ConfigMaps with the same name can be used in different environments.\",\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Creating a ConfigMap in Kubernetes using the `kubectl create configmap` command.\\n2. Using a ConfigMap to configure an application's environment variables.\\n\\n**Important Details:**\\n\\n* To create a ConfigMap, use the `kubectl create configmap` command instead of `kubectl create -f`.\\n* You can define a ConfigMap with multiple entries by passing literals using the `--from-literal` argument.\\n* ConfigMap keys must be valid DNS subdomains and may contain alphanumeric characters, dashes, underscores, and dots.\\n* To inspect a ConfigMap's YAML descriptor, use the `kubectl get configmap` command with the `-o yaml` option.\\n\\n**Example:**\\n\\n* Creating a simple ConfigMap with a single entry using literals:\\n```\\n$ kubectl create configmap fortune-config --from-literal=sleep-interval=25\\n```\\n* Creating a ConfigMap with multiple entries using literals:\\n```\\n$ kubectl create configmap myconfigmap --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two\\n```\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Creating a ConfigMap from a YAML file using `kubectl create -f fortune-config.yaml`\\n2. Creating a ConfigMap from multiple files or directories using `kubectl create configmap`\\n3. Using various options to customize the creation process, such as specifying keys for individual files\\n\\n**Important Details:**\\n\\n* To create a ConfigMap from a file, use `--from-file` with the file path and optional key\\n* Multiple files can be imported by using `--from-file` multiple times or importing an entire directory\\n* Files in a directory will be imported as individual map entries if their names are valid ConfigMap keys\\n* A combination of options can be used to create a ConfigMap from different sources, such as files, directories, and literal values.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* To get values from a ConfigMap into a pod\\'s container, you have three options.\\n* One option is to set an environment variable using the `valueFrom` field in the pod descriptor.\\n\\n**Important Details:**\\n\\n* The `valueFrom` field allows you to specify a ConfigMap entry as an environment variable for a container.\\n* In the example provided, a ConfigMap named \"my-config\" is referenced and its entries are used to set environment variables in a pod\\'s container.\\n* The pod descriptor uses the `env` field to specify the environment variables, which are populated from the ConfigMap using the `valueFrom` field.\\n\\n**Key Terms:**\\n\\n* ConfigMap: a Kubernetes object that stores configuration data as key-value pairs.\\n* Pod descriptor: a YAML or JSON file that defines a pod and its configuration.\\n* Environment variable: a variable that is set in a container\\'s environment, which can be used by the application running inside the container.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. A Kubernetes pod can reference a ConfigMap to set an environment variable.\\n2. This allows configuration to be decoupled from the pod specification, making it easier to manage and update configurations across multiple pods.\\n\\n**Important Details:**\\n\\n* If a referenced ConfigMap does not exist when the pod is created, the container referencing it will fail to start, but the other containers will continue to run.\\n* However, if the missing ConfigMap is created later, the failed container can be started without requiring the pod to be recreated.\\n* A reference to a ConfigMap can also be marked as optional, allowing the container to start even if the ConfigMap does not exist.\\n\\n**Example:**\\n\\n* In this example, an environment variable called INTERVAL is set to the value stored in the \"sleep-interval\" key of a ConfigMap named \"fortune-config\".\\n* This allows the html-generator container to read the INTERVAL environment variable and use its value (25) as needed.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. In Kubernetes 1.6 and later, you can expose all entries of a ConfigMap as environment variables at once using the `envFrom` attribute.\\n2. This allows you to avoid creating individual environment variables for each entry in the ConfigMap.\\n\\n**Important Details:**\\n\\n* The `envFrom` attribute can be used instead of `env` to create environment variables from a ConfigMap.\\n* You can specify a prefix for the environment variables, which will be added to the name of each key in the ConfigMap. If no prefix is specified, the environment variable names will match the keys exactly.\\n* However, if a ConfigMap key contains a dash (e.g., `FOO-BAR`), it will not be converted to an underscore and will be skipped as an environment variable.\\n* To pass values from a ConfigMap as arguments to the main process running in the container, you can first initialize an environment variable from the ConfigMap entry and then reference that variable inside the arguments.\\n\\n**Key Takeaways:**\\n\\n* Using `envFrom` with a prefix can simplify the creation of environment variables from a ConfigMap.\\n* Be aware of the limitations on valid environment variable names (e.g., no dashes).\\n* You may need to use workarounds, such as initializing an environment variable and then referencing it in arguments, to pass values from a ConfigMap to the main process running in the container.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. To pass configuration options to a container, you can use environment variables or command-line arguments.\\n2. However, when dealing with longer config values, it\\'s more suitable to use a ConfigMap.\\n3. A ConfigMap volume can expose each entry in the ConfigMap as a file, which can be read by the process running in the container.\\n\\n**Important Details:**\\n\\n* To reference an environment variable in a command-line argument, you can use the `$(ENV_VARIABLE_NAME)` syntax.\\n* In the example provided, a ConfigMap named \"fortune-config\" is used to expose its entries as files, and one of those entries (named \"sleep-interval\") is referenced in a command-line argument using the `$(INTERVAL)` syntax.\\n* The image being used takes the interval value from the first argument, not from an environment variable.',\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n* ConfigMaps are used to pass configuration data to containers, but can also be used for short single values.\\n* A new example is presented where a ConfigMap is created to configure an Nginx web server running inside a pod's container.\\n\\n**Important Details:**\\n\\n* To create the ConfigMap, a directory called `configmap-files` is created and two files are stored in it:\\n\\t+ `my-nginx-config.conf`: contains the Nginx configuration file with gzip compression enabled.\\n\\t+ `sleep-interval`: contains a single value of 25.\\n* The ConfigMap is created from the contents of the `configmap-files` directory using the command: `$ kubectl create configmap fortune-config --from-file=configmap-files`\\n* The resulting ConfigMap, called `fortune-config`, can be used to configure the Nginx web server running inside a pod's container.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. A ConfigMap named \"fortune-config\" was created with two entries: \"my-nginx-config.conf\" and \"sleep-interval\".\\n2. The ConfigMap contains configuration files for Nginx and a sleep interval value.\\n3. To use the ConfigMap\\'s entries in a pod, create a volume that references the ConfigMap by name and mounts it in a container.\\n\\n**Important Details:**\\n\\n1. The ConfigMap\\'s YAML definition is shown, with two entries containing multi-line values.\\n2. Nginx reads its config file from /etc/nginx/nginx.conf, but you can add custom configuration files to the /etc/nginx/conf.d/ directory.\\n3. To avoid overriding the default Nginx config file, create a separate volume for your custom config file and mount it in the container.\\n\\n**Key Takeaways:**\\n\\n* ConfigMaps allow you to decouple configuration from code.\\n* You can use ConfigMap entries as files within a pod\\'s containers.\\n* Be mindful of how Nginx reads its config files to avoid overriding default configurations.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* A Kubernetes Pod definition is provided that references a ConfigMap named \"fortune-config\".\\n* The ConfigMap is mounted as a volume in the Pod, specifically at the path \"/etc/nginx/conf.d\".\\n* This allows Nginx to use the configuration from the ConfigMap.\\n* To verify this, port-forwarding is enabled and curl is used to check the server\\'s response.\\n\\n**Important Details:**\\n\\n* The ConfigMap named \"fortune-config\" contains configuration entries that are mounted as files in the Pod.\\n* The volume is mounted at \"/etc/nginx/conf.d\", which allows Nginx to use the configuration from the ConfigMap.\\n* Port-forwarding and curl are used to verify that Nginx is using the mounted config file.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The author demonstrates how to use a ConfigMap to configure a pod\\'s containers, specifically an Nginx server and a fortuneloop container.\\n2. The ConfigMap is used to populate a volume that is mounted in the /etc/nginx/conf.d directory of the web-server container.\\n3. The author shows how to select specific entries from the ConfigMap to be exposed as files in the volume using the `items` attribute.\\n\\n**Important Details:**\\n\\n* The author uses `kubectl port-forward` to forward traffic from a local port to the pod\\'s port, and then uses `curl` to verify that the Nginx server is responding correctly.\\n* The ConfigMap is named \"fortune-config\" and contains two entries: `my-nginx-config.conf` and `sleep-interval`.\\n* The author notes that it feels wrong to use multiple ConfigMaps to configure containers of the same pod, but instead uses a single ConfigMap with specific entries exposed as files in the volume.\\n* The `items` attribute is used to specify which entries from the ConfigMap should be included in the volume.',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. When mounting a ConfigMap or Secret as a directory, existing files in that directory are hidden by default.\\n2. This can have unintended consequences if mounted to an important directory like /etc.\\n3. To add individual files from a ConfigMap without hiding other files, use the `subPath` property on the volumeMount.\\n\\n**Important Details:**\\n\\n* When mounting a directory, existing files in that directory are inaccessible for as long as the filesystem is mounted.\\n* The `subPath` property allows you to mount either a single file or a single directory from the volume instead of mounting the whole volume.\\n* By using `subPath`, individual files can be added to an existing directory without affecting other files.\\n\\n**Example:**\\n\\n* A ConfigMap contains two files: `myconfig.conf` and `another-file`.\\n* To add `myconfig.conf` to the /etc directory as `someconfig.conf`, use the `subPath` property in the pod definition.\\n* The resulting filesystem would have `/etc/someconfig.conf` containing the contents of `myconfig.conf`, while other files like `existingfile1` and `existingfile2` remain unaffected.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Decoupling configuration from an application using a ConfigMap allows for updating configuration without restarting the app.\\n2. A ConfigMap can be mounted as a volume in a pod, making its contents available to the application.\\n3. The `subPath` property can be used to mount only specific files or parts of a volume instead of the entire volume.\\n\\n**Important Details:**\\n\\n* By default, file permissions in a ConfigMap volume are set to 644 (-rw-r--r--).\\n* The `defaultMode` property can be used to change the file permissions for all files in a ConfigMap volume.\\n* Updating a ConfigMap will update the files in all volumes referencing it, but this may take up to one minute.\\n* Kubernetes may eventually support sending a signal to the container after updating the files.\\n\\n**Key Takeaways:**\\n\\n* Using a ConfigMap and mounting it as a volume allows for dynamic configuration updates without restarting the app.\\n* The `subPath` property can be used to mount individual files or parts of a volume, but this method has limitations related to file updates.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. ConfigMaps can be edited using `kubectl edit` to update the configuration files exposed in the pod.\\n2. The updated ConfigMap will eventually propagate to the actual file in the volume, but it may take some time.\\n3. Nginx does not automatically reload its config files when they are updated, so a manual reload is required.\\n\\n**Important Details:**\\n\\n* To edit a ConfigMap, use `kubectl edit <configmap_name>`.\\n* The updated ConfigMap will update the actual file in the volume atomically, meaning all updates occur at once.\\n* Nginx can be signaled to reload its config files using `nginx -s reload` within the pod.\\n* If an app detects config file changes and reloads them before Kubernetes has finished updating all files, it cannot happen because of atomic updates.\\n\\n**Key Takeaways:**\\n\\n* ConfigMaps can be updated without restarting the container or recreating the pod.\\n* Nginx requires a manual reload to apply changes to its config files.\\n* The update process is atomic, ensuring that all files in the ConfigMap volume are updated simultaneously.',\n",
       "  \"Here's a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When using ConfigMaps to pass data to containers, files in mounted volumes are symbolic links that point to files in a directory called `.data`.\\n2. Updating a ConfigMap can cause issues if files are mounted individually instead of as a whole volume.\\n3. Modifying an existing ConfigMap while pods are using it may not be a good idea, especially if the app doesn't support reloading its configuration.\\n\\n**Important Details:**\\n\\n* When updating a ConfigMap, Kubernetes creates a new directory and re-links the `.data` symbolic link to the new directory, effectively changing all files at once.\\n* If individual files are mounted instead of the whole volume, they will not be updated when the ConfigMap is updated.\\n* One workaround for this issue is to mount the whole volume into a different directory and create a symbolic link pointing to the file in question.\\n* Modifying an existing ConfigMap can cause issues with immutability, especially if the app doesn't support reloading its configuration. This can result in different running instances being configured differently.\\n* If the app does support reloading, modifying the ConfigMap may not be a big deal, but it's still important to be aware that files in individual pods may be out of sync for up to a whole minute.\\n\\n**Context:**\\n\\nThe text is discussing how to use ConfigMaps and Secrets to pass sensitive data to containers in Kubernetes. The main points are relevant to anyone using these features to manage configuration data in their applications.\",\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes provides a separate object called a Secret to store sensitive information.\\n2. Secrets are similar to ConfigMaps but are used for storing sensitive data that needs to be kept secure.\\n3. Secrets can be used in the same way as ConfigMaps, such as passing entries to containers as environment variables or exposing them as files in a volume.\\n\\n**Important Details:**\\n\\n1. Kubernetes helps keep Secrets safe by distributing them only to nodes that run pods that need access to the Secret.\\n2. On the master node (etcd), Secrets were previously stored unencrypted but are now encrypted from version 1.7 onwards, making the system more secure.\\n3. To choose between using a ConfigMap or a Secret:\\n\\t* Use a ConfigMap for non-sensitive, plain configuration data.\\n\\t* Use a Secret for sensitive data that needs to be kept under key.\\n4. A default token Secret is automatically mounted into every container and can be listed with `kubectl get secrets`.\\n\\n**Key Concepts:**\\n\\n1. Secrets are used to store sensitive information in Kubernetes.\\n2. ConfigMaps and Secrets have similar uses but differ in their purpose (ConfigMaps for non-sensitive data, Secrets for sensitive data).\\n3. The default token Secret is a built-in Secret that is automatically mounted into every container.',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. Kubernetes Secrets are used to pass sensitive data to containers.\\n2. The `kubectl describe` command can be used to view information about a Secret.\\n3. A Secret contains multiple entries, which can be accessed as files within a container.\\n\\n**Important Details:**\\n\\n* By default, the \"default-token\" Secret is mounted into every container.\\n* You can disable this by setting the `automountServiceAccountToken` field in the pod spec to false or on the service account used by the pod.\\n* The Secret contains three entries: `ca.crt`, `namespace`, and `token`.\\n* These files are mounted at `/var/run/secrets/kubernetes.io/serviceaccount/` within a container.\\n* You can use `kubectl exec` to view the contents of this directory.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* Creating a Secret to configure an application (in this case, Nginx) to serve HTTPS traffic\\n* Generating a certificate and private key files using OpenSSL\\n* Creating a dummy file called \"foo\" containing the string \"bar\"\\n* Using `kubectl create secret` to create a Secret from the three files (certificate, private key, and foo)\\n* Understanding that creating a Secret is similar to creating a ConfigMap\\n\\n**Important Details:**\\n\\n* The private key needs to be kept secure, so it\\'s stored in a Secret along with the certificate\\n* The `kubectl create secret` command can include multiple files from a directory using the `--from-file` option\\n* A Secret can have multiple entries, each with its own file or directory contents\\n* In Kubernetes, a default token Secret is created automatically and mounted as a volume in each pod.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Kubernetes provides two types of objects for storing sensitive data: Secrets and ConfigMaps.\\n* Secrets are used to store sensitive data, such as passwords or certificates, while ConfigMaps are used to store non-sensitive configuration data.\\n\\n**Important Details:**\\n\\n* When creating a Secret, the contents of its entries are Base64-encoded strings, whereas ConfigMap entries are shown in clear text.\\n* The reason for using Base64 encoding is that Secrets can contain binary values, not just plain-text, and this encoding allows them to be included in YAML or JSON manifests.\\n* Secrets have a maximum size limit of 1MB.\\n* While Secrets are typically used for sensitive data, they can also be used for non-sensitive binary data.\\n\\n**Key Takeaways:**\\n\\n* Understand the difference between Secrets and ConfigMaps in Kubernetes.\\n* Be aware that Secrets use Base64 encoding to store binary values.\\n* Consider using Secrets even for non-sensitive binary data, but be mindful of their 1MB size limit.',\n",
       "  \"Here is a summary of the main points and important details from the chunk of text:\\n\\n**Main Points:**\\n\\n1. Kubernetes allows setting sensitive data in a Secret through the `stringData` field, which can be used for non-binary data.\\n2. The `stringData` field is write-only and cannot be read back when retrieving the Secret's YAML with `kubectl get -o yaml`.\\n3. When exposing a Secret to a container through a secret volume or environment variable, the value of the Secret entry is decoded and written in its actual form.\\n\\n**Important Details:**\\n\\n1. The `stringData` field can only be used to set values, not read them.\\n2. Entries specified in the `stringData` field are Base64-encoded like other entries in a Secret.\\n3. To use a Secret with plain text data in a pod, you need to expose it through a secret volume or environment variable.\\n4. The example shows modifying a ConfigMap to enable HTTPS by adding an entry for the SSL certificate and key.\\n\\n**Key Terms:**\\n\\n1. `stringData` field: used to set non-binary sensitive data in a Secret.\\n2. Write-only: the `stringData` field can only be used to set values, not read them.\\n3. Secret volume: exposing a Secret to a container through a secret volume allows it to be accessed directly.\\n4. Environment variable: exposing a Secret through an environment variable allows it to be accessed by the app without decoding.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The configuration uses secrets to pass sensitive data (certificate and key files) to containers.\\n2. A new pod called \"fortune-https\" is created to mount the secret volume holding the certificate and key into the web-server container.\\n\\n**Important Details:**\\n\\n* The Nginx server is configured to read the cert and key files from `/etc/nginx/certs`.\\n* To use this configuration, a Secret volume must be mounted at `/etc/nginx/certs` in the pod.\\n* The `fortune-https` pod has two containers: `html-generator` and `web-server`, which are based on different images (`luksa/fortune:env` and `nginx:alpine`, respectively).\\n* The pod mounts four volumes:\\n\\t+ `html`: mounted at `/var/htdocs`\\n\\t+ `config`: mounted at `/etc/nginx/conf.d`\\n\\t+ `certs`: mounted at `/etc/nginx/certs` (to hold the secret certificate and key files)\\n\\t+ `html` again: mounted at `/usr/share/nginx/html` to serve HTML content.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* The text describes how to configure an application using Kubernetes ConfigMaps and Secrets.\\n* A pod descriptor is shown in YAML format, which includes:\\n\\t+ A container port (443)\\n\\t+ Three volumes:\\n\\t\\t- An emptyDir volume named \"html\"\\n\\t\\t- A configMap volume named \"config\" that references a ConfigMap named \"fortune-config\"\\n\\t\\t- A secret volume named \"certs\" that references a Secret named \"fortune-https\"\\n* The pod also includes environment variables, including INTERVAL=25.\\n* The text notes that the default-token Secret and volume are not shown in the figure.\\n\\n**Important Details:**\\n\\n* ConfigMap volumes support specifying file permissions for exposed files through the defaultMode property.\\n* Secret volumes can reference existing Secrets, which can be used to store sensitive data such as certificates.\\n* The pod descriptor includes a container named \"web-server\" and another container named \"html-generator\".\\n* The secret volume is mounted at /etc/nginx/certs/ and the configMap volume is mounted at /etc/nginx/conf.d/.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The article demonstrates how to use Kubernetes secrets to pass sensitive data (e.g., certificates and keys) to containers.\\n2. It shows how to test if Nginx is using the certificate and key from the secret by opening a port-forward tunnel and sending an HTTPS request.\\n3. The article explains that secret volumes are stored in memory (tmpfs) and therefore do not write sensitive data to disk.\\n\\n**Important Details:**\\n\\n* To test if Nginx is using the correct certificate, you can use `curl` with verbose logging (`-v`) to inspect the server's certificate.\\n* Secret volumes use an in-memory filesystem (tmpfs), which means that sensitive data is never written to disk.\\n* Instead of using a volume, individual secret entries can be exposed as environment variables.\\n\\n**Key Takeaways:**\\n\\n* Using secrets to pass sensitive data to containers is a secure way to manage and protect sensitive information.\\n* Secret volumes are stored in memory, providing an additional layer of security against data breaches.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. ConfigMaps and Secrets are used to configure applications in Kubernetes.\\n2. Environment variables can be used to expose Secrets, but it's not recommended due to security risks.\\n3. Secret volumes should be used instead of environment variables to expose Secrets.\\n\\n**Important Details:**\\n\\n* To use a private image registry, you need to create a Secret holding the credentials for the Docker registry.\\n* You must reference that Secret in the `imagePullSecrets` field of the pod manifest.\\n* Using environment variables to pass Secrets can lead to security risks, such as exposing them in error reports or application logs.\\n\\n**Key Concepts:**\\n\\n* ConfigMaps and Secrets\\n* Environment variables vs. Secret volumes\\n* Private image registries and Docker Hub\\n* `imagePullSecrets` field in the pod manifest\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Creating a Secret for authenticating with a Docker registry involves using the `kubectl create secret` command with specific options.\\n2. The Secret can be used to pull images from private Docker Hub repositories.\\n3. To use the Secret in a pod definition, specify its name as an imagePullSecrets entry.\\n\\n**Important Details:**\\n\\n* When creating a Secret for authenticating with a Docker registry, you need to specify your Docker Hub username, password, and email.\\n* The Secret is stored in Kubernetes as a `.dockercfg` file, which is equivalent to the `.dockercfg` file created by Docker when you run `docker login`.\\n* You can use the Secret in a pod definition by specifying its name as an imagePullSecrets entry.\\n* Image pull Secrets can be added automatically to all pods if they are added to a ServiceAccount (covered in Chapter 12).',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* This chapter has covered how to pass configuration data to containers using various methods.\\n* Key takeaways include:\\n\\t+ Overriding default container commands and passing command-line arguments\\n\\t+ Setting environment variables for containers\\n\\t+ Decoupling configuration from pod specifications using ConfigMaps\\n\\t+ Storing sensitive data in Secrets and delivering it securely to containers\\n\\t+ Creating a Docker-registry Secret to pull images from private registries\\n\\n**Important Details:**\\n\\n* The next chapter will cover passing pod and container metadata to applications.\\n* A default token Secret is used to communicate with the API server from within a pod.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Summary**\\n\\nThis chapter focuses on accessing pod metadata and other resources from applications running in a Kubernetes cluster. It covers various ways to pass information into containers, interact with the Kubernetes API server, and access cluster resources.\\n\\n**Main Points:**\\n\\n1. **Downward API**: Passing information into containers using the Downward API.\\n2. **Kubernetes REST API**: Exploring the Kubernetes REST API for accessing cluster resources.\\n3. **API Server Access**: Accessing the API server from within a container.\\n4. **Ambassador Container Pattern**: Understanding the ambassador container pattern for interacting with the API server.\\n5. **Client Libraries**: Using Kubernetes client libraries to interact with the API server.\\n\\n**Important Details:**\\n\\n* The chapter will cover how applications can access information about themselves and other components in the cluster.\\n* It will also discuss authentication and server verification, which is handled by `kubectl proxy`.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* The Kubernetes Downward API allows applications to access metadata and other resources from pods, such as IP addresses, node names, and pod labels.\\n* This solves two problems:\\n\\t+ Passing data that isn't known until runtime (e.g. pod's IP address)\\n\\t+ Avoiding duplication of information in multiple places\\n* The Downward API exposes pod metadata through environment variables or files in a downwardAPI volume.\\n\\n**Important Details:**\\n\\n* The Downward API is not a REST endpoint, but rather a way to populate environment variables or files with values from the pod's specification or status.\\n* Currently, the Downward API allows passing of the following information:\\n\\t+ Pod name\\n\\t+ Pod IP address\\n\\t+ Other metadata (e.g. labels, annotations)\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* The Downward API allows passing metadata from a pod to its containers.\\n* Metadata that can be passed includes:\\n\\t+ Namespace and node name\\n\\t+ Service account name\\n\\t+ CPU and memory requests and limits for each container\\n\\t+ Pod labels and annotations\\n* Most metadata can be passed through environment variables or a Downward API volume, but labels and annotations require the volume.\\n\\n**Important Details:**\\n\\n* A service account is an account that a pod authenticates as when talking to the API server.\\n* CPU and memory requests and limits are the amount of resources guaranteed to a container and the maximum it can get.\\n* The Downward API provides a simpler alternative for acquiring metadata, but some data can also be acquired directly from the operating system.\\n\\n**Example:**\\n\\n* An example is provided showing how to pass pod and container metadata to a container through environment variables using a YAML manifest.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Environment variables can be defined in a pod spec to expose metadata and other resources to applications running within the pod.\\n2. These environment variables can be used to access information such as the pod's name, IP address, namespace, node name, service account name, CPU requests, and memory limits.\\n\\n**Important Details:**\\n\\n1. Environment variables are defined using the `valueFrom` field in the pod spec.\\n2. The `fieldRef` field is used to reference metadata fields from the pod manifest, such as `metadata.name`.\\n3. For resource fields (e.g., CPU requests and memory limits), the `resourceFieldRef` field is used instead of `fieldRef`.\\n4. A divisor can be specified for resource fields to get the value in a specific unit.\\n5. Examples are provided for creating environment variables that expose:\\n\\t* Pod name: `POD_NAME`\\n\\t* Pod IP address: `POD_IP`\\n\\t* Namespace: `POD_NAMESPACE`\\n\\t* Node name: `NODE_NAME`\\n\\t* Service account name: `SERVICE_ACCOUNT`\\n\\t* Container CPU request in millicores: `CONTAINER_CPU_REQUEST_MILLICORES`\\n\\t* Container memory limit in Kibibytes: `CONTAINER_MEMORY_LIMIT_KIBIBYTES`\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The Downward API allows passing metadata from the Kubernetes cluster to containers as environment variables.\\n* This metadata includes information such as CPU limits, memory requests, pod name, namespace, IP address, and more.\\n\\n**Important Details:**\\n\\n* The divisor for CPU limits and requests can be either 1 (one whole core) or 1m (one millicore).\\n* The divisor for memory limits/requests can be in various units such as bytes, kilobytes, kibibytes, megabytes, mebibytes, etc.\\n* Environment variables are exposed to the container using `kubectl exec` and can be viewed using `kubectl exec downward env`.\\n* A pod manifest example is provided that demonstrates how to pass metadata through environment variables.\\n\\n**Key Takeaways:**\\n\\n* The Downward API allows for flexible passing of metadata from the cluster to containers.\\n* Understanding the divisor units for CPU and memory limits/requests is crucial when configuring pods.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Processes running inside a container can read and use environment variables set by the pod\\'s metadata.\\n2. To expose pod metadata through files instead of environment variables, you can use a \"downwardAPI\" volume.\\n3. This type of volume is required to expose a pod\\'s labels or annotations.\\n\\n**Important Details:**\\n\\n* Each metadata field must be specified explicitly if it is to be exposed through the downwardAPI volume.\\n* The downwardAPI volume can be used to expose a pod\\'s name, namespace, and other metadata fields as files within the container.\\n* In the example provided, a downwardAPI volume named \"downward\" is created and mounted at /etc/downward in the container. This allows the pod\\'s name (metadata.name) to be written to a file called \"podName\" within the container.\\n\\n**Key Terms:**\\n\\n* DownwardAPI volume: a type of volume that exposes pod metadata as files within the container.\\n* FieldRef: a field reference used to specify which metadata fields should be exposed through the downwardAPI volume.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Instead of passing metadata through environment variables, you can use the Downward API to pass metadata from a Pod to a Container.\\n* The Downward API uses a volume called \"downward\" that contains files configured under the `downwardAPI.items` attribute in the volume specification.\\n\\n**Important Details:**\\n\\n* Each item in the `downwardAPI.items` list specifies the path (filename) where the metadata should be written and references either a pod-level field or a container resource field.\\n* Pod labels are written to `/etc/downward/labels` and annotations are written to `/etc/downward/annotations`.\\n* The example provided shows how to use the Downward API in a Pod manifest, with a volume named \"downward\" that contains files for passing metadata from the Pod to the Container.\\n\\n**Key Concepts:**\\n\\n* Downward API\\n* Volume called \"downward\"\\n* `downwardAPI.items` attribute\\n* Passing metadata from Pod to Container using a volume',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. The `downwardAPI` volume allows applications to access pod metadata and other resources.\\n2. This can be done by mounting the volume under a specific directory, such as `/etc/downward/`.\\n3. The contents of this directory include files that correspond to various metadata fields, including labels, annotations, container CPU and memory requests, and more.\\n\\n**Important Details:**\\n\\n1. Each file in the `downwardAPI` volume corresponds to a specific metadata field.\\n2. File permissions can be changed through the `defaultMode` property in the pod spec.\\n3. Labels and annotations are stored as key-value pairs on separate lines, with multi-line values denoted by `\\\\n`.\\n4. When labels or annotations change while a pod is running, Kubernetes updates the corresponding files in the `downwardAPI` volume.\\n5. This means that if labels or annotations were exposed through environment variables, there would be no way to expose new values after they are modified.\\n\\n**Key Takeaways:**\\n\\n* The `downwardAPI` volume provides a way for applications to access pod metadata and other resources.\\n* Labels and annotations can be accessed through the `downwardAPI` volume, but not through environment variables.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When exposing container-level metadata using the Downward API, you must specify the name of the container whose resource field you're referencing.\\n2. Volumes are defined at the pod level, not at the container level, so you need to explicitly specify the container name when referring to a container's resource field inside a volume specification.\\n3. Using volumes to expose a container's resource requests and/or limits is more complicated than using environment variables but allows passing one container's resource fields to another container if needed.\\n\\n**Important Details:**\\n\\n* The Downward API provides a simple way to pass certain pod and container metadata to the process running inside them, but it only exposes the pod's own metadata and a subset of all of the pod's data.\\n* If your app needs to know more about other pods or resources defined in your cluster, you'll need to obtain that information from the Kubernetes API server directly (covered in Section 8.2).\\n* The Downward API is useful for keeping applications Kubernetes-agnostic by exposing metadata without requiring application rewriting or shell scripting.\\n\\n**Key Takeaways:**\\n\\n* Specify container name when referencing resource fields inside a volume specification.\\n* Use volumes to pass one container's resource fields to another container if needed (but both containers must be in the same pod).\\n* The Downward API is useful for keeping applications Kubernetes-agnostic, but its metadata availability is limited.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To access up-to-date information about resources or talk to the Kubernetes API server directly, an application within a pod needs to communicate with the API server.\\n2. The API server's REST endpoints can be accessed from a local machine using tools like `curl`, but this requires authentication and is not straightforward.\\n3. A better approach is to use the `kubectl proxy` command to run a proxy server that takes care of authentication and verifies the API server's certificate.\\n\\n**Important Details:**\\n\\n* To get the URL of the Kubernetes API server, run `kubectl cluster-info`.\\n* The `kubectl proxy` command runs a local proxy server that accepts HTTP connections and proxies them to the API server while handling authentication.\\n* Using `kubectl proxy` makes it easier to access the API server's REST endpoints without dealing with authentication manually.\\n\\n**Key Concepts:**\\n\\n* Kubernetes API server\\n* REST endpoints\\n* Authentication\\n* `kubectl proxy` command\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Running the `kubectl proxy` command starts a local proxy to the Kubernetes API server, allowing you to interact with it locally.\\n* The proxy accepts connections on port 8001 and can be accessed using `curl` or a web browser.\\n\\n**Important Details:**\\n\\n* When running `kubectl proxy`, no additional arguments are needed as `kubectl` already knows the necessary information (API server URL, authorization token, etc.).\\n* Accessing the API server through the proxy returns a list of available paths, which correspond to API groups and versions.\\n* The listed paths include `/api/v1`, `/apis/apps`, `/apis/batch`, etc., which match the API groups and versions used in resource definitions (e.g., Pods, Services, Jobs).\\n* Understanding these paths can help you explore and interact with the Kubernetes API server using tools like `curl` or a web browser.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Kubernetes has a concept of API groups, which were introduced later in its development.\\n* The initial resource types without an API group are now considered to belong to the core API group.\\n* The Batch API group contains two versions (v1 and v2alpha1), with v1 being the preferred version for clients.\\n\\n**Important Details:**\\n\\n* To access metadata and resources from applications, you can use the `/apis/batch` path to explore the Batch API group.\\n* Using `curl`, you can retrieve information about the Batch API group by accessing the following endpoints:\\n\\t+ `/apis/batch`: Returns a description of the batch API group, including available versions and preferred version.\\n\\t+ `/apis/batch/v1`: Returns a list of API resources in the batch/v1 API group, including the Job resource.\\n\\n**Key Takeaways:**\\n\\n* The Batch API group contains two versions (v1 and v2alpha1), with v1 being the recommended version for clients.\\n* Understanding API groups and their versions is important for accessing metadata and resources from applications.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. The Kubernetes API server returns a list of resource types and REST endpoints in the batch/v1 API group.\\n2. One of these resources is the Job resource, which can be retrieved, updated, deleted, and watched through the /apis/batch/v1/jobs endpoint.\\n\\n**Important Details:**\\n\\n* The Job resource has the following verbs associated with it:\\n\\t+ Create\\n\\t+ Delete (individual or collection)\\n\\t+ Get\\n\\t+ Watch\\n\\t+ Update\\n* There is a special REST endpoint for modifying the status of a Job, which can be retrieved, patched, or updated.\\n* To get a list of Jobs in the cluster, perform a GET request on path /apis/batch/v1/jobs.\\n\\n**Key Concepts:**\\n\\n* Namespaced resources (e.g. Jobs) are only accessible within a specific namespace.\\n* Verbs refer to the actions that can be performed on a resource through its REST endpoint.\\n* The API server returns metadata about each resource, including its kind, name, and verbs.',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Accessing Pod Metadata and Other Resources from Applications**\\n\\n* The Kubernetes API server provides a REST API that can be accessed to retrieve metadata and other resources.\\n* To access the API server, you can use `curl` commands or specialized tools like `kubectl`.\\n* **Retrieving a List of All Jobs**: Using the `/apis/batch/v1/namespaces/default/jobs/` endpoint returns a list of all Jobs across all namespaces. The response is in JSON format.\\n* **Retrieving a Specific Job Instance by Name**: To retrieve a specific Job instance, you need to specify its name and namespace in the URL. For example: `/apis/batch/v1/namespaces/default/jobs/my-job`.\\n* **Talking to the API Server from within a Pod**:\\n\\t+ You need to find the location of the API server.\\n\\t+ Ensure you're talking to the actual API server, not an impersonator.\\n\\t+ Authenticate with the server to access resources.\\n\\nImportant details:\\n\\n* The `items` array will be empty if there are no Job resources deployed in your cluster.\\n* Deploying a Job resource (e.g., using `kubectl apply`) and accessing the REST endpoint again can provide the same output as shown in Listing 8.10.\\n* Using `curl` to access the API server is sufficient for basic exploration, but a better option is described at the end of this chapter.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. To communicate with the Kubernetes API server, you need to run a pod that can access the API server.\\n2. You\\'ll use a pod definition file (curl.yaml) to create a pod that runs a container with the curl binary.\\n3. The pod will be used to try out communication with the API server using curl.\\n\\n**Important Details:**\\n\\n* To access the API server, you need to find its IP address and port, which can be obtained from environment variables KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT.\\n* A Service called \"kubernetes\" is automatically exposed in the default namespace and configured to point to the API server.\\n* You\\'ll use a container image that contains the curl binary (e.g. tutum/curl) to create the pod.\\n* The pod will run a shell inside its container using kubectl exec, allowing you to access the API server from within the pod.\\n\\n**Key Steps:**\\n\\n1. Create a pod definition file (curl.yaml) with the necessary configuration.\\n2. Run the pod using kubectl apply -f curl.yaml.\\n3. Access the pod\\'s shell using kubectl exec and run curl commands to try out communication with the API server.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes services have a DNS entry, allowing applications to access them directly without needing environment variables.\\n2. To access a service's port number, you can use DNS SRV record lookup or look up environment variables.\\n3. When accessing the API server using `curl`, it is recommended to verify the server's certificate instead of skipping verification.\\n\\n**Important Details:**\\n\\n* The default port for HTTPS is 443.\\n* Using the `-k` option with `curl` disables certificate verification, which should not be used in production applications due to security risks.\\n* To verify the server's identity, use the `--cacert` option with `curl` and specify the CA certificate from the `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt` file.\\n* The `Unauthorized` error message may appear if the verification fails.\\n\\n**Key Concepts:**\\n\\n* Kubernetes services have a DNS entry and can be accessed directly by applications.\\n* Certificate verification is crucial when accessing the API server to prevent security risks.\",\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. The `curl` command has verified the Kubernetes API server\\'s identity using its certificate.\\n2. Authentication with the API server is still needed to access it.\\n3. To simplify authentication, the `CURL_CA_BUNDLE` environment variable can be set to point to a trusted CA certificate.\\n4. An authentication token from the default-token Secret is used to authenticate with the API server.\\n\\n**Important Details:**\\n\\n* The `CURL_CA_BUNDLE` environment variable can be set to `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt` to avoid specifying the `--cacert` flag every time.\\n* The authentication token is stored in the `/var/run/secrets/kubernetes.io/serviceaccount/token` file and can be loaded into an environment variable called `TOKEN`.\\n* To authenticate with the API server, the `curl` command uses the `-H \"Authorization: Bearer $TOKEN\"` option to include the authentication token in the request.\\n\\n**Key Steps:**\\n\\n1. Set the `CURL_CA_BUNDLE` environment variable.\\n2. Load the authentication token from the default-token Secret into an environment variable called `TOKEN`.\\n3. Use the `curl` command with the `-H \"Authorization: Bearer $TOKEN\"` option to authenticate with the API server.',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. The API server recognized an authentication token passed in the Authorization HTTP header and returned a proper response, allowing access to cluster resources.\\n2. To list all pods in the same namespace as the current pod, you can read the contents of a file called \"namespace\" in the mounted secret volume into an environment variable (NS).\\n3. You can then use this environment variable to make API requests to retrieve other objects and even update them by sending PUT or PATCH requests.\\n\\n**Important Details:**\\n\\n* The namespace is stored in a file called \"namespace\" within the mounted secret volume.\\n* To disable role-based access control (RBAC) and allow service accounts to access the API server, you can run a command to create a clusterrolebinding with cluster-admin privileges for all service accounts.\\n* This should only be done on test clusters, as it is not recommended for production clusters due to security risks.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To access the Kubernetes API server, an app running inside a pod must verify the API server\\'s certificate, authenticate itself, and specify the namespace.\\n2. The verification process involves checking if the API server\\'s certificate is signed by a trusted Certificate Authority (CA) whose certificate is in the `ca.crt` file.\\n3. Authentication is done by sending an \"Authorization\" header with a bearer token from the `token` file.\\n4. The namespace is specified using the `namespace` file.\\n\\n**Important Details:**\\n\\n* CRUD operations refer to Create, Read, Update, and Delete operations on API objects, which correspond to HTTP methods POST, GET, PATCH/PUT, and DELETE respectively.\\n* Figure 8.5 illustrates the communication process between a pod and the API server.\\n* The text also mentions that dealing with HTTPS, certificates, and authentication tokens can be complicated for developers, but using an Ambassador container can simplify this process while keeping it secure.\\n\\n**Key Files:**\\n\\n* `ca.crt`: contains the trusted Certificate Authority (CA) certificate\\n* `token` file: contains the bearer token used for authentication\\n* `namespace` file: specifies the namespace to use when performing CRUD operations on API objects',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. The `kubectl proxy` command can be used inside pods to access the API server.\\n2. An \"ambassador container\" pattern allows an application to communicate with the API server through a proxy, rather than directly.\\n3. This approach takes care of authentication, encryption, and server verification.\\n\\n**Important Details:**\\n\\n* To use this approach, `kubectl proxy` is run in an ambassador container alongside the main container.\\n* The ambassador container uses files from the default token\\'s secret volume to handle HTTPS connections to the API server.\\n* All containers in a pod share the same loopback network interface, allowing the application to access the proxy through localhost.\\n* A new pod with an additional ambassador container is created using a general-purpose `kubectl-proxy` container image.\\n\\n**Key Concepts:**\\n\\n* Ambassador container pattern\\n* `kubectl proxy`\\n* Loopback network interface\\n* Secret volume',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. A pod named \"curl-with-ambassador\" was created with two containers: one running curl and another acting as an ambassador to connect to the Kubernetes API server.\\n2. The ambassador container used the kubectl-proxy image to proxy requests to the API server.\\n3. The main container (running curl) was able to access the API server through the ambassador container without needing to handle authentication tokens or server certificates.\\n\\n**Important Details:**\\n\\n* The ambassador container is reusable across different apps and can simplify connections to external services.\\n* Running an additional process (the ambassador container) consumes additional resources.\\n* To run a command in a specific container, use the `-c` option with `kubectl exec`, e.g. `kubectl exec -it curl-with-ambassador -c main bash`.\\n* The API server\\'s response was accessed through the ambassador container using `curl localhost:8001`.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When accessing Kubernetes API servers from applications, using client libraries is recommended for complex operations.\\n2. Two official Kubernetes API client libraries are supported by the API Machinery special interest group (SIG): Golang client and Python client.\\n\\n**Important Details:**\\n\\n* For simple operations, a regular HTTP client library can be used with the help of kubectl-proxy ambassador container.\\n* The two officially supported client libraries are:\\n\\t+ Golang client: https://github.com/kubernetes/client-go\\n\\t+ Python client: https://github.com/kubernetes-incubator/client-python\\n* Additional user-contributed client libraries exist for various languages, including Java, Node.js, and PHP.\\n* These libraries can be used to access Kubernetes API servers and perform complex operations.\\n\\n**Additional Context:**\\n\\n* The text mentions the existence of Special Interest Groups (SIGs) and Working Groups within the Kubernetes community that focus on specific parts of the ecosystem.',\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Talking to the Kubernetes API Server**\\n\\nThere are several libraries available in various programming languages that allow you to interact with the Kubernetes API server. These libraries include:\\n\\n* Ruby: kubr, kubeclient\\n* Clojure: clj-kubernetes-api\\n* Scala: skuber\\n* Perl: Net::Kubernetes\\n\\nThese libraries typically support HTTPS and handle authentication, so you don't need to use an ambassador container.\\n\\n**Example with the Fabric8 Java Client**\\n\\nThe text provides an example of how to interact with the Kubernetes API server using the Fabric8 Java client. The example shows how to:\\n\\n* List services in a Java app\\n* Create a pod\\n* Edit a pod (add a label to it)\\n\\nThe code snippet demonstrates how to use the Fabric8 Java client to perform these actions, including creating a new pod and editing an existing one.\\n\\n**Key Points**\\n\\n* There are various libraries available for interacting with the Kubernetes API server in different programming languages.\\n* These libraries typically support HTTPS and handle authentication.\\n* The Fabric8 Java client provides an example of how to interact with the Kubernetes API server in Java.\",\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Accessing Pod Metadata and Resources**\\n\\n* The Fabric8 client provides a fluent DSL API to access pod metadata and other resources from applications.\\n* An example code snippet demonstrates how to add a label to a pod, wait for 1 minute, and then delete the pod.\\n\\n**Building Your Own Library with Swagger and OpenAPI**\\n\\n* If no client library is available for your programming language, you can use the Swagger API framework to generate the client library and documentation.\\n* The Kubernetes API server exposes Swagger API definitions at `/swaggerapi` and OpenAPI spec at `/swagger.json`.\\n\\n**Exploring the API with Swagger UI**\\n\\n* Swagger provides a web UI for exploring REST APIs if they expose Swagger API definitions.\\n* Kubernetes has Swagger UI integrated into its API server, but it's not enabled by default.\\n* To enable Swagger UI, run the API server with the `--enable-swagger-ui=true` option or use Minikube with the `--extra-config=apiserver.Features.Enable-SwaggerUI=true` flag.\\n* Once enabled, access the Swagger UI at `http(s)://<api server>:<port>/swagger-ui`.\\n* The Swagger UI allows you to browse and interact with the Kubernetes API, including posting JSON resource manifests, patching resources, or deleting them.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Summary**\\n\\nThis chapter has taught you how your application running inside a pod can access data about itself, other pods, and cluster components. You've learned how to use various methods to expose metadata, such as environment variables or files in a downward API volume.\\n\\n**Key Points:**\\n\\n1. Pods can get metadata (name, namespace, etc.) through environment variables or files.\\n2. CPU and memory requests/limits are passed to the app in any unit required.\\n3. Pods can use downward API volumes for up-to-date metadata (labels, annotations).\\n4. You can browse the Kubernetes REST API using `kubectl proxy`.\\n5. Pods can find the API server's location through environment variables or DNS.\\n6. Applications can verify they're talking to the API server and authenticate themselves.\\n7. Using an ambassador container simplifies communication with the API server.\\n8. Client libraries enable quick interaction with Kubernetes.\\n\\n**Next Steps:**\\n\\nThe next chapter will delve into how the API server works, but first you'll learn about Deployments and StatefulSets in the next two chapters.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The chapter covers how to update applications running in a Kubernetes cluster.\\n2. It explains how to achieve zero-downtime updates using various Kubernetes resources.\\n\\n**Important Details:**\\n\\n* The chapter discusses updating apps by replacing pods with newer versions, updating managed pods, and performing rolling updates.\\n* It highlights the use of **Deployment** resources for declarative updates, which allows for automatic blocking of bad versions and controlling the rollout rate.\\n* Other topics covered include:\\n\\t+ Updating pods using ReplicationControllers or ReplicaSets\\n\\t+ Reverting pods to a previous version\\n\\t+ Automatically blocking rollouts of bad versions\\n\\t+ Controlling the rate of the rollout\\n\\nOverall, this chapter provides guidance on how to update applications in a Kubernetes cluster with minimal downtime and disruption.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Updating applications running in pods in Kubernetes requires a strategy to replace old pods with new ones.\\n* There are two ways to update all pods:\\n\\t1. Delete existing pods first, then start new ones (with a short period of downtime).\\n\\t2. Start new pods and delete old ones gradually or at once.\\n\\n**Important Details:**\\n\\n* The application is backed by a ReplicationController or ReplicaSet, which ensures a specified number of replicas (pods) are running.\\n* A Service exists to enable clients to access the pods.\\n* Updating the application requires replacing all existing pods with new ones running the updated image.\\n* The second strategy (starting new pods and deleting old ones gradually) requires the application to handle running two versions at the same time, which can be complex if data is stored in a database.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. There are two ways to update applications in Kubernetes: manual updates and declarative updates.\\n2. The chapter discusses how to perform these updates using ReplicationControllers.\\n\\n**Important Details:**\\n\\n* **Manual Update Method 1:** Deleting old pods and replacing them with new ones by updating the pod template of a ReplicationController, which will spin up new instances when it detects that no pods match its label selector.\\n\\t+ This method involves short downtime between the deletion of old pods and the creation of new ones.\\n* **Manual Update Method 2:** Spinning up new pods and then deleting the old ones by updating the pod template of a ReplicationController, which will create new instances before deleting the old ones.\\n\\t+ This method does not involve downtime if the application supports running multiple versions at once.\\n\\n**Key Concepts:**\\n\\n* Pod templates\\n* ReplicationControllers\\n* Label selectors',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Updating Applications Running in Pods**\\n\\nThere are three methods to update applications running in pods:\\n\\n1. **Method 1: Update, then delete**: This method involves updating all pods at once, which requires more hardware resources. It's a slightly more complex method that combines knowledge of ReplicationControllers and Services.\\n2. **Blue-Green Deployment**: This method involves bringing up new pods while keeping the old ones running, and then switching the Service to point to the new pods. Once confirmed, the old pods can be deleted by deleting the old ReplicationController.\\n3. **Rolling Update**: This method involves replacing pods step by step, scaling down the previous ReplicationController and scaling up the new one. The Service's pod selector is updated to include both old and new pods.\\n\\n**Important Details**\\n\\n* To perform a blue-green deployment, you can use the `kubectl set selector` command to update the Service's pod selector.\\n* Rolling updates can be performed manually or using a single Kubernetes command (to be covered in the next section).\\n* Figures 9.3 and 9.4 illustrate the blue-green deployment and rolling update methods, respectively.\",\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* This chapter discusses performing automatic rolling updates on applications using Kubernetes.\\n* The process involves creating an initial version of the app, updating it to a new version, and then rolling out the updated version to all pods.\\n\\n**Important Details:**\\n\\n* To perform an automatic rolling update, you can use `kubectl` to automate the process.\\n* The initial version of the app is created by modifying the Kubia NodeJS app from Chapter 2 to also return its version number in the response.\\n* A new image for the updated app (`luksa/kubia:v1`) has been built and pushed to Docker Hub.\\n* The rolling update process involves creating two ReplicationControllers, one for each version of the app (v1 and v2).\\n* As pods are terminated or created, they are replaced with the newer version of the app.\\n\\n**Key Concepts:**\\n\\n* ReplicationController\\n* Rolling updates\\n* Kubernetes deployment',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. The text describes how to run an application using a ReplicationController and a LoadBalancer Service in Kubernetes.\\n2. Instead of creating these resources separately, a single YAML file can be used to create both resources at once.\\n\\n**Important Details:**\\n\\n* A YAML manifest can contain multiple objects, separated by a line with three dashes.\\n* The YAML file defines:\\n\\t+ A ReplicationController called \"kubia-v1\" that creates 3 pods running the image \"luksa/kubia:v1\".\\n\\t+ A Service called \"kubia\" that fronts all pods created by the ReplicationController and exposes port 80 to targetPort 8080.\\n* The YAML file is posted to the Kubernetes API using a single `kubectl create` command.\\n* After creating the resources, the external IP of the Service can be looked up and used to access the application with `curl`.',\n",
       "  'Here is a summary of the text with main points and important details:\\n\\n**Main Points:**\\n\\n1. The chapter discusses deploying applications declaratively using Kubernetes.\\n2. A rolling update is performed on an existing application by updating its code and pushing it to a new version (v2).\\n3. The importance of setting the `imagePullPolicy` property to \"Always\" when making changes to an image without changing its tag.\\n\\n**Important Details:**\\n\\n* When accessing an application in a Kubernetes cluster, you can use the Service\\'s external IP or node port if load balancer services are not supported.\\n* Modifying an app and pushing changes to the same image tag is not recommended, as it may lead to different versions of the pod running on different nodes.\\n* The default `imagePullPolicy` depends on the image tag: \"Always\" for the \"latest\" tag, and \"IfNotPresent\" for other tags.\\n* To ensure that changes are picked up, set the `imagePullPolicy` property to \"Always\" or push changes under a new tag.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Performing an automatic rolling update using a ReplicationController in Kubernetes\\n* Using the `kubectl` command to initiate the rolling update\\n* Replacing an existing ReplicationController (`kubia-v1`) with a new one (`kubia-v2`) that uses a newer image version\\n\\n**Important Details:**\\n\\n* The `kubectl rolling-update` command is used to replace the old ReplicationController with a new one.\\n* The command takes three arguments: the name of the old ReplicationController, the name of the new ReplicationController, and the image URL for the new ReplicationController.\\n* A new ReplicationController (`kubia-v2`) is created immediately, referencing the newer image version (`luksa/kubia:v2`).\\n* The initial desired replica count for the new ReplicationController is set to 0.\\n* The rolling update process scales up the new ReplicationController from 0 to 3 replicas while scaling down the old one from 3 to 0.',\n",
       "  \"Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n* When performing a rolling update with `kubectl`, it creates a new ReplicationController (`kubia-v2`) to manage the updated application.\\n* The old ReplicationController (`kubia-v1`) is modified to have an additional label (`deployment`) and its selector is also modified.\\n\\n**Important Details:**\\n\\n* To avoid conflicts between the two ReplicationControllers, the `deployment` label is added to the pods created by the new controller.\\n* Even with this label, the old controller's selector would still match the new pods if not for a catch:\\n\\t+ The rolling-update process modifies the labels of the live pods (created by the old controller) just before modifying the ReplicationController's selector.\\n\\t+ This ensures that the old controller sees zero matching pods and does not interfere with the new controller.\\n\\n**Key Takeaways:**\\n\\n* `kubectl` performs a series of steps to ensure a smooth rolling update, including modifying the labels of live pods and the selectors of ReplicationControllers.\\n* Understanding these steps is crucial for successful deployments in Kubernetes.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Performing a manual rolling update with a ReplicationController can be error-prone and may result in all pods being killed off.\\n2. kubectl uses a rolling update strategy to replace old pods with new ones by scaling up and down two ReplicationControllers.\\n\\n**Important Details:**\\n\\n* The process starts by scaling up the new controller (kubia-v2) to 1, creating the first v2 pod.\\n* Then, the old ReplicationController (kubia-v1) is scaled down by 1.\\n* As the Service targets all pods with the app=kubia label, curl requests are redirected to both old and new pods during the rolling update.\\n* The process continues until eventually, only the new v2 pods remain, and the original v1 pods are deleted.\\n\\n**Key Figures:**\\n\\n* Figure 9.7 shows the current state of the system during the rolling update, with a mix of old and new pods being targeted by the Service.',\n",
       "  \"Here's a summary of the text:\\n\\n**Main Points:**\\n\\n* A rolling update was performed on an application using `kubectl rolling-update`, where the old version (`kubia-v1`) was scaled down and eventually deleted, while the new version (`kubia-v2`) was scaled up.\\n* The process resulted in zero downtime for the service.\\n* However, it's mentioned that a better way of performing updates has been introduced.\\n\\n**Important Details:**\\n\\n* `kubectl rolling-update` is now considered obsolete due to its limitations and potential issues with Kubernetes modifying objects created by users.\\n* A major concern with `kubectl rolling-update` is that it modifies labels on pods and ReplicationControllers, which can cause unexpected behavior.\\n* The `--v 6` option can be used to increase logging level and see the HTTP requests sent by `kubectl` to the API server, revealing that `kubectl` itself performs the scaling operations.\\n\\nOverall, the text highlights the limitations of `kubectl rolling-update` and suggests a better approach for performing updates in Kubernetes.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Using `kubectl` to update applications declaratively can be problematic if network connectivity is lost mid-update.\\n2. This approach is imperative, whereas Kubernetes prefers a declarative approach where you specify the desired state and let Kubernetes achieve it.\\n3. A new resource called Deployment was introduced to address this issue.\\n\\n**Important Details:**\\n\\n* Deployments are higher-level resources for deploying applications and updating them declaratively.\\n* When creating a Deployment, a ReplicaSet resource is created underneath to manage pods.\\n* The actual pods are created and managed by the Deployment's ReplicaSets, not directly by the Deployment.\\n* Using Deployments provides a more robust way of updating applications, as demonstrated in the rolling update example.\\n\\n**Key Takeaways:**\\n\\n* Use Deployments for declarative updates instead of relying on `kubectl` or lower-level resources like ReplicationControllers and ReplicaSets.\\n* Deployments provide a more robust and scalable way of managing applications.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Deployments are a higher-level construct than ReplicationControllers that make updating applications easier.\\n* A Deployment resource coordinates multiple controllers to update an application without conflicts.\\n* Creating a Deployment involves defining a desired state through a single resource, which Kubernetes takes care of.\\n\\n**Important Details:**\\n\\n* To create a Deployment, you need to specify a deployment strategy, which defines how updates should be performed.\\n* The main difference between creating a Deployment and a ReplicationController is the addition of a deployment strategy field.\\n* A Deployment can manage multiple pod versions running under its wing, so its name shouldn't reference the app version.\\n* Deployments are in the apps API group, version v1beta1.\\n\\n**Key Takeaways:**\\n\\n* Using a Deployment instead of a lower-level construct like a ReplicationController makes updating an application much easier.\\n* A Deployment resource takes care of coordinating multiple controllers to update an application without conflicts.\",\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. To update an app declaratively using Deployments, delete any existing ReplicationControllers and pods, but keep the Service.\\n2. Create a Deployment resource by running `kubectl create -f kubia-deployment-v1.yaml --record`.\\n3. Use `kubectl rollout status deployment kubia` to check the status of the Deployment rollout.\\n\\n**Important Details:**\\n\\n* When creating a Deployment, include the `--record` option to record the command in the revision history.\\n* A Deployment creates ReplicaSets and leaves pod management to them.\\n* The names of pods created by a Deployment include an additional numeric value (a hash) that corresponds to the hashed value of the pod template in the Deployment and the ReplicaSet managing the pods.\\n* You can use `kubectl get replicasets` to see the ReplicaSet(s) created by the Deployment.\\n\\n**Key Commands:**\\n\\n1. `kubectl delete rc --all` to delete all ReplicationControllers\\n2. `kubectl create -f kubia-deployment-v1.yaml --record` to create a Deployment resource\\n3. `kubectl rollout status deployment kubia` to check the status of the Deployment rollout\\n4. `kubectl get replicasets` to see the ReplicaSet(s) created by the Deployment',\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Deployments are used to update applications declaratively, meaning that you define what you want the system state to be, and Kubernetes takes care of making it happen.\\n2. When creating a Deployment, you can use a hash value of the pod template to always use the same ReplicaSet for a given version of the pod template.\\n3. Deployments provide a more convenient way to update applications compared to ReplicationControllers.\\n\\n**Important Details:**\\n\\n1. To update an application using a Deployment, you only need to modify the pod template defined in the Deployment resource, and Kubernetes will take care of updating the system state accordingly.\\n2. There are two deployment strategies available:\\n\\t* RollingUpdate (default): performs a rolling update, where new pods are created and old ones are deleted one by one.\\n\\t* Recreate: deletes all old pods at once and creates new ones, which can cause a short period of time when the app becomes completely unavailable.\\n3. The Recreate strategy should be used when the application doesn't support running multiple versions in parallel and requires the old version to be stopped completely before the new one is started.\\n\\nOverall, Deployments provide a more convenient and flexible way to update applications compared to ReplicationControllers, with features like declarative updates and different deployment strategies.\",\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n* The RollingUpdate strategy for Deployments updates apps by removing old pods one by one while adding new ones, keeping the app available throughout the process.\\n* This strategy is the default and should be used only when the app can handle running both old and new versions at the same time.\\n\\n**Important Details:**\\n\\n* To slow down the RollingUpdate process for demonstration purposes, you can set the `minReadySeconds` attribute on the Deployment to a value (e.g., 10 seconds) using the `kubectl patch` command.\\n* Changing other Deployment properties, such as the desired replica count or deployment strategy, does not trigger a rollout because it doesn't affect existing individual pods.\\n* To trigger the RollingUpdate, you can use the `kubectl set image` command to update the image used in the pod container (e.g., changing from `luksa/kubia:v1` to `luksa/kubia:v2`).\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Modifying Deployments and Other Resources in Kubernetes**\\n\\nThis chapter discusses various ways to modify existing resources, such as Deployments, in Kubernetes. The main methods for modifying these resources are:\\n\\n1. **kubectl edit**: Opens the object's manifest in an editor, allowing changes to be made directly.\\n2. **kubectl patch**: Modifies individual properties of an object.\\n3. **kubectl apply**: Applies property values from a full YAML or JSON file to update the object.\\n4. **kubectl replace**: Replaces the object with a new one from a YAML/JSON file (requires the object to exist).\\n5. **kubectl set image**: Changes the container image defined in a Pod, Deployment, etc.\\n\\nThese methods are equivalent and change the resource's specification, triggering a rollout process. The examples provided demonstrate how to use these commands to update a Deployment's pod template or container image.\",\n",
       "  'Here is a summary of the text with main points and important details:\\n\\n**Main Points:**\\n\\n* Using Deployments in Kubernetes allows for declarative updates to applications, making it simpler than using `kubectl` commands.\\n* By updating the pod template in a Deployment resource, you can update your app to a newer version without running special commands.\\n\\n**Important Details:**\\n\\n* During an update, an additional ReplicaSet is created and scaled up slowly while the previous ReplicaSet is scaled down to zero.\\n* The events that occur during an update are similar to those that happen during a `kubectl` rolling-update.\\n* If the pod template references a ConfigMap or Secret, modifying it will not trigger an update. To trigger an update when modifying config, create a new ConfigMap and modify the pod template to reference it.\\n\\n**Key Takeaways:**\\n\\n* Deployments provide a simpler way to update applications in Kubernetes compared to using `kubectl` commands.\\n* Understanding how Deployments work can help you manage your applications more efficiently.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The chapter discusses deploying applications declaratively using Deployments, which manage pods through ReplicaSets.\\n* Unlike ReplicationControllers, Deployments create and manage multiple ReplicaSets behind the scenes, making it easier to manage a single Deployment object.\\n\\n**Important Details:**\\n\\n* A problem is introduced in version 3 of the application, where it handles only the first four requests properly and returns an internal server error (HTTP status code 500) for all subsequent requests.\\n* The bug is simulated by adding an if statement at the beginning of the handler function to return a 500 error after the fifth request.\\n* This creates a scenario where a deployment can be rolled back to a previous version, which will be discussed in the next section.',\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. The author updates an application to version 3 by changing the image in the Deployment specification.\\n2. The rollout process is monitored using `kubectl rollout status`.\\n3. After a few requests, web clients start receiving errors due to issues with the new version.\\n4. To fix this, the author uses `kubectl rollout undo` to roll back to the previous revision.\\n\\n**Important Details:**\\n\\n* The author creates a new image for version 3 and updates the Deployment specification using `kubectl set image`.\\n* The rollout process is monitored in real-time using `kubectl rollout status`, which shows the progress of the rollout.\\n* The new version causes internal server errors, which need to be addressed quickly.\\n* Using `kubectl rollout undo` allows the author to roll back to the previous revision, effectively undoing the bad rollout.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Deployments in Kubernetes keep a revision history, which allows for rolling back to previous versions.\\n* The revision history can be displayed using the `kubectl rollout history` command.\\n* Rollbacks are possible because old ReplicaSets are not deleted when a new version is deployed.\\n\\n**Important Details:**\\n\\n* The `--record` command-line option should be used when creating a Deployment to keep track of changes and their causes.\\n* To roll back to a specific revision, use the `kubectl rollout undo` command with the `--to-revision` option.\\n* Old ReplicaSets are not deleted automatically, but rather stored as part of the revision history. However, the length of this history is limited by the `revisionHistoryLimit` property on the Deployment resource (defaulting to 2).\\n* Older ReplicaSets are deleted automatically when they reach the end of the revision history.\\n* It's recommended not to delete old ReplicaSets manually, as this can prevent rolling back to previous revisions.\",\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. Deployments in Kubernetes allow for declarative updates to applications.\\n2. The `maxSurge` and `maxUnavailable` properties can be used to control the rate of rollout during an update.\\n\\n**Important Details:**\\n\\n* In version `v1beta1`, the default `revisionHistoryLimit` is not set, whereas in version `v1beta2`, it is 10.\\n* The `kubectl rollout status` command can be used to track the progress of a rollout.\\n* The `maxSurge` property determines how many pod instances can exist above the desired replica count during an update (defaulting to 25% or an absolute value).\\n* The `maxUnavailable` property determines how many pod instances can be unavailable relative to the desired replica count during an update (defaulting to 25% or an absolute value).\\n\\n**Key Takeaways:**\\n\\n* By setting `maxSurge` and `maxUnavailable`, you can control the number of pods that are created and deleted during a rollout.\\n* These properties can help ensure that there is always a minimum number of available pod instances during an update.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* The `maxUnavailable` property in Deployments determines how many pods can be unavailable at any given time.\\n* In the default configuration, `maxUnavailable` is set to 25% of the total number of replicas, meaning that up to 25% of the pods can be unavailable while rolling out an update.\\n* However, when using the `extensions/v1beta1` version of Deployments, both `maxSurge` and `maxUnavailable` are set to 1 by default, which affects how rollouts occur.\\n\\n**Important Details:**\\n\\n* When `maxSurge` is set to 1 and `maxUnavailable` is set to 0 (default configuration), the rollout process involves creating new pods of the updated version (`v2`) while keeping all existing pods (`v1`) available.\\n* When `maxSurge` is set to 1 and `maxUnavailable` is set to 1, the rollout process involves deleting one old pod (`v1`) and creating one new pod (`v2`) at a time, until both versions are available.\\n\\nThese details highlight the importance of understanding how `maxSurge` and `maxUnavailable` affect the rollout process in Deployments.',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. When updating an application using Deployments, it\\'s possible to have one replica unavailable while still meeting the desired replica count.\\n2. The `maxUnavailable` property doesn\\'t mean that only one pod can be unavailable; rather, it means that at least one pod must remain available for each replica in the desired replica count.\\n3. To pause a rollout process, you can use the `kubectl rollout pause deployment` command.\\n\\n**Important Details:**\\n\\n* When updating an application, the rollout process may delete one pod and create two new ones to ensure that the maximum number of pods isn\\'t exceeded.\\n* The `maxUnavailable` property is relative to the desired replica count, so if the replica count is set to three and `maxUnavailable` is set to one, at least two pods must remain available while the update process can exceed one pod being unavailable.\\n* Pausing a rollout process allows you to verify that everything is fine with the new version before proceeding with the rest of the rollout.\\n* A \"canary release\" is a technique for minimizing the risk of rolling out a bad version of an application by replacing only one or a small number of old pods with new ones, allowing you to test the new version with a small number of users first.',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes provides a way to update applications declaratively using Deployments.\\n2. The rollout process can be paused or resumed to control the deployment of new versions.\\n3. Pausing a Deployment can prevent updates from kicking off the rollout process.\\n\\n**Important Details:**\\n\\n* To pause a rollout, use `kubectl rollout pause deployment <deployment-name>`.\\n* To resume a rollout, use `kubectl rollout resume deployment <deployment-name>`.\\n* The `minReadySeconds` property can be used to slow down or block rollouts of bad versions.\\n* A pod is considered ready when its readiness probes return success. If a pod fails its readiness probe before `minReadySeconds` have passed, the rollout will be blocked.\\n\\n**Key Concepts:**\\n\\n* Deployments: a way to update applications declaratively in Kubernetes.\\n* Rollout process: the process of deploying new versions of an application.\\n* Pausing and resuming rollouts: controlling the deployment of new versions.\\n* `minReadySeconds`: a property that specifies how long a newly created pod should be ready before it is treated as available.',\n",
       "  'Here is a summary of the text with main points and important details:\\n\\n**Main Points:**\\n\\n1. Using deployments for updating apps declaratively can help prevent issues in production environments.\\n2. A readiness probe can be used to ensure that an app is functioning correctly before it is exposed to clients.\\n\\n**Important Details:**\\n\\n* The author previously deployed a buggy version of an app (v3) without a readiness probe, which caused problems.\\n* To prevent this issue from happening again, the author will deploy version v3 with a readiness probe defined.\\n* A YAML file (`kubia-deployment-v3-with-readinesscheck.yaml`) is used to update the deployment and introduce the readiness probe.\\n* The YAML file includes:\\n\\t+ `minReadySeconds` set to 10, which means that Kubernetes will wait for 10 seconds before considering a pod ready.\\n\\t+ A readiness probe defined for the container.\\n\\t+ `maxUnavailable` set to 0, which means that the deployment will replace pods one by one.\\n\\n**Key Takeaways:**\\n\\n* Using a readiness probe can help prevent issues in production environments.\\n* Deploying an app with a readiness probe can ensure that it is functioning correctly before it is exposed to clients.',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. To update a Deployment with kubectl, use the `apply` command with a YAML file containing the updated configuration.\\n2. When using `kubectl apply`, be careful not to include the `replicas` field in the YAML if you don\\'t want to change the desired replica count.\\n3. After applying the updates, use the `rollout status` command to check on the rollout process.\\n\\n**Important Details:**\\n\\n1. The readiness probe is a mechanism that checks whether a pod is ready to receive traffic.\\n2. In this example, the readiness probe performs an HTTP GET request against the container every second.\\n3. When updating a Deployment with kubectl `apply`, it\\'s possible to inadvertently scale the Deployment if the new YAML file contains a different replica count than the existing one.\\n\\n**Key Commands:**\\n\\n1. `kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml deployment \"kubia\" configured`\\n2. `kubectl rollout status deployment kubia`',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n* A deployment process was used to update an application declaratively.\\n* The rollout process was blocked due to a failing readiness probe in the new pod.\\n\\n**Important Details:**\\n\\n* The readiness probe failed after 5 requests, causing the pod to be marked as not ready.\\n* As a result, the pod was removed from the service and clients were not directed to it.\\n* The rollout process would not continue because the new pod needed to be available for at least 10 seconds before being considered ready.\\n* The `maxUnavailable` property was set to 0, preventing any original pods from being removed during the rollout process.\\n\\n**Key Takeaways:**\\n\\n* A failing readiness probe can prevent a deployment from rolling out a new version of an application.\\n* The rollout process will wait for a pod to be available and ready before creating additional replicas or removing original ones.',\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. A rollout of a deployment can be stuck if it's not making progress, which is actually a good thing because it prevents a non-working service from being deployed.\\n2. The `readiness probe` helps prevent a bad version of an application from being rolled out across all pods.\\n3. If the readiness probe fails shortly after succeeding, the bad version can be rolled out across all pods if `minReadySeconds` is not set properly.\\n\\n**Important Details:**\\n\\n1. By default, a rollout is considered failed after 10 minutes if it's not making progress.\\n2. The time after which a deployment is considered failed is configurable through the `progressDeadlineSeconds` property in the Deployment spec.\\n3. If a rollout fails to make progress within the specified deadline, it can be aborted using the `kubectl rollout undo` command.\\n4. In future versions of Kubernetes, the rollout will be automatically aborted when the time specified in `progressDeadlineSeconds` is exceeded.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* This chapter has taught you how to use a declarative approach to deploying and updating applications in Kubernetes, making your life easier.\\n* You now know how to perform various tasks related to Deployments, including:\\n\\t+ Rolling updates of pods managed by a ReplicationController\\n\\t+ Creating Deployments instead of lower-level resources\\n\\t+ Updating pods by editing the pod template in the Deployment specification\\n\\t+ Rolling back or aborting a Deployment\\n\\t+ Controlling the rate of rolling updates through maxSurge and maxUnavailable properties\\n* You also learned how to use minReadySeconds and readiness probes to block the rollout of faulty versions.\\n* Additionally, you know how to define multiple resources in a single YAML file using three dashes as a separator.\\n\\n**Important Details:**\\n\\n* The declarative approach allows for easier deployment and management of sets of pods created from the same pod template.\\n* This chapter has not covered running sets of pods with individual persistent storage, which will be discussed in the next chapter.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* The chapter covers deploying stateful, clustered applications using StatefulSets.\\n* Current knowledge includes running single-instance and replicated stateless pods, as well as stateful pods with persistent storage.\\n\\n**Important Details:**\\n\\n* The goal is to replicate a database pod instance that uses persistent storage.\\n* Key topics include:\\n\\t+ Deploying stateful clustered applications\\n\\t+ Providing separate storage for each instance of a replicated pod\\n\\t+ Guaranteeing a stable name and hostname for pod replicas\\n\\t+ Starting and stopping pod replicas in a predictable order\\n\\t+ Discovering peers through DNS SRV records',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* ReplicaSets create multiple pod replicas from a single template, but all replicas share the same PersistentVolumeClaim (PVC) and PersistentVolume.\\n* This means that each replica cannot have its own separate storage volume.\\n* Running multiple replicas with separate storage for each is not possible using ReplicaSets alone.\\n\\n**Important Details:**\\n\\n* The issue arises because the reference to a PVC is in the pod template, which is used to create multiple replicas. Each replica will use the same PVC and PersistentVolume.\\n* To run multiple replicas with separate storage, you cannot use a single ReplicaSet.\\n* Two alternatives are proposed:\\n\\t1. Creating pods manually, but this requires manual management and recreation of pods in case of node failures or accidental deletions.\\n\\t2. Using one ReplicaSet per pod instance, where each ReplicaSet has its own PVC and PersistentVolumeClaim. This approach is more cumbersome than using a single ReplicaSet.\\n\\nOverall, the text highlights the limitations of ReplicaSets when it comes to running multiple replicas with separate storage for each, and proposes two alternatives that have their own trade-offs.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Using multiple ReplicaSets to deploy stateful applications can be cumbersome, as it's difficult to change the desired replica count.\\n* A better solution is to use a single ReplicaSet, but have each pod maintain its own persistent state using separate directories within the same storage volume.\\n\\n**Important Details:**\\n\\n* To achieve this, all pods in the ReplicaSet must use the same PersistentVolume, but with separate file directories for each instance (Figure 10.3).\\n* Each pod can automatically select and create a data directory that isn't being used by any other instance at the time.\\n* This solution requires coordination between instances and makes the shared storage volume the bottleneck.\\n\\n**Additional Context:**\\n\\n* Some clustered applications require each instance to have a long-lived stable identity, which can be challenging when pods are killed and replaced (Section 10.1.2).\\n* Figures 10.2 and 10.3 illustrate the different approaches to deploying stateful applications using ReplicaSets and Persistent Volumes.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When a ReplicaSet replaces a pod, it creates a new pod with a new hostname and IP, even if the data in its storage volume comes from the old pod.\\n2. Certain distributed stateful applications require a stable network identity for each cluster member.\\n3. This can cause problems when pods are rescheduled, as the whole application cluster would need to be reconfigured.\\n\\n**Important Details:**\\n\\n* To work around this problem, you can create a dedicated Kubernetes Service for each individual pod instance, providing a stable network address.\\n* This is similar to creating a ReplicaSet for each member to provide individual storage.\\n* Combining these two techniques results in a setup where each pod has its own service and volume, but the pods themselves cannot know which service they are exposed through.\\n\\n**Key Takeaways:**\\n\\n* Creating a dedicated Service for each pod instance can help provide a stable network address, but it is not a perfect solution.\\n* The individual pods still have limitations, such as being unable to self-register in other pods using their stable IP.',\n",
       "  \"Here's a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes provides a simple solution for running replicated stateful applications using StatefulSets.\\n2. StatefulSets are designed to manage applications where instances must be treated as non-fungible individuals, each with a stable name and state.\\n\\n**Important Details:**\\n\\n* **Pets vs. Cattle Analogy:** Stateful apps are like pets, requiring special attention and replacement with identical instances, while stateless apps are like cattle, easily replaceable without notice.\\n* **Comparison to ReplicaSets/ReplicationControllers:** StatefulSets differ from ReplicaSets/ReplicationControllers in that they manage pods as individual entities (like pets), whereas ReplicaSets/ReplicationControllers treat pods as interchangeable units (like cattle).\\n* **Key Features of StatefulSets:**\\n\\t+ Each pod instance has a stable name and state.\\n\\t+ When a pod instance dies or the node it's running on fails, the new instance must be resurrected with the same name, network identity, and state.\\n\\nOverall, StatefulSets provide a way to manage replicated stateful applications in Kubernetes, ensuring that each instance is treated as an individual entity with its own stable name and state.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. A StatefulSet ensures that pods are scheduled in a way that retains their identity and state.\\n2. It allows for easy scaling up or down, like a ReplicaSet.\\n3. Each pod created by a StatefulSet has its own set of volumes (storage) and a predictable identity.\\n\\n**Important Details:**\\n\\n1. Pods created by a StatefulSet are not exact replicas of each other, unlike those created by a ReplicaSet.\\n2. Each pod is assigned an ordinal index (zero-based), which is used to derive its name and hostname.\\n3. A StatefulSet requires the creation of a corresponding governing headless Service to provide network identity to each pod.\\n4. This Service provides a DNS entry for each pod, allowing peers and other clients to address it by its hostname.\\n\\n**Key Takeaways:**\\n\\n* StatefulSets are designed for applications that require persistent state and predictable identities.\\n* They offer a way to scale up or down while maintaining the integrity of the application's state.\\n* The use of a governing Service is essential to provide network identity and DNS entries for each pod.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. **StatefulSets**: Provide a way to deploy replicated stateful applications, allowing for persistent storage and consistent naming conventions across pods.\\n2. **Unique Naming Conventions**: StatefulSets assign unique names and hostnames to each pod instance, making it easier to access and manage them using DNS lookups (e.g., `a-0.foo.default.svc.cluster.local`).\\n3. **Replacing Lost Pods**: When a pod managed by a StatefulSet disappears, the set automatically replaces it with a new instance having the same name and hostname as the lost pod.\\n\\n**Important Details:**\\n\\n* StatefulSets use DNS lookups to discover members of the set (e.g., SRV records).\\n* The replacement pod gets the same name and hostname as the lost pod, unlike ReplicaSets which assign a completely new unrelated name.\\n* This distinction is illustrated in Figure 10.6, showing how a StatefulSet replaces a lost pod with a new instance having the same identity, whereas a ReplicaSet replaces it with an unrelated pod.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Key Points:**\\n\\n* StatefulSets ensure that stateful pods have a stable identity, even if they are rescheduled to a different node.\\n* Scaling up or down a StatefulSet creates a new pod instance with the next unused ordinal index.\\n* When scaling down a StatefulSet, the instances with the highest ordinal index are removed first, making the effects of scale-down predictable.\\n\\n**Important Details:**\\n\\n* StatefulSets do not permit scale-down operations if any of the instances are unhealthy.\\n* Each stateful pod instance needs to use its own storage, and when a stateful pod is rescheduled, the new instance must have the same storage attached to it.\\n* StatefulSets achieve this by using stable dedicated storage for each instance.\\n\\n**Key Benefits:**\\n\\n* Predictable scale-down operations\\n* Stable identity for stateful pods\\n* Dedicated storage for each instance',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. StatefulSets require persistent storage for stateful pods, which must be decoupled from the pods themselves.\\n2. To achieve this, StatefulSets can create PersistentVolumeClaims (PVCs) along with each pod instance.\\n3. PVCs are created using volume claim templates, which enable the StatefulSet to stamp out multiple PVCs as needed.\\n\\n**Important Details:**\\n\\n1. Each pod in a StatefulSet must reference a different PVC to have its own separate storage.\\n2. The StatefulSet itself creates the PVCs, rather than relying on an administrator to create them upfront.\\n3. Scaling up a StatefulSet creates new API objects (pods and PVCs), while scaling down deletes only the pod, leaving the PVCs intact.\\n4. To avoid data loss, PVCs must be deleted manually when scaling down a StatefulSet.\\n\\n**Key Concepts:**\\n\\n1. PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs)\\n2. Volume claim templates\\n3. Scaling up and down a StatefulSet\\n4. Data persistence and storage for stateful applications',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When a StatefulSet is scaled down, its Persistent Volume Claim (PVC) remains attached to the cluster, allowing for easy reattachment when scaling back up.\\n2. This means that if you accidentally scale down a StatefulSet, you can undo the mistake by scaling up again and the new pod will get the same persisted state as before.\\n\\n**Important Details:**\\n\\n* StatefulSets have different guarantees regarding their pods compared to ReplicaSets or Replication-Controllers.\\n* StatefulSets ensure that pods are always replaced with an identical pod (same name, hostname, persistent storage, etc.) when they are deleted.\\n* However, if Kubernetes can't be sure about the state of a pod, creating a replacement pod with the same identity could lead to two instances of the app running in the system, both bound to the same storage.\\n\\n**Key Takeaways:**\\n\\n* StatefulSets provide stable identity and storage for their pods, which is not the case with regular, stateless pods.\\n* This stability comes with guarantees regarding pod replacement and PVC reattachment.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* Kubernetes needs to ensure that two stateful pod instances do not run with the same identity and are bound to the same PersistentVolumeClaim.\\n* A StatefulSet must guarantee \"at-most-one\" semantics for stateful pod instances, meaning it must be certain that a pod is no longer running before creating a replacement pod.\\n* This has implications for how node failures are handled.\\n\\n**Important Details:**\\n\\n* To demonstrate this concept, the author will create a simple clustered data store using a StatefulSet.\\n* The data store app will allow storing and retrieving a single data entry on each pod instance.\\n* The app\\'s source code is shown in Listing 10.1, which includes functions for handling POST requests (storing data) and GET requests (returning hostname and data file contents).\\n* The author will use the kubia app as a starting point to create this new data store app.\\n\\n**Key Takeaways:**\\n\\n* StatefulSets require careful management of stateful pod instances to ensure that two instances do not run with the same identity.\\n* A StatefulSet must guarantee \"at-most-one\" semantics for stateful pod instances, which has implications for how node failures are handled.',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. The author creates a simple web application that writes POST data to a file and returns GET requests with the hostname and stored data.\\n2. The application is deployed using a StatefulSet in Kubernetes.\\n3. To deploy the app, three types of objects are created: PersistentVolumes for storing data files, a governing Service, and the StatefulSet itself.\\n\\n**Important Details:**\\n\\n* The Dockerfile for building the container image remains unchanged from previous versions.\\n* Three PersistentVolumes are required to scale the StatefulSet up to three replicas.\\n* If using Minikube, PersistentVolumes can be deployed using the `persistent-volumes-hostpath.yaml` file in the code archive.\\n* If using Google Kubernetes Engine (GKE), actual GCE Persistent Disks must be created using the `gcloud` command.\\n\\n**Key Takeaways:**\\n\\n* The StatefulSet is used to deploy a stateful application that requires persistent storage.\\n* PersistentVolumes are required to store data files, and can be dynamically provisioned if supported by the cluster.\\n* A governing Service is also required for the StatefulSet.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The chapter discusses deploying replicated stateful applications using StatefulSets in Kubernetes.\\n2. Three PersistentVolumes (pv-a, pv-b, and pv-c) are created from the `persistent-volumes-gcepd.yaml` file.\\n3. A headless Service is created to provide network identity for stateful pods.\\n\\n**Important Details:**\\n\\n* The PersistentVolumes use GCE Persistent Disks as the underlying storage mechanism, which may not be suitable for clusters running outside of Google Kubernetes Engine or Google Compute Engine.\\n* If running on a non-Google cluster, the PersistentVolume definition should be modified to use an alternative volume type, such as NFS.\\n* The Service created is headless and will provide network identity for stateful pods in the StatefulSet.\\n\\n**Key Takeaways:**\\n\\n* To deploy a StatefulSet, you need to create a headless Service first.\\n* Three PersistentVolumes are created with capacities of 1 Mebibyte each.\\n* The PersistentVolumes use GCE Persistent Disks as storage.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Creating a StatefulSet for an application named \"kubia\" that uses a headless Service to enable peer discovery between pods.\\n2. Defining a StatefulSet manifest to create multiple replicas of the kubia application.\\n\\n**Important Details:**\\n\\n* The Service is created with `clusterIP` set to None, making it a headless Service that enables peer discovery.\\n* The StatefulSet has 2 replicas and uses a template to define the container image, ports, and volume mounts for each pod.\\n* A PersistentVolumeClaim (PVC) named \"data\" is defined in the StatefulSet manifest using `volumeClaimTemplates`.\\n* Each pod created by the StatefulSet will have an \"app=kubia\" label and mount a PVC at `/var/data`.\\n\\n**Key Takeaways:**\\n\\n* The StatefulSet manifest is similar to ReplicaSet or Deployment manifests, but with additional features for managing stateful applications.\\n* The `volumeClaimTemplates` list allows you to define a template for creating multiple PVCs that can be used by each pod in the StatefulSet.',\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. Creating a StatefulSet with a pod template does not automatically add a volume, but the StatefulSet adds it to the pod specification and configures it to be bound to a PersistentVolumeClaim.\\n2. When creating a StatefulSet, the first pod is created before the second one, even if the configuration specifies two replicas.\\n3. This behavior is intentional to avoid race conditions in clustered stateful applications.\\n\\n**Important Details:**\\n\\n* The `kubectl create` command is used to create a StatefulSet from a YAML file (`kubia-statefulset.yaml`).\\n* Initially, only one pod (kubia-0) is created and running.\\n* The second pod (kubia-1) is created after the first one is up and ready.\\n* The `kubectl get po` command can be used to list pods and track their status.\\n* The `kubectl get po <pod_name> -o yaml` command can be used to view a pod's specification in YAML format.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Using a StatefulSet**\\n\\n* A StatefulSet was used to create a PersistentVolumeClaim (PVC) named `data-kubia-0` and a corresponding persistent volume.\\n* The PVC was created using a template, which generated two PVCs: `data-kubia-0` and `data-kubia-1`.\\n* The names of the generated PVCs are composed of the name defined in the volumeClaimTemplate and the name of each pod.\\n\\n**Examining Generated Persistent Volume Claims**\\n\\n* The generated PVCs were listed using `kubectl get pvc`, which confirmed that they were created.\\n* The PVCs were bound to persistent volumes named `pv-c` and `pv-a`.\\n\\n**Communicating with Pods through the API Server**\\n\\n* Since the Service is headless, it's not possible to communicate with pods through it.\\n* Instead, you can use the API server as a proxy to individual pods by hitting a URL in the format `<apiServerHost>:<port>/api/v1/namespaces/default/pods/<podName>/proxy/<path>`.\\n* This method is cumbersome due to security restrictions, but `kubectl proxy` can be used to simplify communication with pods through the API server.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The chapter discusses using StatefulSets to deploy replicated stateful applications.\\n2. A proxy is used to communicate with the API server without dealing with authentication and SSL certificates.\\n3. A GET request is sent to a pod through the kubectl proxy, demonstrating that the request was received and handled by the app running in the pod.\\n\\n**Important Details:**\\n\\n* To use the kubectl proxy, run `kubectl proxy` and access the API server at `localhost:8001`.\\n* To send a request to a pod, use the URL format `localhost:8001/api/v1/namespaces/default/pods/<pod_name>/proxy/`.\\n* The request goes through two proxies: the kubectl proxy and the API server proxy.\\n* POST requests can also be sent through the API server by using the same URL format as GET requests.\\n* When a POST request is received, the app stores data from the request body into a local file.\\n\\n**Key Takeaways:**\\n\\n* Using StatefulSets to deploy replicated stateful applications\\n* Communicating with the API server through a proxy for easier access\\n* Sending GET and POST requests to pods through the API server',\n",
       "  'Here is a summary of the text with main points and important details:\\n\\n**Main Points:**\\n\\n1. The data stored on a StatefulSet pod (kubia-0) was retrieved successfully using a GET request.\\n2. When deleting the kubia-0 pod, it was rescheduled to a new node, but retained its state and identity.\\n3. The new pod (kubia-0) was created with the same name and storage as the old pod.\\n\\n**Important Details:**\\n\\n* Each StatefulSet pod has its own state, which is not shared between pods.\\n* When deleting a StatefulSet pod, it is rescheduled to a new node, but retains its identity (name, hostname, and storage).\\n* The new pod may be scheduled to a different node than the old pod, especially in a multi-node cluster.\\n* In a single-node cluster like Minikube, this behavior cannot be observed.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. StatefulSets are used to deploy replicated stateful applications, where each pod has a unique identity.\\n2. When a pod in a StatefulSet is deleted, it is replaced by a new pod with the same hostname and persistent data as the original pod.\\n3. Scaling down a StatefulSet deletes pods but leaves PersistentVolumeClaims (PVCs) untouched.\\n\\n**Important Details:**\\n\\n* A StatefulSet always replaces a deleted pod with a new one that has the same identity, including hostname and persistent data.\\n* When scaling down a StatefulSet, the pod with the highest ordinal number is deleted first, followed by the pod with the second-highest ordinal number, and so on.\\n* Scaling up a StatefulSet after an extended time period should be no different than deleting pods and having the StatefulSet recreate them immediately.\\n\\n**Additional Context:**\\n\\n* The text also mentions exposing stateful pods through a regular, non-headless Service, which is a common way for clients to connect to pods in a cluster.\\n* A figure (Figure 10.11) illustrates how a stateful pod may be rescheduled to a different node while retaining its name, hostname, and storage.',\n",
       "  \"Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. Accessing a ClusterIP service (like a StatefulSet) from within a cluster can be done without creating a pod to access it.\\n2. The API server provides a proxy feature that allows accessing services like individual pods.\\n3. To access a service, use the URI path `/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>`.\\n\\n**Important Details:**\\n\\n* A ClusterIP service is not externally exposed and can only be accessed from within the cluster.\\n* The API server's proxy feature allows accessing services like individual pods.\\n* Clients inside the cluster can use a service for storing and reading data from a clustered data store, but each request will land on a random node.\\n* Peer discovery in a StatefulSet is an important requirement that allows each member to find other members of the cluster.\\n\\n**Key Concepts:**\\n\\n* ClusterIP service\\n* API server's proxy feature\\n* Peer discovery in a StatefulSet\",\n",
       "  \"Here's a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes uses SRV (Service Location) records to allow pods to discover their peers without contacting the API.\\n2. SRV records point to hostnames and ports of servers providing a specific service.\\n3. To list the SRV records for your stateful pods, you can run a temporary pod with the `dig` DNS lookup tool.\\n\\n**Important Details:**\\n\\n1. Kubernetes creates SRV records to point to the hostnames of the pods backing a headless service.\\n2. The command to list SRV records is `kubectl run -it srvlookup --image=tutum/dnsutils --rm --restart=Never -- dig SRV kubia.default.svc.cluster.local`.\\n3. This command runs a one-off pod that lists the SRV records for your headless service.\\n4. The output shows two SRV records pointing to the two pods backing the headless service, along with their own A records.\\n5. To enable each pod to discover its peers in an app, you can use the `dns.resolveSrv` function in Node.js.\\n\\n**Key Concepts:**\\n\\n1. SRV records\\n2. Headless services\\n3. DNS lookup tool (`dig`)\\n4. StatefulSets and peer discovery\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The order of returned SRV records in a StatefulSet is random due to equal priority, so don\\'t expect a specific order (e.g., kubia-0 before kubia-1).\\n2. A client connecting to a data store cluster through the kubia-public Service lands on a random node and currently has no way to see all entries.\\n3. To improve this, each node needs to find its peers using DNS SRV records.\\n\\n**Important Details:**\\n\\n* The app uses DNS SRV records to discover peers in a StatefulSet.\\n* The `dns.resolveSrv()` function is used to perform the DNS lookup and obtain SRV records.\\n* If no peers are discovered, the response will indicate \"No peers discovered.\"\\n* The code snippet shows how to modify an app\\'s source code to implement peer discovery through DNS.\\n\\n**Context:**\\n\\nThe text appears to be from a book or tutorial on Kubernetes and StatefulSets. The goal is to improve client interactions with a data store cluster by having each node respond with data from all cluster nodes, which requires discovering peers using DNS SRV records.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* A simple distributed data store application (Kubia) is deployed using a StatefulSet, which allows for replicated stateful applications.\\n* When a GET request is received by the app, it performs an SRV record lookup and sends GET requests to each pod backing the service, including itself.\\n* The app returns a list of all nodes along with the data stored on each node.\\n\\n**Important Details:**\\n\\n* The container image for this new version of the app is available at docker.io/luksa/kubia-pet-peers.\\n* To update the StatefulSet, you can modify its pod template to use the new image and set the replica count to 3.\\n* The process involves contacting each pod pointed to by an SRV record to get its data.\\n\\n**Key Concepts:**\\n\\n* StatefulSets\\n* Replicated stateful applications\\n* SRV records\\n* Distributed data store application (Kubia)',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Discovering Peers in a StatefulSet**\\n\\n* To update a StatefulSet, use `kubectl edit` or `patch` command.\\n* Update the `spec.replicas` to 3 and modify the `spec.template.spec.containers.image` attribute to point to a new image (`luksa/kubia-pet-peers`).\\n* Save the file and exit the editor to update the StatefulSet.\\n\\n**Result**\\n\\n* A new replica called `kubia-2` starts running with the new image.\\n* The existing two replicas (`kubia-0` and `kubia-1`) are not updated, as StatefulSets do not perform a rollout when the template is modified (until Kubernetes version 1.7).\\n* To update the existing replicas, delete them manually using `kubectl delete po`, and the StatefulSet will bring them up again based on the new template.\\n\\n**Trying Out Your Clustered Data Store**\\n\\n* Once the two pods are up, test your clustered data store by posting requests to the cluster.\\n* Use `curl` commands to post data to the cluster and read stored data from each pod (`kubia-0` and `kubia-1`).',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The chapter discusses StatefulSets, which are used to deploy replicated stateful applications.\\n* A StatefulSet ensures that there will never be two pods running with the same identity and storage.\\n\\n**Important Details:**\\n\\n* When a client request reaches one of the cluster nodes, it discovers all its peers, gathers data from them, and sends all the data back to the client. This allows for horizontal scaling.\\n* If a node fails abruptly, Kubernetes cannot know the state of the node or its pods, so a StatefulSet cannot create a replacement pod until it knows for certain that the pod is no longer running.\\n* To resolve this issue, the cluster administrator must either delete the pod or delete the whole node (which deletes all the pods scheduled to the node).\\n* The chapter includes an exercise where you simulate a node's disconnection from the network by shutting down its eth0 network interface.\\n\\n**Key Takeaways:**\\n\\n* StatefulSets provide a way to deploy replicated stateful applications.\\n* StatefulSets ensure that there will never be two pods running with the same identity and storage.\\n* If a node fails, Kubernetes cannot create a replacement pod until it knows for certain that the pod is no longer running.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. A StatefulSet's behavior when a node fails can be understood by simulating a node failure.\\n2. When a node's network interface is shut down, the Kubelet running on that node can no longer contact the Kubernetes API server.\\n3. The control plane marks the node as NotReady and the status of all pods on that node becomes Unknown.\\n\\n**Important Details:**\\n\\n* If the node comes back online and reports its status again, the pod's status will be marked as Running again.\\n* However, if the pod's status remains unknown for more than a few minutes (a configurable time), the pod is automatically evicted from the node by the master.\\n* When a pod is evicted, it is deleted by the master and the Kubelet terminates the pod.\\n* If the Kubelet cannot reach the master (e.g., because the node's network interface was shut down), the pod will continue to run.\\n\\n**Key Terms:**\\n\\n* NotReady: The status of a node that has failed or is unreachable.\\n* Unknown: The status of a pod whose node has failed or is unreachable.\\n* Evicted: A pod that has been automatically deleted by the master due to its status remaining unknown for too long.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* A StatefulSet named \"kubia\" has three pods (kubia-0, kubia-1, and kubia-2) running on a Kubernetes cluster.\\n* The node hosting pod kubia-0 becomes unresponsive and is considered lost by the control plane.\\n* Despite this, the pod\\'s container continues to run fine.\\n\\n**Important Details:**\\n\\n* When trying to delete the pod manually using `kubectl delete po kubia-0`, it appears to be deleted, but the same pod remains in the cluster with an \"Unknown\" status.\\n* The StatefulSet does not immediately create a replacement pod as expected.\\n* The AGE column of the `kubectl get po` command shows that the remaining pod is not new, indicating that deleting the original pod did not trigger a new one to be created.\\n\\n**Key Takeaways:**\\n\\n* Deleting a pod in a StatefulSet does not necessarily result in an immediate replacement pod being created.\\n* The behavior of StatefulSets can differ from other Kubernetes resources.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Why the Pod Isn\\'t Deleted**\\n\\n* The pod was marked for deletion before it was actually deleted, as the control plane had already deleted it to evict it from the node.\\n* The pod\\'s status shows \"Terminating\", but it will not be removed until the Kubelet on its node notifies the API server that the pod\\'s containers have terminated.\\n* However, since the node\\'s network is down, this notification will never happen.\\n\\n**Forcibly Deleting the Pod**\\n\\n* To delete the pod without waiting for confirmation from the Kubelet, use `kubectl delete po <pod_name> --force --grace-period 0`.\\n* This warning is displayed: \"Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\"\\n* After forcibly deleting the pod, a new pod will be created.\\n\\n**Important Warning**\\n\\n* Do not delete stateful pods forcibly unless you know the node is no longer running or is unreachable and will remain so forever.\\n* Before continuing, try to bring the disconnected node back online by restarting it through the GCE web console or using the `gcloud compute instances reset` command.\\n\\n**Summary of Chapter 10**\\n\\n* This chapter concludes with a summary of key points on using StatefulSets to deploy stateful apps:\\n\\t+ Give replicated pods individual storage\\n\\t+ Provide a stable identity to a pod\\n\\t+ Create a StatefulSet and corresponding headless governing Service\\n\\t+ Scale and update a StatefulSet\\n\\t+ Discover other members of the StatefulSet through DNS',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* StatefulSets in Kubernetes allow applications to connect with other members through their host names.\\n* StatefulSets enable the forcible deletion of stateful pods.\\n\\n**Important Details:**\\n\\n* The next chapter will delve into the individual components that control and manage the Kubernetes cluster, ensuring apps run smoothly.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The book has been building an understanding of what Kubernetes does, but now it's time to dive into how it works.\\n* This chapter will cover the internal workings of Kubernetes.\\n\\n**Important Details:**\\n\\n* The chapter will explain:\\n\\t+ What components make up a Kubernetes cluster\\n\\t+ How each component functions and interacts with others\\n\\t+ How Deployments are created and result in running pods\\n\\t+ What a running pod is and how it works\\n\\t+ How the network between pods operates\\n\\t+ How Kubernetes Services work to provide high-availability\\n* The chapter aims to provide a detailed understanding of the inner workings of Kubernetes, including its components, controllers, and resource management.\",\n",
       "  \"Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n* A Kubernetes cluster consists of two parts: the Control Plane and worker nodes.\\n* The Control Plane manages the cluster's state, while worker nodes run application containers.\\n\\n**Components of the Control Plane:**\\n\\n* etcd (distributed persistent storage)\\n* API server\\n* Scheduler\\n* Controller Manager\\n\\nThese components store and manage the cluster's state but do not run application containers.\\n\\n**Components Running on Worker Nodes:**\\n\\n* Kubelet\\n* Kubernetes Service Proxy (kube-proxy)\\n* Container Runtime (e.g., Docker, rkt)\\n\\n**Add-on Components:**\\n\\n* Kubernetes DNS server\\n* Dashboard\\n* Ingress controller\\n* Heapster (explained in chapter 14)\\n* Container Network Interface network plugin\\n\\n**Important Details:**\\n\\n* All components run as individual processes.\\n* The components and their inter-dependencies are shown in a figure (not included in the summary).\\n* The Control Plane and worker nodes work together to provide a fully functional Kubernetes cluster.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The various components of a Kubernetes system can run independently, but all features are enabled when they are running together.\\n* Components communicate only through the API server, which is the central hub for communication.\\n* The API server communicates directly with etcd, while other components modify the cluster state by talking to the API server.\\n\\n**Important Details:**\\n\\n* The API server initiates connections with other components, but can also connect to the Kubelet in certain situations (e.g., fetching logs or attaching to a container).\\n* The kubectl attach command is similar to kubectl exec, but attaches to the main process running in a container instead of running an additional one.\\n* Multiple instances of individual components can be run on separate servers, but all components of the Control Plane must be run on the same node (master node).\\n* The API server exposes a ComponentStatus resource that shows the health status of each Control Plane component.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes Control Plane components can be run in multiple instances for high availability, but only one instance of the Scheduler and Controller Manager can be active at a time.\\n2. The Kubelet runs as a regular system component and is responsible for running other components as pods.\\n3. The Control Plane components (API server, Scheduler, Controller Manager) can be deployed on the system directly or run as pods.\\n\\n**Important Details:**\\n\\n1. etcd stores all Kubernetes objects in a persistent manner to ensure their manifests survive API server restarts and failures.\\n2. The Kubelet is deployed on the master node and runs other components as pods.\\n3. Worker nodes run kube-proxy and Flannel networking pods, which provide overlay networks for pods.\\n4. The Control Plane components can be run in multiple instances for high availability, but only one instance of the Scheduler and Controller Manager can be active at a time.\\n\\n**Key Components:**\\n\\n1. etcd (persistent storage)\\n2. API server\\n3. Scheduler\\n4. Controller Manager\\n5. Kubelet (runs other components as pods)\\n6. kube-proxy (overlay network for pods)\\n7. Flannel (overlay network for pods)',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. **etcd**: etcd is a fast, distributed, and consistent key-value store used by Kubernetes to store cluster state and metadata.\\n2. **Architecture**: The only component that talks directly to etcd is the Kubernetes API server, while other components read and write data indirectly through the API server.\\n3. **Resource Storage**: Kubernetes stores all its data in etcd under /registry, with each resource having a unique key.\\n\\n**Important Details:**\\n\\n1. **etcd versions**: Kubernetes can use either etcd version 2 or version 3, but version 3 is now recommended due to improved performance.\\n2. **Optimistic Concurrency Control**: etcd uses optimistic concurrency control (also known as optimistic locking) to prevent data conflicts when multiple clients try to update the same resource.\\n3. **ResourceVersion field**: Each Kubernetes resource includes a metadata.resourceVersion field, which must be passed back to the API server when updating an object to ensure that the update is successful.\\n\\n**Key Takeaways:**\\n\\n* etcd is a critical component of the Kubernetes architecture\\n* Optimistic concurrency control ensures data consistency and prevents conflicts when multiple clients try to update the same resource.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes stores its resources (e.g., pods, namespaces) as key-value entries in etcd.\\n2. The keys correspond to resource types learned about in previous chapters.\\n3. Each entry represents an individual pod or other resource.\\n\\n**Important Details:**\\n\\n* Prior to v1.7 of Kubernetes, Secret resources were stored in plaintext JSON format in etcd, but are now encrypted for security.\\n* Multiple Control Plane components access the store directly and must ensure consistency and validity of stored objects.\\n* The API server stores the complete JSON representation of a resource in etcd.\\n* The hierarchical key space in etcd can be thought of as a filesystem with resources represented as JSON files.\\n\\n**Key Takeaways:**\\n\\n* Understanding how Kubernetes stores its resources is crucial for managing and troubleshooting clusters.\\n* Ensuring consistency and validity of stored objects is essential to prevent data corruption or inconsistencies.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Ensuring Consistency in etcd**\\n\\n* When running multiple instances of etcd for high availability, consistency across all nodes must be ensured.\\n* etcd uses the RAFT consensus algorithm to achieve this, which ensures that each node's state is either the current agreed-upon state or one of the previously agreed-upon states.\\n* The consensus algorithm requires a majority (or quorum) of nodes to progress to the next state. If the cluster splits into two disconnected groups, the state in both groups can never diverge.\\n\\n**Key Points**\\n\\n* Optimistic locking mechanism is used in Kubernetes to handle conflicts properly and ensure consistent data.\\n* The API server ensures that updates to the cluster state are always consistent and valid, and that changes are only performed by authorized clients.\\n* In a split-brain scenario (where the cluster splits into two disconnected groups), only the side with the majority (quorum) accepts state changes.\\n\\n**Important Details**\\n\\n* etcd uses RAFT consensus algorithm to ensure consistency across multiple instances.\\n* A quorum is required for the cluster to progress to the next state.\\n* If one group of nodes has a majority, it can modify the cluster state, while the other group cannot.\",\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* An etcd cluster should have an odd number of instances to ensure that a majority can exist even if one instance fails.\\n* Having two or four etcd instances is worse than having one or three, respectively, as it increases the chance of the whole cluster failing.\\n* For large clusters, an etcd cluster of five or seven nodes is sufficient and can handle a two- or three-node failure.\\n\\n**Important Details:**\\n\\n* The Kubernetes API server provides a CRUD interface for querying and modifying the cluster state over a RESTful API.\\n* The API server stores its state in etcd and performs validation on objects to prevent improperly configured objects from being stored.\\n* The API server also handles optimistic locking, ensuring that changes to an object are not overridden by other clients in the event of concurrent updates.\\n* The API server has multiple plugins, including authentication, authorization, admission control, and resource validation, which work together to manage incoming requests.\\n\\n**Key Takeaways:**\\n\\n* Understanding the importance of having an odd number of etcd instances is crucial for ensuring cluster stability.\\n* The Kubernetes API server plays a central role in managing the cluster state and providing a consistent interface for clients.',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Authentication and Authorization Process**\\n\\nThe API server authenticates clients using one or more authentication plugins. These plugins inspect the HTTP request to determine who is sending it. The authenticated user's data (username, ID, groups) is then used for authorization.\\n\\n**Authorization Plugins**\\n\\nOne or more authorization plugins are configured on the API server to determine whether the authenticated user can perform a requested action on a resource. If any plugin allows the action, the API server proceeds to the next stage.\\n\\n**Admission Control Plugins**\\n\\nIf the request is trying to create, modify, or delete a resource, it passes through Admission Control plugins. These plugins can:\\n\\n* Modify resources by initializing missing fields or overriding values\\n* Reject requests for various reasons\\n* Even modify related resources not in the original request\\n\\nExamples of Admission Control plugins include:\\n\\n* AlwaysPullImages: forces image pulling every time a pod is deployed\\n* ServiceAccount: applies default service account to pods without explicit specification\\n* NamespaceLifecycle: prevents creation of pods in namespaces being deleted or non-existent\\n* ResourceQuota: ensures pods use only allotted CPU and memory resources\\n\\nNote that Admission Control plugins are not used for read-only requests.\",\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. After passing through Admission Control plugins, the Kubernetes API server validates resources and stores them persistently in etcd.\\n2. The API server does not create or manage resources itself; it only enables controllers to observe changes to deployed resources.\\n3. Clients can watch for changes to resources by opening an HTTP connection to the API server.\\n\\n**Important Details:**\\n\\n1. Controllers, such as those in the Controller Manager, are responsible for creating and managing resources (e.g., pods, ReplicaSets).\\n2. The API server notifies clients of resource changes through a streaming mechanism.\\n3. Clients can use tools like kubectl with the --watch flag to be notified of creation, modification, or deletion of resources.\\n4. When an object is updated, the API server sends the new version of the object to all connected clients watching that object.\\n\\n**Key Concepts:**\\n\\n1. Admission Control plugins\\n2. etcd (persistent storage for Kubernetes resources)\\n3. Controllers and Controller Manager\\n4. Resource watching and streaming notifications',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Understanding Kubernetes Architecture**\\n\\nThe text discusses the architecture of Kubernetes, specifically the watch mechanism used by the API server. The watch mechanism allows for real-time updates on pod status.\\n\\n**Pod Status Updates**\\n\\nThe text shows an example of pod status updates using `kubectl get pods -o yaml --watch`, which displays the entire YAML configuration for each update.\\n\\n**Understanding the Scheduler**\\n\\nThe Scheduler is a Control Plane component that assigns nodes to newly created pods. It uses the watch mechanism to wait for new pods and assign a node to each one. The Scheduler does not instruct the selected node to run the pod, but rather updates the pod definition through the API server, which then notifies the Kubelet.\\n\\n**Scheduling Algorithm**\\n\\nThe default scheduling algorithm in Kubernetes involves two parts:\\n\\n1. **Filtering**: Filtering the list of all nodes to obtain a list of acceptable nodes that the pod can be scheduled to.\\n2. **Prioritization**: Prioritizing the acceptable nodes and selecting the best one. If multiple nodes have the highest score, round-robin is used to ensure pods are deployed across all of them evenly.\\n\\n**Important Details**\\n\\n* The Scheduler does not use machine learning or advanced techniques by default.\\n* The selection of a node involves filtering and prioritization.\\n* Round-robin is used if multiple nodes have the same highest score.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To determine which nodes are suitable to host a pod, the Kubernetes Scheduler passes each node through a list of configured predicate functions.\\n2. These functions check various conditions such as resource availability, specific node requests, label matching, port availability, volume usage, taints, and affinity rules.\\n\\n**Important Details:**\\n\\n* The Scheduler checks for:\\n\\t+ Resource availability (e.g., hardware resources, memory, disk pressure)\\n\\t+ Specific node requests (e.g., scheduling to a specific node by name)\\n\\t+ Label matching (e.g., matching a node label with the pod's node selector)\\n\\t+ Port and volume availability\\n\\t+ Taints and tolerations\\n\\t+ Affinity rules (e.g., node and/or pod affinity or anti-affinity rules)\\n* If all conditions are met, the Scheduler selects the best node for the pod based on factors such as resource utilization, number of pods already running on each node.\\n* The Scheduler prioritizes nodes that have more available resources and fewer pods running, making it a better choice to host the new pod.\",\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Pod Scheduling**\\n\\n* When scheduling pods to nodes provided by cloud infrastructure, it may be more cost-effective to schedule them on one node and relinquish the other back to the provider.\\n* Ideally, multiple replicas of a pod should be spread across different nodes to ensure service availability in case of a single node failure.\\n\\n**Pod Affinity and Anti-Affinity**\\n\\n* By default, pods belonging to the same Service or ReplicaSet are spread across multiple nodes.\\n* However, this can be customized by defining pod affinity and anti-affinity rules (explained in chapter 16).\\n\\n**Scheduling Complexity**\\n\\n* Scheduling is complex due to various factors, such as infrastructure specifics and pod requirements.\\n* As a result, the Scheduler can be configured or replaced with a custom implementation.\\n\\n**Using Multiple Schedulers**\\n\\n* Instead of running a single Scheduler, multiple Schedulers can be run in the cluster.\\n* Each pod can specify which Scheduler should schedule it by setting the `schedulerName` property in the pod spec.\\n\\n**Controller Manager**\\n\\n* The API server and Scheduler only assign resources and nodes to pods; other components are needed to ensure the system converges towards its desired state.\\n* Controllers running inside the Controller Manager perform reconciliation tasks, such as:\\n\\t+ Replication Manager (for ReplicationController resources)\\n\\t+ ReplicaSet, DaemonSet, and Job controllers\\n* These controllers will eventually be split into separate processes, allowing for custom implementations if necessary.',\n",
       "  \"Here's a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes has various controllers that manage different resources, such as Deployments, Services, and Persistent Volumes.\\n2. Controllers watch the API server for changes to resources and perform operations in response, including creating or updating other resources.\\n3. Controllers run a reconciliation loop to ensure the actual state matches the desired state specified in the resource's spec section.\\n\\n**Important Details:**\\n\\n1. Each controller connects to the API server independently and uses the watch mechanism to be notified of changes.\\n2. Controllers use a re-list operation periodically to ensure they haven't missed any events, as using watches doesn't guarantee they won't miss an event.\\n3. Controllers do not communicate directly with each other; instead, they connect to the API server and watch for changes to resources they are responsible for.\\n4. The source code for controllers is available on GitHub and can be explored by browsing through their constructor functions, which create Informers that listen for API object updates.\\n\\n**Key Concepts:**\\n\\n1. Controllers\\n2. Reconciliation loop\\n3. Watch mechanism\\n4. Informer (listener that gets called every time an API object gets updated)\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The **Replication Manager** is the controller that brings ReplicationController resources to life.\\n2. It's not the ReplicationControllers themselves that do the actual work, but rather the Replication Manager.\\n\\n**Important Details:**\\n\\n* The Replication Manager uses an infinite loop to monitor the number of pods matching its pod selector and compares it to the desired replica count.\\n* Instead of polling the pods in every iteration, the controller is notified by the API server through a watch mechanism of any changes that may affect the desired replica count or the number of matched pods.\\n* The Replication Manager creates an Informer to listen for changes to specific resources (e.g. pod instances).\\n* When too few pod instances are running, the Replication-Controller runs additional instances by creating them, but it doesn't actually run them itself.\\n\\n**Key Concepts:**\\n\\n* Watch mechanism: a way for the API server to notify clients of any changes that may affect the desired replica count or the number of matched pods.\\n* Informer: an object that listens for changes to specific resources (e.g. pod instances).\\n* SyncHandler: a function that gets invoked each time the controller needs to do something, and is often stored in a field called syncHandler or something similar.\",\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Kubernetes Controllers**\\n\\nThe chapter discusses various Kubernetes controllers, which are responsible for managing and running pods in a cluster. The main controllers covered are:\\n\\n1. **Replication Manager**: Creates and manages pod resources through the API server.\\n2. **ReplicaSet**, **DaemonSet**, and **Job** controllers: Similar to Replication Manager, they create pod resources from templates and let the Kubelet run them.\\n3. **Deployment** controller: Manages deployments by creating a ReplicaSet and scaling old and new pods based on the deployment strategy.\\n4. **StatefulSet** controller: Creates, manages, and deletes pods according to a StatefulSet resource spec, and also instantiates and manages PersistentVolumeClaims.\\n5. **Node** controller: Manages Node resources, keeping them in sync with actual cluster nodes, monitoring node health, and evicting pods from unreachable nodes.\\n6. **Service** controller: Requests and releases load balancers for LoadBalancer-type Services.\\n\\n**Key Points**\\n\\n* Controllers don't run pods directly; they create pod definitions that are then executed by the Kubelet.\\n* Deployment and StatefulSet controllers manage rollouts of new versions based on changes to their respective API objects.\\n* Node controller monitors node health and evicts pods from unreachable nodes.\\n* Service controller requests and releases load balancers for LoadBalancer-type Services.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. **Endpoints Controller**: Manages the list of endpoints (IPs and ports) for Services by watching both Services and Pods. When a Service or Pod is added, updated, or deleted, it updates the Endpoints resource accordingly.\\n2. **Namespace Controller**: Responsible for deleting all resources belonging to a namespace when that namespace is deleted.\\n3. **Persistent Volume Controller**: Finds an appropriate PersistentVolume to bind to a PersistentVolumeClaim based on capacity and access mode.\\n\\n**Important Details:**\\n\\n* The Endpoints controller creates or deletes the Endpoints object as necessary.\\n* The Namespace controller uses the API server to delete all resources belonging to a namespace when it's deleted.\\n* The Persistent Volume controller selects the smallest PersistentVolume with matching access mode and declared capacity above the requested capacity to bind to a PersistentVolumeClaim.\\n\\n**Key Concepts:**\\n\\n* Services, Pods, Endpoints, Namespaces, PersistentVolumes, and PersistentVolumeClaims are key concepts in Kubernetes that these controllers manage.\",\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Understanding Kubernetes Internals**\\n\\nThe chapter discusses the internal workings of Kubernetes, focusing on controllers, the Control Plane, and the Kubelet.\\n\\n**Key Points:**\\n\\n1. **Controllers**: These components operate on API objects through the API server, without communicating with Kubelets directly. They update resources in the API server, which are then acted upon by Kubelets.\\n2. **Persistent Volume Claims (PVCs)**: When a PVC is created, the controller keeps an ordered list of Persistent Volumes for each access mode and returns the first volume from the list. When the PVC is deleted, the volume is unbound and reclaimed according to its reclaim policy.\\n3. **Kubelet**: This component runs on worker nodes and is responsible for everything running on a node. Its main tasks include:\\n\\t* Registering the node by creating a Node resource in the API server\\n\\t* Monitoring the API server for Pods scheduled to the node and starting containers\\n\\t* Constantly monitoring running containers and reporting their status, events, and resource consumption to the API server\\n\\t* Running container liveness probes and restarting containers when probes fail\\n\\t* Terminating containers when their Pod is deleted from the API server\\n4. **Kubelet's Manifest Directory**: The Kubelet can run pods based on pod manifest files in a specific local directory, allowing for running containerized versions of Control Plane components as pods.\\n\\n**Important Details:**\\n\\n1. Controllers and Kubelets operate independently, with controllers updating resources in the API server and Kubelets acting upon those updates.\\n2. The Kubelet is responsible for managing containers on worker nodes, while the Control Plane handles one part of the operation of the whole system.\\n3. Understanding how the Kubelet works is crucial to understanding how things unfold in a Kubernetes cluster.\",\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Running custom system containers in Kubernetes can be done using DaemonSets, which is the recommended method.\\n2. Every worker node runs the kube-proxy, which ensures clients can connect to services defined through the Kubernetes API.\\n3. The kube-proxy performs load balancing across pods backing a service.\\n\\n**Important Details:**\\n\\n* The initial implementation of the kube-proxy was a userspace proxy that used an actual server process to accept connections and redirect them to pods.\\n* The userspace proxy configured iptables rules to intercept connections destined for service IPs and redirect them to the proxy server.\\n* A diagram (Figure 11.9) illustrates the userspace proxy mode, showing how the kube-proxy configures iptables rules to redirect connections to the proxy server.\\n\\n**Key Components:**\\n\\n* DaemonSets\\n* kube-proxy\\n* Kubernetes API\\n* Services\\n* Pods\\n* Load balancing',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Kubernetes Internals**\\n\\n* The `kube-proxy` component was originally an actual proxy, but now uses `iptables` rules to redirect packets to backend pods without passing them through a proxy server (called \"iptables proxy mode\").\\n* This has a major impact on performance, as it handles packets only in kernel space instead of user space.\\n* In iptables proxy mode, connections are not balanced across pods in a true round-robin fashion, but rather randomly selected.\\n\\n**Kubernetes Add-ons**\\n\\n* Kubernetes add-ons are components that enable features such as DNS lookup and exposing multiple HTTP services through a single external IP address.\\n* These add-ons are deployed as pods using YAML manifests submitted to the API server.\\n* Some add-ons are deployed using a `Deployment` or `ReplicationController` resource, while others use a `DaemonSet`.\\n* Examples of add-ons include the Ingress controller and dashboard, which are deployed in Minikube using a `ReplicationController`.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. The architecture of a Kubernetes cluster is explored, focusing on the DNS add-on and Ingress controllers.\\n2. The DNS server pod is deployed as a deployment in the `kube-system` namespace and provides internal DNS services to pods in the cluster.\\n3. The DNS server uses the API server's watch mechanism to update its records with changes to Services and Endpoints.\\n4. Most Ingress controllers work by running a reverse proxy server (e.g., Nginx) that observes Ingress, Service, and Endpoints resources through the watch mechanism and updates the proxy configuration accordingly.\\n\\n**Important Details:**\\n\\n1. The DNS add-on is deployed as a deployment in the `kube-system` namespace.\\n2. The DNS server pod uses the API server's watch mechanism to update its records with changes to Services and Endpoints.\\n3. Ingress controllers forward traffic directly to the service's pod instead of going through the service IP, which affects client IP preservation.\\n4. Ingress controllers are preferred over Services in certain use cases due to their ability to preserve client IPs.\\n\\n**Context:** The text appears to be from a book or tutorial on Kubernetes and is discussing the architecture and functionality of the DNS add-on and Ingress controllers in a Kubernetes cluster.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The Kubernetes system consists of small, loosely coupled components that work together to maintain the desired state of the system.\\n* These components include the API server, Scheduler, Controller Manager, Kubelet, and kube-proxy.\\n* When a user submits a pod manifest or creates a Deployment resource, various Kubernetes components coordinate to bring the actual state in line with the desired state.\\n\\n**Important Details:**\\n\\n* The controllers (e.g. Deployment controller), Scheduler, and Kubelet watch the API server for changes to their respective resource types.\\n* When a Pod resource is created, the following components are involved:\\n\\t+ Controllers (e.g. Deployment controller)\\n\\t+ Scheduler\\n\\t+ Kubelet\\n\\t+ etcd (hidden behind the API server)\\n* The process of creating a pod involves a coordinated dance of these components to bring the actual state in line with the desired state.\\n\\n**Key Concepts:**\\n\\n* Kubernetes components work together to maintain the desired state of the system.\\n* Controllers, Scheduler, and Kubelet watch the API server for changes to their respective resource types.\\n* Creating a Pod or Deployment resource triggers a coordinated process involving multiple Kubernetes components.',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. When a YAML file containing a Deployment manifest is submitted to Kubernetes using `kubectl`, it triggers a chain of events.\\n2. The Kubernetes API server validates the Deployment specification, stores it in etcd, and returns a response to `kubectl`.\\n3. The Deployment controller creates a ReplicaSet for the new Deployment resource.\\n\\n**Important Details:**\\n\\n* The API server notifies all clients watching the list of Deployments through its watch mechanism, including the Deployment controller.\\n* The Deployment controller creates a ReplicaSet based on the current specification of the Deployment.\\n* The ReplicaSet then creates actual pods.\\n* This process involves creating a new ReplicaSet resource through the Kubernetes API.\\n* The chain of events is triggered by the creation of a new Deployment resource and involves multiple components, including the API server, controllers, and nodes.\\n\\n**Key Components:**\\n\\n* `kubectl`: submits the YAML file to the Kubernetes API server\\n* API server: validates the Deployment specification, stores it in etcd, and returns a response to `kubectl`\\n* Deployment controller: creates a ReplicaSet for the new Deployment resource\\n* ReplicaSet: creates actual pods based on the current specification of the Deployment',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. The ReplicaSet controller creates Pod resources based on the pod template in the ReplicaSet.\\n2. The Scheduler assigns a node to each newly created Pod.\\n3. The Kubelet runs the Pod's containers on the assigned node.\\n\\n**Important Details:**\\n\\n* The ReplicaSet controller verifies whether enough existing Pods match the selector before creating new ones.\\n* The Pod resources are stored in etcd, but lack an associated node until the Scheduler assigns one.\\n* The Kubelet is responsible for running the Pod's containers and instructing Docker to start them.\\n* Both Control Plane components and the Kubelet emit events to the API server as they perform actions.\\n\\n**Additional Information:**\\n\\n* Events can be retrieved using `kubectl get events`, but this can be painful due to the lack of temporal order.\\n* Using `kubectl get events` with the `--watch` option is recommended for seeing what's happening in the cluster.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Understanding Running Pods**\\n\\nThe text explains what happens when a pod is created in Kubernetes. The process is shown through a series of `kubectl get events` commands, which display a list of events emitted by the controllers.\\n\\n**Key Points:**\\n\\n1. A running pod is not just a single container; it's a collection of resources (containers, volumes, etc.) managed by the Kubelet.\\n2. When a pod is created, the controller creates a ReplicaSet, which in turn creates one or more pods.\\n3. The Kubelet runs each pod as a separate Docker container on a worker node.\\n4. Each pod has its own set of resources (e.g., containers, volumes) and can be managed independently.\\n\\n**Important Details:**\\n\\n1. The `kubectl get events` command shows the events emitted by the controllers, including the source of the event (e.g., deployment-controller, replicaset-controller).\\n2. The NAME and KIND columns show the resource being acted upon, while the REASON column provides more details about what happened.\\n3. A pod can contain multiple containers, each running as a separate Docker container on a worker node.\\n\\nOverall, this text helps readers understand the inner workings of Kubernetes pods and how they are managed by the Kubelet.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To understand Kubernetes internals, you can SSH into a node using tools like minikube or gcloud compute ssh.\\n2. Once inside the node, you can list running containers using docker ps.\\n\\n**Important Details:**\\n\\n* When listing running containers, you may see an additional container called \"pause\" that is not doing any work (its command is \"/pause\").\\n* This \"pause\" container is actually a pod infrastructure container that holds all the namespaces for the containers in a pod.\\n* The pause container\\'s lifecycle is tied to the pod\\'s schedule and deletion, ensuring that all containers in the pod share the same Linux namespaces.\\n* When an application container dies and restarts, it can reuse the namespaces provided by the pause container.\\n\\n**Key Concepts:**\\n\\n* Pod infrastructure container (pause container)\\n* Shared Linux namespaces between containers in a pod\\n* Lifecycle management of containers within a pod',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Each pod in a Kubernetes cluster gets its own unique IP address.\\n* Pods can communicate with each other through a flat, NAT-less network.\\n* The network setup is not managed by Kubernetes itself, but rather by the system administrator or a Container Network Interface (CNI) plugin.\\n\\n**Important Details:**\\n\\n* The pods' containers must be able to communicate with each other, regardless of whether they are running on the same worker node or not.\\n* The network must allow for direct communication between pods without any network address translation (NAT).\\n* When pod A sends a packet to pod B, the source IP that pod B sees must be the same as the IP that pod A sees as its own.\\n* This NAT-less network allows applications running inside pods to self-register in other pods and simplifies networking for these applications.\\n\\n**Key Concept:**\\n\\n* The absence of NAT between pods enables applications running inside them to communicate directly, just like they would on a traditional network with multiple machines connected to the same switch.\",\n",
       "  \"Here's a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. In Kubernetes, pods can communicate with each other without NAT (Network Address Translation) using virtual Ethernet interfaces.\\n2. When a pod communicates with services outside the cluster, its source IP address is changed to the host worker node's IP address.\\n3. Building a proper Kubernetes cluster involves setting up networking according to these requirements.\\n\\n**Important Details:**\\n\\n1. A pod's IP address and network namespace are set up and held by the infrastructure container (pause container).\\n2. The pod's containers use its network namespace, which is connected to other pods on the same node through virtual Ethernet interface pairs.\\n3. Each pod has a veth pair, with one end in the host's namespace and the other end in the pod's namespace.\\n4. These veth pairs are connected to a bridge (10.1.1.0/24) that allows communication between pods on the same node.\\n\\n**Key Concepts:**\\n\\n* Virtual Ethernet interfaces (veth pairs)\\n* Network namespaces\\n* Bridge (10.1.1.0/24)\\n* Infrastructure container (pause container)\\n\\nOverall, this text explains how Kubernetes enables communication between pods on the same node using virtual Ethernet interfaces and network namespaces.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Containers in a pod share the same network namespace and can communicate with each other through a bridge interface.\\n2. To enable communication between pods on different nodes, the bridges on those nodes must be connected somehow.\\n3. This connection can be achieved using overlay or underlay networks, or regular layer 3 routing.\\n\\n**Important Details:**\\n\\n* Each pod has its own veth pair (virtual Ethernet) that connects to the bridge interface in the host's namespace.\\n* Containers within a pod share the same IP address range and can communicate with each other through the bridge interface.\\n* To prevent IP address conflicts, bridges on different nodes must use non-overlapping IP address ranges.\\n* In a plain layer 3 networking setup, routing tables on both nodes need to be configured to route packets between the two nodes' IP address ranges.\\n\\n**Key Concepts:**\\n\\n* Network namespace: a virtual network environment shared by containers within a pod.\\n* Bridge interface: a network interface that connects multiple networks together.\\n* Veth pair: a virtual Ethernet interface that connects a container's network namespace to the host's network namespace.\\n* Overlay or underlay networks: methods for connecting bridges on different nodes to enable communication between pods.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Understanding Kubernetes Internals**\\n\\n* The chapter discusses how Kubernetes handles network communication between containers on different nodes.\\n* By default, packets sent from one container to another must be routed through a bridge, then over the wire, and finally through the veth pair of the destination container. This only works when nodes are connected to the same network switch without routers in between.\\n* If there are routers in between, it becomes difficult and error-prone to configure them to route packets between nodes. Instead, a Software Defined Network (SDN) can be used to make nodes appear as though they're connected to the same network switch.\\n\\n**Container Network Interface (CNI)**\\n\\n* A project called Container Network Interface (CNI) was started to make it easier to connect containers into a network.\\n* CNI allows Kubernetes to use any CNI plugin, including Calico, Flannel, Romana, Weave Net, and others.\\n* Installing a network plugin is relatively easy, requiring only the deployment of a YAML file containing a DaemonSet and supporting resources.\\n\\n**How Services are Implemented**\\n\\n* The chapter explains how Services work in Kubernetes, which was previously discussed in Chapter 5.\\n* To truly understand Services, it's essential to know how they're implemented, including their use of network communication between containers.\",\n",
       "  \"Here's a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The kube-proxy process on each node handles everything related to Services.\\n* There are two modes for kube-proxy: userspace proxy mode (now deprecated) and iptables proxy mode (default).\\n* Each Service gets its own stable IP address and port, which is virtual and not assigned to any network interface.\\n\\n**Important Details:**\\n\\n* When a service is created, the API server assigns a virtual IP address immediately.\\n* The kube-proxy process on each node sets up iptables rules to redirect packets destined for the service IP/port pair to one of the pods backing the service.\\n* Kube-proxy also watches for changes to Endpoints objects, which hold the IP/port pairs of all the pods that back the service.\\n* When a client pod sends a packet to the Service, kube-proxy intercepts it and redirects it to one of the pods backing the service.\\n\\n**Key Takeaways:**\\n\\n* Services are implemented using the kube-proxy process on each node.\\n* The virtual IP address assigned to a Service is not directly accessible by clients.\\n* Kube-proxy uses iptables rules to redirect packets destined for the Service to the underlying pods.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When a packet is sent to a Kubernetes Service's virtual IP and port, it is first handled by the node's kernel according to iptables rules.\\n2. The kernel checks if the packet matches any of the iptables rules, which can redirect the packet to a randomly selected backend pod.\\n3. In this example, the packet was redirected to Pod B2.\\n\\n**Important Details:**\\n\\n* The iptables rule in question replaces the destination IP and port with those of a randomly selected pod.\\n* The packet's destination is changed from 172.30.0.1:80 to 10.1.2.1:8080, which is the IP and port of Pod B2.\\n* This process is handled by kube-proxy, which watches for changes to services and endpoints.\\n\\n**Key Concepts:**\\n\\n* iptables rules are used to redirect packets to a randomly selected backend pod.\\n* kube-proxy plays a key role in this process by watching for changes to services and endpoints.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Running applications in Kubernetes allows for high availability without manual intervention.\\n2. To achieve high availability, both the application and the Kubernetes Control Plane components must be up all the time.\\n3. Ensuring app high availability is relatively simple using a Deployment resource and configuring an appropriate number of replicas.\\n\\n**Important Details:**\\n\\n* To reduce downtime, apps should be horizontally scalable or use leader-election mechanisms to ensure only one instance is active at a time.\\n* Leader-election can be implemented using a sidecar container that performs the necessary operations and signals the main container when it should become active.\\n* Even if an app is not horizontally scalable, running a Deployment with a replica count of 1 can still provide high availability, although there may be a short period of downtime in case of node failure.\\n\\n**Key Concepts:**\\n\\n* Horizontal scalability\\n* Leader-election mechanisms\\n* Sidecar containers\\n* Kubernetes Control Plane components',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* To make Kubernetes highly available, multiple master nodes are required.\\n* Each master node runs multiple instances of key components:\\n\\t+ etcd (distributed data store)\\n\\t+ API server\\n\\t+ Controller Manager\\n\\t+ Scheduler\\n\\n**Important Details:**\\n\\n* Making etcd highly available is straightforward, as it was designed to run in a distributed system. Multiple instances can be run on separate machines and configured to replicate data across each other.\\n* A three-machine cluster can still accept read and write operations even if one node fails.\\n* To increase fault tolerance beyond a single node, five or seven etcd nodes are recommended, allowing the cluster to handle two or three node failures respectively.\\n\\nOverall, making Kubernetes highly available requires running multiple master nodes with redundant instances of key components.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Running highly available clusters requires careful consideration of etcd, API server, and controller/Scheduler instances.\\n2. Multiple etcd instances can be run for high availability, but more than 7 instances are not necessary and may impact performance.\\n3. The API server can be made highly available by running multiple instances with a load balancer in front to direct clients to healthy servers.\\n\\n**Important Details:**\\n\\n* Running multiple API server instances is simple because they are stateless and do not need to communicate with each other.\\n* Controllers (Manager, Scheduler) and the Scheduler cannot run multiple instances simultaneously due to potential conflicts and undesired effects. Instead, only one instance can be active at a time using the `--leader-elect` option.\\n* When an active controller/Scheduler fails, another instance takes over as leader, ensuring that only one instance performs actual work at any given time.\\n\\n**Key Takeaways:**\\n\\n* Running multiple etcd instances is relatively simple but may impact performance beyond 7 instances.\\n* API server high availability can be achieved with multiple instances and a load balancer.\\n* Controllers and the Scheduler require careful management to ensure high availability, using the `--leader-elect` option to prevent conflicts.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes control plane components (Controller Manager and Scheduler) can run collocated or on separate machines.\\n2. Leader election mechanism in these components uses a simple resource creation approach, specifically using an Endpoints object.\\n\\n**Important Details:**\\n\\n* The leader election mechanism does not require direct communication between components; instead, it relies on the API server to manage the process.\\n* Any other resource could be used for leader election, but Endpoints is chosen because it has no side effects unless a Service with the same name exists.\\n* The Scheduler creates an Endpoints resource called \"kube-scheduler\" in the kube-system namespace to elect a leader.\\n* The control-plane.alpha.kubernetes.io/leader annotation on this resource contains information about the current leader, including its identity and last update time.\\n* Optimistic concurrency ensures that only one instance can write its name to the resource, making it the leader.\\n* Once elected as leader, the instance must periodically update the resource (every 2 seconds by default) to maintain its leadership status.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The chapter aims to improve knowledge of Kubernetes inner workings.\\n* It covers various components that make up a Kubernetes cluster and their responsibilities.\\n\\n**Important Details:**\\n\\n* Key components include:\\n\\t+ API server\\n\\t+ Scheduler\\n\\t+ Controllers running in the Controller Manager\\n\\t+ Kubelet\\n\\t+ Infrastructure container (binding containers within a pod)\\n\\t+ Network bridge (enabling communication between pods on the same node)\\n\\t+ Kube-proxy (performing load balancing across pods in the same service)\\n* The chapter also discusses how to make the cluster highly available by running multiple instances of each Control Plane component.\\n* A future topic will be securing the API server and the entire cluster.',\n",
       "  'Here is a summary of the text:\\n\\n**Securing the Kubernetes API Server**\\n\\nThis chapter focuses on securing the Kubernetes API server, which allows applications to interact with the cluster. The main points are:\\n\\n* **Authentication**: The API server can be configured with one or more authentication plugins, which verify the identity of users making requests.\\n* **ServiceAccounts**: ServiceAccounts are used to authenticate pods and allow them to access resources in the cluster. Their permissions can be configured.\\n* **Role-Based Access Control (RBAC)**: This plugin is used to manage permissions for different subjects (users, groups, service accounts) in the cluster.\\n* **Roles and RoleBindings**: Roles define a set of permissions, while RoleBindings assign these roles to specific users or service accounts.\\n* **ClusterRoles and ClusterRoleBindings**: These are similar to Roles and RoleBindings, but apply to all clusters in an organization.\\n* **Default roles and bindings**: The chapter will cover the default roles and bindings that come with Kubernetes.\\n\\nThese points highlight the importance of authentication and authorization in securing the Kubernetes API server.',\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Authentication in Kubernetes**\\n\\n* The API server uses authentication plugins to determine who's sending a request. Each plugin examines the request and returns the username, user ID, and groups back to the API server core.\\n* Available authentication methods include:\\n\\t+ Client certificate\\n\\t+ Authentication token passed in an HTTP header\\n\\t+ Basic HTTP authentication\\n\\t+ Others\\n* Authentication plugins are enabled through command-line options when starting the API server.\\n\\n**Users and Groups**\\n\\n* Kubernetes distinguishes between two types of clients: human users and pods (applications running inside them).\\n* Both types of clients are authenticated using authentication plugins.\\n* Users are managed by an external system, such as a Single Sign On (SSO) system, while pods use service accounts created and stored in the cluster.\\n* No resource represents user accounts, so they cannot be created, updated, or deleted through the API server.\\n\\n**Important Details**\\n\\n* Service accounts are essential for running pods and will be explored in detail.\\n* Groups are used to grant permissions to multiple users at once, rather than granting them individually.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* In Kubernetes, groups returned by authentication plugins are strings representing arbitrary group names, but built-in groups have special meaning (e.g. system:unauthenticated, system:authenticated).\\n* ServiceAccounts are used to authenticate applications running inside pods.\\n* A ServiceAccount is associated with a pod and represents the identity of the app running in it.\\n\\n**Important Details:**\\n\\n* The API server requires clients to authenticate themselves before performing operations on the server.\\n* Pods can authenticate by sending the contents of the file /var/run/secrets/kubernetes.io/serviceaccount/token, which holds the ServiceAccount's authentication token.\\n* ServiceAccount usernames are formatted like this: system:serviceaccount:<namespace>:<service account name>.\\n* The API server passes the ServiceAccount username to authorization plugins to determine whether an action is allowed to be performed by the ServiceAccount.\\n* ServiceAccounts are resources scoped to individual namespaces and can be listed using `kubectl get sa`.\\n* A default ServiceAccount is automatically created for each namespace.\",\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* A Kubernetes namespace contains its own default ServiceAccount, but additional ones can be created if necessary.\\n* Each pod is associated with exactly one ServiceAccount, but multiple pods can use the same ServiceAccount.\\n* ServiceAccounts are used to control which resources each pod has access to.\\n\\n**Important Details:**\\n\\n* To assign a ServiceAccount to a pod, specify its name in the pod manifest. If not specified, the pod will use the default ServiceAccount in the namespace.\\n* The API server uses the authentication token from a request to authenticate the client and determine whether the related ServiceAccount is allowed to perform the requested operation.\\n* The system-wide authorization plugin configured by the cluster administrator determines which ServiceAccounts have access to which resources.\\n* From Kubernetes version 1.6 on, the role-based access control (RBAC) plugin is recommended for most clusters.\\n\\n**Key Concepts:**\\n\\n* ServiceAccounts can be used to constrain pods' access to cluster metadata and API objects.\\n* Creating additional ServiceAccounts can improve cluster security by limiting the privileges of pods that don't need them.\\n* Pods can use the same ServiceAccount, but each pod must have its own ServiceAccount if it needs to modify resources.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Creating a new ServiceAccount in Kubernetes is easy using the `kubectl create serviceaccount` command.\\n2. A ServiceAccount can be associated with a custom token Secret, which contains authentication tokens (JWT) that can be used by pods to authenticate.\\n3. The token Secret can be mounted inside a pod, allowing it to access the cluster.\\n\\n**Important Details:**\\n\\n* When creating a new ServiceAccount, Kubernetes automatically creates a custom token Secret associated with it.\\n* The `kubectl describe` command can be used to inspect a ServiceAccount and its associated token Secret.\\n* The token Secret contains authentication tokens (JWT) that can be used by pods to authenticate.\\n* A ServiceAccount's mountable secrets list specifies which Secrets can be mounted inside a pod using this ServiceAccount. By default, all Secrets are mountable, but this can be configured to restrict access to specific Secrets.\\n\\n**Key Concepts:**\\n\\n* ServiceAccounts\\n* Token Secrets (JWT)\\n* Mountable Secrets\\n* Authentication tokens\",\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. A ServiceAccount can be configured to allow pods to mount only its own Secrets, by adding the annotation `kubernetes.io/enforce-mountable-secrets=\"true\"`.\\n2. A ServiceAccount can also contain image pull Secrets, which are used to authenticate with private container registries.\\n3. Image pull Secrets added to a ServiceAccount are automatically applied to all pods using that ServiceAccount.\\n\\n**Important Details:**\\n\\n* To use a custom ServiceAccount with a pod, you must set the `spec.serviceAccountName` field in the pod definition to the name of the desired ServiceAccount.\\n* A pod\\'s ServiceAccount cannot be changed after it is created.\\n* Image pull Secrets added to a ServiceAccount are used by all pods using that ServiceAccount, whereas mountable Secrets determine which Secrets can be mounted by each individual pod.\\n\\n**Key Concepts:**\\n\\n* ServiceAccounts\\n* Mountable Secrets\\n* Image pull Secrets\\n* Pod configuration',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. A Kubernetes Pod named \"curl-custom-sa\" was created to test using a custom ServiceAccount\\'s token to talk to the API server.\\n2. The Pod uses two containers: one for running a curl command, and another (ambassador) that listens on localhost:8001 and uses the custom ServiceAccount\\'s token to authenticate with the API server.\\n\\n**Important Details:**\\n\\n* The custom ServiceAccount is named \"foo\" and its token was mounted into both containers.\\n* The ambassador container uses the custom ServiceAccount\\'s token to talk to the API server, which allows it to list pods (in this case).\\n* This may be because the cluster doesn\\'t use RBAC authorization or has given all ServiceAccounts full permissions.\\n\\n**Key Findings:**\\n\\n* The custom ServiceAccount\\'s token was successfully used to authenticate with the API server.\\n* The Pod was able to list pods using the custom ServiceAccount, which suggests that it has been granted permission to do so.',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Securing a Kubernetes Cluster with Role-Based Access Control (RBAC)**\\n\\n* In earlier versions of Kubernetes, clusters were vulnerable to security threats if an attacker acquired an authentication token from a pod.\\n* With version 1.8.0, RBAC authorization plugin is enabled by default on many clusters and prevents unauthorized users from viewing or modifying the cluster state.\\n* The default ServiceAccount is not allowed to view or modify the cluster state unless additional privileges are granted.\\n\\n**Key Points**\\n\\n* Creating additional ServiceAccounts is necessary when using RBAC authorization plugin.\\n* RBAC prevents unauthorized access to cluster resources, such as Pods, Services, and Secrets.\\n* Users must authenticate themselves with credentials (e.g. authentication token, username and password, or client certificate) to perform actions on the Kubernetes API server.\\n\\n**Important Details**\\n\\n* The Kubernetes API server can be configured to use an authorization plugin to check whether an action is allowed to be performed by a user requesting the action.\\n* RBAC is the standard authorization plugin in Kubernetes, but other plugins such as Attribute-based access control (ABAC) and custom implementations are also available.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The Kubernetes API server uses HTTP methods to perform actions on resources, such as GET, POST, PUT, etc.\\n2. An authorization plugin (RBAC) determines whether a client can perform an action on a resource or not.\\n3. RBAC rules can apply to whole resource types, specific instances of a resource, and non-resource URL paths.\\n\\n**Important Details:**\\n\\n1. The RBAC plugin uses user roles as the key factor in determining permissions.\\n2. A subject (user, ServiceAccount, group) is associated with one or more roles, which determine what actions can be performed on resources.\\n3. If a user has multiple roles, they have access to all the permissions of any of their roles.\\n4. The API server will prevent users from performing actions that are not allowed by their roles.\\n\\n**Key Concepts:**\\n\\n1. RBAC (Role-Based Access Control) plugin\\n2. HTTP methods and verbs (GET, POST, PUT, etc.)\\n3. Resource types (Pods, Services, Secrets)\\n4. Roles and permissions in RBAC\\n\\nOverall, the text explains how the Kubernetes API server uses HTTP methods to perform actions on resources and how the RBAC plugin determines what actions can be performed by users based on their roles.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Kubernetes uses Role-Based Access Control (RBAC) to secure clusters with four resources:\\n\\t* Roles and ClusterRoles (define what can be done)\\n\\t* RoleBindings and ClusterRoleBindings (bind roles to users, groups, or ServiceAccounts)\\n2. The distinction between namespaced and cluster-level resources is important:\\n\\t* Roles and RoleBindings are namespaced\\n\\t* ClusterRoles and ClusterRoleBindings are cluster-level\\n3. Multiple bindings can exist in a single namespace, and multiple cluster-level bindings can be created.\\n\\n**Important Details:**\\n\\n1. Roles define what actions can be performed on specific resources.\\n2. RoleBindings bind roles to specific users, groups, or ServiceAccounts.\\n3. ClusterRoles and ClusterRoleBindings are used for cluster-wide access control.\\n4. Trying out these resources in a hands-on exercise is recommended to understand their effects.\\n\\n**Key Concepts:**\\n\\n1. Roles grant permissions\\n2. RoleBindings bind roles to subjects (users, groups, etc.)\\n3. RBAC provides fine-grained access control to Kubernetes clusters',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To explore RBAC (Role-Based Access Control) resources, you need to ensure that RBAC is enabled in your Kubernetes cluster.\\n2. You must be using at least version 1.6 of Kubernetes and have only the RBAC plugin configured as the authorization plugin.\\n3. If you're using GKE or Minikube, you may need to take additional steps to enable RBAC.\\n\\n**Important Details:**\\n\\n* To disable legacy authorization in GKE, use the `--no-enable-legacy-authorization` option when creating the cluster.\\n* To enable RBAC in Minikube, start it with the `--extra-config=apiserver.Authorization.Mode=RBAC` flag.\\n* If you previously disabled RBAC, re-enable it by running the command `$ kubectl delete clusterrolebinding permissive-binding`.\\n* The goal is to run two pods in different namespaces to demonstrate per-namespace security behavior.\\n\\n**Key Concepts:**\\n\\n* Roles and RoleBindings are namespaced resources, while ClusterRoles and ClusterRoleBindings have a cluster scope.\\n* Understanding these concepts is crucial for securing the Kubernetes API server using RBAC.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Creating two pods in separate namespaces (foo and bar) using kubectl.\\n2. Verifying that role-based access control (RBAC) is enabled by attempting to list services from within each pod.\\n3. Understanding how RBAC prevents a ServiceAccount from listing or modifying resources, even when running in the same namespace.\\n\\n**Important Details:**\\n\\n* The pods are created using the command `kubectl run test --image=luksa/kubectl-proxy -n foo` and `kubectl run test --image=luksa/kubectl-proxy -n bar`.\\n* To access a pod's shell, use `kubectl exec -it <pod_name> -n <namespace> sh`, where `<pod_name>` is the name of the pod and `<namespace>` is the namespace it runs in.\\n* The `curl` command is used to list services from within each pod, which demonstrates RBAC in action.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Roles are used to define what actions can be taken on which resources in Kubernetes.\\n2. A Role resource defines specific permissions for users, such as getting or listing services.\\n\\n**Important Details:**\\n\\n* Roles are namespaced and must specify the namespace where they will be created.\\n* Resources belong to API groups, which must be specified in the Role definition using the `apiGroups` field.\\n* The plural form of resources (e.g. \"services\" instead of \"service\") must be used when specifying permissions.\\n* Permissions can be limited to specific Service instances by specifying their names through an additional `resourceNames` field.\\n\\n**Key Points:**\\n\\n* A Role is created in the foo namespace and allows users to get and list Services in that namespace.\\n* The Role does not allow users to get or list Services in other namespaces.',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. Creating roles with role-based access control (RBAC) in Kubernetes.\\n2. Binding roles to ServiceAccounts to specify who can perform actions defined by the role.\\n\\n**Important Details:**\\n\\n* To create a role, use `kubectl create` with a YAML file or the `create role` command.\\n* Roles define what actions can be performed, but do not specify who can perform them.\\n* To bind a role to a ServiceAccount, create a RoleBinding resource using `kubectl create rolebinding`.\\n* The RoleBinding binds the role to the default ServiceAccount in its namespace.\\n* You can also bind roles to users or groups using the `--user` and `--group` arguments.\\n\\n**Key Commands:**\\n\\n* `kubectl create -f service-reader.yaml -n foo`: Create a role from a YAML file in the foo namespace.\\n* `kubectl create rolebinding test --role=service-reader --serviceaccount=foo:default -n foo`: Bind the role to the default ServiceAccount in the foo namespace.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* A RoleBinding in Kubernetes refers to a single Role, but can bind that Role to multiple subjects (e.g. ServiceAccounts, users, groups).\\n* The example shows a RoleBinding named \"test\" that binds the \"service-reader\" Role to the default ServiceAccount in namespace \"foo\".\\n* This allows the pod running under the default ServiceAccount in namespace \"foo\" to list Services from within that pod.\\n\\n**Important Details:**\\n\\n* A RoleBinding references a single Role using the `roleRef` property.\\n* The RoleBinding can bind the referenced Role to multiple subjects, such as one or more ServiceAccounts and users or groups.\\n* In this example, the RoleBinding binds the \"service-reader\" Role to the default ServiceAccount in namespace \"foo\", allowing the pod running under that ServiceAccount to list Services from within that namespace.',\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. To allow a pod in one namespace to access resources in another namespace, you can add the other pod's ServiceAccount to a RoleBinding in the target namespace.\\n2. This allows the pod to list Services in the target namespace.\\n\\n**Important Details:**\\n\\n* The command `kubectl edit rolebinding test -n foo` is used to modify a RoleBinding in the foo namespace.\\n* You need to add the following lines to the list of subjects:\\n\\t+ `kind: ServiceAccount`\\n\\t+ `name: default`\\n\\t+ `namespace: bar`\\n* This allows the pod running in the bar namespace to list Services in the foo namespace.\\n* A summary is provided of the current RBAC resources, including a RoleBinding in namespace foo that references the service-reader Role and binds the default ServiceAccounts in both the foo and bar namespaces.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes has two types of Role-Based Access Control (RBAC) resources: namespaced (e.g., Roles and RoleBindings) and cluster-level (e.g., ClusterRoles and ClusterRoleBindings).\\n2. ClusterRoles are used to allow access to non-namespaced resources or non-resource URLs.\\n3. ClusterRoles can be used as a common role to be bound inside individual namespaces, saving the need to redefine the same role in each namespace.\\n\\n**Important Details:**\\n\\n1. Regular Roles only allow access to resources within the same namespace and cannot grant access to cluster-level resources or non-resource URLs.\\n2. To allow access to resources across different namespaces, you must create a Role and RoleBinding in every namespace.\\n3. ClusterRoles can be used to allow access to cluster-level resources such as PersistentVolumes.\\n4. A ClusterRole is created using the `kubectl create clusterrole` command with the desired verb (e.g., get, list) and resource (e.g., persistentvolumes).\\n5. The YAML definition of a ClusterRole includes fields for name, resourceVersion, selfLink, and uid.\\n\\n**Key Concepts:**\\n\\n1. Namespaced resources (e.g., Roles and RoleBindings)\\n2. Cluster-level resources (e.g., ClusterRoles and ClusterRoleBindings)\\n3. Non-namespaced resources (e.g., PersistentVolumes)\\n4. Non-resource URLs (e.g., /healthz)',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The text discusses securing a Kubernetes cluster using role-based access control (RBAC) rules.\\n2. A ClusterRole named \"pv-reader\" is created to allow pods to list PersistentVolumes.\\n3. A RoleBinding named \"pv-test\" is created to bind the \"pv-reader\" ClusterRole to a ServiceAccount in the \"foo\" namespace.\\n\\n**Important Details:**\\n\\n* The default ServiceAccount cannot list PersistentVolumes, so a ClusterRole and RoleBinding are needed to grant this permission.\\n* The ClusterRole has rules that allow listing of PersistentVolumes at the cluster scope.\\n* A RoleBinding is created with the \"pv-reader\" ClusterRole and bound to the ServiceAccount in the \"foo\" namespace.\\n* Initially, the RoleBinding does not work as expected, but further investigation reveals that the issue lies in the YAML output of the RoleBinding.',\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Problem:** Unable to access Persistent Volumes (PVs) using a Service Account, despite having a Role Binding that references a Cluster Role.\\n\\n**Solution:**\\n\\n1. Delete the existing Role Binding (`kubectl delete rolebinding pv-test`).\\n2. Create a new ClusterRoleBinding instead of a Role Binding (`kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=foo:default`).\\n\\n**Key Points:**\\n\\n* To grant access to cluster-level resources, you must use a ClusterRoleBinding, not a Role Binding.\\n* The command to create a ClusterRoleBinding is similar to creating a Role Binding, but uses `clusterrolebinding` instead of `rolebinding`.\\n* You don't need to specify the namespace when creating a ClusterRoleBinding.\\n\\n**Result:** After creating the ClusterRoleBinding, you should be able to access Persistent Volumes using the Service Account.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. To secure a Kubernetes cluster with role-based access control (RBAC), you must use a `ClusterRole` and a `ClusterRoleBinding`.\\n2. Access to cluster-level resources, such as Persistent Volumes, requires a `ClusterRole` and a `ClusterRoleBinding`, not just a `RoleBinding`.\\n3. Non-resource URLs exposed by the API server also require explicit access control.\\n\\n**Important Details:**\\n\\n* A `RoleBinding` cannot grant access to cluster-level resources, even if it references a `ClusterRoleBinding`.\\n* The system automatically grants access to non-resource URLs through the `system:discovery` ClusterRole and its corresponding ClusterRoleBinding.\\n* The `system:discovery` ClusterRole defines rules for accessing non-resource URLs, such as `/api`, `/healthz`, and `/swaggerapi`.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n* The `system:discovery` ClusterRole allows access to non-resource URLs using only the HTTP GET method.\\n* This ClusterRole must be bound with a ClusterRoleBinding, not a RoleBinding.\\n\\n**Important Details:**\\n\\n* Non-resource URLs use plain HTTP verbs (e.g. POST, PUT) instead of Kubernetes resource verbs (e.g. create, update).\\n* The `system:discovery` ClusterRole is bound to all users through the `system:authenticated` and `system:unauthenticated` groups.\\n* This means that anyone can access the URLs listed in the ClusterRole.\\n* Groups are determined by the authentication plugin used by the API server.\\n* You can confirm this by accessing the `/api` URL path from inside a pod using `kubectl proxy`.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The text discusses securing a Kubernetes cluster using role-based access control (RBAC).\\n2. It shows how to use ClusterRoles and RoleBindings to grant access to cluster-level resources and non-resource URLs.\\n3. It explains how ClusterRoles can be used with namespaced RoleBindings to grant access to namespaced resources in a specific namespace.\\n\\n**Important Details:**\\n\\n* The `view` ClusterRole is introduced, which allows reading (getting, listing, watching) of namespaced resources such as ConfigMaps, Endpoints, and Pods.\\n* This ClusterRole has many rules, but only one rule is shown in the example, which applies to getting, listing, and watching resources like ConfigMaps, Endpoints, Persistent-VolumeClaims, and so on.\\n* The `view` ClusterRole does not allow writing (creating, updating, deleting) of these resources.\\n\\n**Key Takeaways:**\\n\\n* ClusterRoles can be used with namespaced RoleBindings to grant access to specific namespaces.\\n* The `view` ClusterRole is a predefined ClusterRole that allows reading of namespaced resources.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The Kubernetes API server can be bound to either a ClusterRoleBinding or a RoleBinding, which determines what resources a subject (e.g. a pod) can view.\\n* A ClusterRoleBinding allows subjects to view resources across all namespaces, while a RoleBinding only allows viewing of resources in the namespace where the binding is created.\\n\\n**Important Details:**\\n\\n* Initially, without any bindings, a test pod's ServiceAccount cannot list pods across all namespaces or in a specific namespace (foo).\\n* Creating a ClusterRoleBinding and binding it to the pod's ServiceAccount allows the pod to list pods in any namespace.\\n* The pod can now list pods not only in its own namespace (foo), but also in other namespaces (e.g. bar) by hitting the /api/v1/namespaces/{namespace}/pods URL path.\\n* Additionally, the pod can retrieve a list of all pods across all namespaces by hitting the /api/v1/pods URL path.\\n\\nOverall, this text demonstrates how to use ClusterRoleBindings and RoleBindings to control access to resources in Kubernetes.\",\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. A pod can access all pods in a cluster using a combination of a `ClusterRole` and a `ClusterRoleBinding`.\\n2. Replacing the `ClusterRoleBinding` with a `RoleBinding` limits access to resources within a specific namespace.\\n\\n**Important Details:**\\n\\n* To create a `RoleBinding`, you must specify the namespace where it will be created.\\n* A `RoleBinding` binds a service account in a namespace to a cluster role, allowing the service account to access resources in that namespace and any other namespaces.\\n* In this example, a `RoleBinding` was created in the `foo` namespace, binding the default service account with the `view` cluster role. This allows the pod to view pods in any namespace.\\n\\n**Key Concepts:**\\n\\n* `ClusterRole`: grants permission to access resources across all namespaces.\\n* `ClusterRoleBinding`: binds a service account or user to a cluster role, granting access to resources across all namespaces.\\n* `RoleBinding`: binds a service account or user to a role, granting access to resources within a specific namespace.',\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. The Kubernetes API server has different levels of access control, including role-based access control (RBAC).\\n2. There are two types of roles in RBAC: ClusterRoles and Roles.\\n3. ClusterRoles grant access to cluster-level resources, while Roles grant access to namespaced resources.\\n\\n**Important Details:**\\n\\n* A user with a ClusterRole can list pods across all namespaces using `curl localhost:8001/api/v1/pods`.\\n* However, the same user cannot list pods in other specific namespaces or across all namespaces.\\n* To access cluster-level resources, use a ClusterRole and a ClusterRoleBinding.\\n* To access namespaced resources in any namespace (and across all namespaces), use a ClusterRole and a ClusterRoleBinding.\\n* To access namespaced resources in a specific namespace, reuse the same ClusterRole in multiple namespaces using a RoleBinding.\\n* If you need to access namespaced resources in a specific namespace, define a Role for each namespace and bind it to a user or service account using a RoleBinding.\\n\\n**Key Takeaways:**\\n\\n* Use ClusterRoles and ClusterRoleBindings for cluster-level resources and namespaced resources across all namespaces.\\n* Use ClusterRoles and RoleBindings for namespaced resources in specific namespaces.\\n* Define Roles for each namespace if you need to access namespaced resources in a specific namespace.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* The text discusses securing a Kubernetes cluster using role-based access control (RBAC).\\n* It explains how to understand default ClusterRoles and ClusterRoleBindings in a Kubernetes cluster.\\n\\n**Important Details:**\\n\\n* A set of pre-configured ClusterRoles and ClusterRoleBindings are provided by Kubernetes, which are updated every time the API server starts.\\n* These default roles and bindings can be recreated if they are deleted or if a newer version of Kubernetes uses a different configuration.\\n* The text provides examples of how to list all ClusterRoleBindings and ClusterRoles using `kubectl get` commands.\\n\\n**Key Takeaways:**\\n\\n* Understanding default ClusterRoles and ClusterRoleBindings is crucial for securing a Kubernetes cluster with RBAC.\\n* These pre-configured roles and bindings can be used as a starting point or modified to fit specific use cases.',\n",
       "  \"Here's a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. The most important ClusterRoles in Kubernetes are view, edit, admin, and cluster-admin.\\n2. These roles are meant to be bound to ServiceAccounts used by user-defined pods.\\n3. Each role has specific permissions:\\n\\t* View: read-only access to most resources, except Roles, RoleBindings, and Secrets.\\n\\t* Edit: modify resources in a namespace, but not view or modify Roles or RoleBindings.\\n\\t* Admin: complete control of resources in a namespace, except ResourceQuotas and the Namespace resource itself.\\n\\t* Cluster-Admin: complete control of the Kubernetes cluster.\\n\\n**Important Details:**\\n\\n1. The view role does not allow access to Secrets because they may contain authentication tokens with greater privileges than those defined in the view role.\\n2. The edit role allows reading and modifying Secrets, but not viewing or modifying Roles or RoleBindings.\\n3. The admin role does not allow users to modify ResourceQuotas or the Namespace resource itself.\\n4. To prevent privilege escalation, the API server only allows users to create and update Roles if they already have all the permissions listed in that Role (and for the same scope).\\n5. Complete control of a namespace can be given by assigning the admin ClusterRole to a subject, but this does not allow modification of ResourceQuotas or the Namespace resource itself.\\n6. Complete control of the Kubernetes cluster can be given by assigning the cluster-admin ClusterRole to a subject.\\n7. The default ClusterRoles include many other roles that start with the system: prefix and are meant to be used by various Kubernetes components.\",\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. The Controller Manager runs as a single pod, but each controller inside it can use a separate ClusterRole and ClusterRoleBinding (prefixed with \"system: controller:\").\\n2. ServiceAccounts in a namespace have no permissions by default, except for making GET requests on certain non-resource URLs.\\n3. It\\'s essential to grant ServiceAccounts only the necessary permissions to perform their tasks, following the principle of least privilege.\\n\\n**Important Details:**\\n\\n* Creating specific ServiceAccounts for each pod and associating them with tailored Roles or ClusterRoles through RoleBindings is a good practice.\\n* If multiple pods have different requirements, create separate ServiceAccounts and specify the serviceAccountName property in the pod spec to use the correct account.\\n* Expecting apps to be compromised and constraining ServiceAccounts to prevent damage from unauthorized access is crucial for reducing the risk of an intruder gaining control of the cluster.\\n\\n**Key Takeaways:**\\n\\n* Securely managing permissions for ServiceAccounts is critical for protecting the Kubernetes API server.\\n* Following best practices, such as creating specific ServiceAccounts and granting only necessary permissions, can help reduce the risk of unauthorized access.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. By default, Kubernetes pods run under a ServiceAccount created for each namespace automatically.\\n2. Additional ServiceAccounts can be created manually and associated with a pod.\\n3. ServiceAccounts can be configured to control access to Secrets in a pod.\\n\\n**Important Details:**\\n\\n* Roles and ClusterRoles define what actions can be performed on which resources.\\n* RoleBindings and ClusterRoleBindings bind these roles to users, groups, and ServiceAccounts.\\n* Each cluster comes with default ClusterRoles and ClusterRoleBindings.\\n\\nOverall, this text discusses the basics of securing the Kubernetes API server through the use of ServiceAccounts, Roles, and ClusterRoles.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* Securing cluster nodes and networks is crucial, as an attacker with access to the API server could potentially run malicious code.\\n* Containers are not always isolated from each other or the node they're running on.\\n* This chapter covers how to:\\n\\t+ Allow pods to access node resources\\n\\t+ Configure the cluster to limit user actions\\n\\t+ Implement security measures such as:\\n\\t\\t- Using Linux namespaces in pods\\n\\t\\t- Running containers with different users\\n\\t\\t- Running privileged containers\\n\\t\\t- Adding or dropping kernel capabilities for containers\\n\\t\\t- Defining security policies for pods\\n\\t\\t- Securing the pod network\\n\\n**Important Details:**\\n\\n* The chapter aims to educate readers on how to secure cluster nodes and networks, rather than just securing the API server.\\n* The importance of container isolation is highlighted, as it's not always guaranteed.\",\n",
       "  \"Here is a summary of the main points and important details from this chunk of text:\\n\\n**Main Points:**\\n\\n* Securing cluster nodes and network communication between pods\\n* Understanding how containers in a pod use separate Linux namespaces to isolate processes\\n* Using the host node's namespaces in a pod for certain system pods that need to access node-level resources\\n\\n**Important Details:**\\n\\n* Pods usually run under their own Linux namespaces, isolating them from other containers or the node's default namespaces\\n* Each pod gets its own IP and port space, process tree, and IPC namespace\\n* Certain system pods can use the host node's network namespace by setting `hostNetwork` to `true` in the pod spec\\n* When using the host node's network namespace, the pod uses the node's network interfaces instead of its own virtual network adapters\\n* The pod does not get its own IP address and processes that bind to ports are bound to the node's port.\",\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Pods can use the host node's namespaces by setting `hostNetwork: true` in the pod spec.\\n2. This allows the pod to see all the host's network adapters, among other things.\\n3. The Kubernetes Control Plane components (e.g., when deploying with kubeadm) also use this feature.\\n\\n**Important Details:**\\n\\n1. When a pod uses the host node's namespace, it can bind to any port on the host without using the `hostPort` property.\\n2. Pods that use `hostPort` are different from pods exposed through a NodePort service:\\n\\t* With `hostPort`, connections to the node's port are forwarded directly to the pod running on that node.\\n\\t* With NodePort, connections to the node's port are forwarded to a randomly selected pod (possibly on another node).\\n3. Pods using `hostPort` bind the node's port only on nodes that run such pods, whereas NodePort services bind the port on all nodes.\\n\\n**Key Concepts:**\\n\\n1. `hostNetwork`: allows a pod to use the host node's namespace.\\n2. `hostPort`: allows a pod to bind to a port in the node's default namespace without using the host's network namespace.\\n3. NodePort service: exposes a port on all nodes, even if no pods are running on that node.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* When using a specific host port in a pod, only one instance of the pod can be scheduled to each node because multiple processes cannot bind to the same host port.\\n* The Scheduler takes this into account and will not schedule multiple pods to the same node.\\n\\n**Important Details:**\\n\\n* If you have three nodes and want to deploy four pod replicas, only three will be scheduled (one pod will remain Pending).\\n* This is in contrast to using a NodePort service, where multiple pods can be scheduled behind a single port.\\n* Using host ports limits the number of pod instances that can be scheduled to each node.\\n\\nOverall, this text highlights an important consideration when deploying pods with specific host ports: it can limit the scalability and flexibility of your deployment.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. You can bind a pod to a specific port on the host node using the `hostPort` feature in Kubernetes.\\n2. This allows you to access the pod through that port, but only from within the same node.\\n3. The `hostPID` and `hostIPC` features allow a pod's containers to use the node's PID and IPC namespaces, respectively.\\n\\n**Important Details:**\\n\\n* To use `hostPort`, you need to specify the host port in the pod's YAML definition, along with the container port.\\n* If you're using Google Kubernetes Engine (GKE), you'll need to configure the firewall properly to allow access to the pod through the specified port.\\n* The `hostPID` and `hostIPC` features are primarily used for exposing system services deployed by DaemonSets.\\n* These features can also be used to ensure that multiple replicas of the same pod are not scheduled on the same node, but a better way to achieve this is described in Chapter 16.\",\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Pods can see all processes running on the host node, not just their own, by default.\\n2. The `hostIPC` property can be set to `true` to allow containers in a pod to communicate with other processes running on the node through Inter-Process Communication (IPC).\\n3. The security context of a container can be configured using the `securityContext` properties.\\n\\n**Important Details:**\\n\\n* Configuring the security context allows you to:\\n\\t+ Specify the user under which the process will run\\n\\t+ Prevent containers from running as root\\n\\t+ Run containers in privileged mode (giving full access to the node's kernel)\\n\\t+ Configure fine-grained privileges by adding or dropping capabilities\\n\\t+ Set SELinux options to strongly lock down a container\\n\\t+ Prevent processes from writing to the container's filesystem\\n\\n**Key Concepts:**\\n\\n* `hostIPC`: allows containers to communicate with other processes running on the node through IPC\\n* `securityContext`: properties that can be used to configure the security context of a container\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When running a pod without specifying a security context, it defaults to running as root (user ID 0) and belongs to multiple groups.\\n2. To run a container as a specific user, you need to set the `securityContext.runAsUser` property in the pod's configuration.\\n3. The `runAsUser` property requires a user ID, not a username.\\n\\n**Important Details:**\\n\\n* In a Dockerfile, the `USER` directive specifies the user that the container runs as. If omitted, it defaults to root (user ID 0).\\n* The example uses the `alpine` container image and sets the `runAsUser` property to 405, which corresponds to the guest user.\\n* Running the `id` command inside a pod shows its user ID, group ID, and groups membership.\",\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Preventing Containers from Running as Root**\\n\\n* Even if a container runs as a specific user, an attacker can still push a malicious image with root privileges to the registry.\\n* To prevent this, you can specify that the pod\\'s container needs to run as a non-root user using `runAsNonRoot: true` in the pod configuration.\\n\\n**Example Configuration**\\n\\n* The example pod configuration is shown below:\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-run-as-non-root\\nspec:\\n  containers:\\n  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:\\n      runAsNonRoot: true\\n```\\n* If you deploy this pod, it will not be allowed to run if the container image is configured to run as root.\\n\\n**Running Pods in Privileged Mode**\\n\\n* In some cases, pods need to access protected system devices or kernel features that are not accessible to regular containers.\\n* This can be achieved by running the pod in privileged mode, but this should be done with caution and only when necessary.',\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. A pod's security context can be configured to run containers in privileged mode.\\n2. Running a container in privileged mode allows it to access the node's kernel and modify its configuration.\\n\\n**Important Details:**\\n\\n* The `kube-proxy` pod, which is responsible for modifying iptables rules on each cluster node, runs in privileged mode.\\n* To create a privileged pod, set the `privileged` property to `true` in the container's security context.\\n* A non-privileged container can only see a limited list of devices in the `/dev` directory, whereas a privileged container can see all available devices.\\n\\n**Key Takeaways:**\\n\\n* Running containers in privileged mode provides full access to the node's kernel and configuration.\\n* Privileged containers can access device files that are not visible to non-privileged containers.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. In Kubernetes, containers can be given access to specific kernel features through kernel capabilities.\\n2. This approach is safer than making a container privileged, which gives it unlimited permissions.\\n3. Kubernetes allows you to add or drop individual kernel capabilities for each container.\\n\\n**Important Details:**\\n\\n* The device list in the `/proc/devices` file has been expanded and now includes all host node devices, allowing containers to access any device freely.\\n* To give a container specific kernel features without making it privileged, you can use kernel capabilities such as CAP_SYS_TIME, which allows changing the system time.\\n* You can add or drop individual kernel capabilities for each container using Kubernetes configuration files (e.g., `pod-add-settime-capability.yaml`).\\n* The example shows how to add the CAP_SYS_TIME capability to a container, allowing it to change the system time.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Configuring Container Security Context**\\n\\nThe text discusses how to configure a container's security context in Kubernetes. The main points are:\\n\\n1. **Adding capabilities**: You can add Linux kernel capabilities to a container using the `securityContext` property. In this example, the `SYS_TIME` capability is added.\\n2. **Dropping capabilities**: You can also drop capabilities that may be available to the container by default. For example, the `CAP_CHOWN` capability allows processes to change file ownership.\\n3. **Warning**: Adding capabilities like `SYS_TIME` can cause issues with your worker node, such as making it unusable. In Minikube, you may need to reboot the VM to schedule new pods.\\n\\nImportant details:\\n\\n* When specifying Linux kernel capabilities in a pod spec, you must leave out the prefix (`CAP_`).\\n* You can confirm that the node's time has been changed by checking the time on the node running the pod.\\n* Dropping capabilities is a better approach than giving a container full privileges with `privileged: true`.\\n* The list of Linux kernel capabilities can be found in the Linux man pages.\",\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. To prevent containers from modifying certain capabilities, you can drop them by listing them under `securityContext.capabilities.drop` in a Kubernetes Pod configuration file (e.g., YAML).\\n2. By dropping the CHOWN capability, processes running in the container cannot change the owner of files or directories.\\n3. You can also prevent processes from writing to the container's filesystem and only allow writes to mounted volumes by setting `securityContext.readOnlyRootFilesystem` to true.\\n\\n**Important Details:**\\n\\n* Dropping capabilities prevents containers from performing certain actions, such as changing file ownership (CHOWN) or modifying the container's filesystem.\\n* This can help prevent security vulnerabilities, such as allowing attackers to modify executable code in a PHP application.\\n* The `securityContext.readOnlyRootFilesystem` property must be set to true to prevent processes from writing to the container's filesystem.\",\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Configuring Container Security Context**\\n\\n* The `securityContext` property can be set at the container or pod level.\\n* Setting `readOnlyRootFilesystem: true` makes the container run as root but prevents it from writing to the filesystem, except for mounted volumes.\\n\\n**Key Points**\\n\\n* When a container's filesystem is read-only, writing to the `/` directory fails, but writing to a mounted volume is allowed.\\n* To increase security, set `readOnlyRootFilesystem: true` at the pod level when running pods in production.\\n* The pod-level security context can be used to set default values for all containers within the pod, which can be overridden at the container level.\\n\\n**Sharing Volumes between Containers**\\n\\n* When containers run as different users (e.g., using `runAsUser`), they may not have access to files written by other containers in a shared volume.\\n* This is because each container runs with its own user ID and permissions, which can limit access to the shared volume.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Kubernetes allows you to specify supplemental groups for pods running in containers, enabling them to share files regardless of their user IDs.\\n* Two properties are used to achieve this: `fsGroup` and `supplementalGroups`.\\n* These properties can be specified at the pod level.\\n\\n**Important Details:**\\n\\n* The example provided shows a pod with two containers sharing the same volume using `fsGroup` and `supplementalGroups`.\\n* In the example, the first container runs as user ID 1111, while the second container runs as user ID 2222.\\n* Despite running under different user IDs, both containers can access files in the shared volume due to the specified supplemental groups (555, 666, and 777).\\n* The `fsGroup` property is used to specify a single group ID that all containers in the pod will belong to, while `supplementalGroups` allows you to specify additional group IDs for each container.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Restricting Security-Related Features in Pods**\\n\\nThe text discusses how to restrict users from using certain security-related features in pods, such as running privileged containers or modifying file ownership.\\n\\n**Key Points:**\\n\\n1. **fsGroup**: When a pod is created with `fsGroup` set to a specific ID (e.g., 555), the mounted volume will be owned by that group ID.\\n2. **Supplemental Groups**: The `supplementalGroups` property defines additional group IDs associated with a user, which can affect file ownership when creating files in a volume.\\n3. **PodSecurityPolicy**: A cluster-level resource that defines what security-related features users can or cannot use in their pods.\\n\\n**Important Details:**\\n\\n1. When creating files in a mounted volume, the `fsGroup` ID is used for ownership, whereas the user's effective group ID (e.g., 0) is used when creating files in the container's filesystem.\\n2. The cluster administrator can restrict users from using certain security-related features by creating one or more PodSecurityPolicy resources.\\n\\n**Next Steps:**\\n\\nThe text concludes this section and sets up the next topic, which will discuss how a cluster administrator can restrict users from using these security-related features.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. The PodSecurityPolicy admission control plugin validates pod definitions against configured policies before accepting them into the cluster.\\n2. A PodSecurityPolicy resource defines rules for pods, such as:\\n\\t* Access to host namespaces (IPC, PID, Network)\\n\\t* Host ports that can be bound\\n\\t* User IDs that containers can run as\\n\\t* Creation of pods with privileged containers\\n3. To enable RBAC and PodSecurityPolicy admission control in Minikube, you need to use a specific command to start the API server.\\n4. The command requires creating a password file for basic authentication.\\n\\n**Important Details:**\\n\\n1. The PodSecurityPolicy admission control plugin may not be enabled by default in your cluster.\\n2. You need to ensure that the plugin is enabled before running examples, or refer to the next sidebar if using Minikube.\\n3. To run Minikube with all required plugins enabled, you need to use a specific command and create a password file for basic authentication.\\n4. The API server will not start until the password file is created.\\n\\n**Key Takeaways:**\\n\\n* PodSecurityPolicy admission control plugin validates pod definitions against configured policies\\n* A PodSecurityPolicy resource defines rules for pods\\n* To enable RBAC and PodSecurityPolicy admission control in Minikube, use a specific command to start the API server.',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. PodSecurityPolicy (PSP) restricts the use of security-related features in pods, including kernel capabilities, SELinux labels, writable root filesystems, and volume types.\\n2. A sample PSP example is provided, which:\\n\\t* Prevents pods from using host IPC, PID, and network namespaces\\n\\t* Disallows running privileged containers and most host ports (except 10000-11000 and 13000-14000)\\n\\t* Allows containers to run with a read-only root filesystem\\n\\t* Enables containers to run as any user, group, or SELinux group\\n\\t* Permits all volume types in pods\\n\\n**Important Details:**\\n\\n1. The PSP example sets specific constraints on host ports that can be used by containers.\\n2. Containers are not allowed to use the host's IPC, PID, or network namespace.\\n3. Containers cannot run in privileged mode and must use a read-only root filesystem.\\n\\nOverall, this text provides an overview of PodSecurityPolicy and its role in restricting security-related features in pods, as well as an example PSP configuration that demonstrates how to set specific constraints on container behavior.\",\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. When a PodSecurityPolicy is applied to a cluster, it prevents privileged pods from being deployed.\\n2. The policy also makes container filesystems read-only in all pods.\\n3. The policy can be configured to constrain which users and groups containers can run as.\\n\\n**Important Details:**\\n\\n* To allow containers to run as specific user or group IDs, the `MustRunAs` rule must be used instead of `RunAsAny`.\\n* The `MustRunAs` rule requires specifying a range of allowed IDs using the `min` and `max` fields.\\n* Multiple ranges can be specified for each field (e.g. `runAsUser`, `fsGroup`, `supplementalGroups`) to allow multiple IDs.\\n* To set a single specific ID, add a range with `min` equal to `max`.\\n* The policy in the example sets the default filesystem group and supplemental group IDs to be anything from 2-10 or 20-30 (inclusive).',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Restricting Security Features in Pods**\\n\\n* The API server enforces PodSecurityPolicies (PSPs) when creating or updating pods, but not on existing pods.\\n* If a pod spec tries to set `runAsUser` outside of the allowed range specified in a PSP, the pod will be rejected.\\n\\n**Deploying a Pod with an Out-of-Range User ID**\\n\\n* Trying to deploy a pod with a user ID outside of the allowed range (e.g., 405) results in a \"Forbidden\" error.\\n* However, if a container image has a hardcoded user ID that is within the allowed range (e.g., 5), the API server will accept the pod.\\n\\n**Important Details**\\n\\n* The `USER` directive in a Dockerfile can be used to set the user ID of a container.\\n* PodSecurityPolicies can override the user ID hardcoded into a container image.\\n* Existing pods are not affected by changes to PSPs, as they are only enforced when creating or updating pods.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Securing Cluster Nodes and Network**\\n\\nThe chapter discusses two key aspects of securing cluster nodes and networks: preventing root access in containers and configuring Linux kernel capabilities.\\n\\n**MustRunAsNonRoot Rule**\\n\\n* The `MustRunAsNonRoot` rule prevents users from deploying containers that run as root.\\n* Containers must specify a non-zero user ID or the container image itself must run as a non-zero user ID.\\n\\n**Configuring Capabilities**\\n\\n* Three fields influence which capabilities containers can use:\\n\\t+ `allowedCapabilities`: specifies which capabilities pod authors can add to containers.\\n\\t+ `defaultAddCapabilities`: automatically adds specified capabilities to every container.\\n\\t+ `requiredDropCapabilities`: requires containers to drop specified capabilities.\\n* An example is provided, showing how to specify capabilities in a PodSecurityPolicy resource.\\n\\n**Important Details**\\n\\n* The `SYS_ADMIN` capability allows administrative operations and loading/unloading of Linux kernel modules.\\n* The `SYS_MODULE` capability allows loading and unloading of Linux kernel modules.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Restricting Security Features in Pods**\\n\\n* The `defaultAddCapabilities` field allows adding capabilities to all containers in a pod, unless explicitly dropped.\\n* If a capability is added using `defaultAddCapabilities`, it must be explicitly dropped for specific containers if not wanted.\\n\\n**Dropping Capabilities from Containers**\\n\\n* The `requiredDropCapabilities` field automatically drops specified capabilities from every container.\\n* Attempting to add a capability listed in `requiredDropCapabilities` will result in pod creation being rejected.\\n\\n**Constraining Volume Types**\\n\\n* A PodSecurityPolicy (PSP) can define which volume types users can add to their pods.\\n* At minimum, a PSP should allow using emptyDir, configMap, secret, downwardAPI, and persistentVolumeClaim volumes.\\n* If multiple PSPs are in place, pods can use any volume type defined in any of the policies.\\n\\nKey details:\\n\\n* The `defaultAddCapabilities` field allows adding capabilities to all containers by default.\\n* The `requiredDropCapabilities` field automatically drops specified capabilities from every container.\\n* A PSP should allow using at least emptyDir, configMap, secret, downwardAPI, and persistentVolumeClaim volumes.',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. PodSecurityPolicies (PSPs) are cluster-level resources that cannot be stored in or applied to a specific namespace.\\n2. PSPs can be assigned to different users through the Role-Based Access Control (RBAC) mechanism, allowing for fine-grained access control.\\n\\n**Important Details:**\\n\\n* A PSP is created and made available to individual users or groups by creating ClusterRole resources that point to the individual policies by name.\\n* When a pod definition is submitted, the Pod-SecurityPolicy Admission Control plugin only considers the policies accessible to the user creating the pod.\\n* The text describes how to create an additional PSP allowing privileged containers to be deployed.\\n\\n**Key Concepts:**\\n\\n* PodSecurityPolicies (PSPs)\\n* Role-Based Access Control (RBAC)\\n* ClusterRole resources\\n* ClusterRoleBindings',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Restricting Security-Related Features in Pods**\\n\\n* The default PodSecurityPolicy (PSP) does not allow running privileged containers, while the privileged PSP allows it.\\n* To restrict access to security-related features, you can create separate policies for different users.\\n\\n**Using RBAC to Assign Different Policies to Users**\\n\\n* Create two ClusterRoles: `psp-default` and `psp-privileged`, each allowing use of a specific PSP (default and privileged).\\n* Bind the `psp-default` role to all authenticated users using a ClusterRoleBinding, so that anyone can create pods without a policy.\\n* Bind the `psp-privileged` role to individual users who need access to privileged containers.\\n\\n**Key Details**\\n\\n* Use the `use` verb instead of `get`, `list`, or `watch` when referencing a specific instance of a PodSecurityPolicy resource.\\n* Create ClusterRoleBindings for cluster-level resources like PSPs, rather than RoleBindings for namespaced resources.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. A cluster role binding was created to give user \"Bob\" access to both the default and privileged PodSecurityPolicies.\\n2. User \"Alice\" should have access to the default PodSecurityPolicy, but not be able to create privileged pods.\\n3. The commands to authenticate as Alice or Bob using kubectl\\'s config were shown.\\n\\n**Important Details:**\\n\\n* A new cluster role binding was created with `kubectl create clusterrolebinding psp-bob` to give Bob access to both the default and privileged PodSecurityPolicies.\\n* Two new users, \"alice\" and \"bob\", were created in kubectl\\'s config using `kubectl config set-credentials`.\\n* The `--user` option can be used with `kubectl` to specify which user credentials to use for authentication.\\n* When authenticating as Alice, creating a privileged pod is forbidden due to the default PodSecurityPolicy.\\n* When authenticating as Bob, creating a privileged pod is allowed because of his access to both the default and privileged PodSecurityPolicies.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The chapter discusses securing the network between pods by limiting which pods can talk to each other.\\n2. This is configurable depending on the container networking plugin used in the cluster.\\n3. Network isolation can be achieved through NetworkPolicy resources.\\n\\n**Important Details:**\\n\\n* A NetworkPolicy applies to pods that match its label selector and specifies either which sources can access the matched pods (ingress rules) or which destinations can be accessed from the matched pods (egress rules).\\n* Ingress and egress rules can match only pods that match a pod selector, all pods in a namespace whose labels match a namespace selector, or a network IP block specified using CIDR notation.\\n* By default, pods in a given namespace can be accessed by anyone. To change this, a default-deny NetworkPolicy must be created to prevent all clients from connecting to any pod in the namespace.\\n\\n**Key Concepts:**\\n\\n* NetworkPolicy resources\\n* Ingress and egress rules\\n* Pod selector, namespace selector, and CIDR notation for matching pods or networks\\n\\nOverall, the chapter aims to provide guidance on securing the network between pods by using NetworkPolicy resources to limit access between pods.',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. To secure cluster nodes and network connectivity, the CNI plugin or networking solution used must support NetworkPolicy.\\n2. A NetworkPolicy can be created to control which pods in a namespace can connect to specific server pods.\\n\\n**Important Details:**\\n\\n* The example provided creates a NetworkPolicy that allows only pods with the label `app=webserver` to connect to pods with the label `app=database` on port 5432.\\n* Other pods in the same namespace cannot connect to the database pod, and no one can connect to any other ports on the database pod.\\n* The policy is enforced even when connecting through a Service, rather than directly to the pod.\\n* A NetworkPolicy resource must be created in the same namespace as the server pod (in this case, `postgres-netpolicy`) to secure access.\\n\\n**Key Takeaways:**\\n\\n* NetworkPolicies can be used to control inter-pod connectivity and secure cluster nodes and networks.\\n* By creating a NetworkPolicy, you can specify which pods are allowed to connect to specific server pods on specific ports.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* A company (Manning) has multiple tenants using the same Kubernetes cluster, each with their own namespaces labeled with \"tenant: manning\".\\n* One of these tenants wants to run a Shopping Cart microservice that needs to be accessible from all pods in their namespaces.\\n* To secure this microservice, a NetworkPolicy is created to prevent other tenants from accessing it.\\n\\n**Important Details:**\\n\\n* The NetworkPolicy (named \"shoppingcart-netpolicy\") applies to pods labeled as \"microservice= shopping-cart\".\\n* Only pods running in namespaces labeled as \"tenant=manning\" are allowed to access the Shopping Cart microservice.\\n* The policy allows ingress traffic on port 80 from these authorized pods.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* A NetworkPolicy can be used to control access to pods in a Kubernetes cluster, ensuring that only authorized pods can communicate with each other.\\n* In a multi-tenant cluster, tenants cannot add labels or annotations to their namespaces themselves, so NetworkPolicies must be managed by administrators.\\n\\n**Important Details:**\\n\\n* To allow access to a pod, a NetworkPolicy can specify a namespace selector (e.g. `tenant=manning`) or an IP block in CIDR notation (e.g. `192.168.1.0/24`).\\n* If multiple tenants need access to the same pod, additional NetworkPolicies or ingress rules can be created.\\n* Using a namespace selector allows administrators to control access based on labels or annotations added by the cluster administrator, while using an IP block in CIDR notation allows for more fine-grained control over which IP addresses can access the pod.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. NetworkPolicies can be used to limit outbound traffic from pods, in addition to inbound traffic.\\n2. Egress rules can be used to specify which pods or IP addresses a pod can connect to when sending data out of the cluster.\\n3. A NetworkPolicy example is provided that limits webserver pods to only connecting to database pods.\\n\\n**Important Details:**\\n\\n* The NetworkPolicy in the example allows webserver pods to access database pods, but not any other pods or external IP addresses.\\n* This policy can be used to secure a cluster by limiting communication between pods and preventing unauthorized data transmission.\\n* This is part of a larger discussion on securing clusters and pods, including using PodSecurityPolicies and NetworkPolicies to control pod behavior.',\n",
       "  \"Here is a summary of the text:\\n\\n**Managing Pods' Resources**\\n\\nThis chapter focuses on managing the computational resources (CPU and memory) allocated to Kubernetes pods. Key points include:\\n\\n* **Resource allocation**: Setting expected and maximum resource consumption for pods to ensure fair sharing of cluster resources.\\n* **Resource types covered**: Requesting CPU, memory, and other computational resources for containers.\\n* **Quality of Service guarantees**: Understanding the guarantees provided by setting resource limits for pods.\\n* **Namespace-level resource management**: Setting default, minimum, and maximum resources for pods in a namespace, as well as limiting the total amount of resources available in a namespace.\\n\\nThese concepts are crucial for ensuring efficient scheduling and utilization of cluster resources.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Requesting Resources for Pod Containers**\\n\\nWhen creating a pod, you can specify the resources (CPU and memory) needed by each container. These are called \"requests\" and are specified individually for each container, not for the pod as a whole.\\n\\n**Key Points:**\\n\\n1. **Requests**: Specify the amount of CPU and memory required by each container.\\n2. **Limits**: Set a hard limit on what each container may consume (CPU and memory).\\n3. **Pod Resource Requests and Limits**: The sum of the requests and limits of all containers in the pod.\\n\\n**Example:**\\n\\n* A pod manifest with resource requests specified for its single container:\\n\\t+ CPU request: 200m (one-fifth of a CPU core)\\n\\t+ Memory request: 10Mi (10 mebibytes)\\n\\n**Important Details:**\\n\\n1. If no CPU request is specified, the process may not get any CPU time at all.\\n2. Specifying memory requests helps prevent containers from consuming excessive RAM.\\n\\nOverall, specifying resource requests for pod containers allows you to control the resources used by each container and ensure that they do not consume excessive resources.',\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Running a `dd` command in a container consumes as much CPU as possible, but only uses one core due to being single-threaded.\\n2. The Minikube VM has two CPU cores, so the process is shown consuming 50% of the whole CPU, exceeding the requested 200 millicores.\\n3. Resource requests in a pod specify the minimum amount of resources needed and are used by the Scheduler when scheduling the pod to a node.\\n4. If a node's unallocated resources (CPU or memory) are less than what a pod requests, Kubernetes will not schedule the pod to that node.\\n\\n**Important Details:**\\n\\n1. The Scheduler doesn't consider actual resource consumption at the time of scheduling, but rather the sum of requested resources by existing pods on the node.\\n2. This ensures that guarantees made to already deployed pods are maintained, even if they're currently using less than their requested resources.\\n3. Resource limits can be specified to prevent containers from consuming more resources than requested, which will be explored later in the chapter.\\n\\n**Key Concepts:**\\n\\n1. Resource requests and limits\\n2. Pod scheduling by the Scheduler\\n3. Guarantees made to already deployed pods\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* When scheduling pods on nodes, the Kubernetes Scheduler considers the amount of resources requested by each pod, not the actual resources consumed.\\n* Two prioritization functions are used to rank nodes based on their available resources: LeastRequestedPriority (prefers nodes with fewer requested resources) and MostRequestedPriority (prefers nodes with more requested resources).\\n* The Scheduler is configured to use only one of these functions.\\n\\n**Important Details:**\\n\\n* Using the Most-Requested Priority function can save money by keeping pods tightly packed, allowing for nodes to be removed when they are not in use.\\n* The Kubelet reports a node's capacity (CPU and memory) to the API server, making it available through the Kubernetes API.\\n* The Scheduler only considers resource requests, not actual usage, when scheduling pods on nodes.\\n\\n**Key Takeaways:**\\n\\n* Understanding how the Scheduler uses pod requests is crucial for efficient resource allocation in Kubernetes.\\n* Configuring the Scheduler to use the Most-Requested Priority function can lead to cost savings by minimizing the number of nodes used.\",\n",
       "  \"Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. The `kubectl describe nodes` command shows the available resources on a node, including its capacity and allocatable resources.\\n2. A node's capacity represents its total resources, which may not all be available to pods due to reserved resources for Kubernetes and system components.\\n3. The Scheduler bases its decisions only on the allocatable resource amounts.\\n\\n**Important Details:**\\n\\n1. In the example, a node called `minikube` has 2 CPU cores with no reserved resources, making the entire CPU allocatable to pods.\\n2. A pod was created with a resource request of 800 millicores (0.8 CPU cores), which was successfully scheduled and running.\\n3. The total requested resources by two deployed pods is 1,000 millicores or exactly 1 core, leaving another 1,000 millicores available for additional pods.\\n4. However, attempting to deploy a third pod with a resource request of 1,000 millicores (1 CPU core) fails because there are no more allocatable resources available.\\n\\nOverall, the text highlights the importance of understanding a node's capacity and allocatable resources when managing pods' computational resources in Kubernetes.\",\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n* A pod was created with a CPU request of 1 whole core (instead of millicores) and was accepted by the API server.\\n* However, the pod remains in a \"Pending\" state and has not been scheduled on any node.\\n* The reason for this is that the node does not have sufficient CPU resources to accommodate the pod\\'s request.\\n\\n**Important Details:**\\n\\n* The total CPU requests of all three pods (including the new one) equals 2 cores, which matches the available CPU capacity of the single node.\\n* However, when inspecting the node\\'s resource usage with `kubectl describe node`, it is revealed that the pod\\'s request conflicts with another pod\\'s allocation, resulting in insufficient CPU resources.\\n* The solution to this issue requires further investigation and optimization of the node\\'s resource allocation.',\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Pod Resource Management**\\n\\n* The default pod has requested 800m CPU (40%) and 20Mi memory (1%).\\n* Other pods in the kube-system namespace have varying resource requests:\\n\\t+ dflt-http-b...: 10m CPU, 20Mi memory\\n\\t+ kube-addon-...: 5m CPU, 50Mi memory\\n\\t+ kube-dns-26...: 260m CPU, 170Mi memory\\n\\t+ kubernetes-... and nginx-ingre... have no resource requests\\n* The total allocated resources are:\\n\\t+ CPU Requests: 1275m (63%)\\n\\t+ Memory Requests: 210Mi (11%)\\n\\n**Resource Overcommitment**\\n\\n* The Scheduler will not schedule a new pod if it would make the node overcommitted.\\n* In this case, a third pod requested 1,000 millicores, but only 725 millicores were available.\\n* The Scheduler won't schedule the third pod until an equivalent amount of CPU is freed.\\n\\n**Freeing Resources**\\n\\n* To free resources and get the pod scheduled, one of the first two pods needs to be deleted.\\n* When a pod is deleted, the Scheduler will be notified and reschedule the third pod.\\n* This process is demonstrated in the example code listings.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Kubernetes distinguishes between resource requests and limits, where requests determine how much CPU time is allocated to a pod.\\n* When multiple pods request different amounts of CPU time, the unused CPU time is distributed among them based on their requests.\\n* If one container uses up all available CPU time while another sits idle, the first container will be allowed to use the whole CPU time until the second container needs it.\\n\\n**Important Details:**\\n\\n* The example describes two pods with different CPU requests: 200 millicores and 1,000 millicores. The unused CPU time is distributed in a 1:5 ratio between them.\\n* If both containers consume all available CPU time, the first pod will get 16.7% of the CPU time (1/6) and the second pod will get 83.3% (5/6).\\n* Kubernetes allows custom resources to be added to a node and requested in a pod's resource requests.\\n* Custom resources were initially known as Opaque Integer Resources, but are now called Extended Resources.\\n\\n**Key Concepts:**\\n\\n* Resource requests vs. limits\\n* CPU time sharing among multiple pods with different requests\\n* Distribution of unused CPU time based on request ratios\",\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Managing Pods' Computational Resources**\\n\\n* To manage custom resources, add them to the Node object's capacity field using a PATCH HTTP request. The resource name can be anything except kubernetes.io domain, and the quantity must be an integer.\\n* When creating pods, specify the same resource name and requested quantity under the resources.requests field in the container spec or with --requests when using kubectl run.\\n\\n**Custom Resources**\\n\\n* An example of a custom resource is the number of GPU units available on a node. Pods requiring a GPU will specify it in their requests.\\n* The Scheduler ensures that pods are only deployed to nodes with sufficient custom resources available.\\n\\n**Limiting Resources Available to a Container**\\n\\n* Setting resource requests for containers ensures each container gets the minimum amount of resources it needs.\\n* To prevent containers from using up too many resources, Kubernetes allows you to specify resource limits for every container.\\n* CPU is a compressible resource that can be throttled without affecting the process running in the container. Memory, on the other hand, is an incompressible resource that cannot be taken away once allocated.\\n\\n**Creating a Pod with Resource Limits**\\n\\n* To prevent containers from consuming too much memory, specify resource limits for every container.\\n* An example pod manifest with resource limits is provided, showing how to set CPU and memory limits for each container.\",\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Resource limits for containers in Kubernetes can be set to prevent them from consuming too many resources (CPU and memory).\\n2. The `limits` field in a Pod's specification sets the maximum amount of CPU and memory that a container can use.\\n3. In this example, the container is limited to 1 CPU core and 20Mi of memory.\\n\\n**Important Details:**\\n\\n* Resource limits are not constrained by the node's allocatable resource amounts, meaning they can be overcommitted (i.e., exceed 100% of the node's capacity).\\n* When a node runs out of resources, certain containers may need to be killed.\\n* Individual containers can be killed even if they try to use more than their specified resource limits.\\n\\n**Key Concepts:**\\n\\n* Resource requests vs. resource limits\\n* Overcommitting and its consequences\\n* Kubernetes' decision-making process for killing containers when resources are exhausted\",\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. When a process in a container tries to use more resources than allowed, Kubernetes will throttle or limit its usage.\\n2. For CPU usage, the process will not be given more CPU time than configured, but it won't be killed.\\n3. For memory usage, if a process tries to allocate more memory than its limit, it will be killed (OOMKilled) and restarted according to the pod's restart policy.\\n\\n**Important Details:**\\n\\n1. If a container is OOMKilled, Kubernetes will increase the delay between restarts after each crash.\\n2. The delay starts at 10 seconds and increases exponentially up to 300 seconds.\\n3. Once the interval hits the 300-second limit, the Kubelet will keep restarting the container indefinitely every 5 minutes until the pod stops crashing or is deleted.\\n4. To investigate why a container crashed, you can check its log using `kubectl describe pod` command.\\n\\n**Key Terms:**\\n\\n* OOMKilled (Out Of Memory Killed)\\n* CrashLoopBackOff\\n* Restart policy (Always or OnFailure)\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Containers can be killed due to out-of-memory (OOM) issues, even if they are within their allocated limits.\\n2. Setting memory limits too low can cause containers to be killed immediately.\\n3. Containers do not see their own memory limits, but rather the total available memory on the node.\\n\\n**Important Details:**\\n\\n* The `top` command in a container shows the memory usage of the entire node, not just the container's allocated limit.\\n* A container will not be aware of its own memory limit and may use more memory than allocated if it is not properly configured.\\n* This behavior can lead to unexpected OOM issues and container kills.\\n\\n**Key Takeaways:**\\n\\n* Containers should be designed with proper resource management in mind to avoid OOM issues.\\n* Understanding how containers see limits (i.e., the node's total available resources) is crucial for effective resource allocation.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Containers in Kubernetes can see all the node's memory and CPU cores, regardless of configured limits.\\n2. This can cause issues for applications that rely on system memory or CPU counts to make decisions.\\n3. Java apps are particularly affected by this issue if not properly configured.\\n\\n**Important Details:**\\n\\n* When running Java apps without specifying a maximum heap size (-Xmx option), the JVM may exceed container memory limits and be OOMKilled (Out of Memory Killed).\\n* Even with -Xmx set, Java's off-heap memory is not constrained by container limits.\\n* Newer versions of Java take into account configured container limits.\\n* Containers see all CPU cores on a node, regardless of configured CPU limits. This can lead to issues for apps that rely on system CPU counts to decide how many worker threads to run.\\n* Using the Downward API or tapping into cgroups directly can help pass the correct CPU limit to containers and avoid these issues.\\n\\n**Key Takeaways:**\\n\\n* Be aware of how containers interact with system resources in Kubernetes.\\n* Properly configure Java apps, especially when deploying to production environments.\\n* Consider using the Downward API or cgroups directly to manage resource limits for containers.\",\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Pod QoS Classes**\\n\\nKubernetes categorizes pods into three Quality of Service (QoS) classes to prioritize resource allocation:\\n\\n1. **BestEffort**: lowest priority, assigned to pods with no requests or limits set.\\n\\t* Containers may use as much memory as they want if available.\\n\\t* First ones killed when resources need to be freed.\\n2. **Burstable**: not explicitly mentioned in the text, but implied as a middle ground between BestEffort and Guaranteed.\\n3. **Guaranteed**: highest priority, assigned to pods with matching requests and limits for all resources (CPU and memory) across all containers.\\n\\n**Assigning QoS Class**\\n\\nThe QoS class is derived from the combination of resource requests and limits for each container in a pod, not through a separate field in the manifest. A pod's QoS class can be inferred as follows:\\n\\n* BestEffort: no requests or limits set.\\n* Guaranteed: matching requests and limits for all resources (CPU and memory) across all containers.\\n\\nThis system allows Kubernetes to make informed decisions about which pods to prioritize when resources are scarce, ensuring that critical applications receive the necessary resources.\",\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. There are three Quality of Service (QoS) classes for pods in Kubernetes:\\n\\t* BestEffort: No resource guarantees.\\n\\t* Burstable: Gets requested resources, but can use additional resources up to a limit if needed.\\n\\t* Guaranteed: Gets exactly the requested amount of resources, and cannot consume more.\\n2. The QoS class of a pod is determined by the relationships between its containers' requests and limits.\\n\\n**Important Details:**\\n\\n1. A Burstable QoS class includes pods where at least one container has a resource request specified, but not the limit.\\n2. For single-container pods, the QoS class applies to the pod as well.\\n3. Table 14.1 provides a reference for determining a container's QoS class based on its requests and limits.\\n\\n**Key Takeaways:**\\n\\n* Understanding the relationships between requests and limits is crucial in determining a pod's QoS class.\\n* Starting with the QoS class at the container level can make it easier to derive the pod's QoS class.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Understanding pod Quality of Service (QoS) classes in Kubernetes.\\n2. Determining the QoS class of a single-container or multi-container pod based on resource requests and limits.\\n\\n**Important Details:**\\n\\n* If only requests are set, but not limits, refer to the table rows where requests are less than the limits.\\n* If only limits are set, requests default to the limits, so refer to the rows where requests equal limits.\\n* For multi-container pods, if all containers have the same QoS class, that's also the pod's QoS class. If at least one container has a different class, the pod's QoS class is Burstable.\\n* A pod's QoS class can be determined by running `kubectl describe pod` and checking the `status.qosClass` field in the pod's YAML/JSON manifest.\\n\\n**QoS Class Determination:**\\n\\n* Table 14.1 shows how to determine a single-container pod's QoS class based on resource requests and limits.\\n* Table 14.2 shows how to determine a multi-container pod's QoS class based on its container classes.\\n\\nThe text also mentions that in an overcommitted system, the container with the lowest QoS class will be killed first.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When a system is overcommitted, Quality of Service (QoS) classes determine which container gets killed first to free up resources.\\n2. The order in which containers are killed is based on their QoS class: BestEffort, Burstable, and then Guaranteed.\\n3. Within the same QoS class, containers with higher OutOfMemory (OOM) scores get killed first.\\n\\n**Important Details:**\\n\\n* OOM scores are calculated from two factors: the percentage of available memory a process is consuming and a fixed adjustment based on the pod's QoS class and requested memory.\\n* When multiple containers have the same QoS class, the system selects which one to kill by comparing their OOM scores.\\n* In cases where there are only two Burstable pods, the system will prefer one over the other when selecting which process to kill.\\n\\n**Key Figures:**\\n\\n* Figure 14.5 illustrates an example of how containers with different QoS classes and usage levels would be prioritized for killing in case of memory shortage.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When setting resource requests and limits for containers, it's essential to consider not only the relationship between requests and limits but also the expected actual memory consumption.\\n2. Setting default requests and limits for pods per namespace can help prevent issues when other containers specify resource requests and limits.\\n3. The LimitRange resource allows you to set default resource requests for containers that don't specify them explicitly.\\n\\n**Important Details:**\\n\\n* If a container doesn't specify resource requests and limits, it may be at the mercy of other containers that do specify these values.\\n* A LimitRange resource can be used to specify minimum and maximum limits for each resource (CPU and memory) in a namespace.\\n* The default resource requests set by a LimitRange will be applied to containers that don't specify them explicitly.\\n* If a container's requested or limited resources exceed the minimum or maximum values set by a LimitRange, it may be rejected.\",\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. LimitRange resources are used to validate pod manifests against computational resource limits.\\n2. A LimitRange object prevents users from creating pods that exceed the resources available on any node in the cluster.\\n3. Limits specified in a LimitRange apply to individual pods or containers, not the total amount of resources across all pods in the namespace.\\n\\n**Important Details:**\\n\\n1. The LimitRanger Admission Control plugin validates pod manifests against LimitRange objects.\\n2. A LimitRange object can specify minimum and maximum CPU and memory limits for pods as a whole.\\n3. A LimitRange object can also specify default requests and limits for containers that don't specify them explicitly.\\n4. ResourceQuota objects (explained in section 14.5) are used to limit the total amount of resources available across all pods in a namespace.\\n\\n**Key Concepts:**\\n\\n1. LimitRange resources\\n2. Admission Control plugins (e.g., LimitRanger)\\n3. Pod manifests and validation\\n4. Computational resource limits (CPU, memory)\\n5. ResourceQuota objects\",\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. A `LimitRange` object allows you to set default resource requests and limits for pods, containers, and PersistentVolumeClaims (PVCs) per namespace.\\n2. You can configure minimum and maximum limits for CPU and memory at the pod level.\\n3. At the container level, you can also set default resource requests and limits that will be applied if not specified explicitly.\\n\\n**Important Details:**\\n\\n* The `LimitRange` object allows you to set a maximum ratio of limits vs. requests (e.g., CPU maxLimitRequestRatio = 4 means a container's CPU limit cannot be more than four times its request).\\n* You can also set minimum and maximum storage limits for PVCs.\\n* Limits from multiple `LimitRange` objects are consolidated when validating a pod or PVC.\\n* Existing pods and PVCs will not be revalidated if the limits are modified after creation; new pods and PVCs created afterward will use the updated limits.\\n\\n**Example:**\\n\\nThe text provides an example of a `LimitRange` object that sets default resource requests and limits for pods, containers, and PVCs. The example also shows how to create a pod with CPU requests greater than the allowed limit, which will be rejected by the API server.\",\n",
       "  \"Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. A pod's container requests two CPUs, which exceeds the maximum CPU limit set in a LimitRange.\\n2. The pod creation fails due to this excess request.\\n3. The error message lists all reasons why the pod was rejected, including both the container-level and pod-level limits.\\n\\n**Important Details:**\\n\\n* Default resource requests and limits can be set on containers that don't specify them using a LimitRange object.\\n* When creating a pod without specifying resources, default values from a LimitRange are applied automatically.\\n* The default values are applied per namespace, allowing admins to configure different defaults for each namespace.\\n\\n**Key Takeaways:**\\n\\n* Be mindful of the maximum CPU limits set in a LimitRange when creating pods.\\n* Use a LimitRange object to set default resource requests and limits for containers that don't specify them.\",\n",
       "  'Here is a summary of the text with main points and important details:\\n\\n**Main Points:**\\n\\n1. LimitRanges can be used to limit the resources available in a namespace, but they only apply to individual pods, not the total amount of resources.\\n2. To limit the total resources available in a namespace, a ResourceQuota object must be created.\\n3. A ResourceQuota limits the amount of computational resources (CPU and memory) that pods can consume, as well as storage (PersistentVolumeClaims), and the number of pods, claims, and other API objects that can be created.\\n\\n**Important Details:**\\n\\n* LimitRanges only apply to individual pods, not the total amount of resources.\\n* ResourceQuotas are enforced at pod creation time, so creating a ResourceQuota object has no effect on existing pods.\\n* A ResourceQuota can limit CPU and memory requests and limits for all pods in a namespace.\\n* The `hard` field in a ResourceQuota specifies the maximum amount of resources that can be consumed by all pods in the namespace.\\n\\n**Key Terms:**\\n\\n* LimitRange\\n* ResourceQuota\\n* Admission Control plugin (ResourceQuota Admission Control)\\n* CPU and memory requests and limits',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**ResourceQuota Overview**\\n\\n* A ResourceQuota sets separate totals for requests and limits for CPU and memory resources in a namespace.\\n* It defines maximum amounts that can be requested or limited, rather than a single total.\\n\\n**Example ResourceQuota**\\n\\n* The example sets:\\n\\t+ Maximum CPU requests to 400 millicores\\n\\t+ Maximum total CPU limits to 600 millicores\\n\\t+ Maximum memory requests to 200 MiB\\n\\t+ Maximum memory limits to 500 MiB\\n\\n**Applying a ResourceQuota**\\n\\n* A ResourceQuota applies to all pods in the namespace, not individually.\\n* It tracks usage across all pods.\\n\\n**Inspecting Quota Usage**\\n\\n* The `kubectl describe quota` command can be used to see how much of the quota is already used up.\\n* The output shows the current usage and hard limits for CPU and memory resources.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When creating a ResourceQuota, it's recommended to also create a LimitRange object alongside it.\\n2. A ResourceQuota can limit the total resources available in a namespace, including CPU and memory.\\n3. A ResourceQuota can also limit the amount of persistent storage that can be claimed in a namespace.\\n4. A ResourceQuota can limit the number of objects (e.g., Pods, ReplicationControllers, Services) that can be created in a single namespace.\\n\\n**Important Details:**\\n\\n* To create a pod without specifying resource requests or limits, you need to have a LimitRange configured with defaults for those resources.\\n* If a quota is configured for a specific resource (CPU or memory), pods must specify the request or limit for that same resource; otherwise, the API server will not accept the pod.\\n* A ResourceQuota can be used to limit the amount of persistent storage that can be claimed in a namespace, and can also define storage quotas for each StorageClass individually.\\n* A ResourceQuota can be configured to limit the number of objects (e.g., Pods, ReplicationControllers, Services) that can be created in a single namespace.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* ResourceQuota objects can be used to limit the number of computational resources (e.g. pods, services) in a Kubernetes namespace.\\n* These quotas can be set for various types of objects, including:\\n\\t+ Pods\\n\\t+ ReplicationControllers\\n\\t+ Secrets\\n\\t+ ConfigMaps\\n\\t+ PersistentVolumeClaims\\n\\t+ Services (including LoadBalancer and NodePort types)\\n* The ResourceQuota object itself can also have a quota count.\\n\\n**Important Details:**\\n\\n* Quotas can be set for the maximum number of objects that can be created, regardless of whether they are created manually or by a controller.\\n* Specific quotas can be set for certain types of services (e.g. LoadBalancer and NodePort).\\n* The number of other objects, such as ReplicaSets, Jobs, Deployments, Ingresses, etc., cannot currently be limited (but may have changed since the book was published).\\n\\n**Example:**\\n\\nThe text includes an example ResourceQuota object that limits the following resources in a namespace:\\n\\t+ 10 Pods\\n\\t+ 5 ReplicationControllers\\n\\t+ 10 Secrets\\n\\t+ 10 ConfigMaps\\n\\t+ 4 PersistentVolumeClaims\\n\\t+ 5 Services (with specific quotas for LoadBalancer and NodePort types)',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Quotas in Kubernetes can be limited to specific pod states and/or Quality of Service (QoS) classes using quota scopes.\\n* Four quota scopes are available: BestEffort, NotBestEffort, Terminating, and NotTerminating.\\n\\n**Important Details:**\\n\\n* The BestEffort and NotBestEffort scopes determine whether the quota applies to pods with the BestEffort QoS class or one of the other two classes (Burstable and Guaranteed).\\n* The Terminating and NotTerminating scopes apply to pods that have an active deadline set using the activeDeadlineSeconds field in the pod spec.\\n* A pod must match all specified scopes for a quota to apply to it.\\n* Quotas can limit different resources depending on their scope:\\n\\t+ BestEffort scope can only limit the number of pods.\\n\\t+ Other scopes (NotBestEffort, Terminating, and NotTerminating) can limit the number of pods, CPU/memory requests, and CPU/memory limits.\\n\\n**Example:**\\n\\n* A quota can be created to apply to BestEffort and NotTerminating pods, limiting them to a maximum of 4 pods.\\n* The quota would ensure that at most four pods exist with the BestEffort QoS class and do not have an active deadline set.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Properly setting resource requests and limits for pods in a Kubernetes cluster is crucial to avoid underutilization or overloading.\\n2. Monitoring actual resource usage is necessary to find the optimal request and limit settings.\\n3. The Kubelet contains an agent called cAdvisor that collects resource consumption data for individual containers and nodes.\\n\\n**Important Details:**\\n\\n* cAdvisor collects metrics from containers running on a node, while Heapster runs as a pod on one of the nodes and collects metrics from all nodes in the cluster.\\n* Heapster exposes the collected metrics through a Kubernetes Service, making it accessible at a stable IP address.\\n* Monitoring should be done under expected load levels to ensure optimal resource usage.\\n* Adjustments to resource requests and limits may be necessary after exposing the application to the public.\\n\\n**Key Components:**\\n\\n1. cAdvisor (agent in Kubelet)\\n2. Heapster (component that collects metrics from all nodes and exposes them centrally)',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Heapster is a component that collects resource usage data for Kubernetes clusters.\\n2. Heapster connects to cAdvisor, which collects container and node usage data without needing to talk to processes running inside pods' containers.\\n\\n**Important Details:**\\n\\n* In Google Kubernetes Engine (GKE), Heapster is enabled by default.\\n* On other types of Kubernetes clusters, such as Minikube, Heapster can be enabled using the command `minikube addons enable heapster`.\\n* After enabling Heapster, it takes a few minutes to collect metrics before resource usage statistics are available.\\n* Running `kubectl top node` shows actual CPU and memory usage for cluster nodes.\\n* Running `kubectl top pod --all-namespaces` shows actual CPU and memory usage for individual pods.\\n\\n**Key Features:**\\n\\n* Displays actual CPU and memory usage for nodes and individual pods.\\n* Provides runtime usage data, unlike the `kubectl describe node` command which shows requested and limited resources.\",\n",
       "  \"Here's a summary of the main points and important details:\\n\\n**Managing Pods' Computational Resources**\\n\\n* The `kubectl top` command can be used to view current CPU and memory usage for pods.\\n* However, it may take a few minutes for metrics to appear if they are not immediately available from Heapster.\\n\\n**Important Details**\\n\\n* If the `kubectl top` command fails to show metrics, wait a while and try again. Metrics should become available eventually.\\n* To see resource usages across individual containers instead of pods, use the `--containers` option with `kubectl top`.\\n\\n**Analyzing Historical Resource Consumption Statistics**\\n\\n* The `kubectl top` command only shows current resource usage and does not provide historical data.\\n* Additional tools are needed to analyze resource consumption over longer time periods.\\n\\n**Tools for Storing and Analyzing Historical Data**\\n\\n* Google Cloud Monitoring can be used when running a Kubernetes cluster on Google Kubernetes Engine.\\n* For local clusters (e.g. Minikube), InfluxDB is often used to store statistics data, while Grafana is used to visualize and analyze the data.\\n* InfluxDB is an open-source time-series database ideal for storing application metrics and monitoring data.\\n* Grafana is an analytics and visualization suite that allows users to visualize data stored in InfluxDB and discover how resource usage behaves over time.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* InfluxDB and Grafana can be run as pods in a Kubernetes cluster.\\n* The necessary manifests for deploying them are available on GitHub.\\n\\n**Important Details:**\\n\\n* When using Minikube, Heapster (which includes InfluxDB and Grafana) is deployed automatically when the Heapster add-on is enabled.\\n* To analyze resource usage with Grafana, open the web console at a URL found using `kubectl cluster-info`.\\n* The Grafana dashboard provides predefined views to show CPU usage across the entire cluster.\\n\\nLet me know if you'd like me to clarify anything!\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Minikube exposes Grafana's web console through a NodePort Service, allowing you to view resource usage statistics for your cluster.\\n2. The Grafana Home screen displays two dashboards: Cluster and Pods, which show resource usage statistics for nodes and individual pods, respectively.\\n3. The charts display actual usage, requests, and limits for CPU, memory, network, and filesystem resources.\\n\\n**Important Details:**\\n\\n1. Resource requests are guaranteed to a pod, but not the actual usage. If other pods use more resources than requested, your pod's resource usage may be throttled.\\n2. To ensure a pod can use as much CPU as it needs, raise its CPU resource request.\\n3. If a pod is using less memory than requested, decrease its memory request to make the unused memory available to other pods.\\n\\n**Key Takeaways:**\\n\\n* Monitor your cluster's resource usage with Grafana.\\n* Adjust resource requests for pods based on actual usage and limits to ensure optimal performance and utilization of resources.\",\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* To keep pods running smoothly, consider their resource usage and configure both resource requests and limits.\\n* Specifying resource requests helps Kubernetes schedule pods across the cluster, while specifying resource limits prevents pods from starving other pods of resources.\\n\\n**Important Details:**\\n\\n* Unused CPU time is allocated based on containers' CPU requests.\\n* Containers are not killed if they try to use too much CPU, but may be killed if they try to use too much memory.\\n* In an overcommitted system, containers may be killed to free memory for more important pods, based on QoS classes and actual memory usage.\\n* Actual CPU usage is higher than requested, so it's recommended to increase the CPU request.\\n* If actual memory usage is well below requested memory, it's recommended to decrease the memory request to avoid wasting resources.\\n\\n**Key Takeaways:**\\n\\n* Understand how resource requests and limits work in Kubernetes\\n* Be mindful of resource usage to prevent pods from starving each other\\n* Monitor and adjust resource requests and limits as needed to ensure smooth pod operation.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. You can use LimitRange objects to set minimum, maximum, and default resource requests and limits for individual pods.\\n2. ResourceQuota objects can be used to limit the total resources available to all pods in a namespace.\\n3. To determine optimal resource settings for a pod, you need to monitor its resource usage over time.\\n\\n**Important Details:**\\n\\n* Monitoring resource usage is necessary to set effective resource requests and limits for pods.\\n* Kubernetes uses these metrics to automatically scale pods (as described in the next chapter).',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Manual scaling of applications running in pods can be done by increasing replicas, but this approach has limitations.\\n2. Automatic scaling of pods and cluster nodes is preferred for handling sudden and unpredictable traffic increases.\\n\\n**Important Details:**\\n\\n* Manual scaling can only scale horizontally (increase replicas) and not vertically (increase container resources).\\n* Vertical scaling of pods can currently only be done at pod creation time, not while the pod is running.\\n* This chapter covers three main topics:\\n\\t1. Configuring automatic horizontal scaling of pods based on CPU utilization\\n\\t2. Configuring automatic horizontal scaling of pods based on custom metrics\\n\\t3. Understanding why vertical scaling of pods isn't possible yet and automatic horizontal scaling of cluster nodes\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Kubernetes can automatically scale pods and cluster nodes based on CPU usage or other metrics.\\n* This chapter explains how to set up automatic scaling for pods and nodes in Kubernetes.\\n\\n**Important Details:**\\n\\n* The autoscaling feature was rewritten between Kubernetes versions 1.6 and 1.7, so outdated information may be found online.\\n* Horizontal pod autoscaling (HPA) is the process of automatically scaling the number of pod replicas managed by a controller.\\n* HPA involves three steps:\\n\\t1. Obtaining metrics from all pods managed by the scaled resource object.\\n\\t2. Calculating the number of pods required to meet a target metric value.\\n\\t3. Updating the replicas field of the scaled resource (e.g., Deployment, ReplicaSet).\\n* The Autoscaler does not collect pod metrics itself; instead, it queries Heapster for metrics data through REST calls.\\n* Heapster must be running in the cluster for autoscaling to work.\\n\\n**Key Concepts:**\\n\\n* Horizontal pod autoscaling (HPA)\\n* Heapster\\n* cAdvisor\\n* Kubernetes autoscaling',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Horizontal pod autoscaling should be enabled in your cluster, and Heapster add-on must be enabled if not already done.\\n2. The Autoscaler calculates the required number of pods based on metrics from all pods belonging to the resource being scaled.\\n3. The calculation involves finding the average metric value across all replicas and rounding it up to the nearest integer.\\n\\n**Important Details:**\\n\\n1. Prior to Kubernetes 1.6, the HorizontalPodAutoscaler obtained metrics directly from Heapster. From version 1.8, metrics can be obtained through an aggregated resource metrics API.\\n2. Starting from version 1.9, this behavior will be enabled by default.\\n3. The core API server does not expose metrics itself, and cluster administrators must select a metrics collector to use in their clusters.\\n4. When autoscaling is based on multiple pod metrics (e.g., CPU usage and QPS), the Autoscaler calculates the replica count for each metric individually and takes the highest value.\\n\\n**Key Concepts:**\\n\\n1. Heapster add-on\\n2. Horizontal pod autoscaling\\n3. Metrics calculation\\n4. API server aggregation',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The final step in an autoscaling operation involves updating the desired replica count field on the scaled resource object (e.g., ReplicaSet).\\n2. The Autoscaler controller modifies the replicas field through the Scale sub-resource to spin up or delete pods.\\n3. This allows the Autoscaler to operate on any scalable resource, as long as the API server exposes the Scale sub-resource.\\n\\n**Important Details:**\\n\\n* Currently, the Scale sub-resource is exposed for Deployments, ReplicaSets, ReplicationControllers, and StatefulSets.\\n* The Autoscaler can adjust replicas based on metrics such as CPU utilization or QPS (queries per second).\\n* The Autoscaler calculates the number of replicas needed by taking into account multiple metrics and their corresponding weights.\\n\\n**Key Concepts:**\\n\\n* Horizontal Pod Autoscaler (HPA)\\n* Scale sub-resource\\n* Desired replica count field\\n* ReplicaSet, Deployment, StatefulSet, and ReplicationController objects.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The horizontal pod autoscaling (HPA) process involves several components, including cAdvisor, Heapster, and the HPA controller.\\n2. Metrics data flows from pods to cAdvisor, then to Heapster, and finally to the HPA controller.\\n3. Each component receives metrics data periodically, which can cause a delay in scaling actions.\\n\\n**Important Details:**\\n\\n* The HPA process is not immediate; it takes time for metrics data to be propagated and scaling actions to be performed.\\n* CPU utilization is an important metric for autoscaling decisions.\\n* Autoscaling can involve either vertical scaling (increasing the amount of CPU per pod) or horizontal scaling (increasing the number of pods).\\n* The HPA controller adjusts the replicas of a deployment by increasing or decreasing the number of pods.\\n\\n**Key Components:**\\n\\n1. cAdvisor: collects metrics from all containers on a node\\n2. Heapster: collects metrics from all nodes and provides data to the HPA controller\\n3. Horizontal Pod Autoscaler (HPA): adjusts the replicas of a deployment based on metrics data',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n* The chapter focuses on scaling out (increasing the number of pods) based on CPU usage.\\n* Scaling out should be done before the average CPU load reaches or exceeds 80% to leave room for sudden load spikes.\\n* The Autoscaler only considers the pod's guaranteed CPU amount (CPU requests) when determining CPU utilization.\\n\\n**Important Details:**\\n\\n* To enable autoscaling, pods must have CPU resource requests specified in their configuration.\\n* A HorizontalPodAutoscaler can be created based on CPU usage by specifying a Deployment with a pod template that includes a CPU resource request.\\n* The example provided shows how to create a Deployment with a pod template that sets the initial number of replicas to three and specifies the kubia:v1 image.\\n\\n**Key Takeaways:**\\n\\n* Autoscaling is most effective when done before the average CPU load reaches 80%.\\n* Pods must have CPU requests specified for autoscaling to work.\\n* A HorizontalPodAutoscaler can be created based on CPU usage by specifying a Deployment with a pod template that includes a CPU resource request.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To enable horizontal autoscaling for a Deployment, you need to create a HorizontalPodAutoscaler (HPA) object.\\n2. You can use the `kubectl autoscale` command to create an HPA object instead of manually creating a YAML manifest.\\n\\n**Important Details:**\\n\\n* The HPA object is created with a target CPU utilization of 30% and minimum/maximum replica counts of 1-5.\\n* The Autoscaler will adjust the number of replicas to keep their CPU utilization around 30%.\\n* It's recommended to autoscale Deployments instead of underlying ReplicaSets to ensure the desired replica count is preserved across application updates.\\n\\n**HPA Resource Details:**\\n\\n* HPA resources are in the `autoscaling` API group.\\n* Each HPA has a name, minimum and maximum number of replicas, and a target resource (in this case, CPU utilization).\\n* The Autoscaler will adjust the number of pods to keep their CPU utilization around 30% of the requested amount.\",\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n* The chapter discusses automatic scaling of pods and cluster nodes using Horizontal Pod Autoscaling (HPA).\\n* An example is given where an HPA resource is created to scale down a Deployment named \"kubia\" based on CPU utilization.\\n* The autoscaler scales down the Deployment from 3 replicas to 1 replica, deleting two excess pods.\\n\\n**Important Details:**\\n\\n* There are multiple versions of HPA resources (autoscaling/v2beta1 and autoscaling/v1), with the new version being used in this example.\\n* It takes time for cAdvisor to collect CPU metrics and Heapster to process them before the Autoscaler can take action.\\n* The Autoscaler only adjusts the desired replica count on the Deployment, which is then updated by the Deployment controller.\\n* The ReplicaSet controller deletes excess pods based on the updated replica count.\\n* The `kubectl describe` command can be used to see more information about the HPA and its operation.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The Horizontal Pod Autoscaler (HPA) successfully rescaled to one replica because all metrics were below target.\\n2. To trigger a scale-up, requests are sent to the pod, increasing its CPU usage, which should cause the HPA to add more replicas.\\n3. A Service is exposed to allow requests to be sent to multiple pods through a single URL.\\n\\n**Important Details:**\\n\\n* The `kubectl expose` command is used to create a Service for the deployment.\\n* The `watch -n 1 kubectl get hpa,deployment` command is used to monitor the HPA and Deployment objects in real-time.\\n* On OSX, a loop or manual running of `kubectl get` may be necessary instead of using the `--watch` option.\\n* A load-generating pod is run to simulate traffic and trigger the scale-up process.\\n\\n**Key Commands:**\\n\\n* `kubectl expose deployment kubia --port=80 --target-port=8080`\\n* `watch -n 1 kubectl get hpa,deployment`\\n* `kubectl run -it --rm --restart=Never loadgenerator --image=busybox -- sh -c \"while true; do wget -O - -q http://kubia.default; done\"`',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The chapter discusses automatic scaling of pods and cluster nodes using Kubernetes.\\n2. A load-generator pod is run to demonstrate autoscaling, which repeatedly hits the kubia Service.\\n3. The `kubectl run` command is used with options `-it`, `--rm`, and `--restart=Never` to create an unmanaged pod that cleans up after itself.\\n\\n**Important Details:**\\n\\n1. The autoscaler increases the number of replicas as CPU utilization exceeds 30%.\\n2. In this example, the initial average CPU utilization was 108%, which is more than 100% due to a container's actual CPU usage exceeding its requested CPU.\\n3. To determine the number of replicas needed, the autoscaler divides the current CPU utilization (108%) by the target CPU utilization percentage (30%), resulting in 3.6, which is rounded up to 4.\\n\\n**Key Concepts:**\\n\\n1. Horizontal Pod Autoscaling (HPA) scales the number of pods based on CPU utilization.\\n2. The `kubectl run` command with options `-it`, `--rm`, and `--restart=Never` creates an unmanaged pod that cleans up after itself.\\n3. Container CPU utilization is calculated by dividing actual CPU usage by requested CPU, which can result in a percentage greater than 100%.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Horizontal pod autoscaling (HPA) can scale up to four pods if the initial average CPU utilization is around 27%, which is close to the target value of 30%.\\n2. The HPA has limits on how many replicas it can add in a single operation and how soon subsequent scaling operations can occur.\\n3. To modify the target metric value on an existing HPA object, you can edit the resource using `kubectl edit`.\\n\\n**Important Details:**\\n\\n* The HPA will at most double the number of replicas in a single operation if more than two current replicas exist.\\n* A scale-up event occurs only if no rescaling event occurred in the last three minutes, while a scale-down event occurs every five minutes.\\n* To increase the target CPU utilization from 30% to 60%, you can edit the HPA resource using `kubectl edit` and change the `targetAverageUtilization` field to 60.',\n",
       "  \"Here is a summary of the text with main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes provides automatic scaling based on memory consumption, but it's more problematic than CPU-based autoscaling.\\n2. Memory-based autoscaling requires the app to release memory itself, which can lead to infinite scaling if the app doesn't use less memory.\\n3. Custom metrics can be used for autoscaling, but older versions of Kubernetes made this complicated and prone to errors.\\n\\n**Important Details:**\\n\\n* Memory-based autoscaling was introduced in Kubernetes 1.8 and is configured similarly to CPU-based autoscaling.\\n* The initial design of the autoscaler made it difficult to use custom metrics, prompting a redesign by the Kubernetes Autoscaling Special Interest Group (SIG).\\n* Newer versions of Kubernetes have resolved these issues, making it easier to configure autoscaling based on custom metrics.\\n\\n**Key Takeaways:**\\n\\n* CPU-based autoscaling is still a reliable and easy-to-configure option.\\n* Custom metrics can be used for autoscaling, but require careful configuration to avoid infinite scaling or other issues.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Horizontal Pod Autoscaling (HPA)**\\n\\n* The `metrics` field in an HPA object allows you to define multiple metrics for autoscaling decisions.\\n* There are three types of metrics that can be used:\\n\\t1. **Resource**: based on a resource metric, such as CPU or memory requests.\\n\\t2. **Pods**: based on a metric related to the pod directly, such as Queries-Per-Second (QPS) or message queue size.\\n\\t3. **Object**: based on a metric that doesn't pertain directly to the pods, such as QPS for an Ingress object.\\n\\n**Key Points**\\n\\n* To use a Pod metric, you need to specify the `type: Pods` and provide the metric name (e.g., `qps`) and target average value.\\n* To use an Object metric, you need to specify the `type: Object` and provide the metric name for the object being referenced.\\n\\n**Important Details**\\n\\n* When using a Pod metric, the autoscaler obtains the metric for all targeted pods and uses the average value.\\n* When using an Object metric, the autoscaler obtains a single metric from the single object.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To configure an Horizontal Pod Autoscaler (HPA), you need to specify the target object and value, as shown in an example.\\n2. The HPA can be configured to use metrics from other objects, such as Ingress or Deployment resources.\\n3. Not all metrics are suitable for autoscaling, and increasing replicas may not result in a linear decrease of the metric's average value.\\n\\n**Important Details:**\\n\\n* To configure an HPA, you need to specify the target object (e.g., Ingress or Deployment) and its metric name (e.g., latencyMillis).\\n* The target value is the desired level for the metric.\\n* Metrics like memory consumption are not suitable for autoscaling because increasing replicas may not result in a linear decrease of the average value.\\n* Custom metrics, such as Queries per Second (QPS), can be used for autoscaling if their value will behave proportionally when the number of pods increases or decreases.\\n* The HPA currently does not allow scaling down to zero replicas, even if the pods are not doing any work.\\n\\n**Key Concepts:**\\n\\n* Horizontal Pod Autoscaler (HPA)\\n* Metrics and target values\\n* Suitable metrics for autoscaling\\n* Custom metrics and their behavior when scaling up or down\",\n",
       "  \"Here's a summary of the text with the main points and important details:\\n\\n**Vertical Pod Autoscaling**\\n\\nThe author discusses two features related to scaling pods in Kubernetes: Vertical Pod Autoscaling (VPA) and Initial Resources.\\n\\n**Main Points:**\\n\\n1. **Idling and Un-idling**: A feature that allows pods to be scaled down to zero, saving resources for other pods. This is currently not available in Kubernetes.\\n2. **Vertical Pod Autoscaling (VPA)**: A feature that scales a pod vertically by giving it more CPU and/or memory. Currently not available in Kubernetes.\\n3. **Initial Resources**: An experimental feature that sets resource requests on newly created pods if they are not specified explicitly.\\n\\n**Important Details:**\\n\\n* VPA would allow pods to be scaled up or down based on their resource needs, but this feature is still not available in Kubernetes.\\n* Initial Resources uses historical data to set resource requests for new pods without explicit configuration.\\n* A proposal for a new vertical pod autoscaling mechanism is being finalized, which will eventually enable modifying resource requests while a pod is running.\\n\\nOverall, the text highlights the limitations of current Kubernetes features and the need for additional functionality to support more efficient scaling of pods.\",\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes has a feature called Cluster Autoscaling that automatically adds or removes nodes from a cluster based on demand.\\n2. The Horizontal Pod Autoscaler creates additional pod instances, but when all nodes are at capacity, additional nodes need to be added.\\n3. The Cluster Autoscaler takes care of automatically provisioning additional nodes when it detects a lack of resources on existing nodes.\\n\\n**Important Details:**\\n\\n1. The Cluster Autoscaler checks if a new node can accommodate the unscheduled pod before requesting an additional node from the cloud provider.\\n2. Cloud providers group nodes into groups (or pools) of same-sized nodes, so the Cluster Autoscaler needs to specify the node type when requesting an additional node.\\n3. If multiple node types are available, the Cluster Autoscaler must pick the best one, which can be configured or default to a random selection in the worst case.\\n4. The Cluster Autoscaler also de-provisions nodes that are underutilized for longer periods of time.\\n\\n**Key Features:**\\n\\n1. Automatic addition and removal of nodes based on demand\\n2. Ability to specify node type when requesting an additional node\\n3. Selection of best node type if multiple options are available',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Horizontal Scaling**\\n\\n* When a new node joins a Kubernetes cluster, it registers itself with the API server and becomes part of the cluster.\\n* The Cluster Autoscaler also scales down the number of nodes when they are not being utilized enough.\\n\\n**Scaling Down (Relinquishing Nodes)**\\n\\n* The Autoscaler monitors CPU and memory requests on all nodes to determine if a node is unnecessary (below 50% utilization).\\n* A node will only be shut down if:\\n\\t+ No system pods or unmanaged pods with local storage are running on the node.\\n\\t+ All pods running on the node can be rescheduled to other nodes.\\n\\n**Scaling Up**\\n\\n* When a pod cannot be scheduled to existing nodes, the Autoscaler scales up the selected node group.\\n* The Autoscaler determines which type of node would be able to fit the pod and selects one if multiple types could fit.\\n* The process is illustrated in Figure 15.5, showing how the Autoscaler scales up when it finds a pod that cannot be scheduled to existing nodes.\\n\\n**Key Points**\\n\\n* The Cluster Autoscaler monitors CPU and memory requests on all nodes to determine scaling needs.\\n* Nodes are only shut down if they are not being utilized enough and do not have critical pods running on them.\\n* Scaling up involves selecting the correct node type to fit a pod that cannot be scheduled to existing nodes.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. The Cluster Autoscaler is available on Google Kubernetes Engine (GKE), Google Compute Engine (GCE), Amazon Web Services (AWS), and Microsoft Azure.\\n2. To enable the Cluster Autoscaler, you need to follow platform-specific instructions.\\n\\n**Important Details:**\\n\\n* On GKE, you can enable the Cluster Autoscaler using the `gcloud` command: `$ gcloud container clusters update kubia --enable-autoscaling --min-nodes=3 --max-nodes=5`\\n* On GCE, you need to set three environment variables before running `kube-up.sh`: `KUBE_ENABLE_CLUSTER_AUTOSCALER=true`, `KUBE_AUTOSCAler_MIN_NODES=3`, and `KUBE_AUTOSCAler_MAX_NODES=5`.\\n* The Cluster Autoscaler publishes its status to the `cluster-autoscaler-status` ConfigMap in the `kube-system` namespace.\\n* To limit service disruption during cluster scale-down, you can manually cordon and drain nodes using `kubectl` commands: `cordon`, `drain`, and `uncordon`.\\n\\n**Key Concepts:**\\n\\n* Cluster Autoscaling: automatically scales the number of nodes in a Kubernetes cluster based on demand.\\n* Cordon and Drain: marks a node as unschedulable and evicts all pods from it, preventing new pods from being scheduled to that node.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes provides a way to specify a minimum number of pods that must always run, especially for quorum-based clustered applications.\\n2. This is achieved through a PodDisruptionBudget (PDB) resource.\\n3. A PDB ensures that a specified number of pods with a certain label are always running.\\n\\n**Important Details:**\\n\\n* A PDB contains only two fields: a pod label selector and a minimum/maximum number of pods that must be available/unavailable.\\n* The `minAvailable` field specifies the minimum number of pods that must always run, while the `maxUnavailable` field (introduced in Kubernetes 1.7) specifies the maximum number of pods that can be unavailable at any time.\\n* A PDB can be created using the `kubectl create pdb` command and can be used to ensure that a certain percentage of pods with a specific label are always running.\\n* The Cluster Autoscaler and `kubectl drain` command will adhere to a PDB, ensuring that no more than the specified number of pods are evicted.',\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes can scale not only pods but also cluster nodes automatically.\\n2. Horizontal Pod Autoscaling (HPA) allows scaling based on CPU utilization, custom metrics, or other object-related metrics.\\n3. Vertical pod autoscaling is not currently possible.\\n\\n**Important Details:**\\n\\n* When minAvailable is set to a certain number of pods, the pod eviction process will evict pods one by one, waiting for each evicted pod to be replaced before evicting another.\\n* Kubernetes can automatically scale cluster nodes if running on a supported cloud provider.\\n* The `kubectl run` command with `-it` and `--rm` options allows running one-off processes in a pod that will stop and delete automatically when the process is stopped.\\n\\n**Next Chapter:**\\n\\nThe next chapter will cover advanced scheduling features, including how to keep certain pods away from certain nodes and how to schedule pods close together or apart.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. Kubernetes allows for advanced scheduling options beyond initial specifications.\\n2. This chapter covers four main features:\\n\\t* Using node taints and pod tolerations to restrict pod placement on certain nodes.\\n\\t* Defining node affinity rules as an alternative to node selectors.\\n\\t* Co-locating pods using pod affinity.\\n\\t* Keeping pods away from each other using pod anti-affinity.\\n\\n**Important Details:**\\n\\n* Initially, Kubernetes scheduling was limited by specifying a node selector in the pod specification.\\n* Node taints and pod tolerations were later added as mechanisms to expand this functionality.',\n",
       "  \"Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n* Node taints are used to prevent pods from being scheduled on certain nodes, unless those pods tolerate the taint.\\n* Taints are different from node selectors and node affinity rules, which allow selecting specific nodes for pod deployment.\\n\\n**Important Details:**\\n\\n* A pod can only be scheduled on a node if it tolerates the node's taints.\\n* Node taints have three components: key, value, and effect (e.g., NoSchedule).\\n* Taints are added to nodes without modifying existing pods; pods that want to use a tainted node must opt in by tolerating the taint.\\n* The example provided shows how to view a node's taints using `kubectl describe node`.\\n* In a multi-node cluster created with kubeadm, the master node is tainted by default, preventing non-Control Plane pods from being scheduled on it.\\n\\n**Key Terms:**\\n\\n* Taints: prevent pods from being scheduled on certain nodes unless they tolerate the taint.\\n* Toleration: allows a pod to be scheduled on a node that has a taint.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* In a Kubernetes cluster, pods can have \"tolerations\" that allow them to be scheduled on nodes with specific \"taints\".\\n* Tolerations are used to repel pods from certain nodes, while taints are used to mark nodes as unsuitable for scheduling.\\n* A pod\\'s toleration matches the node\\'s taint if they share the same key and effect.\\n\\n**Important Details:**\\n\\n* The kube-proxy pod in a kubeadm cluster has three tolerations that allow it to run on the master node, despite the master node having a \"NoSchedule\" taint.\\n* Tolerations can also specify how long a pod is allowed to run on nodes that are not ready or unreachable.\\n* Pods with no tolerations can only be scheduled on nodes without taints.\\n\\n**Key Concepts:**\\n\\n* Taints: marks a node as unsuitable for scheduling due to some condition (e.g. \"NoSchedule\" if the node is the master).\\n* Toleration: allows a pod to be scheduled on a node with a specific taint.\\n* Matching toleration and taint: when a pod\\'s toleration matches a node\\'s taint, it can be scheduled on that node despite the taint.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Taints and Tolerations**\\n\\n* A taint is a label that can be added to a Kubernetes node, which affects how pods are scheduled on that node.\\n* There are three possible effects associated with a taint: NoSchedule (pods won\\'t be scheduled if they don\\'t tolerate the taint), PreferNoSchedule (scheduler will try to avoid scheduling the pod, but will schedule it if necessary), and NoExecute (also affects running pods, which will be evicted if they don\\'t tolerate the taint).\\n\\n**Adding Custom Taints**\\n\\n* To add a custom taint to a node, use the `kubectl taint` command.\\n* Example: adding a taint with key \"node-type\", value \"production\", and effect NoSchedule.\\n\\n**Tolerations in Pods**\\n\\n* To deploy pods to nodes with a specific taint, they need to tolerate that taint.\\n* The YAML manifest of the pod needs to include a toleration section with the matching key and value from the node\\'s taint.\\n* Example: adding a toleration for the \"node-type=production\" taint with effect NoSchedule.\\n\\n**Key Points**\\n\\n* Taints can be used to control where pods are scheduled in a Kubernetes cluster.\\n* Tolerations allow pods to run on nodes with specific taints.\\n* This feature is useful for separating production and non-production workloads.',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Using Taints and Tolerations in Kubernetes**\\n\\n* **Taints**: A way to mark a node with a specific key-value pair, which can prevent pods from being scheduled on it. There are three types of taints:\\n\\t+ `NoSchedule`: Prevents new pods from being scheduled on the node.\\n\\t+ `PreferNoSchedule`: Defines an unpreferred node for scheduling.\\n\\t+ `NoExecute`: Evicts existing pods from a node.\\n* **Tolerations**: A way to allow a pod to be scheduled on nodes with specific taints. There are two types of tolerations:\\n\\t+ `Equal`: Allows the pod to be scheduled on nodes with a matching key-value pair.\\n\\t+ `Exists`: Allows the pod to be scheduled on nodes with any value for a specific key.\\n\\n**Example**\\n\\n* A deployment is created with a toleration that allows it to be scheduled on production nodes.\\n* However, some pods are still deployed to non-production nodes due to lack of taints and tolerations.\\n* To prevent this, the non-production nodes need to be tainted with `node-type=non-production:NoSchedule`, and the deployment needs to have a matching toleration.\\n\\n**Key Points**\\n\\n* Nodes can have multiple taints, and pods can have multiple tolerations.\\n* Tolerations can tolerate specific values or any value for a specific key.\\n* Taints and tolerations can be used to partition a cluster into separate partitions for different teams.',\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Configuring Rescheduling Time**\\n\\n* Kubernetes allows you to specify how long to wait before rescheduling a pod after a node failure using tolerations.\\n* Tolerations can be used to delay the rescheduling of a pod for up to 300 seconds if the node is not ready or unreachable.\\n\\n**Tolerations Example**\\n\\n* The example shows two tolerations that allow a pod to tolerate a node being notReady or unreachable for 300 seconds.\\n* These tolerations are automatically added to pods that don't define them, but can be overridden by adding them manually to the pod's spec.\\n\\n**Node Affinity**\\n\\n* Node affinity is a mechanism that allows you to schedule pods only to specific subsets of nodes.\\n* This is different from node selectors, which require all labels specified in the field to be present on the node.\\n* Node affinity is more powerful and flexible than node selectors.\\n\\n**Important Details**\\n\\n* Taint-based evictions are currently an alpha feature and may change in future versions of Kubernetes.\\n* To enable taint-based evictions, you need to run the Controller Manager with the `--feature-gates=Taint-BasedEvictions=true` option.\",\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. Node selectors will eventually be deprecated, so it's essential to understand node affinity rules.\\n2. Node affinity allows you to specify hard requirements or preferences for where pods are scheduled on nodes.\\n3. You can use node affinity to attract pods to certain nodes by specifying a preference.\\n\\n**Important Details:**\\n\\n* Node affinity selects nodes based on their labels, similar to node selectors.\\n* The default node labels in a Google Kubernetes Engine (GKE) cluster include:\\n\\t+ `failure-domain.beta.kubernetes.io/region`: specifies the geographical region of the node\\n\\t+ `failure-domain.beta.kubernetes.io/zone`: specifies the availability zone of the node\\n\\t+ `kubernetes.io/hostname`: specifies the node's hostname\\n* These labels can be used in pod affinity rules.\\n* Node affinity allows you to specify hard requirements or preferences for where pods are scheduled on nodes, similar to using a node selector.\\n\\n**Key Takeaways:**\\n\\n* Understand node affinity rules as an alternative to node selectors.\\n* Use node affinity to attract pods to certain nodes by specifying a preference.\\n* Be aware of the default node labels in your GKE cluster and how they can be used in pod affinity rules.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The `nodeSelector` field in a Kubernetes Pod definition specifies which nodes to deploy the pod on based on labels.\\n2. In this example, the pod is scheduled only on nodes that have the label `gpu=true`.\\n3. If you replace `nodeSelector` with `nodeAffinity`, the pod definition becomes more complex but more expressive.\\n\\n**Important Details:**\\n\\n* The `nodeAffinity` field has a long name and can be broken down into two parts:\\n\\t+ `requiredDuringSchedulingIgnoredDuringExecution`: specifies the labels that must be present on a node for the pod to be scheduled.\\n\\t+ `IgnoredDuringExecution`: means the rules defined under this field do not affect pods already executing on the node.\\n* Currently, affinity only affects pod scheduling and does not cause a pod to be evicted from a node.\\n* Eventually, Kubernetes will support `RequiredDuringExecution`, which would allow for eviction of pods that no longer meet the specified conditions.\\n\\n**Key Terms:**\\n\\n* `nodeSelector`: specifies which nodes to deploy a pod on based on labels.\\n* `nodeAffinity`: more complex and expressive than `nodeSelector`, but currently only affects scheduling.\\n* `requiredDuringSchedulingIgnoredDuringExecution`: specifies the labels that must be present on a node for the pod to be scheduled.',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Node affinity in Kubernetes allows you to specify which nodes a pod can be scheduled on based on certain conditions.\\n2. The `nodeSelectorTerms` field defines which expressions a node's labels must match for a pod to be scheduled to that node.\\n\\n**Important Details:**\\n\\n* If a node does not have a required label, pods that require that label will be evicted from the node (although this is not yet supported in Kubernetes).\\n* The `matchExpressions` field defines which expressions a node's labels must match.\\n* Node affinity allows you to prioritize nodes during scheduling using the `preferredDuringSchedulingIgnoredDuringExecution` field.\\n\\n**Key Concepts:**\\n\\n* Node affinity\\n* Node selector terms\\n* Match expressions\\n* Preferred during scheduling ignored during execution\\n\\n**Example Use Case:**\\n\\n* Deploying pods across multiple datacenters (availability zones) and prioritizing them to be scheduled on specific nodes.\",\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. The chapter discusses advanced scheduling in Kubernetes, specifically using No De Affinity to schedule pods on machines reserved for company deployments.\\n2. Labeling nodes with availability zones and share types (dedicated or shared) is necessary for specifying preferential node affinity rules.\\n\\n**Important Details:**\\n\\n* To use No De Affinity, the machine must not have enough room for the pods or other important reasons prevent it from being scheduled there.\\n* The nodes need to be labeled with an availability zone and a share type (dedicated or shared) using `kubectl label` commands.\\n* An example of labeling two worker nodes in a three-node cluster is provided, including setting up a three-node cluster in VMs running locally.\\n* Minikube is not recommended for running these examples because it only runs one node.\\n* The labeled nodes can be viewed using `kubectl get node -L availability-zone -L share-type`.\\n* A Deployment manifest is shown that prefers dedicated nodes in zone1, demonstrating how to specify preferential node affinity rules.\\n\\n**Key Terms:**\\n\\n* No De Affinity\\n* Node labeling\\n* Availability zones\\n* Share types (dedicated or shared)\\n* Preferential node affinity rules',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Node affinity is used to attract pods to specific nodes in a cluster, rather than making it a hard requirement.\\n2. A weight value can be assigned to each preference rule to indicate its importance.\\n\\n**Important Details:**\\n\\n* The node affinity rules are defined as \"preferences\" rather than requirements, allowing for flexibility in scheduling.\\n* The weights assigned to each preference rule determine their priority:\\n\\t+ High priority (80): availability-zone=zone1\\n\\t+ Lower priority (20): share-type=dedicated\\n* When scheduling pods, the nodes are prioritized based on their labels and weights:\\n\\t+ Top priority: nodes with both availability-zone=zone1 and share-type=dedicated labels\\n\\t+ Next priority: shared nodes in zone 1\\n\\t+ Then dedicated nodes in other zones\\n\\t+ Lowest priority: all other nodes\\n\\n**Key Concepts:**\\n\\n* Node affinity preferences\\n* Weight values for preference rules\\n* Prioritization of nodes based on their labels and weights',\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. When deploying a Deployment in a two-node cluster, most pods are deployed to one node (node1) and only one pod is deployed to the other node (node2).\\n2. The reason for this uneven distribution is due to the Scheduler's use of prioritization functions beyond node affinity, such as Selector-SpreadPriority.\\n3. Scaling up the Deployment can result in more pods being deployed to node1, but still some pods are deployed to node2.\\n\\n**Important Details:**\\n\\n* Node affinity rules only affect the affinity between a pod and a node.\\n* Pod affinity allows specifying the affinity between pods themselves, which can be useful for co-locating related pods (e.g., frontend and backend).\\n* In the example provided, a backend pod is deployed first, followed by five frontend pod replicas with pod affinity configured to deploy on the same node as the backend pod.\\n\\n**Key Concepts:**\\n\\n* Node affinity\\n* Pod affinity\\n* Selector-SpreadPriority prioritization function\\n* Co-locating related pods for improved performance and reduced latency.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. A Deployment named \"frontend\" has been created with a podAffinity configuration that requires it to be deployed on the same node as pods labeled \"app=backend\".\\n2. The frontend pod\\'s definition includes a podAffinity rule that specifies a hard requirement for deployment on the same node as backend pods.\\n3. This means that all frontend pods will only be scheduled to run on the same node where the backend pod was initially deployed.\\n\\n**Important Details:**\\n\\n* The podAffinity configuration uses a topologyKey of \"kubernetes.io/hostname\" to specify that the pods should be deployed on the same node.\\n* The labelSelector matches labels with key \"app\" and value \"backend\", which is used to identify the backend pods.\\n* This setup ensures that all frontend pods will run on the same node as the backend pod, rather than being distributed across multiple nodes.',\n",
       "  \"Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. The `matchExpressions` field can be used instead of the simpler `matchLabels` field for more expressive matching.\\n2. A Deployment was created with pod affinity rules to schedule frontend pods on the same node as a backend pod.\\n3. When deleting the backend pod, the Scheduler schedules the pod back to the same node (node2) even though it doesn't define any pod affinity rules itself.\\n\\n**Important Details:**\\n\\n* The `podAffinity` configuration in the Deployment matches pods with the labelSelector defined in the frontend pod's configuration.\\n* If a pod is deleted and rescheduled to a different node, its pod affinity rules would be broken. Therefore, the Scheduler takes other pods' pod affinity rules into account when scheduling new pods.\\n* Increasing the Scheduler's logging level shows that it considers other pods' pod affinity rules when making scheduling decisions.\\n\\n**Key Concepts:**\\n\\n* Pod affinity rules\\n* LabelSelector\\n* MatchExpressions\\n* Taint Toleration Priority and Score\",\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. The Kubernetes scheduler uses various affinity and anti-affinity rules to schedule pods on nodes.\\n2. Pod affinity allows specifying which nodes or pods a pod should be co-located with, while pod anti-affinity specifies which nodes or pods a pod should not be co-located with.\\n3. Co-locating pods in the same availability zone, geographic region, or rack can improve application availability and performance.\\n\\n**Important Details:**\\n\\n* To co-locate pods in the same availability zone, use `topologyKey` = `failure-domain.beta.kubernetes.io/zone`.\\n* To co-locate pods in the same geographic region, use `topologyKey` = `failure-domain.beta.kubernetes.io/region`.\\n* The `topologyKey` can be any custom key (e.g., \"rack\") as long as it is added as a label to nodes.\\n* When scheduling a pod, the scheduler checks the pod\\'s affinity config and selects nodes that match the specified topology key.\\n\\n**Example:**\\n\\n* Suppose you have 20 nodes with 10 in each rack. Label the first 10 nodes as `rack=rack1` and the others as `rack=rack2`.\\n* To schedule a pod to run on the same rack as another pod, set `topologyKey` = \"rack\" in the pod\\'s affinity config.\\n* The scheduler will then select nodes that match the specified topology key (i.e., nodes labeled with the same rack label).',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. When scheduling pods, the Scheduler uses label selectors to match pods with specific labels.\\n2. The label selector can also select pods from other namespaces by adding a `namespaces` field.\\n3. Pod affinity preferences can be expressed instead of hard requirements using the `podAffinity` rule.\\n\\n**Important Details:**\\n\\n* By default, the label selector only matches pods in the same namespace as the pod being scheduled.\\n* The `podAffinity` rule allows for preferred scheduling locations, but also permits scheduling elsewhere if the preferred location is not available.\\n* In Figure 16.5, a frontend pod is scheduled to nodes with the same rack label (`rack=rack2`) as a backend pod.\\n* The `TopologyKey` in `podAffinity` determines the scope of where the pod should be scheduled.\\n\\n**Additional Context:**\\n\\n* This text appears to be from a chapter on advanced scheduling in a Kubernetes or container orchestration context.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* The text discusses using \"pod affinity\" in Kubernetes deployments, which allows for preference rules when scheduling pods on nodes.\\n* Pod affinity can be used to schedule pods on the same node as other pods with certain labels.\\n\\n**Important Details:**\\n\\n* To use pod affinity, a weight must be defined for each rule, along with a topology key and label selector.\\n* The Scheduler will prefer to schedule pods on nodes that meet the specified conditions, but may still schedule them on other nodes if necessary.\\n* In the example provided, four frontend pods are deployed on the same node as a backend pod, and one frontend pod is deployed on another node.\\n\\n**Key Concepts:**\\n\\n* Pod affinity (preferred) allows for preference rules when scheduling pods on nodes.\\n* Weight, topology key, and label selector must be specified to use pod affinity.\\n* The Scheduler will prefer to schedule pods on nodes that meet the specified conditions.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The text discusses advanced scheduling in Kubernetes, specifically using `kubectl` to view running pods.\\n2. It highlights the concept of pod anti-affinity, which keeps pods away from each other.\\n\\n**Important Details:**\\n\\n* Pod anti-affinity is specified using the `podAntiAffinity` property instead of `podAffinity`.\\n* The Scheduler will not choose nodes where pods matching the `podAntiAffinity` label selector are running.\\n* Examples of using pod anti-affinity include:\\n\\t+ Keeping two sets of interfering pods from running on the same node.\\n\\t+ Forcing the Scheduler to spread pods across different availability zones or regions.\\n\\n**Key Terms:**\\n\\n* Pod anti-affinity\\n* `podAntiAffinity` property\\n* Label selector\\n* Topology key',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. The article discusses how to use Kubernetes\\' \"anti-affinity\" feature to prevent pods from being scheduled on the same node.\\n2. A deployment is created with 5 replicas, and pod anti-affinity is configured to ensure that no two frontend pods are scheduled on the same node.\\n\\n**Important Details:**\\n\\n* To configure pod anti-affinity, you use the `podAntiAffinity` property in the deployment\\'s YAML file.\\n* The `requiredDuringSchedulingIgnoredDuringExecution` property is used to specify a hard requirement for pod anti-affinity.\\n* When creating the deployment, only two frontend pods are scheduled (one on each node), and three remaining pods are pending because they cannot be scheduled on the same nodes as existing frontend pods.\\n* An alternative approach using \"preferential\" pod anti-affinity is also discussed, which allows for a soft requirement instead of a hard one.\\n\\n**Key Takeaways:**\\n\\n* Pod anti-affinity can be used to prevent pods from being scheduled on the same node.\\n* Hard requirements for pod anti-affinity should be used in scenarios where it\\'s critical that no two pods are scheduled on the same machine.\\n* Soft requirements (using `preferredDuringSchedulingIgnoredDuringExecution`) may be sufficient in cases where it\\'s not a big problem if multiple pods run on the same node.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. **Taints**: A node can be \"tainted\" to prevent pods from being scheduled on it, unless they tolerate that taint.\\n2. **Node Affinity**: Allows specifying which nodes a pod should be scheduled to, either as a hard requirement or preference.\\n3. **Pod Affinity**: Deploys pods to the same node where another pod is running (based on labels).\\n4. **Pod Anti-Affinity**: Keeps certain pods away from each other.\\n\\n**Important Details:**\\n\\n* There are three types of taints: NoSchedule, Prefer-NoSchedule, and NoExecute.\\n* The topologyKey property in pod affinity specifies how close the pod should be deployed to another pod (e.g., same node, rack, availability zone, or region).\\n* Node affinity can specify a hard requirement or preference for scheduling pods on specific nodes.\\n* Pod anti-affinity can be used to keep certain pods away from each other.\\n\\n**Context:** This text appears to be part of a chapter on advanced scheduling in Kubernetes.',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* This chapter will cover best practices for developing applications on Kubernetes.\\n* It will focus on combining individual resources to create a typical application and making it run smoothly.\\n\\n**Important Details:**\\n\\n* The chapter will explain how to:\\n\\t+ Identify which Kubernetes resources are used in a typical application\\n\\t+ Add lifecycle hooks (post-start and pre-stop) to pods\\n\\t+ Properly terminate an app without breaking client requests\\n\\t+ Make apps easy to manage on Kubernetes\\n\\t+ Use init containers in a pod\\n\\t+ Develop locally with Minikube\\n\\nOverall, this chapter aims to provide guidance on how to create efficient and manageable applications on Kubernetes.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* A typical application consists of various Kubernetes components, which are brought together to create an app manifest.\\n* The app manifest contains one or more Deployment and/or StatefulSet objects, which include pod templates, containers, liveness probes, readiness probes, and services.\\n\\n**Important Details:**\\n\\n* Pod templates reference two types of Secrets:\\n\\t+ Image pull secrets for pulling container images from private registries\\n\\t+ Secrets used directly by the process running inside the pods\\n* Secrets are usually not part of the app manifest and are configured by the operations team.\\n* ServiceAccounts are assigned to individual pods and reference Secrets.\\n* Other resources that may be included in a typical application include:\\n\\t+ Services (LoadBalancer or NodePort-type)\\n\\t+ Ingress resources for external access\\n\\t+ ReplicaSets, Endpoints, Health probes, Environment variables, Volume mounts, Resource reqs/limits, Horizontal Pod Autoscaler, and more.\\n\\nOverall, the text provides an overview of the various components that come together to create a typical Kubernetes application.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The lifecycle of a pod, which is a fundamental resource in Kubernetes, involves various stages including creation, execution, and termination.\\n2. Pods can be initialized with ConfigMaps, which are used to set environment variables or mounted as volumes.\\n3. Additional volumes such as emptyDir, gitRepo, and persistentVolumeClaim (PVC) may be used depending on the application's requirements.\\n4. Jobs, CronJobs, DaemonSets, and HorizontalPodAutoscalers (HPAs) may also be part of an application deployment.\\n5. Cluster administrators create LimitRange and ResourceQuota objects to control compute resource usage.\\n\\n**Important Details:**\\n\\n1. Pods can be killed and relocated by Kubernetes due to scale-down requests or other reasons.\\n2. Applications running in pods must expect to be killed and relocated, unlike those running in traditional VMs.\\n3. Kubernetes controllers automatically create additional objects such as service Endpoints, ReplicaSets, and actual pods.\\n4. Resources are often labeled with labels and annotations for organization and management purposes.\\n\\n**Key Takeaways:**\\n\\n1. Understanding the lifecycle of a pod is crucial for developing applications that can take advantage of their environment.\\n2. Applications running in pods must be designed to handle frequent relocation and termination by Kubernetes.\",\n",
       "  \"Here is a summary of the main points and important details:\\n\\n**Key Takeaways:**\\n\\n1. **Apps need to handle frequent reconfiguration**: Applications should allow for being moved relatively often, as Kubernetes may reschedule them.\\n2. **Stateful apps require special handling**: Stateful applications cannot handle changes in IP address, hostname, or persistent state, but can be run through a StatefulSet to ensure consistency.\\n3. **Data written to disk may disappear**: Data written by an app to disk may not be available after the app is started in a new pod, unless persistent storage is mounted at the same location.\\n\\n**Important Details:**\\n\\n* Kubernetes reschedules pods and containers frequently, which can lead to changes in IP address, hostname, or persistent state.\\n* Stateful apps should use a StatefulSet to ensure consistency across container restarts.\\n* Data written by an app to disk may disappear even within a single pod's lifetime due to container restarts.\\n* Using at least a pod-scoped volume can help preserve data across container restarts.\\n\\n**Recommendations:**\\n\\n* Application developers should design their apps to handle frequent reconfiguration and changes in IP address, hostname, or persistent state.\\n* Stateful apps should use a StatefulSet to ensure consistency across container restarts.\\n* Apps that write data to disk should consider using persistent storage to preserve data across container restarts.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. When a container in a pod crashes or is killed, its lifecycle begins again with a new container and process.\\n2. Files written to the container\\'s file system are lost when the container is restarted.\\n\\n**Important Details:**\\n\\n* To persist data across container restarts, a \"volume\" can be used to mount a shared filesystem between containers in the same pod.\\n* When using a volume, files written by one container can be read and used by another container in the same pod, even if the first container crashes or is killed.\\n\\n**Key Concepts:**\\n\\n* Pod lifecycle\\n* Container restarts\\n* Filesystem persistence\\n* Volumes',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Using volumes to preserve files across container restarts can be problematic if data corruption causes the new process to crash, leading to a continuous crash loop.\\n2. Kubernetes will keep restarting a pod\\'s container indefinitely if it keeps crashing, increasing the time between restarts exponentially until it reaches five minutes.\\n3. Even if a pod is part of a ReplicaSet or similar controller, it won\\'t be automatically removed and rescheduled if its container crashes.\\n\\n**Important Details:**\\n\\n* If a pod\\'s container crashes, Kubernetes will keep restarting it indefinitely, with increasingly longer intervals between restarts (up to five minutes).\\n* The pod will remain in the \"Running\" state, but its container will not be running.\\n* Even if a pod is part of a ReplicaSet or similar controller, it won\\'t be automatically removed and replaced with a new instance if its container crashes.\\n* This can lead to a situation where a ReplicaSet has fewer than the desired number of replicas (e.g., three) because one of the pods is effectively dead.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Understanding the lifecycle of pods in Kubernetes, specifically when they crash and restart.\\n2. The importance of replica count matching the desired number of pods.\\n3. Starting pods in a specific order for multi-pod applications.\\n\\n**Important Details:**\\n\\n* When a pod crashes, it will be restarted every 5 minutes by the Kubelet in an attempt to resolve the underlying issue.\\n* If a pod is crashing repeatedly, Kubernetes will not take any action because the current number of replicas matches the desired replica count.\\n* There is no built-in way to tell Kubernetes to run certain pods first and then start the rest when the first pods are ready.\\n* To manage dependencies between apps, ops personnel can manually post manifests in a specific order, but this approach has limitations.\\n\\n**Key Takeaways:**\\n\\n* Kubernetes manages pods independently of their dependencies, which can lead to issues with multi-pod applications.\\n* There is no straightforward way to start pods in a specific order using Kubernetes' built-in features.\",\n",
       "  'Here\\'s a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. In Kubernetes, a pod can have multiple containers, including \"init containers\" that run before the main containers.\\n2. Init containers are used to initialize the pod by writing data to volumes or performing other setup tasks.\\n3. Init containers can be used to delay the start of the main container(s) until certain preconditions are met.\\n\\n**Important Details:**\\n\\n1. The order in which objects (such as pods and services) are listed in a YAML/JSON file does not guarantee their execution order.\\n2. Init containers run sequentially and only after the last one completes will the main containers start.\\n3. An example is given of using an init container to delay the start of a pod\\'s main container until a required service (fortune Service) is up and running.\\n4. To add an init container to a pod, use the `spec.initContainers` field in the pod spec YAML/JSON file.\\n\\n**Key Concepts:**\\n\\n* Init containers\\n* Pod initialization\\n* Delaying main container start until preconditions are met',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When deploying a pod, only its init container is started initially.\\n2. The main container won't run until dependencies (e.g., Services) are ready.\\n3. It's better to write apps that don't require all dependencies to be ready before starting up.\\n4. Apps should handle internal dependencies not being ready and signal readiness issues through readiness probes.\\n\\n**Important Details:**\\n\\n1. To view the log of an init container, use `kubectl logs` with the `-c` switch specifying the name of the init container.\\n2. The main container won't run until the Service and fortune-server pod are deployed (found in the `fortune-server.yaml` file).\\n3. Kubernetes provides lifecycle hooks for pods:\\n\\t* Post-start hooks: executed when a container starts\\n\\t* Pre-stop hooks: executed before a container stops\\n4. Lifecycle hooks can be specified per container, unlike init containers which apply to the whole pod.\\n5. Lifecycle hooks can execute commands or perform HTTP GET requests against URLs.\\n\\n**Best Practices:**\\n\\n1. Write apps that don't require all dependencies to be ready before starting up.\\n2. Handle internal dependencies not being ready and signal readiness issues through readiness probes.\",\n",
       "  'Here\\'s a summary of the main points and important details from the text:\\n\\n**Post-Start Container Lifecycle Hook**\\n\\n* A post-start hook is executed immediately after a container\\'s main process is started.\\n* It allows you to run additional commands without modifying the application code, which is useful when running an application developed by someone else.\\n* The hook runs in parallel with the main process and does not wait for it to complete.\\n* If the hook fails or returns a non-zero exit code, the main container will be killed.\\n\\n**Key Points**\\n\\n* A post-start hook affects the container\\'s state until it completes, keeping it in a \"Waiting\" state with reason \"ContainerCreating\".\\n* The pod\\'s status will be \"Pending\" instead of \"Running\" while the hook is running.\\n* Debugging lifecycle hooks can be painful because output from the process started by the hook cannot be seen.\\n\\n**Example**\\n\\n* A pod manifest containing a post-start hook is shown, which executes a shell script or binary executable file stored in the container image.\\n* The example demonstrates how to use a post-start hook to run additional commands when an application starts.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Pod Lifecycle Hooks**\\n\\nThe text discusses two types of lifecycle hooks in Kubernetes: PostStart and PreStop.\\n\\n**PostStart Hook**\\n\\n* A PostStart hook is executed immediately after a container starts.\\n* If the hook fails, it will be reported as a \"FailedSync\" event with an error message indicating why the hook failed (e.g. exit code 15).\\n* The standard and error outputs of command-based post-start hooks are not logged anywhere by default.\\n* To debug issues with post-start hooks, you can use `kubectl exec` to examine log files or mount an emptyDir volume for logging.\\n\\n**PreStop Hook**\\n\\n* A PreStop hook is executed immediately before a container is terminated.\\n* The Kubernetes system will run the pre-stop hook and only then send a SIGTERM signal to the process (and later kill it if it doesn\\'t terminate gracefully).\\n* PreStop hooks can be used to initiate a graceful shutdown of the container, perform arbitrary operations before shutdown, or handle third-party apps that don\\'t have built-in shutdown mechanisms.\\n\\n**Key Takeaways**\\n\\n* Lifecycle hooks are useful for customizing pod behavior and handling errors.\\n* PostStart and PreStop hooks have different use cases and behaviors.\\n* Debugging lifecycle hook issues requires careful examination of log files and container events.',\n",
       "  'Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. A \"pre-stop\" hook can be defined in a pod\\'s YAML file to perform an action before the container is terminated.\\n2. The pre-stop hook can be used to send an HTTP GET request, among other actions.\\n\\n**Important Details:**\\n\\n* The pre-stop hook will run regardless of its result (e.g., error response code or non-zero exit code).\\n* If the pre-stop hook fails, a \"FailedPreStopHook\" warning event will be generated, but it may not be noticeable since the pod is deleted soon after.\\n* A common mistake is to define a pre-stop hook solely to send a SIGTERM signal to an app, which can be achieved by configuring the container image to pass the signal to the app process.\\n* Instead of adding a pre-stop hook, it\\'s recommended to fix the issue by handling the signal in the shell script running as the main container process and passing it on to the app.\\n\\n**Key Takeaways:**\\n\\n* Use pre-stop hooks judiciously, especially if the successful completion is critical to system operation.\\n* Be aware of potential issues with SIGTERM signals not being received by apps inside containers.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Understanding Pod Lifecycle**\\n\\n* When a pod is shut down, it's triggered by deleting the Pod object through the API server.\\n* The deletion process involves setting a `deletionTimestamp` field in the Pod object, which marks the pod as terminating.\\n\\n**Container Termination Sequence**\\n\\n* The Kubelet terminates each container in the pod, giving them time to shut down gracefully (called the termination grace period).\\n* The sequence of events is:\\n\\t1. Run the pre-stop hook (if configured) and wait for it to finish.\\n\\t2. Send a SIGTERM signal to the main process of the container.\\n\\t3. Wait until the container shuts down cleanly or until the termination grace period runs out.\\n\\t4. If still running, forcibly kill the process with SIGKILL.\\n\\n**Important Details**\\n\\n* Lifecycle hooks (post-start and pre-stop) target containers, not pods.\\n* Pre-stop hooks should not be used for actions that need to be performed when the pod is terminating, as they may be called multiple times during the pod's lifetime.\\n* The termination grace period can be configured per pod.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The termination grace period can be configured in the pod spec to allow containers time to terminate cleanly before being killed forcibly.\\n2. The default termination grace period is 30 seconds, but this can be overridden when deleting a pod using `kubectl delete`.\\n3. Forcing the deletion of a pod without waiting for its containers to shut down can cause issues with StatefulSets.\\n\\n**Important Details:**\\n\\n* Setting the termination grace period too low can cause issues if your process takes longer than expected to clean up.\\n* Using `--force` and setting the grace period to zero when deleting a pod can cause two instances of the same pod to run at the same time, which may malfunction the stateful cluster.\\n* Applications should react to a SIGTERM signal by starting their shut-down procedure and terminating when it finishes.\\n* If an application cannot predict how long its shut-down procedure will take, it should use a pre-stop hook to notify the app to shut down.\\n\\n**Key Terms:**\\n\\n* Termination grace period\\n* SIGTERM signal\\n* Pre-stop hook\\n* StatefulSets',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When terminating a pod, it's not recommended for the pod to start migrating its data immediately upon receiving a termination signal (SIGTERM or pre-stop hook).\\n2. This is because there are no guarantees that the shut-down procedure will finish before the process is killed.\\n3. A dedicated, constantly running pod or CronJob resource can be used to migrate orphaned data from terminated pods.\\n\\n**Important Details:**\\n\\n* When a node fails during the shut-down sequence, the Kubelet won't restart the shut-down procedure.\\n* StatefulSets don't help with this problem because scaling down a StatefulSet leaves PersistentVolumeClaims orphaned.\\n* A data-migrating pod can be used in conjunction with StatefulSets to prevent data loss.\\n* The data-migrating pod can be configured to wait before performing the migration, allowing time for the stateful pod to come up again.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* Ensuring client requests are handled properly when pods start up or shut down is crucial for preventing broken connections.\\n* Kubernetes does not prevent broken connections by itself, so apps need to follow specific rules to handle connections correctly.\\n\\n**Important Details:**\\n\\n* When a pod starts up, it needs to signal to Kubernetes that it's ready before becoming a service endpoint and receiving client requests.\\n* If no readiness probe is specified in the pod spec, the pod will be considered ready immediately and may receive requests before being fully prepared to handle them.\\n* To prevent broken connections, apps should add an HTTP GET readiness probe that returns success only when the app is ready to properly handle incoming requests.\\n\\n**Key Takeaways:**\\n\\n* Apps need to follow specific rules to ensure client requests are handled properly when pods start up or shut down.\\n* Adding a readiness probe can help prevent broken connections and ensure clients receive successful responses.\",\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Ensuring client requests are handled properly when a pod is shut down.\\n2. Understanding the sequence of events that occur when a pod is deleted in a Kubernetes cluster.\\n\\n**Important Details:**\\n\\n* When a pod is deleted, its containers should start shutting down cleanly after receiving a SIGTERM signal or executing a pre-stop hook.\\n* However, this may not ensure all client requests are handled properly, particularly for persistent HTTP connections and requests that have already been received but haven't completed yet.\\n* The sequence of events when a pod is deleted involves the API server modifying etcd, notifying watchers (Kubelet and Endpoints controller), and removing the pod from iptables.\\n* Two parallel sequences of events occur:\\n\\t+ Sequence A: The Kubelet stops containers, notifies kube-proxy, and removes the pod as an endpoint.\\n\\t+ Sequence B: The Endpoints controller receives notification of the pod deletion, modifies the endpoint, and removes the pod from iptables.\\n\\n**Key Takeaways:**\\n\\n* Handling client requests properly when a pod is shut down requires careful consideration of persistent HTTP connections and incomplete requests.\\n* Understanding the sequence of events that occur when a pod is deleted in a Kubernetes cluster is crucial for ensuring proper behavior.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When a pod is deleted, two sequences of events occur in parallel:\\n\\t* The Kubelet initiates the shutdown sequence for the app running in the pod.\\n\\t* The Endpoints controller removes the pod as an endpoint from all services it's part of.\\n2. In the first sequence (A), the Kubelet sends a SIGTERM signal to the app, which can take a relatively short time to complete.\\n3. In the second sequence (B), the Endpoints controller updates the iptables rules on worker nodes to prevent new connections from being forwarded to the terminating pod.\\n\\n**Important Details:**\\n\\n* Removing the iptables rules has no effect on existing connections; clients already connected to the pod can continue to send requests through those existing connections.\\n* The time it takes for the Kubelet to shut down the app's process is slightly shorter than the time required for the iptables rules to be updated.\\n* The chain of events leading to iptables rule updates is longer, involving multiple steps and communication between components in the Kubernetes control plane.\",\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Problem:** When a pod receives a SIGTERM signal to terminate, it may still receive client requests before the iptables rules are updated on all nodes by the kube-proxy. This can cause clients to experience \"Connection Refused\" errors.\\n\\n**Initial Misconception:** Adding a readiness probe to the pod was thought to solve this problem, but in reality, it has no bearing on the process.\\n\\n**Correct Solution:** The correct solution is for the pod to keep accepting connections even after receiving the termination signal until all kube-proxies have finished updating the iptables rules. This also includes waiting for any Ingress controllers, load balancers, or client-side load-balancing to stop forwarding connections to the pod.\\n\\n**Challenges:**\\n\\n* It\\'s impossible to wait for all clients to notify the pod that they will no longer forward connections.\\n* Even if possible, it\\'s unclear how long to wait and what happens if one of them doesn\\'t respond.\\n* Adding a delay can improve the user experience, but there is no guarantee it will suffice every time.\\n\\n**Recommendation:** A 5-10 second delay should be sufficient in most situations, but a longer delay may be necessary depending on the specific use case.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Properly Shutting Down an Application**\\n\\nTo ensure a smooth shutdown, follow these steps:\\n\\n1. Wait for a few seconds to allow any active requests to finish.\\n2. Close all keep-alive connections not in the middle of a request.\\n3. Stop accepting new connections.\\n4. Wait for all active requests to complete.\\n5. Then shut down completely.\\n\\n**Why This is Important**\\n\\nFailing to follow these steps can cause issues, such as:\\n\\n* The container or pod being shown in lists long after it has been deleted.\\n* Broken connections and frustrated users.\\n\\n**Pre-Stop Hook Example**\\n\\nTo simplify the shutdown process, consider adding a pre-stop hook that waits for a few seconds before shutting down. This can be done using a command like \"sleep 5\" (wait for 5 seconds).\\n\\n**Key Points**\\n\\n* Properly shutting down an application is not as simple as immediately exiting the process upon receiving a termination signal.\\n* A delay of a few seconds allows active requests to finish and prevents broken connections.\\n* The pre-stop hook example provides a simple way to implement this delay.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To make apps easy to run and manage in Kubernetes, they should be designed to handle clients nicely without requiring code modifications.\\n2. Container images for apps should be kept small and minimal, without unnecessary files or cruft.\\n3. Proper image tagging and usage of `imagePullPolicy` are crucial for managing container images.\\n\\n**Important Details:**\\n\\n* Keeping container images small is essential for fast deployment and scaling in Kubernetes.\\n* Go-based container images can be extremely small and efficient, but may require additional tools to be included.\\n* Using the latest image tag in pod manifests can cause problems with versioning and rolling back to previous versions.\\n* Properly tagging images and using `imagePullPolicy` wisely is necessary for managing container images effectively.\\n\\n**Key Takeaways:**\\n\\n* Design apps to handle clients nicely without code modifications.\\n* Keep container images small and minimal, without unnecessary files or cruft.\\n* Use proper image tagging and `imagePullPolicy` usage to manage container images effectively.',\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. Use specific version tags instead of \"latest\" for Docker images.\\n2. Label all resources (not just Pods) with multiple labels to enable selection across different dimensions.\\n3. Add annotations to resources to provide additional information, such as contact details, dependencies between services, build and version info, and metadata.\\n\\n**Important Details:**\\n\\n* Using mutable tags requires setting the imagePullPolicy field to \"Always\", which can slow down pod startup and prevent pods from starting up if the registry cannot be contacted.\\n* Labels should include dimensions like application name, tier, environment, version, release type, tenant, and shard.\\n* Annotations can provide information on dependencies between services, build and version info, and metadata used by tooling or graphical user interfaces.\\n\\n**Key Takeaways:**\\n\\n* Properly labeling and annotating resources makes it easier to manage running applications and troubleshoot issues when they arise.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Point:** Making it easy for Kubernetes operators to manage and troubleshoot applications by providing necessary debug information.\\n\\n**Key Points:**\\n\\n1. **Include debug info in log files**: Make sure your application's log files contain all necessary debug information to make troubleshooting easier.\\n2. **Use Kubernetes feature to show termination reason**: Use the `terminationMessagePath` field in the container definition to write a termination message to a specific file, which can be read by the Kubelet and displayed in the output of `kubectl describe pod`.\\n3. **Override default termination message path**: You can override the default path `/dev/termination-log` by setting the `terminationMessagePath` field.\\n4. **Example usage**: Create a pod with a container that writes a termination message to a specific file, and then use `kubectl describe pod` to see the reason why the container died without having to inspect its logs.\\n\\n**Important Details:**\\n\\n* The default path for the termination message file is `/dev/termination-log`.\\n* You can override this path by setting the `terminationMessagePath` field in the container definition.\\n* When a container terminates, it writes the termination message to the specified file just before exiting.\",\n",
       "  'Here is a summary of the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes does not currently provide a mechanism to display app-specific status messages for running containers.\\n2. A similar mechanism to the \"I\\'ve had enough\" message written to the file /var/termination-reason can be useful for showing app-specific status messages of running containers.\\n\\n**Important Details:**\\n\\n* The \"I\\'ve had enough\" message is written to a file by the container and can be viewed in the Last State section.\\n* If a container doesn\\'t write a message to a file, you can set the terminationMessagePolicy field to FallbackToLogsOnError to use the last few lines of the container\\'s log as its termination message (only when the container terminates unsuccessfully).\\n* Applications should write to standard output instead of files for easy logging with kubectl logs.\\n* If an application logs to a file, you can display the log file using kubectl exec <pod> cat <logfile>.\\n* You can copy files from and into a container using kubectl cp command.',\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. In Kubernetes, it's essential to have a centralized logging solution to collect and store logs from all containers in a cluster.\\n2. Kubernetes itself doesn't provide centralized logging capabilities; additional components are needed to provide this functionality.\\n3. Deploying centralized logging solutions is relatively easy and can be done by deploying YAML/JSON manifests.\\n\\n**Important Details:**\\n\\n1. The EFK stack (ElasticSearch, FluentD, Kibana) is a popular variation of the ELK stack used for centralized logging in Kubernetes.\\n2. In the EFK stack:\\n\\t* FluentD agents run on each cluster node to collect logs from containers and deliver them to ElasticSearch.\\n\\t* ElasticSearch stores logs persistently.\\n\\t* Kibana provides a web interface for viewing and analyzing logs.\\n3. The three components of the EFK stack can be deployed as pods in a Kubernetes cluster, with FluentD agents running as DaemonSets.\\n4. Helm charts can be used to deploy the EFK stack instead of creating custom YAML manifests.\\n\\n**Key Takeaways:**\\n\\n* Centralized logging is crucial for managing and analyzing logs from containers in a Kubernetes cluster.\\n* The EFK stack provides a popular solution for centralized logging, with FluentD agents collecting logs, ElasticSearch storing them persistently, and Kibana providing a web interface for analysis.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Handling Multi-Line Log Statements**\\n\\n* The FluentD agent stores each line of a log file as an entry in ElasticSearch, but this can cause issues with multi-line log statements (e.g. exception stack traces).\\n* To solve this problem, apps can output JSON instead of plain text, allowing for single-entry logging in Kibana.\\n* However, this approach makes viewing logs with `kubectl logs` less human-friendly.\\n* A better solution is to keep outputting human-readable logs to standard output while writing JSON logs to a file and processing them with FluentD.\\n\\n**Best Practices for Development and Testing**\\n\\n* When developing an app that will run in a production Kubernetes cluster, it's not necessary to run the app in Kubernetes during development.\\n* Instead, developers can develop and run apps on their local machine, replicating the Kubernetes environment if needed.\\n* This approach avoids the need to rebuild and redeploy the app after each minor change.\\n\\n**Connecting to Backend Services**\\n\\n* In production, apps that connect to a backend Service can use environment variables (`BACKEND_SERVICE_HOST` and `BACKEND_SERVICE_PORT`) to find the Service's coordinates.\\n* During development, these variables can be set manually on the local machine, or the Service can be made accessible externally by changing it to a NodePort or LoadBalancer-type Service.\",\n",
       "  \"Here is a summary of the text that includes the main points and important details:\\n\\n**Connecting to the API Server**\\n\\n* When developing an app that requires access to the Kubernetes API server, it can easily connect from outside the cluster.\\n* If the app uses a ServiceAccount's token for authentication, you can copy the Secret files to your local machine using `kubectl cp`.\\n* Alternatively, you can use an ambassador container and run `kubectl proxy` on your local machine to connect to the API server.\\n\\n**Running Inside a Container During Development**\\n\\n* You don't have to rebuild the container image every time you make changes.\\n* Instead, you can mount your local filesystem into the container using Docker volumes or hostPath volumes.\\n* This allows you to restart the container without rebuilding the image.\\n\\n**Using Minikube in Development**\\n\\n* Running your app inside Kubernetes during development is not necessary, but it can be useful for seeing how the app behaves in a true Kubernetes environment.\\n* Minikube is a valuable method for trying out your app in Kubernetes and developing resource manifests.\\n* You can mount your local filesystem into the Minikube VM using `minikube mount` and then mount it into your containers.\\n\\n**Additional Tips**\\n\\n* When using Minikube, you can use the Docker daemon inside the VM to build container images instead of building them locally.\\n* This allows you to avoid pushing images to a registry and pulling them back down.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To develop apps on Minikube, set the DOCKER_HOST environment variable by running `eval $(minikube docker-env)` to access the Docker daemon inside the VM.\\n2. Build images locally and use them in pods without pushing to a registry or pulling from an external registry.\\n3. If building images locally is not possible, copy the image over to the Minikube VM using `docker save` and `docker load`.\\n4. Combine Minikube with a proper Kubernetes cluster for development, testing, and deployment.\\n\\n**Important Details:**\\n\\n* When building images locally on Minikube, do not set `imagePullPolicy` to Always in your pod spec, as this would cause the image to be pulled from an external registry again.\\n* Use a Version Control System (VCS) to store resource manifests, enabling code reviews, audit trails, and rollbacks.\\n* Run `kubectl apply` after each commit to reflect changes in deployed resources.\\n\\n**Additional Information:**\\n\\n* Minikube allows for virtually unlimited development possibilities, including combining with remote Kubernetes clusters.\\n* Kubernetes abstracts away underlying infrastructure from apps, making it easy to move workloads between local and remote clusters.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Best practices for development and testing include using an agent to periodically check out manifests from a Version Control System (VCS) and run the apply command, allowing for simple management of running apps.\\n2. A tool called kube-applier was developed by Box to facilitate this process.\\n3. Multiple branches can be used to deploy manifests to different environments (e.g., development, QA, staging, production).\\n4. An alternative to writing YAML/JSON manifests is Ksonnet, a library built on top of Jsonnet.\\n\\n**Important Details:**\\n\\n* Kube-applier allows for automated deployment and management of apps without manual interaction with the Kubernetes API server.\\n* Ksonnet uses parameterized JSON fragments to build complete JSON manifests, reducing code repetition.\\n* An example listing shows how to define a container and deployment using Ksonnet, which can be converted to a full JSON Deployment manifest using the jsonnet command.\\n\\n**Key Takeaways:**\\n\\n* Using an agent with kube-applier can simplify app management.\\n* Ksonnet offers an alternative to writing YAML/JSON manifests, making it easier to build complete Kubernetes resource JSON manifests.',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. The power of Ksonnet and Jsonnet lies in their ability to define higher-level fragments and ensure consistent, duplication-free manifests.\\n2. Employing Continuous Integration and Continuous Delivery (CI/CD) pipelines can automate the deployment of Kubernetes resources, application binaries, container images, and resource manifests.\\n3. A CI/CD pipeline can be set up using tools like Jenkins and Fabric8.\\n\\n**Important Details:**\\n\\n* Ksonnet and Jsonnet are available at https://github.com/ksonnet/ksonnet-lib\\n* The Fabric8 project (http://fabric8.io) is an integrated development platform for Kubernetes that includes Jenkins and other tools.\\n* Google Cloud Platform's online labs on continuous deployment on Kubernetes can be found at https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.\\n\\n**Summary of the Chapter:**\\n\\nThe chapter aims to provide a deeper understanding of how Kubernetes works and how to build apps that feel right at home when deployed to a Kubernetes cluster. The main points covered include:\\n\\n* How all resources in the book come together to represent a typical application running in Kubernetes.\\n* The difference between apps that are rarely moved between machines and apps running as pods, which are relocated more frequently.\\n* The importance of not relying on a specific start-up order for multi-component apps (or microservices).\\n* The use of init containers to initialize a pod or delay the start of the main containers until a precondition is met.\\n* Container lifecycle hooks and when to use them.\\n* Understanding the consequences of the distributed nature of Kubernetes components and its eventual consistency model.\\n* How to make apps shut down properly without breaking client connections.\",\n",
       "  \"Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* The chapter provides tips for managing apps more efficiently, including:\\n\\t+ Keeping image sizes small\\n\\t+ Adding annotations and labels to resources\\n\\t+ Improving termination error messages\\n* It teaches how to develop and run Kubernetes apps locally or in Mini-kube before deploying them on a multi-node cluster.\\n* The final chapter will cover extending Kubernetes with custom API objects and controllers, and creating Platform-as-a-Service solutions.\\n\\n**Important Details:**\\n\\n* The chapter is part of a larger guide or tutorial\\n* It focuses on practical tips for app management and development\\n* It sets up the stage for the final chapter's more advanced topics\",\n",
       "  \"Here is a summary of the text:\\n\\n**Extending Kubernetes**\\n\\nThis section covers how to take Kubernetes to the next level by defining custom API objects, creating controllers for those objects, and building platform-as-a-service (PaaS) solutions on top of it. The main points include:\\n\\n* **Defining Custom API Objects**: Learn how to add custom objects to Kubernetes, create a controller for them, and add custom API servers.\\n* **Self-Provisioning Services**: Explore the Service Catalog feature that allows self-provisioning of services within Kubernetes.\\n* **Real-World Examples**: See how others have extended Kubernetes to build PaaS solutions, including:\\n\\t+ Red Hat's OpenShift Container Platform\\n\\t+ Deis Workflow and Helm\\n\\nThese topics are important for users who want to take their Kubernetes knowledge to the next level and build more complex applications.\",\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Defining Custom API Objects**\\n\\nAs Kubernetes evolves, high-level objects will be created to represent applications or software services. These custom objects can be managed using a custom controller, which creates low-level objects (e.g., Deployments, Services) based on them.\\n\\n**CustomResourceDefinitions (CRDs)**\\n\\nTo define a new resource type, you create a CustomResourceDefinition object (CRD) and post it to the Kubernetes API server. This allows users to create instances of the custom resource using JSON or YAML manifests.\\n\\n**Key Points**\\n\\n* CRDs were previously defined through Third-PartyResource objects, but these were removed in Kubernetes 1.8.\\n* A controller must be deployed with each CRD to make something tangible happen in the cluster.\\n* Custom resources can be used to simplify user interactions with the cluster (e.g., running static websites without dealing with Pods and Services).\\n\\n**Example**\\n\\nThe text provides an example of creating a custom resource called \"Website\" that allows users to run static websites easily. The Website resource will contain information about the website\\'s name and source, and when created, it will trigger the deployment of a web server pod and Service using a custom controller.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. To create custom objects in Kubernetes, you need to define them using a CustomResourceDefinition (CRD) object.\\n2. A CRD defines the structure and behavior of a new resource type, such as a \"Website\" object.\\n\\n**Important Details:**\\n\\n* The CRD object must include an `apiVersion` field with a specific value (`apiextensions.k8s.io/v1beta1`) to indicate that it\\'s a custom resource definition.\\n* The CRD object must also specify the scope of the custom resource, which in this case is \"Namespaced\" (i.e., each instance will be created within its own namespace).\\n* To create instances of the custom resource (e.g., a \"Website\" object), you need to post a YAML file that defines the resource to the Kubernetes API server.\\n* The custom resource definition must include a `kind` field with the name of the new resource type (e.g., \"Website\").\\n* When creating an instance of the custom resource, you\\'ll also need to specify additional fields, such as the Git repository containing the website\\'s files.\\n\\n**Key Concepts:**\\n\\n* CustomResourceDefinition (CRD) object\\n* Namespaced scope for custom resources\\n* `apiVersion` field in CRD objects\\n* `kind` field in custom resource definitions',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Defining Custom API Objects**\\n\\n* A Custom Resource Definition (CRD) was created using the `website-crd-definition.yaml` file to define a custom \"Website\" resource.\\n* The CRD has a unique name, `websites.extensions.example.com`, which is used to prevent name clashes with other resources.\\n* When creating instances of the custom Website resource, the `apiVersion` property must be set to `extensions.example.com/v1`.\\n\\n**Creating an Instance of a Custom Resource**\\n\\n* A YAML manifest was created to define a single instance of the custom Website resource, named \"kubia\".\\n* The manifest specifies the API group and version (`extensions.example.com/v1`) as well as the name of the website instance.\\n* To create the Website object, the `kubectl` command was used with the `-f` flag to specify the YAML file: `$ kubectl create -f kubia-website.yaml`.\\n\\n**Key Points**\\n\\n* The custom API group and version are specified in the CRD definition.\\n* When creating instances of a custom resource, the `apiVersion` property must match the API group and version defined in the CRD.\\n* The name of the custom resource instance can be specified using various forms (singular/plural).',\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n1. A custom \"Website\" object was created and stored in the Kubernetes API server.\\n2. The object can be retrieved, listed, and described using `kubectl`.\\n3. Custom resource instances can be deleted using `kubectl delete`.\\n\\n**Important Details:**\\n\\n* When retrieving a custom resource instance, all metadata fields are initialized by Kubernetes, just like with other resources.\\n* Deleting a custom resource instance deletes the specific instance, not the CustomResourceDefinition (CRD) itself.\\n* The CRD is still in place and can be used to create additional instances of the custom object.\\n\\n**Key Takeaways:**\\n\\n* Creating a CustomResourceDefinition allows for storing, retrieving, and deleting custom objects through the Kubernetes API server.\\n* A controller will need to be created to make these custom objects do something useful.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Custom API objects can be created to store data, but in this case, the goal is to create a web server pod when a Website object is created.\\n2. To achieve this, a custom controller needs to be built and deployed to watch for the creation of Website objects and create a Service and web server Pod (or Deployment) for each one.\\n\\n**Important Details:**\\n\\n* Custom controllers can automate the creation of resources based on specific conditions, such as the creation of a Website object.\\n* The controller will create a Deployment resource instead of an unmanaged Pod to ensure the pod is managed and survives node failures.\\n* A simple initial version of the website controller has been created, but it's not production-ready due to its oversimplification.\\n* The container image and source code for the controller are available on Docker Hub and GitHub, respectively.\\n\\n**Key Concepts:**\\n\\n* Custom API objects (e.g., Website)\\n* Custom controllers\\n* Deployments\\n* Services\\n* ConfigMaps\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The Website Controller starts by watching for changes to Website objects in the Kubernetes API server.\\n2. It does this by sending an HTTP GET request to the kubectl proxy process, which forwards the request to the API server.\\n3. When a new Website object is created, the API server sends a watch event to the controller.\\n4. The controller extracts information from the watch event and creates a Deployment and Service object.\\n\\n**Important Details:**\\n\\n* The Website Controller uses the kubectl proxy process as an ambassador to connect to the API server.\\n* The Deployment resource contains a template for a pod with two containers: one running an nginx server and another running a git-sync process.\\n* The git-sync process keeps a local directory synced with the contents of a Git repository.\\n* The Service is a NodePort Service, which exposes the web server pod through a random port on each node.\\n\\n**Key Concepts:**\\n\\n* Kubernetes API server\\n* kubectl proxy process (ambassador pattern)\\n* Deployment and Service objects\\n* git-sync process\\n* NodePort Service',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When a Website resource instance is deleted, the API server sends a DELETED watch event to the controller, which then deletes associated Deployment and Service resources.\\n2. The controller should periodically re-list all objects in case any watch events were missed.\\n3. Running the controller as a pod is recommended for production deployment.\\n\\n**Important Details:**\\n\\n* The current implementation of the controller has issues with watching API objects and may miss individual watch events.\\n* To run the controller in Kubernetes, a Deployment resource can be used to deploy it.\\n* An example Deployment resource (website-controller.yaml) is provided, which includes a single replica of the controller.\\n* The Deployment resource uses a pod template that includes containers for serving the website and syncing the Git repository.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Deploying a Kubernetes Controller**\\n\\nThe text describes deploying a Kubernetes controller, specifically a \"website-controller\", using a Deployment object. The key points are:\\n\\n1. **Deployment Object**: A Deployment object named \"website-controller\" is created to deploy a two-container pod.\\n2. **ServiceAccount**: A ServiceAccount named \"website-controller\" must be created before deploying the controller.\\n3. **Role-Based Access Control (RBAC)**: If RBAC is enabled, the controller needs a ClusterRoleBinding to access resources like Deployments and Services.\\n4. **ClusterRoleBinding**: A ClusterRoleBinding is created to bind the ServiceAccount to the cluster-admin ClusterRole.\\n5. **Deploying the Controller**: Once the ServiceAccount and ClusterRoleBinding are in place, the Deployment can be deployed.\\n6. **Testing the Controller**: The controller\\'s logs are checked to see if it has received a watch event for a Website resource.\\n\\nImportant details:\\n\\n* The controller runs under its own special ServiceAccount.\\n* The pod has two containers: one running the main controller and another acting as an ambassador container for simpler communication with the API server.\\n* The controller needs access to resources like Deployments and Services, which requires a ClusterRoleBinding.',\n",
       "  'Here is a summary of the text that includes the main points and important details:\\n\\n**Main Points:**\\n\\n1. A custom API object called \"Website\" was defined and deployed to a Kubernetes cluster.\\n2. The controller received an event, created a Service and Deployment for the website, and responded with a 201 Created response.\\n3. The resulting Pod, Service, and Deployment were verified using kubectl get commands.\\n\\n**Important Details:**\\n\\n* The Website object is accessible through a Service on port 32589 on all cluster nodes.\\n* Users can deploy static websites in seconds without knowing about Kubernetes resources, thanks to the custom Website resource.\\n* There are opportunities for improvement, such as updating the status section of the Website resource with the URL it\\'s accessible at or creating an Ingress object for each website.\\n* The API server does not validate the contents of the YAML file for the Website object, allowing users to create invalid objects.\\n\\n**Validation:**\\n\\n* It is not possible to add validation to the controller that would prevent invalid objects from being accepted by the API server.\\n* Instead, the controller can only validate the object when it receives it in a watch event and update the object with an error message if it\\'s invalid.',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. In Kubernetes, custom objects are not validated by default, which can lead to errors.\\n2. Custom object validation was introduced in Kubernetes version 1.8 as an alpha feature.\\n3. A better approach to adding support for custom objects is to implement a custom API server and have clients talk directly to it.\\n\\n**Important Details:**\\n\\n* In Kubernetes version 1.7, API server aggregation was introduced, allowing multiple API servers to be exposed at a single location.\\n* Clients can connect to the aggregated API without knowing that multiple API servers are handling different objects behind the scenes.\\n* Implementing a custom API server for custom objects allows for validation and eliminates the need for CustomResourceDefinitions (CRDs).\\n* Each API server is responsible for storing its own resources, which can be done using its own etcd instance or by using CRDs in the main API server.\\n\\n**Key Concepts:**\\n\\n* Custom object validation\\n* API server aggregation\\n* Custom API servers\\n* CustomResourceDefinitions (CRDs)\\n* etcd storage',\n",
       "  \"Here is a summary of the text, including the main points and important details:\\n\\n**Extending Kubernetes**\\n\\n* Custom resources can be stored in the core API server's etcd store by creating CRD (Custom Resource Definition) instances.\\n* To do this, a CRD object must be created first, followed by instances of the CRD.\\n\\n**Registering a Custom API Server**\\n\\n* A custom API server can be added to a cluster by deploying it as a pod and exposing it through a Service.\\n* An APIService resource is then deployed to integrate the custom API server with the main API server.\\n* The APIService resource specifies the group, version, and priority of the custom API.\\n\\n**Creating Custom Clients**\\n\\n* A custom CLI tool can be built to make deploying custom objects easier.\\n* This allows for dedicated commands for manipulating custom resources, similar to kubectl's resource-specific commands.\\n\\n**Important Details**\\n\\n* Features related to extending Kubernetes are currently being worked on intensively and may change after the book is published.\\n* For up-to-date information, refer to the Kubernetes GitHub repository at http://github.com/kubernetes.\\n* The Service Catalog API server will be one of the first additional API servers added to Kubernetes through API server aggregation.\\n\\n**Key Terms**\\n\\n* CRD (Custom Resource Definition)\\n* APIService\\n* Custom API server\\n* Service Catalog API server\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n* In Kubernetes, users need to manually provision services (e.g., databases) for their applications, which can be time-consuming and not self-service-friendly.\\n* The Kubernetes Service Catalog aims to provide a catalog of services that users can browse and provision themselves without manual intervention.\\n\\n**Important Details:**\\n\\n* The Service Catalog introduces four generic API resources:\\n\\t+ ClusterServiceBroker: describes an external system that can provision services\\n\\t+ ClusterServiceClass: describes a type of service that can be provisioned\\n\\t+ ServiceInstance: represents one instance of a provisioned service\\n\\t+ ServiceBinding: represents a binding between client pods and a ServiceInstance\\n* A cluster admin creates a ClusterServiceBroker resource for each service broker they want to make available in the cluster.\\n* When a user requests a service, they create a ServiceInstance resource and then a ServiceBinding to bind it to client pods.\\n\\n**Key Benefits:**\\n\\n* The Service Catalog enables users to provision services themselves without manual intervention, making Kubernetes more self-service-friendly.\\n* It simplifies the process of deploying applications that require specific services (e.g., databases).',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. The Kubernetes Service Catalog allows users to provision external services from service brokers.\\n2. The Service Catalog uses a distributed system architecture composed of three components: API Server, etcd storage, and Controller Manager.\\n\\n**Important Details:**\\n\\n* The Service Catalog creates four related resources (ServiceInstance, ClusterServiceClass, ServiceBroker, and ProvisionedService) by posting YAML/JSON manifests to the API server.\\n* These resources are stored in the Service Catalog's own etcd instance or use CustomResourceDefinitions in the main API server as an alternative storage mechanism.\\n* The Controller Manager runs controllers that interact with the Service Catalog API server and external service brokers, which provision services on behalf of the Service Catalog.\\n* External service brokers are registered by creating ServiceBroker resources in the Service Catalog API.\\n* Client pods can use the provisioned services to connect to external systems.\\n\\n**Key Components:**\\n\\n1. Service Catalog API Server\\n2. etcd storage\\n3. Controller Manager\\n4. ServiceBroker (external service broker)\\n5. ServiceInstance, ClusterServiceClass, and ProvisionedService resources\",\n",
       "  'Here is a summary of the text, including the main points and important details:\\n\\n**Main Points:**\\n\\n1. A cluster administrator can register external Service Brokers in the Service Catalog.\\n2. Each broker must implement the OpenServiceBroker API.\\n3. The Service Catalog uses the OpenServiceBroker API to communicate with brokers.\\n\\n**Important Details:**\\n\\n* The OpenServiceBroker API is a REST API that provides operations for:\\n\\t+ Retrieving services\\n\\t+ Provisioning service instances\\n\\t+ Updating service instances\\n\\t+ Binding and unbinding service instances\\n\\t+ Deprovisioning service instances\\n* Brokers are registered by posting a ServiceBroker resource manifest to the Service Catalog API.\\n* The Service Catalog retrieves the list of services from the broker and creates ClusterServiceClass resources for each type of service.\\n* Each ClusterServiceClass has one or more service plans associated with it, allowing users to choose the level of service they need.\\n\\n**Key Terms:**\\n\\n* OpenServiceBroker API\\n* Service Brokers\\n* Service Catalog\\n* ClusterServiceBroker resource manifest\\n* ClusterServiceClass resource',\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. Kubernetes Service Catalog allows users to retrieve a list of available services in a cluster using `kubectl get serviceclasses`.\\n2. ClusterServiceClasses are similar to StorageClasses, but allow users to select the type of service rather than storage.\\n3. Users can view details of a ClusterServiceClass by retrieving its YAML.\\n\\n**Important Details:**\\n\\n* The example shows three ClusterServiceClasses for database services (PostgreSQL, MySQL, and MongoDB).\\n* Each ClusterServiceClass has multiple plans, such as \"free\" and \"premium\", which offer different levels of service.\\n* Plans can have varying characteristics, such as storage type (e.g. spinning disk vs SSD) and cost.\\n* The example shows a ClusterServiceClass provided by a broker named \"database-broker\".\\n* Users can select the desired plan for a service using the `kubectl` command.\\n\\nOverall, this text explains how to use the Kubernetes Service Catalog to list available services in a cluster and view details of each service class.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Provisioning a Database Service in Kubernetes**\\n\\nTo use a database service, such as PostgreSQL, in a Kubernetes cluster, you need to provision a service instance. This can be done by creating a `ServiceInstance` resource.\\n\\n**Key Steps:**\\n\\n1. Choose a ClusterServiceClass (e.g., postgres-database) and plan (e.g., free).\\n2. Create a `ServiceInstance` resource with the chosen ClusterServiceClass, plan, and any additional parameters required by the broker.\\n3. The Service Catalog will contact the broker to provision the service instance.\\n4. The broker will spin up a new instance of the database service, which may not be in the same Kubernetes cluster or even in Kubernetes at all.\\n\\n**Important Details:**\\n\\n* The Service Catalog doesn't care about the specifics of how the service is provisioned.\\n* You can check if the service has been provisioned successfully by inspecting the status section of the `ServiceInstance` resource.\\n* Additional parameters may be required to pass to the broker, which should be specified in the broker's documentation.\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. A database instance has been provisioned using the Kubernetes Service Catalog, but it needs to be bound to pods for use.\\n2. To bind the instance, a ServiceBinding resource must be created, referencing the service instance and specifying a Secret name.\\n3. The Service Catalog will create a new Secret with the specified name and store credentials and other data necessary for connecting to the database in it.\\n\\n**Important Details:**\\n\\n* Currently, the Service Catalog does not make it possible to inject pods with the ServiceInstance's credentials directly.\\n* A new Kubernetes feature called PodPresets is expected to enable this functionality in the future.\\n* Until then, users can manually mount the Secret into their pods.\\n* The Secret created by the Service Catalog can be mounted into pods and used to connect to the provisioned service instance.\",\n",
       "  \"Here is a summary of the main points and important details from the text:\\n\\n**Main Points:**\\n\\n1. Kubernetes allows you to create a Secret resource that holds sensitive information, such as credentials for connecting to an external service.\\n2. The Service Catalog in Kubernetes enables service providers to expose their services within any Kubernetes cluster by registering a broker.\\n3. A ServiceBinding resource is used to bind a pod to a service instance, and can be deleted when no longer needed.\\n\\n**Important Details:**\\n\\n* A Secret resource can hold credentials for connecting to an external service, such as a PostgreSQL database.\\n* The Secret resource must exist before a pod can start using it.\\n* Multiple bindings can be created for different pods, but it's recommended to create a new set of credentials for each binding instance.\\n* When deleting a ServiceBinding, the Service Catalog controller will delete the associated Secret and perform an unbinding operation on the service broker.\\n* If you no longer need a service instance (e.g. a PostgreSQL database), you should also delete the ServiceInstance resource to trigger a deprovisioning operation on the service broker.\\n\\n**Key Commands:**\\n\\n* `kubectl get secret postgres-secret -o yaml` to view the Secret resource\\n* `kubectl delete servicebinding my-postgres-db-binding` to delete a ServiceBinding\\n* `kubectl delete serviceinstance my-postgres-db` to delete a ServiceInstance and trigger deprovisioning\",\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. There are 527 platforms built on top of Kubernetes, including service brokers that make it easy to provision services in Kubernetes.\\n2. Kubernetes is becoming a widely accepted foundation for Platform-as-a-Service (PaaS) offerings.\\n3. Two popular PaaS systems built on Kubernetes are Deis Workflow and Red Hat's OpenShift.\\n\\n**Important Details:**\\n\\n1. **Red Hat OpenShift**: A PaaS system that enables rapid development, easy deployment, scaling, and maintenance of applications. It was rebuilt from scratch on top of Kubernetes (version 3).\\n2. **Features of OpenShift**:\\n\\t* Automates rollouts and application scaling\\n\\t* Automates building and deploying application images without requiring a Continuous Integration solution\\n\\t* Provides user and group management for secure multi-tenant Kubernetes clusters\\n\\t* Offers additional API objects, including users, groups, projects, and more\\n3. **Service Brokers**: Allow easy provisioning and exposing of services in Kubernetes, making it an even more awesome platform for deploying applications.\\n\\nOverall, the text highlights the growing ecosystem of platforms built on top of Kubernetes, with a focus on Red Hat's OpenShift as a popular PaaS system that leverages the power of Kubernetes.\",\n",
       "  \"Here is a summary of the text with main points and important details:\\n\\n**Main Points:**\\n\\n1. OpenShift provides a multi-tenant environment, unlike Kubernetes which doesn't have user management features.\\n2. Users in OpenShift are assigned to projects (Kubernetes Namespaces) and can only act on resources within those projects.\\n3. Application templates in OpenShift allow for parameterizable deployment of resources.\\n\\n**Important Details:**\\n\\n* In OpenShift, users are managed through powerful user management features that pre-date Role-Based Access Control (RBAC).\\n* Projects in OpenShift are essentially Kubernetes Namespaces with additional annotations.\\n* Templates in OpenShift are JSON or YAML files containing a list of parameters and resources, which can be instantiated by replacing placeholders with parameter values.\\n* Templates can be stored in the API server like any other object and must have template parameters defined before instantiation.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**OpenShift Features**\\n\\n1. **Template Processing**: OpenShift can process templates, replacing parameters with values, resulting in a processed template that can be created as a Kubernetes resource list.\\n2. **Pre-fabricated Templates**: OpenShift provides pre-built templates for complex applications, allowing users to run them with minimal configuration.\\n3. **Building Images from Source**: OpenShift can build and deploy container images from source code in a Git repository using BuildConfigs.\\n4. **Automatic Deployment**: Once built, new container images can be automatically deployed in the cluster using DeploymentConfigs.\\n\\n**Key Details**\\n\\n* OpenShift can process templates to create Kubernetes resources with minimal configuration.\\n* Pre-fabricated templates allow users to run complex applications with ease.\\n* BuildConfigs enable OpenShift to build and deploy container images from source code in a Git repository.\\n* DeploymentConfigs can automatically deploy new container images built by BuildConfigs.\\n\\n**Benefits**\\n\\n* Developers can focus on coding without worrying about building container images.\\n* Ops teams can deploy OpenShift clusters, giving developers access to it, and then let them develop their code as usual.\\n* Automatic deployment of new container images reduces the need for manual intervention.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Extending Kubernetes**\\n\\n* **DeploymentConfig**: A precursor to Kubernetes' Deployment object, which provides a configurable strategy for transitioning between deployments and allows for pre- and post-deployment hooks.\\n* **Key differences from Kubernetes Deployment**: Creates ReplicationControllers instead of ReplicaSets and provides additional features.\\n\\n**Exposing Services Externally**\\n\\n* **Early OpenShift feature**: Routes were used to expose services externally before Ingress objects were available in Kubernetes.\\n* **Similarities to Ingress**: Routes provide configuration for TLS termination and traffic splitting, similar to an Ingress controller.\\n* **Router requirement**: A Route needs a Router, which is a load balancer or proxy provided out of the box in OpenShift.\\n\\n**Trying Out OpenShift**\\n\\n* **Options for trying OpenShift**: Use Minishift (OpenShift equivalent of Minikube) or try OpenShift Online Starter at https://manage.openshift.com (a free, multi-tenant hosted solution).\\n\\n**Deis Workflow and Helm**\\n\\n* **Acquired by Microsoft**: Deis provides a PaaS called Workflow built on top of Kubernetes.\\n* **Similarities to OpenShift**: Workflow uses BuildConfigs and DeploymentConfigs similar to those in OpenShift.\",\n",
       "  'Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. There are 531 platforms built on top of Kubernetes, including Deis Workflow and Helm.\\n2. Deis Workflow provides a developer-friendly environment for deploying and managing applications in Kubernetes.\\n3. Helm is a package manager for Kubernetes that allows users to deploy and manage application packages.\\n\\n**Important Details:**\\n\\n* Deis Workflow can be deployed to any existing Kubernetes cluster, unlike OpenShift which is a complete cluster with modified API server and other components.\\n* Deis Workflow provides features such as source-to-image mechanism, application rollouts and rollbacks, edge routing, log aggregation, metrics, and alerting.\\n* Helm is comprised of two components: the helm CLI tool (client) and Tiller (server component running as a Pod inside the Kubernetes cluster).\\n* Helm allows users to deploy and manage application packages in a Kubernetes cluster using charts, which are combined with configuration information to create releases.\\n* There are many existing Helm charts available on GitHub for popular applications such as PostgreSQL, MySQL, Magento, and others.\\n\\n**Key Takeaways:**\\n\\n* Deis Workflow provides a developer-friendly environment for deploying and managing applications in Kubernetes.\\n* Helm is a package manager for Kubernetes that allows users to deploy and manage application packages using charts.\\n* There are many existing Helm charts available on GitHub for popular applications.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Main Points:**\\n\\n1. When running a database like PostgreSQL or MySQL in a Kubernetes cluster, don't create custom manifests for it.\\n2. Instead, check if someone has already created a Helm chart for it, which can be easily installed using a single command.\\n\\n**Important Details:**\\n\\n* Helm charts are pre-configured templates that make it easy to install and run applications in a Kubernetes cluster.\\n* A Helm chart for MySQL, for example, will create all necessary Deployments, Services, Secrets, and PersistentVolumeClaims needed to run the database.\\n* Installing a Helm chart is as simple as running a single command, such as `helm install --name my-database stable/mysql`.\\n* Helm charts are available on GitHub repositories, making it easy to find and use pre-configured templates for various applications.\\n\\n**Additional Information:**\\n\\n* The text also mentions an OpenVPN chart that allows developers to access Services in the Kubernetes cluster through a VPN connection.\\n* The overview of Helm (Figure 18.10) is included at the end of the text, highlighting the key components of the tool, including charts, configurations, and releases.\",\n",
       "  'Here is a summary of the text:\\n\\n**Main Points:**\\n\\n* Kubernetes can be extended beyond its existing functionalities through various means.\\n* Custom resources can be added to the API server using Custom-ResourceDefinition objects.\\n* A custom controller can be implemented to bring these custom objects to life.\\n* Kubernetes can be extended with custom API servers through API aggregation.\\n* The Kubernetes Service Catalog enables self-provisioning of external services and exposing them to pods in the cluster.\\n\\n**Important Details:**\\n\\n* Platforms-as-a-Service built on top of Kubernetes make it easy to build containerized applications within the same cluster.\\n* Helm, a package manager, allows for deploying existing apps without requiring manual resource manifests.\\n* The book has provided an overview of these extensions and how companies like Dies and Red Hat have utilized them.',\n",
       "  \"Here is a summary of the text with the main points and important details:\\n\\n**Using kubectl with multiple clusters**\\n\\nThe book's examples can be run in either Minikube or Google Kubernetes Engine (GKE) clusters. To switch between them, follow these steps:\\n\\n* **Switching to Minikube**: Start Minikube using `minikube start`, which reconfigures kubectl to use the Minikube cluster.\\n* **Switching to GKE**: Use the command `gcloud container clusters get-credentials my-gke-cluster` to configure kubectl to use a specific GKE cluster.\\n\\n**Important details**\\n\\n* Minikube automatically sets up kubectl every time you start the cluster, so switching back and forth between Minikube and GKE is straightforward.\\n* To switch from GKE to Minikube, simply stop Minikube and restart it, which will reconfigure kubectl to use the Minikube cluster again.\"],\n",
       " 'output_text': 'Here is a concise bullet point summary of the text:\\n\\n* **Replication Controllers, ReplicaSets, DaemonSets, and Jobs**:\\n\\t+ Replication Controller: ensures a specified number of replicas (pods) are running at any time\\n\\t+ ReplicaSet: recommended over Replication Controller for creating and managing pods; can match pods based on multiple label conditions\\n\\t+ DaemonSet: runs exactly one pod on each node in a Kubernetes cluster\\n\\t+ Job: can be configured to run multiple pods sequentially or in parallel\\n* **Scaling and Configuring Jobs**:\\n\\t+ Scale a Job using `kubectl scale` command\\n\\t+ Configure retry limit by specifying `spec.backoffLimit` field in the Job manifest\\n\\t+ Schedule jobs periodically or once using CronJobs\\n* **Kubernetes Services, Volumes, and Persistent Storage**:\\n\\t+ Services: provide a stable IP address and port for multiple pods that match a certain label selector\\n\\t+ Sharing data between containers using volumes:\\n\\t\\t- Shared volume can be used to share data between containers\\n\\t\\t- Git repository can be used as a volume to share data between containers\\n\\t+ Persistent Volumes: provisioned by cluster admins to represent network storage; users create persistent volume claims (PVCs) to request specific persistent storage.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "222a2950",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, out \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m      3\u001b[0m     map_reduce_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m], map_reduce_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintermediate_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m ):\n\u001b[1;32m      5\u001b[0m     output \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 6\u001b[0m     output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(doc\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstem\n\u001b[1;32m      7\u001b[0m     output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Path(doc\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msuffix\n\u001b[1;32m      8\u001b[0m     output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_number\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "final_mp_data = []\n",
    "for doc, out in zip(\n",
    "    map_reduce_outputs[\"input_documents\"], map_reduce_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = Path(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = Path(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_mp_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_mp_summary = pd.DataFrame.from_dict(final_mp_data)\n",
    "pdf_mp_summary = pdf_mp_summary.sort_values(\n",
    "    by=[\"file_name\", \"page_number\"]\n",
    ")  # sorting the dataframe by filename and page_number\n",
    "pdf_mp_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_mp_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3\n",
    "print(\"[Context]\")\n",
    "print(pdf_mp_summary[\"chunks\"].iloc[index])\n",
    "print(\"\\n\\n [Simple Summary]\")\n",
    "print(pdf_mp_summary[\"concise_summary\"].iloc[index])\n",
    "print(\"\\n\\n [Page number]\")\n",
    "print(pdf_mp_summary[\"page_number\"].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_mp_summary[\"file_name\"].iloc[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
