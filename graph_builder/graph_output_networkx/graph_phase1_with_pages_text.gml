graph [
  directed 1
  node [
    id 0
    label "Kubernetes_in_Action.pdf"
    color "red"
    description "this is the name of the book"
    category "Cloud_Computing"
  ]
  node [
    id 1
    label "89"
    title "Page_89"
    color "blue"
  ]
  node [
    id 2
    label "1"
  ]
  node [
    id 3
    label "text_0"
    title "57&#10;Introducing pods&#10; Therefore, you need to run each process in its own container. That&#8217;s how Docker&#10;and Kubernetes are meant to be used. &#10;3.1.2&#10;Understanding pods&#10;Because you&#8217;re not supposed to group multiple processes into a single container, it&#8217;s&#10;obvious you need another higher-level construct that will allow you to bind containers&#10;together and manage them as a single unit. This is the reasoning behind pods. &#10; A pod of containers allows you to run closely related processes together and pro-&#10;vide them with (almost) the same environment as if they were all running in a single&#10;container, while keeping them somewhat isolated. This way, you get the best of both&#10;worlds. You can take advantage of all the features containers provide, while at the&#10;same time giving the processes the illusion of running together. &#10;UNDERSTANDING THE PARTIAL ISOLATION BETWEEN CONTAINERS OF THE SAME POD&#10;In the previous chapter, you learned that containers are completely isolated from&#10;each other, but now you see that you want to isolate groups of containers instead of&#10;individual ones. You want containers inside each group to share certain resources,&#10;although not all, so that they&#8217;re not fully isolated. Kubernetes achieves this by config-&#10;uring Docker to have all containers of a pod share the same set of Linux namespaces&#10;instead of each container having its own set. &#10; Because all containers of a pod run under the same Network and UTS namespaces&#10;(we&#8217;re talking about Linux namespaces here), they all share the same hostname and&#10;network interfaces. Similarly, all containers of a pod run under the same IPC namespace&#10;and can communicate through IPC. In the latest Kubernetes and Docker versions, they&#10;can also share the same PID namespace, but that feature isn&#8217;t enabled by default. &#10;NOTE&#10;When containers of the same pod use separate PID namespaces, you&#10;only see the container&#8217;s own processes when running ps aux in the container.&#10;But when it comes to the filesystem, things are a little different. Because most of the&#10;container&#8217;s filesystem comes from the container image, by default, the filesystem of&#10;each container is fully isolated from other containers. However, it&#8217;s possible to have&#10;them share file directories using a Kubernetes concept called a Volume, which we&#8217;ll&#10;talk about in chapter 6.&#10;UNDERSTANDING HOW CONTAINERS SHARE THE SAME IP AND PORT SPACE&#10;One thing to stress here is that because containers in a pod run in the same Network&#10;namespace, they share the same IP address and port space. This means processes run-&#10;ning in containers of the same pod need to take care not to bind to the same port&#10;numbers or they&#8217;ll run into port conflicts. But this only concerns containers in the&#10;same pod. Containers of different pods can never run into port conflicts, because&#10;each pod has a separate port space. All the containers in a pod also have the same&#10;loopback network interface, so a container can communicate with other containers in&#10;the same pod through localhost.&#10; &#10;"
    color "green"
  ]
  node [
    id 4
    label "90"
    title "Page_90"
    color "blue"
  ]
  node [
    id 5
    label "text_1"
    title "58&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;INTRODUCING THE FLAT INTER-POD NETWORK&#10;All pods in a Kubernetes cluster reside in a single flat, shared, network-address space&#10;(shown in figure 3.2), which means every pod can access every other pod at the other&#10;pod&#8217;s IP address. No NAT (Network Address Translation) gateways exist between them.&#10;When two pods send network packets between each other, they&#8217;ll each see the actual&#10;IP address of the other as the source IP in the packet.&#10;Consequently, communication between pods is always simple. It doesn&#8217;t matter if two&#10;pods are scheduled onto a single or onto different worker nodes; in both cases the&#10;containers inside those pods can communicate with each other across the flat NAT-&#10;less network, much like computers on a local area network (LAN), regardless of the&#10;actual inter-node network topology. Like a computer on a LAN, each pod gets its own&#10;IP address and is accessible from all other pods through this network established spe-&#10;cifically for pods. This is usually achieved through an additional software-defined net-&#10;work layered on top of the actual network.&#10; To sum up what&#8217;s been covered in this section: pods are logical hosts and behave&#10;much like physical hosts or VMs in the non-container world. Processes running in the&#10;same pod are like processes running on the same physical or virtual machine, except&#10;that each process is encapsulated in a container. &#10;3.1.3&#10;Organizing containers across pods properly&#10;You should think of pods as separate machines, but where each one hosts only a cer-&#10;tain app. Unlike the old days, when we used to cram all sorts of apps onto the same&#10;host, we don&#8217;t do that with pods. Because pods are relatively lightweight, you can have&#10;as many as you need without incurring almost any overhead. Instead of stuffing every-&#10;thing into a single pod, you should organize apps into multiple pods, where each one&#10;contains only tightly related components or processes.&#10;Node 1&#10;Pod A&#10;IP: 10.1.1.6&#10;Container 1&#10;Container 2&#10;Pod B&#10;IP: 10.1.1.7&#10;Container 1&#10;Container 2&#10;Node 2&#10;Flat network&#10;Pod C&#10;IP: 10.1.2.5&#10;Container 1&#10;Container 2&#10;Pod D&#10;IP: 10.1.2.7&#10;Container 1&#10;Container 2&#10;Figure 3.2&#10;Each pod gets a routable IP address and all other pods see the pod under &#10;that IP address.&#10; &#10;"
    color "green"
  ]
  node [
    id 6
    label "91"
    title "Page_91"
    color "blue"
  ]
  node [
    id 7
    label "text_2"
    title "59&#10;Introducing pods&#10; Having said that, do you think a multi-tier application consisting of a frontend&#10;application server and a backend database should be configured as a single pod or as&#10;two pods?&#10;SPLITTING MULTI-TIER APPS INTO MULTIPLE PODS&#10;Although nothing is stopping you from running both the frontend server and the&#10;database in a single pod with two containers, it isn&#8217;t the most appropriate way. We&#8217;ve&#10;said that all containers of the same pod always run co-located, but do the web server&#10;and the database really need to run on the same machine? The answer is obviously no,&#10;so you don&#8217;t want to put them into a single pod. But is it wrong to do so regardless? In&#10;a way, it is.&#10; If both the frontend and backend are in the same pod, then both will always be&#10;run on the same machine. If you have a two-node Kubernetes cluster and only this sin-&#10;gle pod, you&#8217;ll only be using a single worker node and not taking advantage of the&#10;computational resources (CPU and memory) you have at your disposal on the second&#10;node. Splitting the pod into two would allow Kubernetes to schedule the frontend to&#10;one node and the backend to the other node, thereby improving the utilization of&#10;your infrastructure.&#10;SPLITTING INTO MULTIPLE PODS TO ENABLE INDIVIDUAL SCALING&#10;Another reason why you shouldn&#8217;t put them both into a single pod is scaling. A pod is&#10;also the basic unit of scaling. Kubernetes can&#8217;t horizontally scale individual contain-&#10;ers; instead, it scales whole pods. If your pod consists of a frontend and a backend con-&#10;tainer, when you scale up the number of instances of the pod to, let&#8217;s say, two, you end&#10;up with two frontend containers and two backend containers. &#10; Usually, frontend components have completely different scaling requirements&#10;than the backends, so we tend to scale them individually. Not to mention the fact that&#10;backends such as databases are usually much harder to scale compared to (stateless)&#10;frontend web servers. If you need to scale a container individually, this is a clear indi-&#10;cation that it needs to be deployed in a separate pod. &#10;UNDERSTANDING WHEN TO USE MULTIPLE CONTAINERS IN A POD&#10;The main reason to put multiple containers into a single pod is when the application&#10;consists of one main process and one or more complementary processes, as shown in&#10;figure 3.3.&#10;Pod&#10;Main container&#10;Supporting&#10;container 1&#10;Supporting&#10;container 2&#10;Volume&#10;Figure 3.3&#10;Pods should contain tightly coupled &#10;containers, usually a main container and containers &#10;that support the main one.&#10; &#10;"
    color "green"
  ]
  node [
    id 8
    label "92"
    title "Page_92"
    color "blue"
  ]
  node [
    id 9
    label "text_3"
    title "60&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;For example, the main container in a pod could be a web server that serves files from&#10;a certain file directory, while an additional container (a sidecar container) periodi-&#10;cally downloads content from an external source and stores it in the web server&#8217;s&#10;directory. In chapter 6 you&#8217;ll see that you need to use a Kubernetes Volume that you&#10;mount into both containers. &#10; Other examples of sidecar containers include log rotators and collectors, data pro-&#10;cessors, communication adapters, and others.&#10;DECIDING WHEN TO USE MULTIPLE CONTAINERS IN A POD&#10;To recap how containers should be grouped into pods&#8212;when deciding whether to&#10;put two containers into a single pod or into two separate pods, you always need to ask&#10;yourself the following questions:&#10;&#61601;Do they need to be run together or can they run on different hosts?&#10;&#61601;Do they represent a single whole or are they independent components?&#10;&#61601;Must they be scaled together or individually? &#10;Basically, you should always gravitate toward running containers in separate pods,&#10;unless a specific reason requires them to be part of the same pod. Figure 3.4 will help&#10;you memorize this.&#10;Although pods can contain multiple containers, to keep things simple for now, you&#8217;ll&#10;only be dealing with single-container pods in this chapter. You&#8217;ll see how multiple&#10;containers are used in the same pod later, in chapter 6. &#10;Pod&#10;Frontend&#10;process&#10;Backend&#10;process&#10;Container&#10;Pod&#10;Frontend&#10;process&#10;Frontend&#10;container&#10;Frontend pod&#10;Frontend&#10;process&#10;Frontend&#10;container&#10;Backend pod&#10;Backend&#10;process&#10;Backend&#10;container&#10;Backend&#10;process&#10;Backend&#10;container&#10;Figure 3.4&#10;A container shouldn&#8217;t run multiple processes. A pod shouldn&#8217;t contain multiple &#10;containers if they don&#8217;t need to run on the same machine.&#10; &#10;"
    color "green"
  ]
  node [
    id 10
    label "93"
    title "Page_93"
    color "blue"
  ]
  node [
    id 11
    label "text_4"
    title "61&#10;Creating pods from YAML or JSON descriptors&#10;3.2&#10;Creating pods from YAML or JSON descriptors&#10;Pods and other Kubernetes resources are usually created by posting a JSON or YAML&#10;manifest to the Kubernetes REST API endpoint. Also, you can use other, simpler ways&#10;of creating resources, such as the kubectl run command you used in the previous&#10;chapter, but they usually only allow you to configure a limited set of properties, not&#10;all. Additionally, defining all your Kubernetes objects from YAML files makes it possi-&#10;ble to store them in a version control system, with all the benefits it brings.&#10; To configure all aspects of each type of resource, you&#8217;ll need to know and under-&#10;stand the Kubernetes API object definitions. You&#8217;ll get to know most of them as you&#10;learn about each resource type throughout this book. We won&#8217;t explain every single&#10;property, so you should also refer to the Kubernetes API reference documentation at&#10;http:/&#10;/kubernetes.io/docs/reference/ when creating objects.&#10;3.2.1&#10;Examining a YAML descriptor of an existing pod&#10;You already have some existing pods you created in the previous chapter, so let&#8217;s look&#10;at what a YAML definition for one of those pods looks like. You&#8217;ll use the kubectl get&#10;command with the -o yaml option to get the whole YAML definition of the pod, as&#10;shown in the following listing.&#10;$ kubectl get po kubia-zxzij -o yaml&#10;apiVersion: v1                         &#10;kind: Pod                                       &#10;metadata:                                                 &#10;  annotations:                                            &#10;    kubernetes.io/created-by: ...                         &#10;  creationTimestamp: 2016-03-18T12:37:50Z                 &#10;  generateName: kubia-                                    &#10;  labels:                                                 &#10;    run: kubia                                            &#10;  name: kubia-zxzij                                       &#10;  namespace: default                                      &#10;  resourceVersion: &#34;294&#34;                                  &#10;  selfLink: /api/v1/namespaces/default/pods/kubia-zxzij   &#10;  uid: 3a564dc0-ed06-11e5-ba3b-42010af00004               &#10;spec:                                                   &#10;  containers:                                           &#10;  - image: luksa/kubia                                  &#10;    imagePullPolicy: IfNotPresent                       &#10;    name: kubia                                         &#10;    ports:                                              &#10;    - containerPort: 8080                               &#10;      protocol: TCP                                     &#10;    resources:                                          &#10;      requests:                                         &#10;        cpu: 100m                                       &#10;Listing 3.1&#10;Full YAML of a deployed pod&#10;Kubernetes API version used &#10;in this YAML descriptor&#10;Type of Kubernetes &#10;object/resource&#10;Pod metadata (name, &#10;labels, annotations, &#10;and so on)&#10;Pod specification/&#10;contents (list of &#10;pod&#8217;s containers, &#10;volumes, and so on)&#10; &#10;"
    color "green"
  ]
  node [
    id 12
    label "94"
    title "Page_94"
    color "blue"
  ]
  node [
    id 13
    label "text_5"
    title "62&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;    terminationMessagePath: /dev/termination-log      &#10;    volumeMounts:                                     &#10;    - mountPath: /var/run/secrets/k8s.io/servacc      &#10;      name: default-token-kvcqa                       &#10;      readOnly: true                                  &#10;  dnsPolicy: ClusterFirst                             &#10;  nodeName: gke-kubia-e8fe08b8-node-txje              &#10;  restartPolicy: Always                               &#10;  serviceAccount: default                             &#10;  serviceAccountName: default                         &#10;  terminationGracePeriodSeconds: 30                   &#10;  volumes:                                            &#10;  - name: default-token-kvcqa                         &#10;    secret:                                           &#10;      secretName: default-token-kvcqa                 &#10;status:                                                   &#10;  conditions:                                             &#10;  - lastProbeTime: null                                   &#10;    lastTransitionTime: null                              &#10;    status: &#34;True&#34;                                        &#10;    type: Ready                                           &#10;  containerStatuses:                                      &#10;  - containerID: docker://f0276994322d247ba...            &#10;    image: luksa/kubia                                    &#10;    imageID: docker://4c325bcc6b40c110226b89fe...         &#10;    lastState: {}                                         &#10;    name: kubia                                           &#10;    ready: true                                           &#10;    restartCount: 0                                       &#10;    state:                                                &#10;      running:                                            &#10;        startedAt: 2016-03-18T12:46:05Z                   &#10;  hostIP: 10.132.0.4                                      &#10;  phase: Running                                          &#10;  podIP: 10.0.2.3                                         &#10;  startTime: 2016-03-18T12:44:32Z                         &#10;I know this looks complicated, but it becomes simple once you understand the basics&#10;and know how to distinguish between the important parts and the minor details. Also,&#10;you can take comfort in the fact that when creating a new pod, the YAML you need to&#10;write is much shorter, as you&#8217;ll see later.&#10;INTRODUCING THE MAIN PARTS OF A POD DEFINITION&#10;The pod definition consists of a few parts. First, there&#8217;s the Kubernetes API version&#10;used in the YAML and the type of resource the YAML is describing. Then, three&#10;important sections are found in almost all Kubernetes resources:&#10;&#61601;Metadata includes the name, namespace, labels, and other information about&#10;the pod.&#10;&#61601;Spec contains the actual description of the pod&#8217;s contents, such as the pod&#8217;s con-&#10;tainers, volumes, and other data.&#10;Pod specification/&#10;contents (list of &#10;pod&#8217;s containers, &#10;volumes, and so on)&#10;Detailed status &#10;of the pod and &#10;its containers&#10; &#10;"
    color "green"
  ]
  node [
    id 14
    label "95"
    title "Page_95"
    color "blue"
  ]
  node [
    id 15
    label "text_6"
    title "63&#10;Creating pods from YAML or JSON descriptors&#10;&#61601;Status contains the current information about the running pod, such as what&#10;condition the pod is in, the description and status of each container, and the&#10;pod&#8217;s internal IP and other basic info.&#10;Listing 3.1 showed a full description of a running pod, including its status. The status&#10;part contains read-only runtime data that shows the state of the resource at a given&#10;moment. When creating a new pod, you never need to provide the status part. &#10; The three parts described previously show the typical structure of a Kubernetes&#10;API object. As you&#8217;ll see throughout the book, all other objects have the same anat-&#10;omy. This makes understanding new objects relatively easy.&#10; Going through all the individual properties in the previous YAML doesn&#8217;t make&#10;much sense, so, instead, let&#8217;s see what the most basic YAML for creating a pod looks&#10;like. &#10;3.2.2&#10;Creating a simple YAML descriptor for a pod&#10;You&#8217;re going to create a file called kubia-manual.yaml (you can create it in any&#10;directory you want), or download the book&#8217;s code archive, where you&#8217;ll find the&#10;file inside the Chapter03 directory. The following listing shows the entire contents&#10;of the file.&#10;apiVersion: v1         &#10;kind: Pod                             &#10;metadata:     &#10;  name: kubia-manual         &#10;spec: &#10;  containers: &#10;  - image: luksa/kubia          &#10;    name: kubia         &#10;    ports: &#10;    - containerPort: 8080     &#10;      protocol: TCP&#10;I&#8217;m sure you&#8217;ll agree this is much simpler than the definition in listing 3.1. Let&#8217;s exam-&#10;ine this descriptor in detail. It conforms to the v1 version of the Kubernetes API. The&#10;type of resource you&#8217;re describing is a pod, with the name kubia-manual. The pod&#10;consists of a single container based on the luksa/kubia image. You&#8217;ve also given a&#10;name to the container and indicated that it&#8217;s listening on port 8080. &#10;SPECIFYING CONTAINER PORTS&#10;Specifying ports in the pod definition is purely informational. Omitting them has no&#10;effect on whether clients can connect to the pod through the port or not. If the con-&#10;Listing 3.2&#10;A basic pod manifest: kubia-manual.yaml&#10;Descriptor conforms&#10;to version v1 of&#10;Kubernetes API&#10;You&#8217;re &#10;describing a pod.&#10;The name &#10;of the pod&#10;Container image to create &#10;the container from&#10;Name of the container&#10;The port the app &#10;is listening on&#10; &#10;"
    color "green"
  ]
  node [
    id 16
    label "96"
    title "Page_96"
    color "blue"
  ]
  node [
    id 17
    label "text_7"
    title "64&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;tainer is accepting connections through a port bound to the 0.0.0.0 address, other&#10;pods can always connect to it, even if the port isn&#8217;t listed in the pod spec explicitly. But&#10;it makes sense to define the ports explicitly so that everyone using your cluster can&#10;quickly see what ports each pod exposes. Explicitly defining ports also allows you to&#10;assign a name to each port, which can come in handy, as you&#8217;ll see later in the book.&#10;Using kubectl explain to discover possible API object fields&#10;When preparing a manifest, you can either turn to the Kubernetes reference&#10;documentation at http:/&#10;/kubernetes.io/docs/api to see which attributes are&#10;supported by each API object, or you can use the kubectl explain command.&#10;For example, when creating a pod manifest from scratch, you can start by asking&#10;kubectl to explain pods:&#10;$ kubectl explain pods&#10;DESCRIPTION:&#10;Pod is a collection of containers that can run on a host. This resource &#10;is created by clients and scheduled onto hosts.&#10;FIELDS:&#10;   kind      <string>&#10;     Kind is a string value representing the REST resource this object&#10;     represents...&#10;   metadata  <Object>&#10;     Standard object's metadata...&#10;   spec      <Object>&#10;     Specification of the desired behavior of the pod...&#10;   status    <Object>&#10;     Most recently observed status of the pod. This data may not be up to&#10;     date...&#10;Kubectl prints out the explanation of the object and lists the attributes the object&#10;can contain. You can then drill deeper to find out more about each attribute. For&#10;example, you can examine the spec attribute like this:&#10;$ kubectl explain pod.spec&#10;RESOURCE: spec <Object>&#10;DESCRIPTION:&#10;    Specification of the desired behavior of the pod...&#10;    podSpec is a description of a pod.&#10;FIELDS:&#10;   hostPID   <boolean>&#10;     Use the host's pid namespace. Optional: Default to false.&#10;   ...&#10;   volumes   <[]Object>&#10;     List of volumes that can be mounted by containers belonging to the&#10;     pod.&#10; &#10;"
    color "green"
  ]
  node [
    id 18
    label "97"
    title "Page_97"
    color "blue"
  ]
  node [
    id 19
    label "text_8"
    title "65&#10;Creating pods from YAML or JSON descriptors&#10;3.2.3&#10;Using kubectl create to create the pod&#10;To create the pod from your YAML file, use the kubectl create command:&#10;$ kubectl create -f kubia-manual.yaml&#10;pod &#34;kubia-manual&#34; created&#10;The kubectl create -f command is used for creating any resource (not only pods)&#10;from a YAML or JSON file. &#10;RETRIEVING THE WHOLE DEFINITION OF A RUNNING POD&#10;After creating the pod, you can ask Kubernetes for the full YAML of the pod. You&#8217;ll&#10;see it&#8217;s similar to the YAML you saw earlier. You&#8217;ll learn about the additional fields&#10;appearing in the returned definition in the next sections. Go ahead and use the fol-&#10;lowing command to see the full descriptor of the pod:&#10;$ kubectl get po kubia-manual -o yaml&#10;If you&#8217;re more into JSON, you can also tell kubectl to return JSON instead of YAML&#10;like this (this works even if you used YAML to create the pod):&#10;$ kubectl get po kubia-manual -o json&#10;SEEING YOUR NEWLY CREATED POD IN THE LIST OF PODS&#10;Your pod has been created, but how do you know if it&#8217;s running? Let&#8217;s list pods to see&#10;their statuses:&#10;$ kubectl get pods&#10;NAME            READY   STATUS    RESTARTS   AGE&#10;kubia-manual    1/1     Running   0          32s&#10;kubia-zxzij     1/1     Running   0          1d    &#10;There&#8217;s your kubia-manual pod. Its status shows that it&#8217;s running. If you&#8217;re like me,&#10;you&#8217;ll probably want to confirm that&#8217;s true by talking to the pod. You&#8217;ll do that in a&#10;minute. First, you&#8217;ll look at the app&#8217;s log to check for any errors.&#10;3.2.4&#10;Viewing application logs&#10;Your little Node.js application logs to the process&#8217;s standard output. Containerized&#10;applications usually log to the standard output and standard error stream instead of&#10;   Containers  <[]Object> -required-&#10;     List of containers belonging to the pod. Containers cannot currently&#10;     Be added or removed. There must be at least one container in a pod.&#10;     Cannot be updated. More info:&#10;     http://releases.k8s.io/release-1.4/docs/user-guide/containers.md&#10; &#10;"
    color "green"
  ]
  node [
    id 20
    label "98"
    title "Page_98"
    color "blue"
  ]
  node [
    id 21
    label "text_9"
    title "66&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;writing their logs to files. This is to allow users to view logs of different applications in&#10;a simple, standard way. &#10; The container runtime (Docker in your case) redirects those streams to files and&#10;allows you to get the container&#8217;s log by running&#10;$ docker logs <container id>&#10;You could use ssh to log into the node where your pod is running and retrieve its logs&#10;with docker logs, but Kubernetes provides an easier way. &#10;RETRIEVING A POD&#8217;S LOG WITH KUBECTL LOGS&#10;To see your pod&#8217;s log (more precisely, the container&#8217;s log) you run the following com-&#10;mand on your local machine (no need to ssh anywhere):&#10;$ kubectl logs kubia-manual&#10;Kubia server starting...&#10;You haven&#8217;t sent any web requests to your Node.js app, so the log only shows a single&#10;log statement about the server starting up. As you can see, retrieving logs of an appli-&#10;cation running in Kubernetes is incredibly simple if the pod only contains a single&#10;container. &#10;NOTE&#10;Container logs are automatically rotated daily and every time the log file&#10;reaches 10MB in size. The kubectl logs command only shows the log entries&#10;from the last rotation.&#10;SPECIFYING THE CONTAINER NAME WHEN GETTING LOGS OF A MULTI-CONTAINER POD&#10;If your pod includes multiple containers, you have to explicitly specify the container&#10;name by including the -c <container name> option when running kubectl logs. In&#10;your kubia-manual pod, you set the container&#8217;s name to kubia, so if additional con-&#10;tainers exist in the pod, you&#8217;d have to get its logs like this:&#10;$ kubectl logs kubia-manual -c kubia&#10;Kubia server starting...&#10;Note that you can only retrieve container logs of pods that are still in existence. When&#10;a pod is deleted, its logs are also deleted. To make a pod&#8217;s logs available even after the&#10;pod is deleted, you need to set up centralized, cluster-wide logging, which stores all&#10;the logs into a central store. Chapter 17 explains how centralized logging works.&#10;3.2.5&#10;Sending requests to the pod&#10;The pod is now running&#8212;at least that&#8217;s what kubectl get and your app&#8217;s log say. But&#10;how do you see it in action? In the previous chapter, you used the kubectl expose&#10;command to create a service to gain access to the pod externally. You&#8217;re not going to&#10;do that now, because a whole chapter is dedicated to services, and you have other ways&#10;of connecting to a pod for testing and debugging purposes. One of them is through&#10;port forwarding.&#10; &#10;"
    color "green"
  ]
  node [
    id 22
    label "99"
    title "Page_99"
    color "blue"
  ]
  node [
    id 23
    label "text_10"
    title "67&#10;Organizing pods with labels&#10;FORWARDING A LOCAL NETWORK PORT TO A PORT IN THE POD&#10;When you want to talk to a specific pod without going through a service (for debug-&#10;ging or other reasons), Kubernetes allows you to configure port forwarding to the&#10;pod. This is done through the kubectl port-forward command. The following&#10;command will forward your machine&#8217;s local port 8888 to port 8080 of your kubia-&#10;manual pod:&#10;$ kubectl port-forward kubia-manual 8888:8080&#10;... Forwarding from 127.0.0.1:8888 -> 8080&#10;... Forwarding from [::1]:8888 -> 8080&#10;The port forwarder is running and you can now connect to your pod through the&#10;local port. &#10;CONNECTING TO THE POD THROUGH THE PORT FORWARDER&#10;In a different terminal, you can now use curl to send an HTTP request to your pod&#10;through the kubectl port-forward proxy running on localhost:8888:&#10;$ curl localhost:8888&#10;You&#8217;ve hit kubia-manual&#10;Figure 3.5 shows an overly simplified view of what happens when you send the request.&#10;In reality, a couple of additional components sit between the kubectl process and the&#10;pod, but they aren&#8217;t relevant right now.&#10;Using port forwarding like this is an effective way to test an individual pod. You&#8217;ll&#10;learn about other similar methods throughout the book. &#10;3.3&#10;Organizing pods with labels&#10;At this point, you have two pods running in your cluster. When deploying actual&#10;applications, most users will end up running many more pods. As the number of&#10;pods increases, the need for categorizing them into subsets becomes more and&#10;more evident.&#10; For example, with microservices architectures, the number of deployed microser-&#10;vices can easily exceed 20 or more. Those components will probably be replicated&#10;Kubernetes cluster&#10;Port&#10;8080&#10;Local machine&#10;kubectl&#10;port-forward&#10;process&#10;curl&#10;Port&#10;8888&#10;Pod:&#10;kubia-manual&#10;Figure 3.5&#10;A simplified view of what happens when you use curl with kubectl port-forward&#10; &#10;"
    color "green"
  ]
  node [
    id 24
    label "100"
    title "Page_100"
    color "blue"
  ]
  node [
    id 25
    label "text_11"
    title "68&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;(multiple copies of the same component will be deployed) and multiple versions or&#10;releases (stable, beta, canary, and so on) will run concurrently. This can lead to hun-&#10;dreds of pods in the system. Without a mechanism for organizing them, you end up&#10;with a big, incomprehensible mess, such as the one shown in figure 3.6. The figure&#10;shows pods of multiple microservices, with several running multiple replicas, and others&#10;running different releases of the same microservice.&#10;It&#8217;s evident you need a way of organizing them into smaller groups based on arbitrary&#10;criteria, so every developer and system administrator dealing with your system can eas-&#10;ily see which pod is which. And you&#8217;ll want to operate on every pod belonging to a cer-&#10;tain group with a single action instead of having to perform the action for each pod&#10;individually. &#10; Organizing pods and all other Kubernetes objects is done through labels.&#10;3.3.1&#10;Introducing labels&#10;Labels are a simple, yet incredibly powerful, Kubernetes feature for organizing not&#10;only pods, but all other Kubernetes resources. A label is an arbitrary key-value pair you&#10;attach to a resource, which is then utilized when selecting resources using label selectors&#10;(resources are filtered based on whether they include the label specified in the selec-&#10;tor). A resource can have more than one label, as long as the keys of those labels are&#10;unique within that resource. You usually attach labels to resources when you create&#10;them, but you can also add additional labels or even modify the values of existing&#10;labels later without having to recreate the resource. &#10;UI pod&#10;UI pod&#10;UI pod&#10;Account&#10;Service&#10;pod&#10;Product&#10;Catalog&#10;pod&#10;Product&#10;Catalog&#10;pod&#10;Product&#10;Catalog&#10;pod&#10;Shopping&#10;Cart&#10;pod&#10;Shopping&#10;Cart&#10;pod&#10;Order&#10;Service&#10;pod&#10;UI pod&#10;UI pod&#10;Product&#10;Catalog&#10;pod&#10;Product&#10;Catalog&#10;pod&#10;Order&#10;Service&#10;pod&#10;Account&#10;Service&#10;pod&#10;Product&#10;Catalog&#10;pod&#10;Product&#10;Catalog&#10;pod&#10;Order&#10;Service&#10;pod&#10;Figure 3.6&#10;Uncategorized pods in a microservices architecture&#10; &#10;"
    color "green"
  ]
  node [
    id 26
    label "101"
    title "Page_101"
    color "blue"
  ]
  node [
    id 27
    label "text_12"
    title "69&#10;Organizing pods with labels&#10; Let&#8217;s turn back to the microservices example from figure 3.6. By adding labels to&#10;those pods, you get a much-better-organized system that everyone can easily make&#10;sense of. Each pod is labeled with two labels:&#10;&#61601;&#10;app, which specifies which app, component, or microservice the pod belongs to. &#10;&#61601;&#10;rel, which shows whether the application running in the pod is a stable, beta,&#10;or a canary release.&#10;DEFINITION&#10;A canary release is when you deploy a new version of an applica-&#10;tion next to the stable version, and only let a small fraction of users hit the&#10;new version to see how it behaves before rolling it out to all users. This pre-&#10;vents bad releases from being exposed to too many users.&#10;By adding these two labels, you&#8217;ve essentially organized your pods into two dimen-&#10;sions (horizontally by app and vertically by release), as shown in figure 3.7.&#10;Every developer or ops person with access to your cluster can now easily see the sys-&#10;tem&#8217;s structure and where each pod fits in by looking at the pod&#8217;s labels.&#10;3.3.2&#10;Specifying labels when creating a pod&#10;Now, you&#8217;ll see labels in action by creating a new pod with two labels. Create a new file&#10;called kubia-manual-with-labels.yaml with the contents of the following listing.&#10;apiVersion: v1                                         &#10;kind: Pod                                              &#10;metadata:                                              &#10;  name: kubia-manual-v2&#10;Listing 3.3&#10;A pod with labels: kubia-manual-with-labels.yaml&#10;UI pod&#10;app: ui&#10;rel: stable&#10;rel=stable&#10;app=ui&#10;Account&#10;Service&#10;pod&#10;app: as&#10;rel: stable&#10;app=as&#10;app: pc&#10;rel: stable&#10;app=pc&#10;app: sc&#10;rel: stable&#10;app=sc&#10;app: os&#10;rel: stable&#10;app=os&#10;Product&#10;Catalog&#10;pod&#10;Shopping&#10;Cart&#10;pod&#10;Order&#10;Service&#10;pod&#10;UI pod&#10;app: ui&#10;rel: beta&#10;rel=beta&#10;app: pc&#10;rel: beta&#10;app: os&#10;rel: beta&#10;Product&#10;Catalog&#10;pod&#10;Order&#10;Service&#10;pod&#10;rel=canary&#10;Account&#10;Service&#10;pod&#10;app: as&#10;rel: canary&#10;app: pc&#10;rel: canary&#10;app: os&#10;rel: canary&#10;Product&#10;Catalog&#10;pod&#10;Order&#10;Service&#10;pod&#10;Figure 3.7&#10;Organizing pods in a microservices architecture with pod labels&#10; &#10;"
    color "green"
  ]
  node [
    id 28
    label "102"
    title "Page_102"
    color "blue"
  ]
  node [
    id 29
    label "text_13"
    title "70&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;  labels:    &#10;    creation_method: manual          &#10;    env: prod                        &#10;spec: &#10;  containers: &#10;  - image: luksa/kubia&#10;    name: kubia&#10;    ports: &#10;    - containerPort: 8080&#10;      protocol: TCP&#10;You&#8217;ve included the labels creation_method=manual and env=data.labels section.&#10;You&#8217;ll create this pod now:&#10;$ kubectl create -f kubia-manual-with-labels.yaml&#10;pod &#34;kubia-manual-v2&#34; created&#10;The kubectl get pods command doesn&#8217;t list any labels by default, but you can see&#10;them by using the --show-labels switch:&#10;$ kubectl get po --show-labels&#10;NAME            READY  STATUS   RESTARTS  AGE LABELS&#10;kubia-manual    1/1    Running  0         16m <none>&#10;kubia-manual-v2 1/1    Running  0         2m  creat_method=manual,env=prod&#10;kubia-zxzij     1/1    Running  0         1d  run=kubia&#10;Instead of listing all labels, if you&#8217;re only interested in certain labels, you can specify&#10;them with the -L switch and have each displayed in its own column. List pods again&#10;and show the columns for the two labels you&#8217;ve attached to your kubia-manual-v2 pod:&#10;$ kubectl get po -L creation_method,env&#10;NAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV&#10;kubia-manual    1/1     Running   0          16m   <none>            <none>&#10;kubia-manual-v2 1/1     Running   0          2m    manual            prod&#10;kubia-zxzij     1/1     Running   0          1d    <none>            <none>&#10;3.3.3&#10;Modifying labels of existing pods&#10;Labels can also be added to and modified on existing pods. Because the kubia-man-&#10;ual pod was also created manually, let&#8217;s add the creation_method=manual label to it: &#10;$ kubectl label po kubia-manual creation_method=manual&#10;pod &#34;kubia-manual&#34; labeled&#10;Now, let&#8217;s also change the env=prod label to env=debug on the kubia-manual-v2 pod,&#10;to see how existing labels can be changed.&#10;NOTE&#10;You need to use the --overwrite option when changing existing labels.&#10;$ kubectl label po kubia-manual-v2 env=debug --overwrite&#10;pod &#34;kubia-manual-v2&#34; labeled&#10;Two labels are &#10;attached to the pod.&#10; &#10;"
    color "green"
  ]
  node [
    id 30
    label "103"
    title "Page_103"
    color "blue"
  ]
  node [
    id 31
    label "text_14"
    title "71&#10;Listing subsets of pods through label selectors&#10;List the pods again to see the updated labels:&#10;$ kubectl get po -L creation_method,env&#10;NAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV&#10;kubia-manual    1/1     Running   0          16m   manual            <none>&#10;kubia-manual-v2 1/1     Running   0          2m    manual            debug&#10;kubia-zxzij     1/1     Running   0          1d    <none>            <none>&#10;As you can see, attaching labels to resources is trivial, and so is changing them on&#10;existing resources. It may not be evident right now, but this is an incredibly powerful&#10;feature, as you&#8217;ll see in the next chapter. But first, let&#8217;s see what you can do with these&#10;labels, in addition to displaying them when listing pods.&#10;3.4&#10;Listing subsets of pods through label selectors&#10;Attaching labels to resources so you can see the labels next to each resource when list-&#10;ing them isn&#8217;t that interesting. But labels go hand in hand with label selectors. Label&#10;selectors allow you to select a subset of pods tagged with certain labels and perform an&#10;operation on those pods. A label selector is a criterion, which filters resources based&#10;on whether they include a certain label with a certain value. &#10; A label selector can select resources based on whether the resource&#10;&#61601;Contains (or doesn&#8217;t contain) a label with a certain key&#10;&#61601;Contains a label with a certain key and value&#10;&#61601;Contains a label with a certain key, but with a value not equal to the one you&#10;specify&#10;3.4.1&#10;Listing pods using a label selector&#10;Let&#8217;s use label selectors on the pods you&#8217;ve created so far. To see all pods you created&#10;manually (you labeled them with creation_method=manual), do the following:&#10;$ kubectl get po -l creation_method=manual&#10;NAME              READY     STATUS    RESTARTS   AGE&#10;kubia-manual      1/1       Running   0          51m&#10;kubia-manual-v2   1/1       Running   0          37m&#10;To list all pods that include the env label, whatever its value is:&#10;$ kubectl get po -l env&#10;NAME              READY     STATUS    RESTARTS   AGE&#10;kubia-manual-v2   1/1       Running   0          37m&#10;And those that don&#8217;t have the env label:&#10;$ kubectl get po -l '!env'&#10;NAME           READY     STATUS    RESTARTS   AGE&#10;kubia-manual   1/1       Running   0          51m&#10;kubia-zxzij    1/1       Running   0          10d&#10; &#10;"
    color "green"
  ]
  node [
    id 32
    label "104"
    title "Page_104"
    color "blue"
  ]
  node [
    id 33
    label "text_15"
    title "72&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;NOTE&#10;Make sure to use single quotes around !env, so the bash shell doesn&#8217;t&#10;evaluate the exclamation mark.&#10;Similarly, you could also match pods with the following label selectors:&#10;&#61601;&#10;creation_method!=manual to select pods with the creation_method label with&#10;any value other than manual&#10;&#61601;&#10;env in (prod,devel) to select pods with the env label set to either prod or&#10;devel&#10;&#61601;&#10;env notin (prod,devel) to select pods with the env label set to any value other&#10;than prod or devel&#10;Turning back to the pods in the microservices-oriented architecture example, you&#10;could select all pods that are part of the product catalog microservice by using the&#10;app=pc label selector (shown in the following figure).&#10;3.4.2&#10;Using multiple conditions in a label selector&#10;A selector can also include multiple comma-separated criteria. Resources need to&#10;match all of them to match the selector. If, for example, you want to select only pods&#10;running the beta release of the product catalog microservice, you&#8217;d use the following&#10;selector: app=pc,rel=beta (visualized in figure 3.9).&#10; Label selectors aren&#8217;t useful only for listing pods, but also for performing actions&#10;on a subset of all pods. For example, later in the chapter, you&#8217;ll see how to use label&#10;selectors to delete multiple pods at once. But label selectors aren&#8217;t used only by&#10;kubectl. They&#8217;re also used internally, as you&#8217;ll see next.&#10;UI pod&#10;app: ui&#10;rel: stable&#10;rel=stable&#10;app=ui&#10;Account&#10;Service&#10;pod&#10;app: as&#10;rel: stable&#10;app=as&#10;app: pc&#10;rel: stable&#10;app=pc&#10;app: sc&#10;rel: stable&#10;app=sc&#10;app: os&#10;rel: stable&#10;app=os&#10;Product&#10;Catalog&#10;pod&#10;Shopping&#10;Cart&#10;pod&#10;Order&#10;Service&#10;pod&#10;UI pod&#10;app: ui&#10;rel: beta&#10;rel=beta&#10;app: pc&#10;rel: beta&#10;app: os&#10;rel: beta&#10;Product&#10;Catalog&#10;pod&#10;Order&#10;Service&#10;pod&#10;rel=canary&#10;Account&#10;Service&#10;pod&#10;app: as&#10;rel: canary&#10;app: pc&#10;rel: canary&#10;app: os&#10;rel: canary&#10;Product&#10;Catalog&#10;pod&#10;Order&#10;Service&#10;pod&#10;Figure 3.8&#10;Selecting the product catalog microservice pods using the &#8220;app=pc&#8221; label selector&#10; &#10;"
    color "green"
  ]
  node [
    id 34
    label "105"
    title "Page_105"
    color "blue"
  ]
  node [
    id 35
    label "text_16"
    title "73&#10;Using labels and selectors to constrain pod scheduling&#10;3.5&#10;Using labels and selectors to constrain pod scheduling&#10;All the pods you&#8217;ve created so far have been scheduled pretty much randomly across&#10;your worker nodes. As I&#8217;ve mentioned in the previous chapter, this is the proper way&#10;of working in a Kubernetes cluster. Because Kubernetes exposes all the nodes in the&#10;cluster as a single, large deployment platform, it shouldn&#8217;t matter to you what node a&#10;pod is scheduled to. Because each pod gets the exact amount of computational&#10;resources it requests (CPU, memory, and so on) and its accessibility from other pods&#10;isn&#8217;t at all affected by the node the pod is scheduled to, usually there shouldn&#8217;t be any&#10;need for you to tell Kubernetes exactly where to schedule your pods. &#10; Certain cases exist, however, where you&#8217;ll want to have at least a little say in where&#10;a pod should be scheduled. A good example is when your hardware infrastructure&#10;isn&#8217;t homogenous. If part of your worker nodes have spinning hard drives, whereas&#10;others have SSDs, you may want to schedule certain pods to one group of nodes and&#10;the rest to the other. Another example is when you need to schedule pods perform-&#10;ing intensive GPU-based computation only to nodes that provide the required GPU&#10;acceleration. &#10; You never want to say specifically what node a pod should be scheduled to, because&#10;that would couple the application to the infrastructure, whereas the whole idea of&#10;Kubernetes is hiding the actual infrastructure from the apps that run on it. But if you&#10;want to have a say in where a pod should be scheduled, instead of specifying an exact&#10;node, you should describe the node requirements and then let Kubernetes select a&#10;node that matches those requirements. This can be done through node labels and&#10;node label selectors. &#10;UI pod&#10;app: ui&#10;rel: stable&#10;rel=stable&#10;app=ui&#10;Account&#10;Service&#10;pod&#10;app: as&#10;rel: stable&#10;app=as&#10;app: pc&#10;rel: stable&#10;app=pc&#10;app: sc&#10;rel: stable&#10;app=sc&#10;app: os&#10;rel: stable&#10;app=os&#10;Product&#10;Catalog&#10;pod&#10;Shopping&#10;Cart&#10;pod&#10;Order&#10;Service&#10;pod&#10;UI pod&#10;app: ui&#10;rel: beta&#10;rel=beta&#10;app: pc&#10;rel: beta&#10;app: os&#10;rel: beta&#10;Product&#10;Catalog&#10;pod&#10;Order&#10;Service&#10;pod&#10;rel=canary&#10;Account&#10;Service&#10;pod&#10;app: as&#10;rel: canary&#10;app: pc&#10;rel: canary&#10;app: os&#10;rel: canary&#10;Product&#10;Catalog&#10;pod&#10;Order&#10;Service&#10;pod&#10;Figure 3.9&#10;Selecting pods with multiple label selectors&#10; &#10;"
    color "green"
  ]
  node [
    id 36
    label "106"
    title "Page_106"
    color "blue"
  ]
  node [
    id 37
    label "text_17"
    title "74&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;3.5.1&#10;Using labels for categorizing worker nodes&#10;As you learned earlier, pods aren&#8217;t the only Kubernetes resource type that you can&#10;attach a label to. Labels can be attached to any Kubernetes object, including nodes.&#10;Usually, when the ops team adds a new node to the cluster, they&#8217;ll categorize the node&#10;by attaching labels specifying the type of hardware the node provides or anything else&#10;that may come in handy when scheduling pods. &#10; Let&#8217;s imagine one of the nodes in your cluster contains a GPU meant to be used&#10;for general-purpose GPU computing. You want to add a label to the node showing this&#10;feature. You&#8217;re going to add the label gpu=true to one of your nodes (pick one out of&#10;the list returned by kubectl get nodes):&#10;$ kubectl label node gke-kubia-85f6-node-0rrx gpu=true&#10;node &#34;gke-kubia-85f6-node-0rrx&#34; labeled&#10;Now you can use a label selector when listing the nodes, like you did before with pods.&#10;List only nodes that include the label gpu=true:&#10;$ kubectl get nodes -l gpu=true&#10;NAME                      STATUS AGE&#10;gke-kubia-85f6-node-0rrx  Ready  1d&#10;As expected, only one node has this label. You can also try listing all the nodes and tell&#10;kubectl to display an additional column showing the values of each node&#8217;s gpu label&#10;(kubectl get nodes -L gpu).&#10;3.5.2&#10;Scheduling pods to specific nodes&#10;Now imagine you want to deploy a new pod that needs a GPU to perform its work.&#10;To ask the scheduler to only choose among the nodes that provide a GPU, you&#8217;ll&#10;add a node selector to the pod&#8217;s YAML. Create a file called kubia-gpu.yaml with the&#10;following listing&#8217;s contents and then use kubectl create -f kubia-gpu.yaml to cre-&#10;ate the pod.&#10;apiVersion: v1                                         &#10;kind: Pod                                              &#10;metadata:                                              &#10;  name: kubia-gpu&#10;spec: &#10;  nodeSelector:               &#10;    gpu: &#34;true&#34;               &#10;  containers: &#10;  - image: luksa/kubia&#10;    name: kubia&#10;Listing 3.4&#10;Using a label selector to schedule a pod to a specific node: kubia-gpu.yaml&#10;nodeSelector tells Kubernetes &#10;to deploy this pod only to &#10;nodes containing the &#10;gpu=true label.&#10; &#10;"
    color "green"
  ]
  node [
    id 38
    label "107"
    title "Page_107"
    color "blue"
  ]
  node [
    id 39
    label "text_18"
    title "75&#10;Annotating pods&#10;You&#8217;ve added a nodeSelector field under the spec section. When you create the pod,&#10;the scheduler will only choose among the nodes that contain the gpu=true label&#10;(which is only a single node in your case). &#10;3.5.3&#10;Scheduling to one specific node&#10;Similarly, you could also schedule a pod to an exact node, because each node also has&#10;a unique label with the key kubernetes.io/hostname and value set to the actual host-&#10;name of the node. But setting the nodeSelector to a specific node by the hostname&#10;label may lead to the pod being unschedulable if the node is offline. You shouldn&#8217;t&#10;think in terms of individual nodes. Always think about logical groups of nodes that sat-&#10;isfy certain criteria specified through label selectors.&#10; This was a quick demonstration of how labels and label selectors work and how&#10;they can be used to influence the operation of Kubernetes. The importance and use-&#10;fulness of label selectors will become even more evident when we talk about Replication-&#10;Controllers and Services in the next two chapters. &#10;NOTE&#10;Additional ways of influencing which node a pod is scheduled to are&#10;covered in chapter 16.&#10;3.6&#10;Annotating pods&#10;In addition to labels, pods and other objects can also contain annotations. Annotations&#10;are also key-value pairs, so in essence, they&#8217;re similar to labels, but they aren&#8217;t meant to&#10;hold identifying information. They can&#8217;t be used to group objects the way labels can.&#10;While objects can be selected through label selectors, there&#8217;s no such thing as an&#10;annotation selector. &#10; On the other hand, annotations can hold much larger pieces of information and&#10;are primarily meant to be used by tools. Certain annotations are automatically added&#10;to objects by Kubernetes, but others are added by users manually.&#10; Annotations are also commonly used when introducing new features to Kuberne-&#10;tes. Usually, alpha and beta versions of new features don&#8217;t introduce any new fields to&#10;API objects. Annotations are used instead of fields, and then once the required API&#10;changes have become clear and been agreed upon by the Kubernetes developers, new&#10;fields are introduced and the related annotations deprecated.&#10; A great use of annotations is adding descriptions for each pod or other API object,&#10;so that everyone using the cluster can quickly look up information about each individ-&#10;ual object. For example, an annotation used to specify the name of the person who&#10;created the object can make collaboration between everyone working on the cluster&#10;much easier.&#10;3.6.1&#10;Looking up an object&#8217;s annotations&#10;Let&#8217;s see an example of an annotation that Kubernetes added automatically to the&#10;pod you created in the previous chapter. To see the annotations, you&#8217;ll need to&#10; &#10;"
    color "green"
  ]
  node [
    id 40
    label "108"
    title "Page_108"
    color "blue"
  ]
  node [
    id 41
    label "text_19"
    title "76&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;request the full YAML of the pod or use the kubectl describe command. You&#8217;ll use the&#10;first option in the following listing.&#10;$ kubectl get po kubia-zxzij -o yaml&#10;apiVersion: v1&#10;kind: pod&#10;metadata:&#10;  annotations:&#10;    kubernetes.io/created-by: |&#10;      {&#34;kind&#34;:&#34;SerializedReference&#34;, &#34;apiVersion&#34;:&#34;v1&#34;, &#10;      &#34;reference&#34;:{&#34;kind&#34;:&#34;ReplicationController&#34;, &#34;namespace&#34;:&#34;default&#34;, ...&#10;Without going into too many details, as you can see, the kubernetes.io/created-by&#10;annotation holds JSON data about the object that created the pod. That&#8217;s not some-&#10;thing you&#8217;d want to put into a label. Labels should be short, whereas annotations can&#10;contain relatively large blobs of data (up to 256 KB in total).&#10;NOTE&#10;The kubernetes.io/created-by annotations was deprecated in ver-&#10;sion 1.8 and will be removed in 1.9, so you will no longer see it in the YAML.&#10;3.6.2&#10;Adding and modifying annotations&#10;Annotations can obviously be added to pods at creation time, the same way labels can.&#10;They can also be added to or modified on existing pods later. The simplest way to add&#10;an annotation to an existing object is through the kubectl annotate command. &#10; You&#8217;ll try adding an annotation to your kubia-manual pod now:&#10;$ kubectl annotate pod kubia-manual mycompany.com/someannotation=&#34;foo bar&#34;&#10;pod &#34;kubia-manual&#34; annotated&#10;You added the annotation mycompany.com/someannotation with the value foo bar.&#10;It&#8217;s a good idea to use this format for annotation keys to prevent key collisions. When&#10;different tools or libraries add annotations to objects, they may accidentally override&#10;each other&#8217;s annotations if they don&#8217;t use unique prefixes like you did here.&#10; You can use kubectl describe to see the annotation you added:&#10;$ kubectl describe pod kubia-manual&#10;...&#10;Annotations:    mycompany.com/someannotation=foo bar&#10;...&#10;3.7&#10;Using namespaces to group resources&#10;Let&#8217;s turn back to labels for a moment. We&#8217;ve seen how they organize pods and other&#10;objects into groups. Because each object can have multiple labels, those groups of&#10;objects can overlap. Plus, when working with the cluster (through kubectl for example),&#10;if you don&#8217;t explicitly specify a label selector, you&#8217;ll always see all objects. &#10;Listing 3.5&#10;A pod&#8217;s annotations&#10; &#10;"
    color "green"
  ]
  node [
    id 42
    label "109"
    title "Page_109"
    color "blue"
  ]
  node [
    id 43
    label "text_20"
    title "77&#10;Using namespaces to group resources&#10; But what about times when you want to split objects into separate, non-overlapping&#10;groups? You may want to only operate inside one group at a time. For this and other&#10;reasons, Kubernetes also groups objects into namespaces. These aren&#8217;t the Linux&#10;namespaces we talked about in chapter 2, which are used to isolate processes from&#10;each other. Kubernetes namespaces provide a scope for objects names. Instead of hav-&#10;ing all your resources in one single namespace, you can split them into multiple name-&#10;spaces, which also allows you to use the same resource names multiple times (across&#10;different namespaces).&#10;3.7.1&#10;Understanding the need for namespaces&#10;Using multiple namespaces allows you to split complex systems with numerous com-&#10;ponents into smaller distinct groups. They can also be used for separating resources&#10;in a multi-tenant environment, splitting up resources into production, development,&#10;and QA environments, or in any other way you may need. Resource names only need&#10;to be unique within a namespace. Two different namespaces can contain resources of&#10;the same name. But, while most types of resources are namespaced, a few aren&#8217;t. One&#10;of them is the Node resource, which is global and not tied to a single namespace.&#10;You&#8217;ll learn about other cluster-level resources in later chapters.&#10; Let&#8217;s see how to use namespaces now.&#10;3.7.2&#10;Discovering other namespaces and their pods&#10;First, let&#8217;s list all namespaces in your cluster:&#10;$ kubectl get ns&#10;NAME          LABELS    STATUS    AGE&#10;default       <none>    Active    1h&#10;kube-public   <none>    Active    1h&#10;kube-system   <none>    Active    1h&#10;Up to this point, you&#8217;ve operated only in the default namespace. When listing resources&#10;with the kubectl get command, you&#8217;ve never specified the namespace explicitly, so&#10;kubectl always defaulted to the default namespace, showing you only the objects in&#10;that namespace. But as you can see from the list, the kube-public and the kube-system&#10;namespaces also exist. Let&#8217;s look at the pods that belong to the kube-system name-&#10;space, by telling kubectl to list pods in that namespace only:&#10;$ kubectl get po --namespace kube-system&#10;NAME                                 READY     STATUS    RESTARTS   AGE&#10;fluentd-cloud-kubia-e8fe-node-txje   1/1       Running   0          1h&#10;heapster-v11-fz1ge                   1/1       Running   0          1h&#10;kube-dns-v9-p8a4t                    0/4       Pending   0          1h&#10;kube-ui-v4-kdlai                     1/1       Running   0          1h&#10;l7-lb-controller-v0.5.2-bue96        2/2       Running   92         1h&#10;TIP&#10;You can also use -n instead of --namespace.&#10; &#10;"
    color "green"
  ]
  node [
    id 44
    label "110"
    title "Page_110"
    color "blue"
  ]
  node [
    id 45
    label "text_21"
    title "78&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;You&#8217;ll learn about these pods later in the book (don&#8217;t worry if the pods shown here&#10;don&#8217;t match the ones on your system exactly). It&#8217;s clear from the name of the name-&#10;space that these are resources related to the Kubernetes system itself. By having&#10;them in this separate namespace, it keeps everything nicely organized. If they were&#10;all in the default namespace, mixed in with the resources you create yourself, you&#8217;d&#10;have a hard time seeing what belongs where, and you might inadvertently delete sys-&#10;tem resources. &#10; Namespaces enable you to separate resources that don&#8217;t belong together into non-&#10;overlapping groups. If several users or groups of users are using the same Kubernetes&#10;cluster, and they each manage their own distinct set of resources, they should each use&#10;their own namespace. This way, they don&#8217;t need to take any special care not to inad-&#10;vertently modify or delete the other users&#8217; resources and don&#8217;t need to concern them-&#10;selves with name conflicts, because namespaces provide a scope for resource names,&#10;as has already been mentioned.&#10;  Besides isolating resources, namespaces are also used for allowing only certain users&#10;access to particular resources and even for limiting the amount of computational&#10;resources available to individual users. You&#8217;ll learn about this in chapters 12 through 14.&#10;3.7.3&#10;Creating a namespace&#10;A namespace is a Kubernetes resource like any other, so you can create it by posting a&#10;YAML file to the Kubernetes API server. Let&#8217;s see how to do this now. &#10;CREATING A NAMESPACE FROM A YAML FILE&#10;First, create a custom-namespace.yaml file with the following listing&#8217;s contents (you&#8217;ll&#10;find the file in the book&#8217;s code archive).&#10;apiVersion: v1&#10;kind: Namespace         &#10;metadata:&#10;  name: custom-namespace  &#10;Now, use kubectl to post the file to the Kubernetes API server:&#10;$ kubectl create -f custom-namespace.yaml&#10;namespace &#34;custom-namespace&#34; created&#10;CREATING A NAMESPACE WITH KUBECTL CREATE NAMESPACE&#10;Although writing a file like the previous one isn&#8217;t a big deal, it&#8217;s still a hassle. Luckily,&#10;you can also create namespaces with the dedicated kubectl create namespace com-&#10;mand, which is quicker than writing a YAML file. By having you create a YAML mani-&#10;fest for the namespace, I wanted to reinforce the idea that everything in Kubernetes&#10;Listing 3.6&#10;A YAML definition of a namespace: custom-namespace.yaml&#10;This says you&#8217;re &#10;defining a namespace.&#10;This is the name &#10;of the namespace.&#10; &#10;"
    color "green"
  ]
  node [
    id 46
    label "111"
    title "Page_111"
    color "blue"
  ]
  node [
    id 47
    label "text_22"
    title "79&#10;Using namespaces to group resources&#10;has a corresponding API object that you can create, read, update, and delete by post-&#10;ing a YAML manifest to the API server.&#10; You could have created the namespace like this:&#10;$ kubectl create namespace custom-namespace&#10;namespace &#34;custom-namespace&#34; created&#10;NOTE&#10;Although most objects&#8217; names must conform to the naming conven-&#10;tions specified in RFC 1035 (Domain names), which means they may contain&#10;only letters, digits, dashes, and dots, namespaces (and a few others) aren&#8217;t&#10;allowed to contain dots. &#10;3.7.4&#10;Managing objects in other namespaces&#10;To create resources in the namespace you&#8217;ve created, either add a namespace: custom-&#10;namespace entry to the metadata section, or specify the namespace when creating the&#10;resource with the kubectl create command:&#10;$ kubectl create -f kubia-manual.yaml -n custom-namespace&#10;pod &#34;kubia-manual&#34; created&#10;You now have two pods with the same name (kubia-manual). One is in the default&#10;namespace, and the other is in your custom-namespace.&#10; When listing, describing, modifying, or deleting objects in other namespaces, you&#10;need to pass the --namespace (or -n) flag to kubectl. If you don&#8217;t specify the name-&#10;space, kubectl performs the action in the default namespace configured in the cur-&#10;rent kubectl context. The current context&#8217;s namespace and the current context itself&#10;can be changed through kubectl config commands. To learn more about managing&#10;kubectl contexts, refer to appendix A. &#10;TIP&#10;To quickly switch to a different namespace, you can set up the following&#10;alias: alias kcd='kubectl config set-context $(kubectl config current-&#10;context) --namespace '. You can then switch between namespaces using kcd&#10;some-namespace.&#10;3.7.5&#10;Understanding the isolation provided by namespaces&#10;To wrap up this section about namespaces, let me explain what namespaces don&#8217;t pro-&#10;vide&#8212;at least not out of the box. Although namespaces allow you to isolate objects&#10;into distinct groups, which allows you to operate only on those belonging to the speci-&#10;fied namespace, they don&#8217;t provide any kind of isolation of running objects. &#10; For example, you may think that when different users deploy pods across different&#10;namespaces, those pods are isolated from each other and can&#8217;t communicate, but that&#8217;s&#10;not necessarily the case. Whether namespaces provide network isolation depends on&#10;which networking solution is deployed with Kubernetes. When the solution doesn&#8217;t&#10;provide inter-namespace network isolation, if a pod in namespace foo knows the IP&#10; &#10;"
    color "green"
  ]
  node [
    id 48
    label "112"
    title "Page_112"
    color "blue"
  ]
  node [
    id 49
    label "text_23"
    title "80&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;address of a pod in namespace bar, there is nothing preventing it from sending traffic,&#10;such as HTTP requests, to the other pod. &#10;3.8&#10;Stopping and removing pods&#10;You&#8217;ve created a number of pods, which should all still be running. You have four&#10;pods running in the default namespace and one pod in custom-namespace. You&#8217;re&#10;going to stop all of them now, because you don&#8217;t need them anymore.&#10;3.8.1&#10;Deleting a pod by name&#10;First, delete the kubia-gpu pod by name:&#10;$ kubectl delete po kubia-gpu&#10;pod &#34;kubia-gpu&#34; deleted&#10;By deleting a pod, you&#8217;re instructing Kubernetes to terminate all the containers that are&#10;part of that pod. Kubernetes sends a SIGTERM signal to the process and waits a certain&#10;number of seconds (30 by default) for it to shut down gracefully. If it doesn&#8217;t shut down&#10;in time, the process is then killed through SIGKILL. To make sure your processes are&#10;always shut down gracefully, they need to handle the SIGTERM signal properly. &#10;TIP&#10;You can also delete more than one pod by specifying multiple, space-sep-&#10;arated names (for example, kubectl delete po pod1 pod2).&#10;3.8.2&#10;Deleting pods using label selectors&#10;Instead of specifying each pod to delete by name, you&#8217;ll now use what you&#8217;ve learned&#10;about label selectors to stop both the kubia-manual and the kubia-manual-v2 pod.&#10;Both pods include the creation_method=manual label, so you can delete them by&#10;using a label selector:&#10;$ kubectl delete po -l creation_method=manual&#10;pod &#34;kubia-manual&#34; deleted&#10;pod &#34;kubia-manual-v2&#34; deleted &#10;In the earlier microservices example, where you had tens (or possibly hundreds) of&#10;pods, you could, for instance, delete all canary pods at once by specifying the&#10;rel=canary label selector (visualized in figure 3.10):&#10;$ kubectl delete po -l rel=canary&#10;3.8.3&#10;Deleting pods by deleting the whole namespace&#10;Okay, back to your real pods. What about the pod in the custom-namespace? You no&#10;longer need either the pods in that namespace, or the namespace itself. You can&#10; &#10;"
    color "green"
  ]
  node [
    id 50
    label "113"
    title "Page_113"
    color "blue"
  ]
  node [
    id 51
    label "text_24"
    title "81&#10;Stopping and removing pods&#10;delete the whole namespace (the pods will be deleted along with the namespace auto-&#10;matically), using the following command:&#10;$ kubectl delete ns custom-namespace&#10;namespace &#34;custom-namespace&#34; deleted&#10;3.8.4&#10;Deleting all pods in a namespace, while keeping the namespace&#10;You&#8217;ve now cleaned up almost everything. But what about the pod you created with&#10;the kubectl run command in chapter 2? That one is still running:&#10;$ kubectl get pods&#10;NAME            READY   STATUS    RESTARTS   AGE&#10;kubia-zxzij     1/1     Running   0          1d    &#10;This time, instead of deleting the specific pod, tell Kubernetes to delete all pods in the&#10;current namespace by using the --all option:&#10;$ kubectl delete po --all&#10;pod &#34;kubia-zxzij&#34; deleted&#10;Now, double check that no pods were left running:&#10;$ kubectl get pods&#10;NAME            READY   STATUS        RESTARTS   AGE&#10;kubia-09as0     1/1     Running       0          1d    &#10;kubia-zxzij     1/1     Terminating   0          1d    &#10;UI pod&#10;app: ui&#10;rel: stable&#10;rel=stable&#10;app=ui&#10;Account&#10;Service&#10;pod&#10;app: as&#10;rel: stable&#10;app=as&#10;app: pc&#10;rel: stable&#10;app=pc&#10;app: sc&#10;rel: stable&#10;app=sc&#10;app: os&#10;rel: stable&#10;app=os&#10;Product&#10;Catalog&#10;pod&#10;Shopping&#10;Cart&#10;pod&#10;Order&#10;Service&#10;pod&#10;UI pod&#10;app: ui&#10;rel: beta&#10;rel=beta&#10;app: pc&#10;rel: beta&#10;app: os&#10;rel: beta&#10;Product&#10;Catalog&#10;pod&#10;Order&#10;Service&#10;pod&#10;rel=canary&#10;Account&#10;Service&#10;pod&#10;app: as&#10;rel: canary&#10;app: pc&#10;rel: canary&#10;app: os&#10;rel: canary&#10;Product&#10;Catalog&#10;pod&#10;Order&#10;Service&#10;pod&#10;Figure 3.10&#10;Selecting and deleting all canary pods through the rel=canary label selector&#10; &#10;"
    color "green"
  ]
  node [
    id 52
    label "114"
    title "Page_114"
    color "blue"
  ]
  node [
    id 53
    label "text_25"
    title "82&#10;CHAPTER 3&#10;Pods: running containers in Kubernetes&#10;Wait, what!?! The kubia-zxzij pod is terminating, but a new pod called kubia-09as0,&#10;which wasn&#8217;t there before, has appeared. No matter how many times you delete all&#10;pods, a new pod called kubia-something will emerge. &#10; You may remember you created your first pod with the kubectl run command. In&#10;chapter 2, I mentioned that this doesn&#8217;t create a pod directly, but instead creates a&#10;ReplicationController, which then creates the pod. As soon as you delete a pod cre-&#10;ated by the ReplicationController, it immediately creates a new one. To delete the&#10;pod, you also need to delete the ReplicationController. &#10;3.8.5&#10;Deleting (almost) all resources in a namespace&#10;You can delete the ReplicationController and the pods, as well as all the Services&#10;you&#8217;ve created, by deleting all resources in the current namespace with a single&#10;command:&#10;$ kubectl delete all --all&#10;pod &#34;kubia-09as0&#34; deleted&#10;replicationcontroller &#34;kubia&#34; deleted&#10;service &#34;kubernetes&#34; deleted&#10;service &#34;kubia-http&#34; deleted&#10;The first all in the command specifies that you&#8217;re deleting resources of all types, and&#10;the --all option specifies that you&#8217;re deleting all resource instances instead of speci-&#10;fying them by name (you already used this option when you ran the previous delete&#10;command).&#10;NOTE&#10;Deleting everything with the all keyword doesn&#8217;t delete absolutely&#10;everything. Certain resources (like Secrets, which we&#8217;ll introduce in chapter 7)&#10;are preserved and need to be deleted explicitly.&#10;As it deletes resources, kubectl will print the name of every resource it deletes. In the&#10;list, you should see the kubia ReplicationController and the kubia-http Service you&#10;created in chapter 2. &#10;NOTE&#10;The kubectl delete all --all command also deletes the kubernetes&#10;Service, but it should be recreated automatically in a few moments.&#10;3.9&#10;Summary&#10;After reading this chapter, you should now have a decent knowledge of the central&#10;building block in Kubernetes. Every other concept you&#8217;ll learn about in the next few&#10;chapters is directly related to pods. &#10; In this chapter, you&#8217;ve learned&#10;&#61601;How to decide whether certain containers should be grouped together in a pod&#10;or not.&#10; &#10;"
    color "green"
  ]
  node [
    id 54
    label "115"
    title "Page_115"
    color "blue"
  ]
  node [
    id 55
    label "text_26"
    title "83&#10;Summary&#10;&#61601;Pods can run multiple processes and are similar to physical hosts in the non-&#10;container world.&#10;&#61601;YAML or JSON descriptors can be written and used to create pods and then&#10;examined to see the specification of a pod and its current state.&#10;&#61601;Labels and label selectors should be used to organize pods and easily perform&#10;operations on multiple pods at once.&#10;&#61601;You can use node labels and selectors to schedule pods only to nodes that have&#10;certain features.&#10;&#61601;Annotations allow attaching larger blobs of data to pods either by people or&#10;tools and libraries.&#10;&#61601;Namespaces can be used to allow different teams to use the same cluster as&#10;though they were using separate Kubernetes clusters.&#10;&#61601;How to use the kubectl explain command to quickly look up the information&#10;on any Kubernetes resource. &#10;In the next chapter, you&#8217;ll learn about ReplicationControllers and other resources&#10;that manage pods.&#10; &#10;"
    color "green"
  ]
  node [
    id 56
    label "116"
    title "Page_116"
    color "blue"
  ]
  node [
    id 57
    label "text_27"
    title "84&#10;Replication and other&#10;controllers: deploying&#10;managed pods&#10;As you&#8217;ve learned so far, pods represent the basic deployable unit in Kubernetes.&#10;You know how to create, supervise, and manage them manually. But in real-world&#10;use cases, you want your deployments to stay up and running automatically and&#10;remain healthy without any manual intervention. To do this, you almost never cre-&#10;ate pods directly. Instead, you create other types of resources, such as Replication-&#10;Controllers or Deployments, which then create and manage the actual pods.&#10; When you create unmanaged pods (such as the ones you created in the previ-&#10;ous chapter), a cluster node is selected to run the pod and then its containers are&#10;run on that node. In this chapter, you&#8217;ll learn that Kubernetes then monitors&#10;This chapter covers&#10;&#61601;Keeping pods healthy&#10;&#61601;Running multiple instances of the same pod&#10;&#61601;Automatically rescheduling pods after a node fails&#10;&#61601;Scaling pods horizontally&#10;&#61601;Running system-level pods on each cluster node&#10;&#61601;Running batch jobs&#10;&#61601;Scheduling jobs to run periodically or once in &#10;the future&#10; &#10;"
    color "green"
  ]
  node [
    id 58
    label "117"
    title "Page_117"
    color "blue"
  ]
  node [
    id 59
    label "text_28"
    title "85&#10;Keeping pods healthy&#10;those containers and automatically restarts them if they fail. But if the whole node&#10;fails, the pods on the node are lost and will not be replaced with new ones, unless&#10;those pods are managed by the previously mentioned ReplicationControllers or simi-&#10;lar. In this chapter, you&#8217;ll learn how Kubernetes checks if a container is still alive and&#10;restarts it if it isn&#8217;t. You&#8217;ll also learn how to run managed pods&#8212;both those that run&#10;indefinitely and those that perform a single task and then stop. &#10;4.1&#10;Keeping pods healthy&#10;One of the main benefits of using Kubernetes is the ability to give it a list of contain-&#10;ers and let it keep those containers running somewhere in the cluster. You do this by&#10;creating a Pod resource and letting Kubernetes pick a worker node for it and run&#10;the pod&#8217;s containers on that node. But what if one of those containers dies? What if&#10;all containers of a pod die? &#10; As soon as a pod is scheduled to a node, the Kubelet on that node will run its con-&#10;tainers and, from then on, keep them running as long as the pod exists. If the con-&#10;tainer&#8217;s main process crashes, the Kubelet will restart the container. If your&#10;application has a bug that causes it to crash every once in a while, Kubernetes will&#10;restart it automatically, so even without doing anything special in the app itself, run-&#10;ning the app in Kubernetes automatically gives it the ability to heal itself. &#10; But sometimes apps stop working without their process crashing. For example, a&#10;Java app with a memory leak will start throwing OutOfMemoryErrors, but the JVM&#10;process will keep running. It would be great to have a way for an app to signal to&#10;Kubernetes that it&#8217;s no longer functioning properly and have Kubernetes restart it. &#10; We&#8217;ve said that a container that crashes is restarted automatically, so maybe you&#8217;re&#10;thinking you could catch these types of errors in the app and exit the process when&#10;they occur. You can certainly do that, but it still doesn&#8217;t solve all your problems. &#10; For example, what about those situations when your app stops responding because&#10;it falls into an infinite loop or a deadlock? To make sure applications are restarted in&#10;such cases, you must check an application&#8217;s health from the outside and not depend&#10;on the app doing it internally. &#10;4.1.1&#10;Introducing liveness probes&#10;Kubernetes can check if a container is still alive through liveness probes. You can specify&#10;a liveness probe for each container in the pod&#8217;s specification. Kubernetes will periodi-&#10;cally execute the probe and restart the container if the probe fails. &#10;NOTE&#10;Kubernetes also supports readiness probes, which we&#8217;ll learn about in the&#10;next chapter. Be sure not to confuse the two. They&#8217;re used for two different&#10;things.&#10;Kubernetes can probe a container using one of the three mechanisms:&#10;&#61601;An HTTP GET probe performs an HTTP GET request on the container&#8217;s IP&#10;address, a port and path you specify. If the probe receives a response, and the&#10; &#10;"
    color "green"
  ]
  node [
    id 60
    label "118"
    title "Page_118"
    color "blue"
  ]
  node [
    id 61
    label "text_29"
    title "86&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;response code doesn&#8217;t represent an error (in other words, if the HTTP response&#10;code is 2xx or 3xx), the probe is considered successful. If the server returns an&#10;error response code or if it doesn&#8217;t respond at all, the probe is considered a fail-&#10;ure and the container will be restarted as a result.&#10;&#61601;A TCP Socket probe tries to open a TCP connection to the specified port of the&#10;container. If the connection is established successfully, the probe is successful.&#10;Otherwise, the container is restarted.&#10;&#61601;An Exec probe executes an arbitrary command inside the container and checks&#10;the command&#8217;s exit status code. If the status code is 0, the probe is successful.&#10;All other codes are considered failures. &#10;4.1.2&#10;Creating an HTTP-based liveness probe&#10;Let&#8217;s see how to add a liveness probe to your Node.js app. Because it&#8217;s a web app, it&#10;makes sense to add a liveness probe that will check whether its web server is serving&#10;requests. But because this particular Node.js app is too simple to ever fail, you&#8217;ll need&#10;to make the app fail artificially. &#10; To properly demo liveness probes, you&#8217;ll modify the app slightly and make it&#10;return a 500 Internal Server Error HTTP status code for each request after the fifth&#10;one&#8212;your app will handle the first five client requests properly and then return an&#10;error on every subsequent request. Thanks to the liveness probe, it should be restarted&#10;when that happens, allowing it to properly handle client requests again.&#10; You can find the code of the new app in the book&#8217;s code archive (in the folder&#10;Chapter04/kubia-unhealthy). I&#8217;ve pushed the container image to Docker Hub, so you&#10;don&#8217;t need to build it yourself. &#10; You&#8217;ll create a new pod that includes an HTTP GET liveness probe. The following&#10;listing shows the YAML for the pod.&#10;apiVersion: v1&#10;kind: pod&#10;metadata:&#10;  name: kubia-liveness&#10;spec:&#10;  containers:&#10;  - image: luksa/kubia-unhealthy   &#10;    name: kubia&#10;    livenessProbe:                 &#10;      httpGet:                     &#10;        path: /                     &#10;        port: 8080       &#10;Listing 4.1&#10;Adding a liveness probe to a pod: kubia-liveness-probe.yaml&#10;This is the image &#10;containing the &#10;(somewhat) &#10;broken app.&#10;A liveness probe that will &#10;perform an HTTP GET&#10;The path to &#10;request in the &#10;HTTP request&#10;The network port&#10;the probe should&#10;connect to&#10; &#10;"
    color "green"
  ]
  node [
    id 62
    label "119"
    title "Page_119"
    color "blue"
  ]
  node [
    id 63
    label "text_30"
    title "87&#10;Keeping pods healthy&#10;The pod descriptor defines an httpGet liveness probe, which tells Kubernetes to peri-&#10;odically perform HTTP GET requests on path / on port 8080 to determine if the con-&#10;tainer is still healthy. These requests start as soon as the container is run.&#10; After five such requests (or actual client requests), your app starts returning&#10;HTTP status code 500, which Kubernetes will treat as a probe failure, and will thus&#10;restart the container. &#10;4.1.3&#10;Seeing a liveness probe in action&#10;To see what the liveness probe does, try creating the pod now. After about a minute and&#10;a half, the container will be restarted. You can see that by running kubectl get:&#10;$ kubectl get po kubia-liveness&#10;NAME             READY     STATUS    RESTARTS   AGE&#10;kubia-liveness   1/1       Running   1          2m&#10;The RESTARTS column shows that the pod&#8217;s container has been restarted once (if you&#10;wait another minute and a half, it gets restarted again, and then the cycle continues&#10;indefinitely).&#10;You can see why the container had to be restarted by looking at what kubectl describe&#10;prints out, as shown in the following listing.&#10;$ kubectl describe po kubia-liveness&#10;Name:           kubia-liveness&#10;...&#10;Containers:&#10;  kubia:&#10;    Container ID:       docker://480986f8&#10;    Image:              luksa/kubia-unhealthy&#10;    Image ID:           docker://sha256:2b208508&#10;    Port:&#10;    State:              Running                            &#10;      Started:          Sun, 14 May 2017 11:41:40 +0200    &#10;Obtaining the application log of a crashed container&#10;In the previous chapter, you learned how to print the application&#8217;s log with kubectl&#10;logs. If your container is restarted, the kubectl logs command will show the log of&#10;the current container. &#10;When you want to figure out why the previous container terminated, you&#8217;ll want to&#10;see those logs instead of the current container&#8217;s logs. This can be done by using&#10;the --previous option:&#10;$ kubectl logs mypod --previous&#10;Listing 4.2&#10;A pod&#8217;s description after its container is restarted&#10;The container is &#10;currently running.&#10; &#10;"
    color "green"
  ]
  node [
    id 64
    label "120"
    title "Page_120"
    color "blue"
  ]
  node [
    id 65
    label "text_31"
    title "88&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;    Last State:         Terminated                         &#10;      Reason:           Error                              &#10;      Exit Code:        137                                &#10;      Started:          Mon, 01 Jan 0001 00:00:00 +0000    &#10;      Finished:         Sun, 14 May 2017 11:41:38 +0200    &#10;    Ready:              True&#10;    Restart Count:      1                                 &#10;    Liveness:           http-get http://:8080/ delay=0s timeout=1s&#10;                        period=10s #success=1 #failure=3&#10;    ...&#10;Events:&#10;... Killing container with id docker://95246981:pod &#34;kubia-liveness ...&#34;&#10;    container &#34;kubia&#34; is unhealthy, it will be killed and re-created.&#10;You can see that the container is currently running, but it previously terminated&#10;because of an error. The exit code was 137, which has a special meaning&#8212;it denotes&#10;that the process was terminated by an external signal. The number 137 is a sum of two&#10;numbers: 128+x, where x is the signal number sent to the process that caused it to ter-&#10;minate. In the example, x equals 9, which is the number of the SIGKILL signal, mean-&#10;ing the process was killed forcibly.&#10; The events listed at the bottom show why the container was killed&#8212;Kubernetes&#10;detected the container was unhealthy, so it killed and re-created it. &#10;NOTE&#10;When a container is killed, a completely new container is created&#8212;it&#8217;s&#10;not the same container being restarted again.&#10;4.1.4&#10;Configuring additional properties of the liveness probe&#10;You may have noticed that kubectl describe also displays additional information&#10;about the liveness probe:&#10;Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 &#10;          &#10149; #failure=3&#10;Beside the liveness probe options you specified explicitly, you can also see additional&#10;properties, such as delay, timeout, period, and so on. The delay=0s part shows that&#10;the probing begins immediately after the container is started. The timeout is set to&#10;only 1 second, so the container must return a response in 1 second or the probe is&#10;counted as failed. The container is probed every 10 seconds (period=10s) and the&#10;container is restarted after the probe fails three consecutive times (#failure=3). &#10; These additional parameters can be customized when defining the probe. For&#10;example, to set the initial delay, add the initialDelaySeconds property to the live-&#10;ness probe as shown in the following listing.&#10;   livenessProbe:          &#10;     httpGet:              &#10;       path: /             &#10;Listing 4.3&#10;A liveness probe with an initial delay: kubia-liveness-probe-initial-delay.yaml&#10;The previous &#10;container terminated &#10;with an error and &#10;exited with code 137.&#10;The container &#10;has been &#10;restarted once.&#10; &#10;"
    color "green"
  ]
  node [
    id 66
    label "121"
    title "Page_121"
    color "blue"
  ]
  node [
    id 67
    label "text_32"
    title "89&#10;Keeping pods healthy&#10;       port: 8080          &#10;     initialDelaySeconds: 15   &#10;If you don&#8217;t set the initial delay, the prober will start probing the container as soon as&#10;it starts, which usually leads to the probe failing, because the app isn&#8217;t ready to start&#10;receiving requests. If the number of failures exceeds the failure threshold, the con-&#10;tainer is restarted before it&#8217;s even able to start responding to requests properly. &#10;TIP&#10;Always remember to set an initial delay to account for your app&#8217;s startup&#10;time.&#10;I&#8217;ve seen this on many occasions and users were confused why their container was&#10;being restarted. But if they&#8217;d used kubectl describe, they&#8217;d have seen that the con-&#10;tainer terminated with exit code 137 or 143, telling them that the pod was terminated&#10;externally. Additionally, the listing of the pod&#8217;s events would show that the container&#10;was killed because of a failed liveness probe. If you see this happening at pod startup,&#10;it&#8217;s because you failed to set initialDelaySeconds appropriately.&#10;NOTE&#10;Exit code 137 signals that the process was killed by an external signal&#10;(exit code is 128 + 9 (SIGKILL). Likewise, exit code 143 corresponds to 128 +&#10;15 (SIGTERM).&#10;4.1.5&#10;Creating effective liveness probes&#10;For pods running in production, you should always define a liveness probe. Without&#10;one, Kubernetes has no way of knowing whether your app is still alive or not. As long&#10;as the process is still running, Kubernetes will consider the container to be healthy. &#10;WHAT A LIVENESS PROBE SHOULD CHECK&#10;Your simplistic liveness probe simply checks if the server is responding. While this may&#10;seem overly simple, even a liveness probe like this does wonders, because it causes the&#10;container to be restarted if the web server running within the container stops&#10;responding to HTTP requests. Compared to having no liveness probe, this is a major&#10;improvement, and may be sufficient in most cases.&#10; But for a better liveness check, you&#8217;d configure the probe to perform requests on a&#10;specific URL path (/health, for example) and have the app perform an internal sta-&#10;tus check of all the vital components running inside the app to ensure none of them&#10;has died or is unresponsive. &#10;TIP&#10;Make sure the /health HTTP endpoint doesn&#8217;t require authentication;&#10;otherwise the probe will always fail, causing your container to be restarted&#10;indefinitely.&#10;Be sure to check only the internals of the app and nothing influenced by an external&#10;factor. For example, a frontend web server&#8217;s liveness probe shouldn&#8217;t return a failure&#10;when the server can&#8217;t connect to the backend database. If the underlying cause is in&#10;the database itself, restarting the web server container will not fix the problem.&#10;Kubernetes will wait 15 seconds &#10;before executing the first probe.&#10; &#10;"
    color "green"
  ]
  node [
    id 68
    label "122"
    title "Page_122"
    color "blue"
  ]
  node [
    id 69
    label "text_33"
    title "90&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;Because the liveness probe will fail again, you&#8217;ll end up with the container restarting&#10;repeatedly until the database becomes accessible again. &#10;KEEPING PROBES LIGHT&#10;Liveness probes shouldn&#8217;t use too many computational resources and shouldn&#8217;t take&#10;too long to complete. By default, the probes are executed relatively often and are&#10;only allowed one second to complete. Having a probe that does heavy lifting can slow&#10;down your container considerably. Later in the book, you&#8217;ll also learn about how to&#10;limit CPU time available to a container. The probe&#8217;s CPU time is counted in the con-&#10;tainer&#8217;s CPU time quota, so having a heavyweight liveness probe will reduce the CPU&#10;time available to the main application processes.&#10;TIP&#10;If you&#8217;re running a Java app in your container, be sure to use an HTTP&#10;GET liveness probe instead of an Exec probe, where you spin up a whole new&#10;JVM to get the liveness information. The same goes for any JVM-based or sim-&#10;ilar applications, whose start-up procedure requires considerable computa-&#10;tional resources.&#10;DON&#8217;T BOTHER IMPLEMENTING RETRY LOOPS IN YOUR PROBES&#10;You&#8217;ve already seen that the failure threshold for the probe is configurable and usu-&#10;ally the probe must fail multiple times before the container is killed. But even if you&#10;set the failure threshold to 1, Kubernetes will retry the probe several times before con-&#10;sidering it a single failed attempt. Therefore, implementing your own retry loop into&#10;the probe is wasted effort.&#10;LIVENESS PROBE WRAP-UP&#10;You now understand that Kubernetes keeps your containers running by restarting&#10;them if they crash or if their liveness probes fail. This job is performed by the Kubelet&#10;on the node hosting the pod&#8212;the Kubernetes Control Plane components running on&#10;the master(s) have no part in this process. &#10; But if the node itself crashes, it&#8217;s the Control Plane that must create replacements for&#10;all the pods that went down with the node. It doesn&#8217;t do that for pods that you create&#10;directly. Those pods aren&#8217;t managed by anything except by the Kubelet, but because the&#10;Kubelet runs on the node itself, it can&#8217;t do anything if the node fails. &#10; To make sure your app is restarted on another node, you need to have the pod&#10;managed by a ReplicationController or similar mechanism, which we&#8217;ll discuss in the&#10;rest of this chapter. &#10;4.2&#10;Introducing ReplicationControllers&#10;A ReplicationController is a Kubernetes resource that ensures its pods are always&#10;kept running. If the pod disappears for any reason, such as in the event of a node&#10;disappearing from the cluster or because the pod was evicted from the node, the&#10;ReplicationController notices the missing pod and creates a replacement pod. &#10; Figure 4.1 shows what happens when a node goes down and takes two pods with it.&#10;Pod A was created directly and is therefore an unmanaged pod, while pod B is managed&#10; &#10;"
    color "green"
  ]
  node [
    id 70
    label "123"
    title "Page_123"
    color "blue"
  ]
  node [
    id 71
    label "text_34"
    title "91&#10;Introducing ReplicationControllers&#10;by a ReplicationController. After the node fails, the ReplicationController creates a&#10;new pod (pod B2) to replace the missing pod B, whereas pod A is lost completely&#8212;&#10;nothing will ever recreate it.&#10; The ReplicationController in the figure manages only a single pod, but Replication-&#10;Controllers, in general, are meant to create and manage multiple copies (replicas) of a&#10;pod. That&#8217;s where ReplicationControllers got their name from. &#10;4.2.1&#10;The operation of a ReplicationController&#10;A ReplicationController constantly monitors the list of running pods and makes sure&#10;the actual number of pods of a &#8220;type&#8221; always matches the desired number. If too few&#10;such pods are running, it creates new replicas from a pod template. If too many such&#10;pods are running, it removes the excess replicas. &#10; You might be wondering how there can be more than the desired number of repli-&#10;cas. This can happen for a few reasons: &#10;&#61601;Someone creates a pod of the same type manually.&#10;&#61601;Someone changes an existing pod&#8217;s &#8220;type.&#8221;&#10;&#61601;Someone decreases the desired number of pods, and so on.&#10;Node 1&#10;Node 1 fails&#10;Pod A&#10;Pod B&#10;Node 2&#10;Various&#10;other pods&#10;Creates and&#10;manages&#10;Node 1&#10;Pod A&#10;Pod B&#10;Node 2&#10;Various&#10;other pods&#10;ReplicationController&#10;ReplicationController&#10;Pod A goes down with Node 1 and is&#10;not recreated, because there is no&#10;ReplicationController overseeing it.&#10;RC notices pod B is&#10;missing and creates&#10;a new pod instance.&#10;Pod B2&#10;Figure 4.1&#10;When a node fails, only pods backed by a ReplicationController are recreated.&#10; &#10;"
    color "green"
  ]
  node [
    id 72
    label "124"
    title "Page_124"
    color "blue"
  ]
  node [
    id 73
    label "text_35"
    title "92&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;I&#8217;ve used the term pod &#8220;type&#8221; a few times. But no such thing exists. Replication-&#10;Controllers don&#8217;t operate on pod types, but on sets of pods that match a certain label&#10;selector (you learned about them in the previous chapter). &#10;INTRODUCING THE CONTROLLER&#8217;S RECONCILIATION LOOP&#10;A ReplicationController&#8217;s job is to make sure that an exact number of pods always&#10;matches its label selector. If it doesn&#8217;t, the ReplicationController takes the appropriate&#10;action to reconcile the actual with the desired number. The operation of a Replication-&#10;Controller is shown in figure 4.2.&#10;UNDERSTANDING THE THREE PARTS OF A REPLICATIONCONTROLLER&#10;A ReplicationController has three essential parts (also shown in figure 4.3):&#10;&#61601;A label selector, which determines what pods are in the ReplicationController&#8217;s scope&#10;&#61601;A replica count, which specifies the desired number of pods that should be running&#10;&#61601;A pod template, which is used when creating new pod replicas&#10;Start&#10;Compare&#10;matched vs.&#10;desired pod&#10;count&#10;Find pods&#10;matching the&#10;label selector&#10;Create additional&#10;pod(s) from&#10;current template&#10;Delete the&#10;excess pod(s)&#10;Too many&#10;Just enough&#10;Too few&#10;Figure 4.2&#10;A ReplicationController&#8217;s reconciliation loop&#10;app: kubia&#10;Pod&#10;Pod template&#10;ReplicationController: kubia&#10;Pod selector:&#10;app=kubia&#10;Replicas: 3&#10;Figure 4.3&#10;The three key parts of a &#10;ReplicationController (pod selector, &#10;replica count, and pod template)&#10; &#10;"
    color "green"
  ]
  node [
    id 74
    label "125"
    title "Page_125"
    color "blue"
  ]
  node [
    id 75
    label "text_36"
    title "93&#10;Introducing ReplicationControllers&#10;A ReplicationController&#8217;s replica count, the label selector, and even the pod tem-&#10;plate can all be modified at any time, but only changes to the replica count affect&#10;existing pods. &#10;UNDERSTANDING THE EFFECT OF CHANGING THE CONTROLLER&#8217;S LABEL SELECTOR OR POD TEMPLATE&#10;Changes to the label selector and the pod template have no effect on existing pods.&#10;Changing the label selector makes the existing pods fall out of the scope of the&#10;ReplicationController, so the controller stops caring about them. ReplicationCon-&#10;trollers also don&#8217;t care about the actual &#8220;contents&#8221; of its pods (the container images,&#10;environment variables, and other things) after they create the pod. The template&#10;therefore only affects new pods created by this ReplicationController. You can think&#10;of it as a cookie cutter for cutting out new pods.&#10;UNDERSTANDING THE BENEFITS OF USING A REPLICATIONCONTROLLER&#10;Like many things in Kubernetes, a ReplicationController, although an incredibly sim-&#10;ple concept, provides or enables the following powerful features:&#10;&#61601;It makes sure a pod (or multiple pod replicas) is always running by starting a&#10;new pod when an existing one goes missing.&#10;&#61601;When a cluster node fails, it creates replacement replicas for all the pods that&#10;were running on the failed node (those that were under the Replication-&#10;Controller&#8217;s control).&#10;&#61601;It enables easy horizontal scaling of pods&#8212;both manual and automatic (see&#10;horizontal pod auto-scaling in chapter 15).&#10;NOTE&#10;A pod instance is never relocated to another node. Instead, the&#10;ReplicationController creates a completely new pod instance that has no rela-&#10;tion to the instance it&#8217;s replacing. &#10;4.2.2&#10;Creating a ReplicationController&#10;Let&#8217;s look at how to create a ReplicationController and then see how it keeps your&#10;pods running. Like pods and other Kubernetes resources, you create a Replication-&#10;Controller by posting a JSON or YAML descriptor to the Kubernetes API server.&#10; You&#8217;re going to create a YAML file called kubia-rc.yaml for your Replication-&#10;Controller, as shown in the following listing.&#10;apiVersion: v1&#10;kind: ReplicationController     &#10;metadata:&#10;  name: kubia                      &#10;spec:&#10;  replicas: 3                     &#10;  selector:              &#10;    app: kubia           &#10;Listing 4.4&#10;A YAML definition of a ReplicationController: kubia-rc.yaml&#10;This manifest defines a &#10;ReplicationController (RC)&#10;The name of this &#10;ReplicationController&#10;The desired number &#10;of pod instances&#10;The pod selector determining &#10;what pods the RC is operating on&#10; &#10;"
    color "green"
  ]
  node [
    id 76
    label "126"
    title "Page_126"
    color "blue"
  ]
  node [
    id 77
    label "text_37"
    title "94&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;  template:                        &#10;    metadata:                      &#10;      labels:                      &#10;        app: kubia                 &#10;    spec:                          &#10;      containers:                  &#10;      - name: kubia                &#10;        image: luksa/kubia         &#10;        ports:                     &#10;        - containerPort: 8080      &#10;When you post the file to the API server, Kubernetes creates a new Replication-&#10;Controller named kubia, which makes sure three pod instances always match the&#10;label selector app=kubia. When there aren&#8217;t enough pods, new pods will be created&#10;from the provided pod template. The contents of the template are almost identical to&#10;the pod definition you created in the previous chapter. &#10; The pod labels in the template must obviously match the label selector of the&#10;ReplicationController; otherwise the controller would create new pods indefinitely,&#10;because spinning up a new pod wouldn&#8217;t bring the actual replica count any closer to&#10;the desired number of replicas. To prevent such scenarios, the API server verifies the&#10;ReplicationController definition and will not accept it if it&#8217;s misconfigured.&#10; Not specifying the selector at all is also an option. In that case, it will be configured&#10;automatically from the labels in the pod template. &#10;TIP&#10;Don&#8217;t specify a pod selector when defining a ReplicationController. Let&#10;Kubernetes extract it from the pod template. This will keep your YAML&#10;shorter and simpler.&#10;To create the ReplicationController, use the kubectl create command, which you&#10;already know:&#10;$ kubectl create -f kubia-rc.yaml&#10;replicationcontroller &#34;kubia&#34; created&#10;As soon as the ReplicationController is created, it goes to work. Let&#8217;s see what&#10;it does.&#10;4.2.3&#10;Seeing the ReplicationController in action&#10;Because no pods exist with the app=kubia label, the ReplicationController should&#10;spin up three new pods from the pod template. List the pods to see if the Replication-&#10;Controller has done what it&#8217;s supposed to:&#10;$ kubectl get pods&#10;NAME          READY     STATUS              RESTARTS   AGE&#10;kubia-53thy   0/1       ContainerCreating   0          2s&#10;kubia-k0xz6   0/1       ContainerCreating   0          2s&#10;kubia-q3vkg   0/1       ContainerCreating   0          2s&#10;The pod template &#10;for creating new &#10;pods&#10; &#10;"
    color "green"
  ]
  node [
    id 78
    label "127"
    title "Page_127"
    color "blue"
  ]
  node [
    id 79
    label "text_38"
    title "95&#10;Introducing ReplicationControllers&#10;Indeed, it has! You wanted three pods, and it created three pods. It&#8217;s now managing&#10;those three pods. Next you&#8217;ll mess with them a little to see how the Replication-&#10;Controller responds. &#10;SEEING THE REPLICATIONCONTROLLER RESPOND TO A DELETED POD&#10;First, you&#8217;ll delete one of the pods manually to see how the ReplicationController spins&#10;up a new one immediately, bringing the number of matching pods back to three:&#10;$ kubectl delete pod kubia-53thy&#10;pod &#34;kubia-53thy&#34; deleted&#10;Listing the pods again shows four of them, because the one you deleted is terminat-&#10;ing, and a new pod has already been created:&#10;$ kubectl get pods&#10;NAME          READY     STATUS              RESTARTS   AGE&#10;kubia-53thy   1/1       Terminating         0          3m&#10;kubia-oini2   0/1       ContainerCreating   0          2s&#10;kubia-k0xz6   1/1       Running             0          3m&#10;kubia-q3vkg   1/1       Running             0          3m&#10;The ReplicationController has done its job again. It&#8217;s a nice little helper, isn&#8217;t it?&#10;GETTING INFORMATION ABOUT A REPLICATIONCONTROLLER&#10;Now, let&#8217;s see what information the kubectl get command shows for Replication-&#10;Controllers:&#10;$ kubectl get rc&#10;NAME      DESIRED   CURRENT   READY     AGE&#10;kubia     3         3         2         3m&#10;NOTE&#10;We&#8217;re using rc as a shorthand for replicationcontroller.&#10;You see three columns showing the desired number of pods, the actual number of&#10;pods, and how many of them are ready (you&#8217;ll learn what that means in the next chap-&#10;ter, when we talk about readiness probes).&#10; You can see additional information about your ReplicationController with the&#10;kubectl describe command, as shown in the following listing.&#10;$ kubectl describe rc kubia&#10;Name:           kubia&#10;Namespace:      default&#10;Selector:       app=kubia&#10;Labels:         app=kubia&#10;Annotations:    <none>&#10;Replicas:       3 current / 3 desired               &#10;Pods Status:    4 Running / 0 Waiting / 0 Succeeded / 0 Failed  &#10;Pod Template:&#10;  Labels:       app=kubia&#10;  Containers:   ...&#10;Listing 4.5&#10;Displaying details of a ReplicationController with kubectl describe&#10;The actual vs. the &#10;desired number of &#10;pod instances&#10;Number of &#10;pod instances &#10;per pod &#10;status&#10; &#10;"
    color "green"
  ]
  node [
    id 80
    label "128"
    title "Page_128"
    color "blue"
  ]
  node [
    id 81
    label "text_39"
    title "96&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;  Volumes:      <none>&#10;Events:                                                   &#10;From                    Type      Reason           Message&#10;----                    -------  ------            -------&#10;replication-controller  Normal   SuccessfulCreate  Created pod: kubia-53thy&#10;replication-controller  Normal   SuccessfulCreate  Created pod: kubia-k0xz6&#10;replication-controller  Normal   SuccessfulCreate  Created pod: kubia-q3vkg&#10;replication-controller  Normal   SuccessfulCreate  Created pod: kubia-oini2&#10;The current number of replicas matches the desired number, because the controller&#10;has already created a new pod. It shows four running pods because a pod that&#8217;s termi-&#10;nating is still considered running, although it isn&#8217;t counted in the current replica count. &#10; The list of events at the bottom shows the actions taken by the Replication-&#10;Controller&#8212;it has created four pods so far.&#10;UNDERSTANDING EXACTLY WHAT CAUSED THE CONTROLLER TO CREATE A NEW POD&#10;The controller is responding to the deletion of a pod by creating a new replacement&#10;pod (see figure 4.4). Well, technically, it isn&#8217;t responding to the deletion itself, but the&#10;resulting state&#8212;the inadequate number of pods.&#10; While a ReplicationController is immediately notified about a pod being deleted&#10;(the API server allows clients to watch for changes to resources and resource lists), that&#8217;s&#10;not what causes it to create a replacement pod. The notification triggers the controller&#10;to check the actual number of pods and take appropriate action.&#10;The events &#10;related to this &#10;ReplicationController&#10;Before deletion&#10;After deletion&#10;ReplicationController: kubia&#10;Replicas: 3&#10;Selector: app=kubia&#10;app: kubia&#10;Pod:&#10;kubia-q3vkg&#10;app: kubia&#10;Pod:&#10;kubia-oini2&#10;[ContainerCreating]&#10;[Terminating]&#10;app: kubia&#10;Pod:&#10;kubia-k0xz6&#10;app: kubia&#10;Pod:&#10;kubia-53thy&#10;ReplicationController: kubia&#10;Replicas: 3&#10;Selector: app=kubia&#10;app: kubia&#10;Pod:&#10;kubia-q3vkg&#10;app: kubia&#10;Pod:&#10;kubia-k0xz6&#10;app: kubia&#10;Pod:&#10;kubia-53thy&#10;Delete kubia-53thy&#10;Figure 4.4&#10;If a pod disappears, the ReplicationController sees too few pods and creates a new replacement pod.&#10; &#10;"
    color "green"
  ]
  node [
    id 82
    label "129"
    title "Page_129"
    color "blue"
  ]
  node [
    id 83
    label "text_40"
    title "97&#10;Introducing ReplicationControllers&#10;RESPONDING TO A NODE FAILURE&#10;Seeing the ReplicationController respond to the manual deletion of a pod isn&#8217;t too&#10;interesting, so let&#8217;s look at a better example. If you&#8217;re using Google Kubernetes Engine&#10;to run these examples, you have a three-node Kubernetes cluster. You&#8217;re going to dis-&#10;connect one of the nodes from the network to simulate a node failure.&#10;NOTE&#10;If you&#8217;re using Minikube, you can&#8217;t do this exercise, because you only&#10;have one node that acts both as a master and a worker node.&#10;If a node fails in the non-Kubernetes world, the ops team would need to migrate the&#10;applications running on that node to other machines manually. Kubernetes, on the&#10;other hand, does that automatically. Soon after the ReplicationController detects that&#10;its pods are down, it will spin up new pods to replace them. &#10; Let&#8217;s see this in action. You need to ssh into one of the nodes with the gcloud&#10;compute ssh command and then shut down its network interface with sudo ifconfig&#10;eth0 down, as shown in the following listing.&#10;NOTE&#10;Choose a node that runs at least one of your pods by listing pods with&#10;the -o wide option.&#10;$ gcloud compute ssh gke-kubia-default-pool-b46381f1-zwko&#10;Enter passphrase for key '/home/luksa/.ssh/google_compute_engine':&#10;Welcome to Kubernetes v1.6.4!&#10;...&#10;luksa@gke-kubia-default-pool-b46381f1-zwko ~ $ sudo ifconfig eth0 down&#10;When you shut down the network interface, the ssh session will stop responding, so&#10;you need to open up another terminal or hard-exit from the ssh session. In the new&#10;terminal you can list the nodes to see if Kubernetes has detected that the node is&#10;down. This takes a minute or so. Then, the node&#8217;s status is shown as NotReady:&#10;$ kubectl get node&#10;NAME                                   STATUS     AGE&#10;gke-kubia-default-pool-b46381f1-opc5   Ready      5h&#10;gke-kubia-default-pool-b46381f1-s8gj   Ready      5h&#10;gke-kubia-default-pool-b46381f1-zwko   NotReady   5h    &#10;If you list the pods now, you&#8217;ll still see the same three pods as before, because Kuber-&#10;netes waits a while before rescheduling pods (in case the node is unreachable because&#10;of a temporary network glitch or because the Kubelet is restarting). If the node stays&#10;unreachable for several minutes, the status of the pods that were scheduled to that&#10;node changes to Unknown. At that point, the ReplicationController will immediately&#10;spin up a new pod. You can see this by listing the pods again:&#10;Listing 4.6&#10;Simulating a node failure by shutting down its network interface&#10;Node isn&#8217;t ready, &#10;because it&#8217;s &#10;disconnected from &#10;the network&#10; &#10;"
    color "green"
  ]
  node [
    id 84
    label "130"
    title "Page_130"
    color "blue"
  ]
  node [
    id 85
    label "text_41"
    title "98&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;$ kubectl get pods&#10;NAME          READY   STATUS    RESTARTS   AGE&#10;kubia-oini2   1/1     Running   0          10m&#10;kubia-k0xz6   1/1     Running   0          10m&#10;kubia-q3vkg   1/1     Unknown   0          10m    &#10;kubia-dmdck   1/1     Running   0          5s    &#10;Looking at the age of the pods, you see that the kubia-dmdck pod is new. You again&#10;have three pod instances running, which means the ReplicationController has again&#10;done its job of bringing the actual state of the system to the desired state. &#10; The same thing happens if a node fails (either breaks down or becomes unreach-&#10;able). No immediate human intervention is necessary. The system heals itself&#10;automatically. &#10; To bring the node back, you need to reset it with the following command:&#10;$ gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko&#10;When the node boots up again, its status should return to Ready, and the pod whose&#10;status was Unknown will be deleted.&#10;4.2.4&#10;Moving pods in and out of the scope of a ReplicationController&#10;Pods created by a ReplicationController aren&#8217;t tied to the ReplicationController in&#10;any way. At any moment, a ReplicationController manages pods that match its label&#10;selector. By changing a pod&#8217;s labels, it can be removed from or added to the scope&#10;of a ReplicationController. It can even be moved from one ReplicationController to&#10;another.&#10;TIP&#10;Although a pod isn&#8217;t tied to a ReplicationController, the pod does refer-&#10;ence it in the metadata.ownerReferences field, which you can use to easily&#10;find which ReplicationController a pod belongs to.&#10;If you change a pod&#8217;s labels so they no longer match a ReplicationController&#8217;s label&#10;selector, the pod becomes like any other manually created pod. It&#8217;s no longer man-&#10;aged by anything. If the node running the pod fails, the pod is obviously not resched-&#10;uled. But keep in mind that when you changed the pod&#8217;s labels, the replication&#10;controller noticed one pod was missing and spun up a new pod to replace it.&#10; Let&#8217;s try this with your pods. Because your ReplicationController manages pods&#10;that have the app=kubia label, you need to either remove this label or change its value&#10;to move the pod out of the ReplicationController&#8217;s scope. Adding another label will&#10;have no effect, because the ReplicationController doesn&#8217;t care if the pod has any addi-&#10;tional labels. It only cares whether the pod has all the labels referenced in the label&#10;selector. &#10;This pod&#8217;s status is &#10;unknown, because its &#10;node is unreachable.&#10;This pod was created &#10;five seconds ago.&#10; &#10;"
    color "green"
  ]
  node [
    id 86
    label "131"
    title "Page_131"
    color "blue"
  ]
  node [
    id 87
    label "text_42"
    title "99&#10;Introducing ReplicationControllers&#10;ADDING LABELS TO PODS MANAGED BY A REPLICATIONCONTROLLER&#10;Let&#8217;s confirm that a ReplicationController doesn&#8217;t care if you add additional labels to&#10;its managed pods:&#10;$ kubectl label pod kubia-dmdck type=special&#10;pod &#34;kubia-dmdck&#34; labeled&#10;$ kubectl get pods --show-labels&#10;NAME          READY   STATUS    RESTARTS   AGE   LABELS&#10;kubia-oini2   1/1     Running   0          11m   app=kubia&#10;kubia-k0xz6   1/1     Running   0          11m   app=kubia&#10;kubia-dmdck   1/1     Running   0          1m    app=kubia,type=special&#10;You&#8217;ve added the type=special label to one of the pods. Listing all pods again shows&#10;the same three pods as before, because no change occurred as far as the Replication-&#10;Controller is concerned.&#10;CHANGING THE LABELS OF A MANAGED POD&#10;Now, you&#8217;ll change the app=kubia label to something else. This will make the pod no&#10;longer match the ReplicationController&#8217;s label selector, leaving it to only match two&#10;pods. The ReplicationController should therefore start a new pod to bring the num-&#10;ber back to three:&#10;$ kubectl label pod kubia-dmdck app=foo --overwrite&#10;pod &#34;kubia-dmdck&#34; labeled&#10;The --overwrite argument is necessary; otherwise kubectl will only print out a warn-&#10;ing and won&#8217;t change the label, to prevent you from inadvertently changing an exist-&#10;ing label&#8217;s value when your intent is to add a new one. &#10; Listing all the pods again should now show four pods: &#10;$ kubectl get pods -L app&#10;NAME         READY  STATUS             RESTARTS  AGE  APP&#10;kubia-2qneh  0/1    ContainerCreating  0         2s   kubia   &#10;kubia-oini2  1/1    Running            0         20m  kubia&#10;kubia-k0xz6  1/1    Running            0         20m  kubia&#10;kubia-dmdck  1/1    Running            0         10m  foo    &#10;NOTE&#10;You&#8217;re using the -L app option to display the app label in a column.&#10;There, you now have four pods altogether: one that isn&#8217;t managed by your Replication-&#10;Controller and three that are. Among them is the newly created pod.&#10; Figure 4.5 illustrates what happened when you changed the pod&#8217;s labels so they no&#10;longer matched the ReplicationController&#8217;s pod selector. You can see your three pods&#10;and your ReplicationController. After you change the pod&#8217;s label from app=kubia to&#10;app=foo, the ReplicationController no longer cares about the pod. Because the con-&#10;troller&#8217;s replica count is set to 3 and only two pods match the label selector, the&#10;Newly created pod that replaces&#10;the pod you removed from the&#10;scope of the ReplicationController&#10;Pod no longer &#10;managed by the &#10;ReplicationController&#10; &#10;"
    color "green"
  ]
  node [
    id 88
    label "132"
    title "Page_132"
    color "blue"
  ]
  node [
    id 89
    label "text_43"
    title "100&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;ReplicationController spins up pod kubia-2qneh to bring the number back up to&#10;three. Pod kubia-dmdck is now completely independent and will keep running until&#10;you delete it manually (you can do that now, because you don&#8217;t need it anymore).&#10;REMOVING PODS FROM CONTROLLERS IN PRACTICE&#10;Removing a pod from the scope of the ReplicationController comes in handy when&#10;you want to perform actions on a specific pod. For example, you might have a bug&#10;that causes your pod to start behaving badly after a specific amount of time or a spe-&#10;cific event. If you know a pod is malfunctioning, you can take it out of the Replication-&#10;Controller&#8217;s scope, let the controller replace it with a new one, and then debug or&#10;play with the pod in any way you want. Once you&#8217;re done, you delete the pod. &#10;CHANGING THE REPLICATIONCONTROLLER&#8217;S LABEL SELECTOR&#10;As an exercise to see if you fully understand ReplicationControllers, what do you&#10;think would happen if instead of changing the labels of a pod, you modified the&#10;ReplicationController&#8217;s label selector? &#10; If your answer is that it would make all the pods fall out of the scope of the&#10;ReplicationController, which would result in it creating three new pods, you&#8217;re abso-&#10;lutely right. And it shows that you understand how ReplicationControllers work. &#10; Kubernetes does allow you to change a ReplicationController&#8217;s label selector, but&#10;that&#8217;s not the case for the other resources that are covered in the second half of this&#10;Initial state&#10;After re-labelling&#10;Re-label kubia-dmdck&#10;app: kubia&#10;Pod:&#10;kubia-oini2&#10;app: kubia&#10;Pod:&#10;kubia-2qneh&#10;[ContainerCreating]&#10;Pod:&#10;kubia-dmdck&#10;app: kubia&#10;Pod:&#10;kubia-k0xz6&#10;app: kubia&#10;type: special&#10;type: special&#10;app: foo&#10;app: kubia&#10;Pod:&#10;kubia-dmdck&#10;app: kubia&#10;Pod:&#10;kubia-k0xz6&#10;ReplicationController: kubia&#10;Replicas: 3&#10;Selector: app=kubia&#10;ReplicationController: kubia&#10;Replicas: 3&#10;Selector: app=kubia&#10;Pod:&#10;kubia-oini2&#10;Figure 4.5&#10;Removing a pod from the scope of a ReplicationController by changing its labels &#10; &#10;"
    color "green"
  ]
  node [
    id 90
    label "133"
    title "Page_133"
    color "blue"
  ]
  node [
    id 91
    label "text_44"
    title "101&#10;Introducing ReplicationControllers&#10;chapter and which are also used for managing pods. You&#8217;ll never change a controller&#8217;s&#10;label selector, but you&#8217;ll regularly change its pod template. Let&#8217;s take a look at that.&#10;4.2.5&#10;Changing the pod template&#10;A ReplicationController&#8217;s pod template can be modified at any time. Changing the pod&#10;template is like replacing a cookie cutter with another one. It will only affect the cookies&#10;you cut out afterward and will have no effect on the ones you&#8217;ve already cut (see figure&#10;4.6). To modify the old pods, you&#8217;d need to delete them and let the Replication-&#10;Controller replace them with new ones based on the new template.&#10;As an exercise, you can try editing the ReplicationController and adding a label to the&#10;pod template. You can edit the ReplicationController with the following command:&#10;$ kubectl edit rc kubia&#10;This will open the ReplicationController&#8217;s YAML definition in your default text editor.&#10;Find the pod template section and add an additional label to the metadata. After you&#10;save your changes and exit the editor, kubectl will update the ReplicationController&#10;and print the following message:&#10;replicationcontroller &#34;kubia&#34; edited&#10;You can now list pods and their labels again and confirm that they haven&#8217;t changed.&#10;But if you delete the pods and wait for their replacements to be created, you&#8217;ll see the&#10;new label.&#10; Editing a ReplicationController like this to change the container image in the pod&#10;template, deleting the existing pods, and letting them be replaced with new ones from&#10;the new template could be used for upgrading pods, but you&#8217;ll learn a better way of&#10;doing that in chapter 9. &#10;Replication&#10;Controller&#10;Replicas: 3&#10;Template:&#10;A&#10;B&#10;C&#10;Replication&#10;Controller&#10;Replicas: 3&#10;Template:&#10;A&#10;Replication&#10;Controller&#10;Replicas: 3&#10;Template:&#10;A&#10;Replication&#10;Controller&#10;Replicas: 3&#10;Template:&#10;D&#10;A&#10;B&#10;C&#10;A&#10;B&#10;C&#10;A&#10;B&#10;Change&#10;template&#10;Delete&#10;a pod&#10;RC creates&#10;new pod&#10;Figure 4.6&#10;Changing a ReplicationController&#8217;s pod template only affects pods created afterward and has no &#10;effect on existing pods.&#10; &#10;"
    color "green"
  ]
  node [
    id 92
    label "134"
    title "Page_134"
    color "blue"
  ]
  node [
    id 93
    label "text_45"
    title "102&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;4.2.6&#10;Horizontally scaling pods&#10;You&#8217;ve seen how ReplicationControllers make sure a specific number of pod instances&#10;is always running. Because it&#8217;s incredibly simple to change the desired number of rep-&#10;licas, this also means scaling pods horizontally is trivial. &#10; Scaling the number of pods up or down is as easy as changing the value of the rep-&#10;licas field in the ReplicationController resource. After the change, the Replication-&#10;Controller will either see too many pods exist (when scaling down) and delete part of&#10;them, or see too few of them (when scaling up) and create additional pods. &#10;SCALING UP A REPLICATIONCONTROLLER&#10;Your ReplicationController has been keeping three instances of your pod running.&#10;You&#8217;re going to scale that number up to 10 now. As you may remember, you&#8217;ve&#10;already scaled a ReplicationController in chapter 2. You could use the same com-&#10;mand as before:&#10;$ kubectl scale rc kubia --replicas=10&#10;But you&#8217;ll do it differently this time. &#10;SCALING A REPLICATIONCONTROLLER BY EDITING ITS DEFINITION&#10;Instead of using the kubectl scale command, you&#8217;re going to scale it in a declarative&#10;way by editing the ReplicationController&#8217;s definition:&#10;$ kubectl edit rc kubia&#10;When the text editor opens, find the spec.replicas field and change its value to 10,&#10;as shown in the following listing.&#10;# Please edit the object below. Lines beginning with a '#' will be ignored,&#10;# and an empty file will abort the edit. If an error occurs while saving &#10;# this file will be reopened with the relevant failures.&#10;apiVersion: v1&#10;kind: ReplicationController&#10;Configuring kubectl edit to use a different text editor&#10;You can tell kubectl to use a text editor of your choice by setting the KUBE_EDITOR&#10;environment variable. For example, if you&#8217;d like to use nano for editing Kubernetes&#10;resources, execute the following command (or put it into your ~/.bashrc or an&#10;equivalent file):&#10;export KUBE_EDITOR=&#34;/usr/bin/nano&#34;&#10;If the KUBE_EDITOR environment variable isn&#8217;t set, kubectl edit falls back to using&#10;the default editor, usually configured through the EDITOR environment variable.&#10;Listing 4.7&#10;Editing the RC in a text editor by running kubectl edit&#10; &#10;"
    color "green"
  ]
  node [
    id 94
    label "135"
    title "Page_135"
    color "blue"
  ]
  node [
    id 95
    label "text_46"
    title "103&#10;Introducing ReplicationControllers&#10;metadata:&#10;  ...&#10;spec:&#10;  replicas: 3        &#10;  selector:&#10;    app: kubia&#10;  ...&#10;When you save the file and close the editor, the ReplicationController is updated and&#10;it immediately scales the number of pods to 10:&#10;$ kubectl get rc&#10;NAME      DESIRED   CURRENT   READY     AGE&#10;kubia     10        10        4         21m&#10;There you go. If the kubectl scale command makes it look as though you&#8217;re telling&#10;Kubernetes exactly what to do, it&#8217;s now much clearer that you&#8217;re making a declarative&#10;change to the desired state of the ReplicationController and not telling Kubernetes to&#10;do something.&#10;SCALING DOWN WITH THE KUBECTL SCALE COMMAND&#10;Now scale back down to 3. You can use the kubectl scale command:&#10;$ kubectl scale rc kubia --replicas=3&#10;All this command does is modify the spec.replicas field of the ReplicationController&#8217;s&#10;definition&#8212;like when you changed it through kubectl edit. &#10;UNDERSTANDING THE DECLARATIVE APPROACH TO SCALING&#10;Horizontally scaling pods in Kubernetes is a matter of stating your desire: &#8220;I want to&#10;have x number of instances running.&#8221; You&#8217;re not telling Kubernetes what or how to do&#10;it. You&#8217;re just specifying the desired state. &#10; This declarative approach makes interacting with a Kubernetes cluster easy. Imag-&#10;ine if you had to manually determine the current number of running instances and&#10;then explicitly tell Kubernetes how many additional instances to run. That&#8217;s more&#10;work and is much more error-prone. Changing a simple number is much easier, and&#10;in chapter 15, you&#8217;ll learn that even that can be done by Kubernetes itself if you&#10;enable horizontal pod auto-scaling. &#10;4.2.7&#10;Deleting a ReplicationController&#10;When you delete a ReplicationController through kubectl delete, the pods are also&#10;deleted. But because pods created by a ReplicationController aren&#8217;t an integral part&#10;of the ReplicationController, and are only managed by it, you can delete only the&#10;ReplicationController and leave the pods running, as shown in figure 4.7.&#10; This may be useful when you initially have a set of pods managed by a Replication-&#10;Controller, and then decide to replace the ReplicationController with a ReplicaSet,&#10;for example (you&#8217;ll learn about them next.). You can do this without affecting the&#10;Change the number 3 &#10;to number 10 in &#10;this line.&#10; &#10;"
    color "green"
  ]
  node [
    id 96
    label "136"
    title "Page_136"
    color "blue"
  ]
  node [
    id 97
    label "text_47"
    title "104&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;pods and keep them running without interruption while you replace the Replication-&#10;Controller that manages them. &#10; When deleting a ReplicationController with kubectl delete, you can keep its&#10;pods running by passing the --cascade=false option to the command. Try that now:&#10;$ kubectl delete rc kubia --cascade=false&#10;replicationcontroller &#34;kubia&#34; deleted&#10;You&#8217;ve deleted the ReplicationController so the pods are on their own. They are no&#10;longer managed. But you can always create a new ReplicationController with the&#10;proper label selector and make them managed again.&#10;4.3&#10;Using ReplicaSets instead of ReplicationControllers&#10;Initially, ReplicationControllers were the only Kubernetes component for replicating&#10;pods and rescheduling them when nodes failed. Later, a similar resource called a&#10;ReplicaSet was introduced. It&#8217;s a new generation of ReplicationController and&#10;replaces it completely (ReplicationControllers will eventually be deprecated). &#10; You could have started this chapter by creating a ReplicaSet instead of a Replication-&#10;Controller, but I felt it would be a good idea to start with what was initially available in&#10;Kubernetes. Plus, you&#8217;ll still see ReplicationControllers used in the wild, so it&#8217;s good&#10;for you to know about them. That said, you should always create ReplicaSets instead&#10;of ReplicationControllers from now on. They&#8217;re almost identical, so you shouldn&#8217;t&#10;have any trouble using them instead. &#10;Before the RC deletion&#10;After the RC deletion&#10;Delete RC&#10;Pod:&#10;kubia-q3vkg&#10;Pod:&#10;kubia-53thy&#10;Pod:&#10;kubia-k0xz6&#10;Pod:&#10;kubia-q3vkg&#10;Pod:&#10;kubia-53thy&#10;Pod:&#10;kubia-k0xz6&#10;ReplicationController: kubia&#10;Replicas: 3&#10;Selector: app=kubia&#10;app: kubia&#10;app: kubia&#10;app: kubia&#10;app: kubia&#10;app: kubia&#10;app: kubia&#10;Figure 4.7&#10;Deleting a replication controller with --cascade=false leaves pods unmanaged.&#10; &#10;"
    color "green"
  ]
  node [
    id 98
    label "137"
    title "Page_137"
    color "blue"
  ]
  node [
    id 99
    label "text_48"
    title "105&#10;Using ReplicaSets instead of ReplicationControllers&#10; You usually won&#8217;t create them directly, but instead have them created automati-&#10;cally when you create the higher-level Deployment resource, which you&#8217;ll learn about&#10;in chapter 9. In any case, you should understand ReplicaSets, so let&#8217;s see how they dif-&#10;fer from ReplicationControllers.&#10;4.3.1&#10;Comparing a ReplicaSet to a ReplicationController&#10;A ReplicaSet behaves exactly like a ReplicationController, but it has more expressive&#10;pod selectors. Whereas a ReplicationController&#8217;s label selector only allows matching&#10;pods that include a certain label, a ReplicaSet&#8217;s selector also allows matching pods&#10;that lack a certain label or pods that include a certain label key, regardless of&#10;its value.&#10; Also, for example, a single ReplicationController can&#8217;t match pods with the label&#10;env=production and those with the label env=devel at the same time. It can only match&#10;either pods with the env=production label or pods with the env=devel label. But a sin-&#10;gle ReplicaSet can match both sets of pods and treat them as a single group. &#10; Similarly, a ReplicationController can&#8217;t match pods based merely on the presence&#10;of a label key, regardless of its value, whereas a ReplicaSet can. For example, a Replica-&#10;Set can match all pods that include a label with the key env, whatever its actual value is&#10;(you can think of it as env=*).&#10;4.3.2&#10;Defining a ReplicaSet&#10;You&#8217;re going to create a ReplicaSet now to see how the orphaned pods that were cre-&#10;ated by your ReplicationController and then abandoned earlier can now be adopted&#10;by a ReplicaSet. First, you&#8217;ll rewrite your ReplicationController into a ReplicaSet by&#10;creating a new file called kubia-replicaset.yaml with the contents in the following&#10;listing.&#10;apiVersion: apps/v1beta2      &#10;kind: ReplicaSet                    &#10;metadata:&#10;  name: kubia&#10;spec:&#10;  replicas: 3&#10;  selector:&#10;    matchLabels:                 &#10;      app: kubia                 &#10;  template:                        &#10;    metadata:                      &#10;      labels:                      &#10;        app: kubia                 &#10;    spec:                          &#10;      containers:                  &#10;      - name: kubia                &#10;        image: luksa/kubia         &#10;Listing 4.8&#10;A YAML definition of a ReplicaSet: kubia-replicaset.yaml&#10;ReplicaSets aren&#8217;t part of the v1 &#10;API, but belong to the apps API &#10;group and version v1beta2.&#10;You&#8217;re using the simpler matchLabels &#10;selector here, which is much like a &#10;ReplicationController&#8217;s selector.&#10;The template is &#10;the same as in the &#10;ReplicationController.&#10; &#10;"
    color "green"
  ]
  node [
    id 100
    label "138"
    title "Page_138"
    color "blue"
  ]
  node [
    id 101
    label "text_49"
    title "106&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;The first thing to note is that ReplicaSets aren&#8217;t part of the v1 API, so you need to&#10;ensure you specify the proper apiVersion when creating the resource. You&#8217;re creating a&#10;resource of type ReplicaSet which has much the same contents as the Replication-&#10;Controller you created earlier. &#10; The only difference is in the selector. Instead of listing labels the pods need to&#10;have directly under the selector property, you&#8217;re specifying them under selector&#10;.matchLabels. This is the simpler (and less expressive) way of defining label selectors&#10;in a ReplicaSet. Later, you&#8217;ll look at the more expressive option, as well.&#10;Because you still have three pods matching the app=kubia selector running from ear-&#10;lier, creating this ReplicaSet will not cause any new pods to be created. The ReplicaSet&#10;will take those existing three pods under its wing. &#10;4.3.3&#10;Creating and examining a ReplicaSet&#10;Create the ReplicaSet from the YAML file with the kubectl create command. After&#10;that, you can examine the ReplicaSet with kubectl get and kubectl describe:&#10;$ kubectl get rs&#10;NAME      DESIRED   CURRENT   READY     AGE&#10;kubia     3         3         3         3s&#10;TIP&#10;Use rs shorthand, which stands for replicaset.&#10;$ kubectl describe rs&#10;Name:           kubia&#10;Namespace:      default&#10;Selector:       app=kubia&#10;Labels:         app=kubia&#10;Annotations:    <none>&#10;Replicas:       3 current / 3 desired&#10;Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed&#10;Pod Template:&#10;  Labels:       app=kubia&#10;About the API version attribute&#10;This is your first opportunity to see that the apiVersion property specifies two things:&#10;&#61601;The API group (which is apps in this case)&#10;&#61601;The actual API version (v1beta2)&#10;You&#8217;ll see throughout the book that certain Kubernetes resources are in what&#8217;s called&#10;the core API group, which doesn&#8217;t need to be specified in the apiVersion field (you&#10;just specify the version&#8212;for example, you&#8217;ve been using apiVersion: v1 when&#10;defining Pod resources). Other resources, which were introduced in later Kubernetes&#10;versions, are categorized into several API groups. Look at the inside of the book&#8217;s&#10;covers to see all resources and their respective API groups.&#10; &#10;"
    color "green"
  ]
  node [
    id 102
    label "139"
    title "Page_139"
    color "blue"
  ]
  node [
    id 103
    label "text_50"
    title "107&#10;Using ReplicaSets instead of ReplicationControllers&#10;  Containers:   ...&#10;  Volumes:      <none>&#10;Events:         <none>&#10;As you can see, the ReplicaSet isn&#8217;t any different from a ReplicationController. It&#8217;s&#10;showing it has three replicas matching the selector. If you list all the pods, you&#8217;ll see&#10;they&#8217;re still the same three pods you had before. The ReplicaSet didn&#8217;t create any new&#10;ones. &#10;4.3.4&#10;Using the ReplicaSet&#8217;s more expressive label selectors&#10;The main improvements of ReplicaSets over ReplicationControllers are their more&#10;expressive label selectors. You intentionally used the simpler matchLabels selector in&#10;the first ReplicaSet example to see that ReplicaSets are no different from Replication-&#10;Controllers. Now, you&#8217;ll rewrite the selector to use the more powerful matchExpressions&#10;property, as shown in the following listing.&#10; selector:&#10;   matchExpressions:                 &#10;     - key: app           &#10;       operator: In                  &#10;       values:                       &#10;         - kubia                     &#10;NOTE&#10;Only the selector is shown. You&#8217;ll find the whole ReplicaSet definition&#10;in the book&#8217;s code archive.&#10;You can add additional expressions to the selector. As in the example, each expression&#10;must contain a key, an operator, and possibly (depending on the operator) a list of&#10;values. You&#8217;ll see four valid operators:&#10;&#61601;&#10;In&#8212;Label&#8217;s value must match one of the specified values.&#10;&#61601;&#10;NotIn&#8212;Label&#8217;s value must not match any of the specified values.&#10;&#61601;&#10;Exists&#8212;Pod must include a label with the specified key (the value isn&#8217;t import-&#10;ant). When using this operator, you shouldn&#8217;t specify the values field.&#10;&#61601;&#10;DoesNotExist&#8212;Pod must not include a label with the specified key. The values&#10;property must not be specified.&#10;If you specify multiple expressions, all those expressions must evaluate to true for the&#10;selector to match a pod. If you specify both matchLabels and matchExpressions, all&#10;the labels must match and all the expressions must evaluate to true for the pod to&#10;match the selector.&#10;Listing 4.9&#10;A matchExpressions selector: kubia-replicaset-matchexpressions.yaml&#10;This selector requires the pod to &#10;contain a label with the &#8220;app&#8221; key.&#10;The label&#8217;s value &#10;must be &#8220;kubia&#8221;.&#10; &#10;"
    color "green"
  ]
  node [
    id 104
    label "140"
    title "Page_140"
    color "blue"
  ]
  node [
    id 105
    label "text_51"
    title "108&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;4.3.5&#10;Wrapping up ReplicaSets&#10;This was a quick introduction to ReplicaSets as an alternative to ReplicationControllers.&#10;Remember, always use them instead of ReplicationControllers, but you may still find&#10;ReplicationControllers in other people&#8217;s deployments.&#10; Now, delete the ReplicaSet to clean up your cluster a little. You can delete the&#10;ReplicaSet the same way you&#8217;d delete a ReplicationController:&#10;$ kubectl delete rs kubia&#10;replicaset &#34;kubia&#34; deleted&#10;Deleting the ReplicaSet should delete all the pods. List the pods to confirm that&#8217;s&#10;the case. &#10;4.4&#10;Running exactly one pod on each node with &#10;DaemonSets&#10;Both ReplicationControllers and ReplicaSets are used for running a specific number&#10;of pods deployed anywhere in the Kubernetes cluster. But certain cases exist when you&#10;want a pod to run on each and every node in the cluster (and each node needs to run&#10;exactly one instance of the pod, as shown in figure 4.8).&#10; Those cases include infrastructure-related pods that perform system-level opera-&#10;tions. For example, you&#8217;ll want to run a log collector and a resource monitor on every&#10;node. Another good example is Kubernetes&#8217; own kube-proxy process, which needs to&#10;run on all nodes to make services work.&#10;Node 1&#10;Pod&#10;Pod&#10;Pod&#10;ReplicaSet&#10;Replicas: 5&#10;Node 2&#10;Pod&#10;Pod&#10;Node 3&#10;Pod&#10;DaemonSet&#10;Exactly one replica&#10;on each node&#10;Node 4&#10;Pod&#10;Pod&#10;Pod&#10;Figure 4.8&#10;DaemonSets run only a single pod replica on each node, whereas ReplicaSets &#10;scatter them around the whole cluster randomly. &#10; &#10;"
    color "green"
  ]
  node [
    id 106
    label "141"
    title "Page_141"
    color "blue"
  ]
  node [
    id 107
    label "text_52"
    title "109&#10;Running exactly one pod on each node with DaemonSets&#10;Outside of Kubernetes, such processes would usually be started through system init&#10;scripts or the systemd daemon during node boot up. On Kubernetes nodes, you can&#10;still use systemd to run your system processes, but then you can&#8217;t take advantage of all&#10;the features Kubernetes provides. &#10;4.4.1&#10;Using a DaemonSet to run a pod on every node&#10;To run a pod on all cluster nodes, you create a DaemonSet object, which is much&#10;like a ReplicationController or a ReplicaSet, except that pods created by a Daemon-&#10;Set already have a target node specified and skip the Kubernetes Scheduler. They&#10;aren&#8217;t scattered around the cluster randomly. &#10; A DaemonSet makes sure it creates as many pods as there are nodes and deploys&#10;each one on its own node, as shown in figure 4.8.&#10; Whereas a ReplicaSet (or ReplicationController) makes sure that a desired num-&#10;ber of pod replicas exist in the cluster, a DaemonSet doesn&#8217;t have any notion of a&#10;desired replica count. It doesn&#8217;t need it because its job is to ensure that a pod match-&#10;ing its pod selector is running on each node. &#10; If a node goes down, the DaemonSet doesn&#8217;t cause the pod to be created else-&#10;where. But when a new node is added to the cluster, the DaemonSet immediately&#10;deploys a new pod instance to it. It also does the same if someone inadvertently&#10;deletes one of the pods, leaving the node without the DaemonSet&#8217;s pod. Like a Replica-&#10;Set, a DaemonSet creates the pod from the pod template configured in it.&#10;4.4.2&#10;Using a DaemonSet to run pods only on certain nodes&#10;A DaemonSet deploys pods to all nodes in the cluster, unless you specify that the pods&#10;should only run on a subset of all the nodes. This is done by specifying the node-&#10;Selector property in the pod template, which is part of the DaemonSet definition&#10;(similar to the pod template in a ReplicaSet or ReplicationController). &#10; You&#8217;ve already used node selectors to deploy a pod onto specific nodes in chapter 3.&#10;A node selector in a DaemonSet is similar&#8212;it defines the nodes the DaemonSet must&#10;deploy its pods to. &#10;NOTE&#10;Later in the book, you&#8217;ll learn that nodes can be made unschedulable,&#10;preventing pods from being deployed to them. A DaemonSet will deploy pods&#10;even to such nodes, because the unschedulable attribute is only used by the&#10;Scheduler, whereas pods managed by a DaemonSet bypass the Scheduler&#10;completely. This is usually desirable, because DaemonSets are meant to run&#10;system services, which usually need to run even on unschedulable nodes.&#10;EXPLAINING DAEMONSETS WITH AN EXAMPLE&#10;Let&#8217;s imagine having a daemon called ssd-monitor that needs to run on all nodes&#10;that contain a solid-state drive (SSD). You&#8217;ll create a DaemonSet that runs this dae-&#10;mon on all nodes that are marked as having an SSD. The cluster administrators have&#10;added the disk=ssd label to all such nodes, so you&#8217;ll create the DaemonSet with a&#10;node selector that only selects nodes with that label, as shown in figure 4.9.&#10; &#10;"
    color "green"
  ]
  node [
    id 108
    label "142"
    title "Page_142"
    color "blue"
  ]
  node [
    id 109
    label "text_53"
    title "110&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;CREATING A DAEMONSET YAML DEFINITION&#10;You&#8217;ll create a DaemonSet that runs a mock ssd-monitor process, which prints&#10;&#8220;SSD OK&#8221; to the standard output every five seconds. I&#8217;ve already prepared the mock&#10;container image and pushed it to Docker Hub, so you can use it instead of building&#10;your own. Create the YAML for the DaemonSet, as shown in the following listing.&#10;apiVersion: apps/v1beta2      &#10;kind: DaemonSet                     &#10;metadata:&#10;  name: ssd-monitor&#10;spec:                            &#10;  selector:&#10;    matchLabels:&#10;      app: ssd-monitor&#10;  template:&#10;    metadata:&#10;      labels:&#10;        app: ssd-monitor&#10;    spec:&#10;      nodeSelector:                &#10;        disk: ssd                  &#10;      containers:&#10;      - name: main&#10;        image: luksa/ssd-monitor&#10;You&#8217;re defining a DaemonSet that will run a pod with a single container based on the&#10;luksa/ssd-monitor container image. An instance of this pod will be created for each&#10;node that has the disk=ssd label.&#10;Listing 4.10&#10;A YAML for a DaemonSet: ssd-monitor-daemonset.yaml&#10;Node 1&#10;Pod:&#10;ssd-monitor&#10;Node 2&#10;Node 3&#10;DaemonSet:&#10;sssd-monitor&#10;Node selector:&#10;disk=ssd&#10;Node 4&#10;disk: ssd&#10;disk: ssd&#10;disk: ssd&#10;Unschedulable&#10;Pod:&#10;ssd-monitor&#10;Pod:&#10;ssd-monitor&#10;Figure 4.9&#10;Using a DaemonSet with a node selector to deploy system pods only on certain &#10;nodes&#10;DaemonSets are in the &#10;apps API group, &#10;version v1beta2.&#10;The pod template includes a &#10;node selector, which selects &#10;nodes with the disk=ssd label.&#10; &#10;"
    color "green"
  ]
  node [
    id 110
    label "143"
    title "Page_143"
    color "blue"
  ]
  node [
    id 111
    label "text_54"
    title "111&#10;Running exactly one pod on each node with DaemonSets&#10;CREATING THE DAEMONSET&#10;You&#8217;ll create the DaemonSet like you always create resources from a YAML file:&#10;$ kubectl create -f ssd-monitor-daemonset.yaml&#10;daemonset &#34;ssd-monitor&#34; created&#10;Let&#8217;s see the created DaemonSet:&#10;$ kubectl get ds&#10;NAME          DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE-SELECTOR  &#10;ssd-monitor   0        0        0      0           0          disk=ssd&#10;Those zeroes look strange. Didn&#8217;t the DaemonSet deploy any pods? List the pods:&#10;$ kubectl get po&#10;No resources found.&#10;Where are the pods? Do you know what&#8217;s going on? Yes, you forgot to label your nodes&#10;with the disk=ssd label. No problem&#8212;you can do that now. The DaemonSet should&#10;detect that the nodes&#8217; labels have changed and deploy the pod to all nodes with a&#10;matching label. Let&#8217;s see if that&#8217;s true. &#10;ADDING THE REQUIRED LABEL TO YOUR NODE(S)&#10;Regardless if you&#8217;re using Minikube, GKE, or another multi-node cluster, you&#8217;ll need&#10;to list the nodes first, because you&#8217;ll need to know the node&#8217;s name when labeling it:&#10;$ kubectl get node&#10;NAME       STATUS    AGE       VERSION&#10;minikube   Ready     4d        v1.6.0&#10;Now, add the disk=ssd label to one of your nodes like this:&#10;$ kubectl label node minikube disk=ssd&#10;node &#34;minikube&#34; labeled&#10;NOTE&#10;Replace minikube with the name of one of your nodes if you&#8217;re not&#10;using Minikube.&#10;The DaemonSet should have created one pod now. Let&#8217;s see:&#10;$ kubectl get po&#10;NAME                READY     STATUS    RESTARTS   AGE&#10;ssd-monitor-hgxwq   1/1       Running   0          35s&#10;Okay; so far so good. If you have multiple nodes and you add the same label to further&#10;nodes, you&#8217;ll see the DaemonSet spin up pods for each of them. &#10;REMOVING THE REQUIRED LABEL FROM THE NODE&#10;Now, imagine you&#8217;ve made a mistake and have mislabeled one of the nodes. It has a&#10;spinning disk drive, not an SSD. What happens if you change the node&#8217;s label?&#10;$ kubectl label node minikube disk=hdd --overwrite&#10;node &#34;minikube&#34; labeled&#10; &#10;"
    color "green"
  ]
  node [
    id 112
    label "144"
    title "Page_144"
    color "blue"
  ]
  node [
    id 113
    label "text_55"
    title "112&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;Let&#8217;s see if the change has any effect on the pod that was running on that node:&#10;$ kubectl get po&#10;NAME                READY     STATUS        RESTARTS   AGE&#10;ssd-monitor-hgxwq   1/1       Terminating   0          4m&#10;The pod is being terminated. But you knew that was going to happen, right? This&#10;wraps up your exploration of DaemonSets, so you may want to delete your ssd-monitor&#10;DaemonSet. If you still have any other daemon pods running, you&#8217;ll see that deleting&#10;the DaemonSet deletes those pods as well. &#10;4.5&#10;Running pods that perform a single completable task &#10;Up to now, we&#8217;ve only talked about pods than need to run continuously. You&#8217;ll have&#10;cases where you only want to run a task that terminates after completing its work.&#10;ReplicationControllers, ReplicaSets, and DaemonSets run continuous tasks that are&#10;never considered completed. Processes in such pods are restarted when they exit. But&#10;in a completable task, after its process terminates, it should not be restarted again. &#10;4.5.1&#10;Introducing the Job resource&#10;Kubernetes includes support for this through the Job resource, which is similar to the&#10;other resources we&#8217;ve discussed in this chapter, but it allows you to run a pod whose&#10;container isn&#8217;t restarted when the process running inside finishes successfully. Once it&#10;does, the pod is considered complete. &#10; In the event of a node failure, the pods on that node that are managed by a Job will&#10;be rescheduled to other nodes the way ReplicaSet pods are. In the event of a failure of&#10;the process itself (when the process returns an error exit code), the Job can be config-&#10;ured to either restart the container or not.&#10; Figure 4.10 shows how a pod created by a Job is rescheduled to a new node if the&#10;node it was initially scheduled to fails. The figure also shows both a managed pod,&#10;which isn&#8217;t rescheduled, and a pod backed by a ReplicaSet, which is.&#10; For example, Jobs are useful for ad hoc tasks, where it&#8217;s crucial that the task fin-&#10;ishes properly. You could run the task in an unmanaged pod and wait for it to finish,&#10;but in the event of a node failing or the pod being evicted from the node while it is&#10;performing its task, you&#8217;d need to manually recreate it. Doing this manually doesn&#8217;t&#10;make sense&#8212;especially if the job takes hours to complete. &#10; An example of such a job would be if you had data stored somewhere and you&#10;needed to transform and export it somewhere. You&#8217;re going to emulate this by run-&#10;ning a container image built on top of the busybox image, which invokes the sleep&#10;command for two minutes. I&#8217;ve already built the image and pushed it to Docker Hub,&#10;but you can peek into its Dockerfile in the book&#8217;s code archive.&#10; &#10;"
    color "green"
  ]
  node [
    id 114
    label "145"
    title "Page_145"
    color "blue"
  ]
  node [
    id 115
    label "text_56"
    title "113&#10;Running pods that perform a single completable task&#10;4.5.2&#10;Defining a Job resource&#10;Create the Job manifest as in the following listing.&#10;apiVersion: batch/v1        &#10;kind: Job                   &#10;metadata:&#10;  name: batch-job&#10;spec:                                &#10;  template: &#10;    metadata:&#10;      labels:                        &#10;        app: batch-job               &#10;    spec:&#10;      restartPolicy: OnFailure         &#10;      containers:&#10;      - name: main&#10;        image: luksa/batch-job&#10;Jobs are part of the batch API group and v1 API version. The YAML defines a&#10;resource of type Job that will run the luksa/batch-job image, which invokes a pro-&#10;cess that runs for exactly 120 seconds and then exits. &#10; In a pod&#8217;s specification, you can specify what Kubernetes should do when the&#10;processes running in the container finish. This is done through the restartPolicy&#10;Listing 4.11&#10;A YAML definition of a Job: exporter.yaml&#10;Node 1&#10;Pod A (unmanaged)&#10;Pod B (managed by a ReplicaSet)&#10;Pod C (managed by a Job)&#10;Node 2&#10;Node 1 fails&#10;Job C2 &#64257;nishes&#10;Time&#10;Pod B2 (managed by a ReplicaSet)&#10;Pod C2 (managed by a Job)&#10;Pod A isn&#8217;t rescheduled,&#10;because there is nothing&#10;managing it.&#10;Figure 4.10&#10;Pods managed by Jobs are rescheduled until they finish successfully.&#10;Jobs are in the batch &#10;API group, version v1.&#10;You&#8217;re not specifying a pod &#10;selector (it will be created &#10;based on the labels in the &#10;pod template).&#10;Jobs can&#8217;t use the &#10;default restart policy, &#10;which is Always.&#10; &#10;"
    color "green"
  ]
  node [
    id 116
    label "146"
    title "Page_146"
    color "blue"
  ]
  node [
    id 117
    label "text_57"
    title "114&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;pod spec property, which defaults to Always. Job pods can&#8217;t use the default policy,&#10;because they&#8217;re not meant to run indefinitely. Therefore, you need to explicitly set&#10;the restart policy to either OnFailure or Never. This setting is what prevents the con-&#10;tainer from being restarted when it finishes (not the fact that the pod is being man-&#10;aged by a Job resource).&#10;4.5.3&#10;Seeing a Job run a pod&#10;After you create this Job with the kubectl create command, you should see it start up&#10;a pod immediately:&#10;$ kubectl get jobs&#10;NAME        DESIRED   SUCCESSFUL   AGE&#10;batch-job   1         0            2s&#10;$ kubectl get po&#10;NAME              READY     STATUS    RESTARTS   AGE&#10;batch-job-28qf4   1/1       Running   0          4s&#10;After the two minutes have passed, the pod will no longer show up in the pod list and&#10;the Job will be marked as completed. By default, completed pods aren&#8217;t shown when&#10;you list pods, unless you use the --show-all (or -a) switch:&#10;$ kubectl get po -a&#10;NAME              READY     STATUS      RESTARTS   AGE&#10;batch-job-28qf4   0/1       Completed   0          2m&#10;The reason the pod isn&#8217;t deleted when it completes is to allow you to examine its logs;&#10;for example:&#10;$ kubectl logs batch-job-28qf4&#10;Fri Apr 29 09:58:22 UTC 2016 Batch job starting&#10;Fri Apr 29 10:00:22 UTC 2016 Finished succesfully&#10;The pod will be deleted when you delete it or the Job that created it. Before you do&#10;that, let&#8217;s look at the Job resource again:&#10;$ kubectl get job&#10;NAME        DESIRED   SUCCESSFUL   AGE&#10;batch-job   1         1            9m&#10;The Job is shown as having completed successfully. But why is that piece of informa-&#10;tion shown as a number instead of as yes or true? And what does the DESIRED column&#10;indicate? &#10;4.5.4&#10;Running multiple pod instances in a Job&#10;Jobs may be configured to create more than one pod instance and run them in paral-&#10;lel or sequentially. This is done by setting the completions and the parallelism prop-&#10;erties in the Job spec.&#10; &#10;"
    color "green"
  ]
  node [
    id 118
    label "147"
    title "Page_147"
    color "blue"
  ]
  node [
    id 119
    label "text_58"
    title "115&#10;Running pods that perform a single completable task&#10;RUNNING JOB PODS SEQUENTIALLY&#10;If you need a Job to run more than once, you set completions to how many times you&#10;want the Job&#8217;s pod to run. The following listing shows an example.&#10;apiVersion: batch/v1&#10;kind: Job&#10;metadata:&#10;  name: multi-completion-batch-job&#10;spec:&#10;  completions: 5                  &#10;  template:&#10;    <template is the same as in listing 4.11>&#10;This Job will run five pods one after the other. It initially creates one pod, and when&#10;the pod&#8217;s container finishes, it creates the second pod, and so on, until five pods com-&#10;plete successfully. If one of the pods fails, the Job creates a new pod, so the Job may&#10;create more than five pods overall.&#10;RUNNING JOB PODS IN PARALLEL&#10;Instead of running single Job pods one after the other, you can also make the Job run&#10;multiple pods in parallel. You specify how many pods are allowed to run in parallel&#10;with the parallelism  Job spec property, as shown in the following listing.&#10;apiVersion: batch/v1&#10;kind: Job&#10;metadata:&#10;  name: multi-completion-batch-job&#10;spec:&#10;  completions: 5                    &#10;  parallelism: 2                    &#10;  template:&#10;    <same as in listing 4.11>&#10;By setting parallelism to 2, the Job creates two pods and runs them in parallel:&#10;$ kubectl get po&#10;NAME                               READY   STATUS     RESTARTS   AGE&#10;multi-completion-batch-job-lmmnk   1/1     Running    0          21s&#10;multi-completion-batch-job-qx4nq   1/1     Running    0          21s&#10;As soon as one of them finishes, the Job will run the next pod, until five pods finish&#10;successfully.&#10;Listing 4.12&#10;A Job requiring multiple completions: multi-completion-batch-job.yaml&#10;Listing 4.13&#10;Running Job pods in parallel: multi-completion-parallel-batch-job.yaml&#10;Setting completions to &#10;5 makes this Job run &#10;five pods sequentially.&#10;This job must ensure &#10;five pods complete &#10;successfully.&#10;Up to two pods &#10;can run in parallel.&#10; &#10;"
    color "green"
  ]
  node [
    id 120
    label "148"
    title "Page_148"
    color "blue"
  ]
  node [
    id 121
    label "text_59"
    title "116&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10;SCALING A JOB&#10;You can even change a Job&#8217;s parallelism property while the Job is running. This is&#10;similar to scaling a ReplicaSet or ReplicationController, and can be done with the&#10;kubectl scale command:&#10;$ kubectl scale job multi-completion-batch-job --replicas 3&#10;job &#34;multi-completion-batch-job&#34; scaled&#10;Because you&#8217;ve increased parallelism from 2 to 3, another pod is immediately spun&#10;up, so three pods are now running.&#10;4.5.5&#10;Limiting the time allowed for a Job pod to complete&#10;We need to discuss one final thing about Jobs. How long should the Job wait for a pod&#10;to finish? What if the pod gets stuck and can&#8217;t finish at all (or it can&#8217;t finish fast&#10;enough)?&#10; A pod&#8217;s time can be limited by setting the activeDeadlineSeconds property in the&#10;pod spec. If the pod runs longer than that, the system will try to terminate it and will&#10;mark the Job as failed. &#10;NOTE&#10;You can configure how many times a Job can be retried before it is&#10;marked as failed by specifying the spec.backoffLimit field in the Job mani-&#10;fest. If you don't explicitly specify it, it defaults to 6.&#10;4.6&#10;Scheduling Jobs to run periodically or once &#10;in the future&#10;Job resources run their pods immediately when you create the Job resource. But many&#10;batch jobs need to be run at a specific time in the future or repeatedly in the specified&#10;interval. In Linux- and UNIX-like operating systems, these jobs are better known as&#10;cron jobs. Kubernetes supports them, too.&#10; A cron job in Kubernetes is configured by creating a CronJob resource. The&#10;schedule for running the job is specified in the well-known cron format, so if you&#8217;re&#10;familiar with regular cron jobs, you&#8217;ll understand Kubernetes&#8217; CronJobs in a matter&#10;of seconds.&#10; At the configured time, Kubernetes will create a Job resource according to the Job&#10;template configured in the CronJob object. When the Job resource is created, one or&#10;more pod replicas will be created and started according to the Job&#8217;s pod template, as&#10;you learned in the previous section. There&#8217;s nothing more to it.&#10; Let&#8217;s look at how to create CronJobs. &#10;4.6.1&#10;Creating a CronJob&#10;Imagine you need to run the batch job from your previous example every 15 minutes.&#10;To do that, create a CronJob resource with the following specification.&#10; &#10; &#10;"
    color "green"
  ]
  node [
    id 122
    label "149"
    title "Page_149"
    color "blue"
  ]
  node [
    id 123
    label "text_60"
    title "117&#10;Scheduling Jobs to run periodically or once in the future&#10;apiVersion: batch/v1beta1               &#10;kind: CronJob&#10;metadata:&#10;  name: batch-job-every-fifteen-minutes&#10;spec:&#10;  schedule: &#34;0,15,30,45 * * * *&#34;           &#10;  jobTemplate:&#10;    spec:&#10;      template:                            &#10;        metadata:                          &#10;          labels:                          &#10;            app: periodic-batch-job        &#10;        spec:                              &#10;          restartPolicy: OnFailure         &#10;          containers:                      &#10;          - name: main                     &#10;            image: luksa/batch-job         &#10;As you can see, it&#8217;s not too complicated. You&#8217;ve specified a schedule and a template&#10;from which the Job objects will be created. &#10;CONFIGURING THE SCHEDULE&#10;If you&#8217;re unfamiliar with the cron schedule format, you&#8217;ll find great tutorials and&#10;explanations online, but as a quick introduction, from left to right, the schedule con-&#10;tains the following five entries:&#10;&#61601;Minute&#10;&#61601;Hour&#10;&#61601;Day of month&#10;&#61601;Month&#10;&#61601;Day of week.&#10;In the example, you want to run the job every 15 minutes, so the schedule needs to be&#10;&#34;0,15,30,45 * * * *&#34;, which means at the 0, 15, 30 and 45 minutes mark of every hour&#10;(first asterisk), of every day of the month (second asterisk), of every month (third&#10;asterisk) and on every day of the week (fourth asterisk). &#10; If, instead, you wanted it to run every 30 minutes, but only on the first day of the&#10;month, you&#8217;d set the schedule to &#34;0,30 * 1 * *&#34;, and if you want it to run at 3AM every&#10;Sunday, you&#8217;d set it to &#34;0 3 * * 0&#34; (the last zero stands for Sunday).&#10;CONFIGURING THE JOB TEMPLATE&#10;A CronJob creates Job resources from the jobTemplate property configured in the&#10;CronJob spec, so refer to section 4.5 for more information on how to configure it.&#10;4.6.2&#10;Understanding how scheduled jobs are run&#10;Job resources will be created from the CronJob resource at approximately the sched-&#10;uled time. The Job then creates the pods. &#10;Listing 4.14&#10;YAML for a CronJob resource: cronjob.yaml&#10;API group is batch, &#10;version is v1beta1&#10;This job should run at the &#10;0, 15, 30 and 45 minutes of &#10;every hour, every day.&#10;The template for the &#10;Job resources that &#10;will be created by &#10;this CronJob&#10; &#10;"
    color "green"
  ]
  node [
    id 124
    label "150"
    title "Page_150"
    color "blue"
  ]
  node [
    id 125
    label "text_61"
    title "118&#10;CHAPTER 4&#10;Replication and other controllers: deploying managed pods&#10; It may happen that the Job or pod is created and run relatively late. You may have&#10;a hard requirement for the job to not be started too far over the scheduled time. In&#10;that case, you can specify a deadline by specifying the startingDeadlineSeconds field&#10;in the CronJob specification as shown in the following listing.&#10;apiVersion: batch/v1beta1&#10;kind: CronJob&#10;spec:&#10;  schedule: &#34;0,15,30,45 * * * *&#34;&#10;  startingDeadlineSeconds: 15    &#10;  ...&#10;In the example in listing 4.15, one of the times the job is supposed to run is 10:30:00.&#10;If it doesn&#8217;t start by 10:30:15 for whatever reason, the job will not run and will be&#10;shown as Failed. &#10; In normal circumstances, a CronJob always creates only a single Job for each exe-&#10;cution configured in the schedule, but it may happen that two Jobs are created at the&#10;same time, or none at all. To combat the first problem, your jobs should be idempo-&#10;tent (running them multiple times instead of once shouldn&#8217;t lead to unwanted&#10;results). For the second problem, make sure that the next job run performs any work&#10;that should have been done by the previous (missed) run.&#10;4.7&#10;Summary&#10;You&#8217;ve now learned how to keep pods running and have them rescheduled in the&#10;event of node failures. You should now know that&#10;&#61601;You can specify a liveness probe to have Kubernetes restart your container as&#10;soon as it&#8217;s no longer healthy (where the app defines what&#8217;s considered&#10;healthy).&#10;&#61601;Pods shouldn&#8217;t be created directly, because they will not be re-created if they&#8217;re&#10;deleted by mistake, if the node they&#8217;re running on fails, or if they&#8217;re evicted&#10;from the node.&#10;&#61601;ReplicationControllers always keep the desired number of pod replicas&#10;running.&#10;&#61601;Scaling pods horizontally is as easy as changing the desired replica count on a&#10;ReplicationController.&#10;&#61601;Pods aren&#8217;t owned by the ReplicationControllers and can be moved between&#10;them if necessary.&#10;&#61601;A ReplicationController creates new pods from a pod template. Changing the&#10;template has no effect on existing pods.&#10;Listing 4.15&#10;Specifying a startingDeadlineSeconds for a CronJob&#10;At the latest, the pod must &#10;start running at 15 seconds &#10;past the scheduled time.&#10; &#10;"
    color "green"
  ]
  node [
    id 126
    label "151"
    title "Page_151"
    color "blue"
  ]
  node [
    id 127
    label "text_62"
    title "119&#10;Summary&#10;&#61601;ReplicationControllers should be replaced with ReplicaSets and Deployments,&#10;which provide the same functionality, but with additional powerful features.&#10;&#61601;ReplicationControllers and ReplicaSets schedule pods to random cluster nodes,&#10;whereas DaemonSets make sure every node runs a single instance of a pod&#10;defined in the DaemonSet.&#10;&#61601;Pods that perform a batch task should be created through a Kubernetes Job&#10;resource, not directly or through a ReplicationController or similar object.&#10;&#61601;Jobs that need to run sometime in the future can be created through CronJob&#10;resources. &#10; &#10;"
    color "green"
  ]
  node [
    id 128
    label "152"
    title "Page_152"
    color "blue"
  ]
  node [
    id 129
    label "text_63"
    title "120&#10;Services: enabling&#10;clients to discover&#10;and talk to pods&#10;You&#8217;ve learned about pods and how to deploy them through ReplicaSets and similar&#10;resources to ensure they keep running. Although certain pods can do their work&#10;independently of an external stimulus, many applications these days are meant to&#10;respond to external requests. For example, in the case of microservices, pods will&#10;usually respond to HTTP requests coming either from other pods inside the cluster&#10;or from clients outside the cluster. &#10; Pods need a way of finding other pods if they want to consume the services they&#10;provide. Unlike in the non-Kubernetes world, where a sysadmin would configure&#10;This chapter covers&#10;&#61601;Creating Service resources to expose a group of &#10;pods at a single address&#10;&#61601;Discovering services in the cluster&#10;&#61601;Exposing services to external clients&#10;&#61601;Connecting to external services from inside the &#10;cluster&#10;&#61601;Controlling whether a pod is ready to be part of &#10;the service or not&#10;&#61601;Troubleshooting services&#10; &#10;"
    color "green"
  ]
  node [
    id 130
    label "153"
    title "Page_153"
    color "blue"
  ]
  node [
    id 131
    label "text_64"
    title "121&#10;Introducing services&#10;each client app by specifying the exact IP address or hostname of the server providing&#10;the service in the client&#8217;s configuration files, doing the same in Kubernetes wouldn&#8217;t&#10;work, because&#10;&#61601;Pods are ephemeral&#8212;They may come and go at any time, whether it&#8217;s because a&#10;pod is removed from a node to make room for other pods, because someone&#10;scaled down the number of pods, or because a cluster node has failed.&#10;&#61601;Kubernetes assigns an IP address to a pod after the pod has been scheduled to a node&#10;and before it&#8217;s started&#8212;Clients thus can&#8217;t know the IP address of the server pod&#10;up front.&#10;&#61601;Horizontal scaling means multiple pods may provide the same service&#8212;Each of those&#10;pods has its own IP address. Clients shouldn&#8217;t care how many pods are backing&#10;the service and what their IPs are. They shouldn&#8217;t have to keep a list of all the&#10;individual IPs of pods. Instead, all those pods should be accessible through a&#10;single IP address.&#10;To solve these problems, Kubernetes also provides another resource type&#8212;Services&#8212;&#10;that we&#8217;ll discuss in this chapter.&#10;5.1&#10;Introducing services&#10;A Kubernetes Service is a resource you create to make a single, constant point of&#10;entry to a group of pods providing the same service. Each service has an IP address&#10;and port that never change while the service exists. Clients can open connections to&#10;that IP and port, and those connections are then routed to one of the pods backing&#10;that service. This way, clients of a service don&#8217;t need to know the location of individ-&#10;ual pods providing the service, allowing those pods to be moved around the cluster&#10;at any time. &#10;EXPLAINING SERVICES WITH AN EXAMPLE&#10;Let&#8217;s revisit the example where you have a frontend web server and a backend data-&#10;base server. There may be multiple pods that all act as the frontend, but there may&#10;only be a single backend database pod. You need to solve two problems to make the&#10;system function:&#10;&#61601;External clients need to connect to the frontend pods without caring if there&#8217;s&#10;only a single web server or hundreds.&#10;&#61601;The frontend pods need to connect to the backend database. Because the data-&#10;base runs inside a pod, it may be moved around the cluster over time, causing&#10;its IP address to change. You don&#8217;t want to reconfigure the frontend pods every&#10;time the backend database is moved.&#10;By creating a service for the frontend pods and configuring it to be accessible from&#10;outside the cluster, you expose a single, constant IP address through which external&#10;clients can connect to the pods. Similarly, by also creating a service for the backend&#10;pod, you create a stable address for the backend pod. The service address doesn&#8217;t&#10; &#10;"
    color "green"
  ]
  node [
    id 132
    label "154"
    title "Page_154"
    color "blue"
  ]
  node [
    id 133
    label "text_65"
    title "122&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;change even if the pod&#8217;s IP address changes. Additionally, by creating the service, you&#10;also enable the frontend pods to easily find the backend service by its name through&#10;either environment variables or DNS. All the components of your system (the two ser-&#10;vices, the two sets of pods backing those services, and the interdependencies between&#10;them) are shown in figure 5.1.&#10;You now understand the basic idea behind services. Now, let&#8217;s dig deeper by first see-&#10;ing how they can be created.&#10;5.1.1&#10;Creating services&#10;As you&#8217;ve seen, a service can be backed by more than one pod. Connections to the ser-&#10;vice are load-balanced across all the backing pods. But how exactly do you define&#10;which pods are part of the service and which aren&#8217;t? &#10; You probably remember label selectors and how they&#8217;re used in Replication-&#10;Controllers and other pod controllers to specify which pods belong to the same set.&#10;The same mechanism is used by services in the same way, as you can see in figure 5.2.&#10; In the previous chapter, you created a ReplicationController which then ran three&#10;instances of the pod containing the Node.js app. Create the ReplicationController&#10;again and verify three pod instances are up and running. After that, you&#8217;ll create a&#10;Service for those three pods. &#10;Frontend pod 1&#10;IP: 2.1.1.1&#10;External client&#10;Frontend pod 2&#10;IP: 2.1.1.2&#10;Frontend pod 3&#10;IP: 2.1.1.3&#10;Backend pod&#10;IP: 2.1.1.4&#10;Frontend service&#10;IP: 1.1.1.1&#10;Backend service&#10;IP: 1.1.1.2&#10;Frontend components&#10;Backend components&#10;Figure 5.1&#10;Both internal and external clients usually connect to pods through services.&#10; &#10;"
    color "green"
  ]
  node [
    id 134
    label "155"
    title "Page_155"
    color "blue"
  ]
  node [
    id 135
    label "text_66"
    title "123&#10;Introducing services&#10;CREATING A SERVICE THROUGH KUBECTL EXPOSE&#10;The easiest way to create a service is through kubectl expose, which you&#8217;ve already&#10;used in chapter 2 to expose the ReplicationController you created earlier. The&#10;expose command created a Service resource with the same pod selector as the one&#10;used by the ReplicationController, thereby exposing all its pods through a single IP&#10;address and port. &#10; Now, instead of using the expose command, you&#8217;ll create a service manually by&#10;posting a YAML to the Kubernetes API server. &#10;CREATING A SERVICE THROUGH A YAML DESCRIPTOR&#10;Create a file called kubia-svc.yaml with the following listing&#8217;s contents.&#10;apiVersion: v1&#10;kind: Service             &#10;metadata:&#10;  name: kubia              &#10;spec:&#10;  ports:&#10;  - port: 80              &#10;    targetPort: 8080       &#10;  selector:                 &#10;    app: kubia              &#10;You&#8217;re defining a service called kubia, which will accept connections on port 80 and&#10;route each connection to port 8080 of one of the pods matching the app=kubia&#10;label selector. &#10; Go ahead and create the service by posting the file using kubectl create.&#10;Listing 5.1&#10;A definition of a service: kubia-svc.yaml&#10;app: kubia&#10;Pod: kubia-q3vkg&#10;Pod: kubia-k0xz6&#10;Pod: kubia-53thy&#10;Client&#10;Service: kubia&#10;Selector: app=kubia&#10;app: kubia&#10;app: kubia&#10;Figure 5.2&#10;Label selectors &#10;determine which pods belong &#10;to the Service.&#10;The port this service &#10;will be available on&#10;The container port the &#10;service will forward to&#10;All pods with the app=kubia &#10;label will be part of this service.&#10; &#10;"
    color "green"
  ]
  node [
    id 136
    label "156"
    title "Page_156"
    color "blue"
  ]
  node [
    id 137
    label "text_67"
    title "124&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;EXAMINING YOUR NEW SERVICE&#10;After posting the YAML, you can list all Service resources in your namespace and see&#10;that an internal cluster IP has been assigned to your service:&#10;$ kubectl get svc&#10;NAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE&#10;kubernetes   10.111.240.1     <none>        443/TCP   30d&#10;kubia        10.111.249.153   <none>        80/TCP    6m     &#10;The list shows that the IP address assigned to the service is 10.111.249.153. Because&#10;this is the cluster IP, it&#8217;s only accessible from inside the cluster. The primary purpose&#10;of services is exposing groups of pods to other pods in the cluster, but you&#8217;ll usually&#10;also want to expose services externally. You&#8217;ll see how to do that later. For now, let&#8217;s&#10;use your service from inside the cluster and see what it does.&#10;TESTING YOUR SERVICE FROM WITHIN THE CLUSTER&#10;You can send requests to your service from within the cluster in a few ways:&#10;&#61601;The obvious way is to create a pod that will send the request to the service&#8217;s&#10;cluster IP and log the response. You can then examine the pod&#8217;s log to see&#10;what the service&#8217;s response was.&#10;&#61601;You can ssh into one of the Kubernetes nodes and use the curl command.&#10;&#61601;You can execute the curl command inside one of your existing pods through&#10;the kubectl exec command.&#10;Let&#8217;s go for the last option, so you also learn how to run commands in existing pods. &#10;REMOTELY EXECUTING COMMANDS IN RUNNING CONTAINERS&#10;The kubectl exec command allows you to remotely run arbitrary commands inside&#10;an existing container of a pod. This comes in handy when you want to examine the&#10;contents, state, and/or environment of a container. List the pods with the kubectl&#10;get pods command and choose one as your target for the exec command (in the fol-&#10;lowing example, I&#8217;ve chosen the kubia-7nog1 pod as the target). You&#8217;ll also need to&#10;obtain the cluster IP of your service (using kubectl get svc, for example). When run-&#10;ning the following commands yourself, be sure to replace the pod name and the ser-&#10;vice IP with your own: &#10;$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153&#10;You&#8217;ve hit kubia-gzwli&#10;If you&#8217;ve used ssh to execute commands on a remote system before, you&#8217;ll recognize&#10;that kubectl exec isn&#8217;t much different.&#10; &#10; &#10; &#10; &#10;Here&#8217;s your &#10;service.&#10; &#10;"
    color "green"
  ]
  node [
    id 138
    label "157"
    title "Page_157"
    color "blue"
  ]
  node [
    id 139
    label "text_68"
    title "125&#10;Introducing services&#10;Let&#8217;s go over what transpired when you ran the command. Figure 5.3 shows the&#10;sequence of events. You instructed Kubernetes to execute the curl command inside the&#10;container of one of your pods. Curl sent an HTTP request to the service IP, which is&#10;backed by three pods. The Kubernetes service proxy intercepted the connection,&#10;selected a random pod among the three pods, and forwarded the request to it. Node.js&#10;running inside that pod then handled the request and returned an HTTP response con-&#10;taining the pod&#8217;s name. Curl then printed the response to the standard output, which&#10;was intercepted and printed to its standard output on your local machine by kubectl.&#10;Why the double dash?&#10;The double dash (--) in the command signals the end of command options for&#10;kubectl. Everything after the double dash is the command that should be executed&#10;inside the pod. Using the double dash isn&#8217;t necessary if the command has no&#10;arguments that start with a dash. But in your case, if you don&#8217;t use the double dash&#10;there, the -s option would be interpreted as an option for kubectl exec and would&#10;result in the following strange and highly misleading error:&#10;$ kubectl exec kubia-7nog1 curl -s http://10.111.249.153&#10;The connection to the server 10.111.249.153 was refused &#8211; did you &#10;specify the right host or port?&#10;This has nothing to do with your service refusing the connection. It&#8217;s because&#10;kubectl is not able to connect to an API server at 10.111.249.153 (the -s option&#10;is used to tell kubectl to connect to a different API server than the default).&#10;3. Curl sends HTTP&#10;GET request&#10;4. Service redirects HTTP&#10;connection to a randomly&#10;selected pod&#10;2. Curl is executed&#10;inside the container&#10;running node.js&#10;6. The output of the&#10;command is sent&#10;curl&#10;back to kubectl and&#10;printed by it&#10;5. HTTP response is&#10;sent back to curl&#10;Pod: kubia-7nog1&#10;Container&#10;node.js&#10;curl http://&#10;10.111.249.153&#10;Pod: kubia-gzwli&#10;Container&#10;node.js&#10;Pod: kubia-5fje3&#10;Container&#10;node.js&#10;1. kubectl exec&#10;Service: kubia&#10;10.111.249.153:80&#10;Figure 5.3&#10;Using kubectl exec to test out a connection to the service by running curl in one of the pods&#10; &#10;"
    color "green"
  ]
  node [
    id 140
    label "158"
    title "Page_158"
    color "blue"
  ]
  node [
    id 141
    label "text_69"
    title "126&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;In the previous example, you executed the curl command as a separate process, but&#10;inside the pod&#8217;s main container. This isn&#8217;t much different from the actual main pro-&#10;cess in the container talking to the service.&#10;CONFIGURING SESSION AFFINITY ON THE SERVICE&#10;If you execute the same command a few more times, you should hit a different pod&#10;with every invocation, because the service proxy normally forwards each connection&#10;to a randomly selected backing pod, even if the connections are coming from the&#10;same client. &#10; If, on the other hand, you want all requests made by a certain client to be redi-&#10;rected to the same pod every time, you can set the service&#8217;s sessionAffinity property&#10;to ClientIP (instead of None, which is the default), as shown in the following listing.&#10;apiVersion: v1&#10;kind: Service             &#10;spec:&#10;  sessionAffinity: ClientIP&#10;  ...&#10;This makes the service proxy redirect all requests originating from the same client IP&#10;to the same pod. As an exercise, you can create an additional service with session affin-&#10;ity set to ClientIP and try sending requests to it.&#10; Kubernetes supports only two types of service session affinity: None and ClientIP.&#10;You may be surprised it doesn&#8217;t have a cookie-based session affinity option, but you&#10;need to understand that Kubernetes services don&#8217;t operate at the HTTP level. Services&#10;deal with TCP and UDP packets and don&#8217;t care about the payload they carry. Because&#10;cookies are a construct of the HTTP protocol, services don&#8217;t know about them, which&#10;explains why session affinity cannot be based on cookies. &#10;EXPOSING MULTIPLE PORTS IN THE SAME SERVICE&#10;Your service exposes only a single port, but services can also support multiple ports. For&#10;example, if your pods listened on two ports&#8212;let&#8217;s say 8080 for HTTP and 8443 for&#10;HTTPS&#8212;you could use a single service to forward both port 80 and 443 to the pod&#8217;s&#10;ports 8080 and 8443. You don&#8217;t need to create two different services in such cases. Using&#10;a single, multi-port service exposes all the service&#8217;s ports through a single cluster IP.&#10;NOTE&#10;When creating a service with multiple ports, you must specify a name&#10;for each port.&#10;The spec for a multi-port service is shown in the following listing.&#10;apiVersion: v1&#10;kind: Service             &#10;metadata:&#10;  name: kubia              &#10;Listing 5.2&#10;A example of a service with ClientIP session affinity configured&#10;Listing 5.3&#10;Specifying multiple ports in a service definition&#10; &#10;"
    color "green"
  ]
  node [
    id 142
    label "159"
    title "Page_159"
    color "blue"
  ]
  node [
    id 143
    label "text_70"
    title "127&#10;Introducing services&#10;spec:&#10;  ports:&#10;  - name: http              &#10;    port: 80                &#10;    targetPort: 8080        &#10;  - name: https             &#10;    port: 443               &#10;    targetPort: 8443        &#10;  selector:                 &#10;    app: kubia              &#10;NOTE&#10;The label selector applies to the service as a whole&#8212;it can&#8217;t be config-&#10;ured for each port individually. If you want different ports to map to different&#10;subsets of pods, you need to create two services.&#10;Because your kubia pods don&#8217;t listen on multiple ports, creating a multi-port service&#10;and a multi-port pod is left as an exercise to you.&#10;USING NAMED PORTS&#10;In all these examples, you&#8217;ve referred to the target port by its number, but you can also&#10;give a name to each pod&#8217;s port and refer to it by name in the service spec. This makes&#10;the service spec slightly clearer, especially if the port numbers aren&#8217;t well-known.&#10; For example, suppose your pod defines names for its ports as shown in the follow-&#10;ing listing.&#10;kind: Pod&#10;spec:&#10;  containers:&#10;  - name: kubia&#10;    ports:&#10;    - name: http               &#10;      containerPort: 8080      &#10;    - name: https              &#10;      containerPort: 8443      &#10;You can then refer to those ports by name in the service spec, as shown in the follow-&#10;ing listing.&#10;apiVersion: v1&#10;kind: Service             &#10;spec:&#10;  ports:&#10;  - name: http              &#10;    port: 80                &#10;    targetPort: http        &#10;  - name: https             &#10;    port: 443               &#10;    targetPort: https       &#10;Listing 5.4&#10;Specifying port names in a pod definition&#10;Listing 5.5&#10;Referring to named ports in a service&#10;Port 80 is mapped to &#10;the pods&#8217; port 8080.&#10;Port 443 is mapped to &#10;pods&#8217; port 8443.&#10;The label selector always &#10;applies to the whole service.&#10;Container&#8217;s port &#10;8080 is called http&#10;Port 8443 is called https.&#10;Port 80 is mapped to the &#10;container&#8217;s port called http.&#10;Port 443 is mapped to the container&#8217;s &#10;port, whose name is https.&#10; &#10;"
    color "green"
  ]
  node [
    id 144
    label "160"
    title "Page_160"
    color "blue"
  ]
  node [
    id 145
    label "text_71"
    title "128&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;But why should you even bother with naming ports? The biggest benefit of doing so is&#10;that it enables you to change port numbers later without having to change the service&#10;spec. Your pod currently uses port 8080 for http, but what if you later decide you&#8217;d&#10;like to move that to port 80? &#10; If you&#8217;re using named ports, all you need to do is change the port number in the&#10;pod spec (while keeping the port&#8217;s name unchanged). As you spin up pods with the&#10;new ports, client connections will be forwarded to the appropriate port numbers,&#10;depending on the pod receiving the connection (port 8080 on old pods and port 80&#10;on the new ones).&#10;5.1.2&#10;Discovering services&#10;By creating a service, you now have a single and stable IP address and port that you&#10;can hit to access your pods. This address will remain unchanged throughout the&#10;whole lifetime of the service. Pods behind this service may come and go, their IPs may&#10;change, their number can go up or down, but they&#8217;ll always be accessible through the&#10;service&#8217;s single and constant IP address. &#10; But how do the client pods know the IP and port of a service? Do you need to cre-&#10;ate the service first, then manually look up its IP address and pass the IP to the config-&#10;uration options of the client pod? Not really. Kubernetes also provides ways for client&#10;pods to discover a service&#8217;s IP and port.&#10;DISCOVERING SERVICES THROUGH ENVIRONMENT VARIABLES&#10;When a pod is started, Kubernetes initializes a set of environment variables pointing&#10;to each service that exists at that moment. If you create the service before creating the&#10;client pods, processes in those pods can get the IP address and port of the service by&#10;inspecting their environment variables. &#10; Let&#8217;s see what those environment variables look like by examining the environment&#10;of one of your running pods. You&#8217;ve already learned that you can use the kubectl exec&#10;command to run a command in the pod, but because you created the service only&#10;after your pods had been created, the environment variables for the service couldn&#8217;t&#10;have been set yet. You&#8217;ll need to address that first.&#10; Before you can see environment variables for your service, you first need to delete&#10;all the pods and let the ReplicationController create new ones. You may remember&#10;you can delete all pods without specifying their names like this:&#10;$ kubectl delete po --all&#10;pod &#34;kubia-7nog1&#34; deleted&#10;pod &#34;kubia-bf50t&#34; deleted&#10;pod &#34;kubia-gzwli&#34; deleted&#10;Now you can list the new pods (I&#8217;m sure you know how to do that) and pick one as&#10;your target for the kubectl exec command. Once you&#8217;ve selected your target pod,&#10;you can list environment variables by running the env command inside the container,&#10;as shown in the following listing.&#10; &#10;"
    color "green"
  ]
  node [
    id 146
    label "161"
    title "Page_161"
    color "blue"
  ]
  node [
    id 147
    label "text_72"
    title "129&#10;Introducing services&#10;$ kubectl exec kubia-3inly env&#10;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&#10;HOSTNAME=kubia-3inly&#10;KUBERNETES_SERVICE_HOST=10.111.240.1&#10;KUBERNETES_SERVICE_PORT=443&#10;...&#10;KUBIA_SERVICE_HOST=10.111.249.153             &#10;KUBIA_SERVICE_PORT=80                            &#10;...&#10;Two services are defined in your cluster: the kubernetes and the kubia service (you&#10;saw this earlier with the kubectl get svc command); consequently, two sets of service-&#10;related environment variables are in the list. Among the variables that pertain to the&#10;kubia service you created at the beginning of the chapter, you&#8217;ll see the KUBIA_SERVICE&#10;_HOST and the KUBIA_SERVICE_PORT environment variables, which hold the IP address&#10;and port of the kubia service, respectively. &#10; Turning back to the frontend-backend example we started this chapter with, when&#10;you have a frontend pod that requires the use of a backend database server pod, you&#10;can expose the backend pod through a service called backend-database and then&#10;have the frontend pod look up its IP address and port through the environment vari-&#10;ables BACKEND_DATABASE_SERVICE_HOST and BACKEND_DATABASE_SERVICE_PORT.&#10;NOTE&#10;Dashes in the service name are converted to underscores and all let-&#10;ters are uppercased when the service name is used as the prefix in the envi-&#10;ronment variable&#8217;s name. &#10;Environment variables are one way of looking up the IP and port of a service, but isn&#8217;t&#10;this usually the domain of DNS? Why doesn&#8217;t Kubernetes include a DNS server and&#10;allow you to look up service IPs through DNS instead? As it turns out, it does!&#10;DISCOVERING SERVICES THROUGH DNS&#10;Remember in chapter 3 when you listed pods in the kube-system namespace? One of&#10;the pods was called kube-dns. The kube-system namespace also includes a corre-&#10;sponding service with the same name.&#10; As the name suggests, the pod runs a DNS server, which all other pods running in&#10;the cluster are automatically configured to use (Kubernetes does that by modifying&#10;each container&#8217;s /etc/resolv.conf file). Any DNS query performed by a process run-&#10;ning in a pod will be handled by Kubernetes&#8217; own DNS server, which knows all the ser-&#10;vices running in your system. &#10;NOTE&#10;Whether a pod uses the internal DNS server or not is configurable&#10;through the dnsPolicy property in each pod&#8217;s spec.&#10;Each service gets a DNS entry in the internal DNS server, and client pods that know&#10;the name of the service can access it through its fully qualified domain name (FQDN)&#10;instead of resorting to environment variables. &#10;Listing 5.6&#10;Service-related environment variables in a container&#10;Here&#8217;s the cluster &#10;IP of the service.&#10;And here&#8217;s the port the &#10;service is available on.&#10; &#10;"
    color "green"
  ]
  node [
    id 148
    label "162"
    title "Page_162"
    color "blue"
  ]
  node [
    id 149
    label "text_73"
    title "130&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;CONNECTING TO THE SERVICE THROUGH ITS FQDN&#10;To revisit the frontend-backend example, a frontend pod can connect to the backend-&#10;database service by opening a connection to the following FQDN:&#10;backend-database.default.svc.cluster.local&#10;backend-database corresponds to the service name, default stands for the name-&#10;space the service is defined in, and svc.cluster.local is a configurable cluster&#10;domain suffix used in all cluster local service names. &#10;NOTE&#10;The client must still know the service&#8217;s port number. If the service is&#10;using a standard port (for example, 80 for HTTP or 5432 for Postgres), that&#10;shouldn&#8217;t be a problem. If not, the client can get the port number from the&#10;environment variable.&#10;Connecting to a service can be even simpler than that. You can omit the svc.cluster&#10;.local suffix and even the namespace, when the frontend pod is in the same name-&#10;space as the database pod. You can thus refer to the service simply as backend-&#10;database. That&#8217;s incredibly simple, right?&#10; Let&#8217;s try this. You&#8217;ll try to access the kubia service through its FQDN instead of its&#10;IP. Again, you&#8217;ll need to do that inside an existing pod. You already know how to use&#10;kubectl exec to run a single command in a pod&#8217;s container, but this time, instead of&#10;running the curl command directly, you&#8217;ll run the bash shell instead, so you can then&#10;run multiple commands in the container. This is similar to what you did in chapter 2&#10;when you entered the container you ran with Docker by using the docker exec -it&#10;bash command. &#10;RUNNING A SHELL IN A POD&#8217;S CONTAINER&#10;You can use the kubectl exec command to run bash (or any other shell) inside a&#10;pod&#8217;s container. This way you&#8217;re free to explore the container as long as you want,&#10;without having to perform a kubectl exec for every command you want to run.&#10;NOTE&#10;The shell&#8217;s binary executable must be available in the container image&#10;for this to work.&#10;To use the shell properly, you need to pass the -it option to kubectl exec:&#10;$ kubectl exec -it kubia-3inly bash&#10;root@kubia-3inly:/# &#10;You&#8217;re now inside the container. You can use the curl command to access the kubia&#10;service in any of the following ways:&#10;root@kubia-3inly:/# curl http://kubia.default.svc.cluster.local&#10;You&#8217;ve hit kubia-5asi2&#10;root@kubia-3inly:/# curl http://kubia.default&#10;You&#8217;ve hit kubia-3inly&#10; &#10;"
    color "green"
  ]
  node [
    id 150
    label "163"
    title "Page_163"
    color "blue"
  ]
  node [
    id 151
    label "text_74"
    title "131&#10;Connecting to services living outside the cluster&#10;root@kubia-3inly:/# curl http://kubia&#10;You&#8217;ve hit kubia-8awf3&#10;You can hit your service by using the service&#8217;s name as the hostname in the requested&#10;URL. You can omit the namespace and the svc.cluster.local suffix because of how&#10;the DNS resolver inside each pod&#8217;s container is configured. Look at the /etc/resolv.conf&#10;file in the container and you&#8217;ll understand:&#10;root@kubia-3inly:/# cat /etc/resolv.conf&#10;search default.svc.cluster.local svc.cluster.local cluster.local ...&#10;UNDERSTANDING WHY YOU CAN&#8217;T PING A SERVICE IP&#10;One last thing before we move on. You know how to create services now, so you&#8217;ll soon&#10;create your own. But what if, for whatever reason, you can&#8217;t access your service?&#10; You&#8217;ll probably try to figure out what&#8217;s wrong by entering an existing pod and try-&#10;ing to access the service like you did in the last example. Then, if you still can&#8217;t access&#10;the service with a simple curl command, maybe you&#8217;ll try to ping the service IP to see&#10;if it&#8217;s up. Let&#8217;s try that now:&#10;root@kubia-3inly:/# ping kubia&#10;PING kubia.default.svc.cluster.local (10.111.249.153): 56 data bytes&#10;^C--- kubia.default.svc.cluster.local ping statistics ---&#10;54 packets transmitted, 0 packets received, 100% packet loss&#10;Hmm. curl-ing the service works, but pinging it doesn&#8217;t. That&#8217;s because the service&#8217;s&#10;cluster IP is a virtual IP, and only has meaning when combined with the service port.&#10;We&#8217;ll explain what that means and how services work in chapter 11. I wanted to men-&#10;tion that here because it&#8217;s the first thing users do when they try to debug a broken&#10;service and it catches most of them off guard.&#10;5.2&#10;Connecting to services living outside the cluster&#10;Up to now, we&#8217;ve talked about services backed by one or more pods running inside&#10;the cluster. But cases exist when you&#8217;d like to expose external services through the&#10;Kubernetes services feature. Instead of having the service redirect connections to&#10;pods in the cluster, you want it to redirect to external IP(s) and port(s). &#10; This allows you to take advantage of both service load balancing and service discov-&#10;ery. Client pods running in the cluster can connect to the external service like they&#10;connect to internal services.&#10;5.2.1&#10;Introducing service endpoints&#10;Before going into how to do this, let me first shed more light on services. Services&#10;don&#8217;t link to pods directly. Instead, a resource sits in between&#8212;the Endpoints&#10;resource. You may have already noticed endpoints if you used the kubectl describe&#10;command on your service, as shown in the following listing.&#10; &#10;"
    color "green"
  ]
  node [
    id 152
    label "164"
    title "Page_164"
    color "blue"
  ]
  node [
    id 153
    label "text_75"
    title "132&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;$ kubectl describe svc kubia&#10;Name:                kubia&#10;Namespace:           default&#10;Labels:              <none>&#10;Selector:            app=kubia         &#10;Type:                ClusterIP&#10;IP:                  10.111.249.153&#10;Port:                <unset> 80/TCP&#10;Endpoints:           10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   &#10;Session Affinity:    None&#10;No events.&#10;An Endpoints resource (yes, plural) is a list of IP addresses and ports exposing a ser-&#10;vice. The Endpoints resource is like any other Kubernetes resource, so you can display&#10;its basic info with kubectl get:&#10;$ kubectl get endpoints kubia&#10;NAME    ENDPOINTS                                         AGE&#10;kubia   10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h&#10;Although the pod selector is defined in the service spec, it&#8217;s not used directly when&#10;redirecting incoming connections. Instead, the selector is used to build a list of IPs&#10;and ports, which is then stored in the Endpoints resource. When a client connects to a&#10;service, the service proxy selects one of those IP and port pairs and redirects the&#10;incoming connection to the server listening at that location.&#10;5.2.2&#10;Manually configuring service endpoints&#10;You may have probably realized this already, but having the service&#8217;s endpoints decou-&#10;pled from the service allows them to be configured and updated manually. &#10; If you create a service without a pod selector, Kubernetes won&#8217;t even create the&#10;Endpoints resource (after all, without a selector, it can&#8217;t know which pods to include&#10;in the service). It&#8217;s up to you to create the Endpoints resource to specify the list of&#10;endpoints for the service.&#10; To create a service with manually managed endpoints, you need to create both a&#10;Service and an Endpoints resource. &#10;CREATING A SERVICE WITHOUT A SELECTOR&#10;You&#8217;ll first create the YAML for the service itself, as shown in the following listing.&#10;apiVersion: v1&#10;kind: Service&#10;metadata:&#10;  name: external-service     &#10;spec:                       &#10;  ports:&#10;  - port: 80                  &#10;Listing 5.7&#10;Full details of a service displayed with kubectl describe&#10;Listing 5.8&#10;A service without a pod selector: external-service.yaml&#10;The service&#8217;s pod &#10;selector is used to &#10;create the list of &#10;endpoints.&#10;The list of pod&#10;IPs and ports&#10;that represent&#10;the endpoints of&#10;this service&#10;The name of the service must &#10;match the name of the Endpoints &#10;object (see next listing).&#10;This service has no &#10;selector defined.&#10; &#10;"
    color "green"
  ]
  node [
    id 154
    label "165"
    title "Page_165"
    color "blue"
  ]
  node [
    id 155
    label "text_76"
    title "133&#10;Connecting to services living outside the cluster&#10;You&#8217;re defining a service called external-service that will accept incoming connec-&#10;tions on port 80. You didn&#8217;t define a pod selector for the service.&#10;CREATING AN ENDPOINTS RESOURCE FOR A SERVICE WITHOUT A SELECTOR&#10;Endpoints are a separate resource and not an attribute of a service. Because you cre-&#10;ated the service without a selector, the corresponding Endpoints resource hasn&#8217;t been&#10;created automatically, so it&#8217;s up to you to create it. The following listing shows its&#10;YAML manifest.&#10;apiVersion: v1&#10;kind: Endpoints&#10;metadata:&#10;  name: external-service      &#10;subsets:&#10;  - addresses:&#10;    - ip: 11.11.11.11         &#10;    - ip: 22.22.22.22         &#10;    ports:&#10;    - port: 80      &#10;The Endpoints object needs to have the same name as the service and contain the list&#10;of target IP addresses and ports for the service. After both the Service and the End-&#10;points resource are posted to the server, the service is ready to be used like any regular&#10;service with a pod selector. Containers created after the service is created will include&#10;the environment variables for the service, and all connections to its IP:port pair will be&#10;load balanced between the service&#8217;s endpoints. &#10; Figure 5.4 shows three pods connecting to the service with external endpoints.&#10;If you later decide to migrate the external service to pods running inside Kubernetes,&#10;you can add a selector to the service, thereby making its Endpoints managed automat-&#10;ically. The same is also true in reverse&#8212;by removing the selector from a Service,&#10;Listing 5.9&#10;A manually created Endpoints resource: external-service-endpoints.yaml&#10;The name of the Endpoints object &#10;must match the name of the &#10;service (see previous listing).&#10;The IPs of the endpoints that the &#10;service will forward connections to&#10;The target port of the endpoints&#10;Pod&#10;Pod&#10;Pod&#10;External server 1&#10;IP: 11.11.11.11:80&#10;External server 2&#10;IP: 22.22.22.22:80&#10;Service&#10;10.111.249.214:80&#10;Kubernetes cluster&#10;Internet&#10;Figure 5.4&#10;Pods consuming a service with two external endpoints.&#10; &#10;"
    color "green"
  ]
  node [
    id 156
    label "166"
    title "Page_166"
    color "blue"
  ]
  node [
    id 157
    label "text_77"
    title "134&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;Kubernetes stops updating its Endpoints. This means a service IP address can remain&#10;constant while the actual implementation of the service is changed. &#10;5.2.3&#10;Creating an alias for an external service&#10;Instead of exposing an external service by manually configuring the service&#8217;s End-&#10;points, a simpler method allows you to refer to an external service by its fully qualified&#10;domain name (FQDN).&#10;CREATING AN EXTERNALNAME SERVICE&#10;To create a service that serves as an alias for an external service, you create a Service&#10;resource with the type field set to ExternalName. For example, let&#8217;s imagine there&#8217;s a&#10;public API available at api.somecompany.com. You can define a service that points to&#10;it as shown in the following listing.&#10;apiVersion: v1&#10;kind: Service&#10;metadata:&#10;  name: external-service&#10;spec:&#10;  type: ExternalName                       &#10;  externalName: someapi.somecompany.com     &#10;  ports:&#10;  - port: 80&#10;After the service is created, pods can connect to the external service through the&#10;external-service.default.svc.cluster.local domain name (or even external-&#10;service) instead of using the service&#8217;s actual FQDN. This hides the actual service&#10;name and its location from pods consuming the service, allowing you to modify the&#10;service definition and point it to a different service any time later, by only changing&#10;the externalName attribute or by changing the type back to ClusterIP and creating&#10;an Endpoints object for the service&#8212;either manually or by specifying a label selector&#10;on the service and having it created automatically.&#10; ExternalName services are implemented solely at the DNS level&#8212;a simple CNAME&#10;DNS record is created for the service. Therefore, clients connecting to the service will&#10;connect to the external service directly, bypassing the service proxy completely. For&#10;this reason, these types of services don&#8217;t even get a cluster IP. &#10;NOTE&#10;A CNAME record points to a fully qualified domain name instead of a&#10;numeric IP address.&#10;5.3&#10;Exposing services to external clients&#10;Up to now, we&#8217;ve only talked about how services can be consumed by pods from inside&#10;the cluster. But you&#8217;ll also want to expose certain services, such as frontend webserv-&#10;ers, to the outside, so external clients can access them, as depicted in figure 5.5.&#10;Listing 5.10&#10;An ExternalName-type service: external-service-externalname.yaml&#10;Service type is set &#10;to ExternalName&#10;The fully qualified domain &#10;name of the actual service&#10; &#10;"
    color "green"
  ]
  node [
    id 158
    label "167"
    title "Page_167"
    color "blue"
  ]
  node [
    id 159
    label "text_78"
    title "135&#10;Exposing services to external clients&#10;You have a few ways to make a service accessible externally:&#10;&#61601;Setting the service type to NodePort&#8212;For a NodePort service, each cluster node&#10;opens a port on the node itself (hence the name) and redirects traffic received&#10;on that port to the underlying service. The service isn&#8217;t accessible only at the&#10;internal cluster IP and port, but also through a dedicated port on all nodes. &#10;&#61601;Setting the service type to LoadBalancer, an extension of the NodePort type&#8212;This&#10;makes the service accessible through a dedicated load balancer, provisioned&#10;from the cloud infrastructure Kubernetes is running on. The load balancer redi-&#10;rects traffic to the node port across all the nodes. Clients connect to the service&#10;through the load balancer&#8217;s IP.&#10;&#61601;Creating an Ingress resource, a radically different mechanism for exposing multiple ser-&#10;vices through a single IP address&#8212;It operates at the HTTP level (network layer 7)&#10;and can thus offer more features than layer 4 services can. We&#8217;ll explain Ingress&#10;resources in section 5.4. &#10;5.3.1&#10;Using a NodePort service&#10;The first method of exposing a set of pods to external clients is by creating a service&#10;and setting its type to NodePort. By creating a NodePort service, you make Kubernetes&#10;reserve a port on all its nodes (the same port number is used across all of them) and&#10;forward incoming connections to the pods that are part of the service. &#10; This is similar to a regular service (their actual type is ClusterIP), but a NodePort&#10;service can be accessed not only through the service&#8217;s internal cluster IP, but also&#10;through any node&#8217;s IP and the reserved node port. &#10; This will make more sense when you try interacting with a NodePort service.&#10;CREATING A NODEPORT SERVICE&#10;You&#8217;ll now create a NodePort service to see how you can use it. The following listing&#10;shows the YAML for the service.&#10; &#10;Kubernetes cluster&#10;External client&#10;Service&#10;Pod&#10;Pod&#10;Pod&#10;Figure 5.5&#10;Exposing a service to external clients&#10; &#10;"
    color "green"
  ]
  node [
    id 160
    label "168"
    title "Page_168"
    color "blue"
  ]
  node [
    id 161
    label "text_79"
    title "136&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;apiVersion: v1&#10;kind: Service&#10;metadata:&#10;  name: kubia-nodeport&#10;spec:&#10;  type: NodePort            &#10;  ports:&#10;  - port: 80                 &#10;    targetPort: 8080        &#10;    nodePort: 30123        &#10;  selector:&#10;    app: kubia&#10;You set the type to NodePort and specify the node port this service should be bound to&#10;across all cluster nodes. Specifying the port isn&#8217;t mandatory; Kubernetes will choose a&#10;random port if you omit it. &#10;NOTE&#10;When you create the service in GKE, kubectl prints out a warning&#10;about having to configure firewall rules. We&#8217;ll see how to do that soon. &#10;EXAMINING YOUR NODEPORT SERVICE&#10;Let&#8217;s see the basic information of your service to learn more about it:&#10;$ kubectl get svc kubia-nodeport&#10;NAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE&#10;kubia-nodeport   10.111.254.223   <nodes>       80:30123/TCP   2m&#10;Look at the EXTERNAL-IP column. It shows <nodes>, indicating the service is accessible&#10;through the IP address of any cluster node. The PORT(S) column shows both the&#10;internal port of the cluster IP (80) and the node port (30123). The service is accessi-&#10;ble at the following addresses:&#10;&#61601;&#10;10.11.254.223:80&#10;&#61601;&#10;<1st node&#8217;s IP>:30123&#10;&#61601;&#10;<2nd node&#8217;s IP>:30123, and so on.&#10;Figure 5.6 shows your service exposed on port 30123 of both of your cluster nodes&#10;(this applies if you&#8217;re running this on GKE; Minikube only has a single node, but the&#10;principle is the same). An incoming connection to one of those ports will be redi-&#10;rected to a randomly selected pod, which may or may not be the one running on the&#10;node the connection is being made to. &#10; &#10; &#10; &#10;Listing 5.11&#10;A NodePort service definition: kubia-svc-nodeport.yaml&#10;Set the service &#10;type to NodePort.&#10;This is the port of the &#10;service&#8217;s internal cluster IP.&#10;This is the target port &#10;of the backing pods.&#10;The service will be accessible &#10;through port 30123 of each of &#10;your cluster nodes.&#10; &#10;"
    color "green"
  ]
  node [
    id 162
    label "169"
    title "Page_169"
    color "blue"
  ]
  node [
    id 163
    label "text_80"
    title "137&#10;Exposing services to external clients&#10;A connection received on port 30123 of the first node might be forwarded either to&#10;the pod running on the first node or to one of the pods running on the second node.&#10;CHANGING FIREWALL RULES TO LET EXTERNAL CLIENTS ACCESS OUR NODEPORT SERVICE&#10;As I&#8217;ve mentioned previously, before you can access your service through the node&#10;port, you need to configure the Google Cloud Platform&#8217;s firewalls to allow external&#10;connections to your nodes on that port. You&#8217;ll do this now:&#10;$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123&#10;Created [https://www.googleapis.com/compute/v1/projects/kubia-&#10;1295/global/firewalls/kubia-svc-rule].&#10;NAME            NETWORK  SRC_RANGES  RULES      SRC_TAGS  TARGET_TAGS&#10;kubia-svc-rule  default  0.0.0.0/0   tcp:30123&#10;You can access your service through port 30123 of one of the node&#8217;s IPs. But you need&#10;to figure out the IP of a node first. Refer to the sidebar on how to do that.&#10; &#10; &#10; &#10;Kubernetes cluster&#10;External client&#10;Pod&#10;Node 2&#10;IP: 130.211.99.206&#10;Node 1&#10;IP: 130.211.97.55&#10;Port 30123&#10;Port 8080&#10;Pod&#10;Port 8080&#10;Pod&#10;Port 30123&#10;Port 8080&#10;Service&#10;Figure 5.6&#10;An external client connecting to a NodePort service either through Node 1 or 2&#10; &#10;"
    color "green"
  ]
  node [
    id 164
    label "170"
    title "Page_170"
    color "blue"
  ]
  node [
    id 165
    label "text_81"
    title "138&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;Once you know the IPs of your nodes, you can try accessing your service through them:&#10;$ curl http://130.211.97.55:30123&#10;You've hit kubia-ym8or&#10;$ curl http://130.211.99.206:30123&#10;You've hit kubia-xueq1&#10;TIP&#10;When using Minikube, you can easily access your NodePort services&#10;through your browser by running minikube service <service-name> [-n&#10;<namespace>].&#10;As you can see, your pods are now accessible to the whole internet through port 30123&#10;on any of your nodes. It doesn&#8217;t matter what node a client sends the request to. But if&#10;you only point your clients to the first node, when that node fails, your clients can&#8217;t&#10;access the service anymore. That&#8217;s why it makes sense to put a load balancer in front&#10;of the nodes to make sure you&#8217;re spreading requests across all healthy nodes and&#10;never sending them to a node that&#8217;s offline at that moment. &#10; If your Kubernetes cluster supports it (which is mostly true when Kubernetes is&#10;deployed on cloud infrastructure), the load balancer can be provisioned automati-&#10;cally by creating a LoadBalancer instead of a NodePort service. We&#8217;ll look at this next.&#10;5.3.2&#10;Exposing a service through an external load balancer&#10;Kubernetes clusters running on cloud providers usually support the automatic provi-&#10;sion of a load balancer from the cloud infrastructure. All you need to do is set the&#10;Using JSONPath to get the IPs of all your nodes &#10;You can find the IP in the JSON or YAML descriptors of the nodes. But instead of&#10;sifting through the relatively large JSON, you can tell kubectl to print out only the&#10;node IP instead of the whole service definition: &#10;$ kubectl get nodes -o jsonpath='{.items[*].status.&#10;&#10149; addresses[?(@.type==&#34;ExternalIP&#34;)].address}'&#10;130.211.97.55 130.211.99.206&#10;You&#8217;re telling kubectl to only output the information you want by specifying a&#10;JSONPath. You&#8217;re probably familiar with XPath and how it&#8217;s used with XML. JSONPath&#10;is basically XPath for JSON. The JSONPath in the previous example instructs kubectl&#10;to do the following:&#10;&#61601;Go through all the elements in the items attribute.&#10;&#61601;For each element, enter the status attribute.&#10;&#61601;Filter elements of the addresses attribute, taking only those that have the&#10;type attribute set to ExternalIP.&#10;&#61601;Finally, print the address attribute of the filtered elements.&#10;To learn more about how to use JSONPath with kubectl, refer to the documentation&#10;at http:/&#10;/kubernetes.io/docs/user-guide/jsonpath. &#10; &#10;"
    color "green"
  ]
  node [
    id 166
    label "171"
    title "Page_171"
    color "blue"
  ]
  node [
    id 167
    label "text_82"
    title "139&#10;Exposing services to external clients&#10;service&#8217;s type to LoadBalancer instead of NodePort. The load balancer will have its&#10;own unique, publicly accessible IP address and will redirect all connections to your&#10;service. You can thus access your service through the load balancer&#8217;s IP address. &#10; If Kubernetes is running in an environment that doesn&#8217;t support LoadBalancer&#10;services, the load balancer will not be provisioned, but the service will still behave like&#10;a NodePort service. That&#8217;s because a LoadBalancer service is an extension of a Node-&#10;Port service. You&#8217;ll run this example on Google Kubernetes Engine, which supports&#10;LoadBalancer services. Minikube doesn&#8217;t, at least not as of this writing. &#10;CREATING A LOADBALANCER SERVICE&#10;To create a service with a load balancer in front, create the service from the following&#10;YAML manifest, as shown in the following listing.&#10;apiVersion: v1&#10;kind: Service&#10;metadata:&#10;  name: kubia-loadbalancer&#10;spec:&#10;  type: LoadBalancer          &#10;  ports:&#10;  - port: 80&#10;    targetPort: 8080&#10;  selector:&#10;    app: kubia&#10;The service type is set to LoadBalancer instead of NodePort. You&#8217;re not specifying a spe-&#10;cific node port, although you could (you&#8217;re letting Kubernetes choose one instead). &#10;CONNECTING TO THE SERVICE THROUGH THE LOAD BALANCER&#10;After you create the service, it takes time for the cloud infrastructure to create the&#10;load balancer and write its IP address into the Service object. Once it does that, the IP&#10;address will be listed as the external IP address of your service:&#10;$ kubectl get svc kubia-loadbalancer&#10;NAME                 CLUSTER-IP       EXTERNAL-IP      PORT(S)         AGE&#10;kubia-loadbalancer   10.111.241.153   130.211.53.173   80:32143/TCP    1m&#10;In this case, the load balancer is available at IP 130.211.53.173, so you can now access&#10;the service at that IP address:&#10;$ curl http://130.211.53.173&#10;You've hit kubia-xueq1&#10;Success! As you may have noticed, this time you didn&#8217;t need to mess with firewalls the&#10;way you had to before with the NodePort service.&#10;Listing 5.12&#10;A LoadBalancer-type service: kubia-svc-loadbalancer.yaml&#10;This type of service obtains &#10;a load balancer from the &#10;infrastructure hosting the &#10;Kubernetes cluster.&#10; &#10;"
    color "green"
  ]
  node [
    id 168
    label "172"
    title "Page_172"
    color "blue"
  ]
  node [
    id 169
    label "text_83"
    title "140&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;See figure 5.7 to see how HTTP requests are delivered to the pod. External clients&#10;(curl in your case) connect to port 80 of the load balancer and get routed to the&#10;Session affinity and web browsers&#10;Because your service is now exposed externally, you may try accessing it with your&#10;web browser. You&#8217;ll see something that may strike you as odd&#8212;the browser will hit&#10;the exact same pod every time. Did the service&#8217;s session affinity change in the&#10;meantime? With kubectl explain, you can double-check that the service&#8217;s session&#10;affinity is still set to None, so why don&#8217;t different browser requests hit different&#10;pods, as is the case when using curl?&#10;Let me explain what&#8217;s happening. The browser is using keep-alive connections and&#10;sends all its requests through a single connection, whereas curl opens a new&#10;connection every time. Services work at the connection level, so when a connection to a&#10;service is first opened, a random pod is selected and then all network packets belonging&#10;to that connection are all sent to that single pod. Even if session affinity is set to None,&#10;users will always hit the same pod (until the connection is closed).&#10;Kubernetes cluster&#10;External client&#10;Load balancer&#10;IP: 130.211.53.173:80&#10;Pod&#10;Node 2&#10;IP: 130.211.99.206&#10;Node 1&#10;IP: 130.211.97.55&#10;Port 32143&#10;Port 8080&#10;Pod&#10;Port 8080&#10;Pod&#10;Port 32143&#10;Port 8080&#10;Service&#10;Figure 5.7&#10;An external client connecting to a LoadBalancer service&#10; &#10;"
    color "green"
  ]
  node [
    id 170
    label "173"
    title "Page_173"
    color "blue"
  ]
  node [
    id 171
    label "text_84"
    title "141&#10;Exposing services to external clients&#10;implicitly assigned node port on one of the nodes. From there, the connection is for-&#10;warded to one of the pod instances.&#10; As already mentioned, a LoadBalancer-type service is a NodePort service with an&#10;additional infrastructure-provided load balancer. If you use kubectl describe to dis-&#10;play additional info about the service, you&#8217;ll see that a node port has been selected for&#10;the service. If you were to open the firewall for this port, the way you did in the previ-&#10;ous section about NodePort services, you could access the service through the node&#10;IPs as well.&#10;TIP&#10;If you&#8217;re using Minikube, even though the load balancer will never be&#10;provisioned, you can still access the service through the node port (at the&#10;Minikube VM&#8217;s IP address).&#10;5.3.3&#10;Understanding the peculiarities of external connections&#10;You must be aware of several things related to externally originating connections to&#10;services. &#10;UNDERSTANDING AND PREVENTING UNNECESSARY NETWORK HOPS&#10;When an external client connects to a service through the node port (this also&#10;includes cases when it goes through the load balancer first), the randomly chosen&#10;pod may or may not be running on the same node that received the connection. An&#10;additional network hop is required to reach the pod, but this may not always be&#10;desirable. &#10; You can prevent this additional hop by configuring the service to redirect external&#10;traffic only to pods running on the node that received the connection. This is done by&#10;setting the externalTrafficPolicy field in the service&#8217;s spec section:&#10;spec:&#10;  externalTrafficPolicy: Local&#10;  ...&#10;If a service definition includes this setting and an external connection is opened&#10;through the service&#8217;s node port, the service proxy will choose a locally running pod. If&#10;no local pods exist, the connection will hang (it won&#8217;t be forwarded to a random&#10;global pod, the way connections are when not using the annotation). You therefore&#10;need to ensure the load balancer forwards connections only to nodes that have at&#10;least one such pod.&#10; Using this annotation also has other drawbacks. Normally, connections are spread&#10;evenly across all the pods, but when using this annotation, that&#8217;s no longer the case.&#10; Imagine having two nodes and three pods. Let&#8217;s say node A runs one pod and&#10;node B runs the other two. If the load balancer spreads connections evenly across the&#10;two nodes, the pod on node A will receive 50% of all connections, but the two pods on&#10;node B will only receive 25% each, as shown in figure 5.8.&#10; &#10;"
    color "green"
  ]
  node [
    id 172
    label "174"
    title "Page_174"
    color "blue"
  ]
  node [
    id 173
    label "text_85"
    title "142&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;BEING AWARE OF THE NON-PRESERVATION OF THE CLIENT&#8217;S IP&#10;Usually, when clients inside the cluster connect to a service, the pods backing the ser-&#10;vice can obtain the client&#8217;s IP address. But when the connection is received through a&#10;node port, the packets&#8217; source IP is changed, because Source Network Address Trans-&#10;lation (SNAT) is performed on the packets. &#10; The backing pod can&#8217;t see the actual client&#8217;s IP, which may be a problem for some&#10;applications that need to know the client&#8217;s IP. In the case of a web server, for example,&#10;this means the access log won&#8217;t show the browser&#8217;s IP.&#10; The Local external traffic policy described in the previous section affects the pres-&#10;ervation of the client&#8217;s IP, because there&#8217;s no additional hop between the node receiv-&#10;ing the connection and the node hosting the target pod (SNAT isn&#8217;t performed).&#10;5.4&#10;Exposing services externally through an Ingress &#10;resource&#10;You&#8217;ve now seen two ways of exposing a service to clients outside the cluster, but&#10;another method exists&#8212;creating an Ingress resource.&#10;DEFINITION&#10;Ingress (noun)&#8212;The act of going in or entering; the right to&#10;enter; a means or place of entering; entryway. &#10;Let me first explain why you need another way to access Kubernetes services from the&#10;outside. &#10;UNDERSTANDING WHY INGRESSES ARE NEEDED&#10;One important reason is that each LoadBalancer service requires its own load bal-&#10;ancer with its own public IP address, whereas an Ingress only requires one, even when&#10;providing access to dozens of services. When a client sends an HTTP request to the&#10;Ingress, the host and path in the request determine which service the request is for-&#10;warded to, as shown in figure 5.9.&#10; &#10;50%&#10;50%&#10;50%&#10;25%&#10;25%&#10;Node A&#10;Pod&#10;Node B&#10;Pod&#10;Pod&#10;Load balancer&#10;Figure 5.8&#10;A Service using &#10;the Local external traffic &#10;policy may lead to uneven &#10;load distribution across pods.&#10; &#10;"
    color "green"
  ]
  node [
    id 174
    label "175"
    title "Page_175"
    color "blue"
  ]
  node [
    id 175
    label "text_86"
    title "143&#10;Exposing services externally through an Ingress resource&#10;Ingresses operate at the application layer of the network stack (HTTP) and can pro-&#10;vide features such as cookie-based session affinity and the like, which services can&#8217;t.&#10;UNDERSTANDING THAT AN INGRESS CONTROLLER IS REQUIRED&#10;Before we go into the features an Ingress object provides, let me emphasize that to&#10;make Ingress resources work, an Ingress controller needs to be running in the cluster.&#10;Different Kubernetes environments use different implementations of the controller,&#10;but several don&#8217;t provide a default controller at all. &#10; For example, Google Kubernetes Engine uses Google Cloud Platform&#8217;s own HTTP&#10;load-balancing features to provide the Ingress functionality. Initially, Minikube didn&#8217;t&#10;provide a controller out of the box, but it now includes an add-on that can be enabled&#10;to let you try out the Ingress functionality. Follow the instructions in the following&#10;sidebar to ensure it&#8217;s enabled.&#10;Enabling the Ingress add-on in Minikube&#10;If you&#8217;re using Minikube to run the examples in this book, you&#8217;ll need to ensure the&#10;Ingress add-on is enabled. You can check whether it is by listing all the add-ons:&#10;$ minikube addons list&#10;- default-storageclass: enabled&#10;- kube-dns: enabled&#10;- heapster: disabled&#10;- ingress: disabled               &#10;- registry-creds: disabled&#10;- addon-manager: enabled&#10;- dashboard: enabled&#10;You&#8217;ll learn about what these add-ons are throughout the book, but it should be&#10;pretty clear what the dashboard and the kube-dns add-ons do. Enable the Ingress&#10;add-on so you can see Ingresses in action:&#10;$ minikube addons enable ingress&#10;ingress was successfully enabled&#10;Pod&#10;Pod&#10;Pod&#10;Pod&#10;Pod&#10;Pod&#10;Pod&#10;Pod&#10;Pod&#10;Pod&#10;Pod&#10;Pod&#10;Ingress&#10;Client&#10;Service&#10;kubia.example.com/kubia&#10;foo.example.com&#10;kubia.example.com/foo&#10;Service&#10;bar.example.com&#10;Service&#10;Service&#10;Figure 5.9&#10;Multiple services can be exposed through a single Ingress.&#10;The Ingress add-on &#10;isn&#8217;t enabled.&#10; &#10;"
    color "green"
  ]
  node [
    id 176
    label "176"
    title "Page_176"
    color "blue"
  ]
  node [
    id 177
    label "text_87"
    title "144&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;TIP&#10;The --all-namespaces option mentioned in the sidebar is handy when&#10;you don&#8217;t know what namespace your pod (or other type of resource) is in, or&#10;if you want to list resources across all namespaces.&#10;5.4.1&#10;Creating an Ingress resource&#10;You&#8217;ve confirmed there&#8217;s an Ingress controller running in your cluster, so you can&#10;now create an Ingress resource. The following listing shows what the YAML manifest&#10;for the Ingress looks like.&#10;apiVersion: extensions/v1beta1&#10;kind: Ingress&#10;metadata:&#10;  name: kubia&#10;spec:&#10;  rules:&#10;  - host: kubia.example.com             &#10;    http:&#10;      paths:&#10;      - path: /                           &#10;        backend:&#10;          serviceName: kubia-nodeport     &#10;          servicePort: 80                 &#10;This defines an Ingress with a single rule, which makes sure all HTTP requests received&#10;by the Ingress controller, in which the host kubia.example.com is requested, will be&#10;sent to the kubia-nodeport service on port 80. &#10;(continued)&#10;This should have spun up an Ingress controller as another pod. Most likely, the&#10;controller pod will be in the kube-system namespace, but not necessarily, so list all&#10;the running pods across all namespaces by using the --all-namespaces option:&#10;$ kubectl get po --all-namespaces&#10;NAMESPACE    NAME                            READY  STATUS    RESTARTS AGE&#10;default      kubia-rsv5m                     1/1    Running   0        13h&#10;default      kubia-fe4ad                     1/1    Running   0        13h&#10;default      kubia-ke823                     1/1    Running   0        13h&#10;kube-system  default-http-backend-5wb0h      1/1    Running   0        18m&#10;kube-system  kube-addon-manager-minikube     1/1    Running   3        6d&#10;kube-system  kube-dns-v20-101vq              3/3    Running   9        6d&#10;kube-system  kubernetes-dashboard-jxd9l      1/1    Running   3        6d&#10;kube-system  nginx-ingress-controller-gdts0  1/1    Running   0        18m&#10;At the bottom of the output, you see the Ingress controller pod. The name suggests&#10;that Nginx (an open-source HTTP server and reverse proxy) is used to provide the&#10;Ingress functionality.&#10;Listing 5.13&#10;An Ingress resource definition: kubia-ingress.yaml&#10;This Ingress maps the &#10;kubia.example.com domain &#10;name to your service.&#10;All requests will be sent to &#10;port 80 of the kubia-&#10;nodeport service.&#10; &#10;"
    color "green"
  ]
  node [
    id 178
    label "177"
    title "Page_177"
    color "blue"
  ]
  node [
    id 179
    label "text_88"
    title "145&#10;Exposing services externally through an Ingress resource&#10;NOTE&#10;Ingress controllers on cloud providers (in GKE, for example) require&#10;the Ingress to point to a NodePort service. But that&#8217;s not a requirement of&#10;Kubernetes itself.&#10;5.4.2&#10;Accessing the service through the Ingress&#10;To access your service through http:/&#10;/kubia.example.com, you&#8217;ll need to make sure&#10;the domain name resolves to the IP of the Ingress controller. &#10;OBTAINING THE IP ADDRESS OF THE INGRESS&#10;To look up the IP, you need to list Ingresses:&#10;$ kubectl get ingresses&#10;NAME      HOSTS               ADDRESS          PORTS     AGE&#10;kubia     kubia.example.com   192.168.99.100   80        29m&#10;NOTE&#10;When running on cloud providers, the address may take time to appear,&#10;because the Ingress controller provisions a load balancer behind the scenes.&#10;The IP is shown in the ADDRESS column. &#10;ENSURING THE HOST CONFIGURED IN THE INGRESS POINTS TO THE INGRESS&#8217; IP ADDRESS&#10;Once you know the IP, you can then either configure your DNS servers to resolve&#10;kubia.example.com to that IP or you can add the following line to /etc/hosts (or&#10;C:\windows\system32\drivers\etc\hosts on Windows):&#10;192.168.99.100    kubia.example.com&#10;ACCESSING PODS THROUGH THE INGRESS&#10;Everything is now set up, so you can access the service at http:/&#10;/kubia.example.com&#10;(using a browser or curl):&#10;$ curl http://kubia.example.com&#10;You've hit kubia-ke823&#10;You&#8217;ve successfully accessed the service through an Ingress. Let&#8217;s take a better look at&#10;how that unfolded.&#10;UNDERSTANDING HOW INGRESSES WORK&#10;Figure 5.10 shows how the client connected to one of the pods through the Ingress&#10;controller. The client first performed a DNS lookup of kubia.example.com, and the&#10;DNS server (or the local operating system) returned the IP of the Ingress controller.&#10;The client then sent an HTTP request to the Ingress controller and specified&#10;kubia.example.com in the Host header. From that header, the controller determined&#10;which service the client is trying to access, looked up the pod IPs through the End-&#10;points object associated with the service, and forwarded the client&#8217;s request to one of&#10;the pods.&#10; As you can see, the Ingress controller didn&#8217;t forward the request to the service. It&#10;only used it to select a pod. Most, if not all, controllers work like this. &#10; &#10;"
    color "green"
  ]
  node [
    id 180
    label "178"
    title "Page_178"
    color "blue"
  ]
  node [
    id 181
    label "text_89"
    title "146&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;5.4.3&#10;Exposing multiple services through the same Ingress&#10;If you look at the Ingress spec closely, you&#8217;ll see that both rules and paths are arrays,&#10;so they can contain multiple items. An Ingress can map multiple hosts and paths to&#10;multiple services, as you&#8217;ll see next. Let&#8217;s focus on paths first. &#10;MAPPING DIFFERENT SERVICES TO DIFFERENT PATHS OF THE SAME HOST&#10;You can map multiple paths on the same host to different services, as shown in the&#10;following listing.&#10;...&#10;  - host: kubia.example.com&#10;    http:&#10;      paths:&#10;      - path: /kubia                &#10;        backend:                    &#10;          serviceName: kubia        &#10;          servicePort: 80           &#10;      - path: /foo                &#10;        backend:                  &#10;          serviceName: bar        &#10;          servicePort: 80         &#10;In this case, requests will be sent to two different services, depending on the path in&#10;the requested URL. Clients can therefore reach two different services through a single&#10;IP address (that of the Ingress controller).&#10;Listing 5.14&#10;Ingress exposing multiple services on same host, but different paths&#10;Node A&#10;Pod&#10;Node B&#10;Pod&#10;Pod&#10;Ingress&#10;controller&#10;Endpoints&#10;Service&#10;Ingress&#10;Client&#10;2. Client sends HTTP GET&#10;request with header&#10;Host: kubia.example.com&#10;3. Controller sends&#10;request to one of&#10;the pods.&#10;1. Client looks up&#10;kubia.example.com&#10;DNS&#10;Figure 5.10&#10;Accessing pods through an Ingress&#10;Requests to kubia.example.com/kubia &#10;will be routed to the kubia service.&#10;Requests to kubia.example.com/bar &#10;will be routed to the bar service.&#10; &#10;"
    color "green"
  ]
  node [
    id 182
    label "179"
    title "Page_179"
    color "blue"
  ]
  node [
    id 183
    label "text_90"
    title "147&#10;Exposing services externally through an Ingress resource&#10;MAPPING DIFFERENT SERVICES TO DIFFERENT HOSTS&#10;Similarly, you can use an Ingress to map to different services based on the host in the&#10;HTTP request instead of (only) the path, as shown in the next listing.&#10;spec:&#10;  rules:&#10;  - host: foo.example.com          &#10;    http:&#10;      paths:&#10;      - path: / &#10;        backend:&#10;          serviceName: foo         &#10;          servicePort: 80&#10;  - host: bar.example.com          &#10;    http:&#10;      paths:&#10;      - path: /&#10;        backend:&#10;          serviceName: bar         &#10;          servicePort: 80&#10;Requests received by the controller will be forwarded to either service foo or bar,&#10;depending on the Host header in the request (the way virtual hosts are handled in&#10;web servers). DNS needs to point both the foo.example.com and the bar.exam-&#10;ple.com domain names to the Ingress controller&#8217;s IP address. &#10;5.4.4&#10;Configuring Ingress to handle TLS traffic&#10;You&#8217;ve seen how an Ingress forwards HTTP traffic. But what about HTTPS? Let&#8217;s take&#10;a quick look at how to configure Ingress to support TLS. &#10;CREATING A TLS CERTIFICATE FOR THE INGRESS&#10;When a client opens a TLS connection to an Ingress controller, the controller termi-&#10;nates the TLS connection. The communication between the client and the controller&#10;is encrypted, whereas the communication between the controller and the backend&#10;pod isn&#8217;t. The application running in the pod doesn&#8217;t need to support TLS. For exam-&#10;ple, if the pod runs a web server, it can accept only HTTP traffic and let the Ingress&#10;controller take care of everything related to TLS. To enable the controller to do that,&#10;you need to attach a certificate and a private key to the Ingress. The two need to be&#10;stored in a Kubernetes resource called a Secret, which is then referenced in the&#10;Ingress manifest. We&#8217;ll explain Secrets in detail in chapter 7. For now, you&#8217;ll create the&#10;Secret without paying too much attention to it.&#10; First, you need to create the private key and certificate:&#10;$ openssl genrsa -out tls.key 2048&#10;$ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj &#10;&#10149; /CN=kubia.example.com&#10;Listing 5.15&#10;Ingress exposing multiple services on different hosts&#10;Requests for &#10;foo.example.com will be &#10;routed to service foo.&#10;Requests for &#10;bar.example.com will be &#10;routed to service bar.&#10; &#10;"
    color "green"
  ]
  node [
    id 184
    label "180"
    title "Page_180"
    color "blue"
  ]
  node [
    id 185
    label "text_91"
    title "148&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;Then you create the Secret from the two files like this:&#10;$ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key&#10;secret &#34;tls-secret&#34; created&#10;The private key and the certificate are now stored in the Secret called tls-secret.&#10;Now, you can update your Ingress object so it will also accept HTTPS requests for&#10;kubia.example.com. The Ingress manifest should now look like the following listing.&#10;apiVersion: extensions/v1beta1&#10;kind: Ingress&#10;metadata:&#10;  name: kubia&#10;spec:&#10;  tls:                           &#10;  - hosts:                        &#10;    - kubia.example.com           &#10;    secretName: tls-secret       &#10;  rules:&#10;  - host: kubia.example.com&#10;    http:&#10;      paths:&#10;      - path: /&#10;        backend:&#10;          serviceName: kubia-nodeport&#10;          servicePort: 80&#10;TIP&#10;Instead of deleting the Ingress and re-creating it from the new file, you&#10;can invoke kubectl apply -f kubia-ingress-tls.yaml, which updates the&#10;Ingress resource with what&#8217;s specified in the file.&#10;Signing certificates through the CertificateSigningRequest resource&#10;Instead of signing the certificate ourselves, you can get the certificate signed by&#10;creating a CertificateSigningRequest (CSR) resource. Users or their applications&#10;can create a regular certificate request, put it into a CSR, and then either a human&#10;operator or an automated process can approve the request like this:&#10;$ kubectl certificate approve <name of the CSR> &#10;The signed certificate can then be retrieved from the CSR&#8217;s status.certificate&#10;field. &#10;Note that a certificate signer component must be running in the cluster; otherwise&#10;creating CertificateSigningRequest and approving or denying them won&#8217;t have&#10;any effect.&#10;Listing 5.16&#10;Ingress handling TLS traffic: kubia-ingress-tls.yaml&#10;The whole TLS configuration &#10;is under this attribute.&#10;TLS connections will be accepted for &#10;the kubia.example.com hostname.&#10;The private key and the certificate &#10;should be obtained from the tls-&#10;secret you created previously.&#10; &#10;"
    color "green"
  ]
  node [
    id 186
    label "181"
    title "Page_181"
    color "blue"
  ]
  node [
    id 187
    label "text_92"
    title "149&#10;Signaling when a pod is ready to accept connections&#10;You can now use HTTPS to access your service through the Ingress:&#10;$ curl -k -v https://kubia.example.com/kubia&#10;* About to connect() to kubia.example.com port 443 (#0)&#10;...&#10;* Server certificate:&#10;*   subject: CN=kubia.example.com&#10;...&#10;> GET /kubia HTTP/1.1&#10;> ...&#10;You've hit kubia-xueq1&#10;The command&#8217;s output shows the response from the app, as well as the server certifi-&#10;cate you configured the Ingress with.&#10;NOTE&#10;Support for Ingress features varies between the different Ingress con-&#10;troller implementations, so check the implementation-specific documenta-&#10;tion to see what&#8217;s supported. &#10;Ingresses are a relatively new Kubernetes feature, so you can expect to see many&#10;improvements and new features in the future. Although they currently support only&#10;L7 (HTTP/HTTPS) load balancing, support for L4 load balancing is also planned.&#10;5.5&#10;Signaling when a pod is ready to accept connections&#10;There&#8217;s one more thing we need to cover regarding both Services and Ingresses.&#10;You&#8217;ve already learned that pods are included as endpoints of a service if their labels&#10;match the service&#8217;s pod selector. As soon as a new pod with proper labels is created, it&#10;becomes part of the service and requests start to be redirected to the pod. But what if&#10;the pod isn&#8217;t ready to start serving requests immediately? &#10; The pod may need time to load either configuration or data, or it may need to per-&#10;form a warm-up procedure to prevent the first user request from taking too long and&#10;affecting the user experience. In such cases you don&#8217;t want the pod to start receiving&#10;requests immediately, especially when the already-running instances can process&#10;requests properly and quickly. It makes sense to not forward requests to a pod that&#8217;s in&#10;the process of starting up until it&#8217;s fully ready.&#10;5.5.1&#10;Introducing readiness probes&#10;In the previous chapter you learned about liveness probes and how they help keep&#10;your apps healthy by ensuring unhealthy containers are restarted automatically.&#10;Similar to liveness probes, Kubernetes allows you to also define a readiness probe&#10;for your pod.&#10; The readiness probe is invoked periodically and determines whether the specific&#10;pod should receive client requests or not. When a container&#8217;s readiness probe returns&#10;success, it&#8217;s signaling that the container is ready to accept requests. &#10; This notion of being ready is obviously something that&#8217;s specific to each container.&#10;Kubernetes can merely check if the app running in the container responds to a simple&#10; &#10;"
    color "green"
  ]
  node [
    id 188
    label "182"
    title "Page_182"
    color "blue"
  ]
  node [
    id 189
    label "text_93"
    title "150&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;GET / request or it can hit a specific URL path, which causes the app to perform a&#10;whole list of checks to determine if it&#8217;s ready. Such a detailed readiness probe, which&#10;takes the app&#8217;s specifics into account, is the app developer&#8217;s responsibility. &#10;TYPES OF READINESS PROBES&#10;Like liveness probes, three types of readiness probes exist:&#10;&#61601;An Exec probe, where a process is executed. The container&#8217;s status is deter-&#10;mined by the process&#8217; exit status code.&#10;&#61601;An HTTP GET probe, which sends an HTTP GET request to the container and&#10;the HTTP status code of the response determines whether the container is&#10;ready or not.&#10;&#61601;A TCP Socket probe, which opens a TCP connection to a specified port of the&#10;container. If the connection is established, the container is considered ready.&#10;UNDERSTANDING THE OPERATION OF READINESS PROBES&#10;When a container is started, Kubernetes can be configured to wait for a configurable&#10;amount of time to pass before performing the first readiness check. After that, it&#10;invokes the probe periodically and acts based on the result of the readiness probe. If a&#10;pod reports that it&#8217;s not ready, it&#8217;s removed from the service. If the pod then becomes&#10;ready again, it&#8217;s re-added. &#10; Unlike liveness probes, if a container fails the readiness check, it won&#8217;t be killed or&#10;restarted. This is an important distinction between liveness and readiness probes.&#10;Liveness probes keep pods healthy by killing off unhealthy containers and replacing&#10;them with new, healthy ones, whereas readiness probes make sure that only pods that&#10;are ready to serve requests receive them. This is mostly necessary during container&#10;start up, but it&#8217;s also useful after the container has been running for a while. &#10; As you can see in figure 5.11, if a pod&#8217;s readiness probe fails, the pod is removed&#10;from the Endpoints object. Clients connecting to the service will not be redirected to&#10;the pod. The effect is the same as when the pod doesn&#8217;t match the service&#8217;s label&#10;selector at all.&#10;Endpoints&#10;Service&#10;Selector: app=kubia&#10;app: kubia&#10;Pod: kubia-q3vkg&#10;app: kubia&#10;Pod: kubia-k0xz6&#10;app: kubia&#10;Pod: kubia-53thy&#10;Not ready&#10;This pod is no longer&#10;an endpoint, because its&#10;readiness probe has failed.&#10;Figure 5.11&#10;A pod whose readiness probe fails is removed as an endpoint of a service.&#10; &#10;"
    color "green"
  ]
  node [
    id 190
    label "183"
    title "Page_183"
    color "blue"
  ]
  node [
    id 191
    label "text_94"
    title "151&#10;Signaling when a pod is ready to accept connections&#10;UNDERSTANDING WHY READINESS PROBES ARE IMPORTANT&#10;Imagine that a group of pods (for example, pods running application servers)&#10;depends on a service provided by another pod (a backend database, for example). If&#10;at any point one of the frontend pods experiences connectivity problems and can&#8217;t&#10;reach the database anymore, it may be wise for its readiness probe to signal to Kuber-&#10;netes that the pod isn&#8217;t ready to serve any requests at that time. If other pod instances&#10;aren&#8217;t experiencing the same type of connectivity issues, they can serve requests nor-&#10;mally. A readiness probe makes sure clients only talk to those healthy pods and never&#10;notice there&#8217;s anything wrong with the system.&#10;5.5.2&#10;Adding a readiness probe to a pod&#10;Next you&#8217;ll add a readiness probe to your existing pods by modifying the Replication-&#10;Controller&#8217;s pod template. &#10;ADDING A READINESS PROBE TO THE POD TEMPLATE&#10;You&#8217;ll use the kubectl edit command to add the probe to the pod template in your&#10;existing ReplicationController:&#10;$ kubectl edit rc kubia&#10;When the ReplicationController&#8217;s YAML opens in the text editor, find the container&#10;specification in the pod template and add the following readiness probe definition to&#10;the first container under spec.template.spec.containers. The YAML should look&#10;like the following listing.&#10;apiVersion: v1&#10;kind: ReplicationController&#10;...&#10;spec:&#10;  ...&#10;  template:&#10;    ...&#10;    spec:&#10;      containers:&#10;      - name: kubia&#10;        image: luksa/kubia&#10;        readinessProbe:       &#10;          exec:               &#10;            command:          &#10;            - ls              &#10;            - /var/ready      &#10;        ...&#10;The readiness probe will periodically perform the command ls /var/ready inside the&#10;container. The ls command returns exit code zero if the file exists, or a non-zero exit&#10;code otherwise. If the file exists, the readiness probe will succeed; otherwise, it will fail. &#10;Listing 5.17&#10;RC creating a pod with a readiness probe: kubia-rc-readinessprobe.yaml&#10;A readinessProbe may &#10;be defined for each &#10;container in the pod.&#10; &#10;"
    color "green"
  ]
  node [
    id 192
    label "184"
    title "Page_184"
    color "blue"
  ]
  node [
    id 193
    label "text_95"
    title "152&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10; The reason you&#8217;re defining such a strange readiness probe is so you can toggle its&#10;result by creating or removing the file in question. The file doesn&#8217;t exist yet, so all the&#10;pods should now report not being ready, right? Well, not exactly. As you may remem-&#10;ber from the previous chapter, changing a ReplicationController&#8217;s pod template has&#10;no effect on existing pods. &#10; In other words, all your existing pods still have no readiness probe defined. You&#10;can see this by listing the pods with kubectl get pods and looking at the READY col-&#10;umn. You need to delete the pods and have them re-created by the Replication-&#10;Controller. The new pods will fail the readiness check and won&#8217;t be included as&#10;endpoints of the service until you create the /var/ready file in each of them. &#10;OBSERVING AND MODIFYING THE PODS&#8217; READINESS STATUS&#10;List the pods again and inspect whether they&#8217;re ready or not:&#10;$ kubectl get po&#10;NAME          READY     STATUS    RESTARTS   AGE&#10;kubia-2r1qb   0/1       Running   0          1m&#10;kubia-3rax1   0/1       Running   0          1m&#10;kubia-3yw4s   0/1       Running   0          1m&#10;The READY column shows that none of the containers are ready. Now make the readi-&#10;ness probe of one of them start returning success by creating the /var/ready file,&#10;whose existence makes your mock readiness probe succeed:&#10;$ kubectl exec kubia-2r1qb -- touch /var/ready&#10;You&#8217;ve used the kubectl exec command to execute the touch command inside the&#10;container of the kubia-2r1qb pod. The touch command creates the file if it doesn&#8217;t&#10;yet exist. The pod&#8217;s readiness probe command should now exit with status code 0,&#10;which means the probe is successful, and the pod should now be shown as ready. Let&#8217;s&#10;see if it is:&#10;$ kubectl get po kubia-2r1qb&#10;NAME          READY     STATUS    RESTARTS   AGE&#10;kubia-2r1qb   0/1       Running   0          2m&#10;The pod still isn&#8217;t ready. Is there something wrong or is this the expected result? Take&#10;a more detailed look at the pod with kubectl describe. The output should contain&#10;the following line:&#10;Readiness: exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1&#10;&#10149; #failure=3&#10;The readiness probe is checked periodically&#8212;every 10 seconds by default. The pod&#10;isn&#8217;t ready because the readiness probe hasn&#8217;t been invoked yet. But in 10 seconds at&#10;the latest, the pod should become ready and its IP should be listed as the only end-&#10;point of the service (run kubectl get endpoints kubia-loadbalancer to confirm). &#10; &#10;"
    color "green"
  ]
  node [
    id 194
    label "185"
    title "Page_185"
    color "blue"
  ]
  node [
    id 195
    label "text_96"
    title "153&#10;Signaling when a pod is ready to accept connections&#10;HITTING THE SERVICE WITH THE SINGLE READY POD&#10;You can now hit the service URL a few times to see that each and every request is redi-&#10;rected to this one pod:&#10;$ curl http://130.211.53.173&#10;You&#8217;ve hit kubia-2r1qb&#10;$ curl http://130.211.53.173&#10;You&#8217;ve hit kubia-2r1qb&#10;...&#10;$ curl http://130.211.53.173&#10;You&#8217;ve hit kubia-2r1qb&#10;Even though there are three pods running, only a single pod is reporting as being&#10;ready and is therefore the only pod receiving requests. If you now delete the file, the&#10;pod will be removed from the service again. &#10;5.5.3&#10;Understanding what real-world readiness probes should do&#10;This mock readiness probe is useful only for demonstrating what readiness probes do.&#10;In the real world, the readiness probe should return success or failure depending on&#10;whether the app can (and wants to) receive client requests or not. &#10; Manually removing pods from services should be performed by either deleting the&#10;pod or changing the pod&#8217;s labels instead of manually flipping a switch in the probe. &#10;TIP&#10;If you want to add or remove a pod from a service manually, add&#10;enabled=true as a label to your pod and to the label selector of your service.&#10;Remove the label when you want to remove the pod from the service.&#10;ALWAYS DEFINE A READINESS PROBE&#10;Before we conclude this section, there are two final notes about readiness probes that&#10;I need to emphasize. First, if you don&#8217;t add a readiness probe to your pods, they&#8217;ll&#10;become service endpoints almost immediately. If your application takes too long to&#10;start listening for incoming connections, client requests hitting the service will be for-&#10;warded to the pod while it&#8217;s still starting up and not ready to accept incoming connec-&#10;tions. Clients will therefore see &#8220;Connection refused&#8221; types of errors. &#10;TIP&#10;You should always define a readiness probe, even if it&#8217;s as simple as send-&#10;ing an HTTP request to the base URL. &#10;DON&#8217;T INCLUDE POD SHUTDOWN LOGIC INTO YOUR READINESS PROBES&#10;The other thing I need to mention applies to the other end of the pod&#8217;s life (pod&#10;shutdown) and is also related to clients experiencing connection errors. &#10; When a pod is being shut down, the app running in it usually stops accepting con-&#10;nections as soon as it receives the termination signal. Because of this, you might think&#10;you need to make your readiness probe start failing as soon as the shutdown proce-&#10;dure is initiated, ensuring the pod is removed from all services it&#8217;s part of. But that&#8217;s&#10;not necessary, because Kubernetes removes the pod from all services as soon as you&#10;delete the pod.&#10; &#10;"
    color "green"
  ]
  node [
    id 196
    label "186"
    title "Page_186"
    color "blue"
  ]
  node [
    id 197
    label "text_97"
    title "154&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;5.6&#10;Using a headless service for discovering individual pods&#10;You&#8217;ve seen how services can be used to provide a stable IP address allowing clients to&#10;connect to pods (or other endpoints) backing each service. Each connection to the&#10;service is forwarded to one randomly selected backing pod. But what if the client&#10;needs to connect to all of those pods? What if the backing pods themselves need to&#10;each connect to all the other backing pods? Connecting through the service clearly&#10;isn&#8217;t the way to do this. What is?&#10; For a client to connect to all pods, it needs to figure out the the IP of each individ-&#10;ual pod. One option is to have the client call the Kubernetes API server and get the&#10;list of pods and their IP addresses through an API call, but because you should always&#10;strive to keep your apps Kubernetes-agnostic, using the API server isn&#8217;t ideal. &#10; Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually,&#10;when you perform a DNS lookup for a service, the DNS server returns a single IP&#8212;the&#10;service&#8217;s cluster IP. But if you tell Kubernetes you don&#8217;t need a cluster IP for your service&#10;(you do this by setting the clusterIP field to None in the service specification), the DNS&#10;server will return the pod IPs instead of the single service IP.&#10; Instead of returning a single DNS A record, the DNS server will return multiple A&#10;records for the service, each pointing to the IP of an individual pod backing the ser-&#10;vice at that moment. Clients can therefore do a simple DNS A record lookup and get&#10;the IPs of all the pods that are part of the service. The client can then use that infor-&#10;mation to connect to one, many, or all of them.&#10;5.6.1&#10;Creating a headless service&#10;Setting the clusterIP field in a service spec to None makes the service headless, as&#10;Kubernetes won&#8217;t assign it a cluster IP through which clients could connect to the&#10;pods backing it. &#10; You&#8217;ll create a headless service called kubia-headless now. The following listing&#10;shows its definition.&#10;apiVersion: v1&#10;kind: Service&#10;metadata:&#10;  name: kubia-headless&#10;spec:&#10;  clusterIP: None       &#10;  ports:&#10;  - port: 80&#10;    targetPort: 8080&#10;  selector:&#10;    app: kubia&#10;After you create the service with kubectl create, you can inspect it with kubectl get&#10;and kubectl describe. You&#8217;ll see it has no cluster IP and its endpoints include (part of)&#10;Listing 5.18&#10;A headless service: kubia-svc-headless.yaml&#10;This makes the &#10;service headless.&#10; &#10;"
    color "green"
  ]
  node [
    id 198
    label "187"
    title "Page_187"
    color "blue"
  ]
  node [
    id 199
    label "text_98"
    title "155&#10;Using a headless service for discovering individual pods&#10;the pods matching its pod selector. I say &#8220;part of&#8221; because your pods contain a readi-&#10;ness probe, so only pods that are ready will be listed as endpoints of the service.&#10;Before continuing, please make sure at least two pods report being ready, by creating&#10;the /var/ready file, as in the previous example:&#10;$ kubectl exec <pod name> -- touch /var/ready&#10;5.6.2&#10;Discovering pods through DNS&#10;With your pods ready, you can now try performing a DNS lookup to see if you get the&#10;actual pod IPs or not. You&#8217;ll need to perform the lookup from inside one of the pods.&#10;Unfortunately, your kubia container image doesn&#8217;t include the nslookup (or the dig)&#10;binary, so you can&#8217;t use it to perform the DNS lookup.&#10; All you&#8217;re trying to do is perform a DNS lookup from inside a pod running in the&#10;cluster. Why not run a new pod based on an image that contains the binaries you&#10;need? To perform DNS-related actions, you can use the tutum/dnsutils container&#10;image, which is available on Docker Hub and contains both the nslookup and the dig&#10;binaries. To run the pod, you can go through the whole process of creating a YAML&#10;manifest for it and passing it to kubectl create, but that&#8217;s too much work, right?&#10;Luckily, there&#8217;s a faster way.&#10;RUNNING A POD WITHOUT WRITING A YAML MANIFEST&#10;In chapter 1, you already created pods without writing a YAML manifest by using the&#10;kubectl run command. But this time you want to create only a pod&#8212;you don&#8217;t need&#10;to create a ReplicationController to manage the pod. You can do that like this:&#10;$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1&#10;&#10149; --command -- sleep infinity&#10;pod &#34;dnsutils&#34; created&#10;The trick is in the --generator=run-pod/v1 option, which tells kubectl to create the&#10;pod directly, without any kind of ReplicationController or similar behind it. &#10;UNDERSTANDING DNS A RECORDS RETURNED FOR A HEADLESS SERVICE&#10;Let&#8217;s use the newly created pod to perform a DNS lookup:&#10;$ kubectl exec dnsutils nslookup kubia-headless&#10;...&#10;Name:    kubia-headless.default.svc.cluster.local&#10;Address: 10.108.1.4 &#10;Name:    kubia-headless.default.svc.cluster.local&#10;Address: 10.108.2.5 &#10;The DNS server returns two different IPs for the kubia-headless.default.svc&#10;.cluster.local FQDN. Those are the IPs of the two pods that are reporting being&#10;ready. You can confirm this by listing pods with kubectl get pods -o wide, which&#10;shows the pods&#8217; IPs. &#10; &#10;"
    color "green"
  ]
  node [
    id 200
    label "188"
    title "Page_188"
    color "blue"
  ]
  node [
    id 201
    label "text_99"
    title "156&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10; This is different from what DNS returns for regular (non-headless) services, such&#10;as for your kubia service, where the returned IP is the service&#8217;s cluster IP:&#10;$ kubectl exec dnsutils nslookup kubia&#10;...&#10;Name:    kubia.default.svc.cluster.local&#10;Address: 10.111.249.153&#10;Although headless services may seem different from regular services, they aren&#8217;t that&#10;different from the clients&#8217; perspective. Even with a headless service, clients can con-&#10;nect to its pods by connecting to the service&#8217;s DNS name, as they can with regular ser-&#10;vices. But with headless services, because DNS returns the pods&#8217; IPs, clients connect&#10;directly to the pods, instead of through the service proxy. &#10;NOTE&#10;A headless services still provides load balancing across pods, but through&#10;the DNS round-robin mechanism instead of through the service proxy.&#10;5.6.3&#10;Discovering all pods&#8212;even those that aren&#8217;t ready&#10;You&#8217;ve seen that only pods that are ready become endpoints of services. But some-&#10;times you want to use the service discovery mechanism to find all pods matching the&#10;service&#8217;s label selector, even those that aren&#8217;t ready. &#10; Luckily, you don&#8217;t have to resort to querying the Kubernetes API server. You can&#10;use the DNS lookup mechanism to find even those unready pods. To tell Kubernetes&#10;you want all pods added to a service, regardless of the pod&#8217;s readiness status, you must&#10;add the following annotation to the service:&#10;kind: Service&#10;metadata:&#10;  annotations:&#10;    service.alpha.kubernetes.io/tolerate-unready-endpoints: &#34;true&#34;&#10;WARNING&#10;As the annotation name suggests, as I&#8217;m writing this, this is an alpha&#10;feature. The Kubernetes Service API already supports a new service spec field&#10;called publishNotReadyAddresses, which will replace the tolerate-unready-&#10;endpoints annotation. In Kubernetes version 1.9.0, the field is not honored yet&#10;(the annotation is what determines whether unready endpoints are included in&#10;the DNS or not). Check the documentation to see whether that&#8217;s changed.&#10;5.7&#10;Troubleshooting services&#10;Services are a crucial Kubernetes concept and the source of frustration for many&#10;developers. I&#8217;ve seen many developers lose heaps of time figuring out why they can&#8217;t&#10;connect to their pods through the service IP or FQDN. For this reason, a short look at&#10;how to troubleshoot services is in order.&#10; When you&#8217;re unable to access your pods through the service, you should start by&#10;going through the following list:&#10; &#10;"
    color "green"
  ]
  node [
    id 202
    label "189"
    title "Page_189"
    color "blue"
  ]
  node [
    id 203
    label "text_100"
    title "157&#10;Summary&#10;&#61601;First, make sure you&#8217;re connecting to the service&#8217;s cluster IP from within the&#10;cluster, not from the outside.&#10;&#61601;Don&#8217;t bother pinging the service IP to figure out if the service is accessible&#10;(remember, the service&#8217;s cluster IP is a virtual IP and pinging it will never work).&#10;&#61601;If you&#8217;ve defined a readiness probe, make sure it&#8217;s succeeding; otherwise the&#10;pod won&#8217;t be part of the service.&#10;&#61601;To confirm that a pod is part of the service, examine the corresponding End-&#10;points object with kubectl get endpoints.&#10;&#61601;If you&#8217;re trying to access the service through its FQDN or a part of it (for exam-&#10;ple, myservice.mynamespace.svc.cluster.local or myservice.mynamespace) and&#10;it doesn&#8217;t work, see if you can access it using its cluster IP instead of the FQDN.&#10;&#61601;Check whether you&#8217;re connecting to the port exposed by the service and not&#10;the target port.&#10;&#61601;Try connecting to the pod IP directly to confirm your pod is accepting connec-&#10;tions on the correct port.&#10;&#61601;If you can&#8217;t even access your app through the pod&#8217;s IP, make sure your app isn&#8217;t&#10;only binding to localhost.&#10;This should help you resolve most of your service-related problems. You&#8217;ll learn much&#10;more about how services work in chapter 11. By understanding exactly how they&#8217;re&#10;implemented, it should be much easier for you to troubleshoot them.&#10;5.8&#10;Summary&#10;In this chapter, you&#8217;ve learned how to create Kubernetes Service resources to expose&#10;the services available in your application, regardless of how many pod instances are&#10;providing each service. You&#8217;ve learned how Kubernetes&#10;&#61601;Exposes multiple pods that match a certain label selector under a single, stable&#10;IP address and port&#10;&#61601;Makes services accessible from inside the cluster by default, but allows you to&#10;make the service accessible from outside the cluster by setting its type to either&#10;NodePort or LoadBalancer&#10;&#61601;Enables pods to discover services together with their IP addresses and ports by&#10;looking up environment variables&#10;&#61601;Allows discovery of and communication with services residing outside the&#10;cluster by creating a Service resource without specifying a selector, by creating&#10;an associated Endpoints resource instead&#10;&#61601;Provides a DNS CNAME alias for external services with the ExternalName ser-&#10;vice type&#10;&#61601;Exposes multiple HTTP services through a single Ingress (consuming a sin-&#10;gle IP)&#10; &#10;"
    color "green"
  ]
  node [
    id 204
    label "190"
    title "Page_190"
    color "blue"
  ]
  node [
    id 205
    label "text_101"
    title "158&#10;CHAPTER 5&#10;Services: enabling clients to discover and talk to pods&#10;&#61601;Uses a pod container&#8217;s readiness probe to determine whether a pod should or&#10;shouldn&#8217;t be included as a service endpoint&#10;&#61601;Enables discovery of pod IPs through DNS when you create a headless service&#10;Along with getting a better understanding of services, you&#8217;ve also learned how to&#10;&#61601;Troubleshoot them&#10;&#61601;Modify firewall rules in Google Kubernetes/Compute Engine&#10;&#61601;Execute commands in pod containers through kubectl exec &#10;&#61601;Run a bash shell in an existing pod&#8217;s container&#10;&#61601;Modify Kubernetes resources through the kubectl apply command&#10;&#61601;Run an unmanaged ad hoc pod with kubectl run --generator=run-pod/v1&#10; &#10;"
    color "green"
  ]
  node [
    id 206
    label "191"
    title "Page_191"
    color "blue"
  ]
  node [
    id 207
    label "text_102"
    title "159&#10;Volumes: attaching&#10;disk storage to containers&#10;In the previous three chapters, we introduced pods and other Kubernetes resources&#10;that interact with them, namely ReplicationControllers, ReplicaSets, DaemonSets,&#10;Jobs, and Services. Now, we&#8217;re going back inside the pod to learn how its containers&#10;can access external disk storage and/or share storage between them.&#10; We&#8217;ve said that pods are similar to logical hosts where processes running inside&#10;them share resources such as CPU, RAM, network interfaces, and others. One&#10;would expect the processes to also share disks, but that&#8217;s not the case. You&#8217;ll remem-&#10;ber that each container in a pod has its own isolated filesystem, because the file-&#10;system comes from the container&#8217;s image.&#10;This chapter covers&#10;&#61601;Creating multi-container pods&#10;&#61601;Creating a volume to share disk storage between &#10;containers&#10;&#61601;Using a Git repository inside a pod&#10;&#61601;Attaching persistent storage such as a GCE &#10;Persistent Disk to pods&#10;&#61601;Using pre-provisioned persistent storage&#10;&#61601;Dynamic provisioning of persistent storage&#10; &#10;"
    color "green"
  ]
  node [
    id 208
    label "192"
    title "Page_192"
    color "blue"
  ]
  node [
    id 209
    label "text_103"
    title "160&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10; Every new container starts off with the exact set of files that was added to the image&#10;at build time. Combine this with the fact that containers in a pod get restarted (either&#10;because the process died or because the liveness probe signaled to Kubernetes that&#10;the container wasn&#8217;t healthy anymore) and you&#8217;ll realize that the new container will&#10;not see anything that was written to the filesystem by the previous container, even&#10;though the newly started container runs in the same pod.&#10; In certain scenarios you want the new container to continue where the last one fin-&#10;ished, such as when restarting a process on a physical machine. You may not need (or&#10;want) the whole filesystem to be persisted, but you do want to preserve the directories&#10;that hold actual data.&#10; Kubernetes provides this by defining storage volumes. They aren&#8217;t top-level resources&#10;like pods, but are instead defined as a part of a pod and share the same lifecycle as the&#10;pod. This means a volume is created when the pod is started and is destroyed when&#10;the pod is deleted. Because of this, a volume&#8217;s contents will persist across container&#10;restarts. After a container is restarted, the new container can see all the files that were&#10;written to the volume by the previous container. Also, if a pod contains multiple con-&#10;tainers, the volume can be used by all of them at once. &#10;6.1&#10;Introducing volumes&#10;Kubernetes volumes are a component of a pod and are thus defined in the pod&#8217;s spec-&#10;ification&#8212;much like containers. They aren&#8217;t a standalone Kubernetes object and can-&#10;not be created or deleted on their own. A volume is available to all containers in the&#10;pod, but it must be mounted in each container that needs to access it. In each con-&#10;tainer, you can mount the volume in any location of its filesystem.&#10;6.1.1&#10;Explaining volumes in an example&#10;Imagine you have a pod with three containers (shown in figure 6.1). One container&#10;runs a web server that serves HTML pages from the /var/htdocs directory and stores&#10;the access log to /var/logs. The second container runs an agent that creates HTML&#10;files and stores them in /var/html. The third container processes the logs it finds in&#10;the /var/logs directory (rotates them, compresses them, analyzes them, or whatever).&#10; Each container has a nicely defined single responsibility, but on its own each con-&#10;tainer wouldn&#8217;t be of much use. Creating a pod with these three containers without&#10;them sharing disk storage doesn&#8217;t make any sense, because the content generator&#10;would write the generated HTML files inside its own container and the web server&#10;couldn&#8217;t access those files, as it runs in a separate isolated container. Instead, it would&#10;serve an empty directory or whatever you put in the /var/htdocs directory in its con-&#10;tainer image. Similarly, the log rotator would never have anything to do, because its&#10;/var/logs directory would always remain empty with nothing writing logs there. A pod&#10;with these three containers and no volumes basically does nothing.&#10; But if you add two volumes to the pod and mount them at appropriate paths inside&#10;the three containers, as shown in figure 6.2, you&#8217;ve created a system that&#8217;s much more&#10; &#10;"
    color "green"
  ]
  node [
    id 210
    label "193"
    title "Page_193"
    color "blue"
  ]
  node [
    id 211
    label "text_104"
    title "161&#10;Introducing volumes&#10;Pod&#10;Container: WebServer&#10;Filesystem&#10;Webserver&#10;process&#10;Writes&#10;Reads&#10;/&#10;var/&#10;htdocs/&#10;logs/&#10;Container: ContentAgent&#10;Filesystem&#10;ContentAgent&#10;process&#10;Writes&#10;/&#10;var/&#10;html/&#10;Container: LogRotator&#10;Filesystem&#10;LogRotator&#10;process&#10;Reads&#10;/&#10;var/&#10;logs/&#10;Figure 6.1&#10;Three containers of the &#10;same pod without shared storage&#10;Pod&#10;Container: WebServer&#10;Filesystem&#10;/&#10;var/&#10;htdocs/&#10;logs/&#10;Container: ContentAgent&#10;Filesystem&#10;/&#10;var/&#10;html/&#10;Container: LogRotator&#10;Filesystem&#10;/&#10;var/&#10;logs/&#10;Volume:&#10;publicHtml&#10;Volume:&#10;logVol&#10;Figure 6.2&#10;Three containers sharing two &#10;volumes mounted at various mount paths&#10; &#10;"
    color "green"
  ]
  node [
    id 212
    label "194"
    title "Page_194"
    color "blue"
  ]
  node [
    id 213
    label "text_105"
    title "162&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;than the sum of its parts. Linux allows you to mount a filesystem at arbitrary locations&#10;in the file tree. When you do that, the contents of the mounted filesystem are accessi-&#10;ble in the directory it&#8217;s mounted into. By mounting the same volume into two contain-&#10;ers, they can operate on the same files. In your case, you&#8217;re mounting two volumes in&#10;three containers. By doing this, your three containers can work together and do some-&#10;thing useful. Let me explain how.&#10; First, the pod has a volume called publicHtml. This volume is mounted in the Web-&#10;Server container at /var/htdocs, because that&#8217;s the directory the web server serves&#10;files from. The same volume is also mounted in the ContentAgent container, but at&#10;/var/html, because that&#8217;s where the agent writes the files to. By mounting this single vol-&#10;ume like that, the web server will now serve the content generated by the content agent.&#10; Similarly, the pod also has a volume called logVol for storing logs. This volume is&#10;mounted at /var/logs in both the WebServer and the LogRotator containers. Note&#10;that it isn&#8217;t mounted in the ContentAgent container. The container cannot access its&#10;files, even though the container and the volume are part of the same pod. It&#8217;s not&#10;enough to define a volume in the pod; you need to define a VolumeMount inside the&#10;container&#8217;s spec also, if you want the container to be able to access it.&#10; The two volumes in this example can both initially be empty, so you can use a type&#10;of volume called emptyDir. Kubernetes also supports other types of volumes that are&#10;either populated during initialization of the volume from an external source, or an&#10;existing directory is mounted inside the volume. This process of populating or mount-&#10;ing a volume is performed before the pod&#8217;s containers are started. &#10; A volume is bound to the lifecycle of a pod and will stay in existence only while the&#10;pod exists, but depending on the volume type, the volume&#8217;s files may remain intact&#10;even after the pod and volume disappear, and can later be mounted into a new vol-&#10;ume. Let&#8217;s see what types of volumes exist.&#10;6.1.2&#10;Introducing available volume types&#10;A wide variety of volume types is available. Several are generic, while others are spe-&#10;cific to the actual storage technologies used underneath. Don&#8217;t worry if you&#8217;ve never&#10;heard of those technologies&#8212;I hadn&#8217;t heard of at least half of them. You&#8217;ll probably&#10;only use volume types for the technologies you already know and use. Here&#8217;s a list of&#10;several of the available volume types:&#10;&#61601;&#10;emptyDir&#8212;A simple empty directory used for storing transient data.&#10;&#61601;&#10;hostPath&#8212;Used for mounting directories from the worker node&#8217;s filesystem&#10;into the pod.&#10;&#61601;&#10;gitRepo&#8212;A volume initialized by checking out the contents of a Git repository.&#10;&#61601;&#10;nfs&#8212;An NFS share mounted into the pod.&#10;&#61601;&#10;gcePersistentDisk (Google Compute Engine Persistent Disk), awsElastic-&#10;BlockStore (Amazon Web Services Elastic Block Store Volume), azureDisk&#10;(Microsoft Azure Disk Volume)&#8212;Used for mounting cloud provider-specific&#10;storage.&#10; &#10;"
    color "green"
  ]
  node [
    id 214
    label "195"
    title "Page_195"
    color "blue"
  ]
  node [
    id 215
    label "text_106"
    title "163&#10;Using volumes to share data between containers&#10;&#61601;&#10;cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphere-&#10;Volume, photonPersistentDisk, scaleIO&#8212;Used for mounting other types of&#10;network storage.&#10;&#61601;&#10;configMap, secret, downwardAPI&#8212;Special types of volumes used to expose cer-&#10;tain Kubernetes resources and cluster information to the pod.&#10;&#61601;&#10;persistentVolumeClaim&#8212;A way to use a pre- or dynamically provisioned per-&#10;sistent storage. (We&#8217;ll talk about them in the last section of this chapter.)&#10;These volume types serve various purposes. You&#8217;ll learn about some of them in the&#10;following sections. Special types of volumes (secret, downwardAPI, configMap) are&#10;covered in the next two chapters, because they aren&#8217;t used for storing data, but for&#10;exposing Kubernetes metadata to apps running in the pod. &#10; A single pod can use multiple volumes of different types at the same time, and, as&#10;we&#8217;ve mentioned before, each of the pod&#8217;s containers can either have the volume&#10;mounted or not.&#10;6.2&#10;Using volumes to share data between containers&#10;Although a volume can prove useful even when used by a single container, let&#8217;s first&#10;focus on how it&#8217;s used for sharing data between multiple containers in a pod.&#10;6.2.1&#10;Using an emptyDir volume&#10;The simplest volume type is the emptyDir volume, so let&#8217;s look at it in the first exam-&#10;ple of how to define a volume in a pod. As the name suggests, the volume starts out as&#10;an empty directory. The app running inside the pod can then write any files it needs&#10;to it. Because the volume&#8217;s lifetime is tied to that of the pod, the volume&#8217;s contents are&#10;lost when the pod is deleted.&#10; An emptyDir volume is especially useful for sharing files between containers&#10;running in the same pod. But it can also be used by a single container for when a con-&#10;tainer needs to write data to disk temporarily, such as when performing a sort&#10;operation on a large dataset, which can&#8217;t fit into the available memory. The data could&#10;also be written to the container&#8217;s filesystem itself (remember the top read-write layer&#10;in a container?), but subtle differences exist between the two options. A container&#8217;s&#10;filesystem may not even be writable (we&#8217;ll talk about this toward the end of the book),&#10;so writing to a mounted volume might be the only option. &#10;USING AN EMPTYDIR VOLUME IN A POD&#10;Let&#8217;s revisit the previous example where a web server, a content agent, and a log rota-&#10;tor share two volumes, but let&#8217;s simplify a bit. You&#8217;ll build a pod with only the web&#10;server container and the content agent and a single volume for the HTML. &#10; You&#8217;ll use Nginx as the web server and the UNIX fortune command to generate&#10;the HTML content. The fortune command prints out a random quote every time you&#10;run it. You&#8217;ll create a script that invokes the fortune command every 10 seconds and&#10;stores its output in index.html. You&#8217;ll find an existing Nginx image available on&#10; &#10;"
    color "green"
  ]
  node [
    id 216
    label "196"
    title "Page_196"
    color "blue"
  ]
  node [
    id 217
    label "text_107"
    title "164&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;Docker Hub, but you&#8217;ll need to either create the fortune image yourself or use the&#10;one I&#8217;ve already built and pushed to Docker Hub under luksa/fortune. If you want a&#10;refresher on how to build Docker images, refer to the sidebar.&#10;CREATING THE POD&#10;Now that you have the two images required to run your pod, it&#8217;s time to create the pod&#10;manifest. Create a file called fortune-pod.yaml with the contents shown in the follow-&#10;ing listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: fortune&#10;spec:&#10;  containers:&#10;Building the fortune container image&#10;Here&#8217;s how to build the image. Create a new directory called fortune and then inside&#10;it, create a fortuneloop.sh shell script with the following contents:&#10;#!/bin/bash&#10;trap &#34;exit&#34; SIGINT&#10;mkdir /var/htdocs&#10;while :&#10;do&#10;  echo $(date) Writing fortune to /var/htdocs/index.html&#10;  /usr/games/fortune > /var/htdocs/index.html&#10;  sleep 10&#10;done&#10;Then, in the same directory, create a file called Dockerfile containing the following:&#10;FROM ubuntu:latest&#10;RUN apt-get update ; apt-get -y install fortune&#10;ADD fortuneloop.sh /bin/fortuneloop.sh&#10;ENTRYPOINT /bin/fortuneloop.sh&#10;The image is based on the ubuntu:latest image, which doesn&#8217;t include the fortune&#10;binary by default. That&#8217;s why in the second line of the Dockerfile you install it with&#10;apt-get. After that, you add the fortuneloop.sh script to the image&#8217;s /bin folder.&#10;In the last line of the Dockerfile, you specify that the fortuneloop.sh script should&#10;be executed when the image is run.&#10;After preparing both files, build and upload the image to Docker Hub with the following&#10;two commands (replace luksa with your own Docker Hub user ID):&#10;$ docker build -t luksa/fortune .&#10;$ docker push luksa/fortune&#10;Listing 6.1&#10;A pod with two containers sharing the same volume: fortune-pod.yaml&#10; &#10;"
    color "green"
  ]
  node [
    id 218
    label "197"
    title "Page_197"
    color "blue"
  ]
  node [
    id 219
    label "text_108"
    title "165&#10;Using volumes to share data between containers&#10;  - image: luksa/fortune                   &#10;    name: html-generator                   &#10;    volumeMounts:                          &#10;    - name: html                           &#10;      mountPath: /var/htdocs               &#10;  - image: nginx:alpine                   &#10;    name: web-server                      &#10;    volumeMounts:                         &#10;    - name: html                          &#10;      mountPath: /usr/share/nginx/html    &#10;      readOnly: true                      &#10;    ports:&#10;    - containerPort: 80&#10;      protocol: TCP&#10;  volumes:                 &#10;  - name: html             &#10;    emptyDir: {}           &#10;The pod contains two containers and a single volume that&#8217;s mounted in both of&#10;them, yet at different paths. When the html-generator container starts, it starts writ-&#10;ing the output of the fortune command to the /var/htdocs/index.html file every 10&#10;seconds. Because the volume is mounted at /var/htdocs, the index.html file is writ-&#10;ten to the volume instead of the container&#8217;s top layer. As soon as the web-server con-&#10;tainer starts, it starts serving whatever HTML files are in the /usr/share/nginx/html&#10;directory (this is the default directory Nginx serves files from). Because you mounted&#10;the volume in that exact location, Nginx will serve the index.html file written there&#10;by the container running the fortune loop. The end effect is that a client sending an&#10;HTTP request to the pod on port 80 will receive the current fortune message as&#10;the response. &#10;SEEING THE POD IN ACTION&#10;To see the fortune message, you need to enable access to the pod. You&#8217;ll do that by&#10;forwarding a port from your local machine to the pod:&#10;$ kubectl port-forward fortune 8080:80&#10;Forwarding from 127.0.0.1:8080 -> 80&#10;Forwarding from [::1]:8080 -> 80&#10;NOTE&#10;As an exercise, you can also expose the pod through a service instead&#10;of using port forwarding.&#10;Now you can access the Nginx server through port 8080 of your local machine. Use&#10;curl to do that:&#10;$ curl http://localhost:8080&#10;Beware of a tall blond man with one black shoe.&#10;If you wait a few seconds and send another request, you should receive a different&#10;message. By combining two containers, you created a simple app to see how a volume&#10;can glue together two containers and enhance what each of them does.&#10;The first container is called html-generator &#10;and runs the luksa/fortune image.&#10;The volume called html is mounted &#10;at /var/htdocs in the container.&#10;The second container is called web-server &#10;and runs the nginx:alpine image.&#10;The same volume as above is &#10;mounted at /usr/share/nginx/html &#10;as read-only.&#10;A single emptyDir volume &#10;called html that&#8217;s mounted &#10;in the two containers above&#10; &#10;"
    color "green"
  ]
  node [
    id 220
    label "198"
    title "Page_198"
    color "blue"
  ]
  node [
    id 221
    label "text_109"
    title "166&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;SPECIFYING THE MEDIUM TO USE FOR THE EMPTYDIR&#10;The emptyDir you used as the volume was created on the actual disk of the worker&#10;node hosting your pod, so its performance depends on the type of the node&#8217;s disks.&#10;But you can tell Kubernetes to create the emptyDir on a tmpfs filesystem (in memory&#10;instead of on disk). To do this, set the emptyDir&#8217;s medium to Memory like this:&#10;volumes:&#10;  - name: html&#10;    emptyDir:&#10;      medium: Memory    &#10;An emptyDir volume is the simplest type of volume, but other types build upon it.&#10;After the empty directory is created, they populate it with data. One such volume type&#10;is the gitRepo volume type, which we&#8217;ll introduce next.&#10;6.2.2&#10;Using a Git repository as the starting point for a volume &#10;A gitRepo volume is basically an emptyDir volume that gets populated by cloning a&#10;Git repository and checking out a specific revision when the pod is starting up (but&#10;before its containers are created). Figure 6.3 shows how this unfolds.&#10;NOTE&#10;After the gitRepo volume is created, it isn&#8217;t kept in sync with the repo&#10;it&#8217;s referencing. The files in the volume will not be updated when you push&#10;additional commits to the Git repository. However, if your pod is managed by&#10;a ReplicationController, deleting the pod will result in a new pod being cre-&#10;ated and this new pod&#8217;s volume will then contain the latest commits. &#10;For example, you can use a Git repository to store static HTML files of your website&#10;and create a pod containing a web server container and a gitRepo volume. Every time&#10;the pod is created, it pulls the latest version of your website and starts serving it. The&#10;This emptyDir&#8217;s &#10;files should be &#10;stored in memory.&#10;Pod&#10;Container&#10;User&#10;gitRepo&#10;volume&#10;1. User (or a replication&#10;controller) creates pod&#10;with gitRepo volume&#10;2. Kubernetes creates&#10;an empty directory and&#10;clones the speci&#64257;ed Git&#10;repository into it&#10;3. The pod&#8217;s container is started&#10;(with the volume mounted at&#10;the mount path)&#10;Repository&#10;Figure 6.3&#10;A gitRepo volume is an emptyDir volume initially populated with the contents of a &#10;Git repository.&#10; &#10;"
    color "green"
  ]
  node [
    id 222
    label "199"
    title "Page_199"
    color "blue"
  ]
  node [
    id 223
    label "text_110"
    title "167&#10;Using volumes to share data between containers&#10;only drawback to this is that you need to delete the pod every time you push changes&#10;to the gitRepo and want to start serving the new version of the website. &#10; Let&#8217;s do this right now. It&#8217;s not that different from what you did before. &#10;RUNNING A WEB SERVER POD SERVING FILES FROM A CLONED GIT REPOSITORY&#10;Before you create your pod, you&#8217;ll need an actual Git repository with HTML files in it.&#10;I&#8217;ve created a repo on GitHub at https:/&#10;/github.com/luksa/kubia-website-example.git.&#10;You&#8217;ll need to fork it (create your own copy of the repo on GitHub) so you can push&#10;changes to it later. &#10; Once you&#8217;ve created your fork, you can move on to creating the pod. This time,&#10;you&#8217;ll only need a single Nginx container and a single gitRepo volume in the pod (be&#10;sure to point the gitRepo volume to your own fork of my repository), as shown in the&#10;following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: gitrepo-volume-pod&#10;spec:&#10;  containers:&#10;  - image: nginx:alpine&#10;    name: web-server&#10;    volumeMounts:&#10;    - name: html&#10;      mountPath: /usr/share/nginx/html&#10;      readOnly: true&#10;    ports:&#10;    - containerPort: 80&#10;      protocol: TCP&#10;  volumes:&#10;  - name: html&#10;    gitRepo:                     &#10;      repository: https://github.com/luksa/kubia-website-example.git   &#10;      revision: master                     &#10;      directory: .      &#10;When you create the pod, the volume is first initialized as an empty directory and then&#10;the specified Git repository is cloned into it. If you hadn&#8217;t set the directory to . (dot),&#10;the repository would have been cloned into the kubia-website-example subdirectory,&#10;which isn&#8217;t what you want. You want the repo to be cloned into the root directory of&#10;your volume. Along with the repository, you also specified you want Kubernetes to&#10;check out whatever revision the master branch is pointing to at the time the volume&#10;is created. &#10; With the pod running, you can try hitting it through port forwarding, a service, or by&#10;executing the curl command from within the pod (or any other pod inside the cluster). &#10;Listing 6.2&#10;A pod using a gitRepo volume: gitrepo-volume-pod.yaml&#10;You&#8217;re creating a &#10;gitRepo volume.&#10;The volume will clone&#10;this Git repository.&#10;The master branch &#10;will be checked out.&#10;You want the repo to &#10;be cloned into the root &#10;dir of the volume.&#10; &#10;"
    color "green"
  ]
  node [
    id 224
    label "200"
    title "Page_200"
    color "blue"
  ]
  node [
    id 225
    label "text_111"
    title "168&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;CONFIRMING THE FILES AREN&#8217;T KEPT IN SYNC WITH THE GIT REPO&#10;Now you&#8217;ll make changes to the index.html file in your GitHub repository. If you&#10;don&#8217;t use Git locally, you can edit the file on GitHub directly&#8212;click on the file in your&#10;GitHub repository to open it and then click on the pencil icon to start editing it.&#10;Change the text and then commit the changes by clicking the button at the bottom.&#10; The master branch of the Git repository now includes the changes you made to the&#10;HTML file. These changes will not be visible on your Nginx web server yet, because&#10;the gitRepo volume isn&#8217;t kept in sync with the Git repository. You can confirm this by&#10;hitting the pod again. &#10; To see the new version of the website, you need to delete the pod and create&#10;it again. Instead of having to delete the pod every time you make changes, you could&#10;run an additional process, which keeps your volume in sync with the Git repository.&#10;I won&#8217;t explain in detail how to do this. Instead, try doing this yourself as an exer-&#10;cise, but here are a few pointers.&#10;INTRODUCING SIDECAR CONTAINERS&#10;The Git sync process shouldn&#8217;t run in the same container as the Nginx web server, but&#10;in a second container: a sidecar container. A sidecar container is a container that aug-&#10;ments the operation of the main container of the pod. You add a sidecar to a pod so&#10;you can use an existing container image instead of cramming additional logic into the&#10;main app&#8217;s code, which would make it overly complex and less reusable. &#10; To find an existing container image, which keeps a local directory synchronized&#10;with a Git repository, go to Docker Hub and search for &#8220;git sync.&#8221; You&#8217;ll find many&#10;images that do that. Then use the image in a new container in the pod from the previ-&#10;ous example, mount the pod&#8217;s existing gitRepo volume in the new container, and&#10;configure the Git sync container to keep the files in sync with your Git repo. If you set&#10;everything up correctly, you should see that the files the web server is serving are kept&#10;in sync with your GitHub repo. &#10;NOTE&#10;An example in chapter 18 includes using a Git sync container like the&#10;one explained here, so you can wait until you reach chapter 18 and follow the&#10;step-by-step instructions then instead of doing this exercise on your own now. &#10;USING A GITREPO VOLUME WITH PRIVATE GIT REPOSITORIES&#10;There&#8217;s one other reason for having to resort to Git sync sidecar containers. We&#10;haven&#8217;t talked about whether you can use a gitRepo volume with a private Git repo. It&#10;turns out you can&#8217;t. The current consensus among Kubernetes developers is to keep&#10;the gitRepo volume simple and not add any support for cloning private repositories&#10;through the SSH protocol, because that would require adding additional config&#10;options to the gitRepo volume. &#10; If you want to clone a private Git repo into your container, you should use a git-&#10;sync sidecar or a similar method instead of a gitRepo volume.&#10; &#10;"
    color "green"
  ]
  node [
    id 226
    label "201"
    title "Page_201"
    color "blue"
  ]
  node [
    id 227
    label "text_112"
    title "169&#10;Accessing files on the worker node&#8217;s filesystem&#10;WRAPPING UP THE GITREPO VOLUME&#10;A gitRepo volume, like the emptyDir volume, is basically a dedicated directory cre-&#10;ated specifically for, and used exclusively by, the pod that contains the volume. When&#10;the pod is deleted, the volume and its contents are deleted. Other types of volumes,&#10;however, don&#8217;t create a new directory, but instead mount an existing external direc-&#10;tory into the pod&#8217;s container&#8217;s filesystem. The contents of that volume can survive&#10;multiple pod instantiations. We&#8217;ll learn about those types of volumes next.&#10;6.3&#10;Accessing files on the worker node&#8217;s filesystem&#10;Most  pods should be oblivious of their host node, so they shouldn&#8217;t access any files on&#10;the node&#8217;s filesystem. But certain system-level pods (remember, these will usually be&#10;managed by a DaemonSet) do need to either read the node&#8217;s files or use the node&#8217;s&#10;filesystem to access the node&#8217;s devices through the filesystem. Kubernetes makes this&#10;possible through a hostPath volume. &#10;6.3.1&#10;Introducing the hostPath volume&#10;A hostPath volume points to a specific file or directory on the node&#8217;s filesystem (see&#10;figure 6.4). Pods running on the same node and using the same path in their host-&#10;Path volume see the same files.&#10;hostPath volumes are the first type of persistent storage we&#8217;re introducing, because&#10;both the gitRepo and emptyDir volumes&#8217; contents get deleted when a pod is torn&#10;down, whereas a hostPath volume&#8217;s contents don&#8217;t. If a pod is deleted and the next&#10;pod uses a hostPath volume pointing to the same path on the host, the new pod will&#10;see whatever was left behind by the previous pod, but only if it&#8217;s scheduled to the same&#10;node as the first pod.&#10;Node 1&#10;Pod&#10;hostPath&#10;volume&#10;Pod&#10;hostPath&#10;volume&#10;Node 2&#10;Pod&#10;hostPath&#10;volume&#10;/some/path/on/host&#10;/some/path/on/host&#10;Figure 6.4&#10;A hostPath volume mounts a file or directory on the worker node into &#10;the container&#8217;s filesystem.&#10; &#10;"
    color "green"
  ]
  node [
    id 228
    label "202"
    title "Page_202"
    color "blue"
  ]
  node [
    id 229
    label "text_113"
    title "170&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10; If you&#8217;re thinking of using a hostPath volume as the place to store a database&#8217;s&#10;data directory, think again. Because the volume&#8217;s contents are stored on a specific&#10;node&#8217;s filesystem, when the database pod gets rescheduled to another node, it will no&#10;longer see the data. This explains why it&#8217;s not a good idea to use a hostPath volume&#10;for regular pods, because it makes the pod sensitive to what node it&#8217;s scheduled to.&#10;6.3.2&#10;Examining system pods that use hostPath volumes&#10;Let&#8217;s see how a hostPath volume can be used properly. Instead of creating a new pod,&#10;let&#8217;s see if any existing system-wide pods are already using this type of volume. As you&#10;may remember from one of the previous chapters, several such pods are running in&#10;the kube-system namespace. Let&#8217;s list them again:&#10;$ kubectl get pod s --namespace kube-system&#10;NAME                          READY     STATUS    RESTARTS   AGE&#10;fluentd-kubia-4ebc2f1e-9a3e   1/1       Running   1          4d&#10;fluentd-kubia-4ebc2f1e-e2vz   1/1       Running   1          31d&#10;...&#10;Pick the first one and see what kinds of volumes it uses (shown in the following listing).&#10;$ kubectl describe po fluentd-kubia-4ebc2f1e-9a3e --namespace kube-system&#10;Name:           fluentd-cloud-logging-gke-kubia-default-pool-4ebc2f1e-9a3e&#10;Namespace:      kube-system&#10;...&#10;Volumes:&#10;  varlog:&#10;    Type:       HostPath (bare host directory volume)&#10;    Path:       /var/log&#10;  varlibdockercontainers:&#10;    Type:       HostPath (bare host directory volume)&#10;    Path:       /var/lib/docker/containers&#10;TIP&#10;If you&#8217;re using Minikube, try the kube-addon-manager-minikube pod.&#10;Aha! The pod uses two hostPath volumes to gain access to the node&#8217;s /var/log and&#10;the /var/lib/docker/containers directories. You&#8217;d think you were lucky to find a pod&#10;using a hostPath volume on the first try, but not really (at least not on GKE). Check&#10;the other pods, and you&#8217;ll see most use this type of volume either to access the node&#8217;s&#10;log files, kubeconfig (the Kubernetes config file), or the CA certificates.&#10; If you inspect the other pods, you&#8217;ll see none of them uses the hostPath volume&#10;for storing their own data. They all use it to get access to the node&#8217;s data. But as we&#8217;ll&#10;see later in the chapter, hostPath volumes are often used for trying out persistent stor-&#10;age in single-node clusters, such as the one created by Minikube. Read on to learn&#10;about the types of volumes you should use for storing persistent data properly even in&#10;a multi-node cluster.&#10;Listing 6.3&#10; A pod using hostPath volumes to access the node&#8217;s logs&#10; &#10;"
    color "green"
  ]
  node [
    id 230
    label "203"
    title "Page_203"
    color "blue"
  ]
  node [
    id 231
    label "text_114"
    title "171&#10;Using persistent storage&#10;TIP&#10;Remember to use hostPath volumes only if you need to read or write sys-&#10;tem files on the node. Never use them to persist data across pods. &#10;6.4&#10;Using persistent storage&#10;When an application running in a pod needs to persist data to disk and have that&#10;same data available even when the pod is rescheduled to another node, you can&#8217;t use&#10;any of the volume types we&#8217;ve mentioned so far. Because this data needs to be accessi-&#10;ble from any cluster node, it must be stored on some type of network-attached stor-&#10;age (NAS).&#10; To learn about volumes that allow persisting data, you&#8217;ll create a pod that will run&#10;the MongoDB document-oriented NoSQL database. Running a database pod without&#10;a volume or with a non-persistent volume doesn&#8217;t make sense, except for testing&#10;purposes, so you&#8217;ll add an appropriate type of volume to the pod and mount it in the&#10;MongoDB container. &#10;6.4.1&#10;Using a GCE Persistent Disk in a pod volume&#10;If you&#8217;ve been running these examples on Google Kubernetes Engine, which runs&#10;your cluster nodes on Google Compute Engine (GCE), you&#8217;ll use a GCE Persistent&#10;Disk as your underlying storage mechanism. &#10; In the early versions, Kubernetes didn&#8217;t provision the underlying storage automati-&#10;cally&#8212;you had to do that manually. Automatic provisioning is now possible, and you&#8217;ll&#10;learn about it later in the chapter, but first, you&#8217;ll start by provisioning the storage&#10;manually. It will give you a chance to learn exactly what&#8217;s going on underneath. &#10;CREATING A GCE PERSISTENT DISK&#10;You&#8217;ll start by creating the GCE persistent disk first. You need to create it in the same&#10;zone as your Kubernetes cluster. If you don&#8217;t remember what zone you created the&#10;cluster in, you can see it by listing your Kubernetes clusters with the gcloud command&#10;like this:&#10;$ gcloud container clusters list&#10;NAME   ZONE            MASTER_VERSION  MASTER_IP       ...&#10;kubia  europe-west1-b  1.2.5           104.155.84.137  ...&#10;This shows you&#8217;ve created your cluster in zone europe-west1-b, so you need to create&#10;the GCE persistent disk in the same zone as well. You create the disk like this:&#10;$ gcloud compute disks create --size=1GiB --zone=europe-west1-b mongodb&#10;WARNING: You have selected a disk size of under [200GB]. This may result in &#10;poor I/O performance. For more information, see: &#10;https://developers.google.com/compute/docs/disks#pdperformance.&#10;Created [https://www.googleapis.com/compute/v1/projects/rapid-pivot-&#10;136513/zones/europe-west1-b/disks/mongodb].&#10;NAME     ZONE            SIZE_GB  TYPE         STATUS&#10;mongodb  europe-west1-b  1        pd-standard  READY&#10; &#10;"
    color "green"
  ]
  node [
    id 232
    label "204"
    title "Page_204"
    color "blue"
  ]
  node [
    id 233
    label "text_115"
    title "172&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;This command creates a 1 GiB large GCE persistent disk called mongodb. You can&#10;ignore the warning about the disk size, because you don&#8217;t care about the disk&#8217;s perfor-&#10;mance for the tests you&#8217;re about to run.&#10;CREATING A POD USING A GCEPERSISTENTDISK VOLUME&#10;Now that you have your physical storage properly set up, you can use it in a volume&#10;inside your MongoDB pod. You&#8217;re going to prepare the YAML for the pod, which is&#10;shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: mongodb &#10;spec:&#10;  volumes:&#10;  - name: mongodb-data          &#10;    gcePersistentDisk:           &#10;      pdName: mongodb            &#10;      fsType: ext4             &#10;  containers:&#10;  - image: mongo&#10;    name: mongodb&#10;    volumeMounts:                &#10;    - name: mongodb-data         &#10;      mountPath: /data/db      &#10;    ports:&#10;    - containerPort: 27017&#10;      protocol: TCP&#10;NOTE&#10;If you&#8217;re using Minikube, you can&#8217;t use a GCE Persistent Disk, but you&#10;can deploy mongodb-pod-hostpath.yaml, which uses a hostPath volume&#10;instead of a GCE PD.&#10;The pod contains a single container and a single volume backed by the GCE Per-&#10;sistent Disk you&#8217;ve created (as shown in figure 6.5). You&#8217;re mounting the volume&#10;inside the container at /data/db, because that&#8217;s where MongoDB stores its data.&#10;Listing 6.4&#10;A pod using a gcePersistentDisk volume: mongodb-pod-gcepd.yaml&#10;The name&#10;of the&#10;volume&#10;(also&#10;referenced&#10;when&#10;mounting&#10;the volume)&#10;The type of the volume &#10;is a GCE Persistent Disk.&#10;The name of the persistent &#10;disk must match the actual &#10;PD you created earlier.&#10;The filesystem type is EXT4 &#10;(a type of Linux filesystem).&#10;The path where MongoDB &#10;stores its data&#10;Pod: mongodb&#10;Container: mongodb&#10;volumeMounts:&#10;name: mongodb-data&#10;mountPath: /data/db&#10;gcePersistentDisk:&#10;pdName: mongodb&#10;GCE&#10;Persistent Disk:&#10;mongodb&#10;Volume:&#10;mongodb&#10;Figure 6.5&#10;A pod with a single container running MongoDB, which mounts a volume referencing an &#10;external GCE Persistent Disk&#10; &#10;"
    color "green"
  ]
  node [
    id 234
    label "205"
    title "Page_205"
    color "blue"
  ]
  node [
    id 235
    label "text_116"
    title "173&#10;Using persistent storage&#10;WRITING DATA TO THE PERSISTENT STORAGE BY ADDING DOCUMENTS TO YOUR MONGODB DATABASE&#10;Now that you&#8217;ve created the pod and the container has been started, you can run the&#10;MongoDB shell inside the container and use it to write some data to the data store.&#10; You&#8217;ll run the shell as shown in the following listing.&#10;$ kubectl exec -it mongodb mongo&#10;MongoDB shell version: 3.2.8&#10;connecting to: mongodb://127.0.0.1:27017&#10;Welcome to the MongoDB shell.&#10;For interactive help, type &#34;help&#34;.&#10;For more comprehensive documentation, see&#10;    http://docs.mongodb.org/&#10;Questions? Try the support group&#10;    http://groups.google.com/group/mongodb-user&#10;...&#10;> &#10;MongoDB allows storing JSON documents, so you&#8217;ll store one to see if it&#8217;s stored per-&#10;sistently and can be retrieved after the pod is re-created. Insert a new JSON document&#10;with the following commands: &#10;> use mystore&#10;switched to db mystore&#10;> db.foo.insert({name:'foo'})&#10;WriteResult({ &#34;nInserted&#34; : 1 })&#10;You&#8217;ve inserted a simple JSON document with a single property (name: &#8217;foo&#8217;). Now,&#10;use the find() command to see the document you inserted:&#10;> db.foo.find()&#10;{ &#34;_id&#34; : ObjectId(&#34;57a61eb9de0cfd512374cc75&#34;), &#34;name&#34; : &#34;foo&#34; }&#10;There it is. The document should be stored in your GCE persistent disk now. &#10;RE-CREATING THE POD AND VERIFYING THAT IT CAN READ THE DATA PERSISTED BY THE PREVIOUS POD&#10;You can now exit the mongodb shell (type exit and press Enter), and then delete the&#10;pod and recreate it:&#10;$ kubectl delete pod mongodb&#10;pod &#34;mongodb&#34; deleted&#10;$ kubectl create -f mongodb-pod-gcepd.yaml&#10;pod &#34;mongodb&#34; created&#10;The new pod uses the exact same GCE persistent disk as the previous pod, so the&#10;MongoDB container running inside it should see the exact same data, even if the pod&#10;is scheduled to a different node.&#10;TIP&#10;You can see what node a pod is scheduled to by running kubectl get po&#10;-o wide.&#10;Listing 6.5&#10;Entering the MongoDB shell inside the mongodb pod&#10; &#10;"
    color "green"
  ]
  node [
    id 236
    label "206"
    title "Page_206"
    color "blue"
  ]
  node [
    id 237
    label "text_117"
    title "174&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;Once the container is up, you can again run the MongoDB shell and check to see if the&#10;document you stored earlier can still be retrieved, as shown in the following listing.&#10;$ kubectl exec -it mongodb mongo&#10;MongoDB shell version: 3.2.8&#10;connecting to: mongodb://127.0.0.1:27017&#10;Welcome to the MongoDB shell.&#10;...&#10;> use mystore&#10;switched to db mystore&#10;> db.foo.find()&#10;{ &#34;_id&#34; : ObjectId(&#34;57a61eb9de0cfd512374cc75&#34;), &#34;name&#34; : &#34;foo&#34; }&#10;As expected, the data is still there, even though you deleted the pod and re-created it.&#10;This confirms you can use a GCE persistent disk to persist data across multiple pod&#10;instances. &#10; You&#8217;re done playing with the MongoDB pod, so go ahead and delete it again, but&#10;hold off on deleting the underlying GCE persistent disk. You&#8217;ll use it again later in&#10;the chapter.&#10;6.4.2&#10;Using other types of volumes with underlying persistent storage&#10;The reason you created the GCE Persistent Disk volume is because your Kubernetes&#10;cluster runs on Google Kubernetes Engine. When you run your cluster elsewhere, you&#10;should use other types of volumes, depending on the underlying infrastructure.&#10; If your Kubernetes cluster is running on Amazon&#8217;s AWS EC2, for example, you can&#10;use an awsElasticBlockStore volume to provide persistent storage for your pods. If&#10;your cluster runs on Microsoft Azure, you can use the azureFile or the azureDisk&#10;volume. We won&#8217;t go into detail on how to do that here, but it&#8217;s virtually the same as in&#10;the previous example. First, you need to create the actual underlying storage, and&#10;then set the appropriate properties in the volume definition.&#10;USING AN AWS ELASTIC BLOCK STORE VOLUME&#10;For example, to use an AWS elastic block store instead of the GCE Persistent Disk,&#10;you&#8217;d only need to change the volume definition as shown in the following listing (see&#10;those lines printed in bold).&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: mongodb &#10;spec:&#10;  volumes:                       &#10;  - name: mongodb-data           &#10;    awsElasticBlockStore:          &#10;Listing 6.6&#10;Retrieving MongoDB&#8217;s persisted data in a new pod&#10;Listing 6.7&#10;A pod using an awsElasticBlockStore volume: mongodb-pod-aws.yaml&#10;Using awsElasticBlockStore &#10;instead of gcePersistentDisk&#10; &#10;"
    color "green"
  ]
  node [
    id 238
    label "207"
    title "Page_207"
    color "blue"
  ]
  node [
    id 239
    label "text_118"
    title "175&#10;Using persistent storage&#10;      volumeId: my-volume          &#10;      fsType: ext4       &#10;  containers:&#10;  - ...&#10;USING AN NFS VOLUME&#10;If your cluster is running on your own set of servers, you have a vast array of other sup-&#10;ported options for mounting external storage inside your volume. For example, to&#10;mount a simple NFS share, you only need to specify the NFS server and the path&#10;exported by the server, as shown in the following listing.&#10;  volumes:                       &#10;  - name: mongodb-data           &#10;    nfs:                     &#10;      server: 1.2.3.4         &#10;      path: /some/path     &#10;USING OTHER STORAGE TECHNOLOGIES&#10;Other supported options include iscsi for mounting an ISCSI disk resource, glusterfs&#10;for a GlusterFS mount, rbd for a RADOS Block Device, flexVolume, cinder, cephfs,&#10;flocker, fc (Fibre Channel), and others. You don&#8217;t need to know all of them if you&#8217;re&#10;not using them. They&#8217;re mentioned here to show you that Kubernetes supports a&#10;broad range of storage technologies and you can use whichever you prefer and are&#10;used to.&#10; To see details on what properties you need to set for each of these volume types,&#10;you can either turn to the Kubernetes API definitions in the Kubernetes API refer-&#10;ence or look up the information through kubectl explain, as shown in chapter 3. If&#10;you&#8217;re already familiar with a particular storage technology, using the explain com-&#10;mand should allow you to easily figure out how to mount a volume of the proper type&#10;and use it in your pods.&#10; But does a developer need to know all this stuff? Should a developer, when creat-&#10;ing a pod, have to deal with infrastructure-related storage details, or should that be&#10;left to the cluster administrator? &#10; Having a pod&#8217;s volumes refer to the actual underlying infrastructure isn&#8217;t what&#10;Kubernetes is about, is it? For example, for a developer to have to specify the host-&#10;name of the NFS server feels wrong. And that&#8217;s not even the worst thing about it. &#10; Including this type of infrastructure-related information into a pod definition&#10;means the pod definition is pretty much tied to a specific Kubernetes cluster. You&#10;can&#8217;t use the same pod definition in another one. That&#8217;s why using volumes like this&#10;isn&#8217;t the best way to attach persistent storage to your pods. You&#8217;ll learn how to improve&#10;on this in the next section.&#10;Listing 6.8&#10;A pod using an nfs volume: mongodb-pod-nfs.yaml&#10;Specify the ID of the EBS &#10;volume you created.&#10;The filesystem type &#10;is EXT4 as before.&#10;This volume is backed &#10;by an NFS share.&#10;The IP of the &#10;NFS server&#10;The path exported &#10;by the server&#10; &#10;"
    color "green"
  ]
  node [
    id 240
    label "208"
    title "Page_208"
    color "blue"
  ]
  node [
    id 241
    label "text_119"
    title "176&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;6.5&#10;Decoupling pods from the underlying storage technology&#10;All the persistent volume types we&#8217;ve explored so far have required the developer of the&#10;pod to have knowledge of the actual network storage infrastructure available in the clus-&#10;ter. For example, to create a NFS-backed volume, the developer has to know the actual&#10;server the NFS export is located on. This is against the basic idea of Kubernetes, which&#10;aims to hide the actual infrastructure from both the application and its developer, leav-&#10;ing them free from worrying about the specifics of the infrastructure and making apps&#10;portable across a wide array of cloud providers and on-premises datacenters.&#10; Ideally, a developer deploying their apps on Kubernetes should never have to&#10;know what kind of storage technology is used underneath, the same way they don&#8217;t&#10;have to know what type of physical servers are being used to run their pods. Infrastruc-&#10;ture-related dealings should be the sole domain of the cluster administrator.&#10; When a developer needs a certain amount of persistent storage for their applica-&#10;tion, they can request it from Kubernetes, the same way they can request CPU, mem-&#10;ory, and other resources when creating a pod. The system administrator can configure&#10;the cluster so it can give the apps what they request.&#10;6.5.1&#10;Introducing PersistentVolumes and PersistentVolumeClaims&#10;To enable apps to request storage in a Kubernetes cluster without having to deal with&#10;infrastructure specifics, two new resources were introduced. They are Persistent-&#10;Volumes and PersistentVolumeClaims. The names may be a bit misleading, because as&#10;you&#8217;ve seen in the previous few sections, even regular Kubernetes volumes can be&#10;used to store persistent data. &#10; Using a PersistentVolume inside a pod is a little more complex than using a regular&#10;pod volume, so let&#8217;s illustrate how pods, PersistentVolumeClaims, PersistentVolumes,&#10;and the actual underlying storage relate to each other in figure 6.6.&#10;Pod&#10;Admin&#10;Volume&#10;1. Cluster admin sets up some type of&#10;network storage (NFS export or similar)&#10;2. Admin then creates a PersistentVolume (PV)&#10;by posting a PV descriptor to the Kubernetes API&#10;NFS&#10;export&#10;Persistent&#10;Volume&#10;User&#10;Persistent&#10;VolumeClaim&#10;3. User creates a&#10;PersistentVolumeClaim (PVC)&#10;4. Kubernetes &#64257;nds a PV of&#10;adequate size and access&#10;mode and binds the PVC&#10;to the PV&#10;5. User creates a&#10;pod with a volume&#10;referencing the PVC&#10;Figure 6.6&#10;PersistentVolumes are provisioned by cluster admins and consumed by pods &#10;through PersistentVolumeClaims.&#10; &#10;"
    color "green"
  ]
  node [
    id 242
    label "209"
    title "Page_209"
    color "blue"
  ]
  node [
    id 243
    label "text_120"
    title "177&#10;Decoupling pods from the underlying storage technology&#10;Instead of the developer adding a technology-specific volume to their pod, it&#8217;s the&#10;cluster administrator who sets up the underlying storage and then registers it in&#10;Kubernetes by creating a PersistentVolume resource through the Kubernetes API&#10;server. When creating the PersistentVolume, the admin specifies its size and the access&#10;modes it supports. &#10; When a cluster user needs to use persistent storage in one of their pods, they first&#10;create a PersistentVolumeClaim manifest, specifying the minimum size and the access&#10;mode they require. The user then submits the PersistentVolumeClaim manifest to the&#10;Kubernetes API server, and Kubernetes finds the appropriate PersistentVolume and&#10;binds the volume to the claim. &#10; The PersistentVolumeClaim can then be used as one of the volumes inside a pod.&#10;Other users cannot use the same PersistentVolume until it has been released by delet-&#10;ing the bound PersistentVolumeClaim.&#10;6.5.2&#10;Creating a PersistentVolume&#10;Let&#8217;s revisit the MongoDB example, but unlike before, you won&#8217;t reference the GCE&#10;Persistent Disk in the pod directly. Instead, you&#8217;ll first assume the role of a cluster&#10;administrator and create a PersistentVolume backed by the GCE Persistent Disk. Then&#10;you&#8217;ll assume the role of the application developer and first claim the PersistentVol-&#10;ume and then use it inside your pod.&#10; In section 6.4.1 you set up the physical storage by provisioning the GCE Persistent&#10;Disk, so you don&#8217;t need to do that again. All you need to do is create the Persistent-&#10;Volume resource in Kubernetes by preparing the manifest shown in the following list-&#10;ing and posting it to the API server.&#10;apiVersion: v1&#10;kind: PersistentVolume&#10;metadata:&#10;  name: mongodb-pv&#10;spec:&#10;  capacity:                  &#10;    storage: 1Gi             &#10;  accessModes:                              &#10;  - ReadWriteOnce                           &#10;  - ReadOnlyMany                            &#10;  persistentVolumeReclaimPolicy: Retain    &#10;  gcePersistentDisk:                      &#10;    pdName: mongodb                       &#10;    fsType: ext4                          &#10;Listing 6.9&#10;A gcePersistentDisk PersistentVolume: mongodb-pv-gcepd.yaml&#10;Defining the &#10;PersistentVolume&#8217;s size&#10;It can either be mounted by a single &#10;client for reading and writing or by &#10;multiple clients for reading only.&#10;After the claim is released, &#10;the PersistentVolume &#10;should be retained (not &#10;erased or deleted).&#10;The PersistentVolume is &#10;backed by the GCE Persistent &#10;Disk you created earlier.&#10; &#10;"
    color "green"
  ]
  node [
    id 244
    label "210"
    title "Page_210"
    color "blue"
  ]
  node [
    id 245
    label "text_121"
    title "178&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;NOTE&#10;If you&#8217;re using Minikube, create the PV using the mongodb-pv-host-&#10;path.yaml file.&#10;When creating a PersistentVolume, the administrator needs to tell Kubernetes what its&#10;capacity is and whether it can be read from and/or written to by a single node or by&#10;multiple nodes at the same time. They also need to tell Kubernetes what to do with the&#10;PersistentVolume when it&#8217;s released (when the PersistentVolumeClaim it&#8217;s bound to is&#10;deleted). And last, but certainly not least, they need to specify the type, location, and&#10;other properties of the actual storage this PersistentVolume is backed by. If you look&#10;closely, this last part is exactly the same as earlier, when you referenced the GCE Per-&#10;sistent Disk in the pod volume directly (shown again in the following listing).&#10;spec:&#10;  volumes:                       &#10;  - name: mongodb-data           &#10;    gcePersistentDisk:           &#10;      pdName: mongodb            &#10;      fsType: ext4               &#10;  ...&#10;After you create the PersistentVolume with the kubectl create command, it should&#10;be ready to be claimed. See if it is by listing all PersistentVolumes:&#10;$ kubectl get pv&#10;NAME         CAPACITY   RECLAIMPOLICY   ACCESSMODES   STATUS      CLAIM&#10;mongodb-pv   1Gi        Retain          RWO,ROX       Available   &#10;NOTE&#10;Several columns are omitted. Also, pv is used as a shorthand for&#10;persistentvolume.&#10;As expected, the PersistentVolume is shown as Available, because you haven&#8217;t yet cre-&#10;ated the PersistentVolumeClaim. &#10;NOTE&#10;PersistentVolumes don&#8217;t belong to any namespace (see figure 6.7).&#10;They&#8217;re cluster-level resources like nodes.&#10;Listing 6.10&#10;Referencing a GCE PD in a pod&#8217;s volume&#10; &#10;"
    color "green"
  ]
  node [
    id 246
    label "211"
    title "Page_211"
    color "blue"
  ]
  node [
    id 247
    label "text_122"
    title "179&#10;Decoupling pods from the underlying storage technology&#10;6.5.3&#10;Claiming a PersistentVolume by creating a &#10;PersistentVolumeClaim&#10;Now let&#8217;s lay down our admin hats and put our developer hats back on. Say you need&#10;to deploy a pod that requires persistent storage. You&#8217;ll use the PersistentVolume you&#10;created earlier. But you can&#8217;t use it directly in the pod. You need to claim it first.&#10; Claiming a PersistentVolume is a completely separate process from creating a pod,&#10;because you want the same PersistentVolumeClaim to stay available even if the pod is&#10;rescheduled (remember, rescheduling means the previous pod is deleted and a new&#10;one is created). &#10;CREATING A PERSISTENTVOLUMECLAIM&#10;You&#8217;ll create the claim now. You need to prepare a PersistentVolumeClaim manifest&#10;like the one shown in the following listing and post it to the Kubernetes API through&#10;kubectl create.&#10;apiVersion: v1&#10;kind: PersistentVolumeClaim&#10;metadata:&#10;  name: mongodb-pvc          &#10;Listing 6.11&#10;A PersistentVolumeClaim: mongodb-pvc.yaml&#10;Pod(s)&#10;Pod(s)&#10;Persistent&#10;Volume&#10;Persistent&#10;Volume&#10;Persistent&#10;Volume&#10;Persistent&#10;Volume&#10;...&#10;User A&#10;Persistent&#10;Volume&#10;Claim(s)&#10;Persistent&#10;Volume&#10;Claim(s)&#10;Namespace A&#10;User B&#10;Namespace B&#10;Node&#10;Node&#10;Node&#10;Node&#10;Node&#10;Node&#10;Persistent&#10;Volume&#10;Figure 6.7&#10;PersistentVolumes, like cluster Nodes, don&#8217;t belong to any namespace, unlike pods and &#10;PersistentVolumeClaims.&#10;The name of your claim&#8212;you&#8217;ll &#10;need this later when using the &#10;claim as the pod&#8217;s volume.&#10; &#10;"
    color "green"
  ]
  node [
    id 248
    label "212"
    title "Page_212"
    color "blue"
  ]
  node [
    id 249
    label "text_123"
    title "180&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;spec:&#10;  resources:&#10;    requests:                &#10;      storage: 1Gi           &#10;  accessModes:              &#10;  - ReadWriteOnce           &#10;  storageClassName: &#34;&#34;     &#10;As soon as you create the claim, Kubernetes finds the appropriate PersistentVolume&#10;and binds it to the claim. The PersistentVolume&#8217;s capacity must be large enough to&#10;accommodate what the claim requests. Additionally, the volume&#8217;s access modes must&#10;include the access modes requested by the claim. In your case, the claim requests 1 GiB&#10;of storage and a ReadWriteOnce access mode. The PersistentVolume you created ear-&#10;lier matches those two requirements so it is bound to your claim. You can see this by&#10;inspecting the claim.&#10;LISTING PERSISTENTVOLUMECLAIMS&#10;List all PersistentVolumeClaims to see the state of your PVC:&#10;$ kubectl get pvc&#10;NAME          STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE&#10;mongodb-pvc   Bound     mongodb-pv   1Gi        RWO,ROX       3s&#10;NOTE&#10;We&#8217;re using pvc as a shorthand for persistentvolumeclaim.&#10;The claim is shown as Bound to PersistentVolume mongodb-pv. Note the abbreviations&#10;used for the access modes:&#10;&#61601;&#10;RWO&#8212;ReadWriteOnce&#8212;Only a single node can mount the volume for reading&#10;and writing.&#10;&#61601;&#10;ROX&#8212;ReadOnlyMany&#8212;Multiple nodes can mount the volume for reading.&#10;&#61601;&#10;RWX&#8212;ReadWriteMany&#8212;Multiple nodes can mount the volume for both reading&#10;and writing.&#10;NOTE&#10;RWO, ROX, and RWX pertain to the number of worker nodes that can use&#10;the volume at the same time, not to the number of pods!&#10;LISTING PERSISTENTVOLUMES&#10;You can also see that the PersistentVolume is now Bound and no longer Available by&#10;inspecting it with kubectl get:&#10;$ kubectl get pv&#10;NAME         CAPACITY   ACCESSMODES   STATUS   CLAIM                 AGE&#10;mongodb-pv   1Gi        RWO,ROX       Bound    default/mongodb-pvc   1m&#10;The PersistentVolume shows it&#8217;s bound to claim default/mongodb-pvc. The default&#10;part is the namespace the claim resides in (you created the claim in the default&#10;Requesting 1 GiB of storage&#10;You want the storage to support a single &#10;client (performing both reads and writes).&#10;You&#8217;ll learn about this in the section &#10;about dynamic provisioning.&#10; &#10;"
    color "green"
  ]
  node [
    id 250
    label "213"
    title "Page_213"
    color "blue"
  ]
  node [
    id 251
    label "text_124"
    title "181&#10;Decoupling pods from the underlying storage technology&#10;namespace). We&#8217;ve already said that PersistentVolume resources are cluster-scoped&#10;and thus cannot be created in a specific namespace, but PersistentVolumeClaims can&#10;only be created in a specific namespace. They can then only be used by pods in the&#10;same namespace.&#10;6.5.4&#10;Using a PersistentVolumeClaim in a pod&#10;The PersistentVolume is now yours to use. Nobody else can claim the same volume&#10;until you release it. To use it inside a pod, you need to reference the Persistent-&#10;VolumeClaim by name inside the pod&#8217;s volume (yes, the PersistentVolumeClaim, not&#10;the PersistentVolume directly!), as shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: mongodb &#10;spec:&#10;  containers:&#10;  - image: mongo&#10;    name: mongodb&#10;    volumeMounts:&#10;    - name: mongodb-data&#10;      mountPath: /data/db&#10;    ports:&#10;    - containerPort: 27017&#10;      protocol: TCP&#10;  volumes:&#10;  - name: mongodb-data&#10;    persistentVolumeClaim:       &#10;      claimName: mongodb-pvc     &#10;Go ahead and create the pod. Now, check to see if the pod is indeed using the same&#10;PersistentVolume and its underlying GCE PD. You should see the data you stored ear-&#10;lier by running the MongoDB shell again, as shown in the following listing.&#10;$ kubectl exec -it mongodb mongo&#10;MongoDB shell version: 3.2.8&#10;connecting to: mongodb://127.0.0.1:27017&#10;Welcome to the MongoDB shell.&#10;...&#10;> use mystore&#10;switched to db mystore&#10;> db.foo.find()&#10;{ &#34;_id&#34; : ObjectId(&#34;57a61eb9de0cfd512374cc75&#34;), &#34;name&#34; : &#34;foo&#34; }&#10;And there it is. You&#8216;re able to retrieve the document you stored into MongoDB&#10;previously.&#10;Listing 6.12&#10;A pod using a PersistentVolumeClaim volume: mongodb-pod-pvc.yaml&#10;Listing 6.13&#10;Retrieving MongoDB&#8217;s persisted data in the pod using the PVC and PV&#10;Referencing the PersistentVolumeClaim &#10;by name in the pod volume&#10; &#10;"
    color "green"
  ]
  node [
    id 252
    label "214"
    title "Page_214"
    color "blue"
  ]
  node [
    id 253
    label "text_125"
    title "182&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;6.5.5&#10;Understanding the benefits of using PersistentVolumes and claims&#10;Examine figure 6.8, which shows both ways a pod can use a GCE Persistent Disk&#8212;&#10;directly or through a PersistentVolume and claim.&#10;Consider how using this indirect method of obtaining storage from the infrastructure&#10;is much simpler for the application developer (or cluster user). Yes, it does require&#10;the additional steps of creating the PersistentVolume and the PersistentVolumeClaim,&#10;but the developer doesn&#8217;t have to know anything about the actual storage technology&#10;used underneath. &#10; Additionally, the same pod and claim manifests can now be used on many different&#10;Kubernetes clusters, because they don&#8217;t refer to anything infrastructure-specific. The&#10;claim states, &#8220;I need x amount of storage and I need to be able to read and write to it&#10;by a single client at once,&#8221; and then the pod references the claim by name in one of&#10;its volumes.&#10;Pod: mongodb&#10;Container: mongodb&#10;volumeMounts:&#10;name: mongodb-data&#10;mountPath: /data/db&#10;gcePersistentDisk:&#10;pdName: mongodb&#10;GCE&#10;Persistent Disk:&#10;mongodb&#10;Volume:&#10;mongodb&#10;Pod: mongodb&#10;Container: mongodb&#10;volumeMounts:&#10;name: mongodb-data&#10;mountPath: /data/db&#10;persistentVolumeClaim:&#10;claimName: mongodb-pvc&#10;gcePersistentDisk:&#10;pdName: mongodb&#10;GCE&#10;Persistent Disk:&#10;mongodb&#10;PersistentVolume:&#10;mongodb-pv&#10;(1 Gi, RWO, RWX)&#10;Volume:&#10;mongodb&#10;Claim lists&#10;1Gi and&#10;ReadWriteOnce&#10;access&#10;PersistentVolumeClaim:&#10;mongodb-pvc&#10;Figure 6.8&#10;Using the GCE Persistent Disk directly or through a PVC and PV&#10; &#10;"
    color "green"
  ]
  node [
    id 254
    label "215"
    title "Page_215"
    color "blue"
  ]
  node [
    id 255
    label "text_126"
    title "183&#10;Decoupling pods from the underlying storage technology&#10;6.5.6&#10;Recycling PersistentVolumes&#10;Before you wrap up this section on PersistentVolumes, let&#8217;s do one last quick experi-&#10;ment. Delete the pod and the PersistentVolumeClaim:&#10;$ kubectl delete pod mongodb&#10;pod &#34;mongodb&#34; deleted&#10;$ kubectl delete pvc mongodb-pvc&#10;persistentvolumeclaim &#34;mongodb-pvc&#34; deleted&#10;What if you create the PersistentVolumeClaim again? Will it be bound to the Persistent-&#10;Volume or not? After you create the claim, what does kubectl get pvc show?&#10;$ kubectl get pvc&#10;NAME           STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE&#10;mongodb-pvc    Pending                                         13s&#10;The claim&#8217;s status is shown as Pending. Interesting. When you created the claim ear-&#10;lier, it was immediately bound to the PersistentVolume, so why wasn&#8217;t it bound now?&#10;Maybe listing the PersistentVolumes can shed more light on this:&#10;$ kubectl get pv&#10;NAME        CAPACITY  ACCESSMODES  STATUS    CLAIM               REASON AGE&#10;mongodb-pv  1Gi       RWO,ROX      Released  default/mongodb-pvc        5m&#10;The STATUS column shows the PersistentVolume as Released, not Available like&#10;before. Because you&#8217;ve already used the volume, it may contain data and shouldn&#8217;t be&#10;bound to a completely new claim without giving the cluster admin a chance to clean it&#10;up. Without this, a new pod using the same PersistentVolume could read the data&#10;stored there by the previous pod, even if the claim and pod were created in a different&#10;namespace (and thus likely belong to a different cluster tenant).&#10;RECLAIMING PERSISTENTVOLUMES MANUALLY&#10;You told Kubernetes you wanted your PersistentVolume to behave like this when you&#10;created it&#8212;by setting its persistentVolumeReclaimPolicy to Retain. You wanted&#10;Kubernetes to retain the volume and its contents after it&#8217;s released from its claim. As&#10;far as I&#8217;m aware, the only way to manually recycle the PersistentVolume to make it&#10;available again is to delete and recreate the PersistentVolume resource. As you do&#10;that, it&#8217;s your decision what to do with the files on the underlying storage: you can&#10;either delete them or leave them alone so they can be reused by the next  pod.&#10;RECLAIMING PERSISTENTVOLUMES AUTOMATICALLY&#10;Two other possible reclaim policies exist: Recycle and Delete. The first one deletes&#10;the volume&#8217;s contents and makes the volume available to be claimed again. This way,&#10;the PersistentVolume can be reused multiple times by different PersistentVolume-&#10;Claims and different pods, as you can see in figure 6.9.&#10; The Delete policy, on the other hand, deletes the underlying storage. Note that&#10;the Recycle option is currently not available for GCE Persistent Disks. This type of&#10; &#10;"
    color "green"
  ]
  node [
    id 256
    label "216"
    title "Page_216"
    color "blue"
  ]
  node [
    id 257
    label "text_127"
    title "184&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;A PersistentVolume only supports the Retain or Delete policies. Other Persistent-&#10;Volume types may or may not support each of these options, so before creating your&#10;own PersistentVolume, be sure to check what reclaim policies are supported for the&#10;specific underlying storage you&#8217;ll use in the volume.&#10;TIP&#10;You can change the PersistentVolume reclaim policy on an existing&#10;PersistentVolume. For example, if it&#8217;s initially set to Delete, you can easily&#10;change it to Retain to prevent losing valuable data.&#10;6.6&#10;Dynamic provisioning of PersistentVolumes&#10;You&#8217;ve seen how using PersistentVolumes and PersistentVolumeClaims makes it easy&#10;to obtain persistent storage without the developer having to deal with the actual stor-&#10;age technology used underneath. But this still requires a cluster administrator to pro-&#10;vision the actual storage up front. Luckily, Kubernetes can also perform this job&#10;automatically through dynamic provisioning of PersistentVolumes.&#10; The cluster admin, instead of creating PersistentVolumes, can deploy a Persistent-&#10;Volume provisioner and define one or more StorageClass objects to let users choose&#10;what type of PersistentVolume they want. The users can refer to the StorageClass in&#10;their PersistentVolumeClaims and the provisioner will take that into account when&#10;provisioning the persistent storage. &#10;NOTE&#10;Similar to PersistentVolumes, StorageClass resources aren&#8217;t namespaced.&#10;Kubernetes includes provisioners for the most popular cloud providers, so the admin-&#10;istrator doesn&#8217;t always need to deploy a provisioner. But if Kubernetes is deployed&#10;on-premises, a custom provisioner needs to be deployed.&#10;PersistentVolume&#10;PersistentVolumeClaim 1&#10;Pod 1&#10;Pod 2&#10;PersistentVolumeClaim 2&#10;Pod 3&#10;PVC is deleted;&#10;PV is automatically&#10;recycled and ready&#10;to be claimed and&#10;re-used again&#10;User creates&#10;PersistentVolumeClaim&#10;Pod 2&#10;unmounts&#10;PVC&#10;Pod 2&#10;mounts&#10;PVC&#10;Pod 1&#10;mounts&#10;PVC&#10;Pod 1&#10;unmounts&#10;PVC&#10;Admin deletes&#10;PersistentVolume&#10;Admin creates&#10;PersistentVolume&#10;Time&#10;Figure 6.9&#10;The lifespan of a PersistentVolume, PersistentVolumeClaims, and pods using them&#10; &#10;"
    color "green"
  ]
  node [
    id 258
    label "217"
    title "Page_217"
    color "blue"
  ]
  node [
    id 259
    label "text_128"
    title "185&#10;Dynamic provisioning of PersistentVolumes&#10; Instead of the administrator pre-provisioning a bunch of PersistentVolumes, they&#10;need to define one or two (or more) StorageClasses and let the system create a new&#10;PersistentVolume each time one is requested through a PersistentVolumeClaim. The&#10;great thing about this is that it&#8217;s impossible to run out of PersistentVolumes (obviously,&#10;you can run out of storage space). &#10;6.6.1&#10;Defining the available storage types through StorageClass &#10;resources&#10;Before a user can create a PersistentVolumeClaim, which will result in a new Persistent-&#10;Volume being provisioned, an admin needs to create one or more StorageClass&#10;resources. Let&#8217;s look at an example of one in the following listing.&#10;apiVersion: storage.k8s.io/v1&#10;kind: StorageClass&#10;metadata:&#10;  name: fast&#10;provisioner: kubernetes.io/gce-pd       &#10;parameters:&#10;  type: pd-ssd                     &#10;  zone: europe-west1-b             &#10;NOTE&#10;If using Minikube, deploy the file storageclass-fast-hostpath.yaml.&#10;The StorageClass resource specifies which provisioner should be used for provision-&#10;ing the PersistentVolume when a PersistentVolumeClaim requests this StorageClass.&#10;The parameters defined in the StorageClass definition are passed to the provisioner&#10;and are specific to each provisioner plugin. &#10; The StorageClass uses the Google Compute Engine (GCE) Persistent Disk (PD)&#10;provisioner, which means it can be used when Kubernetes is running in GCE. For&#10;other cloud providers, other provisioners need to be used.&#10;6.6.2&#10;Requesting the storage class in a PersistentVolumeClaim&#10;After the StorageClass resource is created, users can refer to the storage class by name&#10;in their PersistentVolumeClaims. &#10;CREATING A PVC DEFINITION REQUESTING A SPECIFIC STORAGE CLASS&#10;You can modify your mongodb-pvc to use dynamic provisioning. The following listing&#10;shows the updated YAML definition of the PVC.&#10;apiVersion: v1&#10;kind: PersistentVolumeClaim&#10;metadata:&#10;  name: mongodb-pvc &#10;Listing 6.14&#10;A StorageClass definition: storageclass-fast-gcepd.yaml&#10;Listing 6.15&#10;A PVC with dynamic provisioning: mongodb-pvc-dp.yaml&#10;The volume plugin to &#10;use for provisioning &#10;the PersistentVolume&#10;The parameters passed &#10;to the provisioner&#10; &#10;"
    color "green"
  ]
  node [
    id 260
    label "218"
    title "Page_218"
    color "blue"
  ]
  node [
    id 261
    label "text_129"
    title "186&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;spec:&#10;  storageClassName: fast     &#10;  resources:&#10;    requests:&#10;      storage: 100Mi&#10;  accessModes:&#10;    - ReadWriteOnce&#10;Apart from specifying the size and access modes, your PersistentVolumeClaim now&#10;also specifies the class of storage you want to use. When you create the claim, the&#10;PersistentVolume is created by the provisioner referenced in the fast StorageClass&#10;resource. The provisioner is used even if an existing manually provisioned Persistent-&#10;Volume matches the PersistentVolumeClaim. &#10;NOTE&#10;If you reference a non-existing storage class in a PVC, the provisioning&#10;of the PV will fail (you&#8217;ll see a ProvisioningFailed event when you use&#10;kubectl describe on the PVC).&#10;EXAMINING THE CREATED PVC AND THE DYNAMICALLY PROVISIONED PV&#10;Next you&#8217;ll create the PVC and then use kubectl get to see it:&#10;$ kubectl get pvc mongodb-pvc&#10;NAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS&#10;mongodb-pvc   Bound    pvc-1e6bc048   1Gi        RWO           fast &#10;The VOLUME column shows the PersistentVolume that&#8217;s bound to this claim (the actual&#10;name is longer than what&#8217;s shown above). You can try listing PersistentVolumes now to&#10;see that a new PV has indeed been created automatically:&#10;$ kubectl get pv&#10;NAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   &#10;mongodb-pv     1Gi       RWO,ROX      Retain         Released &#10;pvc-1e6bc048   1Gi       RWO          Delete         Bound     fast&#10;NOTE&#10;Only pertinent columns are shown.&#10;You can see the dynamically provisioned PersistentVolume. Its capacity and access&#10;modes are what you requested in the PVC. Its reclaim policy is Delete, which means&#10;the PersistentVolume will be deleted when the PVC is deleted. Beside the PV, the pro-&#10;visioner also provisioned the actual storage. Your fast StorageClass is configured to&#10;use the kubernetes.io/gce-pd provisioner, which provisions GCE Persistent Disks.&#10;You can see the disk with the following command:&#10;$ gcloud compute disks list&#10;NAME                          ZONE            SIZE_GB  TYPE         STATUS&#10;gke-kubia-dyn-pvc-1e6bc048    europe-west1-d  1        pd-ssd       READY&#10;gke-kubia-default-pool-71df   europe-west1-d  100      pd-standard  READY&#10;gke-kubia-default-pool-79cd   europe-west1-d  100      pd-standard  READY&#10;gke-kubia-default-pool-blc4   europe-west1-d  100      pd-standard  READY&#10;mongodb                       europe-west1-d  1        pd-standard  READY&#10;This PVC requests the &#10;custom storage class.&#10; &#10;"
    color "green"
  ]
  node [
    id 262
    label "219"
    title "Page_219"
    color "blue"
  ]
  node [
    id 263
    label "text_130"
    title "187&#10;Dynamic provisioning of PersistentVolumes&#10;As you can see, the first persistent disk&#8217;s name suggests it was provisioned dynamically&#10;and its type shows it&#8217;s an SSD, as specified in the storage class you created earlier. &#10;UNDERSTANDING HOW TO USE STORAGE CLASSES&#10;The cluster admin can create multiple storage classes with different performance or&#10;other characteristics. The developer then decides which one is most appropriate for&#10;each claim they create. &#10; The nice thing about StorageClasses is the fact that claims refer to them by&#10;name. The PVC definitions are therefore portable across different clusters, as long&#10;as the StorageClass names are the same across all of them. To see this portability&#10;yourself, you can try running the same example on Minikube, if you&#8217;ve been using&#10;GKE up to this point. As a cluster admin, you&#8217;ll have to create a different storage&#10;class (but with the same name). The storage class defined in the storageclass-fast-&#10;hostpath.yaml file is tailor-made for use in Minikube. Then, once you deploy the stor-&#10;age class, you as a cluster user can deploy the exact same PVC manifest and the exact&#10;same pod manifest as before. This shows how the pods and PVCs are portable across&#10;different clusters.&#10;6.6.3&#10;Dynamic provisioning without specifying a storage class&#10;As we&#8217;ve progressed through this chapter, attaching persistent storage to pods has&#10;become ever simpler. The sections in this chapter reflect how provisioning of storage&#10;has evolved from early Kubernetes versions to now. In this final section, we&#8217;ll look at&#10;the latest and simplest way of attaching a PersistentVolume to a pod. &#10;LISTING STORAGE CLASSES&#10;When you created your custom storage class called fast, you didn&#8217;t check if any exist-&#10;ing storage classes were already defined in your cluster. Why don&#8217;t you do that now?&#10;Here are the storage classes available in GKE:&#10;$ kubectl get sc&#10;NAME                 TYPE&#10;fast                 kubernetes.io/gce-pd&#10;standard (default)   kubernetes.io/gce-pd&#10;NOTE&#10;We&#8217;re using sc as shorthand for storageclass.&#10;Beside the fast storage class, which you created yourself, a standard storage class&#10;exists and is marked as default. You&#8217;ll learn what that means in a moment. Let&#8217;s list the&#10;storage classes available in Minikube, so we can compare:&#10;$ kubectl get sc&#10;NAME                 TYPE&#10;fast                 k8s.io/minikube-hostpath&#10;standard (default)   k8s.io/minikube-hostpath&#10;Again, the fast storage class was created by you and a default standard storage class&#10;exists here as well. Comparing the TYPE columns in the two listings, you see GKE is&#10; &#10;"
    color "green"
  ]
  node [
    id 264
    label "220"
    title "Page_220"
    color "blue"
  ]
  node [
    id 265
    label "text_131"
    title "188&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;using the kubernetes.io/gce-pd provisioner, whereas Minikube is using k8s.io/&#10;minikube-hostpath. &#10;EXAMINING THE DEFAULT STORAGE CLASS&#10;You&#8217;re going to use kubectl get to see more info about the standard storage class in a&#10;GKE cluster, as shown in the following listing.&#10;$ kubectl get sc standard -o yaml&#10;apiVersion: storage.k8s.io/v1&#10;kind: StorageClass&#10;metadata:&#10;  annotations:&#10;    storageclass.beta.kubernetes.io/is-default-class: &#34;true&#34;   &#10;  creationTimestamp: 2017-05-16T15:24:11Z&#10;  labels:&#10;    addonmanager.kubernetes.io/mode: EnsureExists&#10;    kubernetes.io/cluster-service: &#34;true&#34;&#10;  name: standard&#10;  resourceVersion: &#34;180&#34;&#10;  selfLink: /apis/storage.k8s.io/v1/storageclassesstandard&#10;  uid: b6498511-3a4b-11e7-ba2c-42010a840014&#10;parameters:                                    &#10;  type: pd-standard                            &#10;provisioner: kubernetes.io/gce-pd      &#10;If you look closely toward the top of the listing, the storage class definition includes an&#10;annotation, which makes this the default storage class. The default storage class is&#10;what&#8217;s used to dynamically provision a PersistentVolume if the PersistentVolumeClaim&#10;doesn&#8217;t explicitly say which storage class to use. &#10;CREATING A PERSISTENTVOLUMECLAIM WITHOUT SPECIFYING A STORAGE CLASS&#10;You can create a PVC without specifying the storageClassName attribute and (on&#10;Google Kubernetes Engine) a GCE Persistent Disk of type pd-standard will be provi-&#10;sioned for you. Try this by creating a claim from the YAML in the following listing.&#10;apiVersion: v1&#10;kind: PersistentVolumeClaim&#10;metadata:&#10;  name: mongodb-pvc2&#10;spec:                        &#10;  resources:                 &#10;    requests:                &#10;      storage: 100Mi         &#10;  accessModes:               &#10;    - ReadWriteOnce          &#10;Listing 6.16&#10;The definition of the standard storage class on GKE&#10;Listing 6.17&#10;PVC with no storage class defined: mongodb-pvc-dp-nostorageclass.yaml&#10;This annotation &#10;marks the storage &#10;class as default.&#10;The type parameter is used by the provisioner &#10;to know what type of GCE PD to create.&#10;The GCE Persistent Disk provisioner &#10;is used to provision PVs of this class.&#10;You&#8217;re not specifying &#10;the storageClassName &#10;attribute (unlike earlier &#10;examples).&#10; &#10;"
    color "green"
  ]
  node [
    id 266
    label "221"
    title "Page_221"
    color "blue"
  ]
  node [
    id 267
    label "text_132"
    title "189&#10;Dynamic provisioning of PersistentVolumes&#10;This PVC definition includes only the storage size request and the desired access&#10;modes, but no storage class. When you create the PVC, whatever storage class is&#10;marked as default will be used. You can confirm that&#8217;s the case:&#10;$ kubectl get pvc mongodb-pvc2&#10;NAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS&#10;mongodb-pvc2  Bound    pvc-95a5ec12   1Gi        RWO           standard&#10;$ kubectl get pv pvc-95a5ec12&#10;NAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   &#10;pvc-95a5ec12   1Gi       RWO          Delete         Bound     standard&#10;$ gcloud compute disks list&#10;NAME                          ZONE            SIZE_GB  TYPE         STATUS&#10;gke-kubia-dyn-pvc-95a5ec12    europe-west1-d  1        pd-standard  READY&#10;...&#10;FORCING A PERSISTENTVOLUMECLAIM TO BE BOUND TO ONE OF THE PRE-PROVISIONED &#10;PERSISTENTVOLUMES&#10;This finally brings us to why you set storageClassName to an empty string in listing 6.11&#10;(when you wanted the PVC to bind to the PV you&#8217;d provisioned manually). Let me&#10;repeat the relevant lines of that PVC definition here:&#10;kind: PersistentVolumeClaim&#10;spec:&#10;  storageClassName: &#34;&#34;       &#10;If you hadn&#8217;t set the storageClassName attribute to an empty string, the dynamic vol-&#10;ume provisioner would have provisioned a new PersistentVolume, despite there being&#10;an appropriate pre-provisioned PersistentVolume. At that point, I wanted to demon-&#10;strate how a claim gets bound to a manually pre-provisioned PersistentVolume. I didn&#8217;t&#10;want the dynamic provisioner to interfere. &#10;TIP&#10;Explicitly set storageClassName to &#34;&#34; if you want the PVC to use a pre-&#10;provisioned PersistentVolume.&#10;UNDERSTANDING THE COMPLETE PICTURE OF DYNAMIC PERSISTENTVOLUME PROVISIONING&#10;This brings us to the end of this chapter. To summarize, the best way to attach per-&#10;sistent storage to a pod is to only create the PVC (with an explicitly specified storage-&#10;ClassName if necessary) and the pod (which refers to the PVC by name). Everything&#10;else is taken care of by the dynamic PersistentVolume provisioner.&#10; To get a complete picture of the steps involved in getting a dynamically provi-&#10;sioned PersistentVolume, examine figure 6.10.&#10; &#10; &#10; &#10;Specifying an empty string as the storage class &#10;name ensures the PVC binds to a pre-provisioned &#10;PV instead of dynamically provisioning a new one.&#10; &#10;"
    color "green"
  ]
  node [
    id 268
    label "222"
    title "Page_222"
    color "blue"
  ]
  node [
    id 269
    label "text_133"
    title "190&#10;CHAPTER 6&#10;Volumes: attaching disk storage to containers&#10;6.7&#10;Summary&#10;This chapter has shown you how volumes are used to provide either temporary or per-&#10;sistent storage to a pod&#8217;s containers. You&#8217;ve learned how to&#10;&#61601;Create a multi-container pod and have the pod&#8217;s containers operate on the&#10;same files by adding a volume to the pod and mounting it in each container&#10;&#61601;Use the emptyDir volume to store temporary, non-persistent data&#10;&#61601;Use the gitRepo volume to easily populate a directory with the contents of a Git&#10;repository at pod startup&#10;&#61601;Use the hostPath volume to access files from the host node&#10;&#61601;Mount external storage in a volume to persist pod data across pod restarts&#10;&#61601;Decouple the pod from the storage infrastructure by using PersistentVolumes&#10;and PersistentVolumeClaims&#10;&#61601;Have PersistentVolumes of the desired (or the default) storage class dynami-&#10;cally provisioned for each PersistentVolumeClaim&#10;&#61601;Prevent the dynamic provisioner from interfering when you want the Persistent-&#10;VolumeClaim to be bound to a pre-provisioned PersistentVolume&#10;In the next chapter, you&#8217;ll see what mechanisms Kubernetes provides to deliver con-&#10;figuration data, secret information, and metadata about the pod and container to the&#10;processes running inside a pod. This is done with the special types of volumes we&#8217;ve&#10;mentioned in this chapter, but not yet explored.&#10;Pod&#10;Admin&#10;Volume&#10;1. Cluster admin sets up a PersistentVolume&#10;provisioner (if one&#8217;s not already deployed)&#10;2. Admin creates one or&#10;more StorageClasses&#10;and marks one as the&#10;default (it may already&#10;exist)&#10;Actual&#10;storage&#10;Persistent&#10;Volume&#10;User&#10;Persistent&#10;Volume&#10;provisioner&#10;Persistent&#10;VolumeClaim&#10;Storage&#10;Class&#10;3. User creates a PVC referencing one of the&#10;StorageClasses (or none to use the default)&#10;6. User creates a pod with&#10;a volume referencing the&#10;PVC by name&#10;4. Kubernetes looks up the&#10;StorageClass and the provisioner&#10;referenced in it and asks the provisioner&#10;to provision a new PV based on the&#10;PVC&#8217;s requested access mode and&#10;storage size and the parameters&#10;in the StorageClass&#10;5. Provisioner provisions the&#10;actual storage, creates&#10;a PersistentVolume, and&#10;binds it to the PVC&#10;Figure 6.10&#10;The complete picture of dynamic provisioning of PersistentVolumes&#10; &#10;"
    color "green"
  ]
  node [
    id 270
    label "223"
    title "Page_223"
    color "blue"
  ]
  node [
    id 271
    label "text_134"
    title "191&#10;ConfigMaps and Secrets:&#10;configuring applications&#10;Up to now you haven&#8217;t had to pass any kind of configuration data to the apps you&#8217;ve&#10;run in the exercises in this book. Because almost all apps require configuration (set-&#10;tings that differ between deployed instances, credentials for accessing external sys-&#10;tems, and so on), which shouldn&#8217;t be baked into the built app itself, let&#8217;s see how to&#10;pass configuration options to your app when running it in Kubernetes.&#10;7.1&#10;Configuring containerized applications&#10;Before we go over how to pass configuration data to apps running in Kubernetes,&#10;let&#8217;s look at how containerized applications are usually configured.&#10; If you skip the fact that you can bake the configuration into the application&#10;itself, when starting development of a new app, you usually start off by having the&#10;This chapter covers&#10;&#61601;Changing the main process of a container&#10;&#61601;Passing command-line options to the app&#10;&#61601;Setting environment variables exposed to the app&#10;&#61601;Configuring apps through ConfigMaps&#10;&#61601;Passing sensitive information through Secrets&#10; &#10;"
    color "green"
  ]
  node [
    id 272
    label "224"
    title "Page_224"
    color "blue"
  ]
  node [
    id 273
    label "text_135"
    title "192&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;app configured through command-line arguments. Then, as the list of configuration&#10;options grows, you can move the configuration into a config file. &#10; Another way of passing configuration options to an application that&#8217;s widely popu-&#10;lar in containerized applications is through environment variables. Instead of having&#10;the app read a config file or command-line arguments, the app looks up the value of a&#10;certain environment variable. The official MySQL container image, for example, uses&#10;an environment variable called MYSQL_ROOT_PASSWORD for setting the password for the&#10;root super-user account. &#10; But why are environment variables so popular in containers? Using configuration&#10;files inside Docker containers is a bit tricky, because you&#8217;d have to bake the config file&#10;into the container image itself or mount a volume containing the file into the con-&#10;tainer. Obviously, baking files into the image is similar to hardcoding configuration&#10;into the source code of the application, because it requires you to rebuild the image&#10;every time you want to change the config. Plus, everyone with access to the image can&#10;see the config, including any information that should be kept secret, such as creden-&#10;tials or encryption keys. Using a volume is better, but still requires you to make sure&#10;the file is written to the volume before the container is started. &#10; If you&#8217;ve read the previous chapter, you might think of using a gitRepo volume as&#10;a configuration source. That&#8217;s not a bad idea, because it allows you to keep the config&#10;nicely versioned and enables you to easily rollback a config change if necessary. But a&#10;simpler way allows you to put the configuration data into a top-level Kubernetes&#10;resource and store it and all the other resource definitions in the same Git repository&#10;or in any other file-based storage. The Kubernetes resource for storing configuration&#10;data is called a ConfigMap. We&#8217;ll learn how to use it in this chapter.&#10; Regardless if you&#8217;re using a ConfigMap to store configuration data or not, you can&#10;configure your apps by&#10;&#61601;Passing command-line arguments to containers&#10;&#61601;Setting custom environment variables for each container&#10;&#61601;Mounting configuration files into containers through a special type of volume&#10;We&#8217;ll go over all these options in the next few sections, but before we start, let&#8217;s look&#10;at config options from a security perspective. Though most configuration options&#10;don&#8217;t contain any sensitive information, several can. These include credentials, pri-&#10;vate encryption keys, and similar data that needs to be kept secure. This type of infor-&#10;mation needs to be handled with special care, which is why Kubernetes offers&#10;another type of first-class object called a Secret. We&#8217;ll learn about it in the last part of&#10;this chapter.&#10;7.2&#10;Passing command-line arguments to containers&#10;In all the examples so far, you&#8217;ve created containers that ran the default command&#10;defined in the container image, but Kubernetes allows overriding the command as&#10;part of the pod&#8217;s container definition when you want to run a different executable&#10; &#10;"
    color "green"
  ]
  node [
    id 274
    label "225"
    title "Page_225"
    color "blue"
  ]
  node [
    id 275
    label "text_136"
    title "193&#10;Passing command-line arguments to containers&#10;instead of the one specified in the image, or want to run it with a different set of com-&#10;mand-line arguments. We&#8217;ll look at how to do that now.&#10;7.2.1&#10;Defining the command and arguments in Docker&#10;The first thing I need to explain is that the whole command that gets executed in the&#10;container is composed of two parts: the command and the arguments. &#10;UNDERSTANDING ENTRYPOINT AND CMD&#10;In a Dockerfile, two instructions define the two parts:&#10;&#61601;&#10;ENTRYPOINT defines the executable invoked when the container is started.&#10;&#61601;&#10;CMD specifies the arguments that get passed to the ENTRYPOINT.&#10;Although you can use the CMD instruction to specify the command you want to execute&#10;when the image is run, the correct way is to do it through the ENTRYPOINT instruction&#10;and to only specify the CMD if you want to define the default arguments. The image can&#10;then be run without specifying any arguments&#10;$ docker run <image>&#10;or with additional arguments, which override whatever&#8217;s set under CMD in the Dockerfile:&#10;$ docker run <image> <arguments>&#10;UNDERSTANDING THE DIFFERENCE BETWEEN THE SHELL AND EXEC FORMS&#10;But there&#8217;s more. Both instructions support two different forms:&#10;&#61601;&#10;shell form&#8212;For example, ENTRYPOINT node app.js.&#10;&#61601;&#10;exec form&#8212;For example, ENTRYPOINT [&#34;node&#34;, &#34;app.js&#34;].&#10;The difference is whether the specified command is invoked inside a shell or not. &#10; In the kubia image you created in chapter 2, you used the exec form of the ENTRY-&#10;POINT instruction: &#10;ENTRYPOINT [&#34;node&#34;, &#34;app.js&#34;]&#10;This runs the node process directly (not inside a shell), as you can see by listing the&#10;processes running inside the container:&#10;$ docker exec 4675d ps x&#10;  PID TTY      STAT   TIME COMMAND&#10;    1 ?        Ssl    0:00 node app.js&#10;   12 ?        Rs     0:00 ps x&#10;If you&#8217;d used the shell form (ENTRYPOINT node app.js), these would have been the&#10;container&#8217;s processes:&#10;$ docker exec -it e4bad ps x&#10;  PID TTY      STAT   TIME COMMAND&#10;    1 ?        Ss     0:00 /bin/sh -c node app.js&#10; &#10;"
    color "green"
  ]
  node [
    id 276
    label "226"
    title "Page_226"
    color "blue"
  ]
  node [
    id 277
    label "text_137"
    title "194&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;    7 ?        Sl     0:00 node app.js&#10;   13 ?        Rs+    0:00 ps x&#10;As you can see, in that case, the main process (PID 1) would be the shell process&#10;instead of the node process. The node process (PID 7) would be started from that&#10;shell. The shell process is unnecessary, which is why you should always use the exec&#10;form of the ENTRYPOINT instruction.&#10;MAKING THE INTERVAL CONFIGURABLE IN YOUR FORTUNE IMAGE&#10;Let&#8217;s modify your fortune script and image so the delay interval in the loop is configu-&#10;rable. You&#8217;ll add an INTERVAL variable and initialize it with the value of the first com-&#10;mand-line argument, as shown in the following listing.&#10;#!/bin/bash&#10;trap &#34;exit&#34; SIGINT&#10;INTERVAL=$1&#10;echo Configured to generate new fortune every $INTERVAL seconds&#10;mkdir -p /var/htdocs&#10;while :&#10;do&#10;  echo $(date) Writing fortune to /var/htdocs/index.html&#10;  /usr/games/fortune > /var/htdocs/index.html&#10;  sleep $INTERVAL&#10;done&#10;You&#8217;ve added or modified the lines in bold font. Now, you&#8217;ll modify the Dockerfile so&#10;it uses the exec version of the ENTRYPOINT instruction and sets the default interval to&#10;10 seconds using the CMD instruction, as shown in the following listing.&#10;FROM ubuntu:latest&#10;RUN apt-get update ; apt-get -y install fortune&#10;ADD fortuneloop.sh /bin/fortuneloop.sh&#10;ENTRYPOINT [&#34;/bin/fortuneloop.sh&#34;]        &#10;CMD [&#34;10&#34;]                                &#10;You can now build and push the image to Docker Hub. This time, you&#8217;ll tag the image&#10;as args instead of latest:&#10;$ docker build -t docker.io/luksa/fortune:args .&#10;$ docker push docker.io/luksa/fortune:args&#10;You can test the image by running it locally with Docker:&#10;$ docker run -it docker.io/luksa/fortune:args&#10;Configured to generate new fortune every 10 seconds&#10;Fri May 19 10:39:44 UTC 2017 Writing fortune to /var/htdocs/index.html&#10;Listing 7.1&#10;Fortune script with interval configurable through argument: fortune-args/&#10;fortuneloop.sh&#10;Listing 7.2&#10;Dockerfile for the updated fortune image: fortune-args/Dockerfile&#10;The exec form of the &#10;ENTRYPOINT instruction&#10;The default argument &#10;for the executable&#10; &#10;"
    color "green"
  ]
  node [
    id 278
    label "227"
    title "Page_227"
    color "blue"
  ]
  node [
    id 279
    label "text_138"
    title "195&#10;Passing command-line arguments to containers&#10;NOTE&#10;You can stop the script with Control+C.&#10;And you can override the default sleep interval by passing it as an argument:&#10;$ docker run -it docker.io/luksa/fortune:args 15&#10;Configured to generate new fortune every 15 seconds&#10;Now that you&#8217;re sure your image honors the argument passed to it, let&#8217;s see how to use&#10;it in a pod.&#10;7.2.2&#10;Overriding the command and arguments in Kubernetes&#10;In Kubernetes, when specifying a container, you can choose to override both ENTRY-&#10;POINT and CMD. To do that, you set the properties command and args in the container&#10;specification, as shown in the following listing.&#10;kind: Pod&#10;spec:&#10;  containers:&#10;  - image: some/image&#10;    command: [&#34;/bin/command&#34;]&#10;    args: [&#34;arg1&#34;, &#34;arg2&#34;, &#34;arg3&#34;]&#10;In most cases, you&#8217;ll only set custom arguments and rarely override the command&#10;(except in general-purpose images such as busybox, which doesn&#8217;t define an ENTRY-&#10;POINT at all). &#10;NOTE&#10;The command and args fields can&#8217;t be updated after the pod is created.&#10;The two Dockerfile instructions and the equivalent pod spec fields are shown in table 7.1.&#10;RUNNING THE FORTUNE POD WITH A CUSTOM INTERVAL&#10;To run the fortune pod with a custom delay interval, you&#8217;ll copy your fortune-&#10;pod.yaml into fortune-pod-args.yaml and modify it as shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: fortune2s        &#10;Listing 7.3&#10;A pod definition specifying a custom command and arguments&#10;Table 7.1&#10;Specifying the executable and its arguments in Docker vs Kubernetes&#10;Docker&#10;Kubernetes&#10;Description&#10;ENTRYPOINT&#10;command&#10;The executable that&#8217;s executed inside the container&#10;CMD&#10;args&#10;The arguments passed to the executable&#10;Listing 7.4&#10;Passing an argument in the pod definition: fortune-pod-args.yaml&#10;You changed the &#10;pod&#8217;s name.&#10; &#10;"
    color "green"
  ]
  node [
    id 280
    label "228"
    title "Page_228"
    color "blue"
  ]
  node [
    id 281
    label "text_139"
    title "196&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;spec:&#10;  containers:&#10;  - image: luksa/fortune:args      &#10;    args: [&#34;2&#34;]                  &#10;    name: html-generator&#10;    volumeMounts:&#10;    - name: html&#10;      mountPath: /var/htdocs&#10;...&#10;You added the args array to the container definition. Try creating this pod now. The&#10;values of the array will be passed to the container as command-line arguments when it&#10;is run. &#10; The array notation used in this listing is great if you have one argument or a few. If&#10;you have several, you can also use the following notation:&#10;    args:&#10;    - foo&#10;    - bar&#10;    - &#34;15&#34;&#10;TIP&#10;You don&#8217;t need to enclose string values in quotations marks (but you&#10;must enclose numbers). &#10;Specifying arguments is one way of passing config&#10;options to your containers through command-&#10;line arguments. Next, you&#8217;ll see how to do it&#10;through environment variables.&#10;7.3&#10;Setting environment variables for &#10;a container&#10;As I&#8217;ve already mentioned, containerized appli-&#10;cations often use environment variables as a&#10;source of configuration options. Kubernetes&#10;allows you to specify a custom list of environ-&#10;ment variables for each container of a pod, as&#10;shown in figure 7.1. Although it would be use-&#10;ful to also define environment variables at the&#10;pod level and have them be inherited by its&#10;containers, no such option currently exists.&#10;NOTE&#10;Like the container&#8217;s command and&#10;arguments, the list of environment variables&#10;also cannot be updated after the pod is created.&#10;Using fortune:args &#10;instead of fortune:latest&#10;This argument makes the &#10;script generate a new fortune &#10;every two seconds.&#10;Pod&#10;Container A&#10;Environment variables&#10;FOO=BAR&#10;ABC=123&#10;Container B&#10;Environment variables&#10;FOO=FOOBAR&#10;BAR=567&#10;Figure 7.1&#10;Environment variables can &#10;be set per container.&#10; &#10;"
    color "green"
  ]
  node [
    id 282
    label "229"
    title "Page_229"
    color "blue"
  ]
  node [
    id 283
    label "text_140"
    title "197&#10;Setting environment variables for a container&#10;MAKING THE INTERVAL IN YOUR FORTUNE IMAGE CONFIGURABLE THROUGH AN ENVIRONMENT VARIABLE&#10;Let&#8217;s see how to modify your fortuneloop.sh script once again to allow it to be config-&#10;ured from an environment variable, as shown in the following listing.&#10;#!/bin/bash&#10;trap &#34;exit&#34; SIGINT&#10;echo Configured to generate new fortune every $INTERVAL seconds&#10;mkdir -p /var/htdocs&#10;while :&#10;do&#10;  echo $(date) Writing fortune to /var/htdocs/index.html&#10;  /usr/games/fortune > /var/htdocs/index.html&#10;  sleep $INTERVAL&#10;done&#10;All you had to do was remove the row where the INTERVAL variable is initialized. Because&#10;your &#8220;app&#8221; is a simple bash script, you didn&#8217;t need to do anything else. If the app was&#10;written in Java you&#8217;d use System.getenv(&#34;INTERVAL&#34;), whereas in Node.JS you&#8217;d use&#10;process.env.INTERVAL, and in Python you&#8217;d use os.environ['INTERVAL'].&#10;7.3.1&#10;Specifying environment variables in a container definition&#10;After building the new image (I&#8217;ve tagged it as luksa/fortune:env this time) and&#10;pushing it to Docker Hub, you can run it by creating a new pod, in which you pass the&#10;environment variable to the script by including it in your container definition, as&#10;shown in the following listing.&#10;kind: Pod&#10;spec:&#10; containers:&#10; - image: luksa/fortune:env&#10;   env:                        &#10;   - name: INTERVAL            &#10;     value: &#34;30&#34;               &#10;   name: html-generator&#10;...&#10;As mentioned previously, you set the environment variable inside the container defini-&#10;tion, not at the pod level. &#10;NOTE&#10;Don&#8217;t forget that in each container, Kubernetes also automatically&#10;exposes environment variables for each service in the same namespace. These&#10;environment variables are basically auto-injected configuration.&#10;Listing 7.5&#10;Fortune script with interval configurable through env var: fortune-env/&#10;fortuneloop.sh&#10;Listing 7.6&#10;Defining an environment variable in a pod: fortune-pod-env.yaml&#10;Adding a single variable to &#10;the environment variable list&#10; &#10;"
    color "green"
  ]
  node [
    id 284
    label "230"
    title "Page_230"
    color "blue"
  ]
  node [
    id 285
    label "text_141"
    title "198&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;7.3.2&#10;Referring to other environment variables in a variable&#8217;s value&#10;In the previous example, you set a fixed value for the environment variable, but you&#10;can also reference previously defined environment variables or any other existing vari-&#10;ables by using the $(VAR) syntax. If you define two environment variables, the second&#10;one can include the value of the first one as shown in the following listing.&#10;env:&#10;- name: FIRST_VAR&#10;  value: &#34;foo&#34;&#10;- name: SECOND_VAR&#10;  value: &#34;$(FIRST_VAR)bar&#34;&#10;In this case, the SECOND_VAR&#8217;s value will be &#34;foobar&#34;. Similarly, both the command and&#10;args attributes you learned about in section 7.2 can also refer to environment vari-&#10;ables like this. You&#8217;ll use this method in section 7.4.5.&#10;7.3.3&#10;Understanding the drawback of hardcoding environment &#10;variables&#10;Having values effectively hardcoded in the pod definition means you need to have&#10;separate pod definitions for your production and your development pods. To reuse&#10;the same pod definition in multiple environments, it makes sense to decouple the&#10;configuration from the pod descriptor. Luckily, you can do that using a ConfigMap&#10;resource and using it as a source for environment variable values using the valueFrom&#10;instead of the value field. You&#8217;ll learn about this next. &#10;7.4&#10;Decoupling configuration with a ConfigMap&#10;The whole point of an app&#8217;s configuration is to keep the config options that vary&#10;between environments, or change frequently, separate from the application&#8217;s source&#10;code. If you think of a pod descriptor as source code for your app (and in microservices&#10;architectures that&#8217;s what it really is, because it defines how to compose the individual&#10;components into a functioning system), it&#8217;s clear you should move the configuration&#10;out of the pod description.&#10;7.4.1&#10;Introducing ConfigMaps&#10;Kubernetes allows separating configuration options into a separate object called a&#10;ConfigMap, which is a map containing key/value pairs with the values ranging from&#10;short literals to full config files. &#10; An application doesn&#8217;t need to read the ConfigMap directly or even know that it&#10;exists. The contents of the map are instead passed to containers as either environ-&#10;ment variables or as files in a volume (see figure 7.2). And because environment&#10;Listing 7.7&#10;Referring to an environment variable inside another one&#10; &#10;"
    color "green"
  ]
  node [
    id 286
    label "231"
    title "Page_231"
    color "blue"
  ]
  node [
    id 287
    label "text_142"
    title "199&#10;Decoupling configuration with a ConfigMap&#10;variables can be referenced in command-line arguments using the $(ENV_VAR) syn-&#10;tax, you can also pass ConfigMap entries to processes as command-line arguments.&#10;Sure, the application can also read the contents of a ConfigMap directly through the&#10;Kubernetes REST API endpoint if needed, but unless you have a real need for this,&#10;you should keep your app Kubernetes-agnostic as much as possible.&#10; Regardless of how an app consumes a ConfigMap, having the config in a separate&#10;standalone object like this allows you to keep multiple manifests for ConfigMaps with&#10;the same name, each for a different environment (development, testing, QA, produc-&#10;tion, and so on). Because pods reference the ConfigMap by name, you can use a dif-&#10;ferent config in each environment while using the same pod specification across all of&#10;them (see figure 7.3).&#10;Pod&#10;Environment variables&#10;Con&#64257;gMap&#10;key1=value1&#10;key2=value2&#10;...&#10;con&#64257;gMap&#10;volume&#10;Figure 7.2&#10;Pods use ConfigMaps &#10;through environment variables and &#10;configMap volumes.&#10;Con&#64257;gMap:&#10;app-con&#64257;g&#10;Namespace: development&#10;(contains&#10;development&#10;values)&#10;Pod(s)&#10;Con&#64257;gMaps created&#10;from different manifests&#10;Pods created from the&#10;same pod manifests&#10;Namespace: production&#10;Con&#64257;gMap:&#10;app-con&#64257;g&#10;(contains&#10;production&#10;values)&#10;Pod(s)&#10;Figure 7.3&#10;Two different ConfigMaps with the same name used in different &#10;environments&#10; &#10;"
    color "green"
  ]
  node [
    id 288
    label "232"
    title "Page_232"
    color "blue"
  ]
  node [
    id 289
    label "text_143"
    title "200&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;7.4.2&#10;Creating a ConfigMap&#10;Let&#8217;s see how to use a ConfigMap in one of your pods. To start with the simplest exam-&#10;ple, you&#8217;ll first create a map with a single key and use it to fill the INTERVAL environment&#10;variable from your previous example. You&#8217;ll create the ConfigMap with the special&#10;kubectl create configmap command instead of posting a YAML with the generic&#10;kubectl create -f command. &#10;USING THE KUBECTL CREATE CONFIGMAP COMMAND&#10;You can define the map&#8217;s entries by passing literals to the kubectl command or you&#10;can create the ConfigMap from files stored on your disk. Use a simple literal first:&#10;$ kubectl create configmap fortune-config --from-literal=sleep-interval=25&#10;configmap &#34;fortune-config&#34; created&#10;NOTE&#10;ConfigMap keys must be a valid DNS subdomain (they may only con-&#10;tain alphanumeric characters, dashes, underscores, and dots). They may&#10;optionally include a leading dot.&#10;This creates a ConfigMap called fortune-config with the single-entry sleep-interval&#10;=25 (figure 7.4).&#10;ConfigMaps usually contain more than one entry. To create a ConfigMap with multi-&#10;ple literal entries, you add multiple --from-literal arguments:&#10;$ kubectl create configmap myconfigmap&#10;&#10149;  --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two&#10;Let&#8217;s inspect the YAML descriptor of the ConfigMap you created by using the kubectl&#10;get command, as shown in the following listing.&#10;$ kubectl get configmap fortune-config -o yaml&#10;apiVersion: v1&#10;data:&#10;  sleep-interval: &#34;25&#34;                      &#10;kind: ConfigMap                              &#10;metadata:&#10;  creationTimestamp: 2016-08-11T20:31:08Z&#10;  name: fortune-config                      &#10;  namespace: default&#10;  resourceVersion: &#34;910025&#34;&#10;  selfLink: /api/v1/namespaces/default/configmaps/fortune-config&#10;  uid: 88c4167e-6002-11e6-a50d-42010af00237&#10;Listing 7.8&#10;A ConfigMap definition&#10;sleep-interval&#10;25&#10;Con&#64257;gMap: fortune-con&#64257;g&#10;Figure 7.4&#10;The fortune-config &#10;ConfigMap containing a single entry&#10;The single entry &#10;in this map&#10;This descriptor &#10;describes a ConfigMap.&#10;The name of this map &#10;(you&#8217;re referencing it &#10;by this name)&#10; &#10;"
    color "green"
  ]
  node [
    id 290
    label "233"
    title "Page_233"
    color "blue"
  ]
  node [
    id 291
    label "text_144"
    title "201&#10;Decoupling configuration with a ConfigMap&#10;Nothing extraordinary. You could easily have written this YAML yourself (you wouldn&#8217;t&#10;need to specify anything but the name in the metadata section, of course) and posted&#10;it to the Kubernetes API with the well-known&#10;$ kubectl create -f fortune-config.yaml&#10;CREATING A CONFIGMAP ENTRY FROM THE CONTENTS OF A FILE&#10;ConfigMaps can also store coarse-grained config data, such as complete config files.&#10;To do this, the kubectl create configmap command also supports reading files from&#10;disk and storing them as individual entries in the ConfigMap:&#10;$ kubectl create configmap my-config --from-file=config-file.conf&#10;When you run the previous command, kubectl looks for the file config-file.conf in&#10;the directory you run kubectl in. It will then store the contents of the file under the&#10;key config-file.conf in the ConfigMap (the filename is used as the map key), but&#10;you can also specify a key manually like this:&#10;$ kubectl create configmap my-config --from-file=customkey=config-file.conf&#10;This command will store the file&#8217;s contents under the key customkey. As with literals,&#10;you can add multiple files by using the --from-file argument multiple times. &#10;CREATING A CONFIGMAP FROM FILES IN A DIRECTORY&#10;Instead of importing each file individually, you can even import all files from a file&#10;directory:&#10;$ kubectl create configmap my-config --from-file=/path/to/dir&#10;In this case, kubectl will create an individual map entry for each file in the specified&#10;directory, but only for files whose name is a valid ConfigMap key. &#10;COMBINING DIFFERENT OPTIONS&#10;When creating ConfigMaps, you can use a combination of all the options mentioned&#10;here (note that these files aren&#8217;t included in the book&#8217;s code archive&#8212;you can create&#10;them yourself if you&#8217;d like to try out the command):&#10;$ kubectl create configmap my-config  &#10;&#10149;  --from-file=foo.json                  &#10;&#10149;  --from-file=bar=foobar.conf              &#10;&#10149;  --from-file=config-opts/               &#10;&#10149;  --from-literal=some=thing    &#10;Here, you&#8217;ve created the ConfigMap from multiple sources: a whole directory, a file,&#10;another file (but stored under a custom key instead of using the filename as the key),&#10;and a literal value. Figure 7.5 shows all these sources and the resulting ConfigMap.&#10;A single file&#10;A file stored under &#10;a custom key&#10;A whole directory&#10;A literal value&#10; &#10;"
    color "green"
  ]
  node [
    id 292
    label "234"
    title "Page_234"
    color "blue"
  ]
  node [
    id 293
    label "text_145"
    title "202&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;7.4.3&#10;Passing a ConfigMap entry to a container as an environment &#10;variable&#10;How do you now get the values from this map into a pod&#8217;s container? You have three&#10;options. Let&#8217;s start with the simplest&#8212;setting an environment variable. You&#8217;ll use the&#10;valueFrom field I mentioned in section 7.3.3. The pod descriptor should look like&#10;the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;Listing 7.9&#10;Pod with env var from a config map: fortune-pod-env-configmap.yaml&#10;Con&#64257;gMap: my-con&#64257;g&#10;Key&#10;foo.json&#10;foo.json&#10;Value&#10;bar&#10;abc&#10;debug&#10;true&#10;repeat&#10;100&#10;some&#10;thing&#10;{&#10;foo: bar&#10;baz: 5&#10;}&#10;con&#64257;g-opts directory&#10;Literal&#10;some=thing&#10;{&#10;foo: bar&#10;baz: 5&#10;}&#10;--from-&#64257;le=foo.json&#10;--from-&#64257;le=con&#64257;g-opts/&#10;--from-literal=some=thing&#10;foobar.conf&#10;abc&#10;debug&#10;true&#10;repeat&#10;100&#10;--from-&#64257;le=bar=foobar.conf&#10;Figure 7.5&#10;Creating a ConfigMap from individual files, a directory, and a literal value&#10; &#10;"
    color "green"
  ]
  node [
    id 294
    label "235"
    title "Page_235"
    color "blue"
  ]
  node [
    id 295
    label "text_146"
    title "203&#10;Decoupling configuration with a ConfigMap&#10;metadata:&#10;  name: fortune-env-from-configmap&#10;spec:&#10;  containers:&#10;  - image: luksa/fortune:env&#10;    env:                             &#10;    - name: INTERVAL                 &#10;      valueFrom:                       &#10;        configMapKeyRef:               &#10;          name: fortune-config      &#10;          key: sleep-interval    &#10;...&#10;You defined an environment variable called INTERVAL and set its value to whatever is&#10;stored in the fortune-config ConfigMap under the key sleep-interval. When the&#10;process running in the html-generator container reads the INTERVAL environment&#10;variable, it will see the value 25 (shown in figure 7.6).&#10;REFERENCING NON-EXISTING CONFIGMAPS IN A POD&#10;You might wonder what happens if the referenced ConfigMap doesn&#8217;t exist when you&#10;create the pod. Kubernetes schedules the pod normally and tries to run its containers.&#10;The container referencing the non-existing ConfigMap will fail to start, but the other&#10;container will start normally. If you then create the missing ConfigMap, the failed con-&#10;tainer is started without requiring you to recreate the pod.&#10;NOTE&#10;You can also mark a reference to a ConfigMap as optional (by setting&#10;configMapKeyRef.optional: true). In that case, the container starts even if&#10;the ConfigMap doesn&#8217;t exist.&#10;This example shows you how to decouple the configuration from the pod specifica-&#10;tion. This allows you to keep all the configuration options closely together (even for&#10;multiple pods) instead of having them splattered around the pod definition (or dupli-&#10;cated across multiple pod manifests). &#10;You&#8217;re setting the environment &#10;variable called INTERVAL.&#10;Instead of setting a fixed value, you're &#10;initializing it from a ConfigMap key.&#10;The name of the ConfigMap &#10;you're referencing&#10;You're setting the variable to whatever is&#10;stored under this key in the ConfigMap.&#10;Con&#64257;gMap: fortune-con&#64257;g&#10;sleep-interval&#10;25&#10;Pod&#10;Container: web-server&#10;Container: html-generator&#10;Environment variables&#10;INTERVAL=25&#10;fortuneloop.sh&#10;process&#10;Figure 7.6&#10;Passing a ConfigMap entry as &#10;an environment variable to a container&#10; &#10;"
    color "green"
  ]
  node [
    id 296
    label "236"
    title "Page_236"
    color "blue"
  ]
  node [
    id 297
    label "text_147"
    title "204&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;7.4.4&#10;Passing all entries of a ConfigMap as environment variables &#10;at once&#10;When your ConfigMap contains more than just a few entries, it becomes tedious and&#10;error-prone to create environment variables from each entry individually. Luckily,&#10;Kubernetes version 1.6 provides a way to expose all entries of a ConfigMap as environ-&#10;ment variables. &#10; Imagine having a ConfigMap with three keys called FOO, BAR, and FOO-BAR. You can&#10;expose them all as environment variables by using the envFrom attribute, instead of&#10;env the way you did in previous examples. The following listing shows an example.&#10;spec:&#10;  containers:&#10;  - image: some-image&#10;    envFrom:                &#10;    - prefix: CONFIG_             &#10;      configMapRef:              &#10;        name: my-config-map      &#10;...&#10;As you can see, you can also specify a prefix for the environment variables (CONFIG_ in&#10;this case). This results in the following two environment variables being present inside&#10;the container: CONFIG_FOO and CONFIG_BAR. &#10;NOTE&#10;The prefix is optional, so if you omit it the environment variables will&#10;have the same name as the keys. &#10;Did you notice I said two variables, but earlier, I said the ConfigMap has three entries&#10;(FOO, BAR, and FOO-BAR)? Why is there no environment variable for the FOO-BAR&#10;ConfigMap entry?&#10; The reason is that CONFIG_FOO-BAR isn&#8217;t a valid environment variable name&#10;because it contains a dash. Kubernetes doesn&#8217;t convert the keys in any way (it doesn&#8217;t&#10;convert dashes to underscores, for example). If a ConfigMap key isn&#8217;t in the proper&#10;format, it skips the entry (but it does record an event informing you it skipped it).&#10;7.4.5&#10;Passing a ConfigMap entry as a command-line argument&#10;Now, let&#8217;s also look at how to pass values from a ConfigMap as arguments to the main&#10;process running in the container. You can&#8217;t reference ConfigMap entries directly in&#10;the pod.spec.containers.args field, but you can first initialize an environment vari-&#10;able from the ConfigMap entry and then refer to the variable inside the arguments as&#10;shown in figure 7.7.&#10; Listing 7.11 shows an example of how to do this in the YAML.&#10; &#10;Listing 7.10&#10;Pod with env vars from all entries of a ConfigMap&#10;Using envFrom instead of env&#10;All environment variables will &#10;be prefixed with CONFIG_.&#10;Referencing the ConfigMap &#10;called my-config-map&#10; &#10;"
    color "green"
  ]
  node [
    id 298
    label "237"
    title "Page_237"
    color "blue"
  ]
  node [
    id 299
    label "text_148"
    title "205&#10;Decoupling configuration with a ConfigMap&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: fortune-args-from-configmap&#10;spec:&#10;  containers:&#10;  - image: luksa/fortune:args         &#10;    env:                               &#10;    - name: INTERVAL                   &#10;      valueFrom:                       &#10;        configMapKeyRef:               &#10;          name: fortune-config         &#10;          key: sleep-interval          &#10;    args: [&#34;$(INTERVAL)&#34;]      &#10;...&#10;You defined the environment variable exactly as you did before, but then you used the&#10;$(ENV_VARIABLE_NAME) syntax to have Kubernetes inject the value of the variable into&#10;the argument. &#10;7.4.6&#10;Using a configMap volume to expose ConfigMap entries as files&#10;Passing configuration options as environment variables or command-line arguments&#10;is usually used for short variable values. A ConfigMap, as you&#8217;ve seen, can also con-&#10;tain whole config files. When you want to expose those to the container, you can use&#10;one of the special volume types I mentioned in the previous chapter, namely a&#10;configMap volume.&#10; A configMap volume will expose each entry of the ConfigMap as a file. The pro-&#10;cess running in the container can obtain the entry&#8217;s value by reading the contents of&#10;the file.&#10;Listing 7.11&#10;Using ConfigMap entries as arguments: fortune-pod-args-configmap.yaml&#10;Con&#64257;gMap: fortune-con&#64257;g&#10;sleep-interval&#10;25&#10;Pod&#10;Container: web-server&#10;Container: html-generator&#10;Environment variables&#10;INTERVAL=25&#10;fortuneloop.sh $(INTERVAL)&#10;Figure 7.7&#10;Passing a ConfigMap entry as a command-line argument&#10;Using the image that takes the &#10;interval from the first argument, &#10;not from an environment variable&#10;Defining the &#10;environment variable &#10;exactly as before&#10;Referencing the environment &#10;variable in the argument&#10; &#10;"
    color "green"
  ]
  node [
    id 300
    label "238"
    title "Page_238"
    color "blue"
  ]
  node [
    id 301
    label "text_149"
    title "206&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10; Although this method is mostly meant for passing large config files to the con-&#10;tainer, nothing prevents you from passing short single values this way. &#10;CREATING THE CONFIGMAP&#10;Instead of modifying your fortuneloop.sh script once again, you&#8217;ll now try a different&#10;example. You&#8217;ll use a config file to configure the Nginx web server running inside the&#10;fortune pod&#8217;s web-server container. Let&#8217;s say you want your Nginx server to compress&#10;responses it sends to the client. To enable compression, the config file for Nginx&#10;needs to look like the following listing.&#10;server {&#10;  listen              80;&#10;  server_name         www.kubia-example.com;&#10;  gzip on;                                       &#10;  gzip_types text/plain application/xml;         &#10;  location / {&#10;    root   /usr/share/nginx/html;&#10;    index  index.html index.htm;&#10;  }&#10;}&#10;Now delete your existing fortune-config ConfigMap with kubectl delete config-&#10;map fortune-config, so that you can replace it with a new one, which will include the&#10;Nginx config file. You&#8217;ll create the ConfigMap from files stored on your local disk. &#10; Create a new directory called configmap-files and store the Nginx config from the&#10;previous listing into configmap-files/my-nginx-config.conf. To make the ConfigMap&#10;also contain the sleep-interval entry, add a plain text file called sleep-interval to the&#10;same directory and store the number 25 in it (see figure 7.8).&#10;Now create a ConfigMap from all the files in the directory like this:&#10;$ kubectl create configmap fortune-config --from-file=configmap-files&#10;configmap &#34;fortune-config&#34; created&#10;Listing 7.12&#10;An Nginx config with enabled gzip compression: my-nginx-config.conf&#10;This enables gzip compression &#10;for plain text and XML files.&#10;con&#64257;gmap-&#64257;les/&#10;my-nginx-con&#64257;g.conf&#10;server {&#10;listen 80;&#10;server_name www.kubia...&#10;...&#10;}&#10;sleep-interval&#10;25&#10;Figure 7.8&#10;The contents of the &#10;configmap-files directory and its files&#10; &#10;"
    color "green"
  ]
  node [
    id 302
    label "239"
    title "Page_239"
    color "blue"
  ]
  node [
    id 303
    label "text_150"
    title "207&#10;Decoupling configuration with a ConfigMap&#10;The following listing shows what the YAML of this ConfigMap looks like.&#10;$ kubectl get configmap fortune-config -o yaml&#10;apiVersion: v1&#10;data:&#10;  my-nginx-config.conf: |                            &#10;    server {                                         &#10;      listen              80;                        &#10;      server_name         www.kubia-example.com;     &#10;      gzip on;                                       &#10;      gzip_types text/plain application/xml;         &#10;      location / {                                   &#10;        root   /usr/share/nginx/html;                &#10;        index  index.html index.htm;                 &#10;      }                                              &#10;    }                                                &#10;  sleep-interval: |         &#10;    25                      &#10;kind: ConfigMap&#10;...&#10;NOTE&#10;The pipeline character after the colon in the first line of both entries&#10;signals that a literal multi-line value follows.&#10;The ConfigMap contains two entries, with keys corresponding to the actual names&#10;of the files they were created from. You&#8217;ll now use the ConfigMap in both of your&#10;pod&#8217;s containers.&#10;USING THE CONFIGMAP'S ENTRIES IN A VOLUME&#10;Creating a volume populated with the contents of a ConfigMap is as easy as creating&#10;a volume that references the ConfigMap by name and mounting the volume in a&#10;container. You already learned how to create volumes and mount them, so the only&#10;thing left to learn is how to initialize the volume with files created from a Config-&#10;Map&#8217;s entries.&#10; Nginx reads its config file from /etc/nginx/nginx.conf. The Nginx image&#10;already contains this file with default configuration options, which you don&#8217;t want&#10;to override, so you don&#8217;t want to replace this file as a whole. Luckily, the default&#10;config file automatically includes all .conf files in the /etc/nginx/conf.d/ subdirec-&#10;tory as well, so you should add your config file in there. Figure 7.9 shows what you&#10;want to achieve.&#10; The pod descriptor is shown in listing 7.14 (the irrelevant parts are omitted, but&#10;you&#8217;ll find the complete file in the code archive).&#10; &#10; &#10;Listing 7.13&#10;YAML definition of a config map created from a file&#10;The entry holding the &#10;Nginx config file&#8217;s &#10;contents&#10;The sleep-interval entry&#10; &#10;"
    color "green"
  ]
  node [
    id 304
    label "240"
    title "Page_240"
    color "blue"
  ]
  node [
    id 305
    label "text_151"
    title "208&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: fortune-configmap-volume&#10;spec:&#10;  containers:&#10;  - image: nginx:alpine&#10;    name: web-server&#10;    volumeMounts:&#10;    ...&#10;    - name: config&#10;      mountPath: /etc/nginx/conf.d      &#10;      readOnly: true&#10;    ...&#10;  volumes:&#10;  ...&#10;  - name: config              &#10;    configMap:                 &#10;      name: fortune-config     &#10;  ...&#10;This pod definition includes a volume, which references your fortune-config&#10;ConfigMap. You mount the volume into the /etc/nginx/conf.d directory to make&#10;Nginx use it. &#10;VERIFYING NGINX IS USING THE MOUNTED CONFIG FILE&#10;The web server should now be configured to compress the responses it sends. You can&#10;verify this by enabling port-forwarding from localhost:8080 to the pod&#8217;s port 80 and&#10;checking the server&#8217;s response with curl, as shown in the following listing.&#10; &#10;Listing 7.14&#10;A pod with ConfigMap entries mounted as files: fortune-pod-configmap-&#10;volume.yaml&#10;Pod&#10;Container: html-generator&#10;Container: web-server&#10;Filesystem&#10;/&#10;etc/&#10;nginx/&#10;conf.d/&#10;Con&#64257;gMap: fortune-con&#64257;g&#10;my-nginx-con&#64257;g.conf&#10;server {&#10;&#8230;&#10;}&#10;Volume:&#10;con&#64257;g&#10;Figure 7.9&#10;Passing ConfigMap entries to a pod as files in a volume&#10;You&#8217;re mounting the &#10;configMap volume at &#10;this location.&#10;The volume refers to your &#10;fortune-config ConfigMap.&#10; &#10;"
    color "green"
  ]
  node [
    id 306
    label "241"
    title "Page_241"
    color "blue"
  ]
  node [
    id 307
    label "text_152"
    title "209&#10;Decoupling configuration with a ConfigMap&#10;$ kubectl port-forward fortune-configmap-volume 8080:80 &#38;&#10;Forwarding from 127.0.0.1:8080 -> 80&#10;Forwarding from [::1]:8080 -> 80&#10;$ curl -H &#34;Accept-Encoding: gzip&#34; -I localhost:8080&#10;HTTP/1.1 200 OK&#10;Server: nginx/1.11.1&#10;Date: Thu, 18 Aug 2016 11:52:57 GMT&#10;Content-Type: text/html&#10;Last-Modified: Thu, 18 Aug 2016 11:52:55 GMT&#10;Connection: keep-alive&#10;ETag: W/&#34;57b5a197-37&#34;&#10;Content-Encoding: gzip           &#10;EXAMINING THE MOUNTED CONFIGMAP VOLUME&#8217;S CONTENTS&#10;The response shows you achieved what you wanted, but let&#8217;s look at what&#8217;s in the&#10;/etc/nginx/conf.d directory now:&#10;$ kubectl exec fortune-configmap-volume -c web-server ls /etc/nginx/conf.d&#10;my-nginx-config.conf&#10;sleep-interval&#10;Both entries from the ConfigMap have been added as files to the directory. The&#10;sleep-interval entry is also included, although it has no business being there,&#10;because it&#8217;s only meant to be used by the fortuneloop container. You could create&#10;two different ConfigMaps and use one to configure the fortuneloop container and&#10;the other one to configure the web-server container. But somehow it feels wrong to&#10;use multiple ConfigMaps to configure containers of the same pod. After all, having&#10;containers in the same pod implies that the containers are closely related and should&#10;probably also be configured as a unit. &#10;EXPOSING CERTAIN CONFIGMAP ENTRIES IN THE VOLUME&#10;Luckily, you can populate a configMap volume with only part of the ConfigMap&#8217;s&#10;entries&#8212;in your case, only the my-nginx-config.conf entry. This won&#8217;t affect the&#10;fortuneloop container, because you&#8217;re passing the sleep-interval entry to it through&#10;an environment variable and not through the volume. &#10; To define which entries should be exposed as files in a configMap volume, use the&#10;volume&#8217;s items attribute as shown in the following listing.&#10;  volumes:&#10;  - name: config              &#10;    configMap:                                  &#10;      name: fortune-config                      &#10;      items:                       &#10;      - key: my-nginx-config.conf        &#10;        path: gzip.conf                  &#10;Listing 7.15&#10;Seeing if nginx responses have compression enabled&#10;Listing 7.16&#10;A pod with a specific ConfigMap entry mounted into a file directory: &#10;fortune-pod-configmap-volume-with-items.yaml&#10;This shows the response &#10;is compressed.&#10;Selecting which entries to include &#10;in the volume by listing them&#10;You want the entry &#10;under this key included.&#10;The entry&#8217;s value should &#10;be stored in this file.&#10; &#10;"
    color "green"
  ]
  node [
    id 308
    label "242"
    title "Page_242"
    color "blue"
  ]
  node [
    id 309
    label "text_153"
    title "210&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;When specifying individual entries, you need to set the filename for each individual&#10;entry, along with the entry&#8217;s key. If you run the pod from the previous listing, the&#10;/etc/nginx/conf.d directory is kept nice and clean, because it only contains the&#10;gzip.conf file and nothing else. &#10;UNDERSTANDING THAT MOUNTING A DIRECTORY HIDES EXISTING FILES IN THAT DIRECTORY&#10;There&#8217;s one important thing to discuss at this point. In both this and in your previous&#10;example, you mounted the volume as a directory, which means you&#8217;ve hidden any files&#10;that are stored in the /etc/nginx/conf.d directory in the container image itself. &#10; This is generally what happens in Linux when you mount a filesystem into a non-&#10;empty directory. The directory then only contains the files from the mounted filesys-&#10;tem, whereas the original files in that directory are inaccessible for as long as the&#10;filesystem is mounted. &#10; In your case, this has no terrible side effects, but imagine mounting a volume to&#10;the /etc directory, which usually contains many important files. This would most likely&#10;break the whole container, because all of the original files that should be in the /etc&#10;directory would no longer be there. If you need to add a file to a directory like /etc,&#10;you can&#8217;t use this method at all.&#10;MOUNTING INDIVIDUAL CONFIGMAP ENTRIES AS FILES WITHOUT HIDING OTHER FILES IN THE DIRECTORY&#10;Naturally, you&#8217;re now wondering how to add individual files from a ConfigMap into&#10;an existing directory without hiding existing files stored in it. An additional subPath&#10;property on the volumeMount allows you to mount either a single file or a single direc-&#10;tory from the volume instead of mounting the whole volume. Perhaps this is easier to&#10;explain visually (see figure 7.10).&#10; Say you have a configMap volume containing a myconfig.conf file, which you want&#10;to add to the /etc directory as someconfig.conf. You can use the subPath property to&#10;mount it there without affecting any other files in that directory. The relevant part of&#10;the pod definition is shown in the following listing.&#10;Pod&#10;Container&#10;Filesystem&#10;/&#10;etc/&#10;somecon&#64257;g.conf&#10;existing&#64257;le1&#10;existing&#64257;le2&#10;Con&#64257;gMap: app-con&#64257;g&#10;mycon&#64257;g.conf&#10;Contents&#10;of the &#64257;le&#10;another-&#64257;le&#10;Contents&#10;of the &#64257;le&#10;con&#64257;gMap&#10;volume&#10;mycon&#64257;g.conf&#10;another-&#64257;le&#10;existing&#64257;le1&#10;and existing&#64257;le2&#10;aren&#8217;t hidden.&#10;Only mycon&#64257;g.conf is mounted&#10;into the container (yet under a&#10;different &#64257;lename).&#10;another-&#64257;le isn&#8217;t&#10;mounted into the&#10;container.&#10;Figure 7.10&#10;Mounting a single file from a volume&#10; &#10;"
    color "green"
  ]
  node [
    id 310
    label "243"
    title "Page_243"
    color "blue"
  ]
  node [
    id 311
    label "text_154"
    title "211&#10;Decoupling configuration with a ConfigMap&#10;spec:&#10;  containers:&#10;  - image: some/image&#10;    volumeMounts:&#10;    - name: myvolume&#10;      mountPath: /etc/someconfig.conf     &#10;      subPath: myconfig.conf            &#10;The subPath property can be used when mounting any kind of volume. Instead of&#10;mounting the whole volume, you can mount part of it. But this method of mounting&#10;individual files has a relatively big deficiency related to updating files. You&#8217;ll learn&#10;more about this in the following section, but first, let&#8217;s finish talking about the initial&#10;state of a configMap volume by saying a few words about file permissions.&#10;SETTING THE FILE PERMISSIONS FOR FILES IN A CONFIGMAP VOLUME&#10;By default, the permissions on all files in a configMap volume are set to 644 (-rw-r&#8212;r--).&#10;You can change this by setting the defaultMode property in the volume spec, as shown&#10;in the following listing.&#10;  volumes:&#10;  - name: config&#10;    configMap:&#10;      name: fortune-config&#10;      defaultMode: &#34;6600&#34;       &#10;Although ConfigMaps should be used for non-sensitive configuration data, you may&#10;want to make the file readable and writable only to the user and group the file is&#10;owned by, as the example in the previous listing shows. &#10;7.4.7&#10;Updating an app&#8217;s config without having to restart the app&#10;We&#8217;ve said that one of the drawbacks of using environment variables or command-line&#10;arguments as a configuration source is the inability to update them while the pro-&#10;cess is running. Using a ConfigMap and exposing it through a volume brings the&#10;ability to update the configuration without having to recreate the pod or even restart&#10;the container. &#10; When you update a ConfigMap, the files in all the volumes referencing it are&#10;updated. It&#8217;s then up to the process to detect that they&#8217;ve been changed and reload&#10;them. But Kubernetes will most likely eventually also support sending a signal to the&#10;container after updating the files.&#10;WARNING&#10;Be aware that as I&#8217;m writing this, it takes a surprisingly long time&#10;for the files to be updated after you update the ConfigMap (it can take up to&#10;one whole minute).&#10;Listing 7.17&#10;A pod with a specific config map entry mounted into a specific file&#10;Listing 7.18&#10;Setting file permissions: fortune-pod-configmap-volume-defaultMode.yaml &#10;You&#8217;re mounting into &#10;a file, not a directory.&#10;Instead of mounting the whole &#10;volume, you&#8217;re only mounting &#10;the myconfig.conf entry.&#10;This sets the permissions &#10;for all files to -rw-rw------.&#10; &#10;"
    color "green"
  ]
  node [
    id 312
    label "244"
    title "Page_244"
    color "blue"
  ]
  node [
    id 313
    label "text_155"
    title "212&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;EDITING A CONFIGMAP&#10;Let&#8217;s see how you can change a ConfigMap and have the process running in the pod&#10;reload the files exposed in the configMap volume. You&#8217;ll modify the Nginx config file&#10;from your previous example and make Nginx use the new config without restarting&#10;the pod. Try switching gzip compression off by editing the fortune-config Config-&#10;Map with kubectl edit:&#10;$ kubectl edit configmap fortune-config&#10;Once your editor opens, change the gzip on line to gzip off, save the file, and then&#10;close the editor. The ConfigMap is then updated, and soon afterward, the actual file&#10;in the volume is updated as well. You can confirm this by printing the contents of the&#10;file with kubectl exec:&#10;$ kubectl exec fortune-configmap-volume -c web-server&#10;&#10149;  cat /etc/nginx/conf.d/my-nginx-config.conf&#10;If you don&#8217;t see the update yet, wait a while and try again. It takes a while for the&#10;files to get updated. Eventually, you&#8217;ll see the change in the config file, but you&#8217;ll&#10;find this has no effect on Nginx, because it doesn&#8217;t watch the files and reload them&#10;automatically. &#10;SIGNALING NGINX TO RELOAD THE CONFIG&#10;Nginx will continue to compress its responses until you tell it to reload its config files,&#10;which you can do with the following command:&#10;$ kubectl exec fortune-configmap-volume -c web-server -- nginx -s reload&#10;Now, if you try hitting the server again with curl, you should see the response is no&#10;longer compressed (it no longer contains the Content-Encoding: gzip header).&#10;You&#8217;ve effectively changed the app&#8217;s config without having to restart the container or&#10;recreate the pod. &#10;UNDERSTANDING HOW THE FILES ARE UPDATED ATOMICALLY&#10;You may wonder what happens if an app can detect config file changes on its own and&#10;reloads them before Kubernetes has finished updating all the files in the configMap&#10;volume. Luckily, this can&#8217;t happen, because all the files are updated atomically, which&#10;means all updates occur at once. Kubernetes achieves this by using symbolic links. If&#10;you list all the files in the mounted configMap volume, you&#8217;ll see something like the&#10;following listing.&#10;$ kubectl exec -it fortune-configmap-volume -c web-server -- ls -lA &#10;&#10149;  /etc/nginx/conf.d&#10;total 4&#10;drwxr-xr-x  ... 12:15 ..4984_09_04_12_15_06.865837643&#10;Listing 7.19&#10;Files in a mounted configMap volume&#10; &#10;"
    color "green"
  ]
  node [
    id 314
    label "245"
    title "Page_245"
    color "blue"
  ]
  node [
    id 315
    label "text_156"
    title "213&#10;Using Secrets to pass sensitive data to containers&#10;lrwxrwxrwx  ... 12:15 ..data -> ..4984_09_04_12_15_06.865837643&#10;lrwxrwxrwx  ... 12:15 my-nginx-config.conf -> ..data/my-nginx-config.conf&#10;lrwxrwxrwx  ... 12:15 sleep-interval -> ..data/sleep-interval&#10;As you can see, the files in the mounted configMap volume are symbolic links point-&#10;ing to files in the ..data dir. The ..data dir is also a symbolic link pointing to a direc-&#10;tory called ..4984_09_04_something. When the ConfigMap is updated, Kubernetes&#10;creates a new directory like this, writes all the files to it, and then re-links the ..data&#10;symbolic link to the new directory, effectively changing all files at once.&#10;UNDERSTANDING THAT FILES MOUNTED INTO EXISTING DIRECTORIES DON&#8217;T GET UPDATED&#10;One big caveat relates to updating ConfigMap-backed volumes. If you&#8217;ve mounted a&#10;single file in the container instead of the whole volume, the file will not be updated!&#10;At least, this is true at the time of writing this chapter. &#10; For now, if you need to add an individual file and have it updated when you update&#10;its source ConfigMap, one workaround is to mount the whole volume into a different&#10;directory and then create a symbolic link pointing to the file in question. The sym-&#10;link can either be created in the container image itself, or you could create the&#10;symlink when the container starts.&#10;UNDERSTANDING THE CONSEQUENCES OF UPDATING A CONFIGMAP&#10;One of the most important features of containers is their immutability, which allows&#10;us to be certain that no differences exist between multiple running containers created&#10;from the same image, so is it wrong to bypass this immutability by modifying a Config-&#10;Map used by running containers? &#10; The main problem occurs when the app doesn&#8217;t support reloading its configura-&#10;tion. This results in different running instances being configured differently&#8212;those&#10;pods that are created after the ConfigMap is changed will use the new config, whereas&#10;the old pods will still use the old one. And this isn&#8217;t limited to new pods. If a pod&#8217;s con-&#10;tainer is restarted (for whatever reason), the new process will also see the new config.&#10;Therefore, if the app doesn&#8217;t reload its config automatically, modifying an existing&#10;ConfigMap (while pods are using it) may not be a good idea. &#10; If the app does support reloading, modifying the ConfigMap usually isn&#8217;t such a&#10;big deal, but you do need to be aware that because files in the ConfigMap volumes&#10;aren&#8217;t updated synchronously across all running instances, the files in individual pods&#10;may be out of sync for up to a whole minute.&#10;7.5&#10;Using Secrets to pass sensitive data to containers&#10;All the information you&#8217;ve passed to your containers so far is regular, non-sensitive&#10;configuration data that doesn&#8217;t need to be kept secure. But as we mentioned at the&#10;start of the chapter, the config usually also includes sensitive information, such as cre-&#10;dentials and private encryption keys, which need to be kept secure.&#10; &#10;"
    color "green"
  ]
  node [
    id 316
    label "246"
    title "Page_246"
    color "blue"
  ]
  node [
    id 317
    label "text_157"
    title "214&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;7.5.1&#10;Introducing Secrets&#10;To store and distribute such information, Kubernetes provides a separate object called&#10;a Secret. Secrets are much like ConfigMaps&#8212;they&#8217;re also maps that hold key-value&#10;pairs. They can be used the same way as a ConfigMap. You can&#10;&#61601;Pass Secret entries to the container as environment variables&#10;&#61601;Expose Secret entries as files in a volume&#10;Kubernetes helps keep your Secrets safe by making sure each Secret is only distributed&#10;to the nodes that run the pods that need access to the Secret. Also, on the nodes&#10;themselves, Secrets are always stored in memory and never written to physical storage,&#10;which would require wiping the disks after deleting the Secrets from them. &#10; On the master node itself (more specifically in etcd), Secrets used to be stored in&#10;unencrypted form, which meant the master node needs to be secured to keep the sensi-&#10;tive data stored in Secrets secure. This didn&#8217;t only include keeping the etcd storage&#10;secure, but also preventing unauthorized users from using the API server, because any-&#10;one who can create pods can mount the Secret into the pod and gain access to the sen-&#10;sitive data through it. From Kubernetes version 1.7, etcd stores Secrets in encrypted&#10;form, making the system much more secure. Because of this, it&#8217;s imperative you prop-&#10;erly choose when to use a Secret or a ConfigMap. Choosing between them is simple:&#10;&#61601;Use a ConfigMap to store non-sensitive, plain configuration data.&#10;&#61601;Use a Secret to store any data that is sensitive in nature and needs to be kept&#10;under key. If a config file includes both sensitive and not-sensitive data, you&#10;should store the file in a Secret.&#10;You already used Secrets in chapter 5, when you created a Secret to hold the TLS certifi-&#10;cate needed for the Ingress resource. Now you&#8217;ll explore Secrets in more detail.&#10;7.5.2&#10;Introducing the default token Secret&#10;You&#8217;ll start learning about Secrets by examining a Secret that&#8217;s mounted into every&#10;container you run. You may have noticed it when using kubectl describe on a pod.&#10;The command&#8217;s output has always contained something like this:&#10;Volumes:&#10;  default-token-cfee9:&#10;    Type:       Secret (a volume populated by a Secret)&#10;    SecretName: default-token-cfee9&#10;Every pod has a secret volume attached to it automatically. The volume in the previ-&#10;ous kubectl describe output refers to a Secret called default-token-cfee9. Because&#10;Secrets are resources, you can list them with kubectl get secrets and find the&#10;default-token Secret in that list. Let&#8217;s see:&#10;$ kubectl get secrets&#10;NAME                  TYPE                                  DATA      AGE&#10;default-token-cfee9   kubernetes.io/service-account-token   3         39d&#10; &#10;"
    color "green"
  ]
  node [
    id 318
    label "247"
    title "Page_247"
    color "blue"
  ]
  node [
    id 319
    label "text_158"
    title "215&#10;Using Secrets to pass sensitive data to containers&#10;You can also use kubectl describe to learn a bit more about it, as shown in the follow-&#10;ing listing.&#10;$ kubectl describe secrets&#10;Name:        default-token-cfee9&#10;Namespace:   default&#10;Labels:      <none>&#10;Annotations: kubernetes.io/service-account.name=default&#10;             kubernetes.io/service-account.uid=cc04bb39-b53f-42010af00237&#10;Type:        kubernetes.io/service-account-token&#10;Data&#10;====&#10;ca.crt:      1139 bytes                                   &#10;namespace:   7 bytes                                      &#10;token:       eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...      &#10;You can see that the Secret contains three entries&#8212;ca.crt, namespace, and token&#8212;&#10;which represent everything you need to securely talk to the Kubernetes API server&#10;from within your pods, should you need to do that. Although ideally you want your&#10;application to be completely Kubernetes-agnostic, when there&#8217;s no alternative other&#10;than to talk to Kubernetes directly, you&#8217;ll use the files provided through this secret&#10;volume. &#10; The kubectl describe pod command shows where the secret volume is mounted:&#10;Mounts:&#10;  /var/run/secrets/kubernetes.io/serviceaccount from default-token-cfee9&#10;NOTE&#10;By default, the default-token Secret is mounted into every container,&#10;but you can disable that in each pod by setting the automountService-&#10;AccountToken field in the pod spec to false or by setting it to false on the&#10;service account the pod is using. (You&#8217;ll learn about service accounts later in&#10;the book.)&#10;To help you visualize where and how the default token Secret is mounted, see fig-&#10;ure 7.11.&#10; We&#8217;ve said Secrets are like ConfigMaps, so because this Secret contains three&#10;entries, you can expect to see three files in the directory the secret volume is mounted&#10;into. You can check this easily with kubectl exec:&#10;$ kubectl exec mypod ls /var/run/secrets/kubernetes.io/serviceaccount/&#10;ca.crt&#10;namespace&#10;token&#10;You&#8217;ll see how your app can use these files to access the API server in the next chapter.&#10;Listing 7.20&#10;Describing a Secret&#10;This secret &#10;contains three &#10;entries.&#10; &#10;"
    color "green"
  ]
  node [
    id 320
    label "248"
    title "Page_248"
    color "blue"
  ]
  node [
    id 321
    label "text_159"
    title "216&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;7.5.3&#10;Creating a Secret&#10;Now, you&#8217;ll create your own little Secret. You&#8217;ll improve your fortune-serving Nginx&#10;container by configuring it to also serve HTTPS traffic. For this, you need to create a&#10;certificate and a private key. The private key needs to be kept secure, so you&#8217;ll put it&#10;and the certificate into a Secret.&#10; First, generate the certificate and private key files (do this on your local machine).&#10;You can also use the files in the book&#8217;s code archive (the cert and key files are in the&#10;fortune-https directory):&#10;$ openssl genrsa -out https.key 2048&#10;$ openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj &#10;/CN=www.kubia-example.com&#10;Now, to help better demonstrate a few things about Secrets, create an additional&#10;dummy file called foo and make it contain the string bar. You&#8217;ll understand why you&#10;need to do this in a moment or two:&#10;$ echo bar > foo&#10;Now you can use kubectl create secret to create a Secret from the three files:&#10;$ kubectl create secret generic fortune-https --from-file=https.key&#10;&#10149;  --from-file=https.cert --from-file=foo&#10;secret &#34;fortune-https&#34; created&#10;This isn&#8217;t very different from creating ConfigMaps. In this case, you&#8217;re creating a&#10;generic Secret called fortune-https and including two entries in it (https.key with&#10;the contents of the https.key file and likewise for the https.cert key/file). As you&#10;learned earlier, you could also include the whole directory with --from-file=fortune-&#10;https instead of specifying each file individually.&#10;Pod&#10;Container&#10;Filesystem&#10;/&#10;var/&#10;run/&#10;secrets/&#10;kubernetes.io/&#10;serviceaccount/&#10;Default token Secret&#10;Default token&#10;secret&#10;volume&#10;ca.crt&#10;...&#10;...&#10;...&#10;namespace&#10;token&#10;Figure 7.11&#10;The default-token Secret is created automatically and a corresponding &#10;volume is mounted in each pod automatically.&#10; &#10;"
    color "green"
  ]
  node [
    id 322
    label "249"
    title "Page_249"
    color "blue"
  ]
  node [
    id 323
    label "text_160"
    title "217&#10;Using Secrets to pass sensitive data to containers&#10;NOTE&#10;You&#8217;re creating a generic Secret, but you could also have created a tls&#10;Secret with the kubectl create secret tls command, as you did in chapter 5.&#10;This would create the Secret with different entry names, though.&#10;7.5.4&#10;Comparing ConfigMaps and Secrets&#10;Secrets and ConfigMaps have a pretty big difference. This is what drove Kubernetes&#10;developers to create ConfigMaps after Kubernetes had already supported Secrets for a&#10;while. The following listing shows the YAML of the Secret you created.&#10;$ kubectl get secret fortune-https -o yaml&#10;apiVersion: v1&#10;data:&#10;  foo: YmFyCg==&#10;  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...&#10;  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...&#10;kind: Secret&#10;...&#10;Now compare this to the YAML of the ConfigMap you created earlier, which is shown&#10;in the following listing.&#10;$ kubectl get configmap fortune-config -o yaml&#10;apiVersion: v1&#10;data:&#10;  my-nginx-config.conf: |&#10;    server {&#10;      ...&#10;    }&#10;  sleep-interval: |&#10;    25&#10;kind: ConfigMap&#10;...&#10;Notice the difference? The contents of a Secret&#8217;s entries are shown as Base64-encoded&#10;strings, whereas those of a ConfigMap are shown in clear text. This initially made&#10;working with Secrets in YAML and JSON manifests a bit more painful, because you&#10;had to encode and decode them when setting and reading their entries. &#10;USING SECRETS FOR BINARY DATA&#10;The reason for using Base64 encoding is simple. A Secret&#8217;s entries can contain binary&#10;values, not only plain-text. Base64 encoding allows you to include the binary data in&#10;YAML or JSON, which are both plain-text formats. &#10;TIP&#10;You can use Secrets even for non-sensitive binary data, but be aware that&#10;the maximum size of a Secret is limited to 1MB.&#10;Listing 7.21&#10;A Secret&#8217;s YAML definition&#10;Listing 7.22&#10;A ConfigMap&#8217;s YAML definition&#10; &#10;"
    color "green"
  ]
  node [
    id 324
    label "250"
    title "Page_250"
    color "blue"
  ]
  node [
    id 325
    label "text_161"
    title "218&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;INTRODUCING THE STRINGDATA FIELD&#10;Because not all sensitive data is in binary form, Kubernetes also allows setting a Secret&#8217;s&#10;values through the stringData field. The following listing shows how it&#8217;s used.&#10;kind: Secret&#10;apiVersion: v1&#10;stringData:           &#10;  foo: plain text      &#10;data:&#10;  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...&#10;  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...&#10;The stringData field is write-only (note: write-only, not read-only). It can only be&#10;used to set values. When you retrieve the Secret&#8217;s YAML with kubectl get -o yaml, the&#10;stringData field will not be shown. Instead, all entries you specified in the string-&#10;Data field (such as the foo entry in the previous example) will be shown under data&#10;and will be Base64-encoded like all the other entries. &#10;READING A SECRET&#8217;S ENTRY IN A POD&#10;When you expose the Secret to a container through a secret volume, the value of the&#10;Secret entry is decoded and written to the file in its actual form (regardless if it&#8217;s plain&#10;text or binary). The same is also true when exposing the Secret entry through an envi-&#10;ronment variable. In both cases, the app doesn&#8217;t need to decode it, but can read the&#10;file&#8217;s contents or look up the environment variable value and use it directly.&#10;7.5.5&#10;Using the Secret in a pod&#10;With your fortune-https Secret containing both the cert and key files, all you need to&#10;do now is configure Nginx to use them. &#10;MODIFYING THE FORTUNE-CONFIG CONFIGMAP TO ENABLE HTTPS&#10;For this, you need to modify the config file again by editing the ConfigMap:&#10;$ kubectl edit configmap fortune-config&#10;After the text editor opens, modify the part that defines the contents of the my-nginx-&#10;config.conf entry so it looks like the following listing.&#10;...&#10;data:&#10;  my-nginx-config.conf: |&#10;    server {&#10;      listen              80;&#10;      listen              443 ssl;&#10;      server_name         www.kubia-example.com;&#10;Listing 7.23&#10;Adding plain text entries to a Secret using the stringData field&#10;Listing 7.24&#10;Modifying the fortune-config ConfigMap&#8217;s data&#10;The stringData can be used &#10;for non-binary Secret data.&#10;See, &#8220;plain text&#8221; is not Base64-encoded.&#10; &#10;"
    color "green"
  ]
  node [
    id 326
    label "251"
    title "Page_251"
    color "blue"
  ]
  node [
    id 327
    label "text_162"
    title "219&#10;Using Secrets to pass sensitive data to containers&#10;      ssl_certificate     certs/https.cert;           &#10;      ssl_certificate_key certs/https.key;            &#10;      ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;&#10;      ssl_ciphers         HIGH:!aNULL:!MD5;&#10;      location / {&#10;        root   /usr/share/nginx/html;&#10;        index  index.html index.htm;&#10;      }&#10;    }&#10;  sleep-interval: |&#10;...&#10;This configures the server to read the certificate and key files from /etc/nginx/certs,&#10;so you&#8217;ll need to mount the secret volume there. &#10;MOUNTING THE FORTUNE-HTTPS SECRET IN A POD&#10;Next, you&#8217;ll create a new fortune-https pod and mount the secret volume holding&#10;the certificate and key into the proper location in the web-server container, as shown&#10;in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: fortune-https&#10;spec:&#10;  containers:&#10;  - image: luksa/fortune:env&#10;    name: html-generator&#10;    env:&#10;    - name: INTERVAL&#10;      valueFrom: &#10;        configMapKeyRef:&#10;          name: fortune-config&#10;          key: sleep-interval&#10;    volumeMounts:&#10;    - name: html&#10;      mountPath: /var/htdocs&#10;  - image: nginx:alpine&#10;    name: web-server&#10;    volumeMounts:&#10;    - name: html&#10;      mountPath: /usr/share/nginx/html&#10;      readOnly: true&#10;    - name: config&#10;      mountPath: /etc/nginx/conf.d&#10;      readOnly: true&#10;    - name: certs                         &#10;      mountPath: /etc/nginx/certs/        &#10;      readOnly: true                      &#10;    ports:&#10;    - containerPort: 80&#10;Listing 7.25&#10;YAML definition of the fortune-https pod: fortune-pod-https.yaml&#10;The paths are &#10;relative to /etc/nginx.&#10;You configured Nginx to read the cert and &#10;key file from /etc/nginx/certs, so you need &#10;to mount the Secret volume there.&#10; &#10;"
    color "green"
  ]
  node [
    id 328
    label "252"
    title "Page_252"
    color "blue"
  ]
  node [
    id 329
    label "text_163"
    title "220&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;    - containerPort: 443&#10;  volumes:&#10;  - name: html&#10;    emptyDir: {}&#10;  - name: config&#10;    configMap:&#10;      name: fortune-config&#10;      items:&#10;      - key: my-nginx-config.conf&#10;        path: https.conf&#10;  - name: certs                            &#10;    secret:                                &#10;      secretName: fortune-https            &#10;Much is going on in this pod descriptor, so let me help you visualize it. Figure 7.12&#10;shows the components defined in the YAML. The default-token Secret, volume, and&#10;volume mount, which aren&#8217;t part of the YAML, but are added to your pod automati-&#10;cally, aren&#8217;t shown in the figure.&#10;NOTE&#10;Like configMap volumes, secret volumes also support specifying file&#10;permissions for the files exposed in the volume through the defaultMode&#10;property.&#10;You define the secret &#10;volume here, referring to &#10;the fortune-https Secret.&#10;Container: web-server&#10;Container: html-generator&#10;Secret: fortune-https&#10;Default token Secret and volume not shown&#10;secret&#10;volume:&#10;certs&#10;emptyDir&#10;volume:&#10;html&#10;con&#64257;gMap&#10;volume:&#10;con&#64257;g&#10;https.cert&#10;...&#10;...&#10;...&#10;https.key&#10;foo&#10;/etc/nginx/conf.d/&#10;/etc/nginx/certs/&#10;/usr/share/nginx/html/&#10;/var/htdocs&#10;Con&#64257;gMap: fortune-con&#64257;g&#10;my-nginx-con&#64257;g.conf&#10;server {&#10;&#8230;&#10;}&#10;Pod&#10;Environment variables:&#10;INTERVAL=25&#10;sleep-interval&#10;25&#10;Figure 7.12&#10;Combining a ConfigMap and a Secret to run your fortune-https pod&#10; &#10;"
    color "green"
  ]
  node [
    id 330
    label "253"
    title "Page_253"
    color "blue"
  ]
  node [
    id 331
    label "text_164"
    title "221&#10;Using Secrets to pass sensitive data to containers&#10;TESTING WHETHER NGINX IS USING THE CERT AND KEY FROM THE SECRET&#10;Once the pod is running, you can see if it&#8217;s serving HTTPS traffic by opening a port-&#10;forward tunnel to the pod&#8217;s port 443 and using it to send a request to the server&#10;with curl: &#10;$ kubectl port-forward fortune-https 8443:443 &#38;&#10;Forwarding from 127.0.0.1:8443 -> 443&#10;Forwarding from [::1]:8443 -> 443&#10;$ curl https://localhost:8443 -k&#10;If you configured the server properly, you should get a response. You can check the&#10;server&#8217;s certificate to see if it matches the one you generated earlier. This can also be&#10;done with curl by turning on verbose logging using the -v option, as shown in the fol-&#10;lowing listing.&#10;$ curl https://localhost:8443 -k -v&#10;* About to connect() to localhost port 8443 (#0)&#10;*   Trying ::1...&#10;* Connected to localhost (::1) port 8443 (#0)&#10;* Initializing NSS with certpath: sql:/etc/pki/nssdb&#10;* skipping SSL peer certificate verification&#10;* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384&#10;* Server certificate:&#10;*   subject: CN=www.kubia-example.com          &#10;*   start date: aug 16 18:43:13 2016 GMT       &#10;*   expire date: aug 14 18:43:13 2026 GMT      &#10;*   common name: www.kubia-example.com         &#10;*   issuer: CN=www.kubia-example.com           &#10;UNDERSTANDING SECRET VOLUMES ARE STORED IN MEMORY&#10;You successfully delivered your certificate and private key to your container by mount-&#10;ing a secret volume in its directory tree at /etc/nginx/certs. The secret volume uses&#10;an in-memory filesystem (tmpfs) for the Secret files. You can see this if you list mounts&#10;in the container:&#10;$ kubectl exec fortune-https -c web-server -- mount | grep certs&#10;tmpfs on /etc/nginx/certs type tmpfs (ro,relatime) &#10;Because tmpfs is used, the sensitive data stored in the Secret is never written to disk,&#10;where it could be compromised. &#10;EXPOSING A SECRET&#8217;S ENTRIES THROUGH ENVIRONMENT VARIABLES&#10;Instead of using a volume, you could also have exposed individual entries from the&#10;secret as environment variables, the way you did with the sleep-interval entry from&#10;the ConfigMap. For example, if you wanted to expose the foo key from your Secret as&#10;environment variable FOO_SECRET, you&#8217;d add the snippet from the following listing to&#10;the container definition.&#10;Listing 7.26&#10;Displaying the server certificate sent by Nginx&#10;The certificate &#10;matches the one you &#10;created and stored &#10;in the Secret.&#10; &#10;"
    color "green"
  ]
  node [
    id 332
    label "254"
    title "Page_254"
    color "blue"
  ]
  node [
    id 333
    label "text_165"
    title "222&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;    env:&#10;    - name: FOO_SECRET&#10;      valueFrom:                  &#10;        secretKeyRef:             &#10;          name: fortune-https    &#10;          key: foo           &#10;This is almost exactly like when you set the INTERVAL environment variable, except&#10;that this time you&#8217;re referring to a Secret by using secretKeyRef instead of config-&#10;MapKeyRef, which is used to refer to a ConfigMap.&#10; Even though Kubernetes enables you to expose Secrets through environment vari-&#10;ables, it may not be the best idea to use this feature. Applications usually dump envi-&#10;ronment variables in error reports or even write them to the application log at startup,&#10;which may unintentionally expose them. Additionally, child processes inherit all the&#10;environment variables of the parent process, so if your app runs a third-party binary,&#10;you have no way of knowing what happens with your secret data. &#10;TIP&#10;Think twice before using environment variables to pass your Secrets to&#10;your container, because they may get exposed inadvertently. To be safe, always&#10;use secret volumes for exposing Secrets.&#10;7.5.6&#10;Understanding image pull Secrets&#10;You&#8217;ve learned how to pass Secrets to your applications and use the data they contain.&#10;But sometimes Kubernetes itself requires you to pass credentials to it&#8212;for example,&#10;when you&#8217;d like to use images from a private container image registry. This is also&#10;done through Secrets.&#10; Up to now all your container images have been stored on public image registries,&#10;which don&#8217;t require any special credentials to pull images from them. But most orga-&#10;nizations don&#8217;t want their images to be available to everyone and thus use a private&#10;image registry. When deploying a pod, whose container images reside in a private reg-&#10;istry, Kubernetes needs to know the credentials required to pull the image. Let&#8217;s see&#10;how to do that.&#10;USING A PRIVATE IMAGE REPOSITORY ON DOCKER HUB&#10;Docker Hub, in addition to public image repositories, also allows you to create private&#10;repositories. You can mark a repository as private by logging in at http:/&#10;/hub.docker&#10;.com with your web browser, finding the repository and checking a checkbox. &#10; To run a pod, which uses an image from the private repository, you need to do&#10;two things:&#10;&#61601;Create a Secret holding the credentials for the Docker registry.&#10;&#61601;Reference that Secret in the imagePullSecrets field of the pod manifest.&#10;Listing 7.27&#10;Exposing a Secret&#8217;s entry as an environment variable&#10;The variable should be set &#10;from the entry of a Secret.&#10;The name of the Secret &#10;holding the key&#10;The key of the Secret &#10;to expose&#10; &#10;"
    color "green"
  ]
  node [
    id 334
    label "255"
    title "Page_255"
    color "blue"
  ]
  node [
    id 335
    label "text_166"
    title "223&#10;Using Secrets to pass sensitive data to containers&#10;CREATING A SECRET FOR AUTHENTICATING WITH A DOCKER REGISTRY&#10;Creating a Secret holding the credentials for authenticating with a Docker registry&#10;isn&#8217;t that different from creating the generic Secret you created in section 7.5.3. You&#10;use the same kubectl create secret command, but with a different type and&#10;options:&#10;$ kubectl create secret docker-registry mydockerhubsecret \&#10;  --docker-username=myusername --docker-password=mypassword \ &#10;  --docker-email=my.email@provider.com&#10;Rather than create a generic secret, you&#8217;re creating a docker-registry Secret called&#10;mydockerhubsecret. You&#8217;re specifying your Docker Hub username, password, and&#10;email. If you inspect the contents of the newly created Secret with kubectl describe,&#10;you&#8217;ll see that it includes a single entry called .dockercfg. This is equivalent to the&#10;.dockercfg file in your home directory, which is created by Docker when you run the&#10;docker login command.&#10;USING THE DOCKER-REGISTRY SECRET IN A POD DEFINITION&#10;To have Kubernetes use the Secret when pulling images from your private Docker&#10;Hub repository, all you need to do is specify the Secret&#8217;s name in the pod spec, as&#10;shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: private-pod&#10;spec:&#10;  imagePullSecrets:                 &#10;  - name: mydockerhubsecret         &#10;  containers:&#10;  - image: username/private:tag&#10;    name: main&#10;In the pod definition in the previous listing, you&#8217;re specifying the mydockerhubsecret&#10;Secret as one of the imagePullSecrets. I suggest you try this out yourself, because it&#8217;s&#10;likely you&#8217;ll deal with private container images soon.&#10;NOT HAVING TO SPECIFY IMAGE PULL SECRETS ON EVERY POD&#10;Given that people usually run many different pods in their systems, it makes you won-&#10;der if you need to add the same image pull Secrets to every pod. Luckily, that&#8217;s not the&#10;case. In chapter 12 you&#8217;ll learn how image pull Secrets can be added to all your pods&#10;automatically if you add the Secrets to a ServiceAccount.&#10;Listing 7.28&#10;A pod definition using an image pull Secret: pod-with-private-image.yaml&#10;This enables pulling images &#10;from a private image registry.&#10; &#10;"
    color "green"
  ]
  node [
    id 336
    label "256"
    title "Page_256"
    color "blue"
  ]
  node [
    id 337
    label "text_167"
    title "224&#10;CHAPTER 7&#10;ConfigMaps and Secrets: configuring applications&#10;7.6&#10;Summary&#10;This wraps up this chapter on how to pass configuration data to containers. You&#8217;ve&#10;learned how to&#10;&#61601;Override the default command defined in a container image in the pod definition&#10;&#61601;Pass command-line arguments to the main container process&#10;&#61601;Set environment variables for a container&#10;&#61601;Decouple configuration from a pod specification and put it into a ConfigMap&#10;&#61601;Store sensitive data in a Secret and deliver it securely to containers&#10;&#61601;Create a docker-registry Secret and use it to pull images from a private image&#10;registry&#10;In the next chapter, you&#8217;ll learn how to pass pod and container metadata to applica-&#10;tions running inside them. You&#8217;ll also see how the default token Secret, which we&#10;learned about in this chapter, is used to talk to the API server from within a pod. &#10; &#10;"
    color "green"
  ]
  node [
    id 338
    label "257"
    title "Page_257"
    color "blue"
  ]
  node [
    id 339
    label "text_168"
    title "225&#10;Accessing pod metadata&#10;and other resources&#10;from applications&#10;Applications often need information about the environment they&#8217;re running in,&#10;including details about themselves and that of other components in the cluster.&#10;You&#8217;ve already seen how Kubernetes enables service discovery through environ-&#10;ment variables or DNS, but what about other information? In this chapter, you&#8217;ll&#10;see how certain pod and container metadata can be passed to the container and&#10;how easy it is for an app running inside a container to talk to the Kubernetes API&#10;server to get information about the resources deployed in the cluster and even how&#10;to create or modify those resources.&#10;This chapter covers&#10;&#61601;Using the Downward API to pass information into &#10;containers&#10;&#61601;Exploring the Kubernetes REST API&#10;&#61601;Leaving authentication and server verification to &#10;kubectl proxy&#10;&#61601;Accessing the API server from within a container&#10;&#61601;Understanding the ambassador container pattern&#10;&#61601;Using Kubernetes client libraries&#10; &#10;"
    color "green"
  ]
  node [
    id 340
    label "258"
    title "Page_258"
    color "blue"
  ]
  node [
    id 341
    label "text_169"
    title "226&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10;8.1&#10;Passing metadata through the Downward API&#10;In the previous chapter you saw how you can pass configuration data to your appli-&#10;cations through environment variables or through configMap and secret volumes.&#10;This works well for data that you set yourself and that is known before the pod is&#10;scheduled to a node and run there. But what about data that isn&#8217;t known up until&#10;that point&#8212;such as the pod&#8217;s IP, the host node&#8217;s name, or even the pod&#8217;s own name&#10;(when the name is generated; for example, when the pod is created by a ReplicaSet&#10;or similar controller)? And what about data that&#8217;s already specified elsewhere, such&#10;as a pod&#8217;s labels and annotations? You don&#8217;t want to repeat the same information in&#10;multiple places.&#10; Both these problems are solved by the Kubernetes Downward API. It allows you to&#10;pass metadata about the pod and its environment through environment variables or&#10;files (in a downwardAPI volume). Don&#8217;t be confused by the name. The Downward API&#10;isn&#8217;t like a REST endpoint that your app needs to hit so it can get the data. It&#8217;s a way of&#10;having environment variables or files populated with values from the pod&#8217;s specifica-&#10;tion or status, as shown in figure 8.1.&#10;8.1.1&#10;Understanding the available metadata&#10;The Downward API enables you to expose the pod&#8217;s own metadata to the processes&#10;running inside that pod. Currently, it allows you to pass the following information to&#10;your containers:&#10;&#61601;The pod&#8217;s name&#10;&#61601;The pod&#8217;s IP address&#10;Container: main&#10;Environment&#10;variables&#10;API server&#10;Used to initialize environment&#10;variables and &#64257;les in the&#10;downwardAPI volume&#10;Pod manifest&#10;- Metadata&#10;- Status&#10;Pod&#10;downwardAPI&#10;volume&#10;App process&#10;Figure 8.1&#10;The Downward API exposes pod metadata through environment variables or files.&#10; &#10;"
    color "green"
  ]
  node [
    id 342
    label "259"
    title "Page_259"
    color "blue"
  ]
  node [
    id 343
    label "text_170"
    title "227&#10;Passing metadata through the Downward API&#10;&#61601;The namespace the pod belongs to&#10;&#61601;The name of the node the pod is running on&#10;&#61601;The name of the service account the pod is running under&#10;&#61601;The CPU and memory requests for each container&#10;&#61601;The CPU and memory limits for each container&#10;&#61601;The pod&#8217;s labels&#10;&#61601;The pod&#8217;s annotations&#10;Most of the items in the list shouldn&#8217;t require further explanation, except perhaps the&#10;service account and CPU/memory requests and limits, which we haven&#8217;t introduced&#10;yet. We&#8217;ll cover service accounts in detail in chapter 12. For now, all you need to know&#10;is that a service account is the account that the pod authenticates as when talking to&#10;the API server. CPU and memory requests and limits are explained in chapter 14.&#10;They&#8217;re the amount of CPU and memory guaranteed to a container and the maxi-&#10;mum amount it can get.&#10; Most items in the list can be passed to containers either through environment vari-&#10;ables or through a downwardAPI volume, but labels and annotations can only be&#10;exposed through the volume. Part of the data can be acquired by other means (for&#10;example, from the operating system directly), but the Downward API provides a sim-&#10;pler alternative.&#10; Let&#8217;s look at an example to pass metadata to your containerized process.&#10;8.1.2&#10;Exposing metadata through environment variables&#10;First, let&#8217;s look at how you can pass the pod&#8217;s and container&#8217;s metadata to the con-&#10;tainer through environment variables. You&#8217;ll create a simple single-container pod&#10;from the following listing&#8217;s manifest.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: downward&#10;spec:&#10;  containers:&#10;  - name: main&#10;    image: busybox&#10;    command: [&#34;sleep&#34;, &#34;9999999&#34;]&#10;    resources:&#10;      requests:&#10;        cpu: 15m&#10;        memory: 100Ki&#10;      limits:&#10;        cpu: 100m&#10;        memory: 4Mi&#10;    env:&#10;    - name: POD_NAME&#10;Listing 8.1&#10;Downward API used in environment variables: downward-api-env.yaml&#10; &#10;"
    color "green"
  ]
  node [
    id 344
    label "260"
    title "Page_260"
    color "blue"
  ]
  node [
    id 345
    label "text_171"
    title "228&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10;      valueFrom:                            &#10;        fieldRef:                           &#10;          fieldPath: metadata.name          &#10;    - name: POD_NAMESPACE&#10;      valueFrom:&#10;        fieldRef:&#10;          fieldPath: metadata.namespace&#10;    - name: POD_IP&#10;      valueFrom:&#10;        fieldRef:&#10;          fieldPath: status.podIP&#10;    - name: NODE_NAME&#10;      valueFrom:&#10;        fieldRef:&#10;          fieldPath: spec.nodeName&#10;    - name: SERVICE_ACCOUNT&#10;      valueFrom:&#10;        fieldRef:&#10;          fieldPath: spec.serviceAccountName&#10;    - name: CONTAINER_CPU_REQUEST_MILLICORES&#10;      valueFrom:                                   &#10;        resourceFieldRef:                          &#10;          resource: requests.cpu                   &#10;          divisor: 1m                            &#10;    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES&#10;      valueFrom:&#10;        resourceFieldRef:&#10;          resource: limits.memory&#10;          divisor: 1Ki&#10;When your process runs, it can look up all the environment variables you defined in&#10;the pod spec. Figure 8.2 shows the environment variables and the sources of their val-&#10;ues. The pod&#8217;s name, IP, and namespace will be exposed through the POD_NAME,&#10;POD_IP, and POD_NAMESPACE environment variables, respectively. The name of the&#10;node the container is running on will be exposed through the NODE_NAME variable.&#10;The name of the service account is made available through the SERVICE_ACCOUNT&#10;environment variable. You&#8217;re also creating two environment variables that will hold&#10;the amount of CPU requested for this container and the maximum amount of mem-&#10;ory the container is allowed to consume.&#10; For environment variables exposing resource limits or requests, you specify a divi-&#10;sor. The actual value of the limit or the request will be divided by the divisor and the&#10;result exposed through the environment variable. In the previous example, you&#8217;re set-&#10;ting the divisor for CPU requests to 1m (one milli-core, or one one-thousandth of a&#10;CPU core). Because you&#8217;ve set the CPU request to 15m, the environment variable&#10;CONTAINER_CPU_REQUEST_MILLICORES will be set to 15. Likewise, you set the memory&#10;limit to 4Mi (4 mebibytes) and the divisor to 1Ki (1 Kibibyte), so the CONTAINER_MEMORY&#10;_LIMIT_KIBIBYTES environment variable will be set to 4096. &#10;Instead of specifying an absolute value, &#10;you&#8217;re referencing the metadata.name &#10;field from the pod manifest.&#10;A container&#8217;s CPU and memory &#10;requests and limits are referenced &#10;by using resourceFieldRef instead &#10;of fieldRef.&#10;For resource fields, you &#10;define a divisor to get the &#10;value in the unit you need.&#10; &#10;"
    color "green"
  ]
  node [
    id 346
    label "261"
    title "Page_261"
    color "blue"
  ]
  node [
    id 347
    label "text_172"
    title "229&#10;Passing metadata through the Downward API&#10;The divisor for CPU limits and requests can be either 1, which means one whole core,&#10;or 1m, which is one millicore. The divisor for memory limits/requests can be 1 (byte),&#10;1k (kilobyte) or 1Ki (kibibyte), 1M (megabyte) or 1Mi (mebibyte), and so on.&#10; After creating the pod, you can use kubectl exec to see all these environment vari-&#10;ables in your container, as shown in the following listing.&#10;$ kubectl exec downward env&#10;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&#10;HOSTNAME=downward&#10;CONTAINER_MEMORY_LIMIT_KIBIBYTES=4096&#10;POD_NAME=downward&#10;POD_NAMESPACE=default&#10;POD_IP=10.0.0.10&#10;NODE_NAME=gke-kubia-default-pool-32a2cac8-sgl7&#10;SERVICE_ACCOUNT=default&#10;CONTAINER_CPU_REQUEST_MILLICORES=15&#10;KUBERNETES_SERVICE_HOST=10.3.240.1&#10;KUBERNETES_SERVICE_PORT=443&#10;...&#10;Listing 8.2&#10;Environment variables in the downward pod&#10;Pod manifest&#10;metadata:&#10;name: downward&#10;namespace: default&#10;spec:&#10;nodeName: minikube&#10;serviceAccountName: default&#10;containers:&#10;- name: main&#10;image: busybox&#10;command: [&#34;sleep&#34;, &#34;9999999&#34;]&#10;resources:&#10;requests:&#10;cpu: 15m&#10;memory: 100Ki&#10;limits:&#10;cpu: 100m&#10;memory: 4Mi&#10;...&#10;status:&#10;podIP: 172.17.0.4&#10;...&#10;Pod: downward&#10;Container: main&#10;Environment variables&#10;POD_NAME=downward&#10;POD_NAMESPACE=default&#10;POD_IP=172.17.0.4&#10;NODE_NAME=minikube&#10;SERVICE_ACCOUNT=default&#10;CONTAINER_CPU_REQUEST_MILLICORES=15&#10;CONTAINER_MEMORY_LIMIT_KIBIBYTES=4096&#10;divisor: 1m&#10;divisor: 1Ki&#10;Figure 8.2&#10;Pod metadata and attributes can be exposed to the pod through environment variables.&#10; &#10;"
    color "green"
  ]
  node [
    id 348
    label "262"
    title "Page_262"
    color "blue"
  ]
  node [
    id 349
    label "text_173"
    title "230&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10;All processes running inside the container can read those variables and use them how-&#10;ever they need. &#10;8.1.3&#10;Passing metadata through files in a downwardAPI volume&#10;If you prefer to expose the metadata through files instead of environment variables,&#10;you can define a downwardAPI volume and mount it into your container. You must use&#10;a downwardAPI volume for exposing the pod&#8217;s labels or its annotations, because nei-&#10;ther can be exposed through environment variables. We&#8217;ll discuss why later.&#10; As with environment variables, you need to specify each metadata field explicitly if&#10;you want to have it exposed to the process. Let&#8217;s see how to modify the previous exam-&#10;ple to use a volume instead of environment variables, as shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: downward&#10;  labels:                  &#10;    foo: bar               &#10;  annotations:             &#10;    key1: value1           &#10;    key2: |                &#10;      multi                &#10;      line                 &#10;      value                &#10;spec:&#10;  containers:&#10;  - name: main&#10;    image: busybox&#10;    command: [&#34;sleep&#34;, &#34;9999999&#34;]&#10;    resources:&#10;      requests:&#10;        cpu: 15m&#10;        memory: 100Ki&#10;      limits:&#10;        cpu: 100m&#10;        memory: 4Mi&#10;    volumeMounts:                        &#10;    - name: downward                     &#10;      mountPath: /etc/downward           &#10;  volumes:&#10;  - name: downward                 &#10;    downwardAPI:                   &#10;      items:&#10;      - path: &#34;podName&#34;                     &#10;        fieldRef:                           &#10;          fieldPath: metadata.name          &#10;      - path: &#34;podNamespace&#34;&#10;        fieldRef:&#10;          fieldPath: metadata.namespace&#10;Listing 8.3&#10;Pod with a downwardAPI volume: downward-api-volume.yaml&#10;These labels and &#10;annotations will be &#10;exposed through the &#10;downwardAPI volume.&#10;You&#8217;re mounting the &#10;downward volume &#10;under /etc/downward.&#10;You&#8217;re defining a downwardAPI &#10;volume with the name downward.&#10;The pod&#8217;s name (from the metadata.name &#10;field in the manifest) will be written to &#10;the podName file.&#10; &#10;"
    color "green"
  ]
  node [
    id 350
    label "263"
    title "Page_263"
    color "blue"
  ]
  node [
    id 351
    label "text_174"
    title "231&#10;Passing metadata through the Downward API&#10;      - path: &#34;labels&#34;                       &#10;        fieldRef:                            &#10;          fieldPath: metadata.labels         &#10;      - path: &#34;annotations&#34;                       &#10;        fieldRef:                                 &#10;          fieldPath: metadata.annotations         &#10;      - path: &#34;containerCpuRequestMilliCores&#34;&#10;        resourceFieldRef:&#10;          containerName: main&#10;          resource: requests.cpu&#10;          divisor: 1m&#10;      - path: &#34;containerMemoryLimitBytes&#34;&#10;        resourceFieldRef:&#10;          containerName: main&#10;          resource: limits.memory&#10;          divisor: 1&#10;Instead of passing the metadata through environment variables, you&#8217;re defining a vol-&#10;ume called downward and mounting it in your container under /etc/downward. The&#10;files this volume will contain are configured under the downwardAPI.items attribute&#10;in the volume specification.&#10; Each item specifies the path (the filename) where the metadata should be written&#10;to and references either a pod-level field or a container resource field whose value you&#10;want stored in the file (see figure 8.3).&#10;The pod&#8217;s labels will be written &#10;to the /etc/downward/labels file.&#10;The pod&#8217;s annotations will be &#10;written to the /etc/downward/&#10;annotations file.&#10;downwardAPI volume&#10;Pod manifest&#10;metadata:&#10;name: downward&#10;namespace: default&#10;labels:&#10;foo: bar&#10;annotations:&#10;key1: value1&#10;...&#10;spec:&#10;containers:&#10;- name: main&#10;image: busybox&#10;command: [&#34;sleep&#34;, &#34;9999999&#34;]&#10;resources:&#10;requests:&#10;cpu: 15m&#10;memory: 100Ki&#10;limits:&#10;cpu: 100m&#10;memory: 4Mi&#10;...&#10;/podName&#10;/podNamespace&#10;/labels&#10;/annotations&#10;/containerCpuRequestMilliCores&#10;/containerMemoryLimitBytes&#10;divisor: 1&#10;divisor: 1m&#10;Container: main&#10;Pod: downward&#10;Filesystem&#10;/&#10;etc/&#10;downward/&#10;Figure 8.3&#10;Using a downwardAPI volume to pass metadata to the container&#10; &#10;"
    color "green"
  ]
  node [
    id 352
    label "264"
    title "Page_264"
    color "blue"
  ]
  node [
    id 353
    label "text_175"
    title "232&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10;Delete the previous pod and create a new one from the manifest in the previous list-&#10;ing. Then look at the contents of the mounted downwardAPI volume directory. You&#10;mounted the volume under /etc/downward/, so list the files in there, as shown in the&#10;following listing.&#10;$ kubectl exec downward ls -lL /etc/downward&#10;-rw-r--r--   1 root   root   134 May 25 10:23 annotations&#10;-rw-r--r--   1 root   root     2 May 25 10:23 containerCpuRequestMilliCores&#10;-rw-r--r--   1 root   root     7 May 25 10:23 containerMemoryLimitBytes&#10;-rw-r--r--   1 root   root     9 May 25 10:23 labels&#10;-rw-r--r--   1 root   root     8 May 25 10:23 podName&#10;-rw-r--r--   1 root   root     7 May 25 10:23 podNamespace&#10;NOTE&#10;As with the configMap and secret volumes, you can change the file&#10;permissions through the downwardAPI volume&#8217;s defaultMode property in the&#10;pod spec.&#10;Each file corresponds to an item in the volume&#8217;s definition. The contents of files,&#10;which correspond to the same metadata fields as in the previous example, are the&#10;same as the values of environment variables you used before, so we won&#8217;t show them&#10;here. But because you couldn&#8217;t expose labels and annotations through environment&#10;variables before, examine the following listing for the contents of the two files you&#10;exposed them in.&#10;$ kubectl exec downward cat /etc/downward/labels&#10;foo=&#34;bar&#34;&#10;$ kubectl exec downward cat /etc/downward/annotations&#10;key1=&#34;value1&#34;&#10;key2=&#34;multi\nline\nvalue\n&#34;&#10;kubernetes.io/config.seen=&#34;2016-11-28T14:27:45.664924282Z&#34;&#10;kubernetes.io/config.source=&#34;api&#34;&#10;As you can see, each label/annotation is written in the key=value format on a sepa-&#10;rate line. Multi-line values are written to a single line with newline characters denoted&#10;with \n.&#10;UPDATING LABELS AND ANNOTATIONS&#10;You may remember that labels and annotations can be modified while a pod is run-&#10;ning. As you might expect, when they change, Kubernetes updates the files holding&#10;them, allowing the pod to always see up-to-date data. This also explains why labels and&#10;annotations can&#8217;t be exposed through environment variables. Because environment&#10;variable values can&#8217;t be updated afterward, if the labels or annotations of a pod were&#10;exposed through environment variables, there&#8217;s no way to expose the new values after&#10;they&#8217;re modified.&#10;Listing 8.4&#10;Files in the downwardAPI volume&#10;Listing 8.5&#10;Displaying labels and annotations in the downwardAPI volume&#10; &#10;"
    color "green"
  ]
  node [
    id 354
    label "265"
    title "Page_265"
    color "blue"
  ]
  node [
    id 355
    label "text_176"
    title "233&#10;Talking to the Kubernetes API server&#10;REFERRING TO CONTAINER-LEVEL METADATA IN THE VOLUME SPECIFICATION&#10;Before we wrap up this section, we need to point out one thing. When exposing con-&#10;tainer-level metadata, such as a container&#8217;s resource limit or requests (done using&#10;resourceFieldRef), you need to specify the name of the container whose resource&#10;field you&#8217;re referencing, as shown in the following listing.&#10;spec:&#10;  volumes:&#10;  - name: downward                       &#10;    downwardAPI:                         &#10;      items:&#10;      - path: &#34;containerCpuRequestMilliCores&#34;&#10;        resourceFieldRef:&#10;          containerName: main       &#10;          resource: requests.cpu&#10;          divisor: 1m&#10;The reason for this becomes obvious if you consider that volumes are defined at the&#10;pod level, not at the container level. When referring to a container&#8217;s resource field&#10;inside a volume specification, you need to explicitly specify the name of the container&#10;you&#8217;re referring to. This is true even for single-container pods. &#10; Using volumes to expose a container&#8217;s resource requests and/or limits is slightly&#10;more complicated than using environment variables, but the benefit is that it allows&#10;you to pass one container&#8217;s resource fields to a different container if needed (but&#10;both containers need to be in the same pod). With environment variables, a container&#10;can only be passed its own resource limits and requests. &#10;UNDERSTANDING WHEN TO USE THE DOWNWARD API&#10;As you&#8217;ve seen, using the Downward API isn&#8217;t complicated. It allows you to keep the&#10;application Kubernetes-agnostic. This is especially useful when you&#8217;re dealing with an&#10;existing application that expects certain data in environment variables. The Down-&#10;ward API allows you to expose the data to the application without having to rewrite&#10;the application or wrap it in a shell script, which collects the data and then exposes it&#10;through environment variables.&#10; But the metadata available through the Downward API is fairly limited. If you need&#10;more, you&#8217;ll need to obtain it from the Kubernetes API server directly. You&#8217;ll learn&#10;how to do that next.&#10;8.2&#10;Talking to the Kubernetes API server&#10;We&#8217;ve seen how the Downward API provides a simple way to pass certain pod and con-&#10;tainer metadata to the process running inside them. It only exposes the pod&#8217;s own&#10;metadata and a subset of all of the pod&#8217;s data. But sometimes your app will need to&#10;know more about other pods and even other resources defined in your cluster. The&#10;Downward API doesn&#8217;t help in those cases.&#10;Listing 8.6&#10;Referring to container-level metadata in a downwardAPI volume&#10;Container name &#10;must be specified&#10; &#10;"
    color "green"
  ]
  node [
    id 356
    label "266"
    title "Page_266"
    color "blue"
  ]
  node [
    id 357
    label "text_177"
    title "234&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10; As you&#8217;ve seen throughout the book, information about services and pods can be&#10;obtained by looking at the service-related environment variables or through DNS. But&#10;when the app needs data about other resources or when it requires access to the most&#10;up-to-date information as possible, it needs to talk to the API server directly (as shown&#10;in figure 8.4).&#10;Before you see how apps within pods can talk to the Kubernetes API server, let&#8217;s first&#10;explore the server&#8217;s REST endpoints from your local machine, so you can see what&#10;talking to the API server looks like.&#10;8.2.1&#10;Exploring the Kubernetes REST API&#10;You&#8217;ve learned about different Kubernetes resource types. But if you&#8217;re planning on&#10;developing apps that talk to the Kubernetes API, you&#8217;ll want to know the API first. &#10; To do that, you can try hitting the API server directly. You can get its URL by run-&#10;ning kubectl cluster-info:&#10;$ kubectl cluster-info&#10;Kubernetes master is running at https://192.168.99.100:8443&#10;Because the server uses HTTPS and requires authentication, it&#8217;s not simple to talk to&#10;it directly. You can try accessing it with curl and using curl&#8217;s --insecure (or -k)&#10;option to skip the server certificate check, but that doesn&#8217;t get you far:&#10;$ curl https://192.168.99.100:8443 -k&#10;Unauthorized&#10;Luckily, rather than dealing with authentication yourself, you can talk to the server&#10;through a proxy by running the kubectl proxy command. &#10;ACCESSING THE API SERVER THROUGH KUBECTL PROXY &#10;The kubectl proxy command runs a proxy server that accepts HTTP connections on&#10;your local machine and proxies them to the API server while taking care of authenti-&#10;cation, so you don&#8217;t need to pass the authentication token in every request. It also&#10;makes sure you&#8217;re talking to the actual API server and not a man in the middle (by&#10;verifying the server&#8217;s certificate on each request).&#10;Container&#10;API server&#10;Pod&#10;App process&#10;API objects&#10;Figure 8.4&#10;Talking to the API server &#10;from inside a pod to get information &#10;about other API objects&#10; &#10;"
    color "green"
  ]
  node [
    id 358
    label "267"
    title "Page_267"
    color "blue"
  ]
  node [
    id 359
    label "text_178"
    title "235&#10;Talking to the Kubernetes API server&#10; Running the proxy is trivial. All you need to do is run the following command:&#10;$ kubectl proxy&#10;Starting to serve on 127.0.0.1:8001&#10;You don&#8217;t need to pass in any other arguments, because kubectl already knows every-&#10;thing it needs (the API server URL, authorization token, and so on). As soon as it starts&#10;up, the proxy starts accepting connections on local port 8001. Let&#8217;s see if it works:&#10;$ curl localhost:8001&#10;{&#10;  &#34;paths&#34;: [&#10;    &#34;/api&#34;,&#10;    &#34;/api/v1&#34;,&#10;    ...&#10;Voila! You sent the request to the proxy, it sent a request to the API server, and then&#10;the proxy returned whatever the server returned. Now, let&#8217;s start exploring.&#10;EXPLORING THE KUBERNETES API THROUGH THE KUBECTL PROXY&#10;You can continue to use curl, or you can open your web browser and point it to&#10;http:/&#10;/localhost:8001. Let&#8217;s examine what the API server returns when you hit its base&#10;URL more closely. The server responds with a list of paths, as shown in the follow-&#10;ing listing.&#10;$ curl http://localhost:8001&#10;{&#10;  &#34;paths&#34;: [&#10;    &#34;/api&#34;,&#10;    &#34;/api/v1&#34;,                  &#10;    &#34;/apis&#34;,&#10;    &#34;/apis/apps&#34;,&#10;    &#34;/apis/apps/v1beta1&#34;,&#10;    ...&#10;    &#34;/apis/batch&#34;,              &#10;    &#34;/apis/batch/v1&#34;,           &#10;    &#34;/apis/batch/v2alpha1&#34;,     &#10;    ...&#10;These paths correspond to the API groups and versions you specify in your resource&#10;definitions when creating resources such as Pods, Services, and so on. &#10; You may recognize the batch/v1 in the /apis/batch/v1 path as the API group and&#10;version of the Job resources you learned about in chapter 4. Likewise, the /api/v1&#10;corresponds to the apiVersion: v1 you refer to in the common resources you created&#10;(Pods, Services, ReplicationControllers, and so on). The most common resource&#10;types, which were introduced in the earliest versions of Kubernetes, don&#8217;t belong to&#10;Listing 8.7&#10;Listing the API server&#8217;s REST endpoints: http:/&#10;/localhost:8001&#10;Most resource types &#10;can be found here.&#10;The batch API &#10;group and its &#10;two versions&#10; &#10;"
    color "green"
  ]
  node [
    id 360
    label "268"
    title "Page_268"
    color "blue"
  ]
  node [
    id 361
    label "text_179"
    title "236&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10;any specific group, because Kubernetes initially didn&#8217;t even use the concept of API&#10;groups; they were introduced later. &#10;NOTE&#10;These initial resource types without an API group are now considered&#10;to belong to the core API group.&#10;EXPLORING THE BATCH API GROUP&#8217;S REST ENDPOINT&#10;Let&#8217;s explore the Job resource API. You&#8217;ll start by looking at what&#8217;s behind the&#10;/apis/batch path (you&#8217;ll omit the version for now), as shown in the following listing.&#10;$ curl http://localhost:8001/apis/batch&#10;{&#10;  &#34;kind&#34;: &#34;APIGroup&#34;,&#10;  &#34;apiVersion&#34;: &#34;v1&#34;,&#10;  &#34;name&#34;: &#34;batch&#34;,&#10;  &#34;versions&#34;: [&#10;    {&#10;      &#34;groupVersion&#34;: &#34;batch/v1&#34;,             &#10;      &#34;version&#34;: &#34;v1&#34;                         &#10;    },&#10;    {&#10;      &#34;groupVersion&#34;: &#34;batch/v2alpha1&#34;,       &#10;      &#34;version&#34;: &#34;v2alpha1&#34;                   &#10;    }&#10;  ],&#10;  &#34;preferredVersion&#34;: {                    &#10;    &#34;groupVersion&#34;: &#34;batch/v1&#34;,            &#10;    &#34;version&#34;: &#34;v1&#34;                        &#10;  },&#10;  &#34;serverAddressByClientCIDRs&#34;: null&#10;}&#10;The response shows a description of the batch API group, including the available ver-&#10;sions and the preferred version clients should use. Let&#8217;s continue and see what&#8217;s&#10;behind the /apis/batch/v1 path. It&#8217;s shown in the following listing.&#10;$ curl http://localhost:8001/apis/batch/v1&#10;{&#10;  &#34;kind&#34;: &#34;APIResourceList&#34;,              &#10;  &#34;apiVersion&#34;: &#34;v1&#34;,&#10;  &#34;groupVersion&#34;: &#34;batch/v1&#34;,             &#10;  &#34;resources&#34;: [                          &#10;    {&#10;      &#34;name&#34;: &#34;jobs&#34;,             &#10;      &#34;namespaced&#34;: true,         &#10;      &#34;kind&#34;: &#34;Job&#34;,              &#10;Listing 8.8&#10;Listing endpoints under /apis/batch: http:/&#10;/localhost:8001/apis/batch&#10;Listing 8.9&#10;Resource types in batch/v1: http:/&#10;/localhost:8001/apis/batch/v1&#10;The batch API &#10;group contains &#10;two versions.&#10;Clients should use the &#10;v1 version instead of &#10;v2alpha1.&#10;This is a list of API resources &#10;in the batch/v1 API group.&#10;Here&#8217;s an array holding &#10;all the resource types &#10;in this group.&#10;This describes the &#10;Job resource, which &#10;is namespaced.&#10; &#10;"
    color "green"
  ]
  node [
    id 362
    label "269"
    title "Page_269"
    color "blue"
  ]
  node [
    id 363
    label "text_180"
    title "237&#10;Talking to the Kubernetes API server&#10;      &#34;verbs&#34;: [                 &#10;        &#34;create&#34;,                &#10;        &#34;delete&#34;,                &#10;        &#34;deletecollection&#34;,      &#10;        &#34;get&#34;,                   &#10;        &#34;list&#34;,                  &#10;        &#34;patch&#34;,                 &#10;        &#34;update&#34;,                &#10;        &#34;watch&#34;                  &#10;      ]&#10;    },&#10;    {&#10;      &#34;name&#34;: &#34;jobs/status&#34;,            &#10;      &#34;namespaced&#34;: true,                  &#10;      &#34;kind&#34;: &#34;Job&#34;,&#10;      &#34;verbs&#34;: [             &#10;        &#34;get&#34;,               &#10;        &#34;patch&#34;,             &#10;        &#34;update&#34;             &#10;      ]&#10;    }&#10;  ]&#10;}&#10;As you can see, the API server returns a list of resource types and REST endpoints in&#10;the batch/v1 API group. One of those is the Job resource. In addition to the name of&#10;the resource and the associated kind, the API server also includes information on&#10;whether the resource is namespaced or not, its short name (if it has one; Jobs don&#8217;t),&#10;and a list of verbs you can use with the resource. &#10; The returned list describes the REST resources exposed in the API server. The&#10;&#34;name&#34;: &#34;jobs&#34; line tells you that the API contains the /apis/batch/v1/jobs end-&#10;point. The &#34;verbs&#34; array says you can retrieve, update, and delete Job resources&#10;through that endpoint. For certain resources, additional API endpoints are also&#10;exposed (such as the jobs/status path, which allows modifying only the status of&#10;a Job).&#10;LISTING ALL JOB INSTANCES IN THE CLUSTER&#10;To get a list of Jobs in your cluster, perform a GET request on path /apis/batch/&#10;v1/jobs, as shown in the following listing.&#10;$ curl http://localhost:8001/apis/batch/v1/jobs&#10;{&#10;  &#34;kind&#34;: &#34;JobList&#34;,&#10;  &#34;apiVersion&#34;: &#34;batch/v1&#34;,&#10;  &#34;metadata&#34;: {&#10;    &#34;selfLink&#34;: &#34;/apis/batch/v1/jobs&#34;,&#10;    &#34;resourceVersion&#34;: &#34;225162&#34;&#10;  },&#10;Listing 8.10&#10;List of Jobs: http:/&#10;/localhost:8001/apis/batch/v1/jobs&#10;Here are the verbs that can be used &#10;with this resource (you can create &#10;Jobs; delete individual ones or a &#10;collection of them; and retrieve, &#10;watch, and update them).&#10;Resources also have a &#10;special REST endpoint for &#10;modifying their status.&#10;The status can be &#10;retrieved, patched, &#10;or updated.&#10; &#10;"
    color "green"
  ]
  node [
    id 364
    label "270"
    title "Page_270"
    color "blue"
  ]
  node [
    id 365
    label "text_181"
    title "238&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10;  &#34;items&#34;: [&#10;    {&#10;      &#34;metadata&#34;: {&#10;        &#34;name&#34;: &#34;my-job&#34;,&#10;        &#34;namespace&#34;: &#34;default&#34;,&#10;        ...&#10;You probably have no Job resources deployed in your cluster, so the items array will be&#10;empty. You can try deploying the Job in Chapter08/my-job.yaml and hitting the REST&#10;endpoint again to get the same output as in listing 8.10.&#10;RETRIEVING A SPECIFIC JOB INSTANCE BY NAME&#10;The previous endpoint returned a list of all Jobs across all namespaces. To get back&#10;only one specific Job, you need to specify its name and namespace in the URL. To&#10;retrieve the Job shown in the previous listing (name: my-job; namespace: default),&#10;you need to request the following path: /apis/batch/v1/namespaces/default/jobs/&#10;my-job, as shown in the following listing.&#10;$ curl http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job&#10;{&#10;  &#34;kind&#34;: &#34;Job&#34;,&#10;  &#34;apiVersion&#34;: &#34;batch/v1&#34;,&#10;  &#34;metadata&#34;: {&#10;    &#34;name&#34;: &#34;my-job&#34;,&#10;    &#34;namespace&#34;: &#34;default&#34;,&#10;    ...&#10;As you can see, you get back the complete JSON definition of the my-job Job resource,&#10;exactly like you do if you run:&#10;$ kubectl get job my-job -o json&#10;You&#8217;ve seen that you can browse the Kubernetes REST API server without using any&#10;special tools, but to fully explore the REST API and interact with it, a better option is&#10;described at the end of this chapter. For now, exploring it with curl like this is enough&#10;to make you understand how an application running in a pod talks to Kubernetes. &#10;8.2.2&#10;Talking to the API server from within a pod&#10;You&#8217;ve learned how to talk to the API server from your local machine, using the&#10;kubectl proxy. Now, let&#8217;s see how to talk to it from within a pod, where you (usually)&#10;don&#8217;t have kubectl. Therefore, to talk to the API server from inside a pod, you need&#10;to take care of three things:&#10;&#61601;Find the location of the API server.&#10;&#61601;Make sure you&#8217;re talking to the API server and not something impersonating it.&#10;&#61601;Authenticate with the server; otherwise it won&#8217;t let you see or do anything.&#10;Listing 8.11&#10;Retrieving a resource in a specific namespace by name&#10; &#10;"
    color "green"
  ]
  node [
    id 366
    label "271"
    title "Page_271"
    color "blue"
  ]
  node [
    id 367
    label "text_182"
    title "239&#10;Talking to the Kubernetes API server&#10;You&#8217;ll see how this is done in the next three sections. &#10;RUNNING A POD TO TRY OUT COMMUNICATION WITH THE API SERVER&#10;The first thing you need is a pod from which to talk to the API server. You&#8217;ll run a pod&#10;that does nothing (it runs the sleep command in its only container), and then run a&#10;shell in the container with kubectl exec. Then you&#8217;ll try to access the API server from&#10;within that shell using curl.&#10; Therefore, you need to use a container image that contains the curl binary. If you&#10;search for such an image on, say, Docker Hub, you&#8217;ll find the tutum/curl image, so&#10;use it (you can also use any other existing image containing the curl binary or you&#10;can build your own). The pod definition is shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: curl&#10;spec:&#10;  containers:&#10;  - name: main&#10;    image: tutum/curl                &#10;    command: [&#34;sleep&#34;, &#34;9999999&#34;]    &#10;After creating the pod, run kubectl exec to run a bash shell inside its container:&#10;$ kubectl exec -it curl bash&#10;root@curl:/#&#10;You&#8217;re now ready to talk to the API server.&#10;FINDING THE API SERVER&#8217;S ADDRESS&#10;First, you need to find the IP and port of the Kubernetes API server. This is easy,&#10;because a Service called kubernetes is automatically exposed in the default name-&#10;space and configured to point to the API server. You may remember seeing it every&#10;time you listed services with kubectl get svc:&#10;$ kubectl get svc&#10;NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE&#10;kubernetes   10.0.0.1     <none>        443/TCP   46d&#10;And you&#8217;ll remember from chapter 5 that environment variables are configured for&#10;each service. You can get both the IP address and the port of the API server by looking&#10;up the KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT variables (inside&#10;the container):&#10;root@curl:/# env | grep KUBERNETES_SERVICE&#10;KUBERNETES_SERVICE_PORT=443&#10;KUBERNETES_SERVICE_HOST=10.0.0.1&#10;KUBERNETES_SERVICE_PORT_HTTPS=443&#10;Listing 8.12&#10;A pod for trying out communication with the API server: curl.yaml&#10;Using the tutum/curl image, &#10;because you need curl &#10;available in the container&#10;You&#8217;re running the sleep &#10;command with a long delay to &#10;keep your container running.&#10; &#10;"
    color "green"
  ]
  node [
    id 368
    label "272"
    title "Page_272"
    color "blue"
  ]
  node [
    id 369
    label "text_183"
    title "240&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10;You may also remember that each service also gets a DNS entry, so you don&#8217;t even&#10;need to look up the environment variables, but instead simply point curl to&#10;https:/&#10;/kubernetes. To be fair, if you don&#8217;t know which port the service is available at,&#10;you also either need to look up the environment variables or perform a DNS SRV&#10;record lookup to get the service&#8217;s actual port number. &#10; The environment variables shown previously say that the API server is listening on&#10;port 443, which is the default port for HTTPS, so try hitting the server through&#10;HTTPS:&#10;root@curl:/# curl https://kubernetes&#10;curl: (60) SSL certificate problem: unable to get local issuer certificate&#10;...&#10;If you'd like to turn off curl's verification of the certificate, use&#10;  the -k (or --insecure) option.&#10;Although the simplest way to get around this is to use the proposed -k option (and&#10;this is what you&#8217;d normally use when playing with the API server manually), let&#8217;s look&#10;at the longer (and correct) route. Instead of blindly trusting that the server you&#8217;re&#10;connecting to is the authentic API server, you&#8217;ll verify its identity by having curl check&#10;its certificate. &#10;TIP&#10;Never skip checking the server&#8217;s certificate in an actual application.&#10;Doing so could make your app expose its authentication token to an attacker&#10;using a man-in-the-middle attack.&#10;VERIFYING THE SERVER&#8217;S IDENTITY&#10;In the previous chapter, while discussing Secrets, we looked at an automatically cre-&#10;ated Secret called default-token-xyz, which is mounted into each container at&#10;/var/run/secrets/kubernetes.io/serviceaccount/. Let&#8217;s see the contents of that Secret&#10;again, by listing files in that directory:&#10;root@curl:/# &#10;ls &#10;/var/run/secrets/kubernetes.io/serviceaccount/ &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10;ca.crt    namespace    token&#10;The Secret has three entries (and therefore three files in the Secret volume). Right&#10;now, we&#8217;ll focus on the ca.crt file, which holds the certificate of the certificate author-&#10;ity (CA) used to sign the Kubernetes API server&#8217;s certificate. To verify you&#8217;re talking to&#10;the API server, you need to check if the server&#8217;s certificate is signed by the CA. curl&#10;allows you to specify the CA certificate with the --cacert option, so try hitting the API&#10;server again:&#10;root@curl:/# curl --cacert /var/run/secrets/kubernetes.io/serviceaccount&#10;             &#10149; /ca.crt https://kubernetes&#10;Unauthorized&#10;NOTE&#10;You may see a longer error description than &#8220;Unauthorized.&#8221;&#10; &#10;"
    color "green"
  ]
  node [
    id 370
    label "273"
    title "Page_273"
    color "blue"
  ]
  node [
    id 371
    label "text_184"
    title "241&#10;Talking to the Kubernetes API server&#10;Okay, you&#8217;ve made progress. curl verified the server&#8217;s identity because its certificate&#10;was signed by the CA you trust. As the Unauthorized response suggests, you still need&#10;to take care of authentication. You&#8217;ll do that in a moment, but first let&#8217;s see how to&#10;make life easier by setting the CURL_CA_BUNDLE environment variable, so you don&#8217;t&#10;need to specify --cacert every time you run curl:&#10;root@curl:/# export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/&#10;             &#10149; serviceaccount/ca.crt&#10;You can now hit the API server without using --cacert:&#10;root@curl:/# curl https://kubernetes&#10;Unauthorized&#10;This is much nicer now. Your client (curl) trusts the API server now, but the API&#10;server itself says you&#8217;re not authorized to access it, because it doesn&#8217;t know who&#10;you are.&#10;AUTHENTICATING WITH THE API SERVER&#10;You need to authenticate with the server, so it allows you to read and even update&#10;and/or delete the API objects deployed in the cluster. To authenticate, you need an&#10;authentication token. Luckily, the token is provided through the default-token Secret&#10;mentioned previously, and is stored in the token file in the secret volume. As the&#10;Secret&#8217;s name suggests, that&#8217;s the primary purpose of the Secret. &#10; You&#8217;re going to use the token to access the API server. First, load the token into an&#10;environment variable:&#10;root@curl:/# TOKEN=$(cat /var/run/secrets/kubernetes.io/&#10;             &#10149; serviceaccount/token)&#10;The token is now stored in the TOKEN environment variable. You can use it when send-&#10;ing requests to the API server, as shown in the following listing.&#10;root@curl:/# curl -H &#34;Authorization: Bearer $TOKEN&#34; https://kubernetes&#10;{&#10;  &#34;paths&#34;: [&#10;    &#34;/api&#34;,&#10;    &#34;/api/v1&#34;,&#10;    &#34;/apis&#34;,&#10;    &#34;/apis/apps&#34;,&#10;    &#34;/apis/apps/v1beta1&#34;,&#10;    &#34;/apis/authorization.k8s.io&#34;,    &#10;    ...&#10;    &#34;/ui/&#34;,&#10;    &#34;/version&#34;&#10;  ]&#10;}&#10;Listing 8.13&#10;Getting a proper response from the API server&#10; &#10;"
    color "green"
  ]
  node [
    id 372
    label "274"
    title "Page_274"
    color "blue"
  ]
  node [
    id 373
    label "text_185"
    title "242&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10;As you can see, you passed the token inside the Authorization HTTP header in the&#10;request. The API server recognized the token as authentic and returned a proper&#10;response. You can now explore all the resources in your cluster, the way you did a few&#10;sections ago. &#10; For example, you could list all the pods in the same namespace. But first you need&#10;to know what namespace the curl pod is running in.&#10;GETTING THE NAMESPACE THE POD IS RUNNING IN&#10;In the first part of this chapter, you saw how to pass the namespace to the pod&#10;through the Downward API. But if you&#8217;re paying attention, you probably noticed&#10;your secret volume also contains a file called namespace. It contains the name-&#10;space the pod is running in, so you can read the file instead of having to explicitly&#10;pass the namespace to your pod through an environment variable. Load the con-&#10;tents of the file into the NS environment variable and then list all the pods, as shown&#10;in the following listing.&#10;root@curl:/# NS=$(cat /var/run/secrets/kubernetes.io/&#10;             &#10149; serviceaccount/namespace)           &#10;root@curl:/# curl -H &#34;Authorization: Bearer $TOKEN&#34;&#10;             &#10149; https://kubernetes/api/v1/namespaces/$NS/pods&#10;{&#10;  &#34;kind&#34;: &#34;PodList&#34;,&#10;  &#34;apiVersion&#34;: &#34;v1&#34;,&#10;  ...&#10;And there you go. By using the three files in the mounted secret volume directory,&#10;you listed all the pods running in the same namespace as your pod. In the same man-&#10;ner, you could also retrieve other API objects and even update them by sending PUT or&#10;PATCH instead of simple GET requests. &#10;Disabling role-based access control (RBAC)&#10;If you&#8217;re using a Kubernetes cluster with RBAC enabled, the service account may not&#10;be authorized to access (parts of) the API server. You&#8217;ll learn about service accounts&#10;and RBAC in chapter 12. For now, the simplest way to allow you to query the API&#10;server is to work around RBAC by running the following command:&#10;$ kubectl create clusterrolebinding permissive-binding \&#10;  --clusterrole=cluster-admin \&#10;  --group=system:serviceaccounts&#10;This gives all service accounts (we could also say all pods) cluster-admin privileges,&#10;allowing them to do whatever they want. Obviously, doing this is dangerous and&#10;should never be done on production clusters. For test purposes, it&#8217;s fine.&#10;Listing 8.14&#10;Listing pods in the pod&#8217;s own namespace&#10; &#10;"
    color "green"
  ]
  node [
    id 374
    label "275"
    title "Page_275"
    color "blue"
  ]
  node [
    id 375
    label "text_186"
    title "243&#10;Talking to the Kubernetes API server&#10;RECAPPING HOW PODS TALK TO KUBERNETES&#10;Let&#8217;s recap how an app running inside a pod can access the Kubernetes API properly:&#10;&#61601;The app should verify whether the API server&#8217;s certificate is signed by the certif-&#10;icate authority, whose certificate is in the ca.crt file. &#10;&#61601;The app should authenticate itself by sending the Authorization header with&#10;the bearer token from the token file. &#10;&#61601;The namespace file should be used to pass the namespace to the API server when&#10;performing CRUD operations on API objects inside the pod&#8217;s namespace.&#10;DEFINITION&#10;CRUD stands for Create, Read, Update, and Delete. The corre-&#10;sponding HTTP methods are POST, GET, PATCH/PUT, and DELETE, respectively.&#10;All three aspects of pod to API server communication are displayed in figure 8.5.&#10;8.2.3&#10;Simplifying API server communication with ambassador &#10;containers&#10;Dealing with HTTPS, certificates, and authentication tokens sometimes seems too&#10;complicated to developers. I&#8217;ve seen developers disable validation of server certifi-&#10;cates on way too many occasions (and I&#8217;ll admit to doing it myself a few times). Luck-&#10;ily, you can make the communication much simpler while keeping it secure. &#10;API server&#10;GET /api/v1/namespaces/<namespace>/pods&#10;Authorization: Bearer <token>&#10;Pod&#10;Container&#10;Filesystem&#10;App&#10;/&#10;var/&#10;run/&#10;secrets/&#10;kubernetes.io/&#10;serviceaccount/&#10;Default token secret volume&#10;ca.crt&#10;token&#10;namespace&#10;Server&#10;certi&#64257;cate&#10;Validate&#10;certi&#64257;cate&#10;Figure 8.5&#10;Using the files from the default-token Secret to talk to the API server&#10; &#10;"
    color "green"
  ]
  node [
    id 376
    label "276"
    title "Page_276"
    color "blue"
  ]
  node [
    id 377
    label "text_187"
    title "244&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10; Remember the kubectl proxy command we mentioned in section 8.2.1? You ran&#10;the command on your local machine to make it easier to access the API server. Instead&#10;of sending requests to the API server directly, you sent them to the proxy and let it&#10;take care of authentication, encryption, and server verification. The same method can&#10;be used inside your pods, as well.&#10;INTRODUCING THE AMBASSADOR CONTAINER PATTERN&#10;Imagine having an application that (among other things) needs to query the API&#10;server. Instead of it talking to the API server directly, as you did in the previous sec-&#10;tion, you can run kubectl proxy in an ambassador container alongside the main con-&#10;tainer and communicate with the API server through it. &#10; Instead of talking to the API server directly, the app in the main container can con-&#10;nect to the ambassador through HTTP (instead of HTTPS) and let the ambassador&#10;proxy handle the HTTPS connection to the API server, taking care of security trans-&#10;parently (see figure 8.6). It does this by using the files from the default token&#8217;s secret&#10;volume.&#10;Because all containers in a pod share the same loopback network interface, your app&#10;can access the proxy through a port on localhost.&#10;RUNNING THE CURL POD WITH AN ADDITIONAL AMBASSADOR CONTAINER&#10;To see the ambassador container pattern in action, you&#8217;ll create a new pod like the&#10;curl pod you created earlier, but this time, instead of running a single container in&#10;the pod, you&#8217;ll run an additional ambassador container based on a general-purpose&#10;kubectl-proxy container image I&#8217;ve created and pushed to Docker Hub. You&#8217;ll find&#10;the Dockerfile for the image in the code archive (in /Chapter08/kubectl-proxy/) if&#10;you want to build it yourself.&#10; The pod&#8217;s manifest is shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: curl-with-ambassador&#10;spec:&#10;  containers:&#10;  - name: main&#10;Listing 8.15&#10;A pod with an ambassador container: curl-with-ambassador.yaml&#10;Container:&#10;main&#10;Container:&#10;ambassador&#10;HTTP&#10;HTTPS&#10;API server&#10;Pod&#10;Figure 8.6&#10;Using an ambassador to connect to the API server&#10; &#10;"
    color "green"
  ]
  node [
    id 378
    label "277"
    title "Page_277"
    color "blue"
  ]
  node [
    id 379
    label "text_188"
    title "245&#10;Talking to the Kubernetes API server&#10;    image: tutum/curl&#10;    command: [&#34;sleep&#34;, &#34;9999999&#34;]&#10;  - name: ambassador                         &#10;    image: luksa/kubectl-proxy:1.6.2         &#10;The pod spec is almost the same as before, but with a different pod name and an addi-&#10;tional container. Run the pod and then enter the main container with&#10;$ kubectl exec -it curl-with-ambassador -c main bash&#10;root@curl-with-ambassador:/#&#10;Your pod now has two containers, and you want to run bash in the main container,&#10;hence the -c main option. You don&#8217;t need to specify the container explicitly if you&#10;want to run the command in the pod&#8217;s first container. But if you want to run a com-&#10;mand inside any other container, you do need to specify the container&#8217;s name using&#10;the -c option.&#10;TALKING TO THE API SERVER THROUGH THE AMBASSADOR&#10;Next you&#8217;ll try connecting to the API server through the ambassador container. By&#10;default, kubectl proxy binds to port 8001, and because both containers in the pod&#10;share the same network interfaces, including loopback, you can point curl to local-&#10;host:8001, as shown in the following listing.&#10;root@curl-with-ambassador:/# curl localhost:8001&#10;{&#10;  &#34;paths&#34;: [&#10;    &#34;/api&#34;,&#10;    ...&#10;  ]&#10;}&#10;Success! The output printed by curl is the same response you saw earlier, but this time&#10;you didn&#8217;t need to deal with authentication tokens and server certificates. &#10; To get a clear picture of what exactly happened, refer to figure 8.7. curl sent the&#10;plain HTTP request (without any authentication headers) to the proxy running inside&#10;the ambassador container, and then the proxy sent an HTTPS request to the API&#10;server, handling the client authentication by sending the token and checking the&#10;server&#8217;s identity by validating its certificate.&#10; This is a great example of how an ambassador container can be used to hide the&#10;complexities of connecting to an external service and simplify the app running in&#10;the main container. The ambassador container is reusable across many different apps,&#10;regardless of what language the main app is written in. The downside is that an addi-&#10;tional process is running and consuming additional resources.&#10;Listing 8.16&#10;Accessing the API server through the ambassador container&#10;The ambassador container, &#10;running the kubectl-proxy image&#10; &#10;"
    color "green"
  ]
  node [
    id 380
    label "278"
    title "Page_278"
    color "blue"
  ]
  node [
    id 381
    label "text_189"
    title "246&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10;8.2.4&#10;Using client libraries to talk to the API server&#10;If your app only needs to perform a few simple operations on the API server, you can&#10;often use a regular HTTP client library and perform simple HTTP requests, especially&#10;if you take advantage of the kubectl-proxy ambassador container the way you did in&#10;the previous example. But if you plan on doing more than simple API requests, it&#8217;s&#10;better to use one of the existing Kubernetes API client libraries.&#10;USING EXISTING CLIENT LIBRARIES&#10;Currently, two Kubernetes API client libraries exist that are supported by the API&#10;Machinery special interest group (SIG):&#10;&#61601;Golang client&#8212;https:/&#10;/github.com/kubernetes/client-go&#10;&#61601;Python&#8212;https:/&#10;/github.com/kubernetes-incubator/client-python&#10;NOTE&#10;The Kubernetes community has a number of Special Interest Groups&#10;(SIGs) and Working Groups that focus on specific parts of the Kubernetes&#10;ecosystem. You&#8217;ll find a list of them at https:/&#10;/github.com/kubernetes/com-&#10;munity/blob/master/sig-list.md.&#10;In addition to the two officially supported libraries, here&#8217;s a list of user-contributed cli-&#10;ent libraries for many other languages:&#10;&#61601;Java client by Fabric8&#8212;https:/&#10;/github.com/fabric8io/kubernetes-client&#10;&#61601;Java client by Amdatu&#8212;https:/&#10;/bitbucket.org/amdatulabs/amdatu-kubernetes&#10;&#61601;Node.js client by tenxcloud&#8212;https:/&#10;/github.com/tenxcloud/node-kubernetes-client&#10;&#61601;Node.js client by GoDaddy&#8212;https:/&#10;/github.com/godaddy/kubernetes-client&#10;&#61601;PHP&#8212;https:/&#10;/github.com/devstub/kubernetes-api-php-client&#10;&#61601;Another PHP client&#8212;https:/&#10;/github.com/maclof/kubernetes-client&#10;Container: main&#10;API server&#10;sleep&#10;curl&#10;Container: ambassador&#10;kubectl proxy&#10;Port 8001&#10;GET http://localhost:8001&#10;GET https://kubernetes:443&#10;Authorization: Bearer <token>&#10;Pod&#10;Figure 8.7&#10;Offloading encryption, authentication, and server verification to kubectl proxy in an &#10;ambassador container &#10; &#10;"
    color "green"
  ]
  node [
    id 382
    label "279"
    title "Page_279"
    color "blue"
  ]
  node [
    id 383
    label "text_190"
    title "247&#10;Talking to the Kubernetes API server&#10;&#61601;Ruby&#8212;https:/&#10;/github.com/Ch00k/kubr&#10;&#61601;Another Ruby client&#8212;https:/&#10;/github.com/abonas/kubeclient&#10;&#61601;Clojure&#8212;https:/&#10;/github.com/yanatan16/clj-kubernetes-api&#10;&#61601;Scala&#8212;https:/&#10;/github.com/doriordan/skuber&#10;&#61601;Perl&#8212;https:/&#10;/metacpan.org/pod/Net::Kubernetes&#10;These libraries usually support HTTPS and take care of authentication, so you won&#8217;t&#10;need to use the ambassador container. &#10;AN EXAMPLE OF INTERACTING WITH KUBERNETES WITH THE FABRIC8 JAVA CLIENT&#10;To give you a sense of how client libraries enable you to talk to the API server, the fol-&#10;lowing listing shows an example of how to list services in a Java app using the Fabric8&#10;Kubernetes client.&#10;import java.util.Arrays;&#10;import io.fabric8.kubernetes.api.model.Pod;&#10;import io.fabric8.kubernetes.api.model.PodList;&#10;import io.fabric8.kubernetes.client.DefaultKubernetesClient;&#10;import io.fabric8.kubernetes.client.KubernetesClient;&#10;public class Test {&#10;  public static void main(String[] args) throws Exception {&#10;    KubernetesClient client = new DefaultKubernetesClient();&#10;    // list pods in the default namespace&#10;    PodList pods = client.pods().inNamespace(&#34;default&#34;).list();&#10;    pods.getItems().stream()&#10;      .forEach(s -> System.out.println(&#34;Found pod: &#34; +&#10;               s.getMetadata().getName()));&#10;    // create a pod&#10;    System.out.println(&#34;Creating a pod&#34;);&#10;    Pod pod = client.pods().inNamespace(&#34;default&#34;)&#10;      .createNew()&#10;      .withNewMetadata()&#10;        .withName(&#34;programmatically-created-pod&#34;)&#10;      .endMetadata()&#10;      .withNewSpec()&#10;        .addNewContainer()&#10;          .withName(&#34;main&#34;)&#10;          .withImage(&#34;busybox&#34;)&#10;          .withCommand(Arrays.asList(&#34;sleep&#34;, &#34;99999&#34;))&#10;        .endContainer()&#10;      .endSpec()&#10;      .done();&#10;    System.out.println(&#34;Created pod: &#34; + pod);&#10;    // edit the pod (add a label to it)&#10;    client.pods().inNamespace(&#34;default&#34;)&#10;      .withName(&#34;programmatically-created-pod&#34;)&#10;      .edit()&#10;      .editMetadata()&#10;Listing 8.17&#10;Listing, creating, updating, and deleting pods with the Fabric8 Java client&#10; &#10;"
    color "green"
  ]
  node [
    id 384
    label "280"
    title "Page_280"
    color "blue"
  ]
  node [
    id 385
    label "text_191"
    title "248&#10;CHAPTER 8&#10;Accessing pod metadata and other resources from applications&#10;        .addToLabels(&#34;foo&#34;, &#34;bar&#34;)&#10;      .endMetadata()&#10;      .done();&#10;    System.out.println(&#34;Added label foo=bar to pod&#34;);&#10;    System.out.println(&#34;Waiting 1 minute before deleting pod...&#34;);&#10;    Thread.sleep(60000);&#10;    // delete the pod&#10;    client.pods().inNamespace(&#34;default&#34;)&#10;      .withName(&#34;programmatically-created-pod&#34;)&#10;      .delete();&#10;    System.out.println(&#34;Deleted the pod&#34;);&#10;  }&#10;}&#10;The code should be self-explanatory, especially because the Fabric8 client exposes&#10;a nice, fluent Domain-Specific-Language (DSL) API, which is easy to read and&#10;understand.&#10;BUILDING YOUR OWN LIBRARY WITH SWAGGER AND OPENAPI&#10;If no client is available for your programming language of choice, you can use the&#10;Swagger API framework to generate the client library and documentation. The Kuber-&#10;netes API server exposes Swagger API definitions at /swaggerapi and OpenAPI spec at&#10;/swagger.json. &#10; To find out more about the Swagger framework, visit the website at http:/&#10;/swagger.io.&#10;EXPLORING THE API WITH SWAGGER UI&#10;Earlier in the chapter I said I&#8217;d point you to a better way of exploring the REST API&#10;instead of hitting the REST endpoints with curl. Swagger, which I mentioned in the&#10;previous section, is not just a tool for specifying an API, but also provides a web UI for&#10;exploring REST APIs if they expose the Swagger API definitions. The better way of&#10;exploring REST APIs is through this UI.&#10; Kubernetes not only exposes the Swagger API, but it also has Swagger UI inte-&#10;grated into the API server, though it&#8217;s not enabled by default. You can enable it by&#10;running the API server with the --enable-swagger-ui=true option.&#10;TIP&#10;If you&#8217;re using Minikube, you can enable Swagger UI when starting the&#10;cluster: minikube start --extra-config=apiserver.Features.Enable-&#10;SwaggerUI=true&#10;After you enable the UI, you can open it in your browser by pointing it to:&#10;http(s)://<api server>:<port>/swagger-ui&#10;I urge you to give Swagger UI a try. It not only allows you to browse the Kubernetes&#10;API, but also interact with it (you can POST JSON resource manifests, PATCH resources,&#10;or DELETE them, for example). &#10; &#10;"
    color "green"
  ]
  node [
    id 386
    label "281"
    title "Page_281"
    color "blue"
  ]
  node [
    id 387
    label "text_192"
    title "249&#10;Summary&#10;8.3&#10;Summary&#10;After reading this chapter, you now know how your app, running inside a pod, can get&#10;data about itself, other pods, and other components deployed in the cluster. You&#8217;ve&#10;learned&#10;&#61601;How a pod&#8217;s name, namespace, and other metadata can be exposed to the pro-&#10;cess either through environment variables or files in a downwardAPI volume&#10;&#61601;How CPU and memory requests and limits are passed to your app in any unit&#10;the app requires&#10;&#61601;How a pod can use downwardAPI volumes to get up-to-date metadata, which&#10;may change during the lifetime of the pod (such as labels and annotations) &#10;&#61601;How you can browse the Kubernetes REST API through kubectl proxy&#10;&#61601;How pods can find the API server&#8217;s location through environment variables or&#10;DNS, similar to any other Service defined in Kubernetes&#10;&#61601;How an application running in a pod can verify that it&#8217;s talking to the API&#10;server and how it can authenticate itself&#10;&#61601;How using an ambassador container can make talking to the API server from&#10;within an app much simpler&#10;&#61601;How client libraries can get you interacting with Kubernetes in minutes&#10;In this chapter, you learned how to talk to the API server, so the next step is learning&#10;more about how it works. You&#8217;ll do that in chapter 11, but before we dive into such&#10;details, you still need to learn about two other Kubernetes resources&#8212;Deployments&#10;and StatefulSets. They&#8217;re explained in the next two chapters.&#10; &#10;"
    color "green"
  ]
  node [
    id 388
    label "282"
    title "Page_282"
    color "blue"
  ]
  node [
    id 389
    label "text_193"
    title "250&#10;Deployments: updating&#10;applications declaratively&#10;You now know how to package your app components into containers, group them&#10;into pods, provide them with temporary or permanent storage, pass both secret&#10;and non-secret config data to them, and allow pods to find and talk to each other.&#10;You know how to run a full-fledged system composed of independently running&#10;smaller components&#8212;microservices, if you will. Is there anything else? &#10; Eventually, you&#8217;re going to want to update your app. This chapter covers how to&#10;update apps running in a Kubernetes cluster and how Kubernetes helps you move&#10;toward a true zero-downtime update process. Although this can be achieved using&#10;only ReplicationControllers or ReplicaSets, Kubernetes also provides a Deployment&#10;This chapter covers&#10;&#61601;Replacing pods with newer versions&#10;&#61601;Updating managed pods&#10;&#61601;Updating pods declaratively using Deployment &#10;resources&#10;&#61601;Performing rolling updates&#10;&#61601;Automatically blocking rollouts of bad versions&#10;&#61601;Controlling the rate of the rollout&#10;&#61601;Reverting pods to a previous version&#10; &#10;"
    color "green"
  ]
  node [
    id 390
    label "283"
    title "Page_283"
    color "blue"
  ]
  node [
    id 391
    label "text_194"
    title "251&#10;Updating applications running in pods&#10;resource that sits on top of ReplicaSets and enables declarative application updates. If&#10;you&#8217;re not completely sure what that means, keep reading&#8212;it&#8217;s not as complicated as&#10;it sounds.&#10;9.1&#10;Updating applications running in pods&#10;Let&#8217;s start off with a simple example. Imagine having a set of pod instances providing a&#10;service to other pods and/or external clients. After reading this book up to this point,&#10;you likely recognize that these pods are backed by a ReplicationController or a&#10;ReplicaSet. A Service also exists through which clients (apps running in other pods or&#10;external clients) access the pods. This is how a basic application looks in Kubernetes&#10;(shown in figure 9.1).&#10;Initially, the pods run the first version of your application&#8212;let&#8217;s suppose its image is&#10;tagged as v1. You then develop a newer version of the app and push it to an image&#10;repository as a new image, tagged as v2. You&#8217;d next like to replace all the pods with&#10;this new version. Because you can&#8217;t change an existing pod&#8217;s image after the pod is&#10;created, you need to remove the old pods and replace them with new ones running&#10;the new image. &#10; You have two ways of updating all those pods. You can do one of the following:&#10;&#61601;Delete all existing pods first and then start the new ones.&#10;&#61601;Start new ones and, once they&#8217;re up, delete the old ones. You can do this either&#10;by adding all the new pods and then deleting all the old ones at once, or&#10;sequentially, by adding new pods and removing old ones gradually.&#10;Both these strategies have their benefits and drawbacks. The first option would lead to&#10;a short period of time when your application is unavailable. The second option&#10;requires your app to handle running two versions of the app at the same time. If your&#10;app stores data in a data store, the new version shouldn&#8217;t modify the data schema or&#10;the data in such a way that breaks the previous version.&#10;ReplicationController&#10;or ReplicaSet&#10;Clients&#10;Service&#10;Pod&#10;Pod&#10;Pod&#10;Figure 9.1&#10;The basic outline of an &#10;application running in Kubernetes&#10; &#10;"
    color "green"
  ]
  node [
    id 392
    label "284"
    title "Page_284"
    color "blue"
  ]
  node [
    id 393
    label "text_195"
    title "252&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10; How do you perform these two update methods in Kubernetes? First, let&#8217;s look at&#10;how to do this manually; then, once you know what&#8217;s involved in the process, you&#8217;ll&#10;learn how to have Kubernetes perform the update automatically.&#10;9.1.1&#10;Deleting old pods and replacing them with new ones&#10;You already know how to get a ReplicationController to replace all its pod instances&#10;with pods running a new version. You probably remember the pod template of a&#10;ReplicationController can be updated at any time. When the ReplicationController&#10;creates new instances, it uses the updated pod template to create them.&#10; If you have a ReplicationController managing a set of v1 pods, you can easily&#10;replace them by modifying the pod template so it refers to version v2 of the image and&#10;then deleting the old pod instances. The ReplicationController will notice that no&#10;pods match its label selector and it will spin up new instances. The whole process is&#10;shown in figure 9.2.&#10;This is the simplest way to update a set of pods, if you can accept the short downtime&#10;between the time the old pods are deleted and new ones are started.&#10;9.1.2&#10;Spinning up new pods and then deleting the old ones&#10;If you don&#8217;t want to see any downtime and your app supports running multiple ver-&#10;sions at once, you can turn the process around and first spin up all the new pods and&#10;Pod template&#10;changed&#10;v pods deleted&#10;1&#10;manually&#10;ReplicationController&#10;Service&#10;Pod: v1&#10;Pod: v1&#10;Pod&#10;template: v2&#10;ReplicationController&#10;Pod&#10;template: v2&#10;Pod: v1&#10;Service&#10;Pod: v2&#10;Pod: v2&#10;Pod: v2&#10;ReplicationController&#10;Service&#10;Pod: v1&#10;Pod: v1&#10;Pod&#10;template: v1&#10;Pod: v1&#10;ReplicationController&#10;Service&#10;Pod: v1&#10;Pod: v1&#10;Pod: v1&#10;Pod&#10;template: v2&#10;Short period of&#10;downtime here&#10;v2 pods created by&#10;ReplicationController&#10;Figure 9.2&#10;Updating pods by changing a ReplicationController&#8217;s pod template and deleting old Pods&#10; &#10;"
    color "green"
  ]
  node [
    id 394
    label "285"
    title "Page_285"
    color "blue"
  ]
  node [
    id 395
    label "text_196"
    title "253&#10;Updating applications running in pods&#10;only then delete the old ones. This will require more hardware resources, because&#10;you&#8217;ll have double the number of pods running at the same time for a short while. &#10; This is a slightly more complex method compared to the previous one, but you&#10;should be able to do it by combining what you&#8217;ve learned about ReplicationControl-&#10;lers and Services so far.&#10;SWITCHING FROM THE OLD TO THE NEW VERSION AT ONCE&#10;Pods are usually fronted by a Service. It&#8217;s possible to have the Service front only the&#10;initial version of the pods while you bring up the pods running the new version. Then,&#10;once all the new pods are up, you can change the Service&#8217;s label selector and have the&#10;Service switch over to the new pods, as shown in figure 9.3. This is called a blue-green&#10;deployment. After switching over, and once you&#8217;re sure the new version functions cor-&#10;rectly, you&#8217;re free to delete the old pods by deleting the old ReplicationController.&#10;NOTE&#10;You can change a Service&#8217;s pod selector with the kubectl set selec-&#10;tor command.&#10;PERFORMING A ROLLING UPDATE&#10;Instead of bringing up all the new pods and deleting the old pods at once, you can&#10;also perform a rolling update, which replaces pods step by step. You do this by slowly&#10;scaling down the previous ReplicationController and scaling up the new one. In this&#10;case, you&#8217;ll want the Service&#8217;s pod selector to include both the old and the new pods,&#10;so it directs requests toward both sets of pods. See figure 9.4.&#10; Doing a rolling update manually is laborious and error-prone. Depending on the&#10;number of replicas, you&#8217;d need to run a dozen or more commands in the proper&#10;order to perform the update process. Luckily, Kubernetes allows you to perform the&#10;rolling update with a single command. You&#8217;ll learn how in the next section.&#10;Service&#10;Service&#10;ReplicationController:&#10;v1&#10;Pod: v1&#10;Pod: v1&#10;Pod&#10;template: v1&#10;Pod: v1&#10;ReplicationController:&#10;v2&#10;Pod&#10;template: v2&#10;Pod: v2&#10;Pod: v2&#10;Pod: v2&#10;ReplicationController:&#10;v1&#10;Pod: v1&#10;Pod: v1&#10;Pod&#10;template: v1&#10;Pod: v1&#10;ReplicationController:&#10;v2&#10;Pod&#10;template: v2&#10;Pod: v2&#10;Pod: v2&#10;Pod: v2&#10;Figure 9.3&#10;Switching a Service from the old pods to the new ones&#10; &#10;"
    color "green"
  ]
  node [
    id 396
    label "286"
    title "Page_286"
    color "blue"
  ]
  node [
    id 397
    label "text_197"
    title "254&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;9.2&#10;Performing an automatic rolling update with a &#10;ReplicationController&#10;Instead of performing rolling updates using ReplicationControllers manually, you can&#10;have kubectl perform them. Using kubectl to perform the update makes the process&#10;much easier, but, as you&#8217;ll see later, this is now an outdated way of updating apps. Nev-&#10;ertheless, we&#8217;ll walk through this option first, because it was historically the first way of&#10;doing an automatic rolling update, and also allows us to discuss the process without&#10;introducing too many additional concepts. &#10;9.2.1&#10;Running the initial version of the app&#10;Obviously, before you can update an app, you need to have an app deployed. You&#8217;re&#10;going to use a slightly modified version of the kubia NodeJS app you created in chap-&#10;ter 2 as your initial version. In case you don&#8217;t remember what it does, it&#8217;s a simple web-&#10;app that returns the pod&#8217;s hostname in the HTTP response. &#10;CREATING THE V1 APP&#10;You&#8217;ll change the app so it also returns its version number in the response, which will&#10;allow you to distinguish between the different versions you&#8217;re about to build. I&#8217;ve&#10;already built and pushed the app image to Docker Hub under luksa/kubia:v1. The&#10;following listing shows the app&#8217;s code.&#10;const http = require('http');&#10;const os = require('os');&#10;console.log(&#34;Kubia server starting...&#34;);&#10;Listing 9.1&#10;The v1 version of our app: v1/app.js&#10;Service&#10;Pod: v1&#10;Pod: v1&#10;Replication&#10;Controller:&#10;v1&#10;v1&#10;Replication&#10;Controller:&#10;v2&#10;Pod: v2&#10;Service&#10;Pod: v2&#10;Pod: v2&#10;Pod: v2&#10;Service&#10;Pod: v1&#10;Pod: v1&#10;Pod: v1&#10;Service&#10;Pod: v1&#10;Pod: v2&#10;Pod: v2&#10;v2&#10;Replication&#10;Controller:&#10;v1&#10;v1&#10;Replication&#10;Controller:&#10;v2&#10;v2&#10;Replication&#10;Controller:&#10;v1&#10;Replication&#10;Controller:&#10;v2&#10;v2&#10;Replication&#10;Controller:&#10;v1&#10;v1&#10;v1&#10;Replication&#10;Controller:&#10;v2&#10;v2&#10;Figure 9.4&#10;A rolling update of pods using two ReplicationControllers&#10; &#10;"
    color "green"
  ]
  node [
    id 398
    label "287"
    title "Page_287"
    color "blue"
  ]
  node [
    id 399
    label "text_198"
    title "255&#10;Performing an automatic rolling update with a ReplicationController&#10;var handler = function(request, response) {&#10;  console.log(&#34;Received request from &#34; + request.connection.remoteAddress);&#10;  response.writeHead(200);&#10;  response.end(&#34;This is v1 running in pod &#34; + os.hostname() + &#34;\n&#34;);&#10;};&#10;var www = http.createServer(handler);&#10;www.listen(8080);&#10;RUNNING THE APP AND EXPOSING IT THROUGH A SERVICE USING A SINGLE YAML FILE&#10;To run your app, you&#8217;ll create a ReplicationController and a LoadBalancer Service to&#10;enable you to access the app externally. This time, rather than create these two&#10;resources separately, you&#8217;ll create a single YAML for both of them and post it to the&#10;Kubernetes API with a single kubectl create command. A YAML manifest can con-&#10;tain multiple objects delimited with a line containing three dashes, as shown in the&#10;following listing.&#10;apiVersion: v1&#10;kind: ReplicationController&#10;metadata:&#10;  name: kubia-v1&#10;spec:&#10;  replicas: 3&#10;  template:&#10;    metadata:&#10;      name: kubia&#10;      labels:                      &#10;        app: kubia                 &#10;    spec:&#10;      containers:&#10;      - image: luksa/kubia:v1     &#10;        name: nodejs&#10;---                         &#10;apiVersion: v1&#10;kind: Service&#10;metadata:&#10;  name: kubia&#10;spec:&#10;  type: LoadBalancer&#10;  selector:                                        &#10;    app: kubia                                     &#10;  ports:&#10;  - port: 80&#10;    targetPort: 8080&#10;The YAML defines a ReplicationController called kubia-v1 and a Service called&#10;kubia. Go ahead and post the YAML to Kubernetes. After a while, your three v1 pods&#10;and the load balancer should all be running, so you can look up the Service&#8217;s external&#10;IP and start hitting the service with curl, as shown in the following listing.&#10;Listing 9.2&#10;A YAML containing an RC and a Service: kubia-rc-and-service-v1.yaml&#10;The Service fronts all &#10;pods created by the &#10;ReplicationController.&#10;You&#8217;re creating a &#10;ReplicationController for &#10;pods running this image.&#10;YAML files can contain &#10;multiple resource &#10;definitions separated by &#10;a line with three dashes.&#10; &#10;"
    color "green"
  ]
  node [
    id 400
    label "288"
    title "Page_288"
    color "blue"
  ]
  node [
    id 401
    label "text_199"
    title "256&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;$ kubectl get svc kubia&#10;NAME      CLUSTER-IP     EXTERNAL-IP       PORT(S)         AGE&#10;kubia     10.3.246.195   130.211.109.222   80:32143/TCP    5m&#10;$ while true; do curl http://130.211.109.222; done&#10;This is v1 running in pod kubia-v1-qr192&#10;This is v1 running in pod kubia-v1-kbtsk&#10;This is v1 running in pod kubia-v1-qr192&#10;This is v1 running in pod kubia-v1-2321o&#10;...&#10;NOTE&#10;If you&#8217;re using Minikube or any other Kubernetes cluster where load&#10;balancer services aren&#8217;t supported, you can use the Service&#8217;s node port to&#10;access the app. This was explained in chapter 5.&#10;9.2.2&#10;Performing a rolling update with kubectl&#10;Next you&#8217;ll create version 2 of the app. To keep things simple, all you&#8217;ll do is change&#10;the response to say, &#8220;This is v2&#8221;:&#10;  response.end(&#34;This is v2 running in pod &#34; + os.hostname() + &#34;\n&#34;);&#10;This new version is available in the image luksa/kubia:v2 on Docker Hub, so you&#10;don&#8217;t need to build it yourself.&#10;Listing 9.3&#10;Getting the Service&#8217;s external IP and hitting the service in a loop with curl&#10;Pushing updates to the same image tag&#10;Modifying an app and pushing the changes to the same image tag isn&#8217;t a good idea,&#10;but we all tend to do that during development. If you&#8217;re modifying the latest tag,&#10;that&#8217;s not a problem, but when you&#8217;re tagging an image with a different tag (for exam-&#10;ple, tag v1 instead of latest), once the image is pulled by a worker node, the image&#10;will be stored on the node and not pulled again when a new pod using the same&#10;image is run (at least that&#8217;s the default policy for pulling images).&#10;That means any changes you make to the image won&#8217;t be picked up if you push them&#10;to the same tag. If a new pod is scheduled to the same node, the Kubelet will run the&#10;old version of the image. On the other hand, nodes that haven&#8217;t run the old version&#10;will pull and run the new image, so you might end up with two different versions of&#10;the pod running. To make sure this doesn&#8217;t happen, you need to set the container&#8217;s&#10;imagePullPolicy property to Always. &#10;You need to be aware that the default imagePullPolicy depends on the image tag.&#10;If a container refers to the latest tag (either explicitly or by not specifying the tag at&#10;all), imagePullPolicy defaults to Always, but if the container refers to any other&#10;tag, the policy defaults to IfNotPresent. &#10;When using a tag other than latest, you need to set the imagePullPolicy properly&#10;if you make changes to an image without changing the tag. Or better yet, make sure&#10;you always push changes to an image under a new tag.&#10; &#10;"
    color "green"
  ]
  node [
    id 402
    label "289"
    title "Page_289"
    color "blue"
  ]
  node [
    id 403
    label "text_200"
    title "257&#10;Performing an automatic rolling update with a ReplicationController&#10;Keep the curl loop running and open another terminal, where you&#8217;ll get the rolling&#10;update started. To perform the update, you&#8217;ll run the kubectl rolling-update com-&#10;mand. All you need to do is tell it which ReplicationController you&#8217;re replacing, give a&#10;name for the new ReplicationController, and specify the new image you&#8217;d like to&#10;replace the original one with. The following listing shows the full command for per-&#10;forming the rolling update.&#10;$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2&#10;Created kubia-v2&#10;Scaling up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0 (keep 3 &#10;pods available, don't exceed 4 pods)&#10;...&#10;Because you&#8217;re replacing ReplicationController kubia-v1 with one running version 2&#10;of your kubia app, you&#8217;d like the new ReplicationController to be called kubia-v2&#10;and use the luksa/kubia:v2 container image. &#10; When you run the command, a new ReplicationController called kubia-v2 is cre-&#10;ated immediately. The state of the system at this point is shown in figure 9.5.&#10;The new ReplicationController&#8217;s pod template references the luksa/kubia:v2 image&#10;and its initial desired replica count is set to 0, as you can see in the following listing.&#10;$ kubectl describe rc kubia-v2&#10;Name:       kubia-v2&#10;Namespace:  default&#10;Image(s):   luksa/kubia:v2          &#10;Selector:   app=kubia,deployment=757d16a0f02f6a5c387f2b5edb62b155&#10;Labels:     app=kubia            &#10;Replicas:   0 current / 0 desired    &#10;...&#10;Listing 9.4&#10;Initiating a rolling-update of a ReplicationController using kubectl&#10;Listing 9.5&#10;Describing the new ReplicationController created by the rolling update&#10;Pod: v1&#10;Pod: v1&#10;No v2 pods yet&#10;Pod: v1&#10;ReplicationController: kubia-v1&#10;Image: kubia/v1&#10;Replicas: 3&#10;ReplicationController: kubia-v2&#10;Image: kubia/v2&#10;Replicas: 0&#10;Figure 9.5&#10;The state of the system immediately after starting the rolling update&#10;The new &#10;ReplicationController &#10;refers to the v2 image.&#10;Initially, the desired &#10;number of replicas is zero.&#10; &#10;"
    color "green"
  ]
  node [
    id 404
    label "290"
    title "Page_290"
    color "blue"
  ]
  node [
    id 405
    label "text_201"
    title "258&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;UNDERSTANDING THE STEPS PERFORMED BY KUBECTL BEFORE THE ROLLING UPDATE COMMENCES&#10;kubectl created this ReplicationController by copying the kubia-v1 controller and&#10;changing the image in its pod template. If you look closely at the controller&#8217;s label&#10;selector, you&#8217;ll notice it has been modified, too. It includes not only a simple&#10;app=kubia label, but also an additional deployment label which the pods must have in&#10;order to be managed by this ReplicationController.&#10; You probably know this already, but this is necessary to avoid having both the new&#10;and the old ReplicationControllers operating on the same set of pods. But even if pods&#10;created by the new controller have the additional deployment label in addition to the&#10;app=kubia label, doesn&#8217;t this mean they&#8217;ll be selected by the first ReplicationControl-&#10;ler&#8217;s selector, because it&#8217;s set to app=kubia? &#10; Yes, that&#8217;s exactly what would happen, but there&#8217;s a catch. The rolling-update pro-&#10;cess has modified the selector of the first ReplicationController, as well:&#10;$ kubectl describe rc kubia-v1&#10;Name:       kubia-v1&#10;Namespace:  default&#10;Image(s):   luksa/kubia:v1&#10;Selector:   app=kubia,deployment=3ddd307978b502a5b975ed4045ae4964-orig &#10;Okay, but doesn&#8217;t this mean the first controller now sees zero pods matching its selec-&#10;tor, because the three pods previously created by it contain only the app=kubia label?&#10;No, because kubectl had also modified the labels of the live pods just before modify-&#10;ing the ReplicationController&#8217;s selector:&#10;$ kubectl get po --show-labels&#10;NAME            READY  STATUS   RESTARTS  AGE  LABELS&#10;kubia-v1-m33mv  1/1    Running  0         2m   app=kubia,deployment=3ddd...&#10;kubia-v1-nmzw9  1/1    Running  0         2m   app=kubia,deployment=3ddd...&#10;kubia-v1-cdtey  1/1    Running  0         2m   app=kubia,deployment=3ddd...&#10;If this is getting too complicated, examine figure 9.6, which shows the pods, their&#10;labels, and the two ReplicationControllers, along with their pod selectors.&#10;ReplicationController: kubia-v1&#10;Replicas: 3&#10;Selector: app=kubia,&#10;deployment=3ddd&#8230;&#10;ReplicationController: kubia-v2&#10;Replicas: 0&#10;Selector: app=kubia,&#10;deployment=757d...&#10;deployment: 3ddd...&#10;app: kubia&#10;Pod: v1&#10;deployment: 3ddd...&#10;app: kubia&#10;Pod: v1&#10;deployment: 3ddd...&#10;app: kubia&#10;Pod: v1&#10;Figure 9.6&#10;Detailed state of the old and new ReplicationControllers and pods at the start of a rolling &#10;update&#10; &#10;"
    color "green"
  ]
  node [
    id 406
    label "291"
    title "Page_291"
    color "blue"
  ]
  node [
    id 407
    label "text_202"
    title "259&#10;Performing an automatic rolling update with a ReplicationController&#10;kubectl had to do all this before even starting to scale anything up or down. Now&#10;imagine doing the rolling update manually. It&#8217;s easy to see yourself making a mistake&#10;here and possibly having the ReplicationController kill off all your pods&#8212;pods that&#10;are actively serving your production clients!&#10;REPLACING OLD PODS WITH NEW ONES BY SCALING THE TWO REPLICATIONCONTROLLERS&#10;After setting up all this, kubectl starts replacing pods by first scaling up the new&#10;controller to 1. The controller thus creates the first v2 pod. kubectl then scales&#10;down the old ReplicationController by 1. This is shown in the next two lines printed&#10;by kubectl:&#10;Scaling kubia-v2 up to 1&#10;Scaling kubia-v1 down to 2&#10;Because the Service is targeting all pods with the app=kubia label, you should start see-&#10;ing your curl requests redirected to the new v2 pod every few loop iterations:&#10;This is v2 running in pod kubia-v2-nmzw9      &#10;This is v1 running in pod kubia-v1-kbtsk&#10;This is v1 running in pod kubia-v1-2321o&#10;This is v2 running in pod kubia-v2-nmzw9      &#10;...&#10;Figure 9.7 shows the current state of the system.&#10;As kubectl continues with the rolling update, you start seeing a progressively bigger&#10;percentage of requests hitting v2 pods, as the update process deletes more of the v1&#10;pods and replaces them with those running your new image. Eventually, the original&#10;Requests hitting the pod &#10;running the new version&#10;ReplicationController: kubia-v1&#10;Replicas: 2&#10;Selector: app=kubia,&#10;deployment=3ddd&#8230;&#10;ReplicationController: kubia-v2&#10;Replicas: 1&#10;Selector: app=kubia,&#10;deployment=757d&#8230;&#10;deployment: 3ddd...&#10;app: kubia&#10;Pod: v1&#10;deployment: 3ddd...&#10;app: kubia&#10;Pod: v1&#10;deployment: 757d...&#10;app: kubia&#10;Pod: v2&#10;curl&#10;Service&#10;Selector: app=kubia&#10;Figure 9.7&#10;The Service is redirecting requests to both the old and new pods during the &#10;rolling update.&#10; &#10;"
    color "green"
  ]
  node [
    id 408
    label "292"
    title "Page_292"
    color "blue"
  ]
  node [
    id 409
    label "text_203"
    title "260&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;ReplicationController is scaled to zero, causing the last v1 pod to be deleted, which&#10;means the Service will now be backed by v2 pods only. At that point, kubectl will&#10;delete the original ReplicationController and the update process will be finished, as&#10;shown in the following listing.&#10;...&#10;Scaling kubia-v2 up to 2&#10;Scaling kubia-v1 down to 1&#10;Scaling kubia-v2 up to 3&#10;Scaling kubia-v1 down to 0&#10;Update succeeded. Deleting kubia-v1&#10;replicationcontroller &#34;kubia-v1&#34; rolling updated to &#34;kubia-v2&#34;&#10;You&#8217;re now left with only the kubia-v2 ReplicationController and three v2 pods. All&#10;throughout this update process, you&#8217;ve hit your service and gotten a response every&#10;time. You have, in fact, performed a rolling update with zero downtime. &#10;9.2.3&#10;Understanding why kubectl rolling-update is now obsolete&#10;At the beginning of this section, I mentioned an even better way of doing updates&#10;than through kubectl rolling-update. What&#8217;s so wrong with this process that a bet-&#10;ter one had to be introduced? &#10; Well, for starters, I, for one, don&#8217;t like Kubernetes modifying objects I&#8217;ve created.&#10;Okay, it&#8217;s perfectly fine for the scheduler to assign a node to my pods after I create&#10;them, but Kubernetes modifying the labels of my pods and the label selectors of my&#10;ReplicationControllers is something that I don&#8217;t expect and could cause me to go&#10;around the office yelling at my colleagues, &#8220;Who&#8217;s been messing with my controllers!?!?&#8221; &#10; But even more importantly, if you&#8217;ve paid close attention to the words I&#8217;ve used,&#10;you probably noticed that all this time I said explicitly that the kubectl client was the&#10;one performing all these steps of the rolling update. &#10; You can see this by turning on verbose logging with the --v option when triggering&#10;the rolling update:&#10;$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 --v 6&#10;TIP&#10;Using the --v 6 option increases the logging level enough to let you see&#10;the requests kubectl is sending to the API server.&#10;Using this option, kubectl will print out each HTTP request it sends to the Kuberne-&#10;tes API server. You&#8217;ll see PUT requests to&#10;/api/v1/namespaces/default/replicationcontrollers/kubia-v1&#10;which is the RESTful URL representing your kubia-v1 ReplicationController resource.&#10;These requests are the ones scaling down your ReplicationController, which shows&#10;Listing 9.6&#10;The final steps performed by kubectl rolling-update&#10; &#10;"
    color "green"
  ]
  node [
    id 410
    label "293"
    title "Page_293"
    color "blue"
  ]
  node [
    id 411
    label "text_204"
    title "261&#10;Using Deployments for updating apps declaratively&#10;that the kubectl client is the one doing the scaling, instead of it being performed by&#10;the Kubernetes master. &#10;TIP&#10;Use the verbose logging option when running other kubectl commands,&#10;to learn more about the communication between kubectl and the API server. &#10;But why is it such a bad thing that the update process is being performed by the client&#10;instead of on the server? Well, in your case, the update went smoothly, but what if you&#10;lost network connectivity while kubectl was performing the update? The update pro-&#10;cess would be interrupted mid-way. Pods and ReplicationControllers would end up in&#10;an intermediate state.&#10; Another reason why performing an update like this isn&#8217;t as good as it could be is&#10;because it&#8217;s imperative. Throughout this book, I&#8217;ve stressed how Kubernetes is about&#10;you telling it the desired state of the system and having Kubernetes achieve that&#10;state on its own, by figuring out the best way to do it. This is how pods are deployed&#10;and how pods are scaled up and down. You never tell Kubernetes to add an addi-&#10;tional pod or remove an excess one&#8212;you change the number of desired replicas&#10;and that&#8217;s it.&#10; Similarly, you will also want to change the desired image tag in your pod defini-&#10;tions and have Kubernetes replace the pods with new ones running the new image.&#10;This is exactly what drove the introduction of a new resource called a Deployment,&#10;which is now the preferred way of deploying applications in Kubernetes. &#10;9.3&#10;Using Deployments for updating apps declaratively&#10;A Deployment is a higher-level resource meant for deploying applications and&#10;updating them declaratively, instead of doing it through a ReplicationController or&#10;a ReplicaSet, which are both considered lower-level concepts.&#10; When you create a Deployment, a ReplicaSet resource is created underneath&#10;(eventually more of them). As you may remember from chapter 4, ReplicaSets are a&#10;new generation of ReplicationControllers, and should be used instead of them. Replica-&#10;Sets replicate and manage pods, as well. When using a Deployment, the actual pods&#10;are created and managed by the Deployment&#8217;s ReplicaSets, not by the Deployment&#10;directly (the relationship is shown in figure 9.8).&#10;You might wonder why you&#8217;d want to complicate things by introducing another object&#10;on top of a ReplicationController or ReplicaSet, when they&#8217;re what suffices to keep a set&#10;of pod instances running. As the rolling update example in section 9.2 demonstrates,&#10;when updating the app, you need to introduce an additional ReplicationController and&#10;Pods&#10;ReplicaSet&#10;Deployment&#10;Figure 9.8&#10;A Deployment is backed &#10;by a ReplicaSet, which supervises the &#10;deployment&#8217;s pods.&#10; &#10;"
    color "green"
  ]
  node [
    id 412
    label "294"
    title "Page_294"
    color "blue"
  ]
  node [
    id 413
    label "text_205"
    title "262&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;coordinate the two controllers to dance around each other without stepping on each&#10;other&#8217;s toes. You need something coordinating this dance. A Deployment resource&#10;takes care of that (it&#8217;s not the Deployment resource itself, but the controller process&#10;running in the Kubernetes control plane that does that; but we&#8217;ll get to that in chap-&#10;ter 11).&#10; Using a Deployment instead of the lower-level constructs makes updating an app&#10;much easier, because you&#8217;re defining the desired state through the single Deployment&#10;resource and letting Kubernetes take care of the rest, as you&#8217;ll see in the next few pages.&#10;9.3.1&#10;Creating a Deployment&#10;Creating a Deployment isn&#8217;t that different from creating a ReplicationController. A&#10;Deployment is also composed of a label selector, a desired replica count, and a pod&#10;template. In addition to that, it also contains a field, which specifies a deployment&#10;strategy that defines how an update should be performed when the Deployment&#10;resource is modified.  &#10;CREATING A DEPLOYMENT MANIFEST&#10;Let&#8217;s see how to use the kubia-v1 ReplicationController example from earlier in this&#10;chapter and modify it so it describes a Deployment instead of a ReplicationController.&#10;As you&#8217;ll see, this requires only three trivial changes. The following listing shows the&#10;modified YAML.&#10;apiVersion: apps/v1beta1          &#10;kind: Deployment                  &#10;metadata:&#10;  name: kubia          &#10;spec:&#10;  replicas: 3&#10;  template:&#10;    metadata:&#10;      name: kubia&#10;      labels:&#10;        app: kubia&#10;    spec:&#10;      containers:&#10;      - image: luksa/kubia:v1&#10;        name: nodejs&#10;NOTE&#10;You&#8217;ll find an older version of the Deployment resource in extensions/&#10;v1beta1, and a newer one in apps/v1beta2 with different required fields and&#10;different defaults. Be aware that kubectl explain shows the older version.&#10;Because the ReplicationController from before was managing a specific version of the&#10;pods, you called it kubia-v1. A Deployment, on the other hand, is above that version&#10;stuff. At a given point in time, the Deployment can have multiple pod versions run-&#10;ning under its wing, so its name shouldn&#8217;t reference the app version.&#10;Listing 9.7&#10;A Deployment definition: kubia-deployment-v1.yaml&#10;Deployments are in the apps &#10;API group, version v1beta1.&#10;You&#8217;ve changed the kind &#10;from ReplicationController &#10;to Deployment.&#10;There&#8217;s no need to include &#10;the version in the name of &#10;the Deployment.&#10; &#10;"
    color "green"
  ]
  node [
    id 414
    label "295"
    title "Page_295"
    color "blue"
  ]
  node [
    id 415
    label "text_206"
    title "263&#10;Using Deployments for updating apps declaratively&#10;CREATING THE DEPLOYMENT RESOURCE&#10;Before you create this Deployment, make sure you delete any ReplicationControllers&#10;and pods that are still running, but keep the kubia Service for now. You can use the&#10;--all switch to delete all those ReplicationControllers like this:&#10;$ kubectl delete rc --all&#10;You&#8217;re now ready to create the Deployment: &#10;$ kubectl create -f kubia-deployment-v1.yaml --record&#10;deployment &#34;kubia&#34; created&#10;TIP&#10;Be sure to include the --record command-line option when creating it.&#10;This records the command in the revision history, which will be useful later.&#10;DISPLAYING THE STATUS OF THE DEPLOYMENT ROLLOUT&#10;You can use the usual kubectl get deployment and the kubectl describe deployment&#10;commands to see details of the Deployment, but let me point you to an additional&#10;command, which is made specifically for checking a Deployment&#8217;s status:&#10;$ kubectl rollout status deployment kubia&#10;deployment kubia successfully rolled out&#10;According to this, the Deployment has been successfully rolled out, so you should see&#10;the three pod replicas up and running. Let&#8217;s see:&#10;$ kubectl get po&#10;NAME                     READY     STATUS    RESTARTS   AGE&#10;kubia-1506449474-otnnh   1/1       Running   0          14s&#10;kubia-1506449474-vmn7s   1/1       Running   0          14s&#10;kubia-1506449474-xis6m   1/1       Running   0          14s&#10;UNDERSTANDING HOW DEPLOYMENTS CREATE REPLICASETS, WHICH THEN CREATE THE PODS&#10;Take note of the names of these pods. Earlier, when you used a ReplicationController&#10;to create pods, their names were composed of the name of the controller plus a ran-&#10;domly generated string (for example, kubia-v1-m33mv). The three pods created by&#10;the Deployment include an additional numeric value in the middle of their names.&#10;What is that exactly?&#10; The number corresponds to the hashed value of the pod template in the Deploy-&#10;ment and the ReplicaSet managing these pods. As we said earlier, a Deployment&#10;doesn&#8217;t manage pods directly. Instead, it creates ReplicaSets and leaves the managing&#10;to them, so let&#8217;s look at the ReplicaSet created by your Deployment:&#10;$ kubectl get replicasets&#10;NAME               DESIRED   CURRENT   AGE&#10;kubia-1506449474   3         3         10s&#10;The ReplicaSet&#8217;s name also contains the hash value of its pod template. As you&#8217;ll see&#10;later, a Deployment creates multiple ReplicaSets&#8212;one for each version of the pod&#10; &#10;"
    color "green"
  ]
  node [
    id 416
    label "296"
    title "Page_296"
    color "blue"
  ]
  node [
    id 417
    label "text_207"
    title "264&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;template. Using the hash value of the pod template like this allows the Deployment&#10;to always use the same (possibly existing) ReplicaSet for a given version of the pod&#10;template.&#10;ACCESSING THE PODS THROUGH THE SERVICE&#10;With the three replicas created by this ReplicaSet now running, you can use the Ser-&#10;vice you created a while ago to access them, because you made the new pods&#8217; labels&#10;match the Service&#8217;s label selector. &#10; Up until this point, you probably haven&#8217;t seen a good-enough reason why you should&#10;use Deployments over ReplicationControllers. Luckily, creating a Deployment also hasn&#8217;t&#10;been any harder than creating a ReplicationController. Now, you&#8217;ll start doing things&#10;with this Deployment, which will make it clear why Deployments are superior. This will&#10;become clear in the next few moments, when you see how updating the app through&#10;a Deployment resource compares to updating it through a ReplicationController.&#10;9.3.2&#10;Updating a Deployment&#10;Previously, when you ran your app using a ReplicationController, you had to explicitly&#10;tell Kubernetes to perform the update by running kubectl rolling-update. You even&#10;had to specify the name for the new ReplicationController that should replace the old&#10;one. Kubernetes replaced all the original pods with new ones and deleted the original&#10;ReplicationController at the end of the process. During the process, you basically had&#10;to stay around, keeping your terminal open and waiting for kubectl to finish the roll-&#10;ing update. &#10; Now compare this to how you&#8217;re about to update a Deployment. The only thing&#10;you need to do is modify the pod template defined in the Deployment resource and&#10;Kubernetes will take all the steps necessary to get the actual system state to what&#8217;s&#10;defined in the resource. Similar to scaling a ReplicationController or ReplicaSet up or&#10;down, all you need to do is reference a new image tag in the Deployment&#8217;s pod tem-&#10;plate and leave it to Kubernetes to transform your system so it matches the new&#10;desired state.&#10;UNDERSTANDING THE AVAILABLE DEPLOYMENT STRATEGIES&#10;How this new state should be achieved is governed by the deployment strategy config-&#10;ured on the Deployment itself. The default strategy is to perform a rolling update (the&#10;strategy is called RollingUpdate). The alternative is the Recreate strategy, which&#10;deletes all the old pods at once and then creates new ones, similar to modifying a&#10;ReplicationController&#8217;s pod template and then deleting all the pods (we talked about&#10;this in section 9.1.1).&#10; The Recreate strategy causes all old pods to be deleted before the new ones are&#10;created. Use this strategy when your application doesn&#8217;t support running multiple ver-&#10;sions in parallel and requires the old version to be stopped completely before the&#10;new one is started. This strategy does involve a short period of time when your app&#10;becomes completely unavailable.&#10; &#10;"
    color "green"
  ]
  node [
    id 418
    label "297"
    title "Page_297"
    color "blue"
  ]
  node [
    id 419
    label "text_208"
    title "265&#10;Using Deployments for updating apps declaratively&#10; The RollingUpdate strategy, on the other hand, removes old pods one by one,&#10;while adding new ones at the same time, keeping the application available throughout&#10;the whole process, and ensuring there&#8217;s no drop in its capacity to handle requests.&#10;This is the default strategy. The upper and lower limits for the number of pods above&#10;or below the desired replica count are configurable. You should use this strategy only&#10;when your app can handle running both the old and new version at the same time.&#10;SLOWING DOWN THE ROLLING UPDATE FOR DEMO PURPOSES&#10;In the next exercise, you&#8217;ll use the RollingUpdate strategy, but you need to slow down&#10;the update process a little, so you can see that the update is indeed performed in a&#10;rolling fashion. You can do that by setting the minReadySeconds attribute on the&#10;Deployment. We&#8217;ll explain what this attribute does by the end of this chapter. For&#10;now, set it to 10 seconds with the kubectl patch command.&#10;$ kubectl patch deployment kubia -p '{&#34;spec&#34;: {&#34;minReadySeconds&#34;: 10}}'&#10;&#34;kubia&#34; patched&#10;TIP&#10;The kubectl patch command is useful for modifying a single property&#10;or a limited number of properties of a resource without having to edit its defi-&#10;nition in a text editor.&#10;You used the patch command to change the spec of the Deployment. This doesn&#8217;t&#10;cause any kind of update to the pods, because you didn&#8217;t change the pod template.&#10;Changing other Deployment properties, like the desired replica count or the deploy-&#10;ment strategy, also doesn&#8217;t trigger a rollout, because it doesn&#8217;t affect the existing indi-&#10;vidual pods in any way.&#10;TRIGGERING THE ROLLING UPDATE&#10;If you&#8217;d like to track the update process as it progresses, first run the curl loop again&#10;in another terminal to see what&#8217;s happening with the requests (don&#8217;t forget to replace&#10;the IP with the actual external IP of your service):&#10;$ while true; do curl http://130.211.109.222; done&#10;To trigger the actual rollout, you&#8217;ll change the image used in the single pod container&#10;to luksa/kubia:v2. Instead of editing the whole YAML of the Deployment object or&#10;using the patch command to change the image, you&#8217;ll use the kubectl set image&#10;command, which allows changing the image of any resource that contains a container&#10;(ReplicationControllers, ReplicaSets, Deployments, and so on). You&#8217;ll use it to modify&#10;your Deployment like this:&#10;$ kubectl set image deployment kubia nodejs=luksa/kubia:v2&#10;deployment &#34;kubia&#34; image updated&#10;When you execute this command, you&#8217;re updating the kubia Deployment&#8217;s pod tem-&#10;plate so the image used in its nodejs container is changed to luksa/kubia:v2 (from&#10;:v1). This is shown in figure 9.9.&#10; &#10;"
    color "green"
  ]
  node [
    id 420
    label "298"
    title "Page_298"
    color "blue"
  ]
  node [
    id 421
    label "text_209"
    title "266&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;Ways of modifying Deployments and other resources&#10;Over the course of this book, you&#8217;ve learned several ways how to modify an existing&#10;object. Let&#8217;s list all of them together to refresh your memory.&#10;All these methods are equivalent as far as Deployments go. What they do is change&#10;the Deployment&#8217;s specification. This change then triggers the rollout process.&#10;Image registry&#10;Pod template&#10;Deployment&#10;kubectl set image&#8230;&#10;luksa/kubia:v2&#10;Container:&#10;nodejs&#10;:v1&#10;:v2&#10;Image registry&#10;Pod template&#10;Deployment&#10;Container:&#10;nodejs&#10;:v1&#10;:v2&#10;Figure 9.9&#10;Updating a Deployment&#8217;s pod template to point to a new image&#10;Table 9.1&#10;Modifying an existing resource in Kubernetes&#10;Method&#10;What it does&#10;kubectl edit&#10;Opens the object&#8217;s manifest in your default editor. After making &#10;changes, saving the file, and exiting the editor, the object is updated.&#10;Example: kubectl edit deployment kubia&#10;kubectl patch&#10;Modifies individual properties of an object.&#10;Example: kubectl patch deployment kubia -p '{&#34;spec&#34;: &#10;{&#34;template&#34;: {&#34;spec&#34;: {&#34;containers&#34;: [{&#34;name&#34;: &#10;&#34;nodejs&#34;, &#34;image&#34;: &#34;luksa/kubia:v2&#34;}]}}}}'&#10;kubectl apply&#10;Modifies the object by applying property values from a full YAML or &#10;JSON file. If the object specified in the YAML/JSON doesn&#8217;t exist yet, &#10;it&#8217;s created. The file needs to contain the full definition of the &#10;resource (it can&#8217;t include only the fields you want to update, as is the &#10;case with kubectl patch).&#10;Example: kubectl apply -f kubia-deployment-v2.yaml&#10;kubectl replace&#10;Replaces the object with a new one from a YAML/JSON file. In con-&#10;trast to the apply command, this command requires the object to &#10;exist; otherwise it prints an error.&#10;Example: kubectl replace -f kubia-deployment-v2.yaml&#10;kubectl set image&#10;Changes the container image defined in a Pod, ReplicationControl-&#10;ler&#8217;s template, Deployment, DaemonSet, Job, or ReplicaSet.&#10;Example: kubectl set image deployment kubia &#10;nodejs=luksa/kubia:v2&#10; &#10;"
    color "green"
  ]
  node [
    id 422
    label "299"
    title "Page_299"
    color "blue"
  ]
  node [
    id 423
    label "text_210"
    title "267&#10;Using Deployments for updating apps declaratively&#10;If you&#8217;ve run the curl loop, you&#8217;ll see requests initially hitting only the v1 pods; then&#10;more and more of them hit the v2 pods until, finally, all of them hit only the remain-&#10;ing v2 pods, after all v1 pods are deleted. This works much like the rolling update per-&#10;formed by kubectl.&#10;UNDERSTANDING THE AWESOMENESS OF DEPLOYMENTS&#10;Let&#8217;s think about what has happened. By changing the pod template in your Deploy-&#10;ment resource, you&#8217;ve updated your app to a newer version&#8212;by changing a single&#10;field! &#10; The controllers running as part of the Kubernetes control plane then performed&#10;the update. The process wasn&#8217;t performed by the kubectl client, like it was when you&#10;used kubectl rolling-update. I don&#8217;t know about you, but I think that&#8217;s simpler than&#10;having to run a special command telling Kubernetes what to do and then waiting&#10;around for the process to be completed.&#10;NOTE&#10;Be aware that if the pod template in the Deployment references a&#10;ConfigMap (or a Secret), modifying the ConfigMap will not trigger an&#10;update. One way to trigger an update when you need to modify an app&#8217;s con-&#10;fig is to create a new ConfigMap and modify the pod template so it references&#10;the new ConfigMap.&#10;The events that occurred below the Deployment&#8217;s surface during the update are simi-&#10;lar to what happened during the kubectl rolling-update. An additional ReplicaSet&#10;was created and it was then scaled up slowly, while the previous ReplicaSet was scaled&#10;down to zero (the initial and final states are shown in figure 9.10).&#10;You can still see the old ReplicaSet next to the new one if you list them:&#10;$ kubectl get rs&#10;NAME               DESIRED   CURRENT   AGE&#10;kubia-1506449474   0         0         24m&#10;kubia-1581357123   3         3         23m&#10;Pods: v1&#10;ReplicaSet: v1&#10;Replicas: --&#10;Before&#10;After&#10;ReplicaSet: v2&#10;Replicas: ++&#10;Deployment&#10;Pods: v2&#10;ReplicaSet: v1&#10;ReplicaSet: v2&#10;Deployment&#10;Figure 9.10&#10;A Deployment at the start and end of a rolling update&#10; &#10;"
    color "green"
  ]
  node [
    id 424
    label "300"
    title "Page_300"
    color "blue"
  ]
  node [
    id 425
    label "text_211"
    title "268&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;Similar to ReplicationControllers, all your new pods are now managed by the new&#10;ReplicaSet. Unlike before, the old ReplicaSet is still there, whereas the old Replication-&#10;Controller was deleted at the end of the rolling-update process. You&#8217;ll soon see what&#10;the purpose of this inactive ReplicaSet is. &#10; But you shouldn&#8217;t care about ReplicaSets here, because you didn&#8217;t create them&#10;directly. You created and operated only on the Deployment resource; the underlying&#10;ReplicaSets are an implementation detail. You&#8217;ll agree that managing a single Deploy-&#10;ment object is much easier compared to dealing with and keeping track of multiple&#10;ReplicationControllers. &#10; Although this difference may not be so apparent when everything goes well with a&#10;rollout, it becomes much more obvious when you hit a problem during the rollout&#10;process. Let&#8217;s simulate one problem right now.&#10;9.3.3&#10;Rolling back a deployment&#10;You&#8217;re currently running version v2 of your image, so you&#8217;ll need to prepare version 3&#10;first. &#10;CREATING VERSION 3 OF YOUR APP&#10;In version 3, you&#8217;ll introduce a bug that makes your app handle only the first four&#10;requests properly. All requests from the fifth request onward will return an internal&#10;server error (HTTP status code 500). You&#8217;ll simulate this by adding an if statement at&#10;the beginning of the handler function. The following listing shows the new code, with&#10;all required changes shown in bold.&#10;const http = require('http');&#10;const os = require('os');&#10;var requestCount = 0;&#10;console.log(&#34;Kubia server starting...&#34;);&#10;var handler = function(request, response) {&#10;  console.log(&#34;Received request from &#34; + request.connection.remoteAddress);&#10;  if (++requestCount >= 5) {&#10;    response.writeHead(500);&#10;    response.end(&#34;Some internal error has occurred! This is pod &#34; + &#10;os.hostname() + &#34;\n&#34;);&#10;    return;&#10;  }&#10;  response.writeHead(200);&#10;  response.end(&#34;This is v3 running in pod &#34; + os.hostname() + &#34;\n&#34;);&#10;};&#10;var www = http.createServer(handler);&#10;www.listen(8080); &#10;As you can see, on the fifth and all subsequent requests, the code returns a 500 error&#10;with the message &#8220;Some internal error has occurred...&#8221;&#10;Listing 9.8&#10;Version 3 of our app (a broken version): v3/app.js&#10; &#10;"
    color "green"
  ]
  node [
    id 426
    label "301"
    title "Page_301"
    color "blue"
  ]
  node [
    id 427
    label "text_212"
    title "269&#10;Using Deployments for updating apps declaratively&#10;DEPLOYING VERSION 3&#10;I&#8217;ve made the v3 version of the image available as luksa/kubia:v3. You&#8217;ll deploy this&#10;new version by changing the image in the Deployment specification again: &#10;$ kubectl set image deployment kubia nodejs=luksa/kubia:v3&#10;deployment &#34;kubia&#34; image updated&#10;You can follow the progress of the rollout with kubectl rollout status:&#10;$ kubectl rollout status deployment kubia&#10;Waiting for rollout to finish: 1 out of 3 new replicas have been updated...&#10;Waiting for rollout to finish: 2 out of 3 new replicas have been updated...&#10;Waiting for rollout to finish: 1 old replicas are pending termination...&#10;deployment &#34;kubia&#34; successfully rolled out&#10;The new version is now live. As the following listing shows, after a few requests, your&#10;web clients start receiving errors.&#10;$ while true; do curl http://130.211.109.222; done&#10;This is v3 running in pod kubia-1914148340-lalmx&#10;This is v3 running in pod kubia-1914148340-bz35w&#10;This is v3 running in pod kubia-1914148340-w0voh&#10;...&#10;This is v3 running in pod kubia-1914148340-w0voh&#10;Some internal error has occurred! This is pod kubia-1914148340-bz35w&#10;This is v3 running in pod kubia-1914148340-w0voh&#10;Some internal error has occurred! This is pod kubia-1914148340-lalmx&#10;This is v3 running in pod kubia-1914148340-w0voh&#10;Some internal error has occurred! This is pod kubia-1914148340-lalmx&#10;Some internal error has occurred! This is pod kubia-1914148340-bz35w&#10;Some internal error has occurred! This is pod kubia-1914148340-w0voh&#10;UNDOING A ROLLOUT&#10;You can&#8217;t have your users experiencing internal server errors, so you need to do some-&#10;thing about it fast. In section 9.3.6 you&#8217;ll see how to block bad rollouts automatically,&#10;but for now, let&#8217;s see what you can do about your bad rollout manually. Luckily,&#10;Deployments make it easy to roll back to the previously deployed version by telling&#10;Kubernetes to undo the last rollout of a Deployment:&#10;$ kubectl rollout undo deployment kubia&#10;deployment &#34;kubia&#34; rolled back&#10;This rolls the Deployment back to the previous revision. &#10;TIP&#10;The undo command can also be used while the rollout process is still in&#10;progress to essentially abort the rollout. Pods already created during the roll-&#10;out process are removed and replaced with the old ones again.&#10;Listing 9.9&#10;Hitting your broken version 3&#10; &#10;"
    color "green"
  ]
  node [
    id 428
    label "302"
    title "Page_302"
    color "blue"
  ]
  node [
    id 429
    label "text_213"
    title "270&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;DISPLAYING A DEPLOYMENT&#8217;S ROLLOUT HISTORY&#10;Rolling back a rollout is possible because Deployments keep a revision history. As&#10;you&#8217;ll see later, the history is stored in the underlying ReplicaSets. When a rollout&#10;completes, the old ReplicaSet isn&#8217;t deleted, and this enables rolling back to any revi-&#10;sion, not only the previous one. The revision history can be displayed with the&#10;kubectl rollout history command:&#10;$ kubectl rollout history deployment kubia&#10;deployments &#34;kubia&#34;:&#10;REVISION    CHANGE-CAUSE&#10;2           kubectl set image deployment kubia nodejs=luksa/kubia:v2&#10;3           kubectl set image deployment kubia nodejs=luksa/kubia:v3&#10;Remember the --record command-line option you used when creating the Deploy-&#10;ment? Without it, the CHANGE-CAUSE column in the revision history would be empty,&#10;making it much harder to figure out what&#8217;s behind each revision.&#10;ROLLING BACK TO A SPECIFIC DEPLOYMENT REVISION&#10;You can roll back to a specific revision by specifying the revision in the undo com-&#10;mand. For example, if you want to roll back to the first version, you&#8217;d execute the fol-&#10;lowing command:&#10;$ kubectl rollout undo deployment kubia --to-revision=1&#10;Remember the inactive ReplicaSet left over when you modified the Deployment the&#10;first time? The ReplicaSet represents the first revision of your Deployment. All Replica-&#10;Sets created by a Deployment represent the complete revision history, as shown in fig-&#10;ure 9.11. Each ReplicaSet stores the complete information of the Deployment at that&#10;specific revision, so you shouldn&#8217;t delete it manually. If you do, you&#8217;ll lose that specific&#10;revision from the Deployment&#8217;s history, preventing you from rolling back to it.&#10;But having old ReplicaSets cluttering your ReplicaSet list is not ideal, so the length of&#10;the revision history is limited by the revisionHistoryLimit property on the Deploy-&#10;ment resource. It defaults to two, so normally only the current and the previous revision&#10;are shown in the history (and only the current and the previous ReplicaSet are pre-&#10;served). Older ReplicaSets are deleted automatically. &#10;Deployment&#10;v1 ReplicaSet&#10;ReplicaSet&#10;Pods: v1&#10;ReplicaSet&#10;ReplicaSet&#10;ReplicaSet&#10;Revision 2&#10;Revision 4&#10;Revision 3&#10;Revision 1&#10;Revision history&#10;Current revision&#10;Figure 9.11&#10;A Deployment&#8217;s ReplicaSets also act as its revision history.&#10; &#10;"
    color "green"
  ]
  node [
    id 430
    label "303"
    title "Page_303"
    color "blue"
  ]
  node [
    id 431
    label "text_214"
    title "271&#10;Using Deployments for updating apps declaratively&#10;NOTE&#10;The extensions/v1beta1 version of Deployments doesn&#8217;t have a default&#10;revisionHistoryLimit, whereas the default in version apps/v1beta2 is 10.&#10;9.3.4&#10;Controlling the rate of the rollout&#10;When you performed the rollout to v3 and tracked its progress with the kubectl&#10;rollout status command, you saw that first a new pod was created, and when it&#10;became available, one of the old pods was deleted and another new pod was created.&#10;This continued until there were no old pods left. The way new pods are created and&#10;old ones are deleted is configurable through two additional properties of the rolling&#10;update strategy. &#10;INTRODUCING THE MAXSURGE AND MAXUNAVAILABLE PROPERTIES OF THE ROLLING UPDATE STRATEGY&#10;Two properties affect how many pods are replaced at once during a Deployment&#8217;s roll-&#10;ing update. They are maxSurge and maxUnavailable and can be set as part of the&#10;rollingUpdate sub-property of the Deployment&#8217;s strategy attribute, as shown in&#10;the following listing.&#10;spec:&#10;  strategy:&#10;    rollingUpdate:&#10;      maxSurge: 1&#10;      maxUnavailable: 0&#10;    type: RollingUpdate&#10;What these properties do is explained in table 9.2.&#10;Because the desired replica count in your case was three, and both these properties&#10;default to 25%, maxSurge allowed the number of all pods to reach four, and&#10;Listing 9.10&#10;Specifying parameters for the rollingUpdate strategy&#10;Table 9.2&#10;Properties for configuring the rate of the rolling update&#10;Property&#10;What it does&#10;maxSurge&#10;Determines how many pod instances you allow to exist above the desired replica &#10;count configured on the Deployment. It defaults to 25%, so there can be at most &#10;25% more pod instances than the desired count. If the desired replica count is &#10;set to four, there will never be more than five pod instances running at the same &#10;time during an update. When converting a percentage to an absolute number, &#10;the number is rounded up. Instead of a percentage, the value can also be an &#10;absolute value (for example, one or two additional pods can be allowed).&#10;maxUnavailable&#10;Determines how many pod instances can be unavailable relative to the desired &#10;replica count during the update. It also defaults to 25%, so the number of avail-&#10;able pod instances must never fall below 75% of the desired replica count. Here, &#10;when converting a percentage to an absolute number, the number is rounded &#10;down. If the desired replica count is set to four and the percentage is 25%, only &#10;one pod can be unavailable. There will always be at least three pod instances &#10;available to serve requests during the whole rollout. As with maxSurge, you can &#10;also specify an absolute value instead of a percentage.&#10; &#10;"
    color "green"
  ]
  node [
    id 432
    label "304"
    title "Page_304"
    color "blue"
  ]
  node [
    id 433
    label "text_215"
    title "272&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;maxUnavailable disallowed having any unavailable pods (in other words, three pods&#10;had to be available at all times). This is shown in figure 9.12.&#10;UNDERSTANDING THE MAXUNAVAILABLE PROPERTY&#10;The extensions/v1beta1 version of Deployments uses different defaults&#8212;it sets both&#10;maxSurge and maxUnavailable to 1 instead of 25%. In the case of three replicas, max-&#10;Surge is the same as before, but maxUnavailable is different (1 instead of 0). This&#10;makes the rollout process unwind a bit differently, as shown in figure 9.13.&#10;v1&#10;Number&#10;of pods&#10;3&#10;4&#10;2&#10;1&#10;Time&#10;v1&#10;3 available&#10;1 unavailable&#10;Create&#10;one&#10;v2 pod&#10;4 available&#10;3 available&#10;1 unavailable&#10;4 available&#10;3 available&#10;1 unavailable&#10;maxSurge = 1&#10;maxUnavailable = 0&#10;Desired replica count = 3&#10;3 available&#10;v2&#10;v1&#10;v1&#10;v2&#10;v2&#10;v1&#10;v1&#10;v1&#10;v1&#10;v1&#10;v1&#10;v1&#10;v1&#10;v1&#10;v1&#10;v2&#10;v2&#10;v2&#10;v2&#10;v2&#10;v2&#10;v2&#10;v2&#10;v1&#10;v2&#10;v2&#10;v2&#10;v2&#10;4 available&#10;Wait&#10;until&#10;it&#8217;s&#10;available&#10;Delete&#10;one v1&#10;pod and&#10;create one&#10;v2 pod&#10;Wait&#10;until&#10;it&#8217;s&#10;available&#10;Delete&#10;one v1&#10;pod and&#10;create one&#10;v2 pod&#10;Wait&#10;until&#10;it&#8217;s&#10;available&#10;Delete&#10;last&#10;v1 pod&#10;Figure 9.12&#10;Rolling update of a Deployment with three replicas and default maxSurge and maxUnavailable &#10;v1&#10;Number&#10;of pods&#10;3&#10;4&#10;2&#10;1&#10;Time&#10;v1&#10;2 available&#10;2 unavailable&#10;4 available&#10;2 available&#10;1 unavailable&#10;3 available&#10;maxSurge = 1&#10;maxUnavailable = 1&#10;Desired replica count = 3&#10;v1&#10;v1&#10;v1&#10;v1&#10;v1&#10;v2&#10;v2&#10;v2&#10;v2&#10;v2&#10;v2&#10;v2&#10;v2&#10;v2&#10;v2&#10;Wait until&#10;both are&#10;available&#10;Delete&#10;two v1&#10;pods and&#10;create one&#10;v2 pod&#10;Delete v1&#10;pod and&#10;create two&#10;v2 pods&#10;Wait&#10;until it&#8217;s&#10;available&#10;Figure 9.13&#10;Rolling update of a Deployment with the maxSurge=1 and maxUnavailable=1&#10; &#10;"
    color "green"
  ]
  node [
    id 434
    label "305"
    title "Page_305"
    color "blue"
  ]
  node [
    id 435
    label "text_216"
    title "273&#10;Using Deployments for updating apps declaratively&#10;In this case, one replica can be unavailable, so if the desired replica count is three,&#10;only two of them need to be available. That&#8217;s why the rollout process immediately&#10;deletes one pod and creates two new ones. This ensures two pods are available and&#10;that the maximum number of pods isn&#8217;t exceeded (the maximum is four in this&#10;case&#8212;three plus one from maxSurge). As soon as the two new pods are available, the&#10;two remaining old pods are deleted.&#10; This is a bit hard to grasp, especially since the maxUnavailable property leads you&#10;to believe that that&#8217;s the maximum number of unavailable pods that are allowed. If&#10;you look at the previous figure closely, you&#8217;ll see two unavailable pods in the second&#10;column even though maxUnavailable is set to 1. &#10; It&#8217;s important to keep in mind that maxUnavailable is relative to the desired&#10;replica count. If the replica count is set to three and maxUnavailable is set to one,&#10;that means that the update process must always keep at least two (3 minus 1) pods&#10;available, while the number of pods that aren&#8217;t available can exceed one.&#10;9.3.5&#10;Pausing the rollout process&#10;After the bad experience with version 3 of your app, imagine you&#8217;ve now fixed the bug&#10;and pushed version 4 of your image. You&#8217;re a little apprehensive about rolling it out&#10;across all your pods the way you did before. What you want is to run a single v4 pod&#10;next to your existing v2 pods and see how it behaves with only a fraction of all your&#10;users. Then, once you&#8217;re sure everything&#8217;s okay, you can replace all the old pods with&#10;new ones. &#10; You could achieve this by running an additional pod either directly or through an&#10;additional Deployment, ReplicationController, or ReplicaSet, but you do have another&#10;option available on the Deployment itself. A Deployment can also be paused during&#10;the rollout process. This allows you to verify that everything is fine with the new ver-&#10;sion before proceeding with the rest of the rollout.&#10;PAUSING THE ROLLOUT&#10;I&#8217;ve prepared the v4 image, so go ahead and trigger the rollout by changing the image&#10;to luksa/kubia:v4, but then immediately (within a few seconds) pause the rollout:&#10;$ kubectl set image deployment kubia nodejs=luksa/kubia:v4&#10;deployment &#34;kubia&#34; image updated&#10;$ kubectl rollout pause deployment kubia&#10;deployment &#34;kubia&#34; paused&#10;A single new pod should have been created, but all original pods should also still be&#10;running. Once the new pod is up, a part of all requests to the service will be redirected&#10;to the new pod. This way, you&#8217;ve effectively run a canary release. A canary release is a&#10;technique for minimizing the risk of rolling out a bad version of an application and it&#10;affecting all your users. Instead of rolling out the new version to everyone, you replace&#10;only one or a small number of old pods with new ones. This way only a small number&#10;of users will initially hit the new version. You can then verify whether the new version&#10; &#10;"
    color "green"
  ]
  node [
    id 436
    label "306"
    title "Page_306"
    color "blue"
  ]
  node [
    id 437
    label "text_217"
    title "274&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;is working fine or not and then either continue the rollout across all remaining pods&#10;or roll back to the previous version. &#10;RESUMING THE ROLLOUT&#10;In your case, by pausing the rollout process, only a small portion of client requests will&#10;hit your v4 pod, while most will still hit the v3 pods. Once you&#8217;re confident the new&#10;version works as it should, you can resume the deployment to replace all the old pods&#10;with new ones:&#10;$ kubectl rollout resume deployment kubia&#10;deployment &#34;kubia&#34; resumed&#10;Obviously, having to pause the deployment at an exact point in the rollout process&#10;isn&#8217;t what you want to do. In the future, a new upgrade strategy may do that automati-&#10;cally, but currently, the proper way of performing a canary release is by using two dif-&#10;ferent Deployments and scaling them appropriately. &#10;USING THE PAUSE FEATURE TO PREVENT ROLLOUTS&#10;Pausing a Deployment can also be used to prevent updates to the Deployment from&#10;kicking off the rollout process, allowing you to make multiple changes to the Deploy-&#10;ment and starting the rollout only when you&#8217;re done making all the necessary changes.&#10;Once you&#8217;re ready for changes to take effect, you resume the Deployment and the&#10;rollout process will start.&#10;NOTE&#10;If a Deployment is paused, the undo command won&#8217;t undo it until you&#10;resume the Deployment.&#10;9.3.6&#10;Blocking rollouts of bad versions&#10;Before you conclude this chapter, we need to discuss one more property of the Deploy-&#10;ment resource. Remember the minReadySeconds property you set on the Deployment&#10;at the beginning of section 9.3.2? You used it to slow down the rollout, so you could see&#10;it was indeed performing a rolling update and not replacing all the pods at once. The&#10;main function of minReadySeconds is to prevent deploying malfunctioning versions, not&#10;slowing down a deployment for fun. &#10;UNDERSTANDING THE APPLICABILITY OF MINREADYSECONDS&#10;The minReadySeconds property specifies how long a newly created pod should be&#10;ready before the pod is treated as available. Until the pod is available, the rollout pro-&#10;cess will not continue (remember the maxUnavailable property?). A pod is ready&#10;when readiness probes of all its containers return a success. If a new pod isn&#8217;t func-&#10;tioning properly and its readiness probe starts failing before minReadySeconds have&#10;passed, the rollout of the new version will effectively be blocked.&#10; You used this property to slow down your rollout process by having Kubernetes&#10;wait 10 seconds after a pod was ready before continuing with the rollout. Usually,&#10;you&#8217;d set minReadySeconds to something much higher to make sure pods keep report-&#10;ing they&#8217;re ready after they&#8217;ve already started receiving actual traffic. &#10; &#10;"
    color "green"
  ]
  node [
    id 438
    label "307"
    title "Page_307"
    color "blue"
  ]
  node [
    id 439
    label "text_218"
    title "275&#10;Using Deployments for updating apps declaratively&#10; Although you should obviously test your pods both in a test and in a staging envi-&#10;ronment before deploying them into production, using minReadySeconds is like an&#10;airbag that saves your app from making a big mess after you&#8217;ve already let a buggy ver-&#10;sion slip into production. &#10; With a properly configured readiness probe and a proper minReadySeconds set-&#10;ting, Kubernetes would have prevented us from deploying the buggy v3 version ear-&#10;lier. Let me show you how.&#10;DEFINING A READINESS PROBE TO PREVENT OUR V3 VERSION FROM BEING ROLLED OUT FULLY&#10;You&#8217;re going to deploy version v3 again, but this time, you&#8217;ll have the proper readi-&#10;ness probe defined on the pod. Your Deployment is currently at version v4, so before&#10;you start, roll back to version v2 again so you can pretend this is the first time you&#8217;re&#10;upgrading to v3. If you wish, you can go straight from v4 to v3, but the text that fol-&#10;lows assumes you returned to v2 first.&#10; Unlike before, where you only updated the image in the pod template, you&#8217;re now&#10;also going to introduce a readiness probe for the container at the same time. Up until&#10;now, because there was no explicit readiness probe defined, the container and the&#10;pod were always considered ready, even if the app wasn&#8217;t truly ready or was returning&#10;errors. There was no way for Kubernetes to know that the app was malfunctioning and&#10;shouldn&#8217;t be exposed to clients. &#10; To change the image and introduce the readiness probe at once, you&#8217;ll use the&#10;kubectl apply command. You&#8217;ll use the following YAML to update the deployment&#10;(you&#8217;ll store it as kubia-deployment-v3-with-readinesscheck.yaml), as shown in&#10;the following listing.&#10;apiVersion: apps/v1beta1&#10;kind: Deployment&#10;metadata:&#10;  name: kubia&#10;spec:&#10;  replicas: 3&#10;  minReadySeconds: 10           &#10;  strategy:&#10;    rollingUpdate:&#10;      maxSurge: 1                  &#10;      maxUnavailable: 0         &#10;    type: RollingUpdate&#10;  template:&#10;    metadata:&#10;      name: kubia&#10;      labels:&#10;        app: kubia&#10;    spec:&#10;      containers:&#10;      - image: luksa/kubia:v3&#10;Listing 9.11&#10;Deployment with a readiness probe: kubia-deployment-v3-with-&#10;readinesscheck.yaml&#10;You&#8217;re keeping &#10;minReadySeconds &#10;set to 10.&#10;You&#8217;re keeping maxUnavailable &#10;set to 0 to make the deployment &#10;replace pods one by one&#10; &#10;"
    color "green"
  ]
  node [
    id 440
    label "308"
    title "Page_308"
    color "blue"
  ]
  node [
    id 441
    label "text_219"
    title "276&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10;        name: nodejs&#10;        readinessProbe:&#10;          periodSeconds: 1       &#10;          httpGet:                  &#10;            path: /                 &#10;            port: 8080              &#10;UPDATING A DEPLOYMENT WITH KUBECTL APPLY&#10;To update the Deployment this time, you&#8217;ll use kubectl apply like this:&#10;$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml &#10;deployment &#34;kubia&#34; configured&#10;The apply command updates the Deployment with everything that&#8217;s defined in the&#10;YAML file. It not only updates the image but also adds the readiness probe definition&#10;and anything else you&#8217;ve added or modified in the YAML. If the new YAML also con-&#10;tains the replicas field, which doesn&#8217;t match the number of replicas on the existing&#10;Deployment, the apply operation will also scale the Deployment, which isn&#8217;t usually&#10;what you want. &#10;TIP&#10;To keep the desired replica count unchanged when updating a Deploy-&#10;ment with kubectl apply, don&#8217;t include the replicas field in the YAML. &#10;Running the apply command will kick off the update process, which you can again&#10;follow with the rollout status command:&#10;$ kubectl rollout status deployment kubia&#10;Waiting for rollout to finish: 1 out of 3 new replicas have been updated...&#10;Because the status says one new pod has been created, your service should be hitting it&#10;occasionally, right? Let&#8217;s see:&#10;$ while true; do curl http://130.211.109.222; done&#10;This is v2 running in pod kubia-1765119474-jvslk&#10;This is v2 running in pod kubia-1765119474-jvslk&#10;This is v2 running in pod kubia-1765119474-xk5g3&#10;This is v2 running in pod kubia-1765119474-pmb26&#10;This is v2 running in pod kubia-1765119474-pmb26&#10;This is v2 running in pod kubia-1765119474-xk5g3&#10;...&#10;Nope, you never hit the v3 pod. Why not? Is it even there? List the pods:&#10;$ kubectl get po&#10;NAME                     READY     STATUS    RESTARTS   AGE&#10;kubia-1163142519-7ws0i   0/1       Running   0          30s&#10;kubia-1765119474-jvslk   1/1       Running   0          9m&#10;kubia-1765119474-pmb26   1/1       Running   0          9m&#10;kubia-1765119474-xk5g3   1/1       Running   0          8m&#10;You&#8217;re defining a readiness probe &#10;that will be executed every second.&#10;The readiness probe will &#10;perform an HTTP GET request &#10;against our container.&#10; &#10;"
    color "green"
  ]
  node [
    id 442
    label "309"
    title "Page_309"
    color "blue"
  ]
  node [
    id 443
    label "text_220"
    title "277&#10;Using Deployments for updating apps declaratively&#10;Aha! There&#8217;s your problem (or as you&#8217;ll learn soon, your blessing)! The pod is shown&#10;as not ready, but I guess you&#8217;ve been expecting that, right? What has happened?&#10;UNDERSTANDING HOW A READINESS PROBE PREVENTS BAD VERSIONS FROM BEING ROLLED OUT&#10;As soon as your new pod starts, the readiness probe starts being hit every second (you&#10;set the probe&#8217;s interval to one second in the pod spec). On the fifth request the readi-&#10;ness probe began failing, because your app starts returning HTTP status code 500&#10;from the fifth request onward. &#10; As a result, the pod is removed as an endpoint from the service (see figure 9.14).&#10;By the time you start hitting the service in the curl loop, the pod has already been&#10;marked as not ready. This explains why you never hit the new pod with curl. And&#10;that&#8217;s exactly what you want, because you don&#8217;t want clients to hit a pod that&#8217;s not&#10;functioning properly.&#10;But what about the rollout process? The rollout status command shows only one&#10;new replica has started. Thankfully, the rollout process will not continue, because the&#10;new pod will never become available. To be considered available, it needs to be ready&#10;for at least 10 seconds. Until it&#8217;s available, the rollout process will not create any new&#10;pods, and it also won&#8217;t remove any original pods because you&#8217;ve set the maxUnavailable&#10;property to 0. &#10;Service&#10;curl&#10;Pod: v2&#10;Pod: v2&#10;Pod: v3&#10;(unhealthy)&#10;Pod: v2&#10;ReplicaSet: v2&#10;Replicas: 3&#10;Deployment&#10;Replicas: 3&#10;rollingUpdate:&#10;maxSurge: 1&#10;maxUnavailable: 0&#10;ReplicaSet: v3&#10;Replicas: 1&#10;Requests are not forwarded&#10;to v3 pod because of failed&#10;readiness probe&#10;Figure 9.14&#10;Deployment blocked by a failing readiness probe in the new pod&#10; &#10;"
    color "green"
  ]
  node [
    id 444
    label "310"
    title "Page_310"
    color "blue"
  ]
  node [
    id 445
    label "text_221"
    title "278&#10;CHAPTER 9&#10;Deployments: updating applications declaratively&#10; The fact that the deployment is stuck is a good thing, because if it had continued&#10;replacing the old pods with the new ones, you&#8217;d end up with a completely non-working&#10;service, like you did when you first rolled out version 3, when you weren&#8217;t using the&#10;readiness probe. But now, with the readiness probe in place, there was virtually no&#10;negative impact on your users. A few users may have experienced the internal server&#10;error, but that&#8217;s not as big of a problem as if the rollout had replaced all pods with the&#10;faulty version 3.&#10;TIP&#10;If you only define the readiness probe without setting minReadySeconds&#10;properly, new pods are considered available immediately when the first invo-&#10;cation of the readiness probe succeeds. If the readiness probe starts failing&#10;shortly after, the bad version is rolled out across all pods. Therefore, you&#10;should set minReadySeconds appropriately.&#10;CONFIGURING A DEADLINE FOR THE ROLLOUT&#10;By default, after the rollout can&#8217;t make any progress in 10 minutes, it&#8217;s considered as&#10;failed. If you use the kubectl describe deployment command, you&#8217;ll see it display a&#10;ProgressDeadlineExceeded condition, as shown in the following listing.&#10;$ kubectl describe deploy kubia&#10;Name:                   kubia&#10;...&#10;Conditions:&#10;  Type          Status  Reason&#10;  ----          ------  ------&#10;  Available     True    MinimumReplicasAvailable&#10;  Progressing   False   ProgressDeadlineExceeded   &#10;The time after which the Deployment is considered failed is configurable through the&#10;progressDeadlineSeconds property in the Deployment spec.&#10;NOTE&#10;The extensions/v1beta1 version of Deployments doesn&#8217;t set a deadline.&#10;ABORTING A BAD ROLLOUT&#10;Because the rollout will never continue, the only thing to do now is abort the rollout&#10;by undoing it:&#10;$ kubectl rollout undo deployment kubia&#10;deployment &#34;kubia&#34; rolled back&#10;NOTE&#10;In future versions, the rollout will be aborted automatically when the&#10;time specified in progressDeadlineSeconds is exceeded.&#10;Listing 9.12&#10;Seeing the conditions of a Deployment with kubectl describe&#10;The Deployment &#10;took too long to &#10;make progress.&#10; &#10;"
    color "green"
  ]
  node [
    id 446
    label "311"
    title "Page_311"
    color "blue"
  ]
  node [
    id 447
    label "text_222"
    title "279&#10;Summary&#10;9.4&#10;Summary&#10;This chapter has shown you how to make your life easier by using a declarative&#10;approach to deploying and updating applications in Kubernetes. Now that you&#8217;ve&#10;read this chapter, you should know how to&#10;&#61601;Perform a rolling update of pods managed by a ReplicationController&#10;&#61601;Create Deployments instead of lower-level ReplicationControllers or ReplicaSets&#10;&#61601;Update your pods by editing the pod template in the Deployment specification&#10;&#61601;Roll back a Deployment either to the previous revision or to any earlier revision&#10;still listed in the revision history&#10;&#61601;Abort a Deployment mid-way&#10;&#61601;Pause a Deployment to inspect how a single instance of the new version behaves&#10;in production before allowing additional pod instances to replace the old ones&#10;&#61601;Control the rate of the rolling update through maxSurge and maxUnavailable&#10;properties&#10;&#61601;Use minReadySeconds and readiness probes to have the rollout of a faulty ver-&#10;sion blocked automatically&#10;In addition to these Deployment-specific tasks, you also learned how to&#10;&#61601;Use three dashes as a separator to define multiple resources in a single YAML file&#10;&#61601;Turn on kubectl&#8217;s verbose logging to see exactly what it&#8217;s doing behind the&#10;curtains&#10;You now know how to deploy and manage sets of pods created from the same pod&#10;template and thus share the same persistent storage. You even know how to update&#10;them declaratively. But what about running sets of pods, where each instance needs to&#10;use its own persistent storage? We haven&#8217;t looked at that yet. That&#8217;s the subject of our&#10;next chapter.&#10; &#10;"
    color "green"
  ]
  node [
    id 448
    label "312"
    title "Page_312"
    color "blue"
  ]
  node [
    id 449
    label "text_223"
    title "280&#10;StatefulSets:&#10;deploying replicated&#10;stateful applications&#10;You now know how to run both single-instance and replicated stateless pods,&#10;and even stateful pods utilizing persistent storage. You can run several repli-&#10;cated web-server pod instances and you can run a single database pod instance&#10;that uses persistent storage, provided either through plain pod volumes or through&#10;PersistentVolumes bound by a PersistentVolumeClaim. But can you employ a&#10;ReplicaSet to replicate the database pod?&#10;This chapter covers&#10;&#61601;Deploying stateful clustered applications&#10;&#61601;Providing separate storage for each instance of &#10;a replicated pod&#10;&#61601;Guaranteeing a stable name and hostname for &#10;pod replicas&#10;&#61601;Starting and stopping pod replicas in a &#10;predictable order&#10;&#61601;Discovering peers through DNS SRV records&#10; &#10;"
    color "green"
  ]
  node [
    id 450
    label "313"
    title "Page_313"
    color "blue"
  ]
  node [
    id 451
    label "text_224"
    title "281&#10;Replicating stateful pods&#10;10.1&#10;Replicating stateful pods&#10;ReplicaSets create multiple pod replicas from a single pod template. These replicas&#10;don&#8217;t differ from each other, apart from their name and IP address. If the pod tem-&#10;plate includes a volume, which refers to a specific PersistentVolumeClaim, all replicas&#10;of the ReplicaSet will use the exact same PersistentVolumeClaim and therefore the&#10;same PersistentVolume bound by the claim (shown in figure 10.1).&#10;Because the reference to the claim is in the pod template, which is used to stamp out&#10;multiple pod replicas, you can&#8217;t make each replica use its own separate Persistent-&#10;VolumeClaim. You can&#8217;t use a ReplicaSet to run a distributed data store, where each&#10;instance needs its own separate storage&#8212;at least not by using a single ReplicaSet. To&#10;be honest, none of the API objects you&#8217;ve seen so far make running such a data store&#10;possible. You need something else. &#10;10.1.1 Running multiple replicas with separate storage for each&#10;How does one run multiple replicas of a pod and have each pod use its own storage&#10;volume? ReplicaSets create exact copies (replicas) of a pod; therefore you can&#8217;t use&#10;them for these types of pods. What can you use?&#10;CREATING PODS MANUALLY&#10;You could create pods manually and have each of them use its own PersistentVolume-&#10;Claim, but because no ReplicaSet looks after them, you&#8217;d need to manage them man-&#10;ually and recreate them when they disappear (as in the event of a node failure).&#10;Therefore, this isn&#8217;t a viable option.&#10;USING ONE REPLICASET PER POD INSTANCE&#10;Instead of creating pods directly, you could create multiple ReplicaSets&#8212;one for each&#10;pod with each ReplicaSet&#8217;s desired replica count set to one, and each ReplicaSet&#8217;s pod&#10;template referencing a dedicated PersistentVolumeClaim (as shown in figure 10.2).&#10; Although this takes care of the automatic rescheduling in case of node failures or&#10;accidental pod deletions, it&#8217;s much more cumbersome compared to having a single&#10;ReplicaSet. For example, think about how you&#8217;d scale the pods in that case. You&#10;Persistent&#10;Volume&#10;Claim&#10;Persistent&#10;Volume&#10;ReplicaSet&#10;Pod&#10;Pod&#10;Pod&#10;Figure 10.1&#10;All pods from the same ReplicaSet always use the same &#10;PersistentVolumeClaim and PersistentVolume.&#10; &#10;"
    color "green"
  ]
  node [
    id 452
    label "314"
    title "Page_314"
    color "blue"
  ]
  node [
    id 453
    label "text_225"
    title "282&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10;couldn&#8217;t change the desired replica count&#8212;you&#8217;d have to create additional Replica-&#10;Sets instead. &#10; Using multiple ReplicaSets is therefore not the best solution. But could you maybe&#10;use a single ReplicaSet and have each pod instance keep its own persistent state, even&#10;though they&#8217;re all using the same storage volume? &#10;USING MULTIPLE DIRECTORIES IN THE SAME VOLUME&#10;A trick you can use is to have all pods use the same PersistentVolume, but then have a&#10;separate file directory inside that volume for each pod (this is shown in figure 10.3).&#10;Because you can&#8217;t configure pod replicas differently from a single pod template, you&#10;can&#8217;t tell each instance what directory it should use, but you can make each instance&#10;automatically select (and possibly also create) a data directory that isn&#8217;t being used&#10;by any other instance at that time. This solution does require coordination between&#10;the instances, and isn&#8217;t easy to do correctly. It also makes the shared storage volume&#10;the bottleneck.&#10;10.1.2 Providing a stable identity for each pod&#10;In addition to storage, certain clustered applications also require that each instance&#10;has a long-lived stable identity. Pods can be killed from time to time and replaced with&#10;PVC A1&#10;PV A1&#10;ReplicaSet A1&#10;Pod A1-xyz&#10;PVC A2&#10;PV A2&#10;ReplicaSet A2&#10;Pod A2-xzy&#10;PVC A3&#10;PV A3&#10;ReplicaSet A3&#10;Pod A3-zyx&#10;Figure 10.2&#10;Using one ReplicaSet for each pod instance&#10;Persistent&#10;Volume&#10;Claim&#10;PersistentVolume&#10;ReplicaSet&#10;Pod&#10;Pod&#10;Pod&#10;App&#10;App&#10;App&#10;/data/1/&#10;/data/3/&#10;/data/2/&#10;Figure 10.3&#10;Working around the shared storage problem by having the app &#10;in each pod use a different file directory &#10; &#10;"
    color "green"
  ]
  node [
    id 454
    label "315"
    title "Page_315"
    color "blue"
  ]
  node [
    id 455
    label "text_226"
    title "283&#10;Replicating stateful pods&#10;new ones. When a ReplicaSet replaces a pod, the new pod is a completely new pod&#10;with a new hostname and IP, although the data in its storage volume may be that of&#10;the killed pod. For certain apps, starting up with the old instance&#8217;s data but with a&#10;completely new network identity may cause problems.&#10; Why do certain apps mandate a stable network identity? This requirement is&#10;fairly common in distributed stateful applications. Certain apps require the adminis-&#10;trator to list all the other cluster members and their IP addresses (or hostnames) in&#10;each member&#8217;s configuration file. But in Kubernetes, every time a pod is resched-&#10;uled, the new pod gets both a new hostname and a new IP address, so the whole&#10;application cluster would have to be reconfigured every time one of its members is&#10;rescheduled. &#10;USING A DEDICATED SERVICE FOR EACH POD INSTANCE&#10;A trick you can use to work around this problem is to provide a stable network address&#10;for cluster members by creating a dedicated Kubernetes Service for each individual&#10;member. Because service IPs are stable, you can then point to each member through&#10;its service IP (rather than the pod IP) in the configuration. &#10; This is similar to creating a ReplicaSet for each member to provide them with indi-&#10;vidual storage, as described previously. Combining these two techniques results in the&#10;setup shown in figure 10.4 (an additional service covering all the cluster members is&#10;also shown, because you usually need one for clients of the cluster).&#10;The solution is not only ugly, but it still doesn&#8217;t solve everything. The individual pods&#10;can&#8217;t know which Service they are exposed through (and thus can&#8217;t know their stable&#10;IP), so they can&#8217;t self-register in other pods using that IP. &#10;PVC A1&#10;PV A1&#10;ReplicaSet A1&#10;Pod A1-xzy&#10;Service A1&#10;Service A&#10;PVC A2&#10;PV A2&#10;ReplicaSet A2&#10;Pod A2-xzy&#10;Service A2&#10;PVC A3&#10;PV A3&#10;ReplicaSet A3&#10;Pod A3-zyx&#10;Service A3&#10;Figure 10.4&#10;Using one &#10;Service and ReplicaSet per &#10;pod to provide a stable &#10;network address and an &#10;individual volume for each &#10;pod, respectively&#10; &#10;"
    color "green"
  ]
  node [
    id 456
    label "316"
    title "Page_316"
    color "blue"
  ]
  node [
    id 457
    label "text_227"
    title "284&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10; Luckily, Kubernetes saves us from resorting to such complex solutions. The proper&#10;clean and simple way of running these special types of applications in Kubernetes is&#10;through a StatefulSet. &#10;10.2&#10;Understanding StatefulSets&#10;Instead of using a ReplicaSet to run these types of pods, you create a StatefulSet&#10;resource, which is specifically tailored to applications where instances of the applica-&#10;tion must be treated as non-fungible individuals, with each one having a stable name&#10;and state. &#10;10.2.1 Comparing StatefulSets with ReplicaSets&#10;To understand the purpose of StatefulSets, it&#8217;s best to compare them to ReplicaSets or&#10;ReplicationControllers. But first let me explain them with a little analogy that&#8217;s widely&#10;used in the field.&#10;UNDERSTANDING STATEFUL PODS WITH THE PETS VS. CATTLE ANALOGY&#10;You may have already heard of the pets vs. cattle analogy. If not, let me explain it. We&#10;can treat our apps either as pets or as cattle. &#10;NOTE&#10;StatefulSets were initially called PetSets. That name comes from the&#10;pets vs. cattle analogy explained here.&#10;We tend to treat our app instances as pets, where we give each instance a name and&#10;take care of each instance individually. But it&#8217;s usually better to treat instances as cattle&#10;and not pay special attention to each individual instance. This makes it easy to replace&#10;unhealthy instances without giving it a second thought, similar to the way a farmer&#10;replaces unhealthy cattle. &#10; Instances of a stateless app, for example, behave much like heads of cattle. It&#10;doesn&#8217;t matter if an instance dies&#8212;you can create a new instance and people won&#8217;t&#10;notice the difference. &#10; On the other hand, with stateful apps, an app instance is more like a pet. When a&#10;pet dies, you can&#8217;t go buy a new one and expect people not to notice. To replace a lost&#10;pet, you need to find a new one that looks and behaves exactly like the old one. In the&#10;case of apps, this means the new instance needs to have the same state and identity as&#10;the old one.&#10;COMPARING STATEFULSETS WITH REPLICASETS OR REPLICATIONCONTROLLERS&#10;Pod replicas managed by a ReplicaSet or ReplicationController are much like cattle.&#10;Because they&#8217;re mostly stateless, they can be replaced with a completely new pod&#10;replica at any time. Stateful pods require a different approach. When a stateful pod&#10;instance dies (or the node it&#8217;s running on fails), the pod instance needs to be resur-&#10;rected on another node, but the new instance needs to get the same name, network&#10;identity, and state as the one it&#8217;s replacing. This is what happens when the pods are&#10;managed through a StatefulSet. &#10; &#10;"
    color "green"
  ]
  node [
    id 458
    label "317"
    title "Page_317"
    color "blue"
  ]
  node [
    id 459
    label "text_228"
    title "285&#10;Understanding StatefulSets&#10; A StatefulSet makes sure pods are rescheduled in such a way that they retain their&#10;identity and state. It also allows you to easily scale the number of pets up and down. A&#10;StatefulSet, like a ReplicaSet, has a desired replica count field that determines how&#10;many pets you want running at that time. Similar to ReplicaSets, pods are created from&#10;a pod template specified as part of the StatefulSet (remember the cookie-cutter anal-&#10;ogy?). But unlike pods created by ReplicaSets, pods created by the StatefulSet aren&#8217;t&#10;exact replicas of each other. Each can have its own set of volumes&#8212;in other words,&#10;storage (and thus persistent state)&#8212;which differentiates it from its peers. Pet pods&#10;also have a predictable (and stable) identity instead of each new pod instance getting&#10;a completely random one. &#10;10.2.2 Providing a stable network identity&#10;Each pod created by a StatefulSet is assigned an ordinal index (zero-based), which&#10;is then used to derive the pod&#8217;s name and hostname, and to attach stable storage to&#10;the pod. The names of the pods are thus predictable, because each pod&#8217;s name is&#10;derived from the StatefulSet&#8217;s name and the ordinal index of the instance. Rather&#10;than the pods having random names, they&#8217;re nicely organized, as shown in the next&#10;figure.&#10;INTRODUCING THE GOVERNING SERVICE&#10;But it&#8217;s not all about the pods having a predictable name and hostname. Unlike regu-&#10;lar pods, stateful pods sometimes need to be addressable by their hostname, whereas&#10;stateless pods usually don&#8217;t. After all, each stateless pod is like any other. When you&#10;need one, you pick any one of them. But with stateful pods, you usually want to oper-&#10;ate on a specific pod from the group, because they differ from each other (they hold&#10;different state, for example). &#10; For this reason, a StatefulSet requires you to create a corresponding governing&#10;headless Service that&#8217;s used to provide the actual network identity to each pod.&#10;Through this Service, each pod gets its own DNS entry, so its peers and possibly other&#10;clients in the cluster can address the pod by its hostname. For example, if the govern-&#10;ing Service belongs to the default namespace and is called foo, and one of the pods&#10;ReplicaSet A&#10;Pod A-fewrb&#10;Pod A-jwqec&#10;Pod A-dsfwx&#10;StatefulSet A&#10;Pod A-1&#10;Pod A-2&#10;Pod A-0&#10;Figure 10.5&#10;Pods created by a StatefulSet have predictable names (and hostnames), &#10;unlike those created by a ReplicaSet&#10; &#10;"
    color "green"
  ]
  node [
    id 460
    label "318"
    title "Page_318"
    color "blue"
  ]
  node [
    id 461
    label "text_229"
    title "286&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10;is called A-0, you can reach the pod through its fully qualified domain name, which&#10;is a-0.foo.default.svc.cluster.local. You can&#8217;t do that with pods managed by a&#10;ReplicaSet.&#10; Additionally, you can also use DNS to look up all the StatefulSet&#8217;s pods&#8217; names by&#10;looking up SRV records for the foo.default.svc.cluster.local domain. We&#8217;ll&#10;explain SRV records in section 10.4 and learn how they&#8217;re used to discover members&#10;of a StatefulSet.&#10;REPLACING LOST PETS&#10;When a pod instance managed by a StatefulSet disappears (because the node the pod&#10;was running on has failed, it was evicted from the node, or someone deleted the pod&#10;object manually), the StatefulSet makes sure it&#8217;s replaced with a new instance&#8212;similar&#10;to how ReplicaSets do it. But in contrast to ReplicaSets, the replacement pod gets the&#10;same name and hostname as the pod that has disappeared (this distinction between&#10;ReplicaSets and StatefulSets is illustrated in figure 10.6).&#10;Node 1&#10;Node 2&#10;Node 1&#10;Node 2&#10;ReplicaSet B&#10;ReplicaSet B&#10;StatefulSet&#10;StatefulSet A&#10;Pod A-0&#10;Pod A-1&#10;Pod A-0&#10;Pod A-0&#10;Pod A-1&#10;Node 1 fails&#10;StatefulSet A&#10;Node 1&#10;Node 2&#10;Node 1&#10;Node 2&#10;ReplicaSet&#10;Node 1 fails&#10;Pod B-fdawr&#10;Pod B-jkbde&#10;Pod B-fdawr&#10;Pod B-rsqkw&#10;Pod B-jkbde&#10;Figure 10.6&#10;A StatefulSet replaces a lost pod with a new one with the same identity, whereas a &#10;ReplicaSet replaces it with a completely new unrelated pod.&#10; &#10;"
    color "green"
  ]
  node [
    id 462
    label "319"
    title "Page_319"
    color "blue"
  ]
  node [
    id 463
    label "text_230"
    title "287&#10;Understanding StatefulSets&#10;The new pod isn&#8217;t necessarily scheduled to the same node, but as you learned early&#10;on, what node a pod runs on shouldn&#8217;t matter. This holds true even for stateful pods.&#10;Even if the pod is scheduled to a different node, it will still be available and reachable&#10;under the same hostname as before. &#10;SCALING A STATEFULSET&#10;Scaling the StatefulSet creates a new pod instance with the next unused ordinal index.&#10;If you scale up from two to three instances, the new instance will get index 2 (the exist-&#10;ing instances obviously have indexes 0 and 1). &#10; The nice thing about scaling down a StatefulSet is the fact that you always know&#10;what pod will be removed. Again, this is also in contrast to scaling down a ReplicaSet,&#10;where you have no idea what instance will be deleted, and you can&#8217;t even specify&#10;which one you want removed first (but this feature may be introduced in the future).&#10;Scaling down a StatefulSet always removes the instances with the highest ordinal index&#10;first (shown in figure 10.7). This makes the effects of a scale-down predictable.&#10;Because certain stateful applications don&#8217;t handle rapid scale-downs nicely, Stateful-&#10;Sets scale down only one pod instance at a time. A distributed data store, for example,&#10;may lose data if multiple nodes go down at the same time. For example, if a replicated&#10;data store is configured to store two copies of each data entry, in cases where two&#10;nodes go down at the same time, a data entry would be lost if it was stored on exactly&#10;those two nodes. If the scale-down was sequential, the distributed data store has time&#10;to create an additional replica of the data entry somewhere else to replace the (single)&#10;lost copy.&#10; For this exact reason, StatefulSets also never permit scale-down operations if any of&#10;the instances are unhealthy. If an instance is unhealthy, and you scale down by one at&#10;the same time, you&#8217;ve effectively lost two cluster members at once.&#10;10.2.3 Providing stable dedicated storage to each stateful instance&#10;You&#8217;ve seen how StatefulSets ensure stateful pods have a stable identity, but what&#10;about storage? Each stateful pod instance needs to use its own storage, plus if a state-&#10;ful pod is rescheduled (replaced with a new instance but with the same identity as&#10;before), the new instance must have the same storage attached to it. How do Stateful-&#10;Sets achieve this?&#10;Pod&#10;A-0&#10;Pod&#10;A-1&#10;Pod&#10;A-2&#10;StatefulSet A&#10;Replicas: 3&#10;Pod&#10;A-0&#10;Pod&#10;A-1&#10;Pod&#10;A-2&#10;StatefulSet A&#10;Replicas: 2&#10;Pod&#10;A-0&#10;Pod&#10;A-1&#10;StatefulSet A&#10;Replicas: 1&#10;Scale down&#10;Scale down&#10;Figure 10.7&#10;Scaling down a StatefulSet always removes the pod with the highest ordinal index first.&#10; &#10;"
    color "green"
  ]
  node [
    id 464
    label "320"
    title "Page_320"
    color "blue"
  ]
  node [
    id 465
    label "text_231"
    title "288&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10; Obviously, storage for stateful pods needs to be persistent and decoupled from&#10;the pods. In chapter 6 you learned about PersistentVolumes and PersistentVolume-&#10;Claims, which allow persistent storage to be attached to a pod by referencing the&#10;PersistentVolumeClaim in the pod by name. Because PersistentVolumeClaims map&#10;to PersistentVolumes one-to-one, each pod of a StatefulSet needs to reference a dif-&#10;ferent PersistentVolumeClaim to have its own separate PersistentVolume. Because&#10;all pod instances are stamped from the same pod template, how can they each refer&#10;to a different PersistentVolumeClaim? And who creates these claims? Surely you&#8217;re&#10;not expected to create as many PersistentVolumeClaims as the number of pods you&#10;plan to have in the StatefulSet upfront? Of course not.&#10;TEAMING UP POD TEMPLATES WITH VOLUME CLAIM TEMPLATES&#10;The StatefulSet has to create the PersistentVolumeClaims as well, the same way it&#8217;s cre-&#10;ating the pods. For this reason, a StatefulSet can also have one or more volume claim&#10;templates, which enable it to stamp out PersistentVolumeClaims along with each pod&#10;instance (see figure 10.8).&#10;The PersistentVolumes for the claims can either be provisioned up-front by an admin-&#10;istrator or just in time through dynamic provisioning of PersistentVolumes, as explained&#10;at the end of chapter 6. &#10;UNDERSTANDING THE CREATION AND DELETION OF PERSISTENTVOLUMECLAIMS&#10;Scaling up a StatefulSet by one creates two or more API objects (the pod and one or&#10;more PersistentVolumeClaims referenced by the pod). Scaling down, however, deletes&#10;only the pod, leaving the claims alone. The reason for this is obvious, if you consider&#10;what happens when a claim is deleted. After a claim is deleted, the PersistentVolume it&#10;was bound to gets recycled or deleted and its contents are lost. &#10; Because stateful pods are meant to run stateful applications, which implies that the&#10;data they store in the volume is important, deleting the claim on scale-down of a Stateful-&#10;Set could be catastrophic&#8212;especially since triggering a scale-down is as simple as&#10;decreasing the replicas field of the StatefulSet. For this reason, you&#8217;re required to&#10;delete PersistentVolumeClaims manually to release the underlying PersistentVolume.&#10;PVC A-0&#10;PV&#10;Pod A-0&#10;PVC A-1&#10;PV&#10;Pod A-1&#10;PVC A-2&#10;PV&#10;Pod A-2&#10;StatefulSet A&#10;Pod&#10;template&#10;Volume claim&#10;template&#10;Figure 10.8&#10;A StatefulSet creates both pods and PersistentVolumeClaims.&#10; &#10;"
    color "green"
  ]
  node [
    id 466
    label "321"
    title "Page_321"
    color "blue"
  ]
  node [
    id 467
    label "text_232"
    title "289&#10;Understanding StatefulSets&#10;REATTACHING THE PERSISTENTVOLUMECLAIM TO THE NEW INSTANCE OF THE SAME POD&#10;The fact that the PersistentVolumeClaim remains after a scale-down means a subse-&#10;quent scale-up can reattach the same claim along with the bound PersistentVolume&#10;and its contents to the new pod instance (shown in figure 10.9). If you accidentally&#10;scale down a StatefulSet, you can undo the mistake by scaling up again and the new&#10;pod will get the same persisted state again (as well as the same name).&#10;10.2.4 Understanding StatefulSet guarantees&#10;As you&#8217;ve seen so far, StatefulSets behave differently from ReplicaSets or Replication-&#10;Controllers. But this doesn&#8217;t end with the pods having a stable identity and storage.&#10;StatefulSets also have different guarantees regarding their pods. &#10;UNDERSTANDING THE IMPLICATIONS OF STABLE IDENTITY AND STORAGE&#10;While regular, stateless pods are fungible, stateful pods aren&#8217;t. We&#8217;ve already seen how&#10;a stateful pod is always replaced with an identical pod (one having the same name and&#10;hostname, using the same persistent storage, and so on). This happens when Kuber-&#10;netes sees that the old pod is no longer there (for example, when you delete the pod&#10;manually). &#10; But what if Kubernetes can&#8217;t be sure about the state of the pod? If it creates a&#10;replacement pod with the same identity, two instances of the app with the same iden-&#10;tity might be running in the system. The two would also be bound to the same storage,&#10;Pod&#10;A-0&#10;Pod&#10;A-1&#10;StatefulSet A&#10;Replicas: 2&#10;Scale&#10;down&#10;Scale&#10;up&#10;New pod instance created&#10;with same identity as before&#10;PVC is&#10;re-attached&#10;PVC&#10;A-0&#10;PV&#10;PVC&#10;A-1&#10;PV&#10;Pod&#10;A-0&#10;StatefulSet A&#10;Replicas: 1&#10;PVC&#10;A-0&#10;PV&#10;PVC&#10;A-1&#10;PV&#10;Pod&#10;A-0&#10;Pod has been deleted&#10;Pod&#10;A-1&#10;StatefulSet A&#10;Replicas: 2&#10;PVC&#10;A-0&#10;PV&#10;PVC&#10;A-1&#10;PVC has not&#10;been deleted&#10;PV&#10;Figure 10.9&#10;StatefulSets don&#8217;t delete PersistentVolumeClaims when scaling down; then they &#10;reattach them when scaling back up.&#10; &#10;"
    color "green"
  ]
  node [
    id 468
    label "322"
    title "Page_322"
    color "blue"
  ]
  node [
    id 469
    label "text_233"
    title "290&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10;so two processes with the same identity would be writing over the same files. With pods&#10;managed by a ReplicaSet, this isn&#8217;t a problem, because the apps are obviously made to&#10;work on the same files. Also, ReplicaSets create pods with a randomly generated iden-&#10;tity, so there&#8217;s no way for two processes to run with the same identity. &#10;INTRODUCING STATEFULSET&#8217;S AT-MOST-ONE SEMANTICS&#10;Kubernetes must thus take great care to ensure two stateful pod instances are never&#10;running with the same identity and are bound to the same PersistentVolumeClaim. A&#10;StatefulSet must guarantee at-most-one semantics for stateful pod instances. &#10; This means a StatefulSet must be absolutely certain that a pod is no longer run-&#10;ning before it can create a replacement pod. This has a big effect on how node fail-&#10;ures are handled. We&#8217;ll demonstrate this later in the chapter. Before we can do that,&#10;however, you need to create a StatefulSet and see how it behaves. You&#8217;ll also learn a&#10;few more things about them along the way.&#10;10.3&#10;Using a StatefulSet&#10;To properly show StatefulSets in action, you&#8217;ll build your own little clustered data&#10;store. Nothing fancy&#8212;more like a data store from the Stone Age. &#10;10.3.1 Creating the app and container image&#10;You&#8217;ll use the kubia app you&#8217;ve used throughout the book as your starting point. You&#8217;ll&#10;expand it so it allows you to store and retrieve a single data entry on each pod instance. &#10; The important parts of the source code of your data store are shown in the follow-&#10;ing listing.&#10;...&#10;const dataFile = &#34;/var/data/kubia.txt&#34;;&#10;...&#10;var handler = function(request, response) {&#10;  if (request.method == 'POST') {                &#10;    var file = fs.createWriteStream(dataFile);                     &#10;    file.on('open', function (fd) {                                &#10;      request.pipe(file);                                          &#10;      console.log(&#34;New data has been received and stored.&#34;);       &#10;      response.writeHead(200);                                     &#10;      response.end(&#34;Data stored on pod &#34; + os.hostname() + &#34;\n&#34;);  &#10;    });&#10;  } else {                                       &#10;    var data = fileExists(dataFile)                                &#10;      ? fs.readFileSync(dataFile, 'utf8')                          &#10;      : &#34;No data posted yet&#34;;                                      &#10;    response.writeHead(200);                                       &#10;    response.write(&#34;You've hit &#34; + os.hostname() + &#34;\n&#34;);          &#10;    response.end(&#34;Data stored on this pod: &#34; + data + &#34;\n&#34;);       &#10;  }&#10;};&#10;Listing 10.1&#10;A simple stateful app: kubia-pet-image/app.js&#10;On POST &#10;requests, store &#10;the request&#8217;s &#10;body into a &#10;data file.&#10;On GET (and all &#10;other types of) &#10;requests, return &#10;your hostname &#10;and the contents &#10;of the data file.&#10; &#10;"
    color "green"
  ]
  node [
    id 470
    label "323"
    title "Page_323"
    color "blue"
  ]
  node [
    id 471
    label "text_234"
    title "291&#10;Using a StatefulSet&#10;var www = http.createServer(handler);&#10;www.listen(8080);&#10;Whenever the app receives a POST request, it writes the data it receives in the body of&#10;the request to the file /var/data/kubia.txt. Upon a GET request, it returns the host-&#10;name and the stored data (contents of the file). Simple enough, right? This is the first&#10;version of your app. It&#8217;s not clustered yet, but it&#8217;s enough to get you started. You&#8217;ll&#10;expand the app later in the chapter.&#10; The Dockerfile for building the container image is shown in the following listing&#10;and hasn&#8217;t changed from before.&#10;FROM node:7&#10;ADD app.js /app.js&#10;ENTRYPOINT [&#34;node&#34;, &#34;app.js&#34;]&#10;Go ahead and build the image now, or use the one I pushed to docker.io/luksa/kubia-pet.&#10;10.3.2 Deploying the app through a StatefulSet&#10;To deploy your app, you&#8217;ll need to create two (or three) different types of objects:&#10;&#61601;PersistentVolumes for storing your data files (you&#8217;ll need to create these only if&#10;the cluster doesn&#8217;t support dynamic provisioning of PersistentVolumes).&#10;&#61601;A governing Service required by the StatefulSet.&#10;&#61601;The StatefulSet itself.&#10;For each pod instance, the StatefulSet will create a PersistentVolumeClaim that will&#10;bind to a PersistentVolume. If your cluster supports dynamic provisioning, you don&#8217;t&#10;need to create any PersistentVolumes manually (you can skip the next section). If it&#10;doesn&#8217;t, you&#8217;ll need to create them as explained in the next section. &#10;CREATING THE PERSISTENT VOLUMES&#10;You&#8217;ll need three PersistentVolumes, because you&#8217;ll be scaling the StatefulSet up to&#10;three replicas. You must create more if you plan on scaling the StatefulSet up more&#10;than that.&#10; If you&#8217;re using Minikube, deploy the PersistentVolumes defined in the Chapter06/&#10;persistent-volumes-hostpath.yaml file in the book&#8217;s code archive. &#10; If you&#8217;re using Google Kubernetes Engine, you&#8217;ll first need to create the actual&#10;GCE Persistent Disks like this:&#10;$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-a&#10;$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-b&#10;$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-c&#10;NOTE&#10;Make sure to create the disks in the same zone that your nodes are&#10;running in.&#10;Listing 10.2&#10;Dockerfile for the stateful app: kubia-pet-image/Dockerfile&#10; &#10;"
    color "green"
  ]
  node [
    id 472
    label "324"
    title "Page_324"
    color "blue"
  ]
  node [
    id 473
    label "text_235"
    title "292&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10;Then create the PersistentVolumes from the persistent-volumes-gcepd.yaml file,&#10;which is shown in the following listing.&#10;kind: List                     &#10;apiVersion: v1&#10;items:&#10;- apiVersion: v1&#10;  kind: PersistentVolume       &#10;  metadata:&#10;    name: pv-a                &#10;  spec:&#10;    capacity:&#10;      storage: 1Mi            &#10;    accessModes:&#10;      - ReadWriteOnce&#10;    persistentVolumeReclaimPolicy: Recycle     &#10;    gcePersistentDisk:         &#10;      pdName: pv-a             &#10;      fsType: nfs4                         &#10;- apiVersion: v1&#10;  kind: PersistentVolume&#10;  metadata:&#10;    name: pv-b&#10; ...&#10;NOTE&#10;In the previous chapter you specified multiple resources in the same&#10;YAML by delimiting them with a three-dash line. Here you&#8217;re using a differ-&#10;ent approach by defining a List object and listing the resources as items of&#10;the object. Both methods are equivalent.&#10;This manifest creates PersistentVolumes called pv-a, pv-b, and pv-c. They use GCE Per-&#10;sistent Disks as the underlying storage mechanism, so they&#8217;re not appropriate for clus-&#10;ters that aren&#8217;t running on Google Kubernetes Engine or Google Compute Engine. If&#10;you&#8217;re running the cluster elsewhere, you must modify the PersistentVolume definition&#10;and use an appropriate volume type, such as NFS (Network File System), or similar.&#10;CREATING THE GOVERNING SERVICE&#10;As explained earlier, before deploying a StatefulSet, you first need to create a headless&#10;Service, which will be used to provide the network identity for your stateful pods. The&#10;following listing shows the Service manifest.&#10;apiVersion: v1&#10;kind: Service&#10;metadata:&#10;  name: kubia       &#10;spec:&#10;  clusterIP: None    &#10;Listing 10.3&#10;Three PersistentVolumes: persistent-volumes-gcepd.yaml&#10;Listing 10.4&#10;Headless service to be used in the StatefulSet: kubia-service-headless.yaml&#10;File describes a list &#10;of three persistent &#10;volumes&#10;Persistent volumes&#8217; names &#10;are pv-a, pv-b, and pv-c&#10;Capacity of each persistent &#10;volume is 1 Mebibyte&#10;When the volume &#10;is released by the &#10;claim, it&#8217;s recycled &#10;to be used again.&#10;The volume uses a GCE &#10;Persistent Disk as the underlying &#10;storage mechanism.&#10;Name of the &#10;Service&#10;The StatefulSet&#8217;s governing &#10;Service must be headless.&#10; &#10;"
    color "green"
  ]
  node [
    id 474
    label "325"
    title "Page_325"
    color "blue"
  ]
  node [
    id 475
    label "text_236"
    title "293&#10;Using a StatefulSet&#10;  selector:           &#10;    app: kubia        &#10;  ports:&#10;  - name: http&#10;    port: 80&#10;You&#8217;re setting the clusterIP field to None, which makes this a headless Service. It will&#10;enable peer discovery between your pods (you&#8217;ll need this later). Once you create the&#10;Service, you can move on to creating the actual StatefulSet.&#10;CREATING THE STATEFULSET MANIFEST&#10;Now you can finally create the StatefulSet. The following listing shows the manifest.&#10;apiVersion: apps/v1beta1&#10;kind: StatefulSet&#10;metadata:&#10;  name: kubia&#10;spec:&#10;  serviceName: kubia&#10;  replicas: 2&#10;  template:&#10;    metadata:&#10;      labels:                  &#10;        app: kubia             &#10;    spec:&#10;      containers:&#10;      - name: kubia&#10;        image: luksa/kubia-pet&#10;        ports:&#10;        - name: http&#10;          containerPort: 8080&#10;        volumeMounts:&#10;        - name: data                  &#10;          mountPath: /var/data        &#10;  volumeClaimTemplates:&#10;  - metadata:                  &#10;      name: data               &#10;    spec:                      &#10;      resources:               &#10;        requests:              &#10;          storage: 1Mi         &#10;      accessModes:             &#10;      - ReadWriteOnce          &#10;The StatefulSet manifest isn&#8217;t that different from ReplicaSet or Deployment manifests&#10;you&#8217;ve created so far. What&#8217;s new is the volumeClaimTemplates list. In it, you&#8217;re defin-&#10;ing one volume claim template called data, which will be used to create a Persistent-&#10;VolumeClaim for each pod. As you may remember from chapter 6, a pod references a&#10;claim by including a persistentVolumeClaim volume in the manifest. In the previous&#10;Listing 10.5&#10;StatefulSet manifest: kubia-statefulset.yaml&#10;All pods with the app=kubia &#10;label belong to this service.&#10;Pods created by the StatefulSet &#10;will have the app=kubia label.&#10;The container inside the pod will &#10;mount the pvc volume at this path.&#10;The PersistentVolumeClaims &#10;will be created from this &#10;template.&#10; &#10;"
    color "green"
  ]
  node [
    id 476
    label "326"
    title "Page_326"
    color "blue"
  ]
  node [
    id 477
    label "text_237"
    title "294&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10;pod template, you&#8217;ll find no such volume. The StatefulSet adds it to the pod specifica-&#10;tion automatically and configures the volume to be bound to the claim the StatefulSet&#10;created for the specific pod.&#10;CREATING THE STATEFULSET&#10;You&#8217;ll create the StatefulSet now:&#10;$ kubectl create -f kubia-statefulset.yaml &#10;statefulset &#34;kubia&#34; created&#10;Now, list your pods:&#10;$ kubectl get po&#10;NAME      READY     STATUS              RESTARTS   AGE&#10;kubia-0   0/1       ContainerCreating   0          1s&#10;Notice anything strange? Remember how a ReplicationController or a ReplicaSet cre-&#10;ates all the pod instances at the same time? Your StatefulSet is configured to create&#10;two replicas, but it created a single pod. &#10; Don&#8217;t worry, nothing is wrong. The second pod will be created only after the first&#10;one is up and ready. StatefulSets behave this way because certain clustered stateful&#10;apps are sensitive to race conditions if two or more cluster members come up at the&#10;same time, so it&#8217;s safer to bring each member up fully before continuing to bring up&#10;the rest.&#10; List the pods again to see how the pod creation is progressing:&#10;$ kubectl get po&#10;NAME      READY     STATUS              RESTARTS   AGE&#10;kubia-0   1/1       Running             0          8s&#10;kubia-1   0/1       ContainerCreating   0          2s&#10;See, the first pod is now running, and the second one has been created and is being&#10;started. &#10;EXAMINING THE GENERATED STATEFUL POD&#10;Let&#8217;s take a closer look at the first pod&#8217;s spec in the following listing to see how the&#10;StatefulSet has constructed the pod from the pod template and the PersistentVolume-&#10;Claim template.&#10;$ kubectl get po kubia-0 -o yaml&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  ...&#10;spec:&#10;  containers:&#10;  - image: luksa/kubia-pet&#10;    ...&#10;Listing 10.6&#10;A stateful pod created by the StatefulSet&#10; &#10;"
    color "green"
  ]
  node [
    id 478
    label "327"
    title "Page_327"
    color "blue"
  ]
  node [
    id 479
    label "text_238"
    title "295&#10;Using a StatefulSet&#10;    volumeMounts:&#10;    - mountPath: /var/data           &#10;      name: data                     &#10;    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount&#10;      name: default-token-r2m41&#10;      readOnly: true&#10;  ...&#10;  volumes:&#10;  - name: data                       &#10;    persistentVolumeClaim:           &#10;      claimName: data-kubia-0            &#10;  - name: default-token-r2m41&#10;    secret:&#10;      secretName: default-token-r2m41&#10;The PersistentVolumeClaim template was used to create the PersistentVolumeClaim&#10;and the volume inside the pod, which refers to the created PersistentVolumeClaim. &#10;EXAMINING THE GENERATED PERSISTENTVOLUMECLAIMS&#10;Now list the generated PersistentVolumeClaims to confirm they were created:&#10;$ kubectl get pvc&#10;NAME           STATUS    VOLUME    CAPACITY   ACCESSMODES   AGE&#10;data-kubia-0   Bound     pv-c      0                        37s&#10;data-kubia-1   Bound     pv-a      0                        37s&#10;The names of the generated PersistentVolumeClaims are composed of the name&#10;defined in the volumeClaimTemplate and the name of each pod. You can examine the&#10;claims&#8217; YAML to see that they match the template.&#10;10.3.3 Playing with your pods&#10;With the nodes of your data store cluster now running, you can start exploring it. You&#10;can&#8217;t communicate with your pods through the Service you created because it&#8217;s head-&#10;less. You&#8217;ll need to connect to individual pods directly (or create a regular Service, but&#10;that wouldn&#8217;t allow you to talk to a specific pod).&#10; You&#8217;ve already seen ways to connect to a pod directly: by piggybacking on another&#10;pod and running curl inside it, by using port-forwarding, and so on. This time, you&#8217;ll&#10;try another option. You&#8217;ll use the API server as a proxy to the pods. &#10;COMMUNICATING WITH PODS THROUGH THE API SERVER&#10;One useful feature of the API server is the ability to proxy connections directly to indi-&#10;vidual pods. If you want to perform requests against your kubia-0 pod, you hit the fol-&#10;lowing URL:&#10;<apiServerHost>:<port>/api/v1/namespaces/default/pods/kubia-0/proxy/<path>&#10;Because the API server is secured, sending requests to pods through the API server is&#10;cumbersome (among other things, you need to pass the authorization token in each&#10;request). Luckily, in chapter 8 you learned how to use kubectl proxy to talk to the&#10;The volume mount, as &#10;specified in the manifest&#10;The volume created &#10;by the StatefulSet&#10;The claim referenced &#10;by this volume&#10; &#10;"
    color "green"
  ]
  node [
    id 480
    label "328"
    title "Page_328"
    color "blue"
  ]
  node [
    id 481
    label "text_239"
    title "296&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10;API server without having to deal with authentication and SSL certificates. Run the&#10;proxy again:&#10;$ kubectl proxy&#10;Starting to serve on 127.0.0.1:8001&#10;Now, because you&#8217;ll be talking to the API server through the kubectl proxy, you&#8217;ll use&#10;localhost:8001 rather than the actual API server host and port. You&#8217;ll send a request to&#10;the kubia-0 pod like this:&#10;$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/&#10;You've hit kubia-0&#10;Data stored on this pod: No data posted yet&#10;The response shows that the request was indeed received and handled by the app run-&#10;ning in your pod kubia-0. &#10;NOTE&#10;If you receive an empty response, make sure you haven&#8217;t left out that&#10;last slash character at the end of the URL (or make sure curl follows redirects&#10;by using its -L option). &#10;Because you&#8217;re communicating with the pod through the API server, which you&#8217;re&#10;connecting to through the kubectl proxy, the request went through two different&#10;proxies (the first was the kubectl proxy and the other was the API server, which prox-&#10;ied the request to the pod). For a clearer picture, examine figure 10.10.&#10;The request you sent to the pod was a GET request, but you can also send POST&#10;requests through the API server. This is done by sending a POST request to the same&#10;proxy URL as the one you sent the GET request to. &#10; When your app receives a POST request, it stores whatever&#8217;s in the request body&#10;into a local file. Send a POST request to the kubia-0 pod:&#10;$ curl -X POST -d &#34;Hey there! This greeting was submitted to kubia-0.&#34;&#10;&#10149; localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/&#10;Data stored on pod kubia-0&#10;kubectl proxy&#10;curl&#10;GET localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/&#10;GET 192.168.99.100:8443/api/v1/namespaces/default/pods/kubia-0/proxy/&#10;Authorization: Bearer <token>&#10;GET 172.17.0.3:8080/&#10;API server&#10;Pod: kubia-0&#10;192.168.99.100&#10;172.17.0.3&#10;localhost&#10;Figure 10.10&#10;Connecting to a pod through both the kubectl proxy and API server proxy&#10; &#10;"
    color "green"
  ]
  node [
    id 482
    label "329"
    title "Page_329"
    color "blue"
  ]
  node [
    id 483
    label "text_240"
    title "297&#10;Using a StatefulSet&#10;The data you sent should now be stored in that pod. Let&#8217;s see if it returns the stored&#10;data when you perform a GET request again:&#10;$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/&#10;You've hit kubia-0&#10;Data stored on this pod: Hey there! This greeting was submitted to kubia-0.&#10;Okay, so far so good. Now let&#8217;s see what the other cluster node (the kubia-1 pod)&#10;says:&#10;$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/&#10;You've hit kubia-1&#10;Data stored on this pod: No data posted yet&#10;As expected, each node has its own state. But is that state persisted? Let&#8217;s find out.&#10;DELETING A STATEFUL POD TO SEE IF THE RESCHEDULED POD IS REATTACHED TO THE SAME STORAGE&#10;You&#8217;re going to delete the kubia-0 pod and wait for it to be rescheduled. Then you&#8217;ll&#10;see if it&#8217;s still serving the same data as before:&#10;$ kubectl delete po kubia-0&#10;pod &#34;kubia-0&#34; deleted&#10;If you list the pods, you&#8217;ll see that the pod is terminating: &#10;$ kubectl get po&#10;NAME      READY     STATUS        RESTARTS   AGE&#10;kubia-0   1/1       Terminating   0          3m&#10;kubia-1   1/1       Running       0          3m&#10;As soon as it terminates successfully, a new pod with the same name is created by the&#10;StatefulSet:&#10;$ kubectl get po&#10;NAME      READY     STATUS              RESTARTS   AGE&#10;kubia-0   0/1       ContainerCreating   0          6s&#10;kubia-1   1/1       Running             0          4m&#10;$ kubectl get po&#10;NAME      READY     STATUS    RESTARTS   AGE&#10;kubia-0   1/1       Running   0          9s&#10;kubia-1   1/1       Running   0          4m&#10;Let me remind you again that this new pod may be scheduled to any node in the clus-&#10;ter, not necessarily the same node that the old pod was scheduled to. The old pod&#8217;s&#10;whole identity (the name, hostname, and the storage) is effectively moved to the new&#10;node (as shown in figure 10.11). If you&#8217;re using Minikube, you can&#8217;t see this because it&#10;only runs a single node, but in a multi-node cluster, you may see the pod scheduled to&#10;a different node than before.&#10; &#10;"
    color "green"
  ]
  node [
    id 484
    label "330"
    title "Page_330"
    color "blue"
  ]
  node [
    id 485
    label "text_241"
    title "298&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10;With the new pod now running, let&#8217;s check to see if it has the exact same identity as in&#10;its previous incarnation. The pod&#8217;s name is the same, but what about the hostname&#10;and persistent data? You can ask the pod itself to confirm:&#10;$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/&#10;You've hit kubia-0&#10;Data stored on this pod: Hey there! This greeting was submitted to kubia-0.&#10;The pod&#8217;s response shows that both the hostname and the data are the same as before,&#10;confirming that a StatefulSet always replaces a deleted pod with what&#8217;s effectively the&#10;exact same pod. &#10;SCALING A STATEFULSET&#10;Scaling down a StatefulSet and scaling it back up after an extended time period&#10;should be no different than deleting a pod and having the StatefulSet recreate it&#10;immediately. Remember that scaling down a StatefulSet only deletes the pods, but&#10;leaves the PersistentVolumeClaims untouched. I&#8217;ll let you try scaling down the State-&#10;fulSet yourself and confirm this behavior. &#10; The key thing to remember is that scaling down (and up) is performed gradu-&#10;ally&#8212;similar to how individual pods are created when the StatefulSet is created ini-&#10;tially. When scaling down by more than one instance, the pod with the highest ordinal&#10;number is deleted first. Only after the pod terminates completely is the pod with the&#10;second highest ordinal number deleted. &#10;EXPOSING STATEFUL PODS THROUGH A REGULAR, NON-HEADLESS SERVICE&#10;Before you move on to the last part of this chapter, you&#8217;re going to add a proper, non-&#10;headless Service in front of your pods, because clients usually connect to the pods&#10;through a Service rather than connecting directly.&#10;Node 1&#10;Pod: kubia-0&#10;Pod: kubia-1&#10;Delete kubia-0&#10;Storage&#10;Storage&#10;Storage&#10;Pod: kubia-1&#10;Storage&#10;Node 1&#10;kubia-0 rescheduled&#10;Node 1&#10;Node 2&#10;Node 2&#10;Node 2&#10;Storage&#10;Pod: kubia-1&#10;Storage&#10;Pod: kubia-0&#10;Figure 10.11&#10;A stateful pod may be rescheduled to a different node, but it retains the name, hostname, and storage.&#10; &#10;"
    color "green"
  ]
  node [
    id 486
    label "331"
    title "Page_331"
    color "blue"
  ]
  node [
    id 487
    label "text_242"
    title "299&#10;Discovering peers in a StatefulSet&#10; You know how to create the Service by now, but in case you don&#8217;t, the following list-&#10;ing shows the manifest.&#10;apiVersion: v1&#10;kind: Service&#10;metadata:&#10;  name: kubia-public&#10;spec:&#10;  selector:&#10;    app: kubia&#10;  ports:&#10;  - port: 80&#10;    targetPort: 8080&#10;Because this isn&#8217;t an externally exposed Service (it&#8217;s a regular ClusterIP Service, not&#10;a NodePort or a LoadBalancer-type Service), you can only access it from inside the&#10;cluster. You&#8217;ll need a pod to access it from, right? Not necessarily.&#10;CONNECTING TO CLUSTER-INTERNAL SERVICES THROUGH THE API SERVER&#10;Instead of using a piggyback pod to access the service from inside the cluster, you can&#10;use the same proxy feature provided by the API server to access the service the way&#10;you&#8217;ve accessed individual pods.&#10; The URI path for proxy-ing requests to Services is formed like this:&#10;/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>&#10;Therefore, you can run curl on your local machine and access the service through the&#10;kubectl proxy like this (you ran kubectl proxy earlier and it should still be running):&#10;$ curl localhost:8001/api/v1/namespaces/default/services/kubia-&#10;&#10149; public/proxy/&#10;You've hit kubia-1&#10;Data stored on this pod: No data posted yet&#10;Likewise, clients (inside the cluster) can use the kubia-public service for storing to&#10;and reading data from your clustered data store. Of course, each request lands on a&#10;random cluster node, so you&#8217;ll get the data from a random node each time. You&#8217;ll&#10;improve this next.&#10;10.4&#10;Discovering peers in a StatefulSet&#10;We still need to cover one more important thing. An important requirement of clus-&#10;tered apps is peer discovery&#8212;the ability to find other members of the cluster. Each&#10;member of a StatefulSet needs to easily find all the other members. Sure, it could do&#10;that by talking to the API server, but one of Kubernetes&#8217; aims is to expose features that&#10;help keep applications completely Kubernetes-agnostic. Having apps talk to the Kuber-&#10;netes API is therefore undesirable.&#10;Listing 10.7&#10;A regular Service for accessing the stateful pods: kubia-service-public.yaml&#10; &#10;"
    color "green"
  ]
  node [
    id 488
    label "332"
    title "Page_332"
    color "blue"
  ]
  node [
    id 489
    label "text_243"
    title "300&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10; How can a pod discover its peers without talking to the API? Is there an existing,&#10;well-known technology you can use that makes this possible? How about the Domain&#10;Name System (DNS)? Depending on how much you know about DNS, you probably&#10;understand what an A, CNAME, or MX record is used for. Other lesser-known types of&#10;DNS records also exist. One of them is the SRV record.&#10;INTRODUCING SRV RECORDS&#10;SRV records are used to point to hostnames and ports of servers providing a specific&#10;service. Kubernetes creates SRV records to point to the hostnames of the pods back-&#10;ing a headless service. &#10; You&#8217;re going to list the SRV records for your stateful pods by running the dig DNS&#10;lookup tool inside a new temporary pod. This is the command you&#8217;ll use:&#10;$ kubectl run -it srvlookup --image=tutum/dnsutils --rm &#10;&#10149; --restart=Never -- dig SRV kubia.default.svc.cluster.local&#10;The command runs a one-off pod (--restart=Never) called srvlookup, which is&#10;attached to the console (-it) and is deleted as soon as it terminates (--rm). The&#10;pod runs a single container from the tutum/dnsutils image and runs the following&#10;command:&#10;dig SRV kubia.default.svc.cluster.local&#10;The following listing shows what the command prints out.&#10;...&#10;;; ANSWER SECTION:&#10;k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.&#10;k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.&#10;;; ADDITIONAL SECTION:&#10;kubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4&#10;kubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6&#10;...&#10;NOTE&#10;I&#8217;ve had to shorten the actual name to get records to fit into a single&#10;line, so kubia.d.s.c.l is actually kubia.default.svc.cluster.local.&#10;The ANSWER SECTION shows two SRV records pointing to the two pods backing your head-&#10;less service. Each pod also gets its own A record, as shown in ADDITIONAL SECTION.&#10; For a pod to get a list of all the other pods of a StatefulSet, all you need to do is&#10;perform an SRV DNS lookup. In Node.js, for example, the lookup is performed&#10;like this:&#10;dns.resolveSrv(&#34;kubia.default.svc.cluster.local&#34;, callBackFunction);&#10;You&#8217;ll use this command in your app to enable each pod to discover its peers.&#10;Listing 10.8&#10;Listing DNS SRV records of your headless Service&#10; &#10;"
    color "green"
  ]
  node [
    id 490
    label "333"
    title "Page_333"
    color "blue"
  ]
  node [
    id 491
    label "text_244"
    title "301&#10;Discovering peers in a StatefulSet&#10;NOTE&#10;The order of the returned SRV records is random, because they all have&#10;the same priority. Don&#8217;t expect to always see kubia-0 listed before kubia-1.&#10;10.4.1 Implementing peer discovery through DNS&#10;Your Stone Age data store isn&#8217;t clustered yet. Each data store node runs completely&#10;independently of all the others&#8212;no communication exists between them. You&#8217;ll get&#10;them talking to each other next.&#10; Data posted by clients connecting to your data store cluster through the kubia-&#10;public Service lands on a random cluster node. The cluster can store multiple data&#10;entries, but clients currently have no good way to see all those entries. Because ser-&#10;vices forward requests to pods randomly, a client would need to perform many&#10;requests until it hit all the pods if it wanted to get the data from all the pods. &#10; You can improve this by having the node respond with data from all the cluster&#10;nodes. To do this, the node needs to find all its peers. You&#8217;re going to use what you&#10;learned about StatefulSets and SRV records to do this.&#10; You&#8217;ll modify your app&#8217;s source code as shown in the following listing (the full&#10;source is available in the book&#8217;s code archive; the listing shows only the important&#10;parts).&#10;...&#10;const dns = require('dns');&#10;const dataFile = &#34;/var/data/kubia.txt&#34;;&#10;const serviceName = &#34;kubia.default.svc.cluster.local&#34;;&#10;const port = 8080;&#10;...&#10;var handler = function(request, response) {&#10;  if (request.method == 'POST') {&#10;    ...&#10;  } else {&#10;    response.writeHead(200);&#10;    if (request.url == '/data') {&#10;      var data = fileExists(dataFile) &#10;        ? fs.readFileSync(dataFile, 'utf8') &#10;        : &#34;No data posted yet&#34;;&#10;      response.end(data);&#10;    } else {&#10;      response.write(&#34;You've hit &#34; + os.hostname() + &#34;\n&#34;);&#10;      response.write(&#34;Data stored in the cluster:\n&#34;);&#10;      dns.resolveSrv(serviceName, function (err, addresses) {    &#10;        if (err) {&#10;          response.end(&#34;Could not look up DNS SRV records: &#34; + err);&#10;          return;&#10;        }&#10;        var numResponses = 0;&#10;        if (addresses.length == 0) {&#10;          response.end(&#34;No peers discovered.&#34;);&#10;        } else {&#10;Listing 10.9&#10;Discovering peers in a sample app: kubia-pet-peers-image/app.js&#10;The app &#10;performs a DNS &#10;lookup to obtain &#10;SRV records.&#10; &#10;"
    color "green"
  ]
  node [
    id 492
    label "334"
    title "Page_334"
    color "blue"
  ]
  node [
    id 493
    label "text_245"
    title "302&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10;          addresses.forEach(function (item) {                   &#10;            var requestOptions = {&#10;              host: item.name, &#10;              port: port, &#10;              path: '/data'&#10;            };&#10;            httpGet(requestOptions, function (returnedData) {   &#10;              numResponses++;&#10;              response.write(&#34;- &#34; + item.name + &#34;: &#34; + returnedData);&#10;              response.write(&#34;\n&#34;);&#10;              if (numResponses == addresses.length) {&#10;                response.end();&#10;              }&#10;            });&#10;          });&#10;        }&#10;      });&#10;    }&#10;  }&#10;};&#10;...&#10;Figure 10.12 shows what happens when a GET request is received by your app. The&#10;server that receives the request first performs a lookup of SRV records for the head-&#10;less kubia service and then sends a GET request to each of the pods backing the ser-&#10;vice (even to itself, which obviously isn&#8217;t necessary, but I wanted to keep the code as&#10;simple as possible). It then returns a list of all the nodes along with the data stored on&#10;each of them.&#10;The container image containing this new version of the app is available at docker.io/&#10;luksa/kubia-pet-peers.&#10;10.4.2 Updating a StatefulSet&#10;Your StatefulSet is already running, so let&#8217;s see how to update its pod template so the&#10;pods use the new image. You&#8217;ll also set the replica count to 3 at the same time. To&#10;Each pod &#10;pointed to by &#10;an SRV record is &#10;then contacted &#10;to get its data.&#10;curl&#10;DNS&#10;1. GET /&#10;4. GET /data&#10;5. GET /data&#10;2. SRV lookup&#10;6. Return collated data&#10;kubia-0&#10;kubia-1&#10;kubia-2&#10;3. GET /data&#10;Figure 10.12&#10;The operation of your simplistic distributed data store&#10; &#10;"
    color "green"
  ]
  node [
    id 494
    label "335"
    title "Page_335"
    color "blue"
  ]
  node [
    id 495
    label "text_246"
    title "303&#10;Discovering peers in a StatefulSet&#10;update the StatefulSet, use the kubectl edit command (the patch command would&#10;be another option):&#10;$ kubectl edit statefulset kubia&#10;This opens the StatefulSet definition in your default editor. In the definition, change&#10;spec.replicas to 3 and modify the spec.template.spec.containers.image attri-&#10;bute so it points to the new image (luksa/kubia-pet-peers instead of luksa/kubia-&#10;pet). Save the file and exit the editor to update the StatefulSet. Two replicas were&#10;running previously, so you should now see an additional replica called kubia-2 start-&#10;ing. List the pods to confirm:&#10;$ kubectl get po&#10;NAME      READY     STATUS              RESTARTS   AGE&#10;kubia-0   1/1       Running             0          25m&#10;kubia-1   1/1       Running             0          26m&#10;kubia-2   0/1       ContainerCreating   0          4s&#10;The new pod instance is running the new image. But what about the existing two rep-&#10;licas? Judging from their age, they don&#8217;t seem to have been updated. This is expected,&#10;because initially, StatefulSets were more like ReplicaSets and not like Deployments,&#10;so they don&#8217;t perform a rollout when the template is modified. You need to delete&#10;the replicas manually and the StatefulSet will bring them up again based on the new&#10;template:&#10;$ kubectl delete po kubia-0 kubia-1&#10;pod &#34;kubia-0&#34; deleted&#10;pod &#34;kubia-1&#34; deleted&#10;NOTE&#10;Starting from Kubernetes version 1.7, StatefulSets support rolling&#10;updates the same way Deployments and DaemonSets do. See the StatefulSet&#8217;s&#10;spec.updateStrategy field documentation using kubectl explain for more&#10;information.&#10;10.4.3 Trying out your clustered data store&#10;Once the two pods are up, you can see if your shiny new Stone Age data store works as&#10;expected. Post a few requests to the cluster, as shown in the following listing.&#10;$ curl -X POST -d &#34;The sun is shining&#34; \&#10;&#10149; localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/&#10;Data stored on pod kubia-1&#10;$ curl -X POST -d &#34;The weather is sweet&#34; \&#10;&#10149; localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/&#10;Data stored on pod kubia-0&#10;Now, read the stored data, as shown in the following listing.&#10;Listing 10.10&#10;Writing to the clustered data store through the service&#10; &#10;"
    color "green"
  ]
  node [
    id 496
    label "336"
    title "Page_336"
    color "blue"
  ]
  node [
    id 497
    label "text_247"
    title "304&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10;$ curl localhost:8001/api/v1/namespaces/default/services&#10;&#10149; /kubia-public/proxy/&#10;You've hit kubia-2&#10;Data stored on each cluster node:&#10;- kubia-0.kubia.default.svc.cluster.local: The weather is sweet&#10;- kubia-1.kubia.default.svc.cluster.local: The sun is shining&#10;- kubia-2.kubia.default.svc.cluster.local: No data posted yet&#10;Nice! When a client request reaches one of your cluster nodes, it discovers all its&#10;peers, gathers data from them, and sends all the data back to the client. Even if you&#10;scale the StatefulSet up or down, the pod servicing the client&#8217;s request can always find&#10;all the peers running at that time. &#10; The app itself isn&#8217;t that useful, but I hope you found it a fun way to show how&#10;instances of a replicated stateful app can discover their peers and handle horizontal&#10;scaling with ease.&#10;10.5&#10;Understanding how StatefulSets deal with node &#10;failures&#10;In section 10.2.4 we stated that Kubernetes must be absolutely sure that a stateful&#10;pod is no longer running before creating its replacement. When a node fails&#10;abruptly, Kubernetes can&#8217;t know the state of the node or its pods. It can&#8217;t know&#10;whether the pods are no longer running, or if they still are and are possibly even still&#10;reachable, and it&#8217;s only the Kubelet that has stopped reporting the node&#8217;s state to&#10;the master.&#10; Because a StatefulSet guarantees that there will never be two pods running with&#10;the same identity and storage, when a node appears to have failed, the StatefulSet can-&#10;not and should not create a replacement pod until it knows for certain that the pod is&#10;no longer running. &#10; It can only know that when the cluster administrator tells it so. To do that, the&#10;admin needs to either delete the pod or delete the whole node (doing so then deletes&#10;all the pods scheduled to the node).&#10; As your final exercise in this chapter, you&#8217;ll look at what happens to StatefulSets&#10;and their pods when one of the cluster nodes gets disconnected from the network.&#10;10.5.1 Simulating a node&#8217;s disconnection from the network &#10;As in chapter 4, you&#8217;ll simulate the node disconnecting from the network by shutting&#10;down the node&#8217;s eth0 network interface. Because this example requires multiple&#10;nodes, you can&#8217;t run it on Minikube. You&#8217;ll use Google Kubernetes Engine instead.&#10;SHUTTING DOWN THE NODE&#8217;S NETWORK ADAPTER&#10;To shut down a node&#8217;s eth0 interface, you need to ssh into one of the nodes like this:&#10;$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1&#10;Listing 10.11&#10;Reading from the data store&#10; &#10;"
    color "green"
  ]
  node [
    id 498
    label "337"
    title "Page_337"
    color "blue"
  ]
  node [
    id 499
    label "text_248"
    title "305&#10;Understanding how StatefulSets deal with node failures&#10;Then, inside the node, run the following command:&#10;$ sudo ifconfig eth0 down&#10;Your ssh session will stop working, so you&#8217;ll need to open another terminal to continue.&#10;CHECKING THE NODE&#8217;S STATUS AS SEEN BY THE KUBERNETES MASTER&#10;With the node&#8217;s network interface down, the Kubelet running on the node can no&#10;longer contact the Kubernetes API server and let it know that the node and all its pods&#10;are still running.&#10; After a while, the control plane will mark the node as NotReady. You can see this&#10;when listing nodes, as the following listing shows.&#10;$ kubectl get node&#10;NAME                                   STATUS     AGE       VERSION&#10;gke-kubia-default-pool-32a2cac8-596v   Ready      16m       v1.6.2&#10;gke-kubia-default-pool-32a2cac8-m0g1   NotReady   16m       v1.6.2&#10;gke-kubia-default-pool-32a2cac8-sgl7   Ready      16m       v1.6.2&#10;Because the control plane is no longer getting status updates from the node, the&#10;status of all pods on that node is Unknown. This is shown in the pod list in the follow-&#10;ing listing.&#10;$ kubectl get po&#10;NAME      READY     STATUS    RESTARTS   AGE&#10;kubia-0   1/1       Unknown   0          15m&#10;kubia-1   1/1       Running   0          14m&#10;kubia-2   1/1       Running   0          13m&#10;As you can see, the kubia-0 pod&#8217;s status is no longer known because the pod was (and&#10;still is) running on the node whose network interface you shut down.&#10;UNDERSTANDING WHAT HAPPENS TO PODS WHOSE STATUS IS UNKNOWN&#10;If the node were to come back online and report its and its pod statuses again, the pod&#10;would again be marked as Running. But if the pod&#8217;s status remains unknown for more&#10;than a few minutes (this time is configurable), the pod is automatically evicted from&#10;the node. This is done by the master (the Kubernetes control plane). It evicts the pod&#10;by deleting the pod resource. &#10; When the Kubelet sees that the pod has been marked for deletion, it starts ter-&#10;minating the pod. In your case, the Kubelet can no longer reach the master (because&#10;you disconnected the node from the network), which means the pod will keep&#10;running.&#10;Listing 10.12&#10;Observing a failed node&#8217;s status change to NotReady&#10;Listing 10.13&#10;Observing the pod&#8217;s status change after its node becomes NotReady&#10; &#10;"
    color "green"
  ]
  node [
    id 500
    label "338"
    title "Page_338"
    color "blue"
  ]
  node [
    id 501
    label "text_249"
    title "306&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10; Let&#8217;s examine the current situation. Use kubectl describe to display details about&#10;the kubia-0 pod, as shown in the following listing.&#10;$ kubectl describe po kubia-0&#10;Name:        kubia-0&#10;Namespace:   default&#10;Node:        gke-kubia-default-pool-32a2cac8-m0g1/10.132.0.2&#10;...&#10;Status:      Terminating (expires Tue, 23 May 2017 15:06:09 +0200)&#10;Reason:      NodeLost&#10;Message:     Node gke-kubia-default-pool-32a2cac8-m0g1 which was &#10;             running pod kubia-0 is unresponsive&#10;The pod is shown as Terminating, with NodeLost listed as the reason for the termina-&#10;tion. The message says the node is considered lost because it&#8217;s unresponsive.&#10;NOTE&#10;What&#8217;s shown here is the control plane&#8217;s view of the world. In reality,&#10;the pod&#8217;s container is still running perfectly fine. It isn&#8217;t terminating at all.&#10;10.5.2 Deleting the pod manually&#10;You know the node isn&#8217;t coming back, but you need all three pods running to handle&#10;clients properly. You need to get the kubia-0 pod rescheduled to a healthy node. As&#10;mentioned earlier, you need to delete the node or the pod manually. &#10;DELETING THE POD IN THE USUAL WAY&#10;Delete the pod the way you&#8217;ve always deleted pods:&#10;$ kubectl delete po kubia-0&#10;pod &#34;kubia-0&#34; deleted&#10;All done, right? By deleting the pod, the StatefulSet should immediately create a&#10;replacement pod, which will get scheduled to one of the remaining nodes. List the&#10;pods again to confirm: &#10;$ kubectl get po&#10;NAME      READY     STATUS    RESTARTS   AGE&#10;kubia-0   1/1       Unknown   0          15m&#10;kubia-1   1/1       Running   0          14m&#10;kubia-2   1/1       Running   0          13m&#10;That&#8217;s strange. You deleted the pod a moment ago and kubectl said it had deleted it.&#10;Why is the same pod still there? &#10;NOTE&#10;The kubia-0 pod in the listing isn&#8217;t a new pod with the same name&#8212;&#10;this is clear by looking at the AGE column. If it were new, its age would be&#10;merely a few seconds.&#10;Listing 10.14&#10;Displaying details of the pod with the unknown status&#10; &#10;"
    color "green"
  ]
  node [
    id 502
    label "339"
    title "Page_339"
    color "blue"
  ]
  node [
    id 503
    label "text_250"
    title "307&#10;Summary&#10;UNDERSTANDING WHY THE POD ISN&#8217;T DELETED&#10;The pod was marked for deletion even before you deleted it. That&#8217;s because the con-&#10;trol plane itself already deleted it (in order to evict it from the node). &#10; If you look at listing 10.14 again, you&#8217;ll see that the pod&#8217;s status is Terminating.&#10;The pod was already marked for deletion earlier and will be removed as soon as the&#10;Kubelet on its node notifies the API server that the pod&#8217;s containers have terminated.&#10;Because the node&#8217;s network is down, this will never happen. &#10;FORCIBLY DELETING THE POD&#10;The only thing you can do is tell the API server to delete the pod without waiting for&#10;the Kubelet to confirm that the pod is no longer running. You do that like this:&#10;$ kubectl delete po kubia-0 --force --grace-period 0&#10;warning: Immediate deletion does not wait for confirmation that the running &#10;resource has been terminated. The resource may continue to run on the &#10;cluster indefinitely.&#10;pod &#34;kubia-0&#34; deleted&#10;You need to use both the --force and --grace-period 0 options. The warning dis-&#10;played by kubectl notifies you of what you did. If you list the pods again, you&#8217;ll finally&#10;see a new kubia-0 pod created:&#10;$ kubectl get po&#10;NAME          READY     STATUS              RESTARTS   AGE&#10;kubia-0       0/1       ContainerCreating   0          8s&#10;kubia-1       1/1       Running             0          20m&#10;kubia-2       1/1       Running             0          19m&#10;WARNING&#10;Don&#8217;t delete stateful pods forcibly unless you know the node is no&#10;longer running or is unreachable (and will remain so forever). &#10;Before continuing, you may want to bring the node you disconnected back online.&#10;You can do that by restarting the node through the GCE web console or in a terminal&#10;by issuing the following command:&#10;$ gcloud compute instances reset <node name>&#10;10.6&#10;Summary&#10;This concludes the chapter on using StatefulSets to deploy stateful apps. This chapter&#10;has shown you how to&#10;&#61601;Give replicated pods individual storage&#10;&#61601;Provide a stable identity to a pod&#10;&#61601;Create a StatefulSet and a corresponding headless governing Service&#10;&#61601;Scale and update a StatefulSet&#10;&#61601;Discover other members of the StatefulSet through DNS&#10; &#10;"
    color "green"
  ]
  node [
    id 504
    label "340"
    title "Page_340"
    color "blue"
  ]
  node [
    id 505
    label "text_251"
    title "308&#10;CHAPTER 10&#10;StatefulSets: deploying replicated stateful applications&#10;&#61601;Connect to other members through their host names&#10;&#61601;Forcibly delete stateful pods&#10;Now that you know the major building blocks you can use to have Kubernetes run and&#10;manage your apps, we can look more closely at how it does that. In the next chapter,&#10;you&#8217;ll learn about the individual components that control the Kubernetes cluster and&#10;keep your apps running.&#10; &#10;"
    color "green"
  ]
  node [
    id 506
    label "341"
    title "Page_341"
    color "blue"
  ]
  node [
    id 507
    label "text_252"
    title "309&#10;Understanding&#10;Kubernetes internals&#10;By reading this book up to this point, you&#8217;ve become familiar with what Kubernetes&#10;has to offer and what it does. But so far, I&#8217;ve intentionally not spent much time&#10;explaining exactly how it does all this because, in my opinion, it makes no sense to&#10;go into details of how a system works until you have a good understanding of what&#10;the system does. That&#8217;s why we haven&#8217;t talked about exactly how a pod is scheduled&#10;or how the various controllers running inside the Controller Manager make deployed&#10;resources come to life. Because you now know most resources that can be deployed in&#10;Kubernetes, it&#8217;s time to dive into how they&#8217;re implemented.&#10;This chapter covers&#10;&#61601;What components make up a Kubernetes cluster&#10;&#61601;What each component does and how it does it&#10;&#61601;How creating a Deployment object results in a &#10;running pod&#10;&#61601;What a running pod is&#10;&#61601;How the network between pods works&#10;&#61601;How Kubernetes Services work&#10;&#61601;How high-availability is achieved&#10; &#10;"
    color "green"
  ]
  node [
    id 508
    label "342"
    title "Page_342"
    color "blue"
  ]
  node [
    id 509
    label "text_253"
    title "310&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;11.1&#10;Understanding the architecture&#10;Before you look at how Kubernetes does what it does, let&#8217;s take a closer look at the&#10;components that make up a Kubernetes cluster. In chapter 1, you saw that a Kuberne-&#10;tes cluster is split into two parts:&#10;&#61601;The Kubernetes Control Plane&#10;&#61601;The (worker) nodes&#10;Let&#8217;s look more closely at what these two parts do and what&#8217;s running inside them.&#10;COMPONENTS OF THE CONTROL PLANE&#10;The Control Plane is what controls and makes the whole cluster function. To refresh&#10;your memory, the components that make up the Control Plane are&#10;&#61601;The etcd distributed persistent storage&#10;&#61601;The API server&#10;&#61601;The Scheduler&#10;&#61601;The Controller Manager&#10;These components store and manage the state of the cluster, but they aren&#8217;t what runs&#10;the application containers. &#10;COMPONENTS RUNNING ON THE WORKER NODES&#10;The task of running your containers is up to the components running on each&#10;worker node:&#10;&#61601;The Kubelet&#10;&#61601;The Kubernetes Service Proxy (kube-proxy)&#10;&#61601;The Container Runtime (Docker, rkt, or others)&#10;ADD-ON COMPONENTS&#10;Beside the Control Plane components and the components running on the nodes, a&#10;few add-on components are required for the cluster to provide everything discussed&#10;so far. This includes&#10;&#61601;The Kubernetes DNS server&#10;&#61601;The Dashboard&#10;&#61601;An Ingress controller&#10;&#61601;Heapster, which we&#8217;ll talk about in chapter 14&#10;&#61601;The Container Network Interface network plugin (we&#8217;ll explain it later in this&#10;chapter)&#10;11.1.1 The distributed nature of Kubernetes components&#10;The previously mentioned components all run as individual processes. The compo-&#10;nents and their inter-dependencies are shown in figure 11.1.&#10; &#10;"
    color "green"
  ]
  node [
    id 510
    label "343"
    title "Page_343"
    color "blue"
  ]
  node [
    id 511
    label "text_254"
    title "311&#10;Understanding the architecture&#10;To get all the features Kubernetes provides, all these components need to be running.&#10;But several can also perform useful work individually without the other components.&#10;You&#8217;ll see how as we examine each of them.&#10;HOW THESE COMPONENTS COMMUNICATE&#10;Kubernetes system components communicate only with the API server. They don&#8217;t&#10;talk to each other directly. The API server is the only component that communicates&#10;with etcd. None of the other components communicate with etcd directly, but instead&#10;modify the cluster state by talking to the API server.&#10; Connections between the API server and the other components are almost always&#10;initiated by the components, as shown in figure 11.1. But the API server does connect&#10;to the Kubelet when you use kubectl to fetch logs, use kubectl attach to connect to&#10;a running container, or use the kubectl port-forward command.&#10;NOTE&#10;The kubectl attach command is similar to kubectl exec, but it attaches&#10;to the main process running in the container instead of running an addi-&#10;tional one.&#10;RUNNING MULTIPLE INSTANCES OF INDIVIDUAL COMPONENTS&#10;Although the components on the worker nodes all need to run on the same node,&#10;the components of the Control Plane can easily be split across multiple servers. There&#10;Checking the status of the Control Plane components&#10;The API server exposes an API resource called ComponentStatus, which shows the&#10;health status of each Control Plane component. You can list the components and&#10;their statuses with kubectl:&#10;$ kubectl get componentstatuses&#10;NAME                 STATUS    MESSAGE              ERROR&#10;scheduler            Healthy   ok&#10;controller-manager   Healthy   ok&#10;etcd-0               Healthy   {&#34;health&#34;: &#34;true&#34;}&#10;Control Plane (master node)&#10;Worker node(s)&#10;etcd&#10;API server&#10;kube-proxy&#10;Kubelet&#10;Scheduler&#10;Controller&#10;Manager&#10;Controller&#10;Runtime&#10;Figure 11.1&#10;Kubernetes &#10;components of the Control &#10;Plane and the worker nodes&#10; &#10;"
    color "green"
  ]
  node [
    id 512
    label "344"
    title "Page_344"
    color "blue"
  ]
  node [
    id 513
    label "text_255"
    title "312&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;can be more than one instance of each Control Plane component running to ensure&#10;high availability. While multiple instances of etcd and API server can be active at the&#10;same time and do perform their jobs in parallel, only a single instance of the Sched-&#10;uler and the Controller Manager may be active at a given time&#8212;with the others in&#10;standby mode.&#10;HOW COMPONENTS ARE RUN&#10;The Control Plane components, as well as kube-proxy, can either be deployed on the&#10;system directly or they can run as pods (as shown in listing 11.1). You may be surprised&#10;to hear this, but it will all make sense later when we talk about the Kubelet. &#10; The Kubelet is the only component that always runs as a regular system compo-&#10;nent, and it&#8217;s the Kubelet that then runs all the other components as pods. To run the&#10;Control Plane components as pods, the Kubelet is also deployed on the master. The&#10;next listing shows pods in the kube-system namespace in a cluster created with&#10;kubeadm, which is explained in appendix B.&#10;$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName &#10;&#10149; --sort-by spec.nodeName -n kube-system&#10;POD                              NODE&#10;kube-controller-manager-master   master      &#10;kube-dns-2334855451-37d9k        master      &#10;etcd-master                      master      &#10;kube-apiserver-master            master      &#10;kube-scheduler-master            master      &#10;kube-flannel-ds-tgj9k            node1      &#10;kube-proxy-ny3xm                 node1      &#10;kube-flannel-ds-0eek8            node2      &#10;kube-proxy-sp362                 node2      &#10;kube-flannel-ds-r5yf4            node3      &#10;kube-proxy-og9ac                 node3      &#10;As you can see in the listing, all the Control Plane components are running as pods on&#10;the master node. There are three worker nodes, and each one runs the kube-proxy&#10;and a Flannel pod, which provides the overlay network for the pods (we&#8217;ll talk about&#10;Flannel later). &#10;TIP&#10;As shown in the listing, you can tell kubectl to display custom columns&#10;with the -o custom-columns option and sort the resource list with --sort-by.&#10;Now, let&#8217;s look at each of the components up close, starting with the lowest level com-&#10;ponent of the Control Plane&#8212;the persistent storage.&#10;11.1.2 How Kubernetes uses etcd&#10;All the objects you&#8217;ve created throughout this book&#8212;Pods, ReplicationControllers,&#10;Services, Secrets, and so on&#8212;need to be stored somewhere in a persistent manner so&#10;their manifests survive API server restarts and failures. For this, Kubernetes uses etcd,&#10;Listing 11.1&#10;Kubernetes components running as pods&#10;etcd, API server, Scheduler, &#10;Controller Manager, and &#10;the DNS server are running &#10;on the master.&#10;The three nodes each run &#10;a Kube Proxy pod and a &#10;Flannel networking pod.&#10; &#10;"
    color "green"
  ]
  node [
    id 514
    label "345"
    title "Page_345"
    color "blue"
  ]
  node [
    id 515
    label "text_256"
    title "313&#10;Understanding the architecture&#10;which is a fast, distributed, and consistent key-value store. Because it&#8217;s distributed,&#10;you can run more than one etcd instance to provide both high availability and bet-&#10;ter performance.&#10; The only component that talks to etcd directly is the Kubernetes API server. All&#10;other components read and write data to etcd indirectly through the API server. This&#10;brings a few benefits, among them a more robust optimistic locking system as well as&#10;validation; and, by abstracting away the actual storage mechanism from all the other&#10;components, it&#8217;s much simpler to replace it in the future. It&#8217;s worth emphasizing that&#10;etcd is the only place Kubernetes stores cluster state and metadata.&#10;HOW RESOURCES ARE STORED IN ETCD&#10;As I&#8217;m writing this, Kubernetes can use either etcd version 2 or version 3, but version 3&#10;is now recommended because of improved performance. etcd v2 stores keys in a hier-&#10;archical key space, which makes key-value pairs similar to files in a file system. Each&#10;key in etcd is either a directory, which contains other keys, or is a regular key with a&#10;corresponding value. etcd v3 doesn&#8217;t support directories, but because the key format&#10;remains the same (keys can include slashes), you can still think of them as being&#10;grouped into directories. Kubernetes stores all its data in etcd under /registry. The&#10;following listing shows a list of keys stored under /registry.&#10;$ etcdctl ls /registry&#10;/registry/configmaps&#10;/registry/daemonsets&#10;/registry/deployments&#10;/registry/events&#10;/registry/namespaces&#10;/registry/pods&#10;...&#10;About optimistic concurrency control&#10;Optimistic concurrency control (sometimes referred to as optimistic locking) is a&#10;method where instead of locking a piece of data and preventing it from being read or&#10;updated while the lock is in place, the piece of data includes a version number. Every&#10;time the data is updated, the version number increases. When updating the data, the&#10;version number is checked to see if it has increased between the time the client read&#10;the data and the time it submits the update. If this happens, the update is rejected&#10;and the client must re-read the new data and try to update it again. &#10;The result is that when two clients try to update the same data entry, only the first&#10;one succeeds.&#10;All Kubernetes resources include a metadata.resourceVersion field, which clients&#10;need to pass back to the API server when updating an object. If the version doesn&#8217;t&#10;match the one stored in etcd, the API server rejects the update.&#10;Listing 11.2&#10;Top-level entries stored in etcd by Kubernetes&#10; &#10;"
    color "green"
  ]
  node [
    id 516
    label "346"
    title "Page_346"
    color "blue"
  ]
  node [
    id 517
    label "text_257"
    title "314&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;You&#8217;ll recognize that these keys correspond to the resource types you learned about in&#10;the previous chapters. &#10;NOTE&#10;If you&#8217;re using v3 of the etcd API, you can&#8217;t use the ls command to see&#10;the contents of a directory. Instead, you can list all keys that start with a given&#10;prefix with etcdctl get /registry --prefix=true.&#10;The following listing shows the contents of the /registry/pods directory.&#10;$ etcdctl ls /registry/pods&#10;/registry/pods/default&#10;/registry/pods/kube-system&#10;As you can infer from the names, these two entries correspond to the default and the&#10;kube-system namespaces, which means pods are stored per namespace. The follow-&#10;ing listing shows the entries in the /registry/pods/default directory.&#10;$ etcdctl ls /registry/pods/default&#10;/registry/pods/default/kubia-159041347-xk0vc&#10;/registry/pods/default/kubia-159041347-wt6ga&#10;/registry/pods/default/kubia-159041347-hp2o5&#10;Each entry corresponds to an individual pod. These aren&#8217;t directories, but key-value&#10;entries. The following listing shows what&#8217;s stored in one of them.&#10;$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga&#10;{&#34;kind&#34;:&#34;Pod&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;metadata&#34;:{&#34;name&#34;:&#34;kubia-159041347-wt6ga&#34;,&#10;&#34;generateName&#34;:&#34;kubia-159041347-&#34;,&#34;namespace&#34;:&#34;default&#34;,&#34;selfLink&#34;:...&#10;You&#8217;ll recognize that this is nothing other than a pod definition in JSON format. The&#10;API server stores the complete JSON representation of a resource in etcd. Because of&#10;etcd&#8217;s hierarchical key space, you can think of all the stored resources as JSON files in&#10;a filesystem. Simple, right?&#10;WARNING&#10;Prior to Kubernetes version 1.7, the JSON manifest of a Secret&#10;resource was also stored like this (it wasn&#8217;t encrypted). If someone got direct&#10;access to etcd, they knew all your Secrets. From version 1.7, Secrets are&#10;encrypted and thus stored much more securely.&#10;ENSURING THE CONSISTENCY AND VALIDITY OF STORED OBJECTS&#10;Remember Google&#8217;s Borg and Omega systems mentioned in chapter 1, which are&#10;what Kubernetes is based on? Like Kubernetes, Omega also uses a centralized store to&#10;hold the state of the cluster, but in contrast, multiple Control Plane components&#10;access the store directly. All these components need to make sure they all adhere to&#10;Listing 11.3&#10;Keys in the /registry/pods directory&#10;Listing 11.4&#10;etcd entries for pods in the default namespace&#10;Listing 11.5&#10;An etcd entry representing a pod&#10; &#10;"
    color "green"
  ]
  node [
    id 518
    label "347"
    title "Page_347"
    color "blue"
  ]
  node [
    id 519
    label "text_258"
    title "315&#10;Understanding the architecture&#10;the same optimistic locking mechanism to handle conflicts properly. A single compo-&#10;nent not adhering fully to the mechanism may lead to inconsistent data. &#10; Kubernetes improves this by requiring all other Control Plane components to go&#10;through the API server. This way updates to the cluster state are always consistent, because&#10;the optimistic locking mechanism is implemented in a single place, so less chance exists,&#10;if any, of error. The API server also makes sure that the data written to the store is always&#10;valid and that changes to the data are only performed by authorized clients. &#10;ENSURING CONSISTENCY WHEN ETCD IS CLUSTERED&#10;For ensuring high availability, you&#8217;ll usually run more than a single instance of etcd.&#10;Multiple etcd instances will need to remain consistent. Such a distributed system&#10;needs to reach a consensus on what the actual state is. etcd uses the RAFT consensus&#10;algorithm to achieve this, which ensures that at any given moment, each node&#8217;s state is&#10;either what the majority of the nodes agrees is the current state or is one of the previ-&#10;ously agreed upon states. &#10; Clients connecting to different nodes of an etcd cluster will either see the actual&#10;current state or one of the states from the past (in Kubernetes, the only etcd client is&#10;the API server, but there may be multiple instances). &#10; The consensus algorithm requires a majority (or quorum) for the cluster to progress&#10;to the next state. As a result, if the cluster splits into two disconnected groups of nodes,&#10;the state in the two groups can never diverge, because to transition from the previous&#10;state to the new one, there needs to be more than half of the nodes taking part in&#10;the state change. If one group contains the majority of all nodes, the other one obvi-&#10;ously doesn&#8217;t. The first group can modify the cluster state, whereas the other one can&#8217;t.&#10;When the two groups reconnect, the second group can catch up with the state in the&#10;first group (see figure 11.2).&#10;Clients(s)&#10;Clients(s)&#10;Clients(s)&#10;etcd-0&#10;etcd-1&#10;etcd-2&#10;The nodes know&#10;there are three nodes&#10;in the etcd cluster.&#10;etcd-0&#10;etcd-1&#10;These two nodes know&#10;they still have quorum&#10;and can accept state&#10;changes from clients.&#10;etcd-2&#10;This node knows it does&#10;not have quorum and&#10;should therefore not&#10;allow state changes.&#10;Network&#10;split&#10;Figure 11.2&#10;In a split-brain scenario, only the side which still has the majority (quorum) accepts &#10;state changes.&#10; &#10;"
    color "green"
  ]
  node [
    id 520
    label "348"
    title "Page_348"
    color "blue"
  ]
  node [
    id 521
    label "text_259"
    title "316&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;WHY THE NUMBER OF ETCD INSTANCES SHOULD BE AN ODD NUMBER&#10;etcd is usually deployed with an odd number of instances. I&#8217;m sure you&#8217;d like to know&#10;why. Let&#8217;s compare having two vs. having one instance. Having two instances requires&#10;both instances to be present to have a majority. If either of them fails, the etcd cluster&#10;can&#8217;t transition to a new state because no majority exists. Having two instances is worse&#10;than having only a single instance. By having two, the chance of the whole cluster fail-&#10;ing has increased by 100%, compared to that of a single-node cluster failing. &#10; The same applies when comparing three vs. four etcd instances. With three instances,&#10;one instance can fail and a majority (of two) still exists. With four instances, you need&#10;three nodes for a majority (two aren&#8217;t enough). In both three- and four-instance clus-&#10;ters, only a single instance may fail. But when running four instances, if one fails, a&#10;higher possibility exists of an additional instance of the three remaining instances fail-&#10;ing (compared to a three-node cluster with one failed node and two remaining nodes).&#10; Usually, for large clusters, an etcd cluster of five or seven nodes is sufficient. It can&#10;handle a two- or a three-node failure, respectively, which suffices in almost all situations. &#10;11.1.3 What the API server does&#10;The Kubernetes API server is the central component used by all other components&#10;and by clients, such as kubectl. It provides a CRUD (Create, Read, Update, Delete)&#10;interface for querying and modifying the cluster state over a RESTful API. It stores&#10;that state in etcd.&#10; In addition to providing a consistent way of storing objects in etcd, it also performs&#10;validation of those objects, so clients can&#8217;t store improperly configured objects (which&#10;they could if they were writing to the store directly). Along with validation, it also han-&#10;dles optimistic locking, so changes to an object are never overridden by other clients&#10;in the event of concurrent updates.&#10; One of the API server&#8217;s clients is the command-line tool kubectl you&#8217;ve been&#10;using from the beginning of the book. When creating a resource from a JSON file, for&#10;example, kubectl posts the file&#8217;s contents to the API server through an HTTP POST&#10;request. Figure 11.3 shows what happens inside the API server when it receives the&#10;request. This is explained in more detail in the next few paragraphs.&#10;API server&#10;etcd&#10;Authentication&#10;plugin 1&#10;Authentication&#10;plugin 2&#10;Authentication&#10;plugin 3&#10;Client&#10;(&#10;)&#10;kubectl&#10;HTTP POST&#10;request&#10;Authorization&#10;plugin 1&#10;Authorization&#10;plugin 2&#10;Authorization&#10;plugin 3&#10;Admission&#10;control plugin 1&#10;Admission&#10;control plugin 2&#10;Admission&#10;control plugin 3&#10;Resource&#10;validation&#10;Figure 11.3&#10;The operation of the API server&#10; &#10;"
    color "green"
  ]
  node [
    id 522
    label "349"
    title "Page_349"
    color "blue"
  ]
  node [
    id 523
    label "text_260"
    title "317&#10;Understanding the architecture&#10;AUTHENTICATING THE CLIENT WITH AUTHENTICATION PLUGINS&#10;First, the API server needs to authenticate the client sending the request. This is per-&#10;formed by one or more authentication plugins configured in the API server. The API&#10;server calls these plugins in turn, until one of them determines who is sending the&#10;request. It does this by inspecting the HTTP request. &#10; Depending on the authentication method, the user can be extracted from the cli-&#10;ent&#8217;s certificate or an HTTP header, such as Authorization, which you used in chap-&#10;ter 8. The plugin extracts the client&#8217;s username, user ID, and groups the user belongs&#10;to. This data is then used in the next stage, which is authorization.&#10;AUTHORIZING THE CLIENT WITH AUTHORIZATION PLUGINS&#10;Besides authentication plugins, the API server is also configured to use one or more&#10;authorization plugins. Their job is to determine whether the authenticated user can&#10;perform the requested action on the requested resource. For example, when creating&#10;pods, the API server consults all authorization plugins in turn, to determine whether&#10;the user can create pods in the requested namespace. As soon as a plugin says the user&#10;can perform the action, the API server progresses to the next stage.&#10;VALIDATING AND/OR MODIFYING THE RESOURCE IN THE REQUEST WITH ADMISSION CONTROL PLUGINS&#10;If the request is trying to create, modify, or delete a resource, the request is sent&#10;through Admission Control. Again, the server is configured with multiple Admission&#10;Control plugins. These plugins can modify the resource for different reasons. They&#10;may initialize fields missing from the resource specification to the configured default&#10;values or even override them. They may even modify other related resources, which&#10;aren&#8217;t in the request, and can also reject a request for whatever reason. The resource&#10;passes through all Admission Control plugins.&#10;NOTE&#10;When the request is only trying to read data, the request doesn&#8217;t go&#10;through the Admission Control.&#10;Examples of Admission Control plugins include&#10;&#61601;&#10;AlwaysPullImages&#8212;Overrides the pod&#8217;s imagePullPolicy to Always, forcing&#10;the image to be pulled every time the pod is deployed.&#10;&#61601;&#10;ServiceAccount&#8212;Applies the default service account to pods that don&#8217;t specify&#10;it explicitly.&#10;&#61601;&#10;NamespaceLifecycle&#8212;Prevents creation of pods in namespaces that are in the&#10;process of being deleted, as well as in non-existing namespaces.&#10;&#61601;&#10;ResourceQuota&#8212;Ensures pods in a certain namespace only use as much CPU&#10;and memory as has been allotted to the namespace. We&#8217;ll learn more about this&#10;in chapter 14.&#10;You&#8217;ll find a list of additional Admission Control plugins in the Kubernetes documen-&#10;tation at https:/&#10;/kubernetes.io/docs/admin/admission-controllers/.&#10; &#10;"
    color "green"
  ]
  node [
    id 524
    label "350"
    title "Page_350"
    color "blue"
  ]
  node [
    id 525
    label "text_261"
    title "318&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;VALIDATING THE RESOURCE AND STORING IT PERSISTENTLY&#10;After letting the request pass through all the Admission Control plugins, the API server&#10;then validates the object, stores it in etcd, and returns a response to the client.&#10;11.1.4 Understanding how the API server notifies clients of resource &#10;changes&#10;The API server doesn&#8217;t do anything else except what we&#8217;ve discussed. For example, it&#10;doesn&#8217;t create pods when you create a ReplicaSet resource and it doesn&#8217;t manage the&#10;endpoints of a service. That&#8217;s what controllers in the Controller Manager do. &#10; But the API server doesn&#8217;t even tell these controllers what to do. All it does is&#10;enable those controllers and other components to observe changes to deployed&#10;resources. A Control Plane component can request to be notified when a resource is&#10;created, modified, or deleted. This enables the component to perform whatever task&#10;it needs in response to a change of the cluster metadata.&#10; Clients watch for changes by opening an HTTP connection to the API server.&#10;Through this connection, the client will then receive a stream of modifications to the&#10;watched objects. Every time an object is updated, the server sends the new version of&#10;the object to all connected clients watching the object. Figure 11.4 shows how clients&#10;can watch for changes to pods and how a change to one of the pods is stored into etcd&#10;and then relayed to all clients watching pods at that moment.&#10;One of the API server&#8217;s clients is the kubectl tool, which also supports watching&#10;resources. For example, when deploying a pod, you don&#8217;t need to constantly poll the list&#10;of pods by repeatedly executing kubectl get pods. Instead, you can use the --watch&#10;flag and be notified of each creation, modification, or deletion of a pod, as shown in&#10;the following listing.&#10;$ kubectl get pods --watch&#10;NAME                    READY     STATUS              RESTARTS   AGE&#10;Listing 11.6&#10;Watching a pod being created and then deleted&#10;Various&#10;clients&#10;kubectl&#10;API server&#10;1. GET /.../pods?watch=true&#10;2. POST /.../pods/pod-xyz&#10;5. Send updated object&#10;to all watchers&#10;3. Update object&#10;in etcd&#10;4. Modi&#64257;cation&#10;noti&#64257;cation&#10;etcd&#10;Figure 11.4&#10;When an object is updated, the API server sends the updated object to all interested &#10;watchers.&#10; &#10;"
    color "green"
  ]
  node [
    id 526
    label "351"
    title "Page_351"
    color "blue"
  ]
  node [
    id 527
    label "text_262"
    title "319&#10;Understanding the architecture&#10;kubia-159041347-14j3i   0/1       Pending             0          0s&#10;kubia-159041347-14j3i   0/1       Pending             0          0s&#10;kubia-159041347-14j3i   0/1       ContainerCreating   0          1s&#10;kubia-159041347-14j3i   0/1       Running             0          3s&#10;kubia-159041347-14j3i   1/1       Running             0          5s&#10;kubia-159041347-14j3i   1/1       Terminating         0          9s&#10;kubia-159041347-14j3i   0/1       Terminating         0          17s&#10;kubia-159041347-14j3i   0/1       Terminating         0          17s&#10;kubia-159041347-14j3i   0/1       Terminating         0          17s&#10;You can even have kubectl print out the whole YAML on each watch event like this:&#10;$ kubectl get pods -o yaml --watch&#10;The watch mechanism is also used by the Scheduler, which is the next Control Plane&#10;component you&#8217;re going to learn more about.&#10;11.1.5 Understanding the Scheduler&#10;You&#8217;ve already learned that you don&#8217;t usually specify which cluster node a pod should&#10;run on. This is left to the Scheduler. From afar, the operation of the Scheduler looks&#10;simple. All it does is wait for newly created pods through the API server&#8217;s watch mech-&#10;anism and assign a node to each new pod that doesn&#8217;t already have the node set. &#10; The Scheduler doesn&#8217;t instruct the selected node (or the Kubelet running on that&#10;node) to run the pod. All the Scheduler does is update the pod definition through the&#10;API server. The API server then notifies the Kubelet (again, through the watch mech-&#10;anism described previously) that the pod has been scheduled. As soon as the Kubelet&#10;on the target node sees the pod has been scheduled to its node, it creates and runs the&#10;pod&#8217;s containers.&#10; Although a coarse-grained view of the scheduling process seems trivial, the actual&#10;task of selecting the best node for the pod isn&#8217;t that simple. Sure, the simplest&#10;Scheduler could pick a random node and not care about the pods already running on&#10;that node. On the other side of the spectrum, the Scheduler could use advanced tech-&#10;niques such as machine learning to anticipate what kind of pods are about to be&#10;scheduled in the next minutes or hours and schedule pods to maximize future hard-&#10;ware utilization without requiring any rescheduling of existing pods. Kubernetes&#8217;&#10;default Scheduler falls somewhere in between. &#10;UNDERSTANDING THE DEFAULT SCHEDULING ALGORITHM&#10;The selection of a node can be broken down into two parts, as shown in figure 11.5:&#10;&#61601;Filtering the list of all nodes to obtain a list of acceptable nodes the pod can be&#10;scheduled to.&#10;&#61601;Prioritizing the acceptable nodes and choosing the best one. If multiple nodes&#10;have the highest score, round-robin is used to ensure pods are deployed across&#10;all of them evenly.&#10; &#10;"
    color "green"
  ]
  node [
    id 528
    label "352"
    title "Page_352"
    color "blue"
  ]
  node [
    id 529
    label "text_263"
    title "320&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;FINDING ACCEPTABLE NODES&#10;To determine which nodes are acceptable for the pod, the Scheduler passes each&#10;node through a list of configured predicate functions. These check various things&#10;such as&#10;&#61601;Can the node fulfill the pod&#8217;s requests for hardware resources? You&#8217;ll learn how&#10;to specify them in chapter 14.&#10;&#61601;Is the node running out of resources (is it reporting a memory or a disk pres-&#10;sure condition)? &#10;&#61601;If the pod requests to be scheduled to a specific node (by name), is this the node?&#10;&#61601;Does the node have a label that matches the node selector in the pod specifica-&#10;tion (if one is defined)?&#10;&#61601;If the pod requests to be bound to a specific host port (discussed in chapter 13),&#10;is that port already taken on this node or not? &#10;&#61601;If the pod requests a certain type of volume, can this volume be mounted for&#10;this pod on this node, or is another pod on the node already using the same&#10;volume?&#10;&#61601;Does the pod tolerate the taints of the node? Taints and tolerations are explained&#10;in chapter 16.&#10;&#61601;Does the pod specify node and/or pod affinity or anti-affinity rules? If yes,&#10;would scheduling the pod to this node break those rules? This is also explained&#10;in chapter 16.&#10;All these checks must pass for the node to be eligible to host the pod. After perform-&#10;ing these checks on every node, the Scheduler ends up with a subset of the nodes. Any&#10;of these nodes could run the pod, because they have enough available resources for&#10;the pod and conform to all requirements you&#8217;ve specified in the pod definition.&#10;SELECTING THE BEST NODE FOR THE POD&#10;Even though all these nodes are acceptable and can run the pod, several may be a&#10;better choice than others. Suppose you have a two-node cluster. Both nodes are eli-&#10;gible, but one is already running 10 pods, while the other, for whatever reason, isn&#8217;t&#10;running any pods right now. It&#8217;s obvious the Scheduler should favor the second&#10;node in this case. &#10;Node 1&#10;Node 2&#10;Node 3&#10;Node 4&#10;Node 5&#10;...&#10;Find acceptable&#10;nodes&#10;Node 1&#10;Node 2&#10;Node 3&#10;Node 4&#10;Node 5&#10;...&#10;Prioritize nodes&#10;and select the&#10;top one&#10;Node 3&#10;Node 1&#10;Node 4&#10;Figure 11.5&#10;The Scheduler finds acceptable nodes for a pod and then selects the best node &#10;for the pod.&#10; &#10;"
    color "green"
  ]
  node [
    id 530
    label "353"
    title "Page_353"
    color "blue"
  ]
  node [
    id 531
    label "text_264"
    title "321&#10;Understanding the architecture&#10; Or is it? If these two nodes are provided by the cloud infrastructure, it may be bet-&#10;ter to schedule the pod to the first node and relinquish the second node back to the&#10;cloud provider to save money. &#10;ADVANCED SCHEDULING OF PODS&#10;Consider another example. Imagine having multiple replicas of a pod. Ideally, you&#8217;d&#10;want them spread across as many nodes as possible instead of having them all sched-&#10;uled to a single one. Failure of that node would cause the service backed by those&#10;pods to become unavailable. But if the pods were spread across different nodes, a sin-&#10;gle node failure would barely leave a dent in the service&#8217;s capacity. &#10; Pods belonging to the same Service or ReplicaSet are spread across multiple nodes&#10;by default. It&#8217;s not guaranteed that this is always the case. But you can force pods to be&#10;spread around the cluster or kept close together by defining pod affinity and anti-&#10;affinity rules, which are explained in chapter 16. &#10; Even these two simple cases show how complex scheduling can be, because it&#10;depends on a multitude of factors. Because of this, the Scheduler can either be config-&#10;ured to suit your specific needs or infrastructure specifics, or it can even be replaced&#10;with a custom implementation altogether. You could also run a Kubernetes cluster&#10;without a Scheduler, but then you&#8217;d have to perform the scheduling manually.&#10;USING MULTIPLE SCHEDULERS&#10;Instead of running a single Scheduler in the cluster, you can run multiple Schedulers.&#10;Then, for each pod, you specify the Scheduler that should schedule this particular&#10;pod by setting the schedulerName property in the pod spec.&#10; Pods without this property set are scheduled using the default Scheduler, and so&#10;are pods with schedulerName set to default-scheduler. All other pods are ignored by&#10;the default Scheduler, so they need to be scheduled either manually or by another&#10;Scheduler watching for such pods. &#10; You can implement your own Schedulers and deploy them in the cluster, or you&#10;can deploy an additional instance of Kubernetes&#8217; Scheduler with different configura-&#10;tion options.&#10;11.1.6 Introducing the controllers running in the Controller Manager&#10;As previously mentioned, the API server doesn&#8217;t do anything except store resources in&#10;etcd and notify clients about the change. The Scheduler only assigns a node to the&#10;pod, so you need other active components to make sure the actual state of the system&#10;converges toward the desired state, as specified in the resources deployed through the&#10;API server. This work is done by controllers running inside the Controller Manager. &#10; The single Controller Manager process currently combines a multitude of control-&#10;lers performing various reconciliation tasks. Eventually those controllers will be split&#10;up into separate processes, enabling you to replace each one with a custom imple-&#10;mentation if necessary. The list of these controllers includes the&#10;&#61601;Replication Manager (a controller for ReplicationController resources)&#10;&#61601;ReplicaSet, DaemonSet, and Job controllers&#10; &#10;"
    color "green"
  ]
  node [
    id 532
    label "354"
    title "Page_354"
    color "blue"
  ]
  node [
    id 533
    label "text_265"
    title "322&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;&#61601;Deployment controller&#10;&#61601;StatefulSet controller&#10;&#61601;Node controller&#10;&#61601;Service controller&#10;&#61601;Endpoints controller&#10;&#61601;Namespace controller&#10;&#61601;PersistentVolume controller&#10;&#61601;Others&#10;What each of these controllers does should be evident from its name. From the list,&#10;you can tell there&#8217;s a controller for almost every resource you can create. Resources&#10;are descriptions of what should be running in the cluster, whereas the controllers are&#10;the active Kubernetes components that perform actual work as a result of the deployed&#10;resources.&#10;UNDERSTANDING WHAT CONTROLLERS DO AND HOW THEY DO IT&#10;Controllers do many different things, but they all watch the API server for changes to&#10;resources (Deployments, Services, and so on) and perform operations for each change,&#10;whether it&#8217;s a creation of a new object or an update or deletion of an existing object.&#10;Most of the time, these operations include creating other resources or updating the&#10;watched resources themselves (to update the object&#8217;s status, for example).&#10; In general, controllers run a reconciliation loop, which reconciles the actual state&#10;with the desired state (specified in the resource&#8217;s spec section) and writes the new&#10;actual state to the resource&#8217;s status section. Controllers use the watch mechanism to&#10;be notified of changes, but because using watches doesn&#8217;t guarantee the controller&#10;won&#8217;t miss an event, they also perform a re-list operation periodically to make sure&#10;they haven&#8217;t missed anything.&#10; Controllers never talk to each other directly. They don&#8217;t even know any other con-&#10;trollers exist. Each controller connects to the API server and, through the watch&#10;mechanism described in section 11.1.3, asks to be notified when a change occurs in&#10;the list of resources of any type the controller is responsible for. &#10; We&#8217;ll briefly look at what each of the controllers does, but if you&#8217;d like an in-depth&#10;view of what they do, I suggest you look at their source code directly. The sidebar&#10;explains how to get started.&#10;A few pointers on exploring the controllers&#8217; source code&#10;If you&#8217;re interested in seeing exactly how these controllers operate, I strongly encour-&#10;age you to browse through their source code. To make it easier, here are a few tips:&#10;The source code for the controllers is available at https:/&#10;/github.com/kubernetes/&#10;kubernetes/blob/master/pkg/controller.&#10;Each controller usually has a constructor in which it creates an Informer, which is&#10;basically a listener that gets called every time an API object gets updated. Usually,&#10; &#10;"
    color "green"
  ]
  node [
    id 534
    label "355"
    title "Page_355"
    color "blue"
  ]
  node [
    id 535
    label "text_266"
    title "323&#10;Understanding the architecture&#10;THE REPLICATION MANAGER&#10;The controller that makes ReplicationController resources come to life is called the&#10;Replication Manager. We talked about how ReplicationControllers work in chapter 4.&#10;It&#8217;s not the ReplicationControllers that do the actual work, but the Replication Man-&#10;ager. Let&#8217;s quickly review what the controller does, because this will help you under-&#10;stand the rest of the controllers.&#10; In chapter 4, we said that the operation of a ReplicationController could be&#10;thought of as an infinite loop, where in each iteration, the controller finds the num-&#10;ber of pods matching its pod selector and compares the number to the desired replica&#10;count. &#10; Now that you know how the API server can notify clients through the watch&#10;mechanism, it&#8217;s clear that the controller doesn&#8217;t poll the pods in every iteration, but&#10;is instead notified by the watch mechanism of each change that may affect the&#10;desired replica count or the number of matched pods (see figure 11.6). Any such&#10;changes trigger the controller to recheck the desired vs. actual replica count and act&#10;accordingly.&#10; You already know that when too few pod instances are running, the Replication-&#10;Controller runs additional instances. But it doesn&#8217;t actually run them itself. It creates&#10;an Informer listens for changes to a specific type of resource. Looking at the con-&#10;structor will show you which resources the controller is watching.&#10;Next, go look for the worker() method. In it, you&#8217;ll find the method that gets invoked&#10;each time the controller needs to do something. The actual function is often stored&#10;in a field called syncHandler or something similar. This field is also initialized in the&#10;constructor, so that&#8217;s where you&#8217;ll find the name of the function that gets called. That&#10;function is the place where all the magic happens.&#10;Controller Manager&#10;Watches&#10;Creates and&#10;deletes&#10;Replication&#10;Manager&#10;API server&#10;ReplicationController&#10;resources&#10;Pod resources&#10;Other resources&#10;Figure 11.6&#10;The Replication Manager watches for changes to API &#10;objects.&#10; &#10;"
    color "green"
  ]
  node [
    id 536
    label "356"
    title "Page_356"
    color "blue"
  ]
  node [
    id 537
    label "text_267"
    title "324&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;new Pod manifests, posts them to the API server, and lets the Scheduler and the&#10;Kubelet do their job of scheduling and running the pod.&#10; The Replication Manager performs its work by manipulating Pod API objects&#10;through the API server. This is how all controllers operate.&#10;THE REPLICASET, THE DAEMONSET, AND THE JOB CONTROLLERS&#10;The ReplicaSet controller does almost the same thing as the Replication Manager&#10;described previously, so we don&#8217;t have much to add here. The DaemonSet and Job&#10;controllers are similar. They create Pod resources from the pod template defined in&#10;their respective resources. Like the Replication Manager, these controllers don&#8217;t run&#10;the pods, but post Pod definitions to the API server, letting the Kubelet create their&#10;containers and run them.&#10;THE DEPLOYMENT CONTROLLER&#10;The Deployment controller takes care of keeping the actual state of a deployment in&#10;sync with the desired state specified in the corresponding Deployment API object. &#10; The Deployment controller performs a rollout of a new version each time a&#10;Deployment object is modified (if the modification should affect the deployed pods).&#10;It does this by creating a ReplicaSet and then appropriately scaling both the old and&#10;the new ReplicaSet based on the strategy specified in the Deployment, until all the old&#10;pods have been replaced with new ones. It doesn&#8217;t create any pods directly.&#10;THE STATEFULSET CONTROLLER&#10;The StatefulSet controller, similarly to the ReplicaSet controller and other related&#10;controllers, creates, manages, and deletes Pods according to the spec of a StatefulSet&#10;resource. But while those other controllers only manage Pods, the StatefulSet control-&#10;ler also instantiates and manages PersistentVolumeClaims for each Pod instance.&#10;THE NODE CONTROLLER&#10;The Node controller manages the Node resources, which describe the cluster&#8217;s worker&#10;nodes. Among other things, a Node controller keeps the list of Node objects in sync&#10;with the actual list of machines running in the cluster. It also monitors each node&#8217;s&#10;health and evicts pods from unreachable nodes.&#10; The Node controller isn&#8217;t the only component making changes to Node objects.&#10;They&#8217;re also changed by the Kubelet, and can obviously also be modified by users&#10;through REST API calls. &#10;THE SERVICE CONTROLLER&#10;In chapter 5, when we talked about Services, you learned that a few different types&#10;exist. One of them was the LoadBalancer service, which requests a load balancer from&#10;the infrastructure to make the service available externally. The Service controller is&#10;the one requesting and releasing a load balancer from the infrastructure, when a&#10;LoadBalancer-type Service is created or deleted.&#10; &#10;"
    color "green"
  ]
  node [
    id 538
    label "357"
    title "Page_357"
    color "blue"
  ]
  node [
    id 539
    label "text_268"
    title "325&#10;Understanding the architecture&#10;THE ENDPOINTS CONTROLLER&#10;You&#8217;ll remember that Services aren&#8217;t linked directly to pods, but instead contain a list&#10;of endpoints (IPs and ports), which is created and updated either manually or auto-&#10;matically according to the pod selector defined on the Service. The Endpoints con-&#10;troller is the active component that keeps the endpoint list constantly updated with&#10;the IPs and ports of pods matching the label selector.&#10; As figure 11.7 shows, the controller watches both Services and Pods. When&#10;Services are added or updated or Pods are added, updated, or deleted, it selects Pods&#10;matching the Service&#8217;s pod selector and adds their IPs and ports to the Endpoints&#10;resource. Remember, the Endpoints object is a standalone object, so the controller&#10;creates it if necessary. Likewise, it also deletes the Endpoints object when the Service is&#10;deleted.&#10;THE NAMESPACE CONTROLLER&#10;Remember namespaces (we talked about them in chapter 3)? Most resources belong&#10;to a specific namespace. When a Namespace resource is deleted, all the resources in&#10;that namespace must also be deleted. This is what the Namespace controller does.&#10;When it&#8217;s notified of the deletion of a Namespace object, it deletes all the resources&#10;belonging to the namespace through the API server. &#10;THE PERSISTENTVOLUME CONTROLLER&#10;In chapter 6 you learned about PersistentVolumes and PersistentVolumeClaims.&#10;Once a user creates a PersistentVolumeClaim, Kubernetes must find an appropriate&#10;PersistentVolume and bind it to the claim. This is performed by the PersistentVolume&#10;controller. &#10; When a PersistentVolumeClaim pops up, the controller finds the best match for&#10;the claim by selecting the smallest PersistentVolume with the access mode matching&#10;the one requested in the claim and the declared capacity above the capacity requested&#10;Controller Manager&#10;Watches&#10;Creates, modi&#64257;es,&#10;and deletes&#10;Endpoints&#10;controller&#10;API server&#10;Service resources&#10;Pod resources&#10;Endpoints resources&#10;Figure 11.7&#10;The Endpoints controller watches Service and Pod resources, &#10;and manages Endpoints.&#10; &#10;"
    color "green"
  ]
  node [
    id 540
    label "358"
    title "Page_358"
    color "blue"
  ]
  node [
    id 541
    label "text_269"
    title "326&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;in the claim. It does this by keeping an ordered list of PersistentVolumes for each&#10;access mode by ascending capacity and returning the first volume from the list.&#10; Then, when the user deletes the PersistentVolumeClaim, the volume is unbound&#10;and reclaimed according to the volume&#8217;s reclaim policy (left as is, deleted, or emptied).&#10;CONTROLLER WRAP-UP&#10;You should now have a good feel for what each controller does and how controllers&#10;work in general. Again, all these controllers operate on the API objects through the&#10;API server. They don&#8217;t communicate with the Kubelets directly or issue any kind of&#10;instructions to them. In fact, they don&#8217;t even know Kubelets exist. After a controller&#10;updates a resource in the API server, the Kubelets and Kubernetes Service Proxies,&#10;also oblivious of the controllers&#8217; existence, perform their work, such as spinning up a&#10;pod&#8217;s containers and attaching network storage to them, or in the case of services, set-&#10;ting up the actual load balancing across pods. &#10; The Control Plane handles one part of the operation of the whole system, so to&#10;fully understand how things unfold in a Kubernetes cluster, you also need to under-&#10;stand what the Kubelet and the Kubernetes Service Proxy do. We&#8217;ll learn that next.&#10;11.1.7 What the Kubelet does&#10;In contrast to all the controllers, which are part of the Kubernetes Control Plane and&#10;run on the master node(s), the Kubelet and the Service Proxy both run on the worker&#10;nodes, where the actual pods containers run. What does the Kubelet do exactly?&#10;UNDERSTANDING THE KUBELET&#8217;S JOB&#10;In a nutshell, the Kubelet is the component responsible for everything running on a&#10;worker node. Its initial job is to register the node it&#8217;s running on by creating a Node&#10;resource in the API server. Then it needs to continuously monitor the API server for&#10;Pods that have been scheduled to the node, and start the pod&#8217;s containers. It does this&#10;by telling the configured container runtime (which is Docker, CoreOS&#8217; rkt, or some-&#10;thing else) to run a container from a specific container image. The Kubelet then con-&#10;stantly monitors running containers and reports their status, events, and resource&#10;consumption to the API server. &#10; The Kubelet is also the component that runs the container liveness probes, restart-&#10;ing containers when the probes fail. Lastly, it terminates containers when their Pod is&#10;deleted from the API server and notifies the server that the pod has terminated.&#10;RUNNING STATIC PODS WITHOUT THE API SERVER&#10;Although the Kubelet talks to the Kubernetes API server and gets the pod manifests&#10;from there, it can also run pods based on pod manifest files in a specific local direc-&#10;tory as shown in figure 11.8. This feature is used to run the containerized versions of&#10;the Control Plane components as pods, as you saw in the beginning of the chapter.&#10; Instead of running Kubernetes system components natively, you can put their pod&#10;manifests into the Kubelet&#8217;s manifest directory and have the Kubelet run and manage&#10; &#10;"
    color "green"
  ]
  node [
    id 542
    label "359"
    title "Page_359"
    color "blue"
  ]
  node [
    id 543
    label "text_270"
    title "327&#10;Understanding the architecture&#10;them. You can also use the same method to run your custom system containers, but&#10;doing it through a DaemonSet is the recommended method.&#10;11.1.8 The role of the Kubernetes Service Proxy&#10;Beside the Kubelet, every worker node also runs the kube-proxy, whose purpose is to&#10;make sure clients can connect to the services you define through the Kubernetes API.&#10;The kube-proxy makes sure connections to the service IP and port end up at one of&#10;the pods backing that service (or other, non-pod service endpoints). When a service is&#10;backed by more than one pod, the proxy performs load balancing across those pods. &#10;WHY IT&#8217;S CALLED A PROXY&#10;The initial implementation of the kube-proxy was the userspace proxy. It used an&#10;actual server process to accept connections and proxy them to the pods. To inter-&#10;cept connections destined to the service IPs, the proxy configured iptables rules&#10;(iptables is the tool for managing the Linux kernel&#8217;s packet filtering features) to&#10;redirect the connections to the proxy server. A rough diagram of the userspace proxy&#10;mode is shown in figure 11.9.&#10;Container Runtime&#10;(Docker, rkt, ...)&#10;Kubelet&#10;API server&#10;Worker node&#10;Runs, monitors,&#10;and manages&#10;containers&#10;Pod resource&#10;Container A&#10;Container B&#10;Container A&#10;Container B&#10;Container C&#10;Pod manifest (&#64257;le)&#10;Local manifest directory&#10;Container C&#10;Figure 11.8&#10;The Kubelet runs pods based on pod specs from the API server and a local file directory.&#10;Client&#10;kube-proxy&#10;Con&#64257;gures&#10;:&#10;iptables&#10;redirect through proxy server&#10;iptables&#10;Pod&#10;Figure 11.9&#10;The userspace proxy mode&#10; &#10;"
    color "green"
  ]
  node [
    id 544
    label "360"
    title "Page_360"
    color "blue"
  ]
  node [
    id 545
    label "text_271"
    title "328&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;The kube-proxy got its name because it was an actual proxy, but the current, much&#10;better performing implementation only uses iptables rules to redirect packets to a&#10;randomly selected backend pod without passing them through an actual proxy server.&#10;This mode is called the iptables proxy mode and is shown in figure 11.10.&#10;The major difference between these two modes is whether packets pass through the&#10;kube-proxy and must be handled in user space, or whether they&#8217;re handled only by&#10;the Kernel (in kernel space). This has a major impact on performance. &#10; Another smaller difference is that the userspace proxy mode balanced connec-&#10;tions across pods in a true round-robin fashion, while the iptables proxy mode&#10;doesn&#8217;t&#8212;it selects pods randomly. When only a few clients use a service, they may not&#10;be spread evenly across pods. For example, if a service has two backing pods but only&#10;five or so clients, don&#8217;t be surprised if you see four clients connect to pod A and only&#10;one client connect to pod B. With a higher number of clients or pods, this problem&#10;isn&#8217;t so apparent.&#10; You&#8217;ll learn exactly how iptables proxy mode works in section 11.5. &#10;11.1.9 Introducing Kubernetes add-ons&#10;We&#8217;ve now discussed the core components that make a Kubernetes cluster work. But&#10;in the beginning of the chapter, we also listed a few add-ons, which although not&#10;always required, enable features such as DNS lookup of Kubernetes services, exposing&#10;multiple HTTP services through a single external IP address, the Kubernetes web&#10;dashboard, and so on.&#10;HOW ADD-ONS ARE DEPLOYED&#10;These components are available as add-ons and are deployed as pods by submitting&#10;YAML manifests to the API server, the way you&#8217;ve been doing throughout the book.&#10;Some of these components are deployed through a Deployment resource or a Repli-&#10;cationController resource, and some through a DaemonSet. &#10; For example, as I&#8217;m writing this, in Minikube, the Ingress controller and the&#10;dashboard add-ons are deployed as ReplicationControllers, as shown in the follow-&#10;ing listing.&#10; &#10;Client&#10;Con&#64257;gures&#10;:&#10;iptables&#10;redirect straight to pod&#10;(no proxy server in-between)&#10;iptables&#10;Pod&#10;kube-proxy&#10;Figure 11.10&#10;The iptables proxy mode&#10; &#10;"
    color "green"
  ]
  node [
    id 546
    label "361"
    title "Page_361"
    color "blue"
  ]
  node [
    id 547
    label "text_272"
    title "329&#10;Understanding the architecture&#10;$ kubectl get rc -n kube-system&#10;NAME                       DESIRED   CURRENT   READY     AGE&#10;default-http-backend       1         1         1         6d&#10;kubernetes-dashboard       1         1         1         6d&#10;nginx-ingress-controller   1         1         1         6d&#10;The DNS add-on is deployed as a Deployment, as shown in the following listing.&#10;$ kubectl get deploy -n kube-system&#10;NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE&#10;kube-dns   1         1         1            1           6d&#10;Let&#8217;s see how DNS and the Ingress controllers work.&#10;HOW THE DNS SERVER WORKS&#10;All the pods in the cluster are configured to use the cluster&#8217;s internal DNS server by&#10;default. This allows pods to easily look up services by name or even the pod&#8217;s IP&#10;addresses in the case of headless services.&#10; The DNS server pod is exposed through the kube-dns service, allowing the pod to&#10;be moved around the cluster, like any other pod. The service&#8217;s IP address is specified&#10;as the nameserver in the /etc/resolv.conf file inside every container deployed in the&#10;cluster. The kube-dns pod uses the API server&#8217;s watch mechanism to observe changes&#10;to Services and Endpoints and updates its DNS records with every change, allowing its&#10;clients to always get (fairly) up-to-date DNS information. I say fairly because during&#10;the time between the update of the Service or Endpoints resource and the time the&#10;DNS pod receives the watch notification, the DNS records may be invalid.&#10;HOW (MOST) INGRESS CONTROLLERS WORK&#10;Unlike the DNS add-on, you&#8217;ll find a few different implementations of Ingress con-&#10;trollers, but most of them work in the same way. An Ingress controller runs a reverse&#10;proxy server (like Nginx, for example), and keeps it configured according to the&#10;Ingress, Service, and Endpoints resources defined in the cluster. The controller thus&#10;needs to observe those resources (again, through the watch mechanism) and change&#10;the proxy server&#8217;s config every time one of them changes. &#10; Although the Ingress resource&#8217;s definition points to a Service, Ingress controllers&#10;forward traffic to the service&#8217;s pod directly instead of going through the service IP.&#10;This affects the preservation of client IPs when external clients connect through the&#10;Ingress controller, which makes them preferred over Services in certain use cases.&#10;USING OTHER ADD-ONS&#10;You&#8217;ve seen how both the DNS server and the Ingress controller add-ons are similar to&#10;the controllers running in the Controller Manager, except that they also accept client&#10;connections instead of only observing and modifying resources through the API server. &#10;Listing 11.7&#10;Add-ons deployed with ReplicationControllers in Minikube&#10;Listing 11.8&#10;The kube-dns Deployment &#10; &#10;"
    color "green"
  ]
  node [
    id 548
    label "362"
    title "Page_362"
    color "blue"
  ]
  node [
    id 549
    label "text_273"
    title "330&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10; Other add-ons are similar. They all need to observe the cluster state and perform&#10;the necessary actions when that changes. We&#8217;ll introduce a few other add-ons in this&#10;and the remaining chapters.&#10;11.1.10Bringing it all together&#10;You&#8217;ve now learned that the whole Kubernetes system is composed of relatively small,&#10;loosely coupled components with good separation of concerns. The API server, the&#10;Scheduler, the individual controllers running inside the Controller Manager, the&#10;Kubelet, and the kube-proxy all work together to keep the actual state of the system&#10;synchronized with what you specify as the desired state. &#10; For example, submitting a pod manifest to the API server triggers a coordinated&#10;dance of various Kubernetes components, which eventually results in the pod&#8217;s con-&#10;tainers running. You&#8217;ll learn how this dance unfolds in the next section. &#10;11.2&#10;How controllers cooperate&#10;You now know about all the components that a Kubernetes cluster is comprised of.&#10;Now, to solidify your understanding of how Kubernetes works, let&#8217;s go over what hap-&#10;pens when a Pod resource is created. Because you normally don&#8217;t create Pods directly,&#10;you&#8217;re going to create a Deployment resource instead and see everything that must&#10;happen for the pod&#8217;s containers to be started.&#10;11.2.1 Understanding which components are involved&#10;Even before you start the whole process, the controllers, the Scheduler, and the&#10;Kubelet are watching the API server for changes to their respective resource types.&#10;This is shown in figure 11.11. The components depicted in the figure will each play a&#10;part in the process you&#8217;re about to trigger. The diagram doesn&#8217;t include etcd, because&#10;it&#8217;s hidden behind the API server, and you can think of the API server as the place&#10;where objects are stored.&#10;Master node&#10;Controller Manager&#10;Watches&#10;Deployment&#10;controller&#10;Scheduler&#10;ReplicaSet&#10;controller&#10;API server&#10;Deployments&#10;Pods&#10;ReplicaSets&#10;Watches&#10;Watches&#10;Node X&#10;Watches&#10;Docker&#10;Kubelet&#10;Figure 11.11&#10;Kubernetes components watching API objects through the API server&#10; &#10;"
    color "green"
  ]
  node [
    id 550
    label "363"
    title "Page_363"
    color "blue"
  ]
  node [
    id 551
    label "text_274"
    title "331&#10;How controllers cooperate&#10;11.2.2 The chain of events&#10;Imagine you prepared the YAML file containing the Deployment manifest and you&#8217;re&#10;about to submit it to Kubernetes through kubectl. kubectl sends the manifest to the&#10;Kubernetes API server in an HTTP POST request. The API server validates the Deploy-&#10;ment specification, stores it in etcd, and returns a response to kubectl. Now a chain&#10;of events starts to unfold, as shown in figure 11.12.&#10;THE DEPLOYMENT CONTROLLER CREATES THE REPLICASET&#10;All API server clients watching the list of Deployments through the API server&#8217;s watch&#10;mechanism are notified of the newly created Deployment resource immediately after&#10;it&#8217;s created. One of those clients is the Deployment controller, which, as we discussed&#10;earlier, is the active component responsible for handling Deployments. &#10; As you may remember from chapter 9, a Deployment is backed by one or more&#10;ReplicaSets, which then create the actual pods. As a new Deployment object is&#10;detected by the Deployment controller, it creates a ReplicaSet for the current speci-&#10;fication of the Deployment. This involves creating a new ReplicaSet resource&#10;through the Kubernetes API. The Deployment controller doesn&#8217;t deal with individ-&#10;ual pods at all.&#10;Master node&#10;Controller&#10;Manager&#10;2. Noti&#64257;cation&#10;through watch&#10;3. Creates&#10;ReplicaSet&#10;4. Noti&#64257;cation&#10;5. Creates pod&#10;6. Noti&#64257;cation&#10;through watch&#10;7. Assigns pod to node&#10;1. Creates Deployment&#10;resource&#10;Deployment&#10;controller&#10;Scheduler&#10;kubectl&#10;ReplicaSet&#10;controller&#10;API server&#10;Deployment A&#10;Deployments&#10;ReplicaSets&#10;Pod A&#10;Pods&#10;ReplicaSet A&#10;Node X&#10;8. Noti&#64257;cation&#10;through watch&#10;9. Tells Docker to&#10;run containers&#10;Docker&#10;10. Runs&#10;containers&#10;Container(s)&#10;Kubelet&#10;Figure 11.12&#10;The chain of events that unfolds when a Deployment resource is posted to the API server&#10; &#10;"
    color "green"
  ]
  node [
    id 552
    label "364"
    title "Page_364"
    color "blue"
  ]
  node [
    id 553
    label "text_275"
    title "332&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;THE REPLICASET CONTROLLER CREATES THE POD RESOURCES&#10;The newly created ReplicaSet is then picked up by the ReplicaSet controller, which&#10;watches for creations, modifications, and deletions of ReplicaSet resources in the&#10;API server. The controller takes into consideration the replica count and pod selec-&#10;tor defined in the ReplicaSet and verifies whether enough existing Pods match&#10;the selector.&#10; The controller then creates the Pod resources based on the pod template in the&#10;ReplicaSet (the pod template was copied over from the Deployment when the Deploy-&#10;ment controller created the ReplicaSet). &#10;THE SCHEDULER ASSIGNS A NODE TO THE NEWLY CREATED PODS&#10;These newly created Pods are now stored in etcd, but they each still lack one import-&#10;ant thing&#8212;they don&#8217;t have an associated node yet. Their nodeName attribute isn&#8217;t set.&#10;The Scheduler watches for Pods like this, and when it encounters one, chooses the&#10;best node for the Pod and assigns the Pod to the node. The Pod&#8217;s definition now&#10;includes the name of the node it should be running on.&#10; Everything so far has been happening in the Kubernetes Control Plane. None of&#10;the controllers that have taken part in this whole process have done anything tangible&#10;except update the resources through the API server. &#10;THE KUBELET RUNS THE POD&#8217;S CONTAINERS&#10;The worker nodes haven&#8217;t done anything up to this point. The pod&#8217;s containers&#10;haven&#8217;t been started yet. The images for the pod&#8217;s containers haven&#8217;t even been down-&#10;loaded yet. &#10; But with the Pod now scheduled to a specific node, the Kubelet on that node can&#10;finally get to work. The Kubelet, watching for changes to Pods on the API server, sees a&#10;new Pod scheduled to its node, so it inspects the Pod definition and instructs Docker,&#10;or whatever container runtime it&#8217;s using, to start the pod&#8217;s containers. The container&#10;runtime then runs the containers.&#10;11.2.3 Observing cluster events&#10;Both the Control Plane components and the Kubelet emit events to the API server as&#10;they perform these actions. They do this by creating Event resources, which are like&#10;any other Kubernetes resource. You&#8217;ve already seen events pertaining to specific&#10;resources every time you used kubectl describe to inspect those resources, but you&#10;can also retrieve events directly with kubectl get events.&#10; Maybe it&#8217;s me, but using kubectl get to inspect events is painful, because they&#8217;re&#10;not shown in proper temporal order. Instead, if an event occurs multiple times, the&#10;event is displayed only once, showing when it was first seen, when it was last seen, and&#10;the number of times it occurred. Luckily, watching events with the --watch option is&#10;much easier on the eyes and useful for seeing what&#8217;s happening in the cluster. &#10; The following listing shows the events emitted in the process described previously&#10;(some columns have been removed and the output is edited heavily to make it legible&#10;in the limited space on the page).&#10; &#10;"
    color "green"
  ]
  node [
    id 554
    label "365"
    title "Page_365"
    color "blue"
  ]
  node [
    id 555
    label "text_276"
    title "333&#10;Understanding what a running pod is&#10;$ kubectl get events --watch&#10;    NAME             KIND         REASON              SOURCE &#10;... kubia            Deployment   ScalingReplicaSet   deployment-controller  &#10;                     &#10149; Scaled up replica set kubia-193 to 3&#10;... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  &#10;                     &#10149; Created pod: kubia-193-w7ll2&#10;... kubia-193-tpg6j  Pod          Scheduled           default-scheduler   &#10;                     &#10149; Successfully assigned kubia-193-tpg6j to node1&#10;... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  &#10;                     &#10149; Created pod: kubia-193-39590&#10;... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  &#10;                     &#10149; Created pod: kubia-193-tpg6j&#10;... kubia-193-39590  Pod          Scheduled           default-scheduler  &#10;                     &#10149; Successfully assigned kubia-193-39590 to node2&#10;... kubia-193-w7ll2  Pod          Scheduled           default-scheduler  &#10;                     &#10149; Successfully assigned kubia-193-w7ll2 to node2&#10;... kubia-193-tpg6j  Pod          Pulled              kubelet, node1  &#10;                     &#10149; Container image already present on machine&#10;... kubia-193-tpg6j  Pod          Created             kubelet, node1  &#10;                     &#10149; Created container with id 13da752&#10;... kubia-193-39590  Pod          Pulled              kubelet, node2  &#10;                     &#10149; Container image already present on machine&#10;... kubia-193-tpg6j  Pod          Started             kubelet, node1  &#10;                     &#10149; Started container with id 13da752&#10;... kubia-193-w7ll2  Pod          Pulled              kubelet, node2  &#10;                     &#10149; Container image already present on machine&#10;... kubia-193-39590  Pod          Created             kubelet, node2  &#10;                     &#10149; Created container with id 8850184&#10;...&#10;As you can see, the SOURCE column shows the controller performing the action, and&#10;the NAME and KIND columns show the resource the controller is acting on. The REASON&#10;column and the MESSAGE column (shown in every second line) give more details&#10;about what the controller has done.&#10;11.3&#10;Understanding what a running pod is&#10;With the pod now running, let&#8217;s look more closely at what a running pod even is. If a&#10;pod contains a single container, do you think that the Kubelet just runs this single&#10;container, or is there more to it?&#10; You&#8217;ve run several pods throughout this book. If you&#8217;re the investigative type, you&#10;may have already snuck a peek at what exactly Docker ran when you created a pod. If&#10;not, let me explain what you&#8217;d see.&#10; Imagine you run a single container pod. Let&#8217;s say you create an Nginx pod:&#10;$ kubectl run nginx --image=nginx&#10;deployment &#34;nginx&#34; created&#10;You can now ssh into the worker node running the pod and inspect the list of run-&#10;ning Docker containers. I&#8217;m using Minikube to test this out, so to ssh into the single&#10;Listing 11.9&#10;Watching events emitted by the controllers&#10; &#10;"
    color "green"
  ]
  node [
    id 556
    label "366"
    title "Page_366"
    color "blue"
  ]
  node [
    id 557
    label "text_277"
    title "334&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;node, I use minikube ssh. If you&#8217;re using GKE, you can ssh into a node with gcloud&#10;compute ssh <node name>.&#10; Once you&#8217;re inside the node, you can list all the running containers with docker&#10;ps, as shown in the following listing.&#10;docker@minikubeVM:~$ docker ps&#10;CONTAINER ID   IMAGE                  COMMAND                 CREATED&#10;c917a6f3c3f7   nginx                  &#34;nginx -g 'daemon off&#34;  4 seconds ago &#10;98b8bf797174   gcr.io/.../pause:3.0   &#34;/pause&#34;                7 seconds ago&#10;...&#10;NOTE&#10;I&#8217;ve removed irrelevant information from the previous listing&#8212;this&#10;includes both columns and rows. I&#8217;ve also removed all the other running con-&#10;tainers. If you&#8217;re trying this out yourself, pay attention to the two containers&#10;that were created a few seconds ago. &#10;As expected, you see the Nginx container, but also an additional container. Judging&#10;from the COMMAND column, this additional container isn&#8217;t doing anything (the con-&#10;tainer&#8217;s command is &#34;pause&#34;). If you look closely, you&#8217;ll see that this container was&#10;created a few seconds before the Nginx container. What&#8217;s its role?&#10; This pause container is the container that holds all the containers of a pod&#10;together. Remember how all containers of a pod share the same network and other&#10;Linux namespaces? The pause container is an infrastructure container whose sole&#10;purpose is to hold all these namespaces. All other user-defined containers of the pod&#10;then use the namespaces of the pod infrastructure container (see figure 11.13).&#10;Actual application containers may die and get restarted. When such a container starts&#10;up again, it needs to become part of the same Linux namespaces as before. The infra-&#10;structure container makes this possible since its lifecycle is tied to that of the pod&#8212;the&#10;container runs from the time the pod is scheduled until the pod is deleted. If the&#10;infrastructure pod is killed in the meantime, the Kubelet recreates it and all the pod&#8217;s&#10;containers.&#10;Listing 11.10&#10;Listing running Docker containers&#10;Pod&#10;Container A&#10;Container A&#10;Pod infrastructure&#10;container&#10;Container B&#10;Container B&#10;Uses Linux&#10;namespaces from&#10;Uses Linux&#10;namespaces from&#10;Figure 11.13&#10;A two-container pod results in three running containers &#10;sharing the same Linux namespaces.&#10; &#10;"
    color "green"
  ]
  node [
    id 558
    label "367"
    title "Page_367"
    color "blue"
  ]
  node [
    id 559
    label "text_278"
    title "335&#10;Inter-pod networking&#10;11.4&#10;Inter-pod networking&#10;By now, you know that each pod gets its own unique IP address and can communicate&#10;with all other pods through a flat, NAT-less network. How exactly does Kubernetes&#10;achieve this? In short, it doesn&#8217;t. The network is set up by the system administrator or&#10;by a Container Network Interface (CNI) plugin, not by Kubernetes itself. &#10;11.4.1 What the network must be like&#10;Kubernetes doesn&#8217;t require you to use a specific networking technology, but it does&#10;mandate that the pods (or to be more precise, their containers) can communicate&#10;with each other, regardless if they&#8217;re running on the same worker node or not. The&#10;network the pods use to communicate must be such that the IP address a pod sees as&#10;its own is the exact same address that all other pods see as the IP address of the pod in&#10;question. &#10; Look at figure 11.14. When pod A connects to (sends a network packet to) pod B,&#10;the source IP pod B sees must be the same IP that pod A sees as its own. There should&#10;be no network address translation (NAT) performed in between&#8212;the packet sent by&#10;pod A must reach pod B with both the source and destination address unchanged.&#10;This is important, because it makes networking for applications running inside pods&#10;simple and exactly as if they were running on machines connected to the same net-&#10;work switch. The absence of NAT between pods enables applications running inside&#10;them to self-register in other pods. &#10;Node 1&#10;Pod A&#10;IP: 10.1.1.1&#10;srcIP: 10.1.1.1&#10;dstIP: 10.1.2.1&#10;srcIP: 10.1.1.1&#10;dstIP: 10.1.2.1&#10;Packet&#10;Node 2&#10;Pod B&#10;IP: 10.1.2.1&#10;srcIP: 10.1.1.1&#10;dstIP: 10.1.2.1&#10;Packet&#10;Network&#10;No NAT (IPs&#10;are preserved)&#10;Figure 11.14&#10;Kubernetes mandates pods are connected through a NAT-less &#10;network.&#10; &#10;"
    color "green"
  ]
  node [
    id 560
    label "368"
    title "Page_368"
    color "blue"
  ]
  node [
    id 561
    label "text_279"
    title "336&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10; For example, say you have a client pod X and pod Y, which provides a kind of noti-&#10;fication service to all pods that register with it. Pod X connects to pod Y and tells it,&#10;&#8220;Hey, I&#8217;m pod X, available at IP 1.2.3.4; please send updates to me at this IP address.&#8221;&#10;The pod providing the service can connect to the first pod by using the received&#10;IP address. &#10; The requirement for NAT-less communication between pods also extends to pod-&#10;to-node and node-to-pod communication. But when a pod communicates with ser-&#10;vices out on the internet, the source IP of the packets the pod sends does need to be&#10;changed, because the pod&#8217;s IP is private. The source IP of outbound packets is&#10;changed to the host worker node&#8217;s IP address.&#10; Building a proper Kubernetes cluster involves setting up the networking according&#10;to these requirements. There are various methods and technologies available to do&#10;this, each with its own benefits or drawbacks in a given scenario. Because of this, we&#8217;re&#10;not going to go into specific technologies. Instead, let&#8217;s explain how inter-pod net-&#10;working works in general. &#10;11.4.2 Diving deeper into how networking works&#10;In section 11.3, we saw that a pod&#8217;s IP address and network namespace are set up and&#10;held by the infrastructure container (the pause container). The pod&#8217;s containers then&#10;use its network namespace. A pod&#8217;s network interface is thus whatever is set up in the&#10;infrastructure container. Let&#8217;s see how the interface is created and how it&#8217;s connected&#10;to the interfaces in all the other pods. Look at figure 11.15. We&#8217;ll discuss it next.&#10;ENABLING COMMUNICATION BETWEEN PODS ON THE SAME NODE&#10;Before the infrastructure container is started, a virtual Ethernet interface pair (a veth&#10;pair) is created for the container. One interface of the pair remains in the host&#8217;s&#10;namespace (you&#8217;ll see it listed as vethXXX when you run ifconfig on the node),&#10;whereas the other is moved into the container&#8217;s network namespace and renamed&#10;eth0. The two virtual interfaces are like two ends of a pipe (or like two network&#10;devices connected by an Ethernet cable)&#8212;what goes in on one side comes out on the&#10;other, and vice-versa. &#10;Node&#10;Pod A&#10;eth0&#10;10.1.1.1&#10;veth123&#10;Pod B&#10;eth0&#10;10.1.1.2&#10;veth234&#10;Bridge&#10;10.1.1.0/24&#10;This is pod A&#8217;s&#10;veth pair.&#10;This is pod B&#8217;s&#10;veth pair.&#10;Figure 11.15&#10;Pods on a node are &#10;connected to the same bridge through &#10;virtual Ethernet interface pairs.&#10; &#10;"
    color "green"
  ]
  node [
    id 562
    label "369"
    title "Page_369"
    color "blue"
  ]
  node [
    id 563
    label "text_280"
    title "337&#10;Inter-pod networking&#10; The interface in the host&#8217;s network namespace is attached to a network bridge that&#10;the container runtime is configured to use. The eth0 interface in the container is&#10;assigned an IP address from the bridge&#8217;s address range. Anything that an application&#10;running inside the container sends to the eth0 network interface (the one in the con-&#10;tainer&#8217;s namespace), comes out at the other veth interface in the host&#8217;s namespace&#10;and is sent to the bridge. This means it can be received by any network interface that&#8217;s&#10;connected to the bridge. &#10; If pod A sends a network packet to pod B, the packet first goes through pod A&#8217;s&#10;veth pair to the bridge and then through pod B&#8217;s veth pair. All containers on a node&#10;are connected to the same bridge, which means they can all communicate with each&#10;other. But to enable communication between containers running on different nodes,&#10;the bridges on those nodes need to be connected somehow. &#10;ENABLING COMMUNICATION BETWEEN PODS ON DIFFERENT NODES&#10;You have many ways to connect bridges on different nodes. This can be done with&#10;overlay or underlay networks or by regular layer 3 routing, which we&#8217;ll look at next.&#10; You know pod IP addresses must be unique across the whole cluster, so the bridges&#10;across the nodes must use non-overlapping address ranges to prevent pods on differ-&#10;ent nodes from getting the same IP. In the example shown in figure 11.16, the bridge&#10;on node A is using the 10.1.1.0/24 IP range and the bridge on node B is using&#10;10.1.2.0/24, which ensures no IP address conflicts exist.&#10; Figure 11.16 shows that to enable communication between pods across two nodes&#10;with plain layer 3 networking, the node&#8217;s physical network interface needs to be con-&#10;nected to the bridge as well. Routing tables on node A need to be configured so all&#10;packets destined for 10.1.2.0/24 are routed to node B, whereas node B&#8217;s routing&#10;tables need to be configured so packets sent to 10.1.1.0/24 are routed to node A.&#10; With this type of setup, when a packet is sent by a container on one of the nodes&#10;to a container on the other node, the packet first goes through the veth pair, then&#10;Node A&#10;Pod A&#10;Network&#10;eth0&#10;10.1.1.1&#10;veth123&#10;Pod B&#10;eth0&#10;10.1.1.2&#10;veth234&#10;Bridge&#10;10.1.1.0/24&#10;eth0&#10;10.100.0.1&#10;Node B&#10;Pod C&#10;eth0&#10;10.1.2.1&#10;veth345&#10;Pod D&#10;eth0&#10;10.1.2.2&#10;veth456&#10;Bridge&#10;10.1.2.0/24&#10;eth0&#10;10.100.0.2&#10;Figure 11.16&#10;For pods on different nodes to communicate, the bridges need to be connected &#10;somehow.&#10; &#10;"
    color "green"
  ]
  node [
    id 564
    label "370"
    title "Page_370"
    color "blue"
  ]
  node [
    id 565
    label "text_281"
    title "338&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;through the bridge to the node&#8217;s physical adapter, then over the wire to the other&#10;node&#8217;s physical adapter, through the other node&#8217;s bridge, and finally through the veth&#10;pair of the destination container.&#10; This works only when nodes are connected to the same network switch, without&#10;any routers in between; otherwise those routers would drop the packets because&#10;they refer to pod IPs, which are private. Sure, the routers in between could be con-&#10;figured to route packets between the nodes, but this becomes increasingly difficult&#10;and error-prone as the number of routers between the nodes increases. Because of&#10;this, it&#8217;s easier to use a Software Defined Network (SDN), which makes the nodes&#10;appear as though they&#8217;re connected to the same network switch, regardless of the&#10;actual underlying network topology, no matter how complex it is. Packets sent&#10;from the pod are encapsulated and sent over the network to the node running the&#10;other pod, where they are de-encapsulated and delivered to the pod in their origi-&#10;nal form.&#10;11.4.3 Introducing the Container Network Interface&#10;To make it easier to connect containers into a network, a project called Container&#10;Network Interface (CNI) was started. The CNI allows Kubernetes to be configured to&#10;use any CNI plugin that&#8217;s out there. These plugins include&#10;&#61601;Calico&#10;&#61601;Flannel&#10;&#61601;Romana&#10;&#61601;Weave Net &#10;&#61601;And others&#10;We&#8217;re not going to go into the details of these plugins; if you want to learn more about&#10;them, refer to https:/&#10;/kubernetes.io/docs/concepts/cluster-administration/addons/.&#10; Installing a network plugin isn&#8217;t difficult. You only need to deploy a YAML con-&#10;taining a DaemonSet and a few other supporting resources. This YAML is provided&#10;on each plugin&#8217;s project page. As you can imagine, the DaemonSet is used to deploy&#10;a network agent on all cluster nodes. It then ties into the CNI interface on the node,&#10;but be aware that the Kubelet needs to be started with --network-plugin=cni to&#10;use CNI. &#10;11.5&#10;How services are implemented&#10;In chapter 5 you learned about Services, which allow exposing a set of pods at a long-&#10;lived, stable IP address and port. In order to focus on what Services are meant for and&#10;how they can be used, we intentionally didn&#8217;t go into how they work. But to truly&#10;understand Services and have a better feel for where to look when things don&#8217;t behave&#10;the way you expect, you need to understand how they are implemented. &#10; &#10;"
    color "green"
  ]
  node [
    id 566
    label "371"
    title "Page_371"
    color "blue"
  ]
  node [
    id 567
    label "text_282"
    title "339&#10;How services are implemented&#10;11.5.1 Introducing the kube-proxy&#10;Everything related to Services is handled by the kube-proxy process running on each&#10;node. Initially, the kube-proxy was an actual proxy waiting for connections and for&#10;each incoming connection, opening a new connection to one of the pods. This was&#10;called the userspace proxy mode. Later, a better-performing iptables proxy mode&#10;replaced it. This is now the default, but you can still configure Kubernetes to use the&#10;old mode if you want.&#10; Before we continue, let&#8217;s quickly review a few things about Services, which are rele-&#10;vant for understanding the next few paragraphs.&#10; We&#8217;ve learned that each Service gets its own stable IP address and port. Clients&#10;(usually pods) use the service by connecting to this IP address and port. The IP&#10;address is virtual&#8212;it&#8217;s not assigned to any network interfaces and is never listed as&#10;either the source or the destination IP address in a network packet when the packet&#10;leaves the node. A key detail of Services is that they consist of an IP and port pair (or&#10;multiple IP and port pairs in the case of multi-port Services), so the service IP by itself&#10;doesn&#8217;t represent anything. That&#8217;s why you can&#8217;t ping them. &#10;11.5.2 How kube-proxy uses iptables&#10;When a service is created in the API server, the virtual IP address is assigned to it&#10;immediately. Soon afterward, the API server notifies all kube-proxy agents running on&#10;the worker nodes that a new Service has been created. Then, each kube-proxy makes&#10;that service addressable on the node it&#8217;s running on. It does this by setting up a few&#10;iptables rules, which make sure each packet destined for the service IP/port pair is&#10;intercepted and its destination address modified, so the packet is redirected to one of&#10;the pods backing the service. &#10; Besides watching the API server for changes to Services, kube-proxy also watches&#10;for changes to Endpoints objects. We talked about them in chapter 5, but let me&#10;refresh your memory, as it&#8217;s easy to forget they even exist, because you rarely create&#10;them manually. An Endpoints object holds the IP/port pairs of all the pods that back&#10;the service (an IP/port pair can also point to something other than a pod). That&#8217;s&#10;why the kube-proxy must also watch all Endpoints objects. After all, an Endpoints&#10;object changes every time a new backing pod is created or deleted, and when the&#10;pod&#8217;s readiness status changes or the pod&#8217;s labels change and it falls in or out of scope&#10;of the service. &#10; Now let&#8217;s see how kube-proxy enables clients to connect to those pods through the&#10;Service. This is shown in figure 11.17.&#10; The figure shows what the kube-proxy does and how a packet sent by a client pod&#10;reaches one of the pods backing the Service. Let&#8217;s examine what happens to the&#10;packet when it&#8217;s sent by the client pod (pod A in the figure). &#10; The packet&#8217;s destination is initially set to the IP and port of the Service (in the&#10;example, the Service is at 172.30.0.1:80). Before being sent to the network, the&#10; &#10;"
    color "green"
  ]
  node [
    id 568
    label "372"
    title "Page_372"
    color "blue"
  ]
  node [
    id 569
    label "text_283"
    title "340&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;packet is first handled by node A&#8217;s kernel according to the iptables rules set up on&#10;the node. &#10; The kernel checks if the packet matches any of those iptables rules. One of them&#10;says that if any packet has the destination IP equal to 172.30.0.1 and destination port&#10;equal to 80, the packet&#8217;s destination IP and port should be replaced with the IP and&#10;port of a randomly selected pod. &#10; The packet in the example matches that rule and so its destination IP/port is&#10;changed. In the example, pod B2 was randomly selected, so the packet&#8217;s destination&#10;IP is changed to 10.1.2.1 (pod B2&#8217;s IP) and the port to 8080 (the target port specified&#10;in the Service spec). From here on, it&#8217;s exactly as if the client pod had sent the packet&#10;to pod B directly instead of through the service. &#10; It&#8217;s slightly more complicated than that, but that&#8217;s the most important part you&#10;need to understand.&#10; &#10;Node A&#10;Node B&#10;API server&#10;Pod A&#10;Pod B1&#10;Pod B2&#10;Pod B3&#10;Packet X&#10;Source:&#10;10.1.1.1&#10;Destination:&#10;172.30.0.1:80&#10;10.1.2.1:8080&#10;iptables&#10;Service B&#10;172.30.0.1:80&#10;Con&#64257;gures&#10;iptables&#10;Packet X&#10;Source:&#10;10.1.1.1&#10;Destination:&#10;172.30.0.1:80&#10;kube-proxy&#10;Endpoints B&#10;Pod A&#10;IP: 10.1.1.1&#10;Pod B1&#10;IP: 10.1.1.2&#10;Pod B2&#10;IP: 10.1.2.1&#10;Pod B3&#10;IP: 10.1.2.2&#10;Watches for changes to&#10;services and endpoints&#10;Figure 11.17&#10;Network packets sent to a Service&#8217;s virtual IP/port pair are &#10;modified and redirected to a randomly selected backend pod.&#10; &#10;"
    color "green"
  ]
  node [
    id 570
    label "373"
    title "Page_373"
    color "blue"
  ]
  node [
    id 571
    label "text_284"
    title "341&#10;Running highly available clusters&#10;11.6&#10;Running highly available clusters&#10;One of the reasons for running apps inside Kubernetes is to keep them running with-&#10;out interruption with no or limited manual intervention in case of infrastructure&#10;failures. For running services without interruption it&#8217;s not only the apps that need to&#10;be up all the time, but also the Kubernetes Control Plane components. We&#8217;ll look at&#10;what&#8217;s involved in achieving high availability next.&#10;11.6.1 Making your apps highly available&#10;When running apps in Kubernetes, the various controllers make sure your app keeps&#10;running smoothly and at the specified scale even when nodes fail. To ensure your app&#10;is highly available, you only need to run them through a Deployment resource and&#10;configure an appropriate number of replicas; everything else is taken care of by&#10;Kubernetes. &#10;RUNNING MULTIPLE INSTANCES TO REDUCE THE LIKELIHOOD OF DOWNTIME&#10;This requires your apps to be horizontally scalable, but even if that&#8217;s not the case in&#10;your app, you should still use a Deployment with its replica count set to one. If the&#10;replica becomes unavailable, it will be replaced with a new one quickly, although that&#10;doesn&#8217;t happen instantaneously. It takes time for all the involved controllers to notice&#10;that a node has failed, create the new pod replica, and start the pod&#8217;s containers.&#10;There will inevitably be a short period of downtime in between. &#10;USING LEADER-ELECTION FOR NON-HORIZONTALLY SCALABLE APPS&#10;To avoid the downtime, you need to run additional inactive replicas along with the&#10;active one and use a fast-acting lease or leader-election mechanism to make sure only&#10;one is active. In case you&#8217;re unfamiliar with leader election, it&#8217;s a way for multiple app&#10;instances running in a distributed environment to come to an agreement on which is&#10;the leader. That leader is either the only one performing tasks, while all others are&#10;waiting for the leader to fail and then becoming leaders themselves, or they can all be&#10;active, with the leader being the only instance performing writes, while all the others&#10;are providing read-only access to their data, for example. This ensures two instances&#10;are never doing the same job, if that would lead to unpredictable system behavior due&#10;to race conditions.&#10; The mechanism doesn&#8217;t need to be incorporated into the app itself. You can use a&#10;sidecar container that performs all the leader-election operations and signals the&#10;main container when it should become active. You&#8217;ll find an example of leader elec-&#10;tion in Kubernetes at https:/&#10;/github.com/kubernetes/contrib/tree/master/election.&#10; Ensuring your apps are highly available is relatively simple, because Kubernetes&#10;takes care of almost everything. But what if Kubernetes itself fails? What if the servers&#10;running the Kubernetes Control Plane components go down? How are those compo-&#10;nents made highly available?&#10; &#10;"
    color "green"
  ]
  node [
    id 572
    label "374"
    title "Page_374"
    color "blue"
  ]
  node [
    id 573
    label "text_285"
    title "342&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;11.6.2 Making Kubernetes Control Plane components highly available&#10;In the beginning of this chapter, you learned about the few components that make up&#10;a Kubernetes Control Plane. To make Kubernetes highly available, you need to run&#10;multiple master nodes, which run multiple instances of the following components:&#10;&#61601;etcd, which is the distributed data store where all the API objects are kept&#10;&#61601;API server&#10;&#61601;Controller Manager, which is the process in which all the controllers run&#10;&#61601;Scheduler&#10;Without going into the actual details of how to install and run these components, let&#8217;s&#10;see what&#8217;s involved in making each of these components highly available. Figure 11.18&#10;shows an overview of a highly available cluster.&#10;RUNNING AN ETCD CLUSTER&#10;Because etcd was designed as a distributed system, one of its key features is the ability&#10;to run multiple etcd instances, so making it highly available is no big deal. All you&#10;need to do is run it on an appropriate number of machines (three, five, or seven, as&#10;explained earlier in the chapter) and make them aware of each other. You do this by&#10;including the list of all the other instances in every instance&#8217;s configuration. For&#10;example, when starting an instance, you specify the IPs and ports where the other etcd&#10;instances can be reached. &#10; etcd will replicate data across all its instances, so a failure of one of the nodes when&#10;running a three-machine cluster will still allow the cluster to accept both read and&#10;write operations. To increase the fault tolerance to more than a single node, you need&#10;to run five or seven etcd nodes, which would allow the cluster to handle two or three&#10;Node 1&#10;Kubelet&#10;Node 2&#10;Kubelet&#10;Node 3&#10;Kubelet&#10;Node 4&#10;Kubelet&#10;Node 5&#10;Kubelet&#10;...&#10;Node N&#10;Kubelet&#10;Load&#10;balancer&#10;Master 3&#10;etcd&#10;API server&#10;Scheduler&#10;Controller&#10;Manager&#10;[standing-by]&#10;[standing-by]&#10;Master 2&#10;etcd&#10;API server&#10;Scheduler&#10;Controller&#10;Manager&#10;[standing-by]&#10;[standing-by]&#10;Master 1&#10;etcd&#10;API server&#10;Scheduler&#10;Controller&#10;Manager&#10;[active]&#10;[active]&#10;Figure 11.18&#10;A highly-available cluster with three master nodes&#10; &#10;"
    color "green"
  ]
  node [
    id 574
    label "375"
    title "Page_375"
    color "blue"
  ]
  node [
    id 575
    label "text_286"
    title "343&#10;Running highly available clusters&#10;node failures, respectively. Having more than seven etcd instances is almost never nec-&#10;essary and begins impacting performance.&#10;RUNNING MULTIPLE INSTANCES OF THE API SERVER&#10;Making the API server highly available is even simpler. Because the API server is (almost&#10;completely) stateless (all the data is stored in etcd, but the API server does cache it), you&#10;can run as many API servers as you need, and they don&#8217;t need to be aware of each other&#10;at all. Usually, one API server is collocated with every etcd instance. By doing this, the&#10;etcd instances don&#8217;t need any kind of load balancer in front of them, because every API&#10;server instance only talks to the local etcd instance. &#10; The API servers, on the other hand, do need to be fronted by a load balancer, so&#10;clients (kubectl, but also the Controller Manager, Scheduler, and all the Kubelets)&#10;always connect only to the healthy API server instances. &#10;ENSURING HIGH AVAILABILITY OF THE CONTROLLERS AND THE SCHEDULER&#10;Compared to the API server, where multiple replicas can run simultaneously, run-&#10;ning multiple instances of the Controller Manager or the Scheduler isn&#8217;t as simple.&#10;Because controllers and the Scheduler all actively watch the cluster state and act when&#10;it changes, possibly modifying the cluster state further (for example, when the desired&#10;replica count on a ReplicaSet is increased by one, the ReplicaSet controller creates an&#10;additional pod), running multiple instances of each of those components would&#10;result in all of them performing the same action. They&#8217;d be racing each other, which&#10;could cause undesired effects (creating two new pods instead of one, as mentioned in&#10;the previous example).&#10; For this reason, when running multiple instances of these components, only one&#10;instance may be active at any given time. Luckily, this is all taken care of by the compo-&#10;nents themselves (this is controlled with the --leader-elect option, which defaults to&#10;true). Each individual component will only be active when it&#8217;s the elected leader. Only&#10;the leader performs actual work, whereas all other instances are standing by and waiting&#10;for the current leader to fail. When it does, the remaining instances elect a new leader,&#10;which then takes over the work. This mechanism ensures that two components are never&#10;operating at the same time and doing the same work (see figure 11.19).&#10;Master 3&#10;Scheduler&#10;Controller&#10;Manager&#10;[standing-by]&#10;[standing-by]&#10;Master 1&#10;Scheduler&#10;Controller&#10;Manager&#10;[active]&#10;[active]&#10;Master 2&#10;Scheduler&#10;Controller&#10;Manager&#10;[standing-by]&#10;[standing-by]&#10;Only the controllers in&#10;this Controller Manager&#10;are reacting to API&#10;resources being created,&#10;updated, and deleted.&#10;These Controller Managers&#10;and Schedulers aren&#8217;t doing&#10;anything except waiting to&#10;become leaders.&#10;Only this Scheduler&#10;is scheduling pods.&#10;Figure 11.19&#10;Only a single Controller Manager and a single Scheduler are active; others are standing by.&#10; &#10;"
    color "green"
  ]
  node [
    id 576
    label "376"
    title "Page_376"
    color "blue"
  ]
  node [
    id 577
    label "text_287"
    title "344&#10;CHAPTER 11&#10;Understanding Kubernetes internals&#10;The Controller Manager and Scheduler can run collocated with the API server and&#10;etcd, or they can run on separate machines. When collocated, they can talk to the&#10;local API server directly; otherwise they connect to the API servers through the load&#10;balancer.&#10;UNDERSTANDING THE LEADER ELECTION MECHANISM USED IN CONTROL PLANE COMPONENTS&#10;What I find most interesting here is that these components don&#8217;t need to talk to each&#10;other directly to elect a leader. The leader election mechanism works purely by creat-&#10;ing a resource in the API server. And it&#8217;s not even a special kind of resource&#8212;the End-&#10;points resource is used to achieve this (abused is probably a more appropriate term).&#10; There&#8217;s nothing special about using an Endpoints object to do this. It&#8217;s used&#10;because it has no side effects as long as no Service with the same name exists. Any&#10;other resource could be used (in fact, the leader election mechanism will soon use&#10;ConfigMaps instead of Endpoints). &#10; I&#8217;m sure you&#8217;re interested in how a resource can be used for this purpose. Let&#8217;s&#10;take the Scheduler, for example. All instances of the Scheduler try to create (and later&#10;update) an Endpoints resource called kube-scheduler. You&#8217;ll find it in the kube-&#10;system namespace, as the following listing shows.&#10;$ kubectl get endpoints kube-scheduler -n kube-system -o yaml&#10;apiVersion: v1&#10;kind: Endpoints&#10;metadata:&#10;  annotations:&#10;    control-plane.alpha.kubernetes.io/leader: '{&#34;holderIdentity&#34;:&#10;      &#10149; &#34;minikube&#34;,&#34;leaseDurationSeconds&#34;:15,&#34;acquireTime&#34;:&#10;      &#10149; &#34;2017-05-27T18:54:53Z&#34;,&#34;renewTime&#34;:&#34;2017-05-28T13:07:49Z&#34;,&#10;      &#10149; &#34;leaderTransitions&#34;:0}'&#10;  creationTimestamp: 2017-05-27T18:54:53Z&#10;  name: kube-scheduler&#10;  namespace: kube-system&#10;  resourceVersion: &#34;654059&#34;&#10;  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler&#10;  uid: f847bd14-430d-11e7-9720-080027f8fa4e&#10;subsets: []&#10;The control-plane.alpha.kubernetes.io/leader annotation is the important part.&#10;As you can see, it contains a field called holderIdentity, which holds the name of the&#10;current leader. The first instance that succeeds in putting its name there becomes&#10;the leader. Instances race each other to do that, but there&#8217;s always only one winner.&#10; Remember the optimistic concurrency we explained earlier? That&#8217;s what ensures&#10;that if multiple instances try to write their name into the resource only one of them&#10;succeeds. Based on whether the write succeeded or not, each instance knows whether&#10;it is or it isn&#8217;t the leader. &#10; Once becoming the leader, it must periodically update the resource (every two sec-&#10;onds by default), so all other instances know that it&#8217;s still alive. When the leader fails,&#10;Listing 11.11&#10;The kube-scheduler Endpoints resource used for leader-election&#10; &#10;"
    color "green"
  ]
  node [
    id 578
    label "377"
    title "Page_377"
    color "blue"
  ]
  node [
    id 579
    label "text_288"
    title "345&#10;Summary&#10;other instances see that the resource hasn&#8217;t been updated for a while, and try to become&#10;the leader by writing their own name to the resource. Simple, right?&#10;11.7&#10;Summary&#10;Hopefully, this has been an interesting chapter that has improved your knowledge of&#10;the inner workings of Kubernetes. This chapter has shown you&#10;&#61601;What components make up a Kubernetes cluster and what each component is&#10;responsible for&#10;&#61601;How the API server, Scheduler, various controllers running in the Controller&#10;Manager, and the Kubelet work together to bring a pod to life&#10;&#61601;How the infrastructure container binds together all the containers of a pod&#10;&#61601;How pods communicate with other pods running on the same node through&#10;the network bridge, and how those bridges on different nodes are connected,&#10;so pods running on different nodes can talk to each other&#10;&#61601;How the kube-proxy performs load balancing across pods in the same service by&#10;configuring iptables rules on the node&#10;&#61601;How multiple instances of each component of the Control Plane can be run to&#10;make the cluster highly available&#10;Next, we&#8217;ll look at how to secure the API server and, by extension, the cluster as a whole.&#10; &#10;"
    color "green"
  ]
  node [
    id 580
    label "378"
    title "Page_378"
    color "blue"
  ]
  node [
    id 581
    label "text_289"
    title "346&#10;Securing the&#10;Kubernetes API server&#10;In chapter 8 you learned how applications running in pods can talk to the API&#10;server to retrieve or change the state of resources deployed in the cluster. To&#10;authenticate with the API server, you used the ServiceAccount token mounted into&#10;the pod. In this chapter, you&#8217;ll learn what ServiceAccounts are and how to config-&#10;ure their permissions, as well as permissions for other subjects using the cluster. &#10;12.1&#10;Understanding authentication&#10;In the previous chapter, we said the API server can be configured with one or more&#10;authentication plugins (and the same is true for authorization plugins). When a&#10;request is received by the API server, it goes through the list of authentication&#10;This chapter covers&#10;&#61601;Understanding authentication&#10;&#61601;What ServiceAccounts are and why they&#8217;re used&#10;&#61601;Understanding the role-based access control &#10;(RBAC) plugin&#10;&#61601;Using Roles and RoleBindings&#10;&#61601;Using ClusterRoles and ClusterRoleBindings&#10;&#61601;Understanding the default roles and bindings&#10; &#10;"
    color "green"
  ]
  node [
    id 582
    label "379"
    title "Page_379"
    color "blue"
  ]
  node [
    id 583
    label "text_290"
    title "347&#10;Understanding authentication&#10;plugins, so they can each examine the request and try to determine who&#8217;s sending the&#10;request. The first plugin that can extract that information from the request returns&#10;the username, user ID, and the groups the client belongs to back to the API server&#10;core. The API server stops invoking the remaining authentication plugins and contin-&#10;ues onto the authorization phase. &#10; Several authentication plugins are available. They obtain the identity of the client&#10;using the following methods:&#10;&#61601;From the client certificate&#10;&#61601;From an authentication token passed in an HTTP header&#10;&#61601;Basic HTTP authentication&#10;&#61601;Others&#10;The authentication plugins are enabled through command-line options when starting&#10;the API server. &#10;12.1.1 Users and groups&#10;An authentication plugin returns the username and group(s) of the authenticated&#10;user. Kubernetes doesn&#8217;t store that information anywhere; it uses it to verify whether&#10;the user is authorized to perform an action or not.&#10;UNDERSTANDING USERS&#10;Kubernetes distinguishes between two kinds of clients connecting to the API server:&#10;&#61601;Actual humans (users)&#10;&#61601;Pods (more specifically, applications running inside them)&#10;Both these types of clients are authenticated using the aforementioned authentication&#10;plugins. Users are meant to be managed by an external system, such as a Single Sign&#10;On (SSO) system, but the pods use a mechanism called service accounts, which are cre-&#10;ated and stored in the cluster as ServiceAccount resources. In contrast, no resource&#10;represents user accounts, which means you can&#8217;t create, update, or delete users through&#10;the API server. &#10; We won&#8217;t go into any details of how to manage users, but we will explore Service-&#10;Accounts in detail, because they&#8217;re essential for running pods. For more informa-&#10;tion on how to configure the cluster for authentication of human users, cluster&#10;administrators should refer to the Kubernetes Cluster Administrator guide at http:/&#10;/&#10;kubernetes.io/docs/admin.&#10;UNDERSTANDING GROUPS&#10;Both human users and ServiceAccounts can belong to one or more groups. We&#8217;ve said&#10;that the authentication plugin returns groups along with the username and user ID.&#10;Groups are used to grant permissions to several users at once, instead of having to&#10;grant them to individual users. &#10; &#10;"
    color "green"
  ]
  node [
    id 584
    label "380"
    title "Page_380"
    color "blue"
  ]
  node [
    id 585
    label "text_291"
    title "348&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10; Groups returned by the plugin are nothing but strings, representing arbitrary&#10;group names, but built-in groups have special meaning:&#10;&#61601;The system:unauthenticated group is used for requests where none of the&#10;authentication plugins could authenticate the client.&#10;&#61601;The system:authenticated group is automatically assigned to a user who was&#10;authenticated successfully.&#10;&#61601;The system:serviceaccounts group encompasses all ServiceAccounts in the&#10;system.&#10;&#61601;The system:serviceaccounts:<namespace> includes all ServiceAccounts in a&#10;specific namespace.&#10;12.1.2 Introducing ServiceAccounts&#10;Let&#8217;s explore ServiceAccounts up close. You&#8217;ve already learned that the API server&#10;requires clients to authenticate themselves before they&#8217;re allowed to perform opera-&#10;tions on the server. And you&#8217;ve already seen how pods can authenticate by sending the&#10;contents of the file/var/run/secrets/kubernetes.io/serviceaccount/token, which&#10;is mounted into each container&#8217;s filesystem through a secret volume.&#10; But what exactly does that file represent? Every pod is associated with a Service-&#10;Account, which represents the identity of the app running in the pod. The token file&#10;holds the ServiceAccount&#8217;s authentication token. When an app uses this token to con-&#10;nect to the API server, the authentication plugin authenticates the ServiceAccount&#10;and passes the ServiceAccount&#8217;s username back to the API server core. Service-&#10;Account usernames are formatted like this:&#10;system:serviceaccount:<namespace>:<service account name>&#10;The API server passes this username to the configured authorization plugins, which&#10;determine whether the action the app is trying to perform is allowed to be performed&#10;by the ServiceAccount.&#10; ServiceAccounts are nothing more than a way for an application running inside a&#10;pod to authenticate itself with the API server. As already mentioned, applications do&#10;that by passing the ServiceAccount&#8217;s token in the request.&#10;UNDERSTANDING THE SERVICEACCOUNT RESOURCE&#10;ServiceAccounts are resources just like Pods, Secrets, ConfigMaps, and so on, and are&#10;scoped to individual namespaces. A default ServiceAccount is automatically created&#10;for each namespace (that&#8217;s the one your pods have used all along). &#10; You can list ServiceAccounts like you do other resources:&#10;$ kubectl get sa&#10;NAME      SECRETS   AGE&#10;default   1         1d&#10;NOTE&#10;The shorthand for serviceaccount is sa.&#10; &#10;"
    color "green"
  ]
  node [
    id 586
    label "381"
    title "Page_381"
    color "blue"
  ]
  node [
    id 587
    label "text_292"
    title "349&#10;Understanding authentication&#10;As you can see, the current namespace only contains the default ServiceAccount. Addi-&#10;tional ServiceAccounts can be added when required. Each pod is associated with exactly&#10;one ServiceAccount, but multiple pods can use the same ServiceAccount. As you can&#10;see in figure 12.1, a pod can only use a ServiceAccount from the same namespace.&#10;UNDERSTANDING HOW SERVICEACCOUNTS TIE INTO AUTHORIZATION&#10;You can assign a ServiceAccount to a pod by specifying the account&#8217;s name in the pod&#10;manifest. If you don&#8217;t assign it explicitly, the pod will use the default ServiceAccount&#10;in the namespace.&#10; By assigning different ServiceAccounts to pods, you can control which resources&#10;each pod has access to. When a request bearing the authentication token is received&#10;by the API server, the server uses the token to authenticate the client sending the&#10;request and then determines whether or not the related ServiceAccount is allowed to&#10;perform the requested operation. The API server obtains this information from the&#10;system-wide authorization plugin configured by the cluster administrator. One of&#10;the available authorization plugins is the role-based access control (RBAC) plugin,&#10;which is discussed later in this chapter. From Kubernetes version 1.6 on, the RBAC&#10;plugin is the plugin most clusters should use.&#10;12.1.3 Creating ServiceAccounts&#10;We&#8217;ve said every namespace contains its own default ServiceAccount, but additional&#10;ones can be created if necessary. But why should you bother with creating Service-&#10;Accounts instead of using the default one for all your pods? &#10; The obvious reason is cluster security. Pods that don&#8217;t need to read any cluster&#10;metadata should run under a constrained account that doesn&#8217;t allow them to retrieve&#10;or modify any resources deployed in the cluster. Pods that need to retrieve resource&#10;metadata should run under a ServiceAccount that only allows reading those objects&#8217;&#10;metadata, whereas pods that need to modify those objects should run under their own&#10;ServiceAccount allowing modifications of API objects. &#10;Pod&#10;Namespace: foo&#10;Service-&#10;Account:&#10;default&#10;Pod&#10;Pod&#10;Namespace: baz&#10;Pod&#10;Namespace: bar&#10;Pod&#10;Pod&#10;Not possible&#10;Service-&#10;Account:&#10;default&#10;Another&#10;Service-&#10;Account&#10;Multiple pods using the&#10;same ServiceAccount&#10;Figure 12.1&#10;Each pod is associated with a single ServiceAccount in the pod&#8217;s namespace.&#10; &#10;"
    color "green"
  ]
  node [
    id 588
    label "382"
    title "Page_382"
    color "blue"
  ]
  node [
    id 589
    label "text_293"
    title "350&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10; Let&#8217;s see how you can create additional ServiceAccounts, how they relate to Secrets,&#10;and how you can assign them to your pods.&#10;CREATING A SERVICEACCOUNT&#10;Creating a ServiceAccount is incredibly easy, thanks to the dedicated kubectl create&#10;serviceaccount command. Let&#8217;s create a new ServiceAccount called foo:&#10;$ kubectl create serviceaccount foo&#10;serviceaccount &#34;foo&#34; created&#10;Now, you can inspect the ServiceAccount with the describe command, as shown in&#10;the following listing.&#10;$ kubectl describe sa foo&#10;Name:               foo&#10;Namespace:          default&#10;Labels:             <none>&#10;Image pull secrets: <none>             &#10;Mountable secrets:  foo-token-qzq7j    &#10;Tokens:             foo-token-qzq7j    &#10;You can see that a custom token Secret has been created and associated with the&#10;ServiceAccount. If you look at the Secret&#8217;s data with kubectl describe secret foo-&#10;token-qzq7j, you&#8217;ll see it contains the same items (the CA certificate, namespace, and&#10;token) as the default ServiceAccount&#8217;s token does (the token itself will obviously be&#10;different), as shown in the following listing.&#10;$ kubectl describe secret foo-token-qzq7j&#10;...&#10;ca.crt:         1066 bytes&#10;namespace:      7 bytes&#10;token:          eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...&#10;NOTE&#10;You&#8217;ve probably heard of JSON Web Tokens (JWT). The authentica-&#10;tion tokens used in ServiceAccounts are JWT tokens.&#10;UNDERSTANDING A SERVICEACCOUNT&#8217;S MOUNTABLE SECRETS&#10;The token is shown in the Mountable secrets list when you inspect a ServiceAccount&#10;with kubectl describe. Let me explain what that list represents. In chapter 7 you&#10;learned how to create Secrets and mount them inside a pod. By default, a pod can&#10;mount any Secret it wants. But the pod&#8217;s ServiceAccount can be configured to only&#10;Listing 12.1&#10;Inspecting a ServiceAccount with kubectl describe&#10;Listing 12.2&#10;Inspecting the custom ServiceAccount&#8217;s Secret&#10;These will be added &#10;automatically to all pods &#10;using this ServiceAccount.&#10;Pods using this ServiceAccount &#10;can only mount these Secrets if &#10;mountable Secrets are enforced.&#10;Authentication token(s). &#10;The first one is mounted &#10;inside the container.&#10; &#10;"
    color "green"
  ]
  node [
    id 590
    label "383"
    title "Page_383"
    color "blue"
  ]
  node [
    id 591
    label "text_294"
    title "351&#10;Understanding authentication&#10;allow the pod to mount Secrets that are listed as mountable Secrets on the Service-&#10;Account. To enable this feature, the ServiceAccount must contain the following anno-&#10;tation: kubernetes.io/enforce-mountable-secrets=&#34;true&#34;. &#10; If the ServiceAccount is annotated with this annotation, any pods using it can mount&#10;only the ServiceAccount&#8217;s mountable Secrets&#8212;they can&#8217;t use any other Secret.&#10;UNDERSTANDING A SERVICEACCOUNT&#8217;S IMAGE PULL SECRETS&#10;A ServiceAccount can also contain a list of image pull Secrets, which we examined in&#10;chapter 7. In case you don&#8217;t remember, they are Secrets that hold the credentials for&#10;pulling container images from a private image repository. &#10; The following listing shows an example of a ServiceAccount definition, which&#10;includes the image pull Secret you created in chapter 7.&#10;apiVersion: v1&#10;kind: ServiceAccount&#10;metadata:&#10;  name: my-service-account&#10;imagePullSecrets:&#10;- name: my-dockerhub-secret&#10;A ServiceAccount&#8217;s image pull Secrets behave slightly differently than its mountable&#10;Secrets. Unlike mountable Secrets, they don&#8217;t determine which image pull Secrets a&#10;pod can use, but which ones are added automatically to all pods using the Service-&#10;Account. Adding image pull Secrets to a ServiceAccount saves you from having to add&#10;them to each pod individually. &#10;12.1.4 Assigning a ServiceAccount to a pod&#10;After you create additional ServiceAccounts, you need to assign them to pods. This is&#10;done by setting the name of the ServiceAccount in the spec.serviceAccountName&#10;field in the pod definition. &#10;NOTE&#10;A pod&#8217;s ServiceAccount must be set when creating the pod. It can&#8217;t be&#10;changed later. &#10;CREATING A POD WHICH USES A CUSTOM SERVICEACCOUNT&#10;In chapter 8 you deployed a pod that ran a container based on the tutum/curl image&#10;and an ambassador container alongside it. You used it to explore the API server&#8217;s&#10;REST interface. The ambassador container ran the kubectl proxy process, which&#10;used the pod&#8217;s ServiceAccount&#8217;s token to authenticate with the API server. &#10; You can now modify the pod so it uses the foo ServiceAccount you created minutes&#10;ago. The next listing shows the pod definition.&#10; &#10; &#10;Listing 12.3&#10;ServiceAccount with an image pull Secret: sa-image-pull-secrets.yaml&#10; &#10;"
    color "green"
  ]
  node [
    id 592
    label "384"
    title "Page_384"
    color "blue"
  ]
  node [
    id 593
    label "text_295"
    title "352&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: curl-custom-sa&#10;spec:&#10;  serviceAccountName: foo           &#10;  containers:&#10;  - name: main&#10;    image: tutum/curl&#10;    command: [&#34;sleep&#34;, &#34;9999999&#34;]&#10;  - name: ambassador                  &#10;    image: luksa/kubectl-proxy:1.6.2&#10;To confirm that the custom ServiceAccount&#8217;s token is mounted into the two contain-&#10;ers, you can print the contents of the token as shown in the following listing.&#10;$ kubectl exec -it curl-custom-sa -c main &#10;&#10149; cat /var/run/secrets/kubernetes.io/serviceaccount/token&#10;eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...&#10;You can see the token is the one from the foo ServiceAccount by comparing the token&#10;string in listing 12.5 with the one in listing 12.2. &#10;USING THE CUSTOM SERVICEACCOUNT&#8217;S TOKEN TO TALK TO THE API SERVER&#10;Let&#8217;s see if you can talk to the API server using this token. As mentioned previously,&#10;the ambassador container uses the token when talking to the server, so you can test&#10;the token by going through the ambassador, which listens on localhost:8001, as&#10;shown in the following listing.&#10;$ kubectl exec -it curl-custom-sa -c main curl localhost:8001/api/v1/pods&#10;{&#10;  &#34;kind&#34;: &#34;PodList&#34;,&#10;  &#34;apiVersion&#34;: &#34;v1&#34;,&#10;  &#34;metadata&#34;: {&#10;    &#34;selfLink&#34;: &#34;/api/v1/pods&#34;,&#10;    &#34;resourceVersion&#34;: &#34;433895&#34;&#10;  },&#10;  &#34;items&#34;: [&#10;  ...&#10;Okay, you got back a proper response from the server, which means the custom&#10;ServiceAccount is allowed to list pods. This may be because your cluster doesn&#8217;t use&#10;the RBAC authorization plugin, or you gave all ServiceAccounts full permissions, as&#10;instructed in chapter 8. &#10;Listing 12.4&#10;Pod using a non-default ServiceAccount: curl-custom-sa.yaml&#10;Listing 12.5&#10;Inspecting the token mounted into the pod&#8217;s container(s)&#10;Listing 12.6&#10;Talking to the API server with a custom ServiceAccount&#10;This pod uses the &#10;foo ServiceAccount &#10;instead of the default.&#10; &#10;"
    color "green"
  ]
  node [
    id 594
    label "385"
    title "Page_385"
    color "blue"
  ]
  node [
    id 595
    label "text_296"
    title "353&#10;Securing the cluster with role-based access control&#10; When your cluster isn&#8217;t using proper authorization, creating and using additional&#10;ServiceAccounts doesn&#8217;t make much sense, since even the default ServiceAccount is&#10;allowed to do anything. The only reason to use ServiceAccounts in that case is to&#10;enforce mountable Secrets or to provide image pull Secrets through the Service-&#10;Account, as explained earlier. &#10; But creating additional ServiceAccounts is practically a must when you use the&#10;RBAC authorization plugin, which we&#8217;ll explore next.&#10;12.2&#10;Securing the cluster with role-based access control&#10;Starting with Kubernetes version 1.6.0, cluster security was ramped up considerably. In&#10;earlier versions, if you managed to acquire the authentication token from one of the&#10;pods, you could use it to do anything you want in the cluster. If you google around,&#10;you&#8217;ll find demos showing how a path traversal (or directory traversal) attack (where clients&#10;can retrieve files located outside of the web server&#8217;s web root directory) can be used to&#10;get the token and use it to run your malicious pods in an insecure Kubernetes cluster.&#10; But in version 1.8.0, the RBAC authorization plugin graduated to GA (General&#10;Availability) and is now enabled by default on many clusters (for example, when&#10;deploying a cluster with kubadm, as described in appendix B). RBAC prevents unau-&#10;thorized users from viewing or modifying the cluster state. The default Service-&#10;Account isn&#8217;t allowed to view cluster state, let alone modify it in any way, unless you&#10;grant it additional privileges. To write apps that communicate with the Kubernetes&#10;API server (as described in chapter 8), you need to understand how to manage&#10;authorization through RBAC-specific resources.&#10;NOTE&#10;In addition to RBAC, Kubernetes also includes other authorization&#10;plugins, such as the Attribute-based access control (ABAC) plugin, a Web-&#10;Hook plugin and custom plugin implementations. RBAC is the standard,&#10;though.&#10;12.2.1 Introducing the RBAC authorization plugin&#10;The Kubernetes API server can be configured to use an authorization plugin to check&#10;whether an action is allowed to be performed by the user requesting the action. Because&#10;the API server exposes a REST interface, users perform actions by sending HTTP&#10;requests to the server. Users authenticate themselves by including credentials in the&#10;request (an authentication token, username and password, or a client certificate).&#10;UNDERSTANDING ACTIONS&#10;But what actions are there? As you know, REST clients send GET, POST, PUT, DELETE,&#10;and other types of HTTP requests to specific URL paths, which represent specific&#10;REST resources. In Kubernetes, those resources are Pods, Services, Secrets, and so on.&#10;Here are a few examples of actions in Kubernetes:&#10;&#61601;Get Pods&#10;&#61601;Create Services&#10; &#10;"
    color "green"
  ]
  node [
    id 596
    label "386"
    title "Page_386"
    color "blue"
  ]
  node [
    id 597
    label "text_297"
    title "354&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;&#61601;Update Secrets&#10;&#61601;And so on&#10;The verbs in those examples (get, create, update) map to HTTP methods (GET, POST,&#10;PUT) performed by the client (the complete mapping is shown in table 12.1). The&#10;nouns (Pods, Service, Secrets) obviously map to Kubernetes resources. &#10; An authorization plugin such as RBAC, which runs inside the API server, deter-&#10;mines whether a client is allowed to perform the requested verb on the requested&#10;resource or not.&#10;NOTE&#10;The additional verb use is used for PodSecurityPolicy resources, which&#10;are explained in the next chapter.&#10;Besides applying security permissions to whole resource types, RBAC rules can also&#10;apply to specific instances of a resource (for example, a Service called myservice).&#10;And later you&#8217;ll see that permissions can also apply to non-resource URL paths,&#10;because not every path the API server exposes maps to a resource (such as the /api&#10;path itself or the server health information at /healthz). &#10;UNDERSTANDING THE RBAC PLUGIN&#10;The RBAC authorization plugin, as the name suggests, uses user roles as the key factor&#10;in determining whether the user may perform the action or not. A subject (which may&#10;be a human, a ServiceAccount, or a group of users or ServiceAccounts) is associated&#10;with one or more roles and each role is allowed to perform certain verbs on certain&#10;resources. &#10; If a user has multiple roles, they may do anything that any of their roles allows&#10;them to do. If none of the user&#8217;s roles contains a permission to, for example, update&#10;Secrets, the API server will prevent the user from performing PUT or PATCH requests&#10;on Secrets.&#10; Managing authorization through the RBAC plugin is simple. It&#8217;s all done by creat-&#10;ing four RBAC-specific Kubernetes resources, which we&#8217;ll look at next.&#10;Table 12.1&#10;Mapping HTTP methods to authorization verbs&#10;HTTP method&#10;Verb for single resource&#10;Verb for collection&#10;GET, HEAD&#10;get (and watch for watching)&#10;list (and watch)&#10;POST&#10;create&#10;n/a&#10;PUT&#10;update&#10;n/a&#10;PATCH&#10;patch&#10;n/a&#10;DELETE&#10;delete&#10;deletecollection&#10; &#10;"
    color "green"
  ]
  node [
    id 598
    label "387"
    title "Page_387"
    color "blue"
  ]
  node [
    id 599
    label "text_298"
    title "355&#10;Securing the cluster with role-based access control&#10;12.2.2 Introducing RBAC resources&#10;The RBAC authorization rules are configured through four resources, which can be&#10;grouped into two groups:&#10;&#61601;Roles and ClusterRoles, which specify which verbs can be performed on which&#10;resources.&#10;&#61601;RoleBindings and ClusterRoleBindings, which bind the above roles to specific&#10;users, groups, or ServiceAccounts.&#10;Roles define what can be done, while bindings define who can do it (this is shown in&#10;figure 12.2).&#10;The distinction between a Role and a ClusterRole, or between a RoleBinding and a&#10;ClusterRoleBinding, is that the Role and RoleBinding are namespaced resources,&#10;whereas the ClusterRole and ClusterRoleBinding are cluster-level resources (not&#10;namespaced). This is depicted in figure 12.3.&#10; As you can see from the figure, multiple RoleBindings can exist in a single name-&#10;space (this is also true for Roles). Likewise, multiple ClusterRoleBindings and Cluster-&#10;Roles can be created. Another thing shown in the figure is that although RoleBindings&#10;are namespaced, they can also reference ClusterRoles, which aren&#8217;t. &#10; The best way to learn about these four resources and what their effects are is by try-&#10;ing them out in a hands-on exercise. You&#8217;ll do that now.&#10; &#10; &#10; &#10; &#10;What?&#10;Role&#10;Binding&#10;Some&#10;resources&#10;Other&#10;resources&#10;Role&#10;Doesn&#8217;t allow&#10;doing anything&#10;with other resources&#10;User A&#10;Who?&#10;Admins group&#10;Allows users&#10;to access&#10;Service-&#10;Account:&#10;x&#10;Figure 12.2&#10;Roles grant permissions, whereas RoleBindings bind Roles to subjects.&#10; &#10;"
    color "green"
  ]
  node [
    id 600
    label "388"
    title "Page_388"
    color "blue"
  ]
  node [
    id 601
    label "text_299"
    title "356&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;SETTING UP YOUR EXERCISE&#10;Before you can explore how RBAC resources affect what you can do through the API&#10;server, you need to make sure RBAC is enabled in your cluster. First, ensure you&#8217;re&#10;using at least version 1.6 of Kubernetes and that the RBAC plugin is the only config-&#10;ured authorization plugin. There can be multiple plugins enabled in parallel and if&#10;one of them allows an action to be performed, the action is allowed.&#10;NOTE&#10;If you&#8217;re using GKE 1.6 or 1.7, you need to explicitly disable legacy autho-&#10;rization by creating the cluster with the --no-enable-legacy-authorization&#10;option. If you&#8217;re using Minikube, you also may need to enable RBAC by start-&#10;ing Minikube with --extra-config=apiserver.Authorization.Mode=RBAC&#10;If you followed the instructions on how to disable RBAC in chapter 8, now&#8217;s the time&#10;to re-enable it by running the following command:&#10;$ kubectl delete clusterrolebinding permissive-binding&#10;To try out RBAC, you&#8217;ll run a pod through which you&#8217;ll try to talk to the API server,&#10;the way you did in chapter 8. But this time you&#8217;ll run two pods in different namespaces&#10;to see how per-namespace security behaves.&#10; In the examples in chapter 8, you ran two containers to demonstrate how an appli-&#10;cation in one container uses the other container to talk to the API server. This time,&#10;you&#8217;ll run a single container (based on the kubectl-proxy image) and use kubectl&#10;exec to run curl inside that container directly. The proxy will take care of authentica-&#10;tion and HTTPS, so you can focus on the authorization aspect of API server security.&#10;Namespace C&#10;Namespaced&#10;resources&#10;Cluster-level&#10;resources&#10;RoleBinding&#10;RoleBinding&#10;Role&#10;Namespace B&#10;Namespaced&#10;resources&#10;RoleBinding&#10;Role&#10;Namespace A&#10;Namespaced&#10;resources&#10;RoleBinding&#10;Role&#10;Cluster scope (resources that aren&#8217;t namespaced)&#10;ClusterRoleBinding&#10;ClusterRole&#10;Figure 12.3&#10;Roles and RoleBindings are namespaced; ClusterRoles and ClusterRoleBindings aren&#8217;t.&#10; &#10;"
    color "green"
  ]
  node [
    id 602
    label "389"
    title "Page_389"
    color "blue"
  ]
  node [
    id 603
    label "text_300"
    title "357&#10;Securing the cluster with role-based access control&#10;CREATING THE NAMESPACES AND RUNNING THE PODS&#10;You&#8217;re going to create one pod in namespace foo and the other one in namespace&#10;bar, as shown in the following listing.&#10;$ kubectl create ns foo&#10;namespace &#34;foo&#34; created&#10;$ kubectl run test --image=luksa/kubectl-proxy -n foo&#10;deployment &#34;test&#34; created&#10;$ kubectl create ns bar&#10;namespace &#34;bar&#34; created&#10;$ kubectl run test --image=luksa/kubectl-proxy -n bar&#10;deployment &#34;test&#34; created&#10;Now open two terminals and use kubectl exec to run a shell inside each of the two&#10;pods (one in each terminal). For example, to run the shell in the pod in namespace&#10;foo, first get the name of the pod:&#10;$ kubectl get po -n foo&#10;NAME                   READY     STATUS    RESTARTS   AGE&#10;test-145485760-ttq36   1/1       Running   0          1m&#10;Then use the name in the kubectl exec command:&#10;$ kubectl exec -it test-145485760-ttq36 -n foo sh&#10;/ #&#10;Do the same in the other terminal, but for the pod in the bar namespace.&#10;LISTING SERVICES FROM YOUR PODS&#10;To verify that RBAC is enabled and preventing the pod from reading cluster state, use&#10;curl to list Services in the foo namespace:&#10;/ # curl localhost:8001/api/v1/namespaces/foo/services&#10;User &#34;system:serviceaccount:foo:default&#34; cannot list services in the &#10;namespace &#34;foo&#34;.&#10;You&#8217;re connecting to localhost:8001, which is where the kubectl proxy process is&#10;listening (as explained in chapter 8). The process received your request and sent it to&#10;the API server while authenticating as the default ServiceAccount in the foo name-&#10;space (as evident from the API server&#8217;s response). &#10; The API server responded that the ServiceAccount isn&#8217;t allowed to list Services in&#10;the foo namespace, even though the pod is running in that same namespace. You&#8217;re&#10;seeing RBAC in action. The default permissions for a ServiceAccount don&#8217;t allow it to&#10;list or modify any resources. Now, let&#8217;s learn how to allow the ServiceAccount to do&#10;that. First, you&#8217;ll need to create a Role resource.&#10;Listing 12.7&#10;Running test pods in different namespaces&#10; &#10;"
    color "green"
  ]
  node [
    id 604
    label "390"
    title "Page_390"
    color "blue"
  ]
  node [
    id 605
    label "text_301"
    title "358&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;12.2.3 Using Roles and RoleBindings&#10;A Role resource defines what actions can be taken on which resources (or, as&#10;explained earlier, which types of HTTP requests can be performed on which RESTful&#10;resources). The following listing defines a Role, which allows users to get and list&#10;Services in the foo namespace.&#10;apiVersion: rbac.authorization.k8s.io/v1&#10;kind: Role&#10;metadata:&#10;  namespace: foo            &#10;  name: service-reader&#10;rules:&#10;- apiGroups: [&#34;&#34;]            &#10;  verbs: [&#34;get&#34;, &#34;list&#34;]     &#10;  resources: [&#34;services&#34;]   &#10;WARNING&#10;The plural form must be used when specifying resources.&#10;This Role resource will be created in the foo namespace. In chapter 8, you learned that&#10;each resource type belongs to an API group, which you specify in the apiVersion field&#10;(along with the version) in the resource&#8217;s manifest. In a Role definition, you need to spec-&#10;ify the apiGroup for the resources listed in each rule included in the definition. If you&#8217;re&#10;allowing access to resources belonging to different API groups, you use multiple rules.&#10;NOTE&#10;In the example, you&#8217;re allowing access to all Service resources, but you&#10;could also limit access only to specific Service instances by specifying their&#10;names through an additional resourceNames field.&#10;Figure 12.4 shows the Role, its verbs and resources, and the namespace it will be cre-&#10;ated in.&#10;Listing 12.8&#10;A definition of a Role: service-reader.yaml&#10;Roles are namespaced (if namespace is &#10;omitted, the current namespace is used).&#10;Services are resources in the core apiGroup, &#10;which has no name &#8211; hence the &#34;&#34;.&#10;Getting individual Services (by name) &#10;and listing all of them is allowed.&#10;This rule pertains to services &#10;(plural name must be used!).&#10;Allows getting&#10;Allows listing&#10;Services&#10;Role:&#10;service-reader&#10;Services&#10;Namespace: foo&#10;Namespace: bar&#10;Does not allow users to&#10;get or list Services in&#10;other namespaces&#10;Figure 12.4&#10;The service-reader Role allows getting and listing Services in the foo namespace.&#10; &#10;"
    color "green"
  ]
  node [
    id 606
    label "391"
    title "Page_391"
    color "blue"
  ]
  node [
    id 607
    label "text_302"
    title "359&#10;Securing the cluster with role-based access control&#10;CREATING A ROLE&#10;Create the previous Role in the foo namespace now:&#10;$ kubectl create -f service-reader.yaml -n foo&#10;role &#34;service-reader&#34; created&#10;NOTE&#10;The -n option is shorthand for --namespace.&#10;Note that if you&#8217;re using GKE, the previous command may fail because you don&#8217;t have&#10;cluster-admin rights. To grant the rights, run the following command:&#10;$ kubectl create clusterrolebinding cluster-admin-binding &#10;&#10149; --clusterrole=cluster-admin --user=your.email@address.com&#10;Instead of creating the service-reader Role from a YAML file, you could also create&#10;it with the special kubectl create role command. Let&#8217;s use this method to create the&#10;Role in the bar namespace:&#10;$ kubectl create role service-reader --verb=get --verb=list &#10;&#10149; --resource=services -n bar&#10;role &#34;service-reader&#34; created&#10;These two Roles will allow you to list Services in the foo and bar namespaces from&#10;within your two pods (running in the foo and bar namespace, respectively). But cre-&#10;ating the two Roles isn&#8217;t enough (you can check by executing the curl command&#10;again). You need to bind each of the Roles to the ServiceAccounts in their respec-&#10;tive namespaces. &#10;BINDING A ROLE TO A SERVICEACCOUNT&#10;A Role defines what actions can be performed, but it doesn&#8217;t specify who can perform&#10;them. To do that, you must bind the Role to a subject, which can be a user, a Service-&#10;Account, or a group (of users or ServiceAccounts).&#10; Binding Roles to subjects is achieved by creating a RoleBinding resource. To bind&#10;the Role to the default ServiceAccount, run the following command:&#10;$ kubectl create rolebinding test --role=service-reader &#10;&#10149; --serviceaccount=foo:default -n foo&#10;rolebinding &#34;test&#34; created&#10;The command should be self-explanatory. You&#8217;re creating a RoleBinding, which binds&#10;the service-reader Role to the default ServiceAccount in namespace foo. You&#8217;re cre-&#10;ating the RoleBinding in namespace foo. The RoleBinding and the referenced Service-&#10;Account and Role are shown in figure 12.5.&#10;NOTE&#10;To bind a Role to a user instead of a ServiceAccount, use the --user&#10;argument to specify the username. To bind it to a group, use --group.&#10; &#10;"
    color "green"
  ]
  node [
    id 608
    label "392"
    title "Page_392"
    color "blue"
  ]
  node [
    id 609
    label "text_303"
    title "360&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;The following listing shows the YAML of the RoleBinding you created.&#10;$ kubectl get rolebinding test -n foo -o yaml&#10;apiVersion: rbac.authorization.k8s.io/v1&#10;kind: RoleBinding&#10;metadata:&#10;  name: test&#10;  namespace: foo&#10;  ...&#10;roleRef:&#10;  apiGroup: rbac.authorization.k8s.io&#10;  kind: Role                         &#10;  name: service-reader               &#10;subjects:&#10;- kind: ServiceAccount       &#10;  name: default              &#10;  namespace: foo             &#10;As you can see, a RoleBinding always references a single Role (as evident from the&#10;roleRef property), but can bind the Role to multiple subjects (for example, one or&#10;more ServiceAccounts and any number of users or groups). Because this RoleBinding&#10;binds the Role to the ServiceAccount the pod in namespace foo is running under, you&#10;can now list Services from within that pod.&#10;/ # curl localhost:8001/api/v1/namespaces/foo/services&#10;{&#10;  &#34;kind&#34;: &#34;ServiceList&#34;,&#10;  &#34;apiVersion&#34;: &#34;v1&#34;,&#10;  &#34;metadata&#34;: {&#10;    &#34;selfLink&#34;: &#34;/api/v1/namespaces/foo/services&#34;,&#10;Listing 12.9&#10;A RoleBinding referencing a Role&#10;Listing 12.10&#10;Getting Services from the API server&#10;Namespace: foo&#10;Role:&#10;service-reader&#10;Get, list&#10;Default ServiceAccount&#10;is allowed to get and list&#10;services in this namespace&#10;Services&#10;RoleBinding:&#10;test&#10;Service-&#10;Account:&#10;default&#10;Figure 12.5&#10;The test RoleBinding binds the default ServiceAccount with the &#10;service-reader Role.&#10;This RoleBinding references &#10;the service-reader Role.&#10;And binds it to the &#10;default ServiceAccount &#10;in the foo namespace.&#10; &#10;"
    color "green"
  ]
  node [
    id 610
    label "393"
    title "Page_393"
    color "blue"
  ]
  node [
    id 611
    label "text_304"
    title "361&#10;Securing the cluster with role-based access control&#10;    &#34;resourceVersion&#34;: &#34;24906&#34;&#10;  },&#10;  &#34;items&#34;: []     &#10;}&#10;INCLUDING SERVICEACCOUNTS FROM OTHER NAMESPACES IN A ROLEBINDING&#10;The pod in namespace bar can&#8217;t list the Services in its own namespace, and obviously&#10;also not those in the foo namespace. But you can edit your RoleBinding in the foo&#10;namespace and add the other pod&#8217;s ServiceAccount, even though it&#8217;s in a different&#10;namespace. Run the following command:&#10;$ kubectl edit rolebinding test -n foo&#10;Then add the following lines to the list of subjects, as shown in the following listing.&#10;subjects:&#10;- kind: ServiceAccount&#10;  name: default          &#10;  namespace: bar         &#10;Now you can also list Services in the foo namespace from inside the pod running in&#10;the bar namespace. Run the same command as in listing 12.10, but do it in the other&#10;terminal, where you&#8217;re running the shell in the other pod.&#10; Before moving on to ClusterRoles and ClusterRoleBindings, let&#8217;s summarize&#10;what RBAC resources you currently have. You have a RoleBinding in namespace&#10;foo, which references the service-reader Role (also in the foo namespace) and&#10;binds the default ServiceAccounts in both the foo and the bar namespaces, as&#10;depicted in figure 12.6.&#10;Listing 12.11&#10;Referencing a ServiceAccount from another namespace&#10;The list of items is empty, &#10;because no Services exist.&#10;You&#8217;re referencing the default &#10;ServiceAccount in the bar namespace.&#10;Namespace: foo&#10;Role:&#10;service-reader&#10;Get, list&#10;Both ServiceAccounts are&#10;allowed to get and list Services&#10;in the foo namespace&#10;Services&#10;Namespace: bar&#10;RoleBinding:&#10;test&#10;Service-&#10;Account:&#10;default&#10;Service-&#10;Account:&#10;default&#10;Figure 12.6&#10;A RoleBinding binding ServiceAccounts from different namespaces to the same Role.&#10; &#10;"
    color "green"
  ]
  node [
    id 612
    label "394"
    title "Page_394"
    color "blue"
  ]
  node [
    id 613
    label "text_305"
    title "362&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;12.2.4 Using ClusterRoles and ClusterRoleBindings&#10;Roles and RoleBindings are namespaced resources, meaning they reside in and apply&#10;to resources in a single namespace, but, as we saw, RoleBindings can refer to Service-&#10;Accounts from other namespaces, too. &#10; In addition to these namespaced resources, two cluster-level RBAC resources also&#10;exist: ClusterRole and ClusterRoleBinding. They&#8217;re not namespaced. Let&#8217;s see why&#10;you need them.&#10; A regular Role only allows access to resources in the same namespace the Role is&#10;in. If you want to allow someone access to resources across different namespaces, you&#10;have to create a Role and RoleBinding in every one of those namespaces. If you want&#10;to extend this to all namespaces (this is something a cluster administrator would prob-&#10;ably need), you need to create the same Role and RoleBinding in each namespace.&#10;When creating an additional namespace, you have to remember to create the two&#10;resources there as well. &#10; As you&#8217;ve learned throughout the book, certain resources aren&#8217;t namespaced at&#10;all (this includes Nodes, PersistentVolumes, Namespaces, and so on). We&#8217;ve also&#10;mentioned the API server exposes some URL paths that don&#8217;t represent resources&#10;(/healthz for example). Regular Roles can&#8217;t grant access to those resources or non-&#10;resource URLs, but ClusterRoles can.&#10; A ClusterRole is a cluster-level resource for allowing access to non-namespaced&#10;resources or non-resource URLs or used as a common role to be bound inside individ-&#10;ual namespaces, saving you from having to redefine the same role in each of them.&#10;ALLOWING ACCESS TO CLUSTER-LEVEL RESOURCES&#10;As mentioned, a ClusterRole can be used to allow access to cluster-level resources.&#10;Let&#8217;s look at how to allow your pod to list PersistentVolumes in your cluster. First,&#10;you&#8217;ll create a ClusterRole called pv-reader:&#10;$ kubectl create clusterrole pv-reader --verb=get,list &#10;&#10149; --resource=persistentvolumes&#10;clusterrole &#34;pv-reader&#34; created&#10;The ClusterRole&#8217;s YAML is shown in the following listing.&#10;$ kubectl get clusterrole pv-reader -o yaml&#10;apiVersion: rbac.authorization.k8s.io/v1&#10;kind: ClusterRole&#10;metadata:                                       &#10;  name: pv-reader                               &#10;  resourceVersion: &#34;39932&#34;                      &#10;  selfLink: ...                                 &#10;  uid: e9ac1099-30e2-11e7-955c-080027e6b159     &#10;Listing 12.12&#10;A ClusterRole definition&#10;ClusterRoles aren&#8217;t &#10;namespaced, hence &#10;no namespace field.&#10; &#10;"
    color "green"
  ]
  node [
    id 614
    label "395"
    title "Page_395"
    color "blue"
  ]
  node [
    id 615
    label "text_306"
    title "363&#10;Securing the cluster with role-based access control&#10;rules:&#10;- apiGroups:                      &#10;  - &#34;&#34;                            &#10;  resources:                      &#10;  - persistentvolumes             &#10;  verbs:                          &#10;  - get                           &#10;  - list                          &#10;Before you bind this ClusterRole to your pod&#8217;s ServiceAccount, verify whether the pod&#10;can list PersistentVolumes. Run the following command in the first terminal, where&#10;you&#8217;re running the shell inside the pod in the foo namespace:&#10;/ # curl localhost:8001/api/v1/persistentvolumes&#10;User &#34;system:serviceaccount:foo:default&#34; cannot list persistentvolumes at the &#10;cluster scope.&#10;NOTE&#10;The URL contains no namespace, because PersistentVolumes aren&#8217;t&#10;namespaced. &#10;As expected, the default ServiceAccount can&#8217;t list PersistentVolumes. You need to&#10;bind the ClusterRole to your ServiceAccount to allow it to do that. ClusterRoles can&#10;be bound to subjects with regular RoleBindings, so you&#8217;ll create a RoleBinding now:&#10;$ kubectl create rolebinding pv-test --clusterrole=pv-reader &#10;&#10149; --serviceaccount=foo:default -n foo&#10;rolebinding &#34;pv-test&#34; created&#10;Can you list PersistentVolumes now?&#10;/ # curl localhost:8001/api/v1/persistentvolumes&#10;User &#34;system:serviceaccount:foo:default&#34; cannot list persistentvolumes at the &#10;cluster scope.&#10;Hmm, that&#8217;s strange. Let&#8217;s examine the RoleBinding&#8217;s YAML in the following listing.&#10;Can you tell what (if anything) is wrong with it?&#10;$ kubectl get rolebindings pv-test -o yaml&#10;apiVersion: rbac.authorization.k8s.io/v1&#10;kind: RoleBinding&#10;metadata:&#10;  name: pv-test&#10;  namespace: foo&#10;  ...&#10;roleRef:&#10;  apiGroup: rbac.authorization.k8s.io&#10;  kind: ClusterRole              &#10;  name: pv-reader                &#10;Listing 12.13&#10;A RoleBinding referencing a ClusterRole&#10;In this case, the &#10;rules are exactly &#10;like those in a &#10;regular Role.&#10;The binding references the &#10;pv-reader ClusterRole.&#10; &#10;"
    color "green"
  ]
  node [
    id 616
    label "396"
    title "Page_396"
    color "blue"
  ]
  node [
    id 617
    label "text_307"
    title "364&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;subjects:&#10;- kind: ServiceAccount          &#10;  name: default                 &#10;  namespace: foo                &#10;The YAML looks perfectly fine. You&#8217;re referencing the correct ClusterRole and the&#10;correct ServiceAccount, as shown in figure 12.7, so what&#8217;s wrong?&#10;Although you can create a RoleBinding and have it reference a ClusterRole when you&#10;want to enable access to namespaced resources, you can&#8217;t use the same approach for&#10;cluster-level (non-namespaced) resources. To grant access to cluster-level resources,&#10;you must always use a ClusterRoleBinding.&#10; Luckily, creating a ClusterRoleBinding isn&#8217;t that different from creating a Role-&#10;Binding, but you&#8217;ll clean up and delete the RoleBinding first:&#10;$ kubectl delete rolebinding pv-test&#10;rolebinding &#34;pv-test&#34; deleted&#10;Now create the ClusterRoleBinding:&#10;$ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader &#10;&#10149; --serviceaccount=foo:default&#10;clusterrolebinding &#34;pv-test&#34; created&#10;As you can see, you replaced rolebinding with clusterrolebinding in the command&#10;and didn&#8217;t (need to) specify the namespace. Figure 12.8 shows what you have now.&#10; Let&#8217;s see if you can list PersistentVolumes now:&#10;/ # curl localhost:8001/api/v1/persistentvolumes&#10;{&#10;  &#34;kind&#34;: &#34;PersistentVolumeList&#34;,&#10;  &#34;apiVersion&#34;: &#34;v1&#34;,&#10;...&#10;The bound subject is the &#10;default ServiceAccount in &#10;the foo namespace.&#10;Namespace: foo&#10;Cluster-level resources&#10;ClusterRole:&#10;pv-reader&#10;Get, list&#10;Persistent&#10;Volumes&#10;RoleBinding:&#10;pv-test&#10;Default ServiceAccount&#10;is unable to get and list&#10;PersistentVolumes&#10;Service-&#10;Account:&#10;default&#10;Figure 12.7&#10;A RoleBinding referencing a ClusterRole doesn&#8217;t grant access to cluster-&#10;level resources.&#10; &#10;"
    color "green"
  ]
  node [
    id 618
    label "397"
    title "Page_397"
    color "blue"
  ]
  node [
    id 619
    label "text_308"
    title "365&#10;Securing the cluster with role-based access control&#10;You can! It turns out you must use a ClusterRole and a ClusterRoleBinding when&#10;granting access to cluster-level resources.&#10;TIP&#10;Remember that a RoleBinding can&#8217;t grant access to cluster-level resources,&#10;even if it references a ClusterRoleBinding.&#10;ALLOWING ACCESS TO NON-RESOURCE URLS&#10;We&#8217;ve mentioned that the API server also exposes non-resource URLs. Access to these&#10;URLs must also be granted explicitly; otherwise the API server will reject the client&#8217;s&#10;request. Usually, this is done for you automatically through the system:discovery&#10;ClusterRole and the identically named ClusterRoleBinding, which appear among&#10;other predefined ClusterRoles and ClusterRoleBindings (we&#8217;ll explore them in sec-&#10;tion 12.2.5). &#10; Let&#8217;s inspect the system:discovery ClusterRole shown in the following listing.&#10;$ kubectl get clusterrole system:discovery -o yaml&#10;apiVersion: rbac.authorization.k8s.io/v1&#10;kind: ClusterRole&#10;metadata:&#10;  name: system:discovery&#10;  ...&#10;rules:&#10;- nonResourceURLs:      &#10;  - /api                &#10;  - /api/*              &#10;  - /apis               &#10;  - /apis/*             &#10;  - /healthz            &#10;  - /swaggerapi         &#10;  - /swaggerapi/*       &#10;  - /version            &#10;Listing 12.14&#10;The default system:discovery ClusterRole&#10;Namespace: foo&#10;Cluster-level resources&#10;ClusterRole:&#10;pv-reader&#10;Get, list&#10;Persistent&#10;Volumes&#10;ClusterRoleBinding:&#10;pv-test&#10;Default ServiceAccount in&#10;foo namespace is now allowed&#10;to get and list PersistentVolumes&#10;Service-&#10;Account:&#10;default&#10;Figure 12.8&#10;A ClusterRoleBinding and ClusterRole must be used to grant access to cluster-&#10;level resources.&#10;Instead of referring &#10;to resources, this rule &#10;refers to non-resource &#10;URLs.&#10; &#10;"
    color "green"
  ]
  node [
    id 620
    label "398"
    title "Page_398"
    color "blue"
  ]
  node [
    id 621
    label "text_309"
    title "366&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;  verbs:             &#10;  - get              &#10;You can see this ClusterRole refers to URLs instead of resources (field nonResource-&#10;URLs is used instead of the resources field). The verbs field only allows the GET HTTP&#10;method to be used on these URLs.&#10;NOTE&#10;For non-resource URLs, plain HTTP verbs such as post, put, and&#10;patch are used instead of create or update. The verbs need to be specified in&#10;lowercase.&#10;As with cluster-level resources, ClusterRoles for non-resource URLs must be bound&#10;with a ClusterRoleBinding. Binding them with a RoleBinding won&#8217;t have any effect.&#10;The system:discovery ClusterRole has a corresponding system:discovery Cluster-&#10;RoleBinding, so let&#8217;s see what&#8217;s in it by examining the following listing.&#10;$ kubectl get clusterrolebinding system:discovery -o yaml&#10;apiVersion: rbac.authorization.k8s.io/v1&#10;kind: ClusterRoleBinding&#10;metadata:&#10;  name: system:discovery&#10;  ...&#10;roleRef:&#10;  apiGroup: rbac.authorization.k8s.io&#10;  kind: ClusterRole                           &#10;  name: system:discovery                      &#10;subjects:&#10;- apiGroup: rbac.authorization.k8s.io&#10;  kind: Group                                 &#10;  name: system:authenticated                  &#10;- apiGroup: rbac.authorization.k8s.io&#10;  kind: Group                                 &#10;  name: system:unauthenticated                &#10;The YAML shows the ClusterRoleBinding refers to the system:discovery ClusterRole,&#10;as expected. It&#8217;s bound to two groups, system:authenticated and system:unauthenti-&#10;cated, which makes it bound to all users. This means absolutely everyone can access&#10;the URLs listed in the ClusterRole. &#10;NOTE&#10;Groups are in the domain of the authentication plugin. When a&#10;request is received by the API server, it calls the authentication plugin to&#10;obtain the list of groups the user belongs to. This information is then used&#10;in authorization.&#10;You can confirm this by accessing the /api URL path from inside the pod (through&#10;the kubectl proxy, which means you&#8217;ll be authenticated as the pod&#8217;s ServiceAccount)&#10;Listing 12.15&#10;The default system:discovery ClusterRoleBinding&#10;Only the HTTP GET method &#10;is allowed for these URLs.&#10;This ClusterRoleBinding references &#10;the system:discovery ClusterRole.&#10;It binds the ClusterRole &#10;to all authenticated and &#10;unauthenticated users &#10;(that is, everyone).&#10; &#10;"
    color "green"
  ]
  node [
    id 622
    label "399"
    title "Page_399"
    color "blue"
  ]
  node [
    id 623
    label "text_310"
    title "367&#10;Securing the cluster with role-based access control&#10;and from your local machine, without specifying any authentication tokens (making&#10;you an unauthenticated user):&#10;$ curl https://$(minikube ip):8443/api -k&#10;{&#10;  &#34;kind&#34;: &#34;APIVersions&#34;,&#10;  &#34;versions&#34;: [&#10;  ...&#10;You&#8217;ve now used ClusterRoles and ClusterRoleBindings to grant access to cluster-level&#10;resources and non-resource URLs. Now let&#8217;s look at how ClusterRoles can be used&#10;with namespaced RoleBindings to grant access to namespaced resources in the Role-&#10;Binding&#8217;s namespace.&#10;USING CLUSTERROLES TO GRANT ACCESS TO RESOURCES IN SPECIFIC NAMESPACES&#10;ClusterRoles don&#8217;t always need to be bound with cluster-level ClusterRoleBindings.&#10;They can also be bound with regular, namespaced RoleBindings. You&#8217;ve already&#10;started looking at predefined ClusterRoles, so let&#8217;s look at another one called view,&#10;which is shown in the following listing.&#10;$ kubectl get clusterrole view -o yaml&#10;apiVersion: rbac.authorization.k8s.io/v1&#10;kind: ClusterRole&#10;metadata:&#10;  name: view&#10;  ...&#10;rules:&#10;- apiGroups:&#10;  - &#34;&#34;&#10;  resources:                           &#10;  - configmaps                         &#10;  - endpoints                          &#10;  - persistentvolumeclaims             &#10;  - pods                               &#10;  - replicationcontrollers             &#10;  - replicationcontrollers/scale       &#10;  - serviceaccounts                    &#10;  - services                           &#10;  verbs:                &#10;  - get                 &#10;  - list                &#10;  - watch               &#10;...&#10;This ClusterRole has many rules. Only the first one is shown in the listing. The rule&#10;allows getting, listing, and watching resources like ConfigMaps, Endpoints, Persistent-&#10;VolumeClaims, and so on. These are namespaced resources, even though you&#8217;re&#10;looking at a ClusterRole (not a regular, namespaced Role). What exactly does this&#10;ClusterRole do?&#10;Listing 12.16&#10;The default view ClusterRole&#10;This rule applies to &#10;these resources (note: &#10;they&#8217;re all namespaced &#10;resources).&#10;As the ClusterRole&#8217;s name &#10;suggests, it only allows &#10;reading, not writing the &#10;resources listed. &#10; &#10;"
    color "green"
  ]
  node [
    id 624
    label "400"
    title "Page_400"
    color "blue"
  ]
  node [
    id 625
    label "text_311"
    title "368&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10; It depends whether it&#8217;s bound with a ClusterRoleBinding or a RoleBinding (it can&#10;be bound with either). If you create a ClusterRoleBinding and reference the Cluster-&#10;Role in it, the subjects listed in the binding can view the specified resources across all&#10;namespaces. If, on the other hand, you create a RoleBinding, the subjects listed in the&#10;binding can only view resources in the namespace of the RoleBinding. You&#8217;ll try both&#10;options now.&#10; You&#8217;ll see how the two options affect your test pod&#8217;s ability to list pods. First, let&#8217;s&#10;see what happens before any bindings are in place:&#10;/ # curl localhost:8001/api/v1/pods&#10;User &#34;system:serviceaccount:foo:default&#34; cannot list pods at the cluster &#10;scope./ #&#10;/ # curl localhost:8001/api/v1/namespaces/foo/pods&#10;User &#34;system:serviceaccount:foo:default&#34; cannot list pods in the namespace &#10;&#34;foo&#34;.&#10;With the first command, you&#8217;re trying to list pods across all namespaces. With the sec-&#10;ond, you&#8217;re trying to list pods in the foo namespace. The server doesn&#8217;t allow you to&#10;do either.&#10; Now, let&#8217;s see what happens when you create a ClusterRoleBinding and bind it to&#10;the pod&#8217;s ServiceAccount:&#10;$ kubectl create clusterrolebinding view-test --clusterrole=view &#10;&#10149; --serviceaccount=foo:default&#10;clusterrolebinding &#34;view-test&#34; created&#10;Can the pod now list pods in the foo namespace?&#10;/ # curl localhost:8001/api/v1/namespaces/foo/pods&#10;{&#10;  &#34;kind&#34;: &#34;PodList&#34;,&#10;  &#34;apiVersion&#34;: &#34;v1&#34;,&#10;  ...&#10;It can! Because you created a ClusterRoleBinding, it applies across all namespaces.&#10;The pod in namespace foo can list pods in the bar namespace as well:&#10;/ # curl localhost:8001/api/v1/namespaces/bar/pods&#10;{&#10;  &#34;kind&#34;: &#34;PodList&#34;,&#10;  &#34;apiVersion&#34;: &#34;v1&#34;,&#10;  ...&#10;Okay, the pod is allowed to list pods in a different namespace. It can also retrieve pods&#10;across all namespaces by hitting the /api/v1/pods URL path:&#10;/ # curl localhost:8001/api/v1/pods&#10;{&#10;  &#34;kind&#34;: &#34;PodList&#34;,&#10;  &#34;apiVersion&#34;: &#34;v1&#34;,&#10;  ...&#10; &#10;"
    color "green"
  ]
  node [
    id 626
    label "401"
    title "Page_401"
    color "blue"
  ]
  node [
    id 627
    label "text_312"
    title "369&#10;Securing the cluster with role-based access control&#10;As expected, the pod can get a list of all the pods in the cluster. To summarize, com-&#10;bining a ClusterRoleBinding with a ClusterRole referring to namespaced resources&#10;allows the pod to access namespaced resources in any namespace, as shown in fig-&#10;ure 12.9.&#10;Now, let&#8217;s see what happens if you replace the ClusterRoleBinding with a RoleBinding.&#10;First, delete the ClusterRoleBinding:&#10;$ kubectl delete clusterrolebinding view-test&#10;clusterrolebinding &#34;view-test&#34; deleted&#10;Next create a RoleBinding instead. Because a RoleBinding is namespaced, you need&#10;to specify the namespace you want to create it in. Create it in the foo namespace:&#10;$ kubectl create rolebinding view-test --clusterrole=view &#10;&#10149; --serviceaccount=foo:default -n foo&#10;rolebinding &#34;view-test&#34; created&#10;You now have a RoleBinding in the foo namespace, binding the default Service-&#10;Account in that same namespace with the view ClusterRole. What can your pod&#10;access now?&#10;/ # curl localhost:8001/api/v1/namespaces/foo/pods&#10;{&#10;  &#34;kind&#34;: &#34;PodList&#34;,&#10;  &#34;apiVersion&#34;: &#34;v1&#34;,&#10;  ...&#10;Namespace: foo&#10;Cluster-level&#10;resources&#10;Namespace: bar&#10;Pods&#10;Pods&#10;Default&#10;ServiceAccount&#10;in foo namespace&#10;is allowed to&#10;view pods in&#10;any namespace&#10;ClusterRole:&#10;view&#10;Allows getting,&#10;listing, watching&#10;ClusterRoleBinding:&#10;view-test&#10;Pods,&#10;Services,&#10;Endpoints,&#10;Con&#64257;gMaps,&#10;&#8230;&#10;Service-&#10;Account:&#10;default&#10;Figure 12.9&#10;A ClusterRoleBinding and ClusterRole grants permission to resources across all &#10;namespaces.&#10; &#10;"
    color "green"
  ]
  node [
    id 628
    label "402"
    title "Page_402"
    color "blue"
  ]
  node [
    id 629
    label "text_313"
    title "370&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;/ # curl localhost:8001/api/v1/namespaces/bar/pods&#10;User &#34;system:serviceaccount:foo:default&#34; cannot list pods in the namespace &#10;&#34;bar&#34;.&#10;/ # curl localhost:8001/api/v1/pods&#10;User &#34;system:serviceaccount:foo:default&#34; cannot list pods at the cluster &#10;scope.&#10;As you can see, your pod can list pods in the foo namespace, but not in any other spe-&#10;cific namespace or across all namespaces. This is visualized in figure 12.10.&#10;SUMMARIZING ROLE, CLUSTERROLE, ROLEBINDING, AND CLUSTERROLEBINDING COMBINATIONS&#10;We&#8217;ve covered many different combinations and it may be hard for you to remember&#10;when to use each one. Let&#8217;s see if we can make sense of all these combinations by cat-&#10;egorizing them per specific use case. Refer to table 12.2.&#10;Table 12.2&#10;When to use specific combinations of role and binding types&#10;For accessing&#10;Role type to use&#10;Binding type to use&#10;Cluster-level resources (Nodes, PersistentVolumes, ...)&#10;ClusterRole&#10;ClusterRoleBinding&#10;Non-resource URLs (/api, /healthz, ...)&#10;ClusterRole&#10;ClusterRoleBinding&#10;Namespaced resources in any namespace (and &#10;across all namespaces)&#10;ClusterRole&#10;ClusterRoleBinding&#10;Namespaced resources in a specific namespace (reus-&#10;ing the same ClusterRole in multiple namespaces)&#10;ClusterRole&#10;RoleBinding&#10;Namespaced resources in a specific namespace &#10;(Role must be defined in each namespace)&#10;Role&#10;RoleBinding&#10;Namespace: foo&#10;Cluster-level resources&#10;Namespace: bar&#10;Pods&#10;Pods&#10;ClusterRole:&#10;view&#10;Allows getting,&#10;listing, watching&#10;RoleBinding:&#10;view-test&#10;Pods,&#10;Services,&#10;Endpoints,&#10;Con&#64257;gMaps,&#10;&#8230;&#10;Default ServiceAccount in&#10;foo namespace is only allowed&#10;to view pods in namespace foo,&#10;despite using a ClusterRole&#10;Service-&#10;Account:&#10;default&#10;Figure 12.10&#10;A RoleBinding referring to a ClusterRole only grants access to resources inside the &#10;RoleBinding&#8217;s namespace.&#10; &#10;"
    color "green"
  ]
  node [
    id 630
    label "403"
    title "Page_403"
    color "blue"
  ]
  node [
    id 631
    label "text_314"
    title "371&#10;Securing the cluster with role-based access control&#10;Hopefully, the relationships between the four RBAC resources are much clearer&#10;now. Don&#8217;t worry if you still feel like you don&#8217;t yet grasp everything. Things may&#10;clear up as we explore the pre-configured ClusterRoles and ClusterRoleBindings in&#10;the next section.&#10;12.2.5 Understanding default ClusterRoles and ClusterRoleBindings&#10;Kubernetes comes with a default set of ClusterRoles and ClusterRoleBindings, which&#10;are updated every time the API server starts. This ensures all the default roles and&#10;bindings are recreated if you mistakenly delete them or if a newer version of Kuberne-&#10;tes uses a different configuration of cluster roles and bindings.&#10; You can see the default cluster roles and bindings in the following listing.&#10;$ kubectl get clusterrolebindings&#10;NAME                                           AGE&#10;cluster-admin                                  1d&#10;system:basic-user                              1d&#10;system:controller:attachdetach-controller      1d&#10;...&#10;system:controller:ttl-controller               1d&#10;system:discovery                               1d&#10;system:kube-controller-manager                 1d&#10;system:kube-dns                                1d&#10;system:kube-scheduler                          1d&#10;system:node                                    1d&#10;system:node-proxier                            1d&#10;$ kubectl get clusterroles&#10;NAME                                           AGE&#10;admin                                          1d&#10;cluster-admin                                  1d&#10;edit                                           1d&#10;system:auth-delegator                          1d&#10;system:basic-user                              1d&#10;system:controller:attachdetach-controller      1d&#10;...&#10;system:controller:ttl-controller               1d&#10;system:discovery                               1d&#10;system:heapster                                1d&#10;system:kube-aggregator                         1d&#10;system:kube-controller-manager                 1d&#10;system:kube-dns                                1d&#10;system:kube-scheduler                          1d&#10;system:node                                    1d&#10;system:node-bootstrapper                       1d&#10;system:node-problem-detector                   1d&#10;system:node-proxier                            1d&#10;system:persistent-volume-provisioner           1d&#10;view                                           1d&#10;Listing 12.17&#10;Listing all ClusterRoleBindings and ClusterRoles&#10; &#10;"
    color "green"
  ]
  node [
    id 632
    label "404"
    title "Page_404"
    color "blue"
  ]
  node [
    id 633
    label "text_315"
    title "372&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;The most important roles are the view, edit, admin, and cluster-admin ClusterRoles.&#10;They&#8217;re meant to be bound to ServiceAccounts used by user-defined pods.&#10;ALLOWING READ-ONLY ACCESS TO RESOURCES WITH THE VIEW CLUSTERROLE&#10;You already used the default view ClusterRole in the previous example. It allows read-&#10;ing most resources in a namespace, except for Roles, RoleBindings, and Secrets. You&#8217;re&#10;probably wondering, why not Secrets? Because one of those Secrets might include an&#10;authentication token with greater privileges than those defined in the view Cluster-&#10;Role and could allow the user to masquerade as a different user to gain additional&#10;privileges (privilege escalation). &#10;ALLOWING MODIFYING RESOURCES WITH THE EDIT CLUSTERROLE&#10;Next is the edit ClusterRole, which allows you to modify resources in a namespace,&#10;but also allows both reading and modifying Secrets. It doesn&#8217;t, however, allow viewing&#10;or modifying Roles or RoleBindings&#8212;again, this is to prevent privilege escalation.&#10;GRANTING FULL CONTROL OF A NAMESPACE WITH THE ADMIN CLUSTERROLE&#10;Complete control of the resources in a namespace is granted in the admin Cluster-&#10;Role. Subjects with this ClusterRole can read and modify any resource in the name-&#10;space, except ResourceQuotas (we&#8217;ll learn what those are in chapter 14) and the&#10;Namespace resource itself. The main difference between the edit and the admin Cluster-&#10;Roles is in the ability to view and modify Roles and RoleBindings in the namespace.&#10;NOTE&#10;To prevent privilege escalation, the API server only allows users to cre-&#10;ate and update Roles if they already have all the permissions listed in that&#10;Role (and for the same scope). &#10;ALLOWING COMPLETE CONTROL WITH THE CLUSTER-ADMIN CLUSTERROLE &#10;Complete control of the Kubernetes cluster can be given by assigning the cluster-&#10;admin ClusterRole to a subject. As you&#8217;ve seen before, the admin ClusterRole doesn&#8217;t&#10;allow users to modify the namespace&#8217;s ResourceQuota objects or the Namespace&#10;resource itself. If you want to allow a user to do that, you need to create a RoleBinding&#10;that references the cluster-admin ClusterRole. This gives the user included in the&#10;RoleBinding complete control over all aspects of the namespace in which the Role-&#10;Binding is created.&#10; If you&#8217;ve paid attention, you probably already know how to give users complete&#10;control of all the namespaces in the cluster. Yes, by referencing the cluster-admin&#10;ClusterRole in a ClusterRoleBinding instead of a RoleBinding.&#10;UNDERSTANDING THE OTHER DEFAULT CLUSTERROLES&#10;The list of default ClusterRoles includes a large number of other ClusterRoles, which&#10;start with the system: prefix. These are meant to be used by the various Kubernetes&#10;components. Among them, you&#8217;ll find roles such as system:kube-scheduler, which&#10;is obviously used by the Scheduler, system:node, which is used by the Kubelets, and&#10;so on. &#10; &#10;"
    color "green"
  ]
  node [
    id 634
    label "405"
    title "Page_405"
    color "blue"
  ]
  node [
    id 635
    label "text_316"
    title "373&#10;Summary&#10; Although the Controller Manager runs as a single pod, each controller running&#10;inside it can use a separate ClusterRole and ClusterRoleBinding (they&#8217;re prefixed&#10;with system: controller:). &#10; Each of these system ClusterRoles has a matching ClusterRoleBinding, which binds&#10;it to the user the system component authenticates as. The system:kube-scheduler&#10;ClusterRoleBinding, for example, assigns the identically named ClusterRole to the&#10;system:kube-scheduler user, which is the username the scheduler Authenticates as. &#10;12.2.6 Granting authorization permissions wisely&#10;By default, the default ServiceAccount in a namespace has no permissions other than&#10;those of an unauthenticated user (as you may remember from one of the previous&#10;examples, the system:discovery ClusterRole and associated binding allow anyone to&#10;make GET requests on a few non-resource URLs). Therefore, pods, by default, can&#8217;t&#10;even view cluster state. It&#8217;s up to you to grant them appropriate permissions to do that. &#10; Obviously, giving all your ServiceAccounts the cluster-admin ClusterRole is a&#10;bad idea. As is always the case with security, it&#8217;s best to give everyone only the permis-&#10;sions they need to do their job and not a single permission more (principle of least&#10;privilege).&#10;CREATING SPECIFIC SERVICEACCOUNTS FOR EACH POD&#10;It&#8217;s a good idea to create a specific ServiceAccount for each pod (or a set of pod rep-&#10;licas) and then associate it with a tailor-made Role (or a ClusterRole) through a&#10;RoleBinding (not a ClusterRoleBinding, because that would give the pod access to&#10;resources in other namespaces, which is probably not what you want). &#10; If one of your pods (the application running within it) only needs to read pods,&#10;while the other also needs to modify them, then create two different ServiceAccounts&#10;and make those pods use them by specifying the serviceAccountName property in the&#10;pod spec, as you learned in the first part of this chapter. Don&#8217;t add all the necessary&#10;permissions required by both pods to the default ServiceAccount in the namespace. &#10;EXPECTING YOUR APPS TO BE COMPROMISED&#10;Your aim is to reduce the possibility of an intruder getting hold of your cluster. Today&#8217;s&#10;complex apps contain many vulnerabilities. You should expect unwanted persons to&#10;eventually get their hands on the ServiceAccount&#8217;s authentication token, so you should&#10;always constrain the ServiceAccount to prevent them from doing any real damage.&#10;12.3&#10;Summary&#10;This chapter has given you a foundation on how to secure the Kubernetes API server.&#10;You learned the following:&#10;&#61601;Clients of the API server include both human users and applications running&#10;in pods.&#10;&#61601;Applications in pods are associated with a ServiceAccount. &#10;&#61601;Both users and ServiceAccounts are associated with groups.&#10; &#10;"
    color "green"
  ]
  node [
    id 636
    label "406"
    title "Page_406"
    color "blue"
  ]
  node [
    id 637
    label "text_317"
    title "374&#10;CHAPTER 12&#10;Securing the Kubernetes API server&#10;&#61601;By default, pods run under the default ServiceAccount, which is created for&#10;each namespace automatically.&#10;&#61601;Additional ServiceAccounts can be created manually and associated with a pod.&#10;&#61601;ServiceAccounts can be configured to allow mounting only a constrained list of&#10;Secrets in a given pod.&#10;&#61601;A ServiceAccount can also be used to attach image pull Secrets to pods, so you&#10;don&#8217;t need to specify the Secrets in every pod.&#10;&#61601;Roles and ClusterRoles define what actions can be performed on which resources.&#10;&#61601;RoleBindings and ClusterRoleBindings bind Roles and ClusterRoles to users,&#10;groups, and ServiceAccounts.&#10;&#61601;Each cluster comes with default ClusterRoles and ClusterRoleBindings.&#10;In the next chapter, you&#8217;ll learn how to protect the cluster nodes from pods and how&#10;to isolate pods from each other by securing the network.&#10; &#10;"
    color "green"
  ]
  node [
    id 638
    label "407"
    title "Page_407"
    color "blue"
  ]
  node [
    id 639
    label "text_318"
    title "375&#10;Securing cluster nodes&#10;and the network&#10;In the previous chapter, we talked about securing the API server. If an attacker&#10;gets access to the API server, they can run whatever they like by packaging their&#10;code into a container image and running it in a pod. But can they do any real&#10;damage? Aren&#8217;t containers isolated from other containers and from the node&#10;they&#8217;re running on? &#10; Not necessarily. In this chapter, you&#8217;ll learn how to allow pods to access the&#10;resources of the node they&#8217;re running on. You&#8217;ll also learn how to configure the&#10;cluster so users aren&#8217;t able to do whatever they want with their pods. Then, in&#10;This chapter covers&#10;&#61601;Using the node&#8217;s default Linux namespaces &#10;in pods&#10;&#61601;Running containers as different users&#10;&#61601;Running privileged containers&#10;&#61601;Adding or dropping a container&#8217;s kernel &#10;capabilities&#10;&#61601;Defining security policies to limit what pods can do&#10;&#61601;Securing the pod network&#10; &#10;"
    color "green"
  ]
  node [
    id 640
    label "408"
    title "Page_408"
    color "blue"
  ]
  node [
    id 641
    label "text_319"
    title "376&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;the last part of the chapter, you&#8217;ll also learn how to secure the network the pods use&#10;to communicate.&#10;13.1&#10;Using the host node&#8217;s namespaces in a pod&#10;Containers in a pod usually run under separate Linux namespaces, which isolate&#10;their processes from processes running in other containers or in the node&#8217;s default&#10;namespaces. &#10; For example, we learned that each pod gets its own IP and port space, because it&#10;uses its own network namespace. Likewise, each pod has its own process tree, because&#10;it has its own PID namespace, and it also uses its own IPC namespace, allowing only&#10;processes in the same pod to communicate with each other through the Inter-Process&#10;Communication mechanism (IPC).&#10;13.1.1 Using the node&#8217;s network namespace in a pod&#10;Certain pods (usually system pods) need to operate in the host&#8217;s default namespaces,&#10;allowing them to see and manipulate node-level resources and devices. For example, a&#10;pod may need to use the node&#8217;s network adapters instead of its own virtual network&#10;adapters. This can be achieved by setting the hostNetwork property in the pod spec&#10;to true.&#10; In that case, the pod gets to use the node&#8217;s network interfaces instead of having its&#10;own set, as shown in figure 13.1. This means the pod doesn&#8217;t get its own IP address and&#10;if it runs a process that binds to a port, the process will be bound to the node&#8217;s port.&#10;You can try running such a pod. The next listing shows an example pod manifest.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-with-host-network&#10;Listing 13.1&#10;A pod using the node&#8217;s network namespace: pod-with-host-network.yaml&#10;Node&#10;Pod A&#10;Pod&#8217;s own network&#10;namespace&#10;eth0&#10;lo&#10;eth0&#10;docker0&#10;lo&#10;eth1&#10;Node&#8217;s default network&#10;namespace&#10;Pod B&#10;hostNetwork: true&#10;Figure 13.1&#10;A pod &#10;with hostNetwork: &#10;true uses the node&#8217;s &#10;network interfaces &#10;instead of its own.&#10; &#10;"
    color "green"
  ]
  node [
    id 642
    label "409"
    title "Page_409"
    color "blue"
  ]
  node [
    id 643
    label "text_320"
    title "377&#10;Using the host node&#8217;s namespaces in a pod&#10;spec:&#10;  hostNetwork: true              &#10;  containers:&#10;  - name: main&#10;    image: alpine&#10;    command: [&#34;/bin/sleep&#34;, &#34;999999&#34;]&#10;After you run the pod, you can use the following command to see that it&#8217;s indeed using&#10;the host&#8217;s network namespace (it sees all the host&#8217;s network adapters, for example).&#10;$ kubectl exec pod-with-host-network ifconfig&#10;docker0   Link encap:Ethernet  HWaddr 02:42:14:08:23:47&#10;          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0&#10;          ...&#10;eth0      Link encap:Ethernet  HWaddr 08:00:27:F8:FA:4E&#10;          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0&#10;          ...&#10;lo        Link encap:Local Loopback&#10;          inet addr:127.0.0.1  Mask:255.0.0.0&#10;          ...&#10;veth1178d4f Link encap:Ethernet  HWaddr 1E:03:8D:D6:E1:2C&#10;          inet6 addr: fe80::1c03:8dff:fed6:e12c/64 Scope:Link&#10;          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1&#10;...&#10;When the Kubernetes Control Plane components are deployed as pods (such as when&#10;you deploy your cluster with kubeadm, as explained in appendix B), you&#8217;ll find that&#10;those pods use the hostNetwork option, effectively making them behave as if they&#10;weren&#8217;t running inside a pod.&#10;13.1.2 Binding to a host port without using the host&#8217;s network &#10;namespace&#10;A related feature allows pods to bind to a port in the node&#8217;s default namespace, but&#10;still have their own network namespace. This is done by using the hostPort property&#10;in one of the container&#8217;s ports defined in the spec.containers.ports field.&#10; Don&#8217;t confuse pods using hostPort with pods exposed through a NodePort service.&#10;They&#8217;re two different things, as explained in figure 13.2.&#10; The first thing you&#8217;ll notice in the figure is that when a pod is using a hostPort, a&#10;connection to the node&#8217;s port is forwarded directly to the pod running on that node,&#10;whereas with a NodePort service, a connection to the node&#8217;s port is forwarded to a&#10;randomly selected pod (possibly on another node). The other difference is that with&#10;pods using a hostPort, the node&#8217;s port is only bound on nodes that run such pods,&#10;whereas NodePort services bind the port on all nodes, even on those that don&#8217;t run&#10;such a pod (as on node 3 in the figure).&#10;Listing 13.2&#10;Network interfaces in a pod using the host&#8217;s network namespace&#10;Using the host node&#8217;s &#10;network namespace&#10; &#10;"
    color "green"
  ]
  node [
    id 644
    label "410"
    title "Page_410"
    color "blue"
  ]
  node [
    id 645
    label "text_321"
    title "378&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;It&#8217;s important to understand that if a pod is using a specific host port, only one&#10;instance of the pod can be scheduled to each node, because two processes can&#8217;t bind&#10;to the same host port. The Scheduler takes this into account when scheduling pods, so&#10;it doesn&#8217;t schedule multiple pods to the same node, as shown in figure 13.3. If you&#10;have three nodes and want to deploy four pod replicas, only three will be scheduled&#10;(one pod will remain Pending).&#10;Node 1&#10;Pod 1&#10;Two pods using&#10;hostPort&#10;Port&#10;8080&#10;Port&#10;9000&#10;Node 2&#10;Pod 2&#10;Port&#10;8080&#10;Port&#10;9000&#10;Node 3&#10;Node 1&#10;Pod 1&#10;Two pods under&#10;the same&#10;NodePort&#10;service&#10;Port&#10;8080&#10;Node 2&#10;Pod 2&#10;Port&#10;8080&#10;Node 3&#10;Port&#10;88&#10;Port&#10;88&#10;Port&#10;88&#10;Service&#10;(&#10;)&#10;iptables&#10;Service&#10;(&#10;)&#10;iptables&#10;Service&#10;(&#10;)&#10;iptables&#10;Figure 13.2&#10;Difference between pods using a hostPort and pods behind a NodePort service.&#10;Node 1&#10;Pod 1&#10;Port&#10;8080&#10;Host&#10;port&#10;9000&#10;Host&#10;port&#10;9000&#10;Pod 2&#10;Port&#10;8080&#10;Node 2&#10;Pod 3&#10;Port&#10;8080&#10;Host&#10;port&#10;9000&#10;Node 3&#10;Pod 4&#10;Port&#10;8080&#10;Cannot be scheduled to the same&#10;node, because the port is already bound&#10;Only a single&#10;replica per node&#10;Figure 13.3&#10;If a host port is used, only a single pod instance can be scheduled to a node.&#10; &#10;"
    color "green"
  ]
  node [
    id 646
    label "411"
    title "Page_411"
    color "blue"
  ]
  node [
    id 647
    label "text_322"
    title "379&#10;Using the host node&#8217;s namespaces in a pod&#10;Let&#8217;s see how to define the hostPort in a pod&#8217;s YAML definition. The following listing&#10;shows the YAML to run your kubia pod and bind it to the node&#8217;s port 9000.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: kubia-hostport&#10;spec:&#10;  containers:&#10;  - image: luksa/kubia&#10;    name: kubia&#10;    ports:&#10;    - containerPort: 8080    &#10;      hostPort: 9000        &#10;      protocol: TCP&#10;After you create this pod, you can access it through port 9000 of the node it&#8217;s sched-&#10;uled to. If you have multiple nodes, you&#8217;ll see you can&#8217;t access the pod through that&#10;port on the other nodes. &#10;NOTE&#10;If you&#8217;re trying this on GKE, you need to configure the firewall prop-&#10;erly using gcloud compute firewall-rules, the way you did in chapter 5.&#10;The hostPort feature is primarily used for exposing system services, which are&#10;deployed to every node using DaemonSets. Initially, people also used it to ensure two&#10;replicas of the same pod were never scheduled to the same node, but now you have a&#10;better way of achieving this&#8212;it&#8217;s explained in chapter 16.&#10;13.1.3 Using the node&#8217;s PID and IPC namespaces&#10;Similar to the hostNetwork option are the hostPID and hostIPC pod spec properties.&#10;When you set them to true, the pod&#8217;s containers will use the node&#8217;s PID and IPC&#10;namespaces, allowing processes running in the containers to see all the other pro-&#10;cesses on the node or communicate with them through IPC, respectively. See the fol-&#10;lowing listing for an example.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-with-host-pid-and-ipc&#10;spec:&#10;  hostPID: true                    &#10;  hostIPC: true                     &#10;  containers:&#10;  - name: main&#10;    image: alpine&#10;    command: [&#34;/bin/sleep&#34;, &#34;999999&#34;]&#10;Listing 13.3&#10;Binding a pod to a port in the node&#8217;s port space: kubia-hostport.yaml&#10;Listing 13.4&#10;Using the host&#8217;s PID and IPC namespaces: pod-with-host-pid-and-ipc.yaml&#10;The container can be &#10;reached on port 8080 &#10;of the pod&#8217;s IP.&#10;It can also be reached &#10;on port 9000 of the &#10;node it&#8217;s deployed on.&#10;You want the pod to &#10;use the host&#8217;s PID &#10;namespace.&#10;You also want the &#10;pod to use the host&#8217;s &#10;IPC namespace.&#10; &#10;"
    color "green"
  ]
  node [
    id 648
    label "412"
    title "Page_412"
    color "blue"
  ]
  node [
    id 649
    label "text_323"
    title "380&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;You&#8217;ll remember that pods usually see only their own processes, but if you run this pod&#10;and then list the processes from within its container, you&#8217;ll see all the processes run-&#10;ning on the host node, not only the ones running in the container, as shown in the&#10;following listing.&#10;$ kubectl exec pod-with-host-pid-and-ipc ps aux&#10;PID   USER     TIME   COMMAND&#10;    1 root       0:01 /usr/lib/systemd/systemd --switched-root --system ...&#10;    2 root       0:00 [kthreadd]&#10;    3 root       0:00 [ksoftirqd/0]&#10;    5 root       0:00 [kworker/0:0H]&#10;    6 root       0:00 [kworker/u2:0]&#10;    7 root       0:00 [migration/0]&#10;    8 root       0:00 [rcu_bh]&#10;    9 root       0:00 [rcu_sched]&#10;   10 root       0:00 [watchdog/0]&#10;...&#10;By setting the hostIPC property to true, processes in the pod&#8217;s containers can also&#10;communicate with all the other processes running on the node, through Inter-Process&#10;Communication.&#10;13.2&#10;Configuring the container&#8217;s security context&#10;Besides allowing the pod to use the host&#8217;s Linux namespaces, other security-related&#10;features can also be configured on the pod and its container through the security-&#10;Context properties, which can be specified under the pod spec directly and inside the&#10;spec of individual containers.&#10;UNDERSTANDING WHAT&#8217;S CONFIGURABLE IN THE SECURITY CONTEXT&#10;Configuring the security context allows you to do various things:&#10;&#61601;Specify the user (the user&#8217;s ID) under which the process in the container will run.&#10;&#61601;Prevent the container from running as root (the default user a container runs&#10;as is usually defined in the container image itself, so you may want to prevent&#10;containers from running as root).&#10;&#61601;Run the container in privileged mode, giving it full access to the node&#8217;s kernel.&#10;&#61601;Configure fine-grained privileges, by adding or dropping capabilities&#8212;in con-&#10;trast to giving the container all possible permissions by running it in privi-&#10;leged mode.&#10;&#61601;Set SELinux (Security Enhanced Linux) options to strongly lock down a&#10;container.&#10;&#61601;Prevent the process from writing to the container&#8217;s filesystem.&#10;We&#8217;ll explore these options next. &#10;Listing 13.5&#10;Processes visible in a pod with hostPID: true&#10; &#10;"
    color "green"
  ]
  node [
    id 650
    label "413"
    title "Page_413"
    color "blue"
  ]
  node [
    id 651
    label "text_324"
    title "381&#10;Configuring the container&#8217;s security context&#10;RUNNING A POD WITHOUT SPECIFYING A SECURITY CONTEXT&#10;First, run a pod with the default security context options (by not specifying them at&#10;all), so you can see how it behaves compared to pods with a custom security context:&#10;$ kubectl run pod-with-defaults --image alpine --restart Never &#10;&#10149;  -- /bin/sleep 999999&#10;pod &#34;pod-with-defaults&#34; created&#10;Let&#8217;s see what user and group ID the container is running as, and which groups it&#10;belongs to. You can see this by running the id command inside the container:&#10;$ kubectl exec pod-with-defaults id&#10;uid=0(root) gid=0(root) groups=0(root), 1(bin), 2(daemon), 3(sys), 4(adm), &#10;6(disk), 10(wheel), 11(floppy), 20(dialout), 26(tape), 27(video)&#10;The container is running as user ID (uid) 0, which is root, and group ID (gid) 0 (also&#10;root). It&#8217;s also a member of multiple other groups. &#10;NOTE&#10;What user the container runs as is specified in the container image. In&#10;a Dockerfile, this is done using the USER directive. If omitted, the container&#10;runs as root.&#10;Now, you&#8217;ll run a pod where the container runs as a different user.&#10;13.2.1 Running a container as a specific user&#10;To run a pod under a different user ID than the one that&#8217;s baked into the container&#10;image, you&#8217;ll need to set the pod&#8217;s securityContext.runAsUser property. You&#8217;ll&#10;make the container run as user guest, whose user ID in the alpine container image is&#10;405, as shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-as-user-guest&#10;spec:&#10;  containers:&#10;  - name: main&#10;    image: alpine&#10;    command: [&#34;/bin/sleep&#34;, &#34;999999&#34;]&#10;    securityContext:&#10;      runAsUser: 405      &#10;Now, to see the effect of the runAsUser property, run the id command in this new&#10;pod, the way you did before:&#10;$ kubectl exec pod-as-user-guest id&#10;uid=405(guest) gid=100(users)&#10;Listing 13.6&#10;Running containers as a specific user: pod-as-user-guest.yaml&#10;You need to specify a user ID, not &#10;a username (id 405 corresponds &#10;to the guest user).&#10; &#10;"
    color "green"
  ]
  node [
    id 652
    label "414"
    title "Page_414"
    color "blue"
  ]
  node [
    id 653
    label "text_325"
    title "382&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;As requested, the container is running as the guest user. &#10;13.2.2 Preventing a container from running as root&#10;What if you don&#8217;t care what user the container runs as, but you still want to prevent it&#10;from running as root? &#10; Imagine having a pod deployed with a container image that was built with a USER&#10;daemon directive in the Dockerfile, which makes the container run under the daemon&#10;user. What if an attacker gets access to your image registry and pushes a different&#10;image under the same tag? The attacker&#8217;s image is configured to run as the root user.&#10;When Kubernetes schedules a new instance of your pod, the Kubelet will download&#10;the attacker&#8217;s image and run whatever code they put into it. &#10; Although containers are mostly isolated from the host system, running their pro-&#10;cesses as root is still considered a bad practice. For example, when a host directory is&#10;mounted into the container, if the process running in the container is running as&#10;root, it has full access to the mounted directory, whereas if it&#8217;s running as non-root,&#10;it won&#8217;t. &#10; To prevent the attack scenario described previously, you can specify that the pod&#8217;s&#10;container needs to run as a non-root user, as shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-run-as-non-root&#10;spec:&#10;  containers:&#10;  - name: main&#10;    image: alpine&#10;    command: [&#34;/bin/sleep&#34;, &#34;999999&#34;]&#10;    securityContext:                   &#10;      runAsNonRoot: true               &#10;If you deploy this pod, it gets scheduled, but is not allowed to run:&#10;$ kubectl get po pod-run-as-non-root&#10;NAME                 READY  STATUS                                                  &#10;pod-run-as-non-root  0/1    container has runAsNonRoot and image will run &#10;                            &#10149;  as root&#10;Now, if anyone tampers with your container images, they won&#8217;t get far.&#10;13.2.3 Running pods in privileged mode&#10;Sometimes pods need to do everything that the node they&#8217;re running on can do, such&#10;as use protected system devices or other kernel features, which aren&#8217;t accessible to&#10;regular containers. &#10;Listing 13.7&#10;Preventing containers from running as root: pod-run-as-non-root.yaml&#10;This container will only &#10;be allowed to run as a &#10;non-root user.&#10; &#10;"
    color "green"
  ]
  node [
    id 654
    label "415"
    title "Page_415"
    color "blue"
  ]
  node [
    id 655
    label "text_326"
    title "383&#10;Configuring the container&#8217;s security context&#10; An example of such a pod is the kube-proxy pod, which needs to modify the node&#8217;s&#10;iptables rules to make services work, as was explained in chapter 11. If you follow the&#10;instructions in appendix B and deploy a cluster with kubeadm, you&#8217;ll see every cluster&#10;node runs a kube-proxy pod and you can examine its YAML specification to see all the&#10;special features it&#8217;s using. &#10; To get full access to the node&#8217;s kernel, the pod&#8217;s container runs in privileged&#10;mode. This is achieved by setting the privileged property in the container&#8217;s security-&#10;Context property to true. You&#8217;ll create a privileged pod from the YAML in the follow-&#10;ing listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-privileged&#10;spec:&#10;  containers:&#10;  - name: main&#10;    image: alpine&#10;    command: [&#34;/bin/sleep&#34;, &#34;999999&#34;]&#10;    securityContext:&#10;      privileged: true     &#10;Go ahead and deploy this pod, so you can compare it with the non-privileged pod you&#10;ran earlier. &#10; If you&#8217;re familiar with Linux, you may know it has a special file directory called /dev,&#10;which contains device files for all the devices on the system. These aren&#8217;t regular files on&#10;disk, but are special files used to communicate with devices. Let&#8217;s see what devices are&#10;visible in the non-privileged container you deployed earlier (the pod-with-defaults&#10;pod), by listing files in its /dev directory, as shown in the following listing.&#10;$ kubectl exec -it pod-with-defaults ls /dev&#10;core             null             stderr           urandom&#10;fd               ptmx             stdin            zero&#10;full             pts              stdout&#10;fuse             random           termination-log&#10;mqueue           shm              tty&#10;The listing shows all the devices. The list is fairly short. Now, compare this with the fol-&#10;lowing listing, which shows the device files your privileged pod can see.&#10;$ kubectl exec -it pod-privileged ls /dev&#10;autofs              snd                 tty46&#10;bsg                 sr0                 tty47&#10;Listing 13.8&#10;A pod with a privileged container: pod-privileged.yaml&#10;Listing 13.9&#10;List of available devices in a non-privileged pod&#10;Listing 13.10&#10;List of available devices in a privileged pod&#10;This container will &#10;run in privileged &#10;mode&#10; &#10;"
    color "green"
  ]
  node [
    id 656
    label "416"
    title "Page_416"
    color "blue"
  ]
  node [
    id 657
    label "text_327"
    title "384&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;btrfs-control       stderr              tty48&#10;core                stdin               tty49&#10;cpu                 stdout              tty5&#10;cpu_dma_latency     termination-log     tty50&#10;fd                  tty                 tty51&#10;full                tty0                tty52&#10;fuse                tty1                tty53&#10;hpet                tty10               tty54&#10;hwrng               tty11               tty55&#10;...                 ...                 ...&#10;I haven&#8217;t included the whole list, because it&#8217;s too long for the book, but it&#8217;s evident&#10;that the device list is much longer than before. In fact, the privileged container sees&#10;all the host node&#8217;s devices. This means it can use any device freely. &#10; For example, I had to use privileged mode like this when I wanted a pod running&#10;on a Raspberry Pi to control LEDs connected it.&#10;13.2.4 Adding individual kernel capabilities to a container&#10;In the previous section, you saw one way of giving a container unlimited power. In the&#10;old days, traditional UNIX implementations only distinguished between privileged&#10;and unprivileged processes, but for many years, Linux has supported a much more&#10;fine-grained permission system through kernel capabilities.&#10; Instead of making a container privileged and giving it unlimited permissions, a&#10;much safer method (from a security perspective) is to give it access only to the kernel&#10;features it really requires. Kubernetes allows you to add capabilities to each container&#10;or drop part of them, which allows you to fine-tune the container&#8217;s permissions and&#10;limit the impact of a potential intrusion by an attacker.&#10; For example, a container usually isn&#8217;t allowed to change the system time (the hard-&#10;ware clock&#8217;s time). You can confirm this by trying to set the time in your pod-with-&#10;defaults pod:&#10;$ kubectl exec -it pod-with-defaults -- date +%T -s &#34;12:00:00&#34;&#10;date: can't set date: Operation not permitted&#10;If you want to allow the container to change the system time, you can add a capabil-&#10;ity called CAP_SYS_TIME to the container&#8217;s capabilities list, as shown in the follow-&#10;ing listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-add-settime-capability&#10;spec:&#10;  containers:&#10;  - name: main&#10;    image: alpine&#10;Listing 13.11&#10;Adding the CAP_SYS_TIME capability: pod-add-settime-capability.yaml&#10; &#10;"
    color "green"
  ]
  node [
    id 658
    label "417"
    title "Page_417"
    color "blue"
  ]
  node [
    id 659
    label "text_328"
    title "385&#10;Configuring the container&#8217;s security context&#10;    command: [&#34;/bin/sleep&#34;, &#34;999999&#34;]&#10;    securityContext:                     &#10;      capabilities:                      &#10;        add:                  &#10;        - SYS_TIME            &#10;NOTE&#10;Linux kernel capabilities are usually prefixed with CAP_. But when&#10;specifying them in a pod spec, you must leave out the prefix.&#10;If you run the same command in this new pod&#8217;s container, the system time is changed&#10;successfully:&#10;$ kubectl exec -it pod-add-settime-capability -- date +%T -s &#34;12:00:00&#34;&#10;12:00:00&#10;$ kubectl exec -it pod-add-settime-capability -- date&#10;Sun May  7 12:00:03 UTC 2017&#10;WARNING&#10;If you try this yourself, be aware that it may cause your worker&#10;node to become unusable. In Minikube, although the system time was auto-&#10;matically reset back by the Network Time Protocol (NTP) daemon, I had to&#10;reboot the VM to schedule new pods. &#10;You can confirm the node&#8217;s time has been changed by checking the time on the node&#10;running the pod. In my case, I&#8217;m using Minikube, so I have only one node and I can&#10;get its time like this:&#10;$ minikube ssh date&#10;Sun May  7 12:00:07 UTC 2017&#10;Adding capabilities like this is a much better way than giving a container full privileges&#10;with privileged: true. Admittedly, it does require you to know and understand what&#10;each capability does.&#10;TIP&#10;You&#8217;ll find the list of Linux kernel capabilities in the Linux man pages.&#10;13.2.5 Dropping capabilities from a container&#10;You&#8217;ve seen how to add capabilities, but you can also drop capabilities that may oth-&#10;erwise be available to the container. For example, the default capabilities given to a&#10;container include the CAP_CHOWN capability, which allows processes to change the&#10;ownership of files in the filesystem. &#10; You can see that&#8217;s the case by changing the ownership of the /tmp directory in&#10;your pod-with-defaults pod to the guest user, for example:&#10;$ kubectl exec pod-with-defaults chown guest /tmp&#10;$ kubectl exec pod-with-defaults -- ls -la / | grep tmp&#10;drwxrwxrwt    2 guest    root             6 May 25 15:18 tmp&#10;Capabilities are added or dropped &#10;under the securityContext property.&#10;You&#8217;re adding the &#10;SYS_TIME capability.&#10; &#10;"
    color "green"
  ]
  node [
    id 660
    label "418"
    title "Page_418"
    color "blue"
  ]
  node [
    id 661
    label "text_329"
    title "386&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;To prevent the container from doing that, you need to drop the capability by listing it&#10;under the container&#8217;s securityContext.capabilities.drop property, as shown in&#10;the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-drop-chown-capability&#10;spec:&#10;  containers:&#10;  - name: main&#10;    image: alpine&#10;    command: [&#34;/bin/sleep&#34;, &#34;999999&#34;]&#10;    securityContext:&#10;      capabilities:&#10;        drop:                   &#10;        - CHOWN                 &#10;By dropping the CHOWN capability, you&#8217;re not allowed to change the owner of the /tmp&#10;directory in this pod:&#10;$ kubectl exec pod-drop-chown-capability chown guest /tmp&#10;chown: /tmp: Operation not permitted&#10;You&#8217;re almost done exploring the container&#8217;s security context options. Let&#8217;s look at&#10;one more.&#10;13.2.6 Preventing processes from writing to the container&#8217;s filesystem&#10;You may want to prevent the processes running in the container from writing to the&#10;container&#8217;s filesystem, and only allow them to write to mounted volumes. You&#8217;d want&#10;to do that mostly for security reasons. &#10; Let&#8217;s imagine you&#8217;re running a PHP application with a hidden vulnerability, allow-&#10;ing an attacker to write to the filesystem. The PHP files are added to the container&#10;image at build time and are served from the container&#8217;s filesystem. Because of the vul-&#10;nerability, the attacker can modify those files and inject them with malicious code. &#10; These types of attacks can be thwarted by preventing the container from writing to&#10;its filesystem, where the app&#8217;s executable code is normally stored. This is done by set-&#10;ting the container&#8217;s securityContext.readOnlyRootFilesystem property to true, as&#10;shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-with-readonly-filesystem&#10;Listing 13.12&#10;Dropping a capability from a container: pod-drop-chown-capability.yaml&#10;Listing 13.13&#10;A container with a read-only filesystem: pod-with-readonly-filesystem.yaml&#10;You&#8217;re not allowing this container &#10;to change file ownership.&#10; &#10;"
    color "green"
  ]
  node [
    id 662
    label "419"
    title "Page_419"
    color "blue"
  ]
  node [
    id 663
    label "text_330"
    title "387&#10;Configuring the container&#8217;s security context&#10;spec:&#10;  containers:&#10;  - name: main&#10;    image: alpine&#10;    command: [&#34;/bin/sleep&#34;, &#34;999999&#34;]&#10;    securityContext:                      &#10;      readOnlyRootFilesystem: true        &#10;    volumeMounts:                      &#10;    - name: my-volume                  &#10;      mountPath: /volume               &#10;      readOnly: false                  &#10;  volumes:&#10;  - name: my-volume&#10;    emptyDir:&#10;When you deploy this pod, the container is running as root, which has write permis-&#10;sions to the / directory, but trying to write a file there fails:&#10;$ kubectl exec -it pod-with-readonly-filesystem touch /new-file&#10;touch: /new-file: Read-only file system&#10;On the other hand, writing to the mounted volume is allowed:&#10;$ kubectl exec -it pod-with-readonly-filesystem touch /volume/newfile&#10;$ kubectl exec -it pod-with-readonly-filesystem -- ls -la /volume/newfile&#10;-rw-r--r--    1 root     root       0 May  7 19:11 /mountedVolume/newfile&#10;As shown in the example, when you make the container&#8217;s filesystem read-only, you&#8217;ll&#10;probably want to mount a volume in every directory the application writes to (for&#10;example, logs, on-disk caches, and so on).&#10;TIP&#10;To increase security, when running pods in production, set their con-&#10;tainer&#8217;s readOnlyRootFilesystem property to true.&#10;SETTING SECURITY CONTEXT OPTIONS AT THE POD LEVEL&#10;In all these examples, you&#8217;ve set the security context of an individual container. Sev-&#10;eral of these options can also be set at the pod level (through the pod.spec.security-&#10;Context property). They serve as a default for all the pod&#8217;s containers but can be&#10;overridden at the container level. The pod-level security context also allows you to set&#10;additional properties, which we&#8217;ll explain next.&#10;13.2.7 Sharing volumes when containers run as different users&#10;In chapter 6, we explained how volumes are used to share data between the pod&#8217;s&#10;containers. You had no trouble writing files in one container and reading them in&#10;the other. &#10; But this was only because both containers were running as root, giving them full&#10;access to all the files in the volume. Now imagine using the runAsUser option we&#10;explained earlier. You may need to run the two containers as two different users (per-&#10;haps you&#8217;re using two third-party container images, where each one runs its process&#10;This container&#8217;s filesystem &#10;can&#8217;t be written to...&#10;...but writing to /volume is &#10;allowed, becase a volume &#10;is mounted there.&#10; &#10;"
    color "green"
  ]
  node [
    id 664
    label "420"
    title "Page_420"
    color "blue"
  ]
  node [
    id 665
    label "text_331"
    title "388&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;under its own specific user). If those two containers use a volume to share files, they&#10;may not necessarily be able to read or write files of one another. &#10; That&#8217;s why Kubernetes allows you to specify supplemental groups for all the pods&#10;running in the container, allowing them to share files, regardless of the user IDs&#10;they&#8217;re running as. This is done using the following two properties:&#10;&#61601;&#10;fsGroup&#10;&#61601;&#10;supplementalGroups&#10;What they do is best explained in an example, so let&#8217;s see how to use them in a pod&#10;and then see what their effect is. The next listing describes a pod with two containers&#10;sharing the same volume.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-with-shared-volume-fsgroup&#10;spec:&#10;  securityContext:                       &#10;    fsGroup: 555                         &#10;    supplementalGroups: [666, 777]       &#10;  containers:&#10;  - name: first&#10;    image: alpine&#10;    command: [&#34;/bin/sleep&#34;, &#34;999999&#34;]&#10;    securityContext:                     &#10;      runAsUser: 1111                    &#10;    volumeMounts:                               &#10;    - name: shared-volume                       &#10;      mountPath: /volume&#10;      readOnly: false&#10;  - name: second&#10;    image: alpine&#10;    command: [&#34;/bin/sleep&#34;, &#34;999999&#34;]&#10;    securityContext:                     &#10;      runAsUser: 2222                    &#10;    volumeMounts:                               &#10;    - name: shared-volume                       &#10;      mountPath: /volume&#10;      readOnly: false&#10;  volumes:                                      &#10;  - name: shared-volume                         &#10;    emptyDir:&#10;After you create this pod, run a shell in its first container and see what user and group&#10;IDs the container is running as:&#10;$ kubectl exec -it pod-with-shared-volume-fsgroup -c first sh&#10;/ $ id&#10;uid=1111 gid=0(root) groups=555,666,777&#10;Listing 13.14&#10;fsGroup &#38; supplementalGroups: pod-with-shared-volume-fsgroup.yaml&#10;The fsGroup and supplementalGroups &#10;are defined in the security context at &#10;the pod level.&#10;The first container &#10;runs as user ID 1111.&#10;Both containers &#10;use the same &#10;volume&#10;The second&#10;container&#10;runs as user&#10;ID 2222.&#10; &#10;"
    color "green"
  ]
  node [
    id 666
    label "421"
    title "Page_421"
    color "blue"
  ]
  node [
    id 667
    label "text_332"
    title "389&#10;Restricting the use of security-related features in pods&#10;The id command shows the container is running with user ID 1111, as specified in the&#10;pod definition. The effective group ID is 0(root), but group IDs 555, 666, and 777 are&#10;also associated with the user. &#10; In the pod definition, you set fsGroup to 555. Because of this, the mounted volume&#10;will be owned by group ID 555, as shown here:&#10;/ $ ls -l / | grep volume&#10;drwxrwsrwx    2 root     555              6 May 29 12:23 volume&#10;If you create a file in the mounted volume&#8217;s directory, the file is owned by user ID&#10;1111 (that&#8217;s the user ID the container is running as) and by group ID 555:&#10;/ $ echo foo > /volume/foo&#10;/ $ ls -l /volume&#10;total 4&#10;-rw-r--r--    1 1111     555              4 May 29 12:25 foo&#10;This is different from how ownership is otherwise set up for newly created files. Usu-&#10;ally, the user&#8217;s effective group ID, which is 0 in your case, is used when a user creates&#10;files. You can see this by creating a file in the container&#8217;s filesystem instead of in the&#10;volume:&#10;/ $ echo foo > /tmp/foo&#10;/ $ ls -l /tmp&#10;total 4&#10;-rw-r--r--    1 1111     root             4 May 29 12:41 foo&#10;As you can see, the fsGroup security context property is used when the process cre-&#10;ates files in a volume (but this depends on the volume plugin used), whereas the&#10;supplementalGroups property defines a list of additional group IDs the user is asso-&#10;ciated with. &#10; This concludes this section about the configuration of the container&#8217;s security con-&#10;text. Next, we&#8217;ll see how a cluster administrator can restrict users from doing so.&#10;13.3&#10;Restricting the use of security-related features in pods&#10;The examples in the previous sections have shown how a person deploying pods can&#10;do whatever they want on any cluster node, by deploying a privileged pod to the&#10;node, for example. Obviously, a mechanism must prevent users from doing part or&#10;all of what&#8217;s been explained. The cluster admin can restrict the use of the previously&#10;described security-related features by creating one or more PodSecurityPolicy&#10;resources.&#10;13.3.1 Introducing the PodSecurityPolicy resource&#10;PodSecurityPolicy is a cluster-level (non-namespaced) resource, which defines what&#10;security-related features users can or can&#8217;t use in their pods. The job of upholding&#10;the policies configured in PodSecurityPolicy resources is performed by the&#10; &#10;"
    color "green"
  ]
  node [
    id 668
    label "422"
    title "Page_422"
    color "blue"
  ]
  node [
    id 669
    label "text_333"
    title "390&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;PodSecurityPolicy admission control plugin running in the API server (we explained&#10;admission control plugins in chapter 11).&#10;NOTE&#10;The PodSecurityPolicy admission control plugin may not be enabled&#10;in your cluster. Before running the following examples, ensure it&#8217;s enabled. If&#10;you&#8217;re using Minikube, refer to the next sidebar.&#10;When someone posts a pod resource to the API server, the PodSecurityPolicy admis-&#10;sion control plugin validates the pod definition against the configured PodSecurity-&#10;Policies. If the pod conforms to the cluster&#8217;s policies, it&#8217;s accepted and stored into&#10;etcd; otherwise it&#8217;s rejected immediately. The plugin may also modify the pod&#10;resource according to defaults configured in the policy.&#10;UNDERSTANDING WHAT A PODSECURITYPOLICY CAN DO&#10;A PodSecurityPolicy resource defines things like the following:&#10;&#61601;Whether a pod can use the host&#8217;s IPC, PID, or Network namespaces&#10;&#61601;Which host ports a pod can bind to&#10;&#61601;What user IDs a container can run as&#10;&#61601;Whether a pod with privileged containers can be created&#10;Enabling RBAC and PodSecurityPolicy admission control in Minikube&#10;I&#8217;m using Minikube version v0.19.0 to run these examples. That version doesn&#8217;t&#10;enable either the PodSecurityPolicy admission control plugin or RBAC authorization,&#10;which is required in part of the exercises. One exercise also requires authenticating&#10;as a different user, so you&#8217;ll also need to enable the basic authentication plugin&#10;where users are defined in a file.&#10;To run Minikube with all these plugins enabled, you may need to use this (or a similar)&#10;command, depending on the version you&#8217;re using: &#10;$ minikube start --extra-config apiserver.Authentication.PasswordFile.&#10;&#10149; BasicAuthFile=/etc/kubernetes/passwd --extra-config=apiserver.&#10;&#10149; Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRun&#10;&#10149; Options.AdmissionControl=NamespaceLifecycle,LimitRanger,Service&#10;&#10149; Account,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,&#10;&#10149; DefaultTolerationSeconds,PodSecurityPolicy&#10;The API server won&#8217;t start up until you create the password file you specified in the&#10;command line options. This is how to create the file:&#10;$ cat <<EOF | minikube ssh sudo tee /etc/kubernetes/passwd&#10;password,alice,1000,basic-user&#10;password,bob,2000,privileged-user&#10;EOF&#10;You&#8217;ll find a shell script that runs both commands in the book&#8217;s code archive in&#10;Chapter13/minikube-with-rbac-and-psp-enabled.sh.&#10; &#10;"
    color "green"
  ]
  node [
    id 670
    label "423"
    title "Page_423"
    color "blue"
  ]
  node [
    id 671
    label "text_334"
    title "391&#10;Restricting the use of security-related features in pods&#10;&#61601;Which kernel capabilities are allowed, which are added by default and which are&#10;always dropped&#10;&#61601;What SELinux labels a container can use&#10;&#61601;Whether a container can use a writable root filesystem or not&#10;&#61601;Which filesystem groups the container can run as&#10;&#61601;Which volume types a pod can use&#10;If you&#8217;ve read this chapter up to this point, everything but the last item in the previous&#10;list should be familiar. The last item should also be fairly clear. &#10;EXAMINING A SAMPLE PODSECURITYPOLICY&#10;The following listing shows a sample PodSecurityPolicy, which prevents pods from&#10;using the host&#8217;s IPC, PID, and Network namespaces, and prevents running privileged&#10;containers and the use of most host ports (except ports from 10000-11000 and 13000-&#10;14000). The policy doesn&#8217;t set any constraints on what users, groups, or SELinux&#10;groups the container can run as.&#10;apiVersion: extensions/v1beta1&#10;kind: PodSecurityPolicy&#10;metadata:&#10;  name: default&#10;spec:&#10;  hostIPC: false                 &#10;  hostPID: false                 &#10;  hostNetwork: false             &#10;  hostPorts:                         &#10;  - min: 10000                       &#10;    max: 11000                       &#10;  - min: 13000                       &#10;    max: 14000                       &#10;  privileged: false              &#10;  readOnlyRootFilesystem: true   &#10;  runAsUser:                      &#10;    rule: RunAsAny                &#10;  fsGroup:                        &#10;    rule: RunAsAny                &#10;  supplementalGroups:             &#10;    rule: RunAsAny                &#10;  seLinux:                      &#10;    rule: RunAsAny              &#10;  volumes:                  &#10;  - '*'                     &#10;Most of the options specified in the example should be self-explanatory, especially if&#10;you&#8217;ve read the previous sections. After this PodSecurityPolicy resource is posted to&#10;Listing 13.15&#10;An example PodSecurityPolicy: pod-security-policy.yaml&#10;Containers aren&#8217;t &#10;allowed to use the &#10;host&#8217;s IPC, PID, or &#10;network namespace.&#10;They can only bind to host ports &#10;10000 to 11000 (inclusive) or &#10;host ports 13000 to 14000.&#10;Containers cannot run &#10;in privileged mode.&#10;Containers are forced to run &#10;with a read-only root filesystem.&#10;Containers can &#10;run as any user &#10;and any group.&#10;They can also use any &#10;SELinux groups they want.&#10;All volume types can &#10;be used in pods.&#10; &#10;"
    color "green"
  ]
  node [
    id 672
    label "424"
    title "Page_424"
    color "blue"
  ]
  node [
    id 673
    label "text_335"
    title "392&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;the cluster, the API server will no longer allow you to deploy the privileged pod used&#10;earlier. For example&#10;$ kubectl create -f pod-privileged.yaml&#10;Error from server (Forbidden): error when creating &#34;pod-privileged.yaml&#34;:&#10;pods &#34;pod-privileged&#34; is forbidden: unable to validate against any pod &#10;security policy: [spec.containers[0].securityContext.privileged: Invalid &#10;value: true: Privileged containers are not allowed]&#10;Likewise, you can no longer deploy pods that want to use the host&#8217;s PID, IPC, or Net-&#10;work namespace. Also, because you set readOnlyRootFilesystem to true in the pol-&#10;icy, the container filesystems in all pods will be read-only (containers can only write&#10;to volumes).&#10;13.3.2 Understanding runAsUser, fsGroup, and supplementalGroups &#10;policies&#10;The policy in the previous example doesn&#8217;t impose any limits on which users and&#10;groups containers can run as, because you&#8217;ve used the RunAsAny rule for the runAs-&#10;User, fsGroup, and supplementalGroups fields. If you want to constrain the list of&#10;allowed user or group IDs, you change the rule to MustRunAs and specify the range of&#10;allowed IDs. &#10;USING THE MUSTRUNAS RULE&#10;Let&#8217;s look at an example. To only allow containers to run as user ID 2 and constrain the&#10;default filesystem group and supplemental group IDs to be anything from 2&#8211;10 or 20&#8211;&#10;30 (all inclusive), you&#8217;d include the following snippet in the PodSecurityPolicy resource.&#10;  runAsUser:&#10;    rule: MustRunAs&#10;    ranges:&#10;    - min: 2                &#10;      max: 2                &#10;  fsGroup:&#10;    rule: MustRunAs&#10;    ranges:&#10;    - min: 2                &#10;      max: 10               &#10;    - min: 20               &#10;      max: 30               &#10;  supplementalGroups:&#10;    rule: MustRunAs&#10;    ranges:&#10;    - min: 2                &#10;      max: 10               &#10;    - min: 20               &#10;      max: 30               &#10;Listing 13.16&#10;Specifying IDs containers must run as: psp-must-run-as.yaml&#10;Add a single range with min equal &#10;to max to set one specific ID.&#10;Multiple ranges are &#10;supported&#8212;here, &#10;group IDs can be 2&#8211;10 &#10;or 20&#8211;30 (inclusive).&#10; &#10;"
    color "green"
  ]
  node [
    id 674
    label "425"
    title "Page_425"
    color "blue"
  ]
  node [
    id 675
    label "text_336"
    title "393&#10;Restricting the use of security-related features in pods&#10;If the pod spec tries to set either of those fields to a value outside of these ranges, the&#10;pod will not be accepted by the API server. To try this, delete the previous PodSecurity-&#10;Policy and create the new one from the psp-must-run-as.yaml file. &#10;NOTE&#10;Changing the policy has no effect on existing pods, because PodSecurity-&#10;Policies are enforced only when creating or updating pods.&#10;DEPLOYING A POD WITH RUNASUSER OUTSIDE OF THE POLICY&#8217;S RANGE&#10;If you try deploying the pod-as-user-guest.yaml file from earlier, which says the con-&#10;tainer should run as user ID 405, the API server rejects the pod:&#10;$ kubectl create -f pod-as-user-guest.yaml&#10;Error from server (Forbidden): error when creating &#34;pod-as-user-guest.yaml&#34;&#10;: pods &#34;pod-as-user-guest&#34; is forbidden: unable to validate against any pod &#10;security policy: [securityContext.runAsUser: Invalid value: 405: UID on &#10;container main does not match required range.  Found 405, allowed: [{2 2}]]&#10;Okay, that was obvious. But what happens if you deploy a pod without setting the runAs-&#10;User property, but the user ID is baked into the container image (using the USER direc-&#10;tive in the Dockerfile)?&#10;DEPLOYING A POD WITH A CONTAINER IMAGE WITH AN OUT-OF-RANGE USER ID&#10;I&#8217;ve created an alternative image for the Node.js app you&#8217;ve used throughout the&#10;book. The image is configured so that the container will run as user ID 5. The Docker-&#10;file for the image is shown in the following listing.&#10;FROM node:7&#10;ADD app.js /app.js&#10;USER 5                         &#10;ENTRYPOINT [&#34;node&#34;, &#34;app.js&#34;]&#10;I pushed the image to Docker Hub as luksa/kubia-run-as-user-5. If I deploy a pod&#10;with that image, the API server doesn&#8217;t reject it:&#10;$ kubectl run run-as-5 --image luksa/kubia-run-as-user-5 --restart Never&#10;pod &#34;run-as-5&#34; created&#10;Unlike before, the API server accepted the pod and the Kubelet has run its container.&#10;Let&#8217;s see what user ID the container is running as:&#10;$ kubectl exec run-as-5 -- id&#10;uid=2(bin) gid=2(bin) groups=2(bin)&#10;As you can see, the container is running as user ID 2, which is the ID you specified in&#10;the PodSecurityPolicy. The PodSecurityPolicy can be used to override the user ID&#10;hardcoded into a container image.&#10;Listing 13.17&#10;Dockerfile with a USER directive: kubia-run-as-user-5/Dockerfile&#10;Containers run from &#10;this image will run &#10;as user ID 5.&#10; &#10;"
    color "green"
  ]
  node [
    id 676
    label "426"
    title "Page_426"
    color "blue"
  ]
  node [
    id 677
    label "text_337"
    title "394&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;USING THE MUSTRUNASNONROOT RULE IN THE RUNASUSER FIELD&#10;For the runAsUser field an additional rule can be used: MustRunAsNonRoot. As the&#10;name suggests, it prevents users from deploying containers that run as root. Either the&#10;container spec must specify a runAsUser field, which can&#8217;t be zero (zero is the root&#10;user&#8217;s ID), or the container image itself must run as a non-zero user ID. We explained&#10;why this is a good thing earlier.&#10;13.3.3 Configuring allowed, default, and disallowed capabilities&#10;As you learned, containers can run in privileged mode or not, and you can define a&#10;more fine-grained permission configuration by adding or dropping Linux kernel&#10;capabilities in each container. Three fields influence which capabilities containers can&#10;or cannot use:&#10;&#61601;&#10;allowedCapabilities&#10;&#61601;&#10;defaultAddCapabilities&#10;&#61601;&#10;requiredDropCapabilities&#10;We&#8217;ll look at an example first, and then discuss what each of the three fields does. The&#10;following listing shows a snippet of a PodSecurityPolicy resource defining three fields&#10;related to capabilities.&#10;apiVersion: extensions/v1beta1 &#10;kind: PodSecurityPolicy&#10;spec:&#10;  allowedCapabilities:          &#10;  - SYS_TIME                    &#10;  defaultAddCapabilities:         &#10;  - CHOWN                         &#10;  requiredDropCapabilities:     &#10;  - SYS_ADMIN                   &#10;  - SYS_MODULE                  &#10;  ...&#10;NOTE&#10;The SYS_ADMIN capability allows a range of administrative operations,&#10;and the SYS_MODULE capability allows loading and unloading of Linux kernel&#10;modules.&#10;SPECIFYING WHICH CAPABILITIES CAN BE ADDED TO A CONTAINER&#10;The allowedCapabilities field is used to specify which capabilities pod authors can&#10;add in the securityContext.capabilities field in the container spec. In one of the&#10;previous examples, you added the SYS_TIME capability to your container. If the Pod-&#10;SecurityPolicy admission control plugin had been enabled, you wouldn&#8217;t have been&#10;able to add that capability, unless it was specified in the PodSecurityPolicy as shown&#10;in listing 13.18.&#10;Listing 13.18&#10;Specifying capabilities in a PodSecurityPolicy: psp-capabilities.yaml&#10;Allow containers to &#10;add the SYS_TIME &#10;capability.&#10;Automatically add the CHOWN &#10;capability to every container.&#10;Require containers to &#10;drop the SYS_ADMIN and &#10;SYS_MODULE capabilities.&#10; &#10;"
    color "green"
  ]
  node [
    id 678
    label "427"
    title "Page_427"
    color "blue"
  ]
  node [
    id 679
    label "text_338"
    title "395&#10;Restricting the use of security-related features in pods&#10;ADDING CAPABILITIES TO ALL CONTAINERS&#10;All capabilities listed under the defaultAddCapabilities field will be added to&#10;every deployed pod&#8217;s containers. If a user doesn&#8217;t want certain containers to have&#10;those capabilities, they need to explicitly drop them in the specs of those containers.&#10; The example in listing 13.18 enables the automatic addition of the CAP_CHOWN capa-&#10;bility to every container, thus allowing processes running in the container to change the&#10;ownership of files in the container (with the chown command, for example).&#10;DROPPING CAPABILITIES FROM A CONTAINER&#10;The final field in this example is requiredDropCapabilities. I must admit, this was a&#10;somewhat strange name for me at first, but it&#8217;s not that complicated. The capabilities&#10;listed in this field are dropped automatically from every container (the PodSecurity-&#10;Policy Admission Control plugin will add them to every container&#8217;s security-&#10;Context.capabilities.drop field). &#10; If a user tries to create a pod where they explicitly add one of the capabilities listed&#10;in the policy&#8217;s requiredDropCapabilities field, the pod is rejected:&#10;$ kubectl create -f pod-add-sysadmin-capability.yaml&#10;Error from server (Forbidden): error when creating &#34;pod-add-sysadmin-&#10;capability.yaml&#34;: pods &#34;pod-add-sysadmin-capability&#34; is forbidden: unable &#10;to validate against any pod security policy: [capabilities.add: Invalid &#10;value: &#34;SYS_ADMIN&#34;: capability may not be added]&#10;13.3.4 Constraining the types of volumes pods can use&#10;The last thing a PodSecurityPolicy resource can do is define which volume types users&#10;can add to their pods. At the minimum, a PodSecurityPolicy should allow using at&#10;least the emptyDir, configMap, secret, downwardAPI, and the persistentVolume-&#10;Claim volumes. The pertinent part of such a PodSecurityPolicy resource is shown in&#10;the following listing.&#10;kind: PodSecurityPolicy&#10;spec:&#10;  volumes:&#10;  - emptyDir&#10;  - configMap&#10;  - secret&#10;  - downwardAPI&#10;  - persistentVolumeClaim&#10;If multiple PodSecurityPolicy resources are in place, pods can use any volume type&#10;defined in any of the policies (the union of all volumes lists is used).&#10;Listing 13.19&#10;A PSP snippet allowing the use of only certain volume types: &#10;psp-volumes.yaml&#10; &#10;"
    color "green"
  ]
  node [
    id 680
    label "428"
    title "Page_428"
    color "blue"
  ]
  node [
    id 681
    label "text_339"
    title "396&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;13.3.5 Assigning different PodSecurityPolicies to different users &#10;and groups&#10;We mentioned that a PodSecurityPolicy is a cluster-level resource, which means it&#10;can&#8217;t be stored in and applied to a specific namespace. Does that mean it always&#10;applies across all namespaces? No, because that would make them relatively unus-&#10;able. After all, system pods must often be allowed to do things that regular pods&#10;shouldn&#8217;t.&#10; Assigning different policies to different users is done through the RBAC mecha-&#10;nism described in the previous chapter. The idea is to create as many policies as you&#10;need and make them available to individual users or groups by creating ClusterRole&#10;resources and pointing them to the individual policies by name. By binding those&#10;ClusterRoles to specific users or groups with ClusterRoleBindings, when the Pod-&#10;SecurityPolicy Admission Control plugin needs to decide whether to admit a pod defi-&#10;nition or not, it will only consider the policies accessible to the user creating the pod. &#10; You&#8217;ll see how to do this in the next exercise. You&#8217;ll start by creating an additional&#10;PodSecurityPolicy.&#10;CREATING A PODSECURITYPOLICY ALLOWING PRIVILEGED CONTAINERS TO BE DEPLOYED&#10;You&#8217;ll create a special PodSecurityPolicy that will allow privileged users to create pods&#10;with privileged containers. The following listing shows the policy&#8217;s definition.&#10;apiVersion: extensions/v1beta1&#10;kind: PodSecurityPolicy&#10;metadata:&#10;  name: privileged          &#10;spec:&#10;  privileged: true        &#10;  runAsUser:&#10;    rule: RunAsAny&#10;  fsGroup:&#10;    rule: RunAsAny&#10;  supplementalGroups:&#10;    rule: RunAsAny&#10;  seLinux:&#10;    rule: RunAsAny&#10;  volumes:&#10;  - '*'&#10;After you post this policy to the API server, you have two policies in the cluster:&#10;$ kubectl get psp&#10;NAME         PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    ...  &#10;default      false   []     RunAsAny   RunAsAny    RunAsAny   ...&#10;privileged   true    []     RunAsAny   RunAsAny    RunAsAny   ...&#10;NOTE&#10;The shorthand for PodSecurityPolicy is psp.&#10;Listing 13.20&#10;A PodSecurityPolicy for privileged users: psp-privileged.yaml&#10;The name of this &#10;policy is &#34;privileged.&#8221;&#10;It allows running &#10;privileged containers.&#10; &#10;"
    color "green"
  ]
  node [
    id 682
    label "429"
    title "Page_429"
    color "blue"
  ]
  node [
    id 683
    label "text_340"
    title "397&#10;Restricting the use of security-related features in pods&#10;As you can see in the PRIV column, the default policy doesn&#8217;t allow running privi-&#10;leged containers, whereas the privileged policy does. Because you&#8217;re currently&#10;logged in as a cluster-admin, you can see all the policies. When creating pods, if any&#10;policy allows you to deploy a pod with certain features, the API server will accept&#10;your pod.&#10; Now imagine two additional users are using your cluster: Alice and Bob. You want&#10;Alice to only deploy restricted (non-privileged) pods, but you want to allow Bob to&#10;also deploy privileged pods. You do this by making sure Alice can only use the default&#10;PodSecurityPolicy, while allowing Bob to use both.&#10;USING RBAC TO ASSIGN DIFFERENT PODSECURITYPOLICIES TO DIFFERENT USERS&#10;In the previous chapter, you used RBAC to grant users access to only certain resource&#10;types, but I mentioned that access can be granted to specific resource instances by ref-&#10;erencing them by name. That&#8217;s what you&#8217;ll use to make users use different Pod-&#10;SecurityPolicy resources.&#10; First, you&#8217;ll create two ClusterRoles, each allowing the use of one of the policies.&#10;You&#8217;ll call the first one psp-default and in it allow the use of the default Pod-&#10;SecurityPolicy resource. You can use kubectl create clusterrole to do that:&#10;$ kubectl create clusterrole psp-default --verb=use &#10;&#10149;  --resource=podsecuritypolicies --resource-name=default&#10;clusterrole &#34;psp-default&#34; created&#10;NOTE&#10;You&#8217;re using the special verb use instead of get, list, watch, or similar.&#10;As you can see, you&#8217;re referring to a specific instance of a PodSecurityPolicy resource by&#10;using the --resource-name option. Now, create another ClusterRole called psp-&#10;privileged, pointing to the privileged policy:&#10;$ kubectl create clusterrole psp-privileged --verb=use&#10;&#10149;  --resource=podsecuritypolicies --resource-name=privileged&#10;clusterrole &#34;psp-privileged&#34; created&#10;Now, you need to bind these two policies to users. As you may remember from the pre-&#10;vious chapter, if you&#8217;re binding a ClusterRole that grants access to cluster-level&#10;resources (which is what PodSecurityPolicy resources are), you need to use a Cluster-&#10;RoleBinding instead of a (namespaced) RoleBinding. &#10; You&#8217;re going to bind the psp-default ClusterRole to all authenticated users, not&#10;only to Alice. This is necessary because otherwise no one could create any pods,&#10;because the Admission Control plugin would complain that no policy is in place.&#10;Authenticated users all belong to the system:authenticated group, so you&#8217;ll bind&#10;the ClusterRole to the group:&#10;$ kubectl create clusterrolebinding psp-all-users &#10;&#10149; --clusterrole=psp-default --group=system:authenticated&#10;clusterrolebinding &#34;psp-all-users&#34; created&#10; &#10;"
    color "green"
  ]
  node [
    id 684
    label "430"
    title "Page_430"
    color "blue"
  ]
  node [
    id 685
    label "text_341"
    title "398&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;You&#8217;ll bind the psp-privileged ClusterRole only to Bob:&#10;$ kubectl create clusterrolebinding psp-bob &#10;&#10149; --clusterrole=psp-privileged --user=bob&#10;clusterrolebinding &#34;psp-bob&#34; created&#10;As an authenticated user, Alice should now have access to the default PodSecurity-&#10;Policy, whereas Bob should have access to both the default and the privileged Pod-&#10;SecurityPolicies. Alice shouldn&#8217;t be able to create privileged pods, whereas Bob&#10;should. Let&#8217;s see if that&#8217;s true.&#10;CREATING ADDITIONAL USERS FOR KUBECTL&#10;But how do you authenticate as Alice or Bob instead of whatever you&#8217;re authenticated&#10;as currently? The book&#8217;s appendix A explains how kubectl can be used with multiple&#10;clusters, but also with multiple contexts. A context includes the user credentials used&#10;for talking to a cluster. Turn to appendix A to find out more. Here we&#8217;ll show the bare&#10;commands enabling you to use kubectl as Alice or Bob. &#10; First, you&#8217;ll create two new users in kubectl&#8217;s config with the following two&#10;commands:&#10;$ kubectl config set-credentials alice --username=alice --password=password&#10;User &#34;alice&#34; set.&#10;$ kubectl config set-credentials bob --username=bob --password=password&#10;User &#34;bob&#34; set.&#10;It should be obvious what the commands do. Because you&#8217;re setting username and&#10;password credentials, kubectl will use basic HTTP authentication for these two users&#10;(other authentication methods include tokens, client certificates, and so on).&#10;CREATING PODS AS A DIFFERENT USER&#10;You can now try creating a privileged pod while authenticating as Alice. You can tell&#10;kubectl which user credentials to use by using the --user option:&#10;$ kubectl --user alice create -f pod-privileged.yaml&#10;Error from server (Forbidden): error when creating &#34;pod-privileged.yaml&#34;: &#10;pods &#34;pod-privileged&#34; is forbidden: unable to validate against any pod &#10;security policy: [spec.containers[0].securityContext.privileged: Invalid &#10;value: true: Privileged containers are not allowed]&#10;As expected, the API server doesn&#8217;t allow Alice to create privileged pods. Now, let&#8217;s see&#10;if it allows Bob to do that:&#10;$ kubectl --user bob create -f pod-privileged.yaml&#10;pod &#34;pod-privileged&#34; created&#10; &#10;"
    color "green"
  ]
  node [
    id 686
    label "431"
    title "Page_431"
    color "blue"
  ]
  node [
    id 687
    label "text_342"
    title "399&#10;Isolating the pod network&#10;And there you go. You&#8217;ve successfully used RBAC to make the Admission Control&#10;plugin use different PodSecurityPolicy resources for different users.&#10;13.4&#10;Isolating the pod network&#10;Up to now in this chapter, we&#8217;ve explored many security-related configuration options&#10;that apply to individual pods and their containers. In the remainder of this chapter,&#10;we&#8217;ll look at how the network between pods can be secured by limiting which pods can&#10;talk to which pods.&#10; Whether this is configurable or not depends on which container networking&#10;plugin is used in the cluster. If the networking plugin supports it, you can configure&#10;network isolation by creating NetworkPolicy resources. &#10; A NetworkPolicy applies to pods that match its label selector and specifies either&#10;which sources can access the matched pods or which destinations can be accessed&#10;from the matched pods. This is configured through ingress and egress rules, respec-&#10;tively. Both types of rules can match only the pods that match a pod selector, all&#10;pods in a namespace whose labels match a namespace selector, or a network IP&#10;block specified using Classless Inter-Domain Routing (CIDR) notation (for example,&#10;192.168.1.0/24). &#10; We&#8217;ll look at both ingress and egress rules and all three matching options.&#10;NOTE&#10;Ingress rules in a NetworkPolicy have nothing to do with the Ingress&#10;resource discussed in chapter 5.&#10;13.4.1 Enabling network isolation in a namespace&#10;By default, pods in a given namespace can be accessed by anyone. First, you&#8217;ll need&#10;to change that. You&#8217;ll create a default-deny NetworkPolicy, which will prevent all&#10;clients from connecting to any pod in your namespace. The NetworkPolicy defini-&#10;tion is shown in the following listing.&#10;apiVersion: networking.k8s.io/v1&#10;kind: NetworkPolicy&#10;metadata:&#10;  name: default-deny&#10;spec:&#10;  podSelector:        &#10;When you create this NetworkPolicy in a certain namespace, no one can connect to&#10;any pod in that namespace. &#10; &#10; &#10; &#10;Listing 13.21&#10;A default-deny NetworkPolicy: network-policy-default-deny.yaml&#10;Empty pod selector &#10;matches all pods in the &#10;same namespace&#10; &#10;"
    color "green"
  ]
  node [
    id 688
    label "432"
    title "Page_432"
    color "blue"
  ]
  node [
    id 689
    label "text_343"
    title "400&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;NOTE&#10;The CNI plugin or other type of networking solution used in the clus-&#10;ter must support NetworkPolicy, or else there will be no effect on inter-pod&#10;connectivity.&#10;13.4.2 Allowing only some pods in the namespace to connect to &#10;a server pod&#10;To let clients connect to the pods in the namespace, you must now explicitly say who&#10;can connect to the pods. By who I mean which pods. Let&#8217;s explore how to do this&#10;through an example. &#10; Imagine having a PostgreSQL database pod running in namespace foo and a web-&#10;server pod that uses the database. Other pods are also in the namespace, and you&#10;don&#8217;t want to allow them to connect to the database. To secure the network, you need&#10;to create the NetworkPolicy resource shown in the following listing in the same name-&#10;space as the database pod.&#10;apiVersion: networking.k8s.io/v1&#10;kind: NetworkPolicy&#10;metadata:&#10;  name: postgres-netpolicy&#10;spec:&#10;  podSelector:                     &#10;    matchLabels:                   &#10;      app: database                &#10;  ingress:                           &#10;  - from:                            &#10;    - podSelector:                   &#10;        matchLabels:                 &#10;          app: webserver             &#10;    ports:                     &#10;    - port: 5432               &#10;The example NetworkPolicy allows pods with the app=webserver label to connect to&#10;pods with the app=database label, and only on port 5432. Other pods can&#8217;t connect to&#10;the database pods, and no one (not even the webserver pods) can connect to anything&#10;other than port 5432 of the database pods. This is shown in figure 13.4.&#10; Client pods usually connect to server pods through a Service instead of directly to&#10;the pod, but that doesn&#8217;t change anything. The NetworkPolicy is enforced when con-&#10;necting through a Service, as well.&#10; &#10; &#10; &#10; &#10;Listing 13.22&#10;A NetworkPolicy for the Postgres pod: network-policy-postgres.yaml&#10;This policy secures &#10;access to pods with &#10;app=database label.&#10;It allows incoming connections &#10;only from pods with the &#10;app=webserver label.&#10;Connections to this &#10;port are allowed.&#10; &#10;"
    color "green"
  ]
  node [
    id 690
    label "433"
    title "Page_433"
    color "blue"
  ]
  node [
    id 691
    label "text_344"
    title "401&#10;Isolating the pod network&#10;13.4.3 Isolating the network between Kubernetes namespaces&#10;Now let&#8217;s look at another example, where multiple tenants are using the same Kuber-&#10;netes cluster. Each tenant can use multiple namespaces, and each namespace has a&#10;label specifying the tenant it belongs to. For example, one of those tenants is Man-&#10;ning. All their namespaces have been labeled with tenant: manning. In one of their&#10;namespaces, they run a Shopping Cart microservice that needs to be available to all&#10;pods running in any of their namespaces. Obviously, they don&#8217;t want any other tenants&#10;to access their microservice.&#10; To secure their microservice, they create the NetworkPolicy resource shown in the&#10;following listing.&#10;apiVersion: networking.k8s.io/v1&#10;kind: NetworkPolicy&#10;metadata:&#10;  name: shoppingcart-netpolicy&#10;spec:&#10;  podSelector:                       &#10;    matchLabels:                     &#10;      app: shopping-cart             &#10;  ingress:&#10;  - from:&#10;    - namespaceSelector:            &#10;        matchLabels:                &#10;          tenant: manning           &#10;    ports:&#10;    - port: 80&#10;Listing 13.23&#10;NetworkPolicy for the shopping cart pod(s): network-policy-cart.yaml&#10;app: database&#10;Pod:&#10;database&#10;Port&#10;5432&#10;Port&#10;9876&#10;app: webserver&#10;Pod:&#10;webserver&#10;Pod selector:&#10;app=webserver&#10;Pod selector:&#10;app=database&#10;app: webserver&#10;Pod:&#10;webserver&#10;Other pods&#10;NetworkPolicy: postgres-netpolicy&#10;Figure 13.4&#10;A NetworkPolicy allowing only some pods to access other pods and only on a specific &#10;port&#10;This policy applies to pods &#10;labeled as microservice= &#10;shopping-cart.&#10;Only pods running in namespaces &#10;labeled as tenant=manning are &#10;allowed to access the microservice.&#10; &#10;"
    color "green"
  ]
  node [
    id 692
    label "434"
    title "Page_434"
    color "blue"
  ]
  node [
    id 693
    label "text_345"
    title "402&#10;CHAPTER 13&#10;Securing cluster nodes and the network&#10;This NetworkPolicy ensures only pods running in namespaces labeled as tenant:&#10;manning can access their Shopping Cart microservice, as shown in figure 13.5.&#10;If the shopping cart provider also wants to give access to other tenants (perhaps to&#10;one of their partner companies), they can either create an additional NetworkPolicy&#10;resource or add an additional ingress rule to their existing NetworkPolicy.&#10;NOTE&#10;In a multi-tenant Kubernetes cluster, tenants usually can&#8217;t add labels&#10;(or annotations) to their namespaces themselves. If they could, they&#8217;d be able&#10;to circumvent the namespaceSelector-based ingress rules.&#10;13.4.4 Isolating using CIDR notation&#10;Instead of specifying a pod- or namespace selector to define who can access the pods&#10;targeted in the NetworkPolicy, you can also specify an IP block in CIDR notation. For&#10;example, to allow the shopping-cart pods from the previous section to only be acces-&#10;sible from IPs in the 192.168.1.1 to .255 range, you&#8217;d specify the ingress rule in the&#10;next listing.&#10;  ingress:&#10;  - from:&#10;    - ipBlock:                    &#10;        cidr: 192.168.1.0/24      &#10;Listing 13.24&#10;Specifying an IP block in an ingress rule: network-policy-cidr.yaml&#10;app: shopping-cart&#10;Pod:&#10;shopping-cart&#10;Port&#10;80&#10;Namespace selector:&#10;tenant=manning&#10;Pod selector:&#10;app=shopping-cart&#10;Other pods&#10;Pods&#10;NetworkPolicy:&#10;shoppingcart-netpolicy&#10;Namespace: manningA&#10;Namespace: ecommerce-ltd&#10;Other namespaces&#10;tenant: manning&#10;Pods&#10;Namespace: manningB&#10;tenant: manning&#10;Figure 13.5&#10;A NetworkPolicy only allowing pods in namespaces matching a namespaceSelector to access a &#10;specific pod.&#10;This ingress rule only allows traffic from &#10;clients in the 192.168.1.0/24 IP block. &#10; &#10;"
    color "green"
  ]
  node [
    id 694
    label "435"
    title "Page_435"
    color "blue"
  ]
  node [
    id 695
    label "text_346"
    title "403&#10;Summary&#10;13.4.5 Limiting the outbound traffic of a set of pods&#10;In all previous examples, you&#8217;ve been limiting the inbound traffic to the pods that&#10;match the NetworkPolicy&#8217;s pod selector using ingress rules, but you can also limit&#10;their outbound traffic through egress rules. An example is shown in the next listing.&#10;spec:&#10;  podSelector:               &#10;    matchLabels:             &#10;      app: webserver         &#10;  egress:               &#10;  - to:                       &#10;    - podSelector:            &#10;        matchLabels:          &#10;          app: database       &#10;The NetworkPolicy in the previous listing allows pods that have the app=webserver&#10;label to only access pods that have the app=database label and nothing else (neither&#10;other pods, nor any other IP, regardless of whether it&#8217;s internal or external to the&#10;cluster).&#10;13.5&#10;Summary&#10;In this chapter, you learned about securing cluster nodes from pods and pods from&#10;other pods. You learned that&#10;&#61601;Pods can use the node&#8217;s Linux namespaces instead of using their own.&#10;&#61601;Containers can be configured to run as a different user and/or group than the&#10;one defined in the container image.&#10;&#61601;Containers can also run in privileged mode, allowing them to access the node&#8217;s&#10;devices that are otherwise not exposed to pods.&#10;&#61601;Containers can be run as read-only, preventing processes from writing to the&#10;container&#8217;s filesystem (and only allowing them to write to mounted volumes).&#10;&#61601;Cluster-level PodSecurityPolicy resources can be created to prevent users from&#10;creating pods that could compromise a node.&#10;&#61601;PodSecurityPolicy resources can be associated with specific users using RBAC&#8217;s&#10;ClusterRoles and ClusterRoleBindings.&#10;&#61601;NetworkPolicy resources are used to limit a pod&#8217;s inbound and/or outbound&#10;traffic.&#10;In the next chapter, you&#8217;ll learn how computational resources available to pods can be&#10;constrained and how a pod&#8217;s quality of service is configured.&#10;Listing 13.25&#10;Using egress rules in a NetworkPolicy: network-policy-egress.yaml&#10;This policy applies to pods with &#10;the app=webserver label.&#10;It limits&#10;the pods&#8217;&#10;outbound&#10;traffic.&#10;Webserver pods may only &#10;connect to pods with the &#10;app=database label.&#10; &#10;"
    color "green"
  ]
  node [
    id 696
    label "436"
    title "Page_436"
    color "blue"
  ]
  node [
    id 697
    label "text_347"
    title "404&#10;Managing pods&#8217;&#10; computational resources&#10;Up to now you&#8217;ve created pods without caring about how much CPU and memory&#10;they&#8217;re allowed to consume. But as you&#8217;ll see in this chapter, setting both how&#10;much a pod is expected to consume and the maximum amount it&#8217;s allowed to con-&#10;sume is a vital part of any pod definition. Setting these two sets of parameters&#10;makes sure that a pod takes only its fair share of the resources provided by the&#10;Kubernetes cluster and also affects how pods are scheduled across the cluster.&#10;This chapter covers&#10;&#61601;Requesting CPU, memory, and other &#10;computational resources for containers&#10;&#61601;Setting a hard limit for CPU and memory&#10;&#61601;Understanding Quality of Service guarantees for &#10;pods&#10;&#61601;Setting default, min, and max resources for pods &#10;in a namespace&#10;&#61601;Limiting the total amount of resources available &#10;in a namespace&#10; &#10;"
    color "green"
  ]
  node [
    id 698
    label "437"
    title "Page_437"
    color "blue"
  ]
  node [
    id 699
    label "text_348"
    title "405&#10;Requesting resources for a pod&#8217;s containers&#10;14.1&#10;Requesting resources for a pod&#8217;s containers&#10;When creating a pod, you can specify the amount of CPU and memory that a con-&#10;tainer needs (these are called requests) and a hard limit on what it may consume&#10;(known as limits). They&#8217;re specified for each container individually, not for the pod as&#10;a whole. The pod&#8217;s resource requests and limits are the sum of the requests and lim-&#10;its of all its containers. &#10;14.1.1 Creating pods with resource requests&#10;Let&#8217;s look at an example pod manifest, which has the CPU and memory requests spec-&#10;ified for its single container, as shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: requests-pod&#10;spec:&#10;  containers:&#10;  - image: busybox&#10;    command: [&#34;dd&#34;, &#34;if=/dev/zero&#34;, &#34;of=/dev/null&#34;]&#10;    name: main              &#10;    resources:              &#10;      requests:             &#10;        cpu: 200m          &#10;        memory: 10Mi    &#10;In the pod manifest, your single container requires one-fifth of a CPU core (200 mil-&#10;licores) to run properly. Five such pods/containers can run sufficiently fast on a single&#10;CPU core. &#10; When you don&#8217;t specify a request for CPU, you&#8217;re saying you don&#8217;t care how much&#10;CPU time the process running in your container is allotted. In the worst case, it may&#10;not get any CPU time at all (this happens when a heavy demand by other processes&#10;exists on the CPU). Although this may be fine for low-priority batch jobs, which aren&#8217;t&#10;time-critical, it obviously isn&#8217;t appropriate for containers handling user requests.&#10; In the pod spec, you&#8217;re also requesting 10 mebibytes of memory for the container.&#10;By doing that, you&#8217;re saying that you expect the processes running inside the con-&#10;tainer to use at most 10 mebibytes of RAM. They might use less, but you&#8217;re not expect-&#10;ing them to use more than that in normal circumstances. Later in this chapter you&#8217;ll&#10;see what happens if they do.&#10; Now you&#8217;ll run the pod. When the pod starts, you can take a quick look at the pro-&#10;cess&#8217; CPU consumption by running the top command inside the container, as shown&#10;in the following listing.&#10;Listing 14.1&#10;A pod with resource requests: requests-pod.yaml&#10;You&#8217;re specifying resource &#10;requests for the main container.&#10;The container requests 200 &#10;millicores (that is, 1/5 of a &#10;single CPU core&#8217;s time).&#10;The container also&#10;requests 10 mebibytes&#10;of memory.&#10; &#10;"
    color "green"
  ]
  node [
    id 700
    label "438"
    title "Page_438"
    color "blue"
  ]
  node [
    id 701
    label "text_349"
    title "406&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;$ kubectl exec -it requests-pod top&#10;Mem: 1288116K used, 760368K free, 9196K shrd, 25748K buff, 814840K cached&#10;CPU:  9.1% usr 42.1% sys  0.0% nic 48.4% idle  0.0% io  0.0% irq  0.2% sirq&#10;Load average: 0.79 0.52 0.29 2/481 10&#10;  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND&#10;    1     0 root     R     1192  0.0   1 50.2 dd if /dev/zero of /dev/null&#10;    7     0 root     R     1200  0.0   0  0.0 top&#10;The dd command you&#8217;re running in the container consumes as much CPU as it can,&#10;but it only runs a single thread so it can only use a single core. The Minikube VM,&#10;which is where this example is running, has two CPU cores allotted to it. That&#8217;s why&#10;the process is shown consuming 50% of the whole CPU. &#10; Fifty percent of two cores is obviously one whole core, which means the container&#10;is using more than the 200 millicores you requested in the pod specification. This is&#10;expected, because requests don&#8217;t limit the amount of CPU a container can use. You&#8217;d&#10;need to specify a CPU limit to do that. You&#8217;ll try that later, but first, let&#8217;s see how spec-&#10;ifying resource requests in a pod affects the scheduling of the pod.&#10;14.1.2 Understanding how resource requests affect scheduling&#10;By specifying resource requests, you&#8217;re specifying the minimum amount of resources&#10;your pod needs. This information is what the Scheduler uses when scheduling the pod&#10;to a node. Each node has a certain amount of CPU and memory it can allocate to&#10;pods. When scheduling a pod, the Scheduler will only consider nodes with enough&#10;unallocated resources to meet the pod&#8217;s resource requirements. If the amount of&#10;unallocated CPU or memory is less than what the pod requests, Kubernetes will not&#10;schedule the pod to that node, because the node can&#8217;t provide the minimum amount&#10;required by the pod.&#10;UNDERSTANDING HOW THE SCHEDULER DETERMINES IF A POD CAN FIT ON A NODE&#10;What&#8217;s important and somewhat surprising here is that the Scheduler doesn&#8217;t look at&#10;how much of each individual resource is being used at the exact time of scheduling&#10;but at the sum of resources requested by the existing pods deployed on the node.&#10;Even though existing pods may be using less than what they&#8217;ve requested, scheduling&#10;another pod based on actual resource consumption would break the guarantee given&#10;to the already deployed pods.&#10; This is visualized in figure 14.1. Three pods are deployed on the node. Together,&#10;they&#8217;ve requested 80% of the node&#8217;s CPU and 60% of the node&#8217;s memory. Pod D,&#10;shown at the bottom right of the figure, cannot be scheduled onto the node because it&#10;requests 25% of the CPU, which is more than the 20% of unallocated CPU. The fact&#10;that the three pods are currently using only 70% of the CPU makes no difference.&#10;Listing 14.2&#10;Examining CPU and memory usage from within a container&#10; &#10;"
    color "green"
  ]
  node [
    id 702
    label "439"
    title "Page_439"
    color "blue"
  ]
  node [
    id 703
    label "text_350"
    title "407&#10;Requesting resources for a pod&#8217;s containers&#10;UNDERSTANDING HOW THE SCHEDULER USES PODS&#8217; REQUESTS WHEN SELECTING THE BEST NODE &#10;FOR A POD&#10;You may remember from chapter 11 that the Scheduler first filters the list of nodes to&#10;exclude those that the pod can&#8217;t fit on and then prioritizes the remaining nodes per the&#10;configured prioritization functions. Among others, two prioritization functions rank&#10;nodes based on the amount of resources requested: LeastRequestedPriority and&#10;MostRequestedPriority. The first one prefers nodes with fewer requested resources&#10;(with a greater amount of unallocated resources), whereas the second one is the exact&#10;opposite&#8212;it prefers nodes that have the most requested resources (a smaller amount of&#10;unallocated CPU and memory). But, as we&#8217;ve discussed, they both consider the amount&#10;of requested resources, not the amount of resources actually consumed.&#10; The Scheduler is configured to use only one of those functions. You may wonder&#10;why anyone would want to use the MostRequestedPriority function. After all, if you&#10;have a set of nodes, you usually want to spread CPU load evenly across them. However,&#10;that&#8217;s not the case when running on cloud infrastructure, where you can add and&#10;remove nodes whenever necessary. By configuring the Scheduler to use the Most-&#10;RequestedPriority function, you guarantee that Kubernetes will use the smallest pos-&#10;sible number of nodes while still providing each pod with the amount of CPU/memory&#10;it requests. By keeping pods tightly packed, certain nodes are left vacant and can be&#10;removed. Because you&#8217;re paying for individual nodes, this saves you money.&#10;INSPECTING A NODE&#8217;S CAPACITY&#10;Let&#8217;s see the Scheduler in action. You&#8217;ll deploy another pod with four times the&#10;amount of requested resources as before. But before you do that, let&#8217;s see your node&#8217;s&#10;capacity. Because the Scheduler needs to know how much CPU and memory each&#10;node has, the Kubelet reports this data to the API server, making it available through&#10;Pod C&#10;Node&#10;Pod A&#10;Unallocated&#10;CPU requests&#10;Pod B&#10;Pod A&#10;Currently unused&#10;CPU usage&#10;Pod B&#10;Pod C&#10;0%&#10;100%&#10;Pod A&#10;Memory requests&#10;Pod B&#10;Pod C&#10;Pod A&#10;Memory usage&#10;Pod B&#10;Pod C&#10;CPU requests&#10;Memory requests&#10;Unallocated&#10;Currently unused&#10;Pod D&#10;Pod D cannot be scheduled; its CPU&#10;requests exceed unallocated CPU&#10;Figure 14.1&#10;The Scheduler only cares about requests, not actual usage.&#10; &#10;"
    color "green"
  ]
  node [
    id 704
    label "440"
    title "Page_440"
    color "blue"
  ]
  node [
    id 705
    label "text_351"
    title "408&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;the Node resource. You can see it by using the kubectl describe command as in the&#10;following listing.&#10;$ kubectl describe nodes&#10;Name:       minikube&#10;...&#10;Capacity:                       &#10;  cpu:           2               &#10;  memory:        2048484Ki       &#10;  pods:          110             &#10;Allocatable:                       &#10;  cpu:           2                  &#10;  memory:        1946084Ki          &#10;  pods:          110                &#10;...&#10;The output shows two sets of amounts related to the available resources on the node:&#10;the node&#8217;s capacity and allocatable resources. The capacity represents the total resources&#10;of a node, which may not all be available to pods. Certain resources may be reserved&#10;for Kubernetes and/or system components. The Scheduler bases its decisions only on&#10;the allocatable resource amounts.&#10; In the previous example, the node called minikube runs in a VM with two cores&#10;and has no CPU reserved, making the whole CPU allocatable to pods. Therefore,&#10;the Scheduler should have no problem scheduling another pod requesting 800&#10;millicores. &#10; Run the pod now. You can use the YAML file in the code archive, or run the pod&#10;with the kubectl run command like this:&#10;$ kubectl run requests-pod-2 --image=busybox --restart Never&#10;&#10149; --requests='cpu=800m,memory=20Mi' -- dd if=/dev/zero of=/dev/null&#10;pod &#34;requests-pod-2&#34; created&#10;Let&#8217;s see if it was scheduled:&#10;$ kubectl get po requests-pod-2&#10;NAME             READY     STATUS    RESTARTS   AGE&#10;requests-pod-2   1/1       Running   0          3m&#10;Okay, the pod has been scheduled and is running. &#10;CREATING A POD THAT DOESN&#8217;T FIT ON ANY NODE&#10;You now have two pods deployed, which together have requested a total of 1,000 mil-&#10;licores or exactly 1 core. You should therefore have another 1,000 millicores available&#10;for additional pods, right? You can deploy another pod with a resource request of&#10;1,000 millicores. Use a similar command as before:&#10;$ kubectl run requests-pod-3 --image=busybox --restart Never&#10;&#10149; --requests='cpu=1,memory=20Mi' -- dd if=/dev/zero of=/dev/null&#10;pod &#34;requests-pod-2&#34; created&#10;Listing 14.3&#10;A node&#8217;s capacity and allocatable resources&#10;The overall capacity &#10;of the node&#10;The resources &#10;allocatable to pods&#10; &#10;"
    color "green"
  ]
  node [
    id 706
    label "441"
    title "Page_441"
    color "blue"
  ]
  node [
    id 707
    label "text_352"
    title "409&#10;Requesting resources for a pod&#8217;s containers&#10;NOTE&#10;This time you&#8217;re specifying the CPU request in whole cores (cpu=1)&#10;instead of millicores (cpu=1000m).&#10;So far, so good. The pod has been accepted by the API server (you&#8217;ll remember from&#10;the previous chapter that the API server can reject pods if they&#8217;re invalid in any way).&#10;Now, check if the pod is running:&#10;$ kubectl get po requests-pod-3&#10;NAME             READY     STATUS    RESTARTS   AGE&#10;requests-pod-3   0/1       Pending   0          4m&#10;Even if you wait a while, the pod is still stuck at Pending. You can see more informa-&#10;tion on why that&#8217;s the case by using the kubectl describe command, as shown in&#10;the following listing.&#10;$ kubectl describe po requests-pod-3&#10;Name:       requests-pod-3&#10;Namespace:  default&#10;Node:       /                    &#10;...&#10;Conditions:&#10;  Type           Status&#10;  PodScheduled   False           &#10;...&#10;Events:&#10;... Warning  FailedScheduling    No nodes are available      &#10;                                 that match all of the       &#10;                                 following predicates::      &#10;                                 Insufficient cpu (1).       &#10;The output shows that the pod hasn&#8217;t been scheduled because it can&#8217;t fit on any node&#10;due to insufficient CPU on your single node. But why is that? The sum of the CPU&#10;requests of all three pods equals 2,000 millicores or exactly two cores, which is exactly&#10;what your node can provide. What&#8217;s wrong?&#10;DETERMINING WHY A POD ISN&#8217;T BEING SCHEDULED&#10;You can figure out why the pod isn&#8217;t being scheduled by inspecting the node resource.&#10;Use the kubectl describe node command again and examine the output more&#10;closely in the following listing.&#10;$ kubectl describe node&#10;Name:                   minikube&#10;...&#10;Non-terminated Pods:    (7 in total)&#10;  Namespace    Name            CPU Requ.   CPU Lim.  Mem Req.    Mem Lim.&#10;  ---------    ----            ----------  --------  ---------   --------&#10;  default      requests-pod    200m (10%)  0 (0%)    10Mi (0%)   0 (0%)&#10;Listing 14.4&#10;Examining why a pod is stuck at Pending with kubectl describe pod&#10;Listing 14.5&#10;Inspecting allocated resources on a node with kubectl describe node&#10;No node is &#10;associated &#10;with the pod.&#10;The pod hasn&#8217;t &#10;been scheduled.&#10;Scheduling has &#10;failed because of &#10;insufficient CPU.&#10; &#10;"
    color "green"
  ]
  node [
    id 708
    label "442"
    title "Page_442"
    color "blue"
  ]
  node [
    id 709
    label "text_353"
    title "410&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;  default      requests-pod-2  800m (40%)  0 (0%)    20Mi (1%)   0 (0%)&#10;  kube-system  dflt-http-b...  10m (0%)    10m (0%)  20Mi (1%)   20Mi (1%)&#10;  kube-system  kube-addon-...  5m (0%)     0 (0%)    50Mi (2%)   0 (0%)&#10;  kube-system  kube-dns-26...  260m (13%)  0 (0%)    110Mi (5%)  170Mi (8%)&#10;  kube-system  kubernetes-...  0 (0%)      0 (0%)    0 (0%)      0 (0%)&#10;  kube-system  nginx-ingre...  0 (0%)      0 (0%)    0 (0%)      0 (0%)&#10;Allocated resources:&#10;  (Total limits may be over 100 percent, i.e., overcommitted.)&#10;  CPU Requests  CPU Limits      Memory Requests Memory Limits&#10;  ------------  ----------      --------------- -------------&#10;  1275m (63%)   10m (0%)        210Mi (11%)     190Mi (9%)&#10;If you look at the bottom left of the listing, you&#8217;ll see a total of 1,275 millicores have&#10;been requested by the running pods, which is 275 millicores more than what you&#10;requested for the first two pods you deployed. Something is eating up additional&#10;CPU resources. &#10; You can find the culprit in the list of pods in the previous listing. Three pods in the&#10;kube-system namespace have explicitly requested CPU resources. Those pods plus&#10;your two pods leave only 725 millicores available for additional pods. Because your&#10;third pod requested 1,000 millicores, the Scheduler won&#8217;t schedule it to this node, as&#10;that would make the node overcommitted. &#10;FREEING RESOURCES TO GET THE POD SCHEDULED&#10;The pod will only be scheduled when an adequate amount of CPU is freed (when one&#10;of the first two pods is deleted, for example). If you delete your second pod, the&#10;Scheduler will be notified of the deletion (through the watch mechanism described in&#10;chapter 11) and will schedule your third pod as soon as the second pod terminates.&#10;This is shown in the following listing.&#10;$ kubectl delete po requests-pod-2&#10;pod &#34;requests-pod-2&#34; deleted &#10;$ kubectl get po&#10;NAME             READY     STATUS        RESTARTS   AGE&#10;requests-pod     1/1       Running       0          2h&#10;requests-pod-2   1/1       Terminating   0          1h&#10;requests-pod-3   0/1       Pending       0          1h&#10;$ kubectl get po&#10;NAME             READY     STATUS    RESTARTS   AGE&#10;requests-pod     1/1       Running   0          2h&#10;requests-pod-3   1/1       Running   0          1h&#10;In all these examples, you&#8217;ve specified a request for memory, but it hasn&#8217;t played any&#10;role in the scheduling because your node has more than enough allocatable memory to&#10;accommodate all your pods&#8217; requests. Both CPU and memory requests are treated the&#10;same way by the Scheduler, but in contrast to memory requests, a pod&#8217;s CPU requests&#10;also play a role elsewhere&#8212;while the pod is running. You&#8217;ll learn about this next.&#10;Listing 14.6&#10;Pod is scheduled after deleting another pod&#10; &#10;"
    color "green"
  ]
  node [
    id 710
    label "443"
    title "Page_443"
    color "blue"
  ]
  node [
    id 711
    label "text_354"
    title "411&#10;Requesting resources for a pod&#8217;s containers&#10;14.1.3 Understanding how CPU requests affect CPU time sharing&#10;You now have two pods running in your cluster (you can disregard the system pods&#10;right now, because they&#8217;re mostly idle). One has requested 200 millicores and the&#10;other one five times as much. At the beginning of the chapter, we said Kubernetes dis-&#10;tinguishes between resource requests and limits. You haven&#8217;t defined any limits yet, so&#10;the two pods are in no way limited when it comes to how much CPU they can each&#10;consume. If the process inside each pod consumes as much CPU time as it can, how&#10;much CPU time does each pod get? &#10; The CPU requests don&#8217;t only affect scheduling&#8212;they also determine how the&#10;remaining (unused) CPU time is distributed between pods. Because your first pod&#10;requested 200 millicores of CPU and the other one 1,000 millicores, any unused CPU&#10;will be split among the two pods in a 1 to 5 ratio, as shown in figure 14.2. If both pods&#10;consume as much CPU as they can, the first pod will get one sixth or 16.7% of the&#10;CPU time and the other one the remaining five sixths or 83.3%.&#10;But if one container wants to use up as much CPU as it can, while the other one is sit-&#10;ting idle at a given moment, the first container will be allowed to use the whole CPU&#10;time (minus the small amount of time used by the second container, if any). After all,&#10;it makes sense to use all the available CPU if no one else is using it, right? As soon as&#10;the second container needs CPU time, it will get it and the first container will be throt-&#10;tled back.&#10;14.1.4 Defining and requesting custom resources&#10;Kubernetes also allows you to add your own custom resources to a node and request&#10;them in the pod&#8217;s resource requests. Initially these were known as Opaque Integer&#10;Resources, but were replaced with Extended Resources in Kubernetes version 1.8.&#10;Pod A:&#10;200 m&#10;CPU&#10;requests&#10;Pod B: 1000 m&#10;800 m available&#10;CPU&#10;usage&#10;2000 m&#10;1000 m&#10;0 m&#10;Pod A and B requests&#10;are in 1:5 ratio.&#10;Available CPU time is&#10;distributed in same ratio.&#10;Pod B: 1667 m&#10;133 m&#10;(1/6)&#10;667 m&#10;(5/6)&#10;Pod A:&#10;333 m&#10;Figure 14.2&#10;Unused CPU time is distributed to containers based on their CPU requests.&#10; &#10;"
    color "green"
  ]
  node [
    id 712
    label "444"
    title "Page_444"
    color "blue"
  ]
  node [
    id 713
    label "text_355"
    title "412&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10; First, you obviously need to make Kubernetes aware of your custom resource by&#10;adding it to the Node object&#8217;s capacity field. This can be done by performing a&#10;PATCH HTTP request. The resource name can be anything, such as example.org/my-&#10;resource, as long as it doesn&#8217;t start with the kubernetes.io domain. The quantity&#10;must be an integer (for example, you can&#8217;t set it to 100 millis, because 0.1 isn&#8217;t an inte-&#10;ger; but you can set it to 1000m or 2000m or, simply, 1 or 2). The value will be copied&#10;from the capacity to the allocatable field automatically.&#10; Then, when creating pods, you specify the same resource name and the requested&#10;quantity under the resources.requests field in the container spec or with --requests&#10;when using kubectl run like you did in previous examples. The Scheduler will make&#10;sure the pod is only deployed to a node that has the requested amount of the custom&#10;resource available. Every deployed pod obviously reduces the number of allocatable&#10;units of the resource.&#10; An example of a custom resource could be the number of GPU units available on the&#10;node. Pods requiring the use of a GPU specify that in their requests. The Scheduler then&#10;makes sure the pod is only scheduled to nodes with at least one GPU still unallocated.&#10;14.2&#10;Limiting resources available to a container&#10;Setting resource requests for containers in a pod ensures each container gets the min-&#10;imum amount of resources it needs. Now let&#8217;s see the other side of the coin&#8212;the&#10;maximum amount the container will be allowed to consume. &#10;14.2.1 Setting a hard limit for the amount of resources a container can use&#10;We&#8217;ve seen how containers are allowed to use up all the CPU if all the other processes&#10;are sitting idle. But you may want to prevent certain containers from using up more&#10;than a specific amount of CPU. And you&#8217;ll always want to limit the amount of memory&#10;a container can consume. &#10; CPU is a compressible resource, which means the amount used by a container can&#10;be throttled without affecting the process running in the container in an adverse way.&#10;Memory is obviously different&#8212;it&#8217;s incompressible. Once a process is given a chunk of&#10;memory, that memory can&#8217;t be taken away from it until it&#8217;s released by the process&#10;itself. That&#8217;s why you need to limit the maximum amount of memory a container can&#10;be given. &#10; Without limiting memory, a container (or a pod) running on a worker node may&#10;eat up all the available memory and affect all other pods on the node and any new&#10;pods scheduled to the node (remember that new pods are scheduled to the node&#10;based on the memory requests and not actual memory usage). A single malfunction-&#10;ing or malicious pod can practically make the whole node unusable.&#10;CREATING A POD WITH RESOURCE LIMITS&#10;To prevent this from happening, Kubernetes allows you to specify resource limits for&#10;every container (along with, and virtually in the same way as, resource requests). The&#10;following listing shows an example pod manifest with resource limits.&#10; &#10;"
    color "green"
  ]
  node [
    id 714
    label "445"
    title "Page_445"
    color "blue"
  ]
  node [
    id 715
    label "text_356"
    title "413&#10;Limiting resources available to a container&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: limited-pod&#10;spec:&#10;  containers:&#10;  - image: busybox&#10;    command: [&#34;dd&#34;, &#34;if=/dev/zero&#34;, &#34;of=/dev/null&#34;]&#10;    name: main&#10;    resources:            &#10;      limits:             &#10;        cpu: 1             &#10;        memory: 20Mi       &#10;This pod&#8217;s container has resource limits configured for both CPU and memory. The&#10;process or processes running inside the container will not be allowed to consume&#10;more than 1 CPU core and 20 mebibytes of memory. &#10;NOTE&#10;Because you haven&#8217;t specified any resource requests, they&#8217;ll be set to&#10;the same values as the resource limits.&#10;OVERCOMMITTING LIMITS&#10;Unlike resource requests, resource limits aren&#8217;t constrained by the node&#8217;s allocatable&#10;resource amounts. The sum of all limits of all the pods on a node is allowed to exceed&#10;100% of the node&#8217;s capacity (figure 14.3). Restated, resource limits can be overcom-&#10;mitted. This has an important consequence&#8212;when 100% of the node&#8217;s resources are&#10;used up, certain containers will need to be killed.&#10;You&#8217;ll see how Kubernetes decides which containers to kill in section 14.3, but individ-&#10;ual containers can be killed even if they try to use more than their resource limits&#10;specify. You&#8217;ll learn more about this next.&#10;Listing 14.7&#10;A pod with a hard limit on CPU and memory: limited-pod.yaml&#10;Specifying resource &#10;limits for the container&#10;This container will be &#10;allowed to use at &#10;most 1 CPU core.&#10;The container will be&#10;allowed to use up to 20&#10;mebibytes of memory.&#10;Node&#10;0%&#10;136%&#10;100%&#10;Pod A&#10;Memory requests&#10;Pod B&#10;Pod C&#10;Pod A&#10;Memory limits&#10;Pod B&#10;Unallocated&#10;Pod C&#10;Figure 14.3&#10;The sum of resource limits of all pods on a node can exceed 100% of the node&#8217;s &#10;capacity.&#10; &#10;"
    color "green"
  ]
  node [
    id 716
    label "446"
    title "Page_446"
    color "blue"
  ]
  node [
    id 717
    label "text_357"
    title "414&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;14.2.2 Exceeding the limits&#10;What happens when a process running in a container tries to use a greater amount of&#10;resources than it&#8217;s allowed to? &#10; You&#8217;ve already learned that CPU is a compressible resource, and it&#8217;s only natural&#10;for a process to want to consume all of the CPU time when not waiting for an I/O&#10;operation. As you&#8217;ve learned, a process&#8217; CPU usage is throttled, so when a CPU&#10;limit is set for a container, the process isn&#8217;t given more CPU time than the config-&#10;ured limit. &#10; With memory, it&#8217;s different. When a process tries to allocate memory over its&#10;limit, the process is killed (it&#8217;s said the container is OOMKilled, where OOM stands&#10;for Out Of Memory). If the pod&#8217;s restart policy is set to Always or OnFailure, the&#10;process is restarted immediately, so you may not even notice it getting killed. But if it&#10;keeps going over the memory limit and getting killed, Kubernetes will begin restart-&#10;ing it with increasing delays between restarts. You&#8217;ll see a CrashLoopBackOff status&#10;in that case:&#10;$ kubectl get po&#10;NAME        READY     STATUS             RESTARTS   AGE&#10;memoryhog   0/1       CrashLoopBackOff   3          1m&#10;The CrashLoopBackOff status doesn&#8217;t mean the Kubelet has given up. It means that&#10;after each crash, the Kubelet is increasing the time period before restarting the con-&#10;tainer. After the first crash, it restarts the container immediately and then, if it crashes&#10;again, waits for 10 seconds before restarting it again. On subsequent crashes, this&#10;delay is then increased exponentially to 20, 40, 80, and 160 seconds, and finally lim-&#10;ited to 300 seconds. Once the interval hits the 300-second limit, the Kubelet keeps&#10;restarting the container indefinitely every five minutes until the pod either stops&#10;crashing or is deleted. &#10; To examine why the container crashed, you can check the pod&#8217;s log and/or use&#10;the kubectl describe pod command, as shown in the following listing.&#10;$ kubectl describe pod&#10;Name:       memoryhog&#10;...&#10;Containers:&#10;  main:&#10;    ...&#10;    State:          Terminated          &#10;      Reason:       OOMKilled           &#10;      Exit Code:    137&#10;      Started:      Tue, 27 Dec 2016 14:55:53 +0100&#10;      Finished:     Tue, 27 Dec 2016 14:55:58 +0100&#10;    Last State:     Terminated            &#10;      Reason:       OOMKilled             &#10;      Exit Code:    137&#10;Listing 14.8&#10;Inspecting why a container terminated with kubectl describe pod&#10;The current container was &#10;killed because it was out &#10;of memory (OOM).&#10;The previous container &#10;was also killed because &#10;it was  OOM&#10; &#10;"
    color "green"
  ]
  node [
    id 718
    label "447"
    title "Page_447"
    color "blue"
  ]
  node [
    id 719
    label "text_358"
    title "415&#10;Limiting resources available to a container&#10;      Started:      Tue, 27 Dec 2016 14:55:37 +0100&#10;      Finished:     Tue, 27 Dec 2016 14:55:50 +0100&#10;    Ready:          False&#10;...&#10;The OOMKilled status tells you that the container was killed because it was out of mem-&#10;ory. In the previous listing, the container went over its memory limit and was killed&#10;immediately. &#10; It&#8217;s important not to set memory limits too low if you don&#8217;t want your container to&#10;be killed. But containers can get OOMKilled even if they aren&#8217;t over their limit. You&#8217;ll&#10;see why in section 14.3.2, but first, let&#8217;s discuss something that catches most users off-&#10;guard the first time they start specifying limits for their containers.&#10;14.2.3 Understanding how apps in containers see limits&#10;If you haven&#8217;t deployed the pod from listing 14.7, deploy it now:&#10;$ kubectl create -f limited-pod.yaml&#10;pod &#34;limited-pod&#34; created&#10;Now, run the top command in the container, the way you did at the beginning of the&#10;chapter. The command&#8217;s output is shown in the following listing.&#10;$ kubectl exec -it limited-pod top&#10;Mem: 1450980K used, 597504K free, 22012K shrd, 65876K buff, 857552K cached&#10;CPU: 10.0% usr 40.0% sys  0.0% nic 50.0% idle  0.0% io  0.0% irq  0.0% sirq&#10;Load average: 0.17 1.19 2.47 4/503 10&#10;  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND&#10;    1     0 root     R     1192  0.0   1 49.9 dd if /dev/zero of /dev/null&#10;    5     0 root     R     1196  0.0   0  0.0 top&#10;First, let me remind you that the pod&#8217;s CPU limit is set to 1 core and its memory limit&#10;is set to 20 MiB. Now, examine the output of the top command closely. Is there any-&#10;thing that strikes you as odd?&#10; Look at the amount of used and free memory. Those numbers are nowhere near&#10;the 20 MiB you set as the limit for the container. Similarly, you set the CPU limit to&#10;one core and it seems like the main process is using only 50% of the available CPU&#10;time, even though the dd command, when used like you&#8217;re using it, usually uses all the&#10;CPU it has available. What&#8217;s going on?&#10;UNDERSTANDING THAT CONTAINERS ALWAYS SEE THE NODE&#8217;S MEMORY, NOT THE CONTAINER&#8217;S&#10;The top command shows the memory amounts of the whole node the container is&#10;running on. Even though you set a limit on how much memory is available to a con-&#10;tainer, the container will not be aware of this limit. &#10;Listing 14.9&#10;Running the top command in a CPU- and memory-limited container&#10; &#10;"
    color "green"
  ]
  node [
    id 720
    label "448"
    title "Page_448"
    color "blue"
  ]
  node [
    id 721
    label "text_359"
    title "416&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10; This has an unfortunate effect on any application that looks up the amount of&#10;memory available on the system and uses that information to decide how much mem-&#10;ory it wants to reserve. &#10; The problem is visible when running Java apps, especially if you don&#8217;t specify the&#10;maximum heap size for the Java Virtual Machine with the -Xmx option. In that case,&#10;the JVM will set the maximum heap size based on the host&#8217;s total memory instead of&#10;the memory available to the container. When you run your containerized Java apps in&#10;a Kubernetes cluster on your laptop, the problem doesn&#8217;t manifest itself, because the&#10;difference between the memory limits you set for the pod and the total memory avail-&#10;able on your laptop is not that great. &#10; But when you deploy your pod onto a production system, where nodes have much&#10;more physical memory, the JVM may go over the container&#8217;s memory limit you config-&#10;ured and will be OOMKilled. &#10; And if you think setting the -Xmx option properly solves the issue, you&#8217;re wrong,&#10;unfortunately. The -Xmx option only constrains the heap size, but does nothing about&#10;the JVM&#8217;s off-heap memory. Luckily, new versions of Java alleviate that problem by tak-&#10;ing the configured container limits into account.&#10;UNDERSTANDING THAT CONTAINERS ALSO SEE ALL THE NODE&#8217;S CPU CORES&#10;Exactly like with memory, containers will also see all the node&#8217;s CPUs, regardless of&#10;the CPU limits configured for the container. Setting a CPU limit to one core doesn&#8217;t&#10;magically only expose only one CPU core to the container. All the CPU limit does is&#10;constrain the amount of CPU time the container can use. &#10; A container with a one-core CPU limit running on a 64-core CPU will get 1/64th&#10;of the overall CPU time. And even though its limit is set to one core, the container&#8217;s&#10;processes will not run on only one core. At different points in time, its code may be&#10;executed on different cores.&#10; Nothing is wrong with this, right? While that&#8217;s generally the case, at least one sce-&#10;nario exists where this situation is catastrophic.&#10; Certain applications look up the number of CPUs on the system to decide how&#10;many worker threads they should run. Again, such an app will run fine on a develop-&#10;ment laptop, but when deployed on a node with a much bigger number of cores, it&#8217;s&#10;going to spin up too many threads, all competing for the (possibly) limited CPU time.&#10;Also, each thread requires additional memory, causing the apps memory usage to sky-&#10;rocket. &#10; You may want to use the Downward API to pass the CPU limit to the container and&#10;use it instead of relying on the number of CPUs your app can see on the system. You&#10;can also tap into the cgroups system directly to get the configured CPU limit by read-&#10;ing the following files:&#10;&#61601;/sys/fs/cgroup/cpu/cpu.cfs_quota_us&#10;&#61601;/sys/fs/cgroup/cpu/cpu.cfs_period_us&#10; &#10;"
    color "green"
  ]
  node [
    id 722
    label "449"
    title "Page_449"
    color "blue"
  ]
  node [
    id 723
    label "text_360"
    title "417&#10;Understanding pod QoS classes&#10;14.3&#10;Understanding pod QoS classes&#10;We&#8217;ve already mentioned that resource limits can be overcommitted and that a&#10;node can&#8217;t necessarily provide all its pods the amount of resources specified in their&#10;resource limits. &#10; Imagine having two pods, where pod A is using, let&#8217;s say, 90% of the node&#8217;s mem-&#10;ory and then pod B suddenly requires more memory than what it had been using up&#10;to that point and the node can&#8217;t provide the required amount of memory. Which&#10;container should be killed? Should it be pod B, because its request for memory can&#8217;t&#10;be satisfied, or should pod A be killed to free up memory, so it can be provided to&#10;pod B? &#10; Obviously, it depends. Kubernetes can&#8217;t make a proper decision on its own. You&#10;need a way to specify which pods have priority in such cases. Kubernetes does this by&#10;categorizing pods into three Quality of Service (QoS) classes:&#10;&#61601;&#10;BestEffort (the lowest priority)&#10;&#61601;&#10;Burstable&#10;&#61601;&#10;Guaranteed (the highest)&#10;14.3.1 Defining the QoS class for a pod&#10;You might expect these classes to be assignable to pods through a separate field in the&#10;manifest, but they aren&#8217;t. The QoS class is derived from the combination of resource&#10;requests and limits for the pod&#8217;s containers. Here&#8217;s how.&#10;ASSIGNING A POD TO THE BESTEFFORT CLASS&#10;The lowest priority QoS class is the BestEffort class. It&#8217;s assigned to pods that don&#8217;t&#10;have any requests or limits set at all (in any of their containers). This is the QoS class&#10;that has been assigned to all the pods you created in previous chapters. Containers&#10;running in these pods have had no resource guarantees whatsoever. In the worst&#10;case, they may get almost no CPU time at all and will be the first ones killed when&#10;memory needs to be freed for other pods. But because a BestEffort pod has no&#10;memory limits set, its containers may use as much memory as they want, if enough&#10;memory is available.&#10;ASSIGNING A POD TO THE GUARANTEED CLASS&#10;On the other end of the spectrum is the Guaranteed QoS class. This class is given to&#10;pods whose containers&#8217; requests are equal to the limits for all resources. For a pod&#8217;s&#10;class to be Guaranteed, three things need to be true:&#10;&#61601;Requests and limits need to be set for both CPU and memory.&#10;&#61601;They need to be set for each container.&#10;&#61601;They need to be equal (the limit needs to match the request for each resource&#10;in each container).&#10;Because a container&#8217;s resource requests, if not set explicitly, default to the limits,&#10;specifying the limits for all resources (for each container in the pod) is enough for&#10; &#10;"
    color "green"
  ]
  node [
    id 724
    label "450"
    title "Page_450"
    color "blue"
  ]
  node [
    id 725
    label "text_361"
    title "418&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;the pod to be Guaranteed. Containers in those pods get the requested amount of&#10;resources, but cannot consume additional ones (because their limits are no higher&#10;than their requests). &#10;ASSIGNING THE BURSTABLE QOS CLASS TO A POD&#10;In between BestEffort and Guaranteed is the Burstable QoS class. All other pods&#10;fall into this class. This includes single-container pods where the container&#8217;s limits&#10;don&#8217;t match its requests and all pods where at least one container has a resource&#10;request specified, but not the limit. It also includes pods where one container&#8217;s&#10;requests match their limits, but another container has no requests or limits specified.&#10;Burstable pods get the amount of resources they request, but are allowed to use addi-&#10;tional resources (up to the limit) if needed.&#10;UNDERSTANDING HOW THE RELATIONSHIP BETWEEN REQUESTS AND LIMITS DEFINES THE QOS CLASS&#10;All three QoS classes and their relationships with requests and limits are shown in fig-&#10;ure 14.4.&#10;Thinking about what QoS class a pod has can make your head spin, because it involves&#10;multiple containers, multiple resources, and all the possible relationships between&#10;requests and limits. It&#8217;s easier if you start by thinking about QoS at the container level&#10;(although QoS classes are a property of pods, not containers) and then derive the&#10;pod&#8217;s QoS class from the QoS classes of containers. &#10;FIGURING OUT A CONTAINER&#8217;S QOS CLASS&#10;Table 14.1 shows the QoS class based on how resource requests and limits are&#10;defined on a single container. For single-container pods, the QoS class applies to&#10;the pod as well.&#10; &#10;BestEffort&#10;QoS&#10;Requests&#10;Limits&#10;Burstable&#10;QoS&#10;Requests&#10;Limits&#10;Guaranteed&#10;QoS&#10;Requests&#10;Limits&#10;Requests and&#10;limits are not set&#10;Requests are&#10;below limits&#10;Requests&#10;equal limits&#10;Figure 14.4&#10;Resource requests, limits and QoS classes&#10; &#10;"
    color "green"
  ]
  node [
    id 726
    label "451"
    title "Page_451"
    color "blue"
  ]
  node [
    id 727
    label "text_362"
    title "419&#10;Understanding pod QoS classes&#10;NOTE&#10;If only requests are set, but not limits, refer to the table rows where&#10;requests are less than the limits. If only limits are set, requests default to the&#10;limits, so refer to the rows where requests equal limits.&#10;FIGURING OUT THE QOS CLASS OF A POD WITH MULTIPLE CONTAINERS&#10;For multi-container pods, if all the containers have the same QoS class, that&#8217;s also the&#10;pod&#8217;s QoS class. If at least one container has a different class, the pod&#8217;s QoS class is&#10;Burstable, regardless of what the container classes are. Table 14.2 shows how a two-&#10;container pod&#8217;s QoS class relates to the classes of its two containers. You can easily&#10;extend this to pods with more than two containers.&#10;NOTE&#10;A pod&#8217;s QoS class is shown when running kubectl describe pod and&#10;in the pod&#8217;s YAML/JSON manifest in the status.qosClass field.&#10;We&#8217;ve explained how QoS classes are determined, but we still need to look at how they&#10;determine which container gets killed in an overcommitted system.&#10;Table 14.1&#10;The QoS class of a single-container pod based on resource requests and limits&#10;CPU requests vs. limits&#10;Memory requests vs. limits&#10;Container QoS class&#10;None set&#10;None set&#10;BestEffort&#10;None set&#10;Requests < Limits&#10;Burstable&#10;None set&#10;Requests = Limits&#10;Burstable&#10;Requests < Limits&#10;None set&#10;Burstable&#10;Requests < Limits&#10;Requests < Limits&#10;Burstable&#10;Requests < Limits&#10;Requests = Limits&#10;Burstable&#10;Requests = Limits&#10;Requests = Limits&#10;Guaranteed&#10;Table 14.2&#10;A Pod&#8217;s QoS class derived from the classes of its containers&#10;Container 1 QoS class&#10;Container 2 QoS class&#10;Pod&#8217;s QoS class&#10;BestEffort&#10;BestEffort&#10;BestEffort&#10;BestEffort&#10;Burstable&#10;Burstable&#10;BestEffort&#10;Guaranteed&#10;Burstable&#10;Burstable&#10;Burstable&#10;Burstable&#10;Burstable&#10;Guaranteed&#10;Burstable&#10;Guaranteed&#10;Guaranteed&#10;Guaranteed&#10; &#10;"
    color "green"
  ]
  node [
    id 728
    label "452"
    title "Page_452"
    color "blue"
  ]
  node [
    id 729
    label "text_363"
    title "420&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;14.3.2 Understanding which process gets killed when memory is low&#10;When the system is overcommitted, the QoS classes determine which container gets&#10;killed first so the freed resources can be given to higher priority pods. First in line to&#10;get killed are pods in the BestEffort class, followed by Burstable pods, and finally&#10;Guaranteed pods, which only get killed if system processes need memory.&#10;UNDERSTANDING HOW QOS CLASSES LINE UP&#10;Let&#8217;s look at the example shown in figure 14.5. Imagine having two single-container&#10;pods, where the first one has the BestEffort QoS class, and the second one&#8217;s is&#10;Burstable. When the node&#8217;s whole memory is already maxed out and one of the pro-&#10;cesses on the node tries to allocate more memory, the system will need to kill one of&#10;the processes (perhaps even the process trying to allocate additional memory) to&#10;honor the allocation request. In this case, the process running in the BestEffort pod&#10;will always be killed before the one in the Burstable pod.&#10;Obviously, a BestEffort pod&#8217;s process will also be killed before any Guaranteed pods&#8217;&#10;processes are killed. Likewise, a Burstable pod&#8217;s process will also be killed before that&#10;of a Guaranteed pod. But what happens if there are only two Burstable pods? Clearly,&#10;the selection process needs to prefer one over the other.&#10;UNDERSTANDING HOW CONTAINERS WITH THE SAME QOS CLASS ARE HANDLED&#10;Each running process has an OutOfMemory (OOM) score. The system selects the&#10;process to kill by comparing OOM scores of all the running processes. When memory&#10;needs to be freed, the process with the highest score gets killed.&#10; OOM scores are calculated from two things: the percentage of the available mem-&#10;ory the process is consuming and a fixed OOM score adjustment, which is based on the&#10;pod&#8217;s QoS class and the container&#8217;s requested memory. When two single-container pods&#10;exist, both in the Burstable class, the system will kill the one using more of its requested&#10;BestEffort&#10;QoS pod&#10;Pod A&#10;First in line&#10;to be killed&#10;Actual usage&#10;Requests&#10;Limits&#10;Burstable&#10;QoS pod&#10;Pod B&#10;Second in line&#10;to be killed&#10;90% used&#10;Requests&#10;Limits&#10;Burstable&#10;QoS pod&#10;Pod C&#10;Third in line&#10;to be killed&#10;70% used&#10;Requests&#10;Limits&#10;Guaranteed&#10;QoS pod&#10;Pod D&#10;Last to&#10;be killed&#10;99% used&#10;Requests&#10;Limits&#10;Figure 14.5&#10;Which pods get killed first&#10; &#10;"
    color "green"
  ]
  node [
    id 730
    label "453"
    title "Page_453"
    color "blue"
  ]
  node [
    id 731
    label "text_364"
    title "421&#10;Setting default requests and limits for pods per namespace&#10;memory than the other, percentage-wise. That&#8217;s why in figure 14.5, pod B, using 90%&#10;of its requested memory, gets killed before pod C, which is only using 70%, even&#10;though it&#8217;s using more megabytes of memory than pod B. &#10; This shows you need to be mindful of not only the relationship between requests&#10;and limits, but also of requests and the expected actual memory consumption. &#10;14.4&#10;Setting default requests and limits for pods per &#10;namespace&#10;We&#8217;ve looked at how resource requests and limits can be set for each individual con-&#10;tainer. If you don&#8217;t set them, the container is at the mercy of all other containers that&#10;do specify resource requests and limits. It&#8217;s a good idea to set requests and limits on&#10;every container.&#10;14.4.1 Introducing the LimitRange resource&#10;Instead of having to do this for every container, you can also do it by creating a Limit-&#10;Range resource. It allows you to specify (for each namespace) not only the minimum&#10;and maximum limit you can set on a container for each resource, but also the default&#10;resource requests for containers that don&#8217;t specify requests explicitly, as depicted in&#10;figure 14.6.&#10;API server&#10;Validation&#10;Pod A&#10;manifest&#10;- Requests&#10;- Limits&#10;Pod A&#10;manifest&#10;- Requests&#10;- Limits&#10;Pod B&#10;manifest&#10;- No&#10;requests&#10;or limits&#10;Pod B&#10;manifest&#10;- No&#10;requests&#10;or limits&#10;Defaulting&#10;Rejected because&#10;requests and limits are&#10;outside min/max values&#10;Defaults&#10;applied&#10;Namespace XYZ&#10;LimitRange&#10;Pod B&#10;manifest&#10;- Default&#10;requests&#10;- Default&#10;limits&#10;Pod B&#10;- Default requests&#10;- Default limits&#10;- Min/max CPU&#10;- Min/max memory&#10;- Default requests&#10;- Default limits&#10;Figure 14.6&#10;A LimitRange is used for validation and defaulting pods.&#10; &#10;"
    color "green"
  ]
  node [
    id 732
    label "454"
    title "Page_454"
    color "blue"
  ]
  node [
    id 733
    label "text_365"
    title "422&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;LimitRange resources are used by the LimitRanger Admission Control plugin (we&#10;explained what those plugins are in chapter 11). When a pod manifest is posted to the&#10;API server, the LimitRanger plugin validates the pod spec. If validation fails, the mani-&#10;fest is rejected immediately. Because of this, a great use-case for LimitRange objects is&#10;to prevent users from creating pods that are bigger than any node in the cluster. With-&#10;out such a LimitRange, the API server will gladly accept the pod, but then never&#10;schedule it. &#10; The limits specified in a LimitRange resource apply to each individual pod/con-&#10;tainer or other kind of object created in the same namespace as the LimitRange&#10;object. They don&#8217;t limit the total amount of resources available across all the pods in&#10;the namespace. This is specified through ResourceQuota objects, which are explained&#10;in section 14.5. &#10;14.4.2 Creating a LimitRange object&#10;Let&#8217;s look at a full example of a LimitRange and see what the individual properties do.&#10;The following listing shows the full definition of a LimitRange resource.&#10;apiVersion: v1&#10;kind: LimitRange&#10;metadata:&#10;  name: example&#10;spec:&#10;  limits:&#10;  - type: Pod           &#10;    min:                         &#10;      cpu: 50m                   &#10;      memory: 5Mi                &#10;    max:                          &#10;      cpu: 1                      &#10;      memory: 1Gi                 &#10;  - type: Container             &#10;    defaultRequest:             &#10;      cpu: 100m                 &#10;      memory: 10Mi              &#10;    default:                      &#10;      cpu: 200m                   &#10;      memory: 100Mi               &#10;    min:                         &#10;      cpu: 50m                   &#10;      memory: 5Mi                &#10;    max:                         &#10;      cpu: 1                     &#10;      memory: 1Gi                &#10;    maxLimitRequestRatio:         &#10;      cpu: 4                      &#10;      memory: 10                  &#10;Listing 14.10&#10;A LimitRange resource: limits.yaml&#10;Specifies the &#10;limits for a pod &#10;as a whole&#10;Minimum CPU and memory all the &#10;pod&#8217;s containers can request in total&#10;Maximum CPU and memory all the pod&#8217;s &#10;containers can request (and limit)&#10;The&#10;container&#10;limits are&#10;specified&#10;below this&#10;line.&#10;Default requests for CPU and memory &#10;that will be applied to containers that &#10;don&#8217;t specify them explicitly&#10;Default limits for containers &#10;that don&#8217;t specify them&#10;Minimum and maximum &#10;requests/limits that a &#10;container can have&#10;Maximum ratio between &#10;the limit and request &#10;for each resource&#10; &#10;"
    color "green"
  ]
  node [
    id 734
    label "455"
    title "Page_455"
    color "blue"
  ]
  node [
    id 735
    label "text_366"
    title "423&#10;Setting default requests and limits for pods per namespace&#10;  - type: PersistentVolumeClaim      &#10;    min:                             &#10;      storage: 1Gi                   &#10;    max:                             &#10;      storage: 10Gi                  &#10;As you can see from the previous example, the minimum and maximum limits for a&#10;whole pod can be configured. They apply to the sum of all the pod&#8217;s containers&#8217;&#10;requests and limits. &#10; Lower down, at the container level, you can set not only the minimum and maxi-&#10;mum, but also default resource requests (defaultRequest) and default limits&#10;(default) that will be applied to each container that doesn&#8217;t specify them explicitly. &#10; Beside the min, max, and default values, you can even set the maximum ratio of&#10;limits vs. requests. The previous listing sets the CPU maxLimitRequestRatio to 4,&#10;which means a container&#8217;s CPU limits will not be allowed to be more than four times&#10;greater than its CPU requests. A container requesting 200 millicores will not be&#10;accepted if its CPU limit is set to 801 millicores or higher. For memory, the maximum&#10;ratio is set to 10.&#10; In chapter 6 we looked at PersistentVolumeClaims (PVC), which allow you to claim&#10;a certain amount of persistent storage similarly to how a pod&#8217;s containers claim CPU&#10;and memory. In the same way you&#8217;re limiting the minimum and maximum amount of&#10;CPU a container can request, you should also limit the amount of storage a single&#10;PVC can request. A LimitRange object allows you to do that as well, as you can see at&#10;the bottom of the example.&#10; The example shows a single LimitRange object containing limits for everything,&#10;but you could also split them into multiple objects if you prefer to have them orga-&#10;nized per type (one for pod limits, another for container limits, and yet another for&#10;PVCs, for example). Limits from multiple LimitRange objects are all consolidated&#10;when validating a pod or PVC.&#10; Because the validation (and defaults) configured in a LimitRange object is per-&#10;formed by the API server when it receives a new pod or PVC manifest, if you modify&#10;the limits afterwards, existing pods and PVCs will not be revalidated&#8212;the new limits&#10;will only apply to pods and PVCs created afterward. &#10;14.4.3 Enforcing the limits&#10;With your limits in place, you can now try creating a pod that requests more CPU than&#10;allowed by the LimitRange. You&#8217;ll find the YAML for the pod in the code archive. The&#10;next listing only shows the part relevant to the discussion.&#10;    resources:&#10;      requests:&#10;        cpu: 2&#10;Listing 14.11&#10;A pod with CPU requests greater than the limit: limits-pod-too-big.yaml&#10;A LimitRange can also set &#10;the minimum and maximum &#10;amount of storage a PVC &#10;can request.&#10; &#10;"
    color "green"
  ]
  node [
    id 736
    label "456"
    title "Page_456"
    color "blue"
  ]
  node [
    id 737
    label "text_367"
    title "424&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;The pod&#8217;s single container is requesting two CPUs, which is more than the maximum&#10;you set in the LimitRange earlier. Creating the pod yields the following result:&#10;$ kubectl create -f limits-pod-too-big.yaml &#10;Error from server (Forbidden): error when creating &#34;limits-pod-too-big.yaml&#34;: &#10;pods &#34;too-big&#34; is forbidden: [&#10;  maximum cpu usage per Pod is 1, but request is 2., &#10;  maximum cpu usage per Container is 1, but request is 2.]&#10;I&#8217;ve modified the output slightly to make it more legible. The nice thing about the&#10;error message from the server is that it lists all the reasons why the pod was rejected,&#10;not only the first one it encountered. As you can see, the pod was rejected for two rea-&#10;sons: you requested two CPUs for the container, but the maximum CPU limit for a&#10;container is one. Likewise, the pod as a whole requested two CPUs, but the maximum&#10;is one CPU (if this was a multi-container pod, even if each individual container&#10;requested less than the maximum amount of CPU, together they&#8217;d still need to&#10;request less than two CPUs to pass the maximum CPU for pods). &#10;14.4.4 Applying default resource requests and limits&#10;Now let&#8217;s also see how default resource requests and limits are set on containers that&#10;don&#8217;t specify them. Deploy the kubia-manual pod from chapter 3 again:&#10;$ kubectl create -f ../Chapter03/kubia-manual.yaml&#10;pod &#34;kubia-manual&#34; created&#10;Before you set up your LimitRange object, all your pods were created without any&#10;resource requests or limits, but now the defaults are applied automatically when creat-&#10;ing the pod. You can confirm this by describing the kubia-manual pod, as shown in&#10;the following listing.&#10;$ kubectl describe po kubia-manual&#10;Name:           kubia-manual&#10;...&#10;Containers:&#10;  kubia:&#10;    Limits:&#10;      cpu:      200m&#10;      memory:   100Mi&#10;    Requests:&#10;      cpu:      100m&#10;      memory:   10Mi&#10;The container&#8217;s requests and limits match the ones you specified in the LimitRange&#10;object. If you used a different LimitRange specification in another namespace, pods&#10;created in that namespace would obviously have different requests and limits. This&#10;allows admins to configure default, min, and max resources for pods per namespace.&#10;Listing 14.12&#10;Inspecting limits that were applied to a pod automatically&#10; &#10;"
    color "green"
  ]
  node [
    id 738
    label "457"
    title "Page_457"
    color "blue"
  ]
  node [
    id 739
    label "text_368"
    title "425&#10;Limiting the total resources available in a namespace&#10;If namespaces are used to separate different teams or to separate development, QA,&#10;staging, and production pods running in the same Kubernetes cluster, using a differ-&#10;ent LimitRange in each namespace ensures large pods can only be created in certain&#10;namespaces, whereas others are constrained to smaller pods.&#10; But remember, the limits configured in a LimitRange only apply to each individual&#10;pod/container. It&#8217;s still possible to create many pods and eat up all the resources avail-&#10;able in the cluster. LimitRanges don&#8217;t provide any protection from that. A Resource-&#10;Quota object, on the other hand, does. You&#8217;ll learn about them next.&#10;14.5&#10;Limiting the total resources available in a namespace&#10;As you&#8217;ve seen, LimitRanges only apply to individual pods, but cluster admins also&#10;need a way to limit the total amount of resources available in a namespace. This is&#10;achieved by creating a ResourceQuota object. &#10;14.5.1 Introducing the ResourceQuota object&#10;In chapter 10 we said that several Admission Control plugins running inside the API&#10;server verify whether the pod may be created or not. In the previous section, I said&#10;that the LimitRanger plugin enforces the policies configured in LimitRange resources.&#10;Similarly, the ResourceQuota Admission Control plugin checks whether the pod&#10;being created would cause the configured ResourceQuota to be exceeded. If that&#8217;s&#10;the case, the pod&#8217;s creation is rejected. Because resource quotas are enforced at pod&#10;creation time, a ResourceQuota object only affects pods created after the Resource-&#10;Quota object is created&#8212;creating it has no effect on existing pods.&#10; A ResourceQuota limits the amount of computational resources the pods and the&#10;amount of storage PersistentVolumeClaims in a namespace can consume. It can also&#10;limit the number of pods, claims, and other API objects users are allowed to create&#10;inside the namespace. Because you&#8217;ve mostly dealt with CPU and memory so far, let&#8217;s&#10;start by looking at how to specify quotas for them.&#10;CREATING A RESOURCEQUOTA FOR CPU AND MEMORY&#10;The overall CPU and memory all the pods in a namespace are allowed to consume is&#10;defined by creating a ResourceQuota object as shown in the following listing.&#10;apiVersion: v1&#10;kind: ResourceQuota&#10;metadata:&#10;  name: cpu-and-mem&#10;spec:&#10;  hard:&#10;    requests.cpu: 400m&#10;    requests.memory: 200Mi&#10;    limits.cpu: 600m&#10;    limits.memory: 500Mi&#10;Listing 14.13&#10;A ResourceQuota resource for CPU and memory: quota-cpu-memory.yaml&#10; &#10;"
    color "green"
  ]
  node [
    id 740
    label "458"
    title "Page_458"
    color "blue"
  ]
  node [
    id 741
    label "text_369"
    title "426&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;Instead of defining a single total for each resource, you define separate totals for&#10;requests and limits for both CPU and memory. You&#8217;ll notice the structure is a bit dif-&#10;ferent, compared to that of a LimitRange. Here, both the requests and the limits for&#10;all resources are defined in a single place. &#10; This ResourceQuota sets the maximum amount of CPU pods in the namespace&#10;can request to 400 millicores. The maximum total CPU limits in the namespace are&#10;set to 600 millicores. For memory, the maximum total requests are set to 200 MiB,&#10;whereas the limits are set to 500 MiB.&#10; A ResourceQuota object applies to the namespace it&#8217;s created in, like a Limit-&#10;Range, but it applies to all the pods&#8217; resource requests and limits in total and not to&#10;each individual pod or container separately, as shown in figure 14.7.&#10;INSPECTING THE QUOTA AND QUOTA USAGE&#10;After you post the ResourceQuota object to the API server, you can use the kubectl&#10;describe command to see how much of the quota is already used up, as shown in&#10;the following listing.&#10;$ kubectl describe quota&#10;Name:           cpu-and-mem&#10;Namespace:      default&#10;Resource        Used   Hard&#10;--------        ----   ----&#10;limits.cpu      200m   600m&#10;limits.memory   100Mi  500Mi&#10;requests.cpu    100m   400m&#10;requests.memory 10Mi   200Mi&#10;I only have the kubia-manual pod running, so the Used column matches its resource&#10;requests and limits. When I run additional pods, their requests and limits are added to&#10;the used amounts.&#10;Listing 14.14&#10;Inspecting the ResourceQuota with kubectl describe quota&#10;LimitRange&#10;ResourceQuota&#10;Namespace: FOO&#10;Pod A&#10;Pod B&#10;Pod C&#10;LimitRange&#10;ResourceQuota&#10;Namespace: BAR&#10;Pod D&#10;Pod E&#10;Pod F&#10;Figure 14.7&#10;LimitRanges apply to individual pods; ResourceQuotas apply to all pods in the &#10;namespace.&#10; &#10;"
    color "green"
  ]
  node [
    id 742
    label "459"
    title "Page_459"
    color "blue"
  ]
  node [
    id 743
    label "text_370"
    title "427&#10;Limiting the total resources available in a namespace&#10;CREATING A LIMITRANGE ALONG WITH A RESOURCEQUOTA&#10;One caveat when creating a ResourceQuota is that you will also want to create a Limit-&#10;Range object alongside it. In your case, you have a LimitRange configured from the&#10;previous section, but if you didn&#8217;t have one, you couldn&#8217;t run the kubia-manual pod,&#10;because it doesn&#8217;t specify any resource requests or limits. Here&#8217;s what would happen&#10;in that case:&#10;$ kubectl create -f ../Chapter03/kubia-manual.yaml&#10;Error from server (Forbidden): error when creating &#34;../Chapter03/kubia-&#10;manual.yaml&#34;: pods &#34;kubia-manual&#34; is forbidden: failed quota: cpu-and-&#10;mem: must specify limits.cpu,limits.memory,requests.cpu,requests.memory&#10;When a quota for a specific resource (CPU or memory) is configured (request or&#10;limit), pods need to have the request or limit (respectively) set for that same resource;&#10;otherwise the API server will not accept the pod. That&#8217;s why having a LimitRange with&#10;defaults for those resources can make life a bit easier for people creating pods.&#10;14.5.2 Specifying a quota for persistent storage&#10;A ResourceQuota object can also limit the amount of persistent storage that can be&#10;claimed in the namespace, as shown in the following listing.&#10;apiVersion: v1&#10;kind: ResourceQuota&#10;metadata:&#10;  name: storage&#10;spec:&#10;  hard:&#10;    requests.storage: 500Gi                               &#10;    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi     &#10;    standard.storageclass.storage.k8s.io/requests.storage: 1Ti&#10;In this example, the amount of storage all PersistentVolumeClaims in a namespace&#10;can request is limited to 500 GiB (by the requests.storage entry in the Resource-&#10;Quota object). But as you&#8217;ll remember from chapter 6, PersistentVolumeClaims can&#10;request a dynamically provisioned PersistentVolume of a specific StorageClass. That&#8217;s&#10;why Kubernetes also makes it possible to define storage quotas for each StorageClass&#10;individually. The previous example limits the total amount of claimable SSD storage&#10;(designated by the ssd StorageClass) to 300 GiB. The less-performant HDD storage&#10;(StorageClass standard) is limited to 1 TiB.&#10;14.5.3 Limiting the number of objects that can be created&#10;A ResourceQuota can also be configured to limit the number of Pods, Replication-&#10;Controllers, Services, and other objects inside a single namespace. This allows the&#10;cluster admin to limit the number of objects users can create based on their payment&#10;Listing 14.15&#10;A ResourceQuota for storage: quota-storage.yaml&#10;The amount of &#10;storage claimable &#10;overall&#10;The amount &#10;of claimable &#10;storage in &#10;StorageClass ssd&#10; &#10;"
    color "green"
  ]
  node [
    id 744
    label "460"
    title "Page_460"
    color "blue"
  ]
  node [
    id 745
    label "text_371"
    title "428&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;plan, for example, and can also limit the number of public IPs or node ports Ser-&#10;vices can use. &#10; The following listing shows what a ResourceQuota object that limits the number of&#10;objects may look like.&#10;apiVersion: v1&#10;kind: ResourceQuota&#10;metadata:&#10;  name: objects&#10;spec:&#10;  hard:&#10;    pods: 10                        &#10;    replicationcontrollers: 5       &#10;    secrets: 10                     &#10;    configmaps: 10                  &#10;    persistentvolumeclaims: 4       &#10;    services: 5                      &#10;    services.loadbalancers: 1        &#10;    services.nodeports: 2            &#10;    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2   &#10;The ResourceQuota in this listing allows users to create at most 10 Pods in the name-&#10;space, regardless if they&#8217;re created manually or by a ReplicationController, Replica-&#10;Set, DaemonSet, Job, and so on. It also limits the number of ReplicationControllers to&#10;five. A maximum of five Services can be created, of which only one can be a LoadBal-&#10;ancer-type Service, and only two can be NodePort Services. Similar to how the maxi-&#10;mum amount of requested storage can be specified per StorageClass, the number of&#10;PersistentVolumeClaims can also be limited per StorageClass.&#10; Object count quotas can currently be set for the following objects: &#10;&#61601;Pods&#10;&#61601;ReplicationControllers &#10;&#61601;Secrets&#10;&#61601;ConfigMaps&#10;&#61601;PersistentVolumeClaims&#10;&#61601;Services (in general), and for two specific types of Services, such as Load-&#10;Balancer Services (services.loadbalancers) and NodePort Services (ser-&#10;vices.nodeports) &#10;Finally, you can even set an object count quota for ResourceQuota objects themselves.&#10;The number of other objects, such as ReplicaSets, Jobs, Deployments, Ingresses, and&#10;so on, cannot be limited yet (but this may have changed since the book was published,&#10;so please check the documentation for up-to-date information).&#10;Listing 14.16&#10;A ResourceQuota for max number of resources: quota-object-count.yaml&#10;Only 10 Pods, 5 ReplicationControllers, &#10;10 Secrets, 10 ConfigMaps, and &#10;4 PersistentVolumeClaims can be &#10;created in the namespace.&#10;Five Services overall can be created, &#10;of which at most one can be a &#10;LoadBalancer Service and at most &#10;two can be NodePort Services.&#10;Only two PVCs can claim storage&#10;with the ssd StorageClass.&#10; &#10;"
    color "green"
  ]
  node [
    id 746
    label "461"
    title "Page_461"
    color "blue"
  ]
  node [
    id 747
    label "text_372"
    title "429&#10;Limiting the total resources available in a namespace&#10;14.5.4 Specifying quotas for specific pod states and/or QoS classes&#10;The quotas you&#8217;ve created so far have applied to all pods, regardless of their current&#10;state and QoS class. But quotas can also be limited to a set of quota scopes. Four scopes are&#10;currently available: BestEffort, NotBestEffort, Terminating, and NotTerminating. &#10; The BestEffort and NotBestEffort scopes determine whether the quota applies&#10;to pods with the BestEffort QoS class or with one of the other two classes (that is,&#10;Burstable and Guaranteed). &#10; The other two scopes (Terminating and NotTerminating) don&#8217;t apply to pods&#10;that are (or aren&#8217;t) in the process of shutting down, as the name might lead you to&#10;believe. We haven&#8217;t talked about this, but you can specify how long each pod is&#10;allowed to run before it&#8217;s terminated and marked as Failed. This is done by setting&#10;the activeDeadlineSeconds field in the pod spec. This property defines the number&#10;of seconds a pod is allowed to be active on the node relative to its start time before it&#8217;s&#10;marked as Failed and then terminated. The Terminating quota scope applies to pods&#10;that have the activeDeadlineSeconds set, whereas the NotTerminating applies to&#10;those that don&#8217;t. &#10; When creating a ResourceQuota, you can specify the scopes that it applies to. A&#10;pod must match all the specified scopes for the quota to apply to it. Additionally, what&#10;a quota can limit depends on the quota&#8217;s scope. BestEffort scope can only limit the&#10;number of pods, whereas the other three scopes can limit the number of pods,&#10;CPU/memory requests, and CPU/memory limits. &#10; If, for example, you want the quota to apply only to BestEffort, NotTerminating&#10;pods, you can create the ResourceQuota object shown in the following listing.&#10;apiVersion: v1&#10;kind: ResourceQuota&#10;metadata:&#10;  name: besteffort-notterminating-pods&#10;spec:&#10;  scopes:                 &#10;  - BestEffort            &#10;  - NotTerminating        &#10;  hard: &#10;    pods: 4          &#10;This quota ensures that at most four pods exist with the BestEffort QoS class,&#10;which don&#8217;t have an active deadline. If the quota was targeting NotBestEffort pods&#10;instead, you could also specify requests.cpu, requests.memory, limits.cpu, and&#10;limits.memory.&#10;NOTE&#10;Before you move on to the next section of this chapter, please delete&#10;all the ResourceQuota and LimitRange resources you created. You won&#8217;t&#10;Listing 14.17&#10;ResourceQuota for BestEffort/NotTerminating pods: &#10;quota-scoped.yaml&#10;This quota only applies to pods &#10;that have the BestEffort QoS and &#10;don&#8217;t have an active deadline set.&#10;Only four such &#10;pods can exist.&#10; &#10;"
    color "green"
  ]
  node [
    id 748
    label "462"
    title "Page_462"
    color "blue"
  ]
  node [
    id 749
    label "text_373"
    title "430&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;need them anymore and they may interfere with examples in the following&#10;chapters.&#10;14.6&#10;Monitoring pod resource usage&#10;Properly setting resource requests and limits is crucial for getting the most out of your&#10;Kubernetes cluster. If requests are set too high, your cluster nodes will be underuti-&#10;lized and you&#8217;ll be throwing money away. If you set them too low, your apps will be&#10;CPU-starved or even killed by the OOM Killer. How do you find the sweet spot for&#10;requests and limits?&#10; You find it by monitoring the actual resource usage of your containers under the&#10;expected load levels. Once the application is exposed to the public, you should keep&#10;monitoring it and adjust the resource requests and limits if required.&#10;14.6.1 Collecting and retrieving actual resource usages&#10;How does one monitor apps running in Kubernetes? Luckily, the Kubelet itself&#10;already contains an agent called cAdvisor, which performs the basic collection of&#10;resource consumption data for both individual containers running on the node and&#10;the node as a whole. Gathering those statistics centrally for the whole cluster requires&#10;you to run an additional component called Heapster. &#10; Heapster runs as a pod on one of the nodes and is exposed through a regular&#10;Kubernetes Service, making it accessible at a stable IP address. It collects the data&#10;from all cAdvisors in the cluster and exposes it in a single location. Figure 14.8&#10;shows the flow of the metrics data from the pods, through cAdvisor and finally into&#10;Heapster.&#10;Kubelet&#10;cAdvisor&#10;Node 1&#10;Pod&#10;Pod&#10;Kubelet&#10;cAdvisor&#10;Node 2&#10;Pod&#10;Kubelet&#10;cAdvisor&#10;Node X&#10;Pod&#10;Heapster&#10;Each cAdvisor collects metrics from&#10;containers running on its node.&#10;Heapster runs on one of the nodes as a&#10;pod and collects metrics from all nodes.&#10;Figure 14.8&#10;The flow of metrics data into Heapster&#10; &#10;"
    color "green"
  ]
  node [
    id 750
    label "463"
    title "Page_463"
    color "blue"
  ]
  node [
    id 751
    label "text_374"
    title "431&#10;Monitoring pod resource usage&#10;The arrows in the figure show how the metrics data flows. They don&#8217;t show which com-&#10;ponent connects to which to get the data. The pods (or the containers running&#10;therein) don&#8217;t know anything about cAdvisor, and cAdvisor doesn&#8217;t know anything&#10;about Heapster. It&#8217;s Heapster that connects to all the cAdvisors, and it&#8217;s the cAdvisors&#10;that collect the container and node usage data without having to talk to the processes&#10;running inside the pods&#8217; containers.&#10;ENABLING HEAPSTER&#10;If you&#8217;re running a cluster in Google Kubernetes Engine, Heapster is enabled by&#10;default. If you&#8217;re using Minikube, it&#8217;s available as an add-on and can be enabled with&#10;the following command:&#10;$ minikube addons enable heapster&#10;heapster was successfully enabled&#10;To run Heapster manually in other types of Kubernetes clusters, you can refer to&#10;instructions located at https:/&#10;/github.com/kubernetes/heapster. &#10; After enabling Heapster, you&#8217;ll need to wait a few minutes for it to collect metrics&#10;before you can see resource usage statistics for your cluster, so be patient. &#10;DISPLAYING CPU AND MEMORY USAGE FOR CLUSTER NODES&#10;Running Heapster in your cluster makes it possible to obtain resource usages for&#10;nodes and individual pods through the kubectl top command. To see how much&#10;CPU and memory is being used on your nodes, you can run the command shown in&#10;the following listing.&#10;$ kubectl top node&#10;NAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%&#10;minikube   170m         8%        556Mi           27%&#10;This shows the actual, current CPU and memory usage of all the pods running on the&#10;node, unlike the kubectl describe node command, which shows the amount of CPU&#10;and memory requests and limits instead of actual runtime usage data. &#10;DISPLAYING CPU AND MEMORY USAGE FOR INDIVIDUAL PODS&#10;To see how much each individual pod is using, you can use the kubectl top pod com-&#10;mand, as shown in the following listing.&#10;$ kubectl top pod --all-namespaces&#10;NAMESPACE      NAME                             CPU(cores)   MEMORY(bytes)&#10;kube-system    influxdb-grafana-2r2w9           1m           32Mi&#10;kube-system    heapster-40j6d                   0m           18Mi&#10;Listing 14.18&#10;Actual CPU and memory usage of nodes&#10;Listing 14.19&#10;Actual CPU and memory usages of pods&#10; &#10;"
    color "green"
  ]
  node [
    id 752
    label "464"
    title "Page_464"
    color "blue"
  ]
  node [
    id 753
    label "text_375"
    title "432&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;default        kubia-3773182134-63bmb           0m           9Mi&#10;kube-system    kube-dns-v20-z0hq6               1m           11Mi&#10;kube-system    kubernetes-dashboard-r53mc       0m           14Mi&#10;kube-system    kube-addon-manager-minikube      7m           33Mi&#10;The outputs of both these commands are fairly simple, so you probably don&#8217;t need me&#10;to explain them, but I do need to warn you about one thing. Sometimes the top pod&#10;command will refuse to show any metrics and instead print out an error like this:&#10;$ kubectl top pod&#10;W0312 22:12:58.021885   15126 top_pod.go:186] Metrics not available for pod &#10;default/kubia-3773182134-63bmb, age: 1h24m19.021873823s&#10;error: Metrics not available for pod default/kubia-3773182134-63bmb, age: &#10;1h24m19.021873823s&#10;If this happens, don&#8217;t start looking for the cause of the error yet. Relax, wait a while,&#10;and rerun the command&#8212;it may take a few minutes, but the metrics should appear&#10;eventually. The kubectl top command gets the metrics from Heapster, which aggre-&#10;gates the data over a few minutes and doesn&#8217;t expose it immediately. &#10;TIP&#10;To see resource usages across individual containers instead of pods, you&#10;can use the --containers option. &#10;14.6.2 Storing and analyzing historical resource consumption statistics&#10;The top command only shows current resource usages&#8212;it doesn&#8217;t show you how&#10;much CPU or memory your pods consumed throughout the last hour, yesterday, or a&#10;week ago, for example. In fact, both cAdvisor and Heapster only hold resource usage&#10;data for a short window of time. If you want to analyze your pods&#8217; resource consump-&#10;tion over longer time periods, you&#8217;ll need to run additional tools.&#10; When using Google Kubernetes Engine, you can monitor your cluster with Google&#10;Cloud Monitoring, but when you&#8217;re running your own local Kubernetes cluster&#10;(either through Minikube or other means), people usually use InfluxDB for storing&#10;statistics data and Grafana for visualizing and analyzing them. &#10;INTRODUCING INFLUXDB AND GRAFANA&#10;InfluxDB is an open source time-series database ideal for storing application metrics&#10;and other monitoring data. Grafana, also open source, is an analytics and visualization&#10;suite with a nice-looking web console that allows you to visualize the data stored in&#10;InfluxDB and discover how your application&#8217;s resource usage behaves over time (an&#10;example showing three Grafana charts is shown in figure 14.9).&#10; &#10; &#10;"
    color "green"
  ]
  node [
    id 754
    label "465"
    title "Page_465"
    color "blue"
  ]
  node [
    id 755
    label "text_376"
    title "433&#10;Monitoring pod resource usage&#10;RUNNING INFLUXDB AND GRAFANA IN YOUR CLUSTER&#10;Both InfluxDB and Grafana can run as pods. Deploying them is straightforward. All&#10;the necessary manifests are available in the Heapster Git repository at http:/&#10;/github&#10;.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb.&#10; When using Minikube, you don&#8217;t even need to deploy them manually, because&#10;they&#8217;re deployed along with Heapster when you enable the Heapster add-on.&#10;ANALYZING RESOURCE USAGE WITH GRAFANA&#10;To discover how much of each resource your pod requires over time, open the&#10;Grafana web console and explore the predefined dashboards. Generally, you can find&#10;out the URL of Grafana&#8217;s web console with kubectl cluster-info:&#10;$ kubectl cluster-info&#10;...&#10;monitoring-grafana is running at &#10;https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-&#10;system/services/monitoring-grafana&#10;Figure 14.9&#10;Grafana dashboard showing CPU usage across the cluster&#10; &#10;"
    color "green"
  ]
  node [
    id 756
    label "466"
    title "Page_466"
    color "blue"
  ]
  node [
    id 757
    label "text_377"
    title "434&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;When using Minikube, Grafana&#8217;s web console is exposed through a NodePort Service,&#10;so you can open it in your browser with the following command:&#10;$ minikube service monitoring-grafana -n kube-system&#10;Opening kubernetes service kube-system/monitoring-grafana in default &#10;browser...&#10;A new browser window or tab will open and show the Grafana Home screen. On the&#10;right-hand side, you&#8217;ll see a list of dashboards containing two entries:&#10;&#61601;Cluster&#10;&#61601;Pods&#10;To see the resource usage statistics of the nodes, open the Cluster dashboard. There&#10;you&#8217;ll see several charts showing the overall cluster usage, usage by node, and the&#10;individual usage for CPU, memory, network, and filesystem. The charts will not only&#10;show the actual usage, but also the requests and limits for those resources (where&#10;they apply).&#10; If you then switch over to the Pods dashboard, you can examine the resource&#10;usages for each individual pod, again with both requests and limits shown alongside&#10;the actual usage. &#10; Initially, the charts show the statistics for the last 30 minutes, but you can zoom out&#10;and see the data for much longer time periods: days, months, or even years.&#10;USING THE INFORMATION SHOWN IN THE CHARTS&#10;By looking at the charts, you can quickly see if the resource requests or limits you&#8217;ve&#10;set for your pods need to be raised or whether they can be lowered to allow more pods&#10;to fit on your nodes. Let&#8217;s look at an example. Figure 14.10 shows the CPU and mem-&#10;ory charts for a pod.&#10; At the far right of the top chart, you can see the pod is using more CPU than was&#10;requested in the pod&#8217;s manifest. Although this isn&#8217;t problematic when this is the only&#10;pod running on the node, you should keep in mind that a pod is only guaranteed as&#10;much of a resource as it requests through resource requests. Your pod may be running&#10;fine now, but when other pods are deployed to the same node and start using the&#10;CPU, your pod&#8217;s CPU time may be throttled. Because of this, to ensure the pod can&#10;use as much CPU as it needs to at any time, you should raise the CPU resource request&#10;for the pod&#8217;s container.&#10; The bottom chart shows the pod&#8217;s memory usage and request. Here the situation is&#10;the exact opposite. The amount of memory the pod is using is well below what was&#10;requested in the pod&#8217;s spec. The requested memory is reserved for the pod and won&#8217;t&#10;be available to other pods. The unused memory is therefore wasted. You should&#10;decrease the pod&#8217;s memory request to make the memory available to other pods run-&#10;ning on the node. &#10; &#10;"
    color "green"
  ]
  node [
    id 758
    label "467"
    title "Page_467"
    color "blue"
  ]
  node [
    id 759
    label "text_378"
    title "435&#10;Summary&#10;14.7&#10;Summary&#10;This chapter has shown you that you need to consider your pod&#8217;s resource usage and&#10;configure both the resource requests and the limits for your pod to keep everything&#10;running smoothly. The key takeaways from this chapter are&#10;&#61601;Specifying resource requests helps Kubernetes schedule pods across the cluster.&#10;&#61601;Specifying resource limits keeps pods from starving other pods of resources.&#10;&#61601;Unused CPU time is allocated based on containers&#8217; CPU requests.&#10;&#61601;Containers never get killed if they try to use too much CPU, but they are killed&#10;if they try to use too much memory.&#10;&#61601;In an overcommitted system, containers also get killed to free memory for more&#10;important pods, based on the pods&#8217; QoS classes and actual memory usage.&#10;Actual CPU usage is higher&#10;than what was requested.&#10;The application&#8217;s CPU time&#10;will be throttled when other&#10;apps demand more CPU.&#10;You should increase the&#10;CPU request.&#10;Actual memory usage is well&#10;below requested memory.&#10;You&#8217;ve reserved too much&#10;memory for this app. You&#8217;re&#10;wasting memory, because it&#10;won&#8217;t ever be used by this&#10;app and also can&#8217;t be used&#10;by other apps. You should&#10;decrease the memory&#10;request.&#10;CPU request&#10;CPU usage&#10;Memory request&#10;Memory usage&#10;Figure 14.10&#10;CPU and memory usage chart for a pod&#10; &#10;"
    color "green"
  ]
  node [
    id 760
    label "468"
    title "Page_468"
    color "blue"
  ]
  node [
    id 761
    label "text_379"
    title "436&#10;CHAPTER 14&#10;Managing pods&#8217; computational resources&#10;&#61601;You can use LimitRange objects to define the minimum, maximum, and default&#10;resource requests and limits for individual pods.&#10;&#61601;You can use ResourceQuota objects to limit the amount of resources available&#10;to all the pods in a namespace.&#10;&#61601;To know how high to set a pod&#8217;s resource requests and limits, you need to mon-&#10;itor how the pod uses resources over a long-enough time period.&#10;In the next chapter, you&#8217;ll see how these metrics can be used by Kubernetes to auto-&#10;matically scale your pods.&#10; &#10;"
    color "green"
  ]
  node [
    id 762
    label "469"
    title "Page_469"
    color "blue"
  ]
  node [
    id 763
    label "text_380"
    title "437&#10;Automatic scaling&#10;of pods and cluster nodes&#10;Applications running in pods can be scaled out manually by increasing the&#10;replicas field in the ReplicationController, ReplicaSet, Deployment, or other&#10;scalable resource. Pods can also be scaled vertically by increasing their container&#8217;s&#10;resource requests and limits (though this can currently only be done at pod cre-&#10;ation time, not while the pod is running). Although manual scaling is okay for&#10;times when you can anticipate load spikes in advance or when the load changes&#10;gradually over longer periods of time, requiring manual intervention to handle&#10;sudden, unpredictable traffic increases isn&#8217;t ideal. &#10;This chapter covers&#10;&#61601;Configuring automatic horizontal scaling of pods &#10;based on CPU utilization&#10;&#61601;Configuring automatic horizontal scaling of pods &#10;based on custom metrics&#10;&#61601;Understanding why vertical scaling of pods isn&#8217;t &#10;possible yet&#10;&#61601;Understanding automatic horizontal scaling of &#10;cluster nodes&#10; &#10;"
    color "green"
  ]
  node [
    id 764
    label "470"
    title "Page_470"
    color "blue"
  ]
  node [
    id 765
    label "text_381"
    title "438&#10;CHAPTER 15&#10;Automatic scaling of pods and cluster nodes&#10; Luckily, Kubernetes can monitor your pods and scale them up automatically as&#10;soon as it detects an increase in the CPU usage or some other metric. If running on a&#10;cloud infrastructure, it can even spin up additional nodes if the existing ones can&#8217;t&#10;accept any more pods. This chapter will explain how to get Kubernetes to do both pod&#10;and node autoscaling.&#10; The autoscaling feature in Kubernetes was completely rewritten between the 1.6&#10;and the 1.7 version, so be aware you may find outdated information on this subject&#10;online.&#10;15.1&#10;Horizontal pod autoscaling&#10;Horizontal pod autoscaling is the automatic scaling of the number of pod replicas man-&#10;aged by a controller. It&#8217;s performed by the Horizontal controller, which is enabled and&#10;configured by creating a HorizontalPodAutoscaler (HPA) resource. The controller&#10;periodically checks pod metrics, calculates the number of replicas required to meet&#10;the target metric value configured in the HorizontalPodAutoscaler resource, and&#10;adjusts the replicas field on the target resource (Deployment, ReplicaSet, Replication-&#10;Controller, or StatefulSet). &#10;15.1.1 Understanding the autoscaling process&#10;The autoscaling process can be split into three steps:&#10;&#61601;Obtain metrics of all the pods managed by the scaled resource object.&#10;&#61601;Calculate the number of pods required to bring the metrics to (or close to) the&#10;specified target value.&#10;&#61601;Update the replicas field of the scaled resource.&#10;Let&#8217;s examine all three steps next.&#10;OBTAINING POD METRICS&#10;The Autoscaler doesn&#8217;t perform the gathering of the pod metrics itself. It gets the&#10;metrics from a different source. As we saw in the previous chapter, pod and node met-&#10;rics are collected by an agent called cAdvisor, which runs in the Kubelet on each node,&#10;and then aggregated by the cluster-wide component called Heapster. The horizontal&#10;pod autoscaler controller gets the metrics of all the pods by querying Heapster&#10;through REST calls. The flow of metrics data is shown in figure 15.1 (although all the&#10;connections are initiated in the opposite direction).&#10;This implies that Heapster must be running in the cluster for autoscaling to work. If&#10;you&#8217;re using Minikube and were following along in the previous chapter, Heapster&#10;Pod(s)&#10;cAdvisor(s)&#10;Horizontal Pod Autoscaler(s)&#10;Heapster&#10;Figure 15.1&#10;Flow of metrics from the pod(s) to the HorizontalPodAutoscaler(s)&#10; &#10;"
    color "green"
  ]
  node [
    id 766
    label "471"
    title "Page_471"
    color "blue"
  ]
  node [
    id 767
    label "text_382"
    title "439&#10;Horizontal pod autoscaling&#10;should already be enabled in your cluster. If not, make sure to enable the Heapster&#10;add-on before trying out any autoscaling examples.&#10; Although you don&#8217;t need to query Heapster directly, if you&#8217;re interested in doing&#10;so, you&#8217;ll find both the Heapster Pod and the Service it&#8217;s exposed through in the&#10;kube-system namespace. &#10;CALCULATING THE REQUIRED NUMBER OF PODS&#10;Once the Autoscaler has metrics for all the pods belonging to the resource the Auto-&#10;scaler is scaling (the Deployment, ReplicaSet, ReplicationController, or StatefulSet&#10;resource), it can use those metrics to figure out the required number of replicas. It&#10;needs to find the number that will bring the average value of the metric across all&#10;those replicas as close to the configured target value as possible. The input to this cal-&#10;culation is a set of pod metrics (possibly multiple metrics per pod) and the output is a&#10;single integer (the number of pod replicas). &#10; When the Autoscaler is configured to consider only a single metric, calculating the&#10;required replica count is simple. All it takes is summing up the metrics values of all&#10;the pods, dividing that by the target value set on the HorizontalPodAutoscaler&#10;resource, and then rounding it up to the next-larger integer. The actual calculation is&#10;a bit more involved than this, because it also makes sure the Autoscaler doesn&#8217;t thrash&#10;around when the metric value is unstable and changes rapidly. &#10; When autoscaling is based on multiple pod metrics (for example, both CPU usage&#10;and Queries-Per-Second [QPS]), the calculation isn&#8217;t that much more complicated.&#10;The Autoscaler calculates the replica count for each metric individually and then&#10;takes the highest value (for example, if four pods are required to achieve the target&#10;CPU usage, and three pods are required to achieve the target QPS, the Autoscaler will&#10;scale to four pods). Figure 15.2 shows this example.&#10;A look at changes related to how the Autoscaler obtains metrics&#10;Prior to Kubernetes version 1.6, the HorizontalPodAutoscaler obtained the metrics&#10;from Heapster directly. In version 1.8, the Autoscaler can get the metrics through an&#10;aggregated version of the resource metrics API by starting the Controller Manager&#10;with the --horizontal-pod-autoscaler-use-rest-clients=true flag. From ver-&#10;sion 1.9, this behavior will be enabled by default.&#10;The core API server will not expose the metrics itself. From version 1.7, Kubernetes&#10;allows registering multiple API servers and making them appear as a single API&#10;server. This allows it to expose metrics through one of those underlying API servers.&#10;We&#8217;ll explain API server aggregation in the last chapter. &#10;Selecting what metrics collector to use in their clusters will be up to cluster adminis-&#10;trators. A simple translation layer is usually required to expose the metrics in the&#10;appropriate API paths and in the appropriate format.&#10; &#10;"
    color "green"
  ]
  node [
    id 768
    label "472"
    title "Page_472"
    color "blue"
  ]
  node [
    id 769
    label "text_383"
    title "440&#10;CHAPTER 15&#10;Automatic scaling of pods and cluster nodes&#10;UPDATING THE DESIRED REPLICA COUNT ON THE SCALED RESOURCE&#10;The final step of an autoscaling operation is updating the desired replica count field&#10;on the scaled resource object (a ReplicaSet, for example) and then letting the Replica-&#10;Set controller take care of spinning up additional pods or deleting excess ones.&#10; The Autoscaler controller modifies the replicas field of the scaled resource&#10;through the Scale sub-resource. It enables the Autoscaler to do its work without know-&#10;ing any details of the resource it&#8217;s scaling, except for what&#8217;s exposed through the Scale&#10;sub-resource (see figure 15.3).&#10;This allows the Autoscaler to operate on any scalable resource, as long as the API&#10;server exposes the Scale sub-resource for it. Currently, it&#8217;s exposed for&#10;&#61601;Deployments&#10;&#61601;ReplicaSets&#10;&#61601;ReplicationControllers&#10;&#61601;StatefulSets&#10;These are currently the only objects you can attach an Autoscaler to.&#10;Pod 1&#10;CPU&#10;utilization&#10;QPS&#10;Pod 2&#10;Pod 3&#10;Target&#10;CPU utilization&#10;Target QPS&#10;Replicas: 4&#10;Replicas: 3&#10;Replicas: 4&#10;30&#10;12&#10;15&#10;20&#10;(15 + 30 + 12) / 20 = 57 / 20&#10;(60 + 90 + 50) / 50 = 200 / 50&#10;Max(4, 3)&#10;50%&#10;60%&#10;90%&#10;50%&#10;Figure 15.2&#10;Calculating the number of replicas from two metrics&#10;Autoscaler adjusts replicas (++ or --)&#10;Horizontal Pod Autoscaler&#10;Deployment, ReplicaSet,&#10;StatefulSet, or&#10;ReplicationController&#10;Scale&#10;sub-resource&#10;Figure 15.3&#10;The Horizontal Pod Autoscaler modifies only on the Scale sub-resource.&#10; &#10;"
    color "green"
  ]
  node [
    id 770
    label "473"
    title "Page_473"
    color "blue"
  ]
  node [
    id 771
    label "text_384"
    title "441&#10;Horizontal pod autoscaling&#10;UNDERSTANDING THE WHOLE AUTOSCALING PROCESS&#10;You now understand the three steps involved in autoscaling, so let&#8217;s visualize all the&#10;components involved in the autoscaling process. They&#8217;re shown in figure 15.4.&#10;The arrows leading from the pods to the cAdvisors, which continue on to Heapster&#10;and finally to the Horizontal Pod Autoscaler, indicate the direction of the flow of met-&#10;rics data. It&#8217;s important to be aware that each component gets the metrics from the&#10;other components periodically (that is, cAdvisor gets the metrics from the pods in a&#10;continuous loop; the same is also true for Heapster and for the HPA controller). The&#10;end effect is that it takes quite a while for the metrics data to be propagated and a res-&#10;caling action to be performed. It isn&#8217;t immediate. Keep this in mind when you observe&#10;the Autoscaler in action next.&#10;15.1.2 Scaling based on CPU utilization&#10;Perhaps the most important metric you&#8217;ll want to base autoscaling on is the amount of&#10;CPU consumed by the processes running inside your pods. Imagine having a few pods&#10;providing a service. When their CPU usage reaches 100% it&#8217;s obvious they can&#8217;t cope&#10;with the demand anymore and need to be scaled either up (vertical scaling&#8212;increas-&#10;ing the amount of CPU the pods can use) or out (horizontal scaling&#8212;increasing the&#10;number of pods). Because we&#8217;re talking about the horizontal pod autoscaler here,&#10;Autoscaler adjusts&#10;replicas (++ or --)&#10;Heapster collects&#10;metrics from all nodes&#10;cAdvisor collects metrics&#10;from all containers on a node&#10;Deployment&#10;ReplicaSet&#10;Autoscaler collects&#10;metrics from Heapster&#10;Kubelet&#10;cAdvisor&#10;Node 1&#10;Pod&#10;Pod&#10;Kubelet&#10;cAdvisor&#10;Node 2&#10;Pod&#10;Node X&#10;Heapster&#10;Horizontal Pod&#10;Autoscaler&#10;Figure 15.4&#10;How the autoscaler obtains metrics and rescales the target deployment &#10; &#10;"
    color "green"
  ]
  node [
    id 772
    label "474"
    title "Page_474"
    color "blue"
  ]
  node [
    id 773
    label "text_385"
    title "442&#10;CHAPTER 15&#10;Automatic scaling of pods and cluster nodes&#10;we&#8217;re only focusing on scaling out (increasing the number of pods). By doing that,&#10;the average CPU usage should come down. &#10; Because CPU usage is usually unstable, it makes sense to scale out even before the&#10;CPU is completely swamped&#8212;perhaps when the average CPU load across the pods&#10;reaches or exceeds 80%. But 80% of what, exactly?&#10;TIP&#10;Always set the target CPU usage well below 100% (and definitely never&#10;above 90%) to leave enough room for handling sudden load spikes.&#10;As you may remember from the previous chapter, the process running inside a con-&#10;tainer is guaranteed the amount of CPU requested through the resource requests&#10;specified for the container. But at times when no other processes need CPU, the pro-&#10;cess may use all the available CPU on the node. When someone says a pod is consum-&#10;ing 80% of the CPU, it&#8217;s not clear if they mean 80% of the node&#8217;s CPU, 80% of the&#10;pod&#8217;s guaranteed CPU (the resource request), or 80% of the hard limit configured&#10;for the pod through resource limits. &#10; As far as the Autoscaler is concerned, only the pod&#8217;s guaranteed CPU amount (the&#10;CPU requests) is important when determining the CPU utilization of a pod. The Auto-&#10;scaler compares the pod&#8217;s actual CPU consumption and its CPU requests, which&#10;means the pods you&#8217;re autoscaling need to have CPU requests set (either directly or&#10;indirectly through a LimitRange object) for the Autoscaler to determine the CPU uti-&#10;lization percentage.&#10;CREATING A HORIZONTALPODAUTOSCALER BASED ON CPU USAGE&#10;Let&#8217;s see how to create a HorizontalPodAutoscaler now and configure it to scale pods&#10;based on their CPU utilization. You&#8217;ll create a Deployment similar to the one in chap-&#10;ter 9, but as we&#8217;ve discussed, you&#8217;ll need to make sure the pods created by the Deploy-&#10;ment all have the CPU resource requests specified in order to make autoscaling&#10;possible. You&#8217;ll have to add a CPU resource request to the Deployment&#8217;s pod tem-&#10;plate, as shown in the following listing.&#10;apiVersion: extensions/v1beta1&#10;kind: Deployment&#10;metadata:&#10;  name: kubia&#10;spec:&#10;  replicas: 3                &#10;  template:&#10;    metadata:&#10;      name: kubia&#10;      labels:&#10;        app: kubia&#10;    spec:&#10;      containers:&#10;      - image: luksa/kubia:v1     &#10;        name: nodejs&#10;Listing 15.1&#10;Deployment with CPU requests set: deployment.yaml&#10;Manually setting the &#10;(initial) desired number &#10;of replicas to three&#10;Running the &#10;kubia:v1 image&#10; &#10;"
    color "green"
  ]
  node [
    id 774
    label "475"
    title "Page_475"
    color "blue"
  ]
  node [
    id 775
    label "text_386"
    title "443&#10;Horizontal pod autoscaling&#10;        resources:              &#10;          requests:             &#10;            cpu: 100m           &#10;This is a regular Deployment object&#8212;it doesn&#8217;t use autoscaling yet. It will run three&#10;instances of the kubia NodeJS app, with each instance requesting 100 millicores&#10;of CPU. &#10; After creating the Deployment, to enable horizontal autoscaling of its pods, you&#10;need to create a HorizontalPodAutoscaler (HPA) object and point it to the Deploy-&#10;ment. You could prepare and post the YAML manifest for the HPA, but an easier way&#10;exists&#8212;using the kubectl autoscale command:&#10;$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5&#10;deployment &#34;kubia&#34; autoscaled&#10;This creates the HPA object for you and sets the Deployment called kubia as the scal-&#10;ing target. You&#8217;re setting the target CPU utilization of the pods to 30% and specifying&#10;the minimum and maximum number of replicas. The Autoscaler will constantly keep&#10;adjusting the number of replicas to keep their CPU utilization around 30%, but it will&#10;never scale down to less than one or scale up to more than five replicas. &#10;TIP&#10;Always make sure to autoscale Deployments instead of the underlying&#10;ReplicaSets. This way, you ensure the desired replica count is preserved across&#10;application updates (remember that a Deployment creates a new ReplicaSet&#10;for each version). The same rule applies to manual scaling, as well.&#10;Let&#8217;s look at the definition of the HorizontalPodAutoscaler resource to gain a better&#10;understanding of it. It&#8217;s shown in the following listing.&#10;$ kubectl get hpa.v2beta1.autoscaling kubia -o yaml&#10;apiVersion: autoscaling/v2beta1            &#10;kind: HorizontalPodAutoscaler              &#10;metadata:&#10;  name: kubia               &#10;  ...&#10;spec:&#10;  maxReplicas: 5                   &#10;  metrics:                              &#10;  - resource:                           &#10;      name: cpu                         &#10;      targetAverageUtilization: 30      &#10;    type: Resource                      &#10;  minReplicas: 1                   &#10;  scaleTargetRef:                          &#10;    apiVersion: extensions/v1beta1         &#10;    kind: Deployment                       &#10;    name: kubia                            &#10;Listing 15.2&#10;A HorizontalPodAutoscaler YAML definition&#10;Requesting 100 millicores &#10;of CPU per pod&#10;HPA resources are in the &#10;autoscaling API group.&#10;Each HPA has a name (it doesn&#8217;t &#10;need to match the name of the &#10;Deployment as in this case).&#10;The&#10;minimum&#10;and&#10;maximum&#10;number of&#10;replicas&#10;you&#10;specified&#10;You&#8217;d like the Autoscaler to &#10;adjust the number of pods &#10;so they each utilize 30% of &#10;requested CPU.&#10;The target resource &#10;which this Autoscaler &#10;will act upon&#10; &#10;"
    color "green"
  ]
  node [
    id 776
    label "476"
    title "Page_476"
    color "blue"
  ]
  node [
    id 777
    label "text_387"
    title "444&#10;CHAPTER 15&#10;Automatic scaling of pods and cluster nodes&#10;status:&#10;  currentMetrics: []        &#10;  currentReplicas: 3        &#10;  desiredReplicas: 0        &#10;NOTE&#10;Multiple versions of HPA resources exist: the new autoscaling/v2beta1&#10;and the old autoscaling/v1. You&#8217;re requesting the new version here.&#10;SEEING THE FIRST AUTOMATIC RESCALE EVENT&#10;It takes a while for cAdvisor to get the CPU metrics and for Heapster to collect them&#10;before the Autoscaler can take action. During that time, if you display the HPA resource&#10;with kubectl get, the TARGETS column will show <unknown>:&#10;$ kubectl get hpa&#10;NAME      REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS&#10;kubia     Deployment/kubia   <unknown> / 30%   1         5         0       &#10;Because you&#8217;re running three pods that are currently receiving no requests, which&#10;means their CPU usage should be close to zero, you should expect the Autoscaler to&#10;scale them down to a single pod, because even with a single pod, the CPU utilization&#10;will still be below the 30% target. &#10; And sure enough, the autoscaler does exactly that. It soon scales the Deployment&#10;down to a single replica:&#10;$ kubectl get deployment&#10;NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE&#10;kubia     1         1         1            1           23m&#10;Remember, the autoscaler only adjusts the desired replica count on the Deployment.&#10;The Deployment controller then takes care of updating the desired replica count on&#10;the ReplicaSet object, which then causes the ReplicaSet controller to delete two excess&#10;pods, leaving one pod running.&#10; You can use kubectl describe to see more information on the HorizontalPod-&#10;Autoscaler and the operation of the underlying controller, as the following listing shows.&#10;$ kubectl describe hpa&#10;Name:                             kubia&#10;Namespace:                        default&#10;Labels:                           <none>&#10;Annotations:                      <none>&#10;CreationTimestamp:                Sat, 03 Jun 2017 12:59:57 +0200&#10;Reference:                        Deployment/kubia&#10;Metrics:                          ( current / target )&#10;  resource cpu on pods  &#10;  (as a percentage of request):   0% (0) / 30%&#10;Min replicas:                     1&#10;Max replicas:                     5&#10;Listing 15.3&#10;Inspecting a HorizontalPodAutoscaler with kubectl describe&#10;The current status &#10;of the Autoscaler&#10; &#10;"
    color "green"
  ]
  node [
    id 778
    label "477"
    title "Page_477"
    color "blue"
  ]
  node [
    id 779
    label "text_388"
    title "445&#10;Horizontal pod autoscaling&#10;Events:&#10;From                        Reason              Message&#10;----                        ------              ---&#10;horizontal-pod-autoscaler   SuccessfulRescale   New size: 1; reason: All &#10;                                                metrics below target&#10;NOTE&#10;The output has been modified to make it more readable.&#10;Turn your focus to the table of events at the bottom of the listing. You see the horizon-&#10;tal pod autoscaler has successfully rescaled to one replica, because all metrics were&#10;below target. &#10;TRIGGERING A SCALE-UP&#10;You&#8217;ve already witnessed your first automatic rescale event (a scale-down). Now, you&#8217;ll&#10;start sending requests to your pod, thereby increasing its CPU usage, and you should&#10;see the autoscaler detect this and start up additional pods.&#10; You&#8217;ll need to expose the pods through a Service, so you can hit all of them through&#10;a single URL. You may remember that the easiest way to do that is with kubectl expose:&#10;$ kubectl expose deployment kubia --port=80 --target-port=8080&#10;service &#34;kubia&#34; exposed&#10;Before you start hitting your pod(s) with requests, you may want to run the follow-&#10;ing command in a separate terminal to keep an eye on what&#8217;s happening with the&#10;HorizontalPodAutoscaler and the Deployment, as shown in the following listing.&#10;$ watch -n 1 kubectl get hpa,deployment&#10;Every &#10;1.0s: &#10;kubectl &#10;get &#10;hpa,deployment &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10; &#10;NAME        REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS  AGE&#10;hpa/kubia   Deployment/kubia   0% / 30%   1         5         1         45m&#10;NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE&#10;deploy/kubia   1         1         1            1           56m&#10;TIP&#10;List multiple resource types with kubectl get by delimiting them with&#10;a comma. &#10;If you&#8217;re using OSX, you&#8217;ll have to replace the watch command with a loop, manually&#10;run kubectl get periodically, or use kubectl&#8217;s --watch option. But although a plain&#10;kubectl get can show multiple types of resources at once, that&#8217;s not the case when&#10;using the aforementioned --watch option, so you&#8217;ll need to use two terminals if you&#10;want to watch both the HPA and the Deployment objects. &#10; Keep an eye on the state of those two objects while you run a load-generating pod.&#10;You&#8217;ll run the following command in another terminal:&#10;$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox &#10;&#10149; -- sh -c &#34;while true; do wget -O - -q http://kubia.default; done&#34;&#10;Listing 15.4&#10;Watching multiple resources in parallel&#10; &#10;"
    color "green"
  ]
  node [
    id 780
    label "478"
    title "Page_478"
    color "blue"
  ]
  node [
    id 781
    label "text_389"
    title "446&#10;CHAPTER 15&#10;Automatic scaling of pods and cluster nodes&#10;This will run a pod which repeatedly hits the kubia Service. You&#8217;ve seen the -it&#10;option a few times when running the kubectl exec command. As you can see, it can&#10;also be used with kubectl run. It allows you to attach the console to the process,&#10;which will not only show you the process&#8217; output directly, but will also terminate the&#10;process as soon as you press CTRL+C. The --rm option causes the pod to be deleted&#10;afterward, and the --restart=Never option causes kubectl run to create an unman-&#10;aged pod directly instead of through a Deployment object, which you don&#8217;t need.&#10;This combination of options is useful for running commands inside the cluster with-&#10;out having to piggyback on an existing pod. It not only behaves the same as if you&#10;were running the command locally, it even cleans up everything when the command&#10;terminates. &#10;SEEING THE AUTOSCALER SCALE UP THE DEPLOYMENT&#10;As the load-generator pod runs, you&#8217;ll see it initially hitting the single pod. As before,&#10;it takes time for the metrics to be updated, but when they are, you&#8217;ll see the autoscaler&#10;increase the number of replicas. In my case, the pod&#8217;s CPU utilization initially jumped&#10;to 108%, which caused the autoscaler to increase the number of pods to four. The&#10;utilization on the individual pods then decreased to 74% and then stabilized at&#10;around 26%. &#10;NOTE&#10;If the CPU load in your case doesn&#8217;t exceed 30%, try running addi-&#10;tional load-generators.&#10;Again, you can inspect autoscaler events with kubectl describe to see what the&#10;autoscaler has done (only the most important information is shown in the following&#10;listing).&#10;From    Reason              Message&#10;----    ------              -------&#10;h-p-a   SuccessfulRescale   New size: 1; reason: All metrics below target&#10;h-p-a   SuccessfulRescale   New size: 4; reason: cpu resource utilization &#10;                            (percentage of request) above target&#10;Does it strike you as odd that the initial average CPU utilization in my case, when I&#10;only had one pod, was 108%, which is more than 100%? Remember, a container&#8217;s&#10;CPU utilization is the container&#8217;s actual CPU usage divided by its requested CPU. The&#10;requested CPU defines the minimum, not maximum amount of CPU available to the&#10;container, so a container may consume more than the requested CPU, bringing the&#10;percentage over 100. &#10; Before we go on, let&#8217;s do a little math and see how the autoscaler concluded that&#10;four replicas are needed. Initially, there was one replica handling requests and its&#10;CPU usage spiked to 108%. Dividing 108 by 30 (the target CPU utilization percent-&#10;age) gives 3.6, which the autoscaler then rounded up to 4. If you divide 108 by 4, you&#10;Listing 15.5&#10;Events of a HorizontalPodAutoscaler&#10; &#10;"
    color "green"
  ]
  node [
    id 782
    label "479"
    title "Page_479"
    color "blue"
  ]
  node [
    id 783
    label "text_390"
    title "447&#10;Horizontal pod autoscaling&#10;get 27%. If the autoscaler scales up to four pods, their average CPU utilization is&#10;expected to be somewhere in the neighborhood of 27%, which is close to the target&#10;value of 30% and almost exactly what the observed CPU utilization was.&#10;UNDERSTANDING THE MAXIMUM RATE OF SCALING&#10;In my case, the CPU usage shot up to 108%, but in general, the initial CPU usage&#10;could spike even higher. Even if the initial average CPU utilization was higher (say&#10;150%), requiring five replicas to achieve the 30% target, the autoscaler would still&#10;only scale up to four pods in the first step, because it has a limit on how many repli-&#10;cas can be added in a single scale-up operation. The autoscaler will at most double&#10;the number of replicas in a single operation, if more than two current replicas&#10;exist. If only one or two exist, it will scale up to a maximum of four replicas in a sin-&#10;gle step. &#10; Additionally, it has a limit on how soon a subsequent autoscale operation can&#10;occur after the previous one. Currently, a scale-up will occur only if no rescaling&#10;event occurred in the last three minutes. A scale-down event is performed even less&#10;frequently&#8212;every five minutes. Keep this in mind so you don&#8217;t wonder why the&#10;autoscaler refuses to perform a rescale operation even if the metrics clearly show&#10;that it should.&#10;MODIFYING THE TARGET METRIC VALUE ON AN EXISTING HPA OBJECT&#10;To wrap up this section, let&#8217;s do one last exercise. Maybe your initial CPU utilization&#10;target of 30% was a bit too low, so increase it to 60%. You do this by editing the HPA&#10;resource with the kubectl edit command. When the text editor opens, change the&#10;targetAverageUtilization field to 60, as shown in the following listing.&#10;...&#10;spec:&#10;  maxReplicas: 5&#10;  metrics:&#10;  - resource:&#10;      name: cpu&#10;      targetAverageUtilization: 60    &#10;    type: Resource&#10;...&#10;As with most other resources, after you modify the resource, your changes will be&#10;detected by the autoscaler controller and acted upon. You could also delete the&#10;resource and recreate it with different target values, because by deleting the HPA&#10;resource, you only disable autoscaling of the target resource (a Deployment in this&#10;case) and leave it at the scale it is at that time. The automatic scaling will resume after&#10;you create a new HPA resource for the Deployment.&#10;Listing 15.6&#10;Increasing the target CPU utilization by editing the HPA resource&#10;Change this &#10;from 30 to 60.&#10; &#10;"
    color "green"
  ]
  node [
    id 784
    label "480"
    title "Page_480"
    color "blue"
  ]
  node [
    id 785
    label "text_391"
    title "448&#10;CHAPTER 15&#10;Automatic scaling of pods and cluster nodes&#10;15.1.3 Scaling based on memory consumption&#10;You&#8217;ve seen how easily the horizontal Autoscaler can be configured to keep CPU uti-&#10;lization at the target level. But what about autoscaling based on the pods&#8217; memory&#10;usage? &#10; Memory-based autoscaling is much more problematic than CPU-based autoscal-&#10;ing. The main reason is because after scaling up, the old pods would somehow need to&#10;be forced to release memory. This needs to be done by the app itself&#8212;it can&#8217;t be done&#10;by the system. All the system could do is kill and restart the app, hoping it would use&#10;less memory than before. But if the app then uses the same amount as before, the&#10;Autoscaler would scale it up again. And again, and again, until it reaches the maxi-&#10;mum number of pods configured on the HPA resource. Obviously, this isn&#8217;t what any-&#10;one wants. Memory-based autoscaling was introduced in Kubernetes version 1.8, and&#10;is configured exactly like CPU-based autoscaling. Exploring it is left up to the reader.&#10;15.1.4 Scaling based on other and custom metrics&#10;You&#8217;ve seen how easy it is to scale pods based on their CPU usage. Initially, this was the&#10;only autoscaling option that was usable in practice. To have the autoscaler use custom,&#10;app-defined metrics to drive its autoscaling decisions was fairly complicated. The ini-&#10;tial design of the autoscaler didn&#8217;t make it easy to move beyond simple CPU-based&#10;scaling. This prompted the Kubernetes Autoscaling Special Interest Group (SIG) to&#10;redesign the autoscaler completely. &#10; If you&#8217;re interested in learning how complicated it was to use the initial autoscaler&#10;with custom metrics, I invite you to read my blog post entitled &#8220;Kubernetes autoscal-&#10;ing based on custom metrics without using a host port,&#8221; which you&#8217;ll find online at&#10;http:/&#10;/medium.com/@marko.luksa. You&#8217;ll learn about all the other problems I&#10;encountered when trying to set up autoscaling based on custom metrics. Luckily,&#10;newer versions of Kubernetes don&#8217;t have those problems. I&#8217;ll cover the subject in a&#10;new blog post. &#10; Instead of going through a complete example here, let&#8217;s quickly go over how to&#10;configure the autoscaler to use different metrics sources. We&#8217;ll start by examining how&#10;we defined what metric to use in our previous example. The following listing shows&#10;how your previous HPA object was configured to use the CPU usage metric.&#10;...&#10;spec:&#10;  maxReplicas: 5&#10;  metrics:&#10;  - type: Resource      &#10;    resource:&#10;      name: cpu                      &#10;      targetAverageUtilization: 30    &#10;...&#10;Listing 15.7&#10;HorizontalPodAutoscaler definition for CPU-based autoscaling&#10;Defines the type &#10;of metric&#10;The resource, whose &#10;utilization will be monitored&#10;The target utilization &#10;of this resource&#10; &#10;"
    color "green"
  ]
  node [
    id 786
    label "481"
    title "Page_481"
    color "blue"
  ]
  node [
    id 787
    label "text_392"
    title "449&#10;Horizontal pod autoscaling&#10;As you can see, the metrics field allows you to define more than one metric to use.&#10;In the listing, you&#8217;re using a single metric. Each entry defines the type of metric&#8212;&#10;in this case, a Resource metric. You have three types of metrics you can use in an&#10;HPA object:&#10;&#61601;&#10;Resource&#10;&#61601;&#10;Pods&#10;&#61601;&#10;Object&#10;UNDERSTANDING THE RESOURCE METRIC TYPE&#10;The Resource type makes the autoscaler base its autoscaling decisions on a resource&#10;metric, like the ones specified in a container&#8217;s resource requests. We&#8217;ve already seen&#10;how to do that, so let&#8217;s focus on the other two types.&#10;UNDERSTANDING THE PODS METRIC TYPE&#10;The Pods type is used to refer to any other (including custom) metric related to the&#10;pod directly. An example of such a metric could be the already mentioned Queries-&#10;Per-Second (QPS) or the number of messages in a message broker&#8217;s queue (when the&#10;message broker is running as a pod). To configure the autoscaler to use the pod&#8217;s QPS&#10;metric, the HPA object would need to include the entry shown in the following listing&#10;under its metrics field.&#10;...&#10;spec:&#10;  metrics:&#10;  - type: Pods              &#10;    resource:&#10;      metricName: qps             &#10;      targetAverageValue: 100    &#10;...&#10;The example in the listing configures the autoscaler to keep the average QPS of all&#10;the pods managed by the ReplicaSet (or other) controller targeted by this HPA&#10;resource at 100. &#10;UNDERSTANDING THE OBJECT METRIC TYPE&#10;The Object metric type is used when you want to make the autoscaler scale pods&#10;based on a metric that doesn&#8217;t pertain directly to those pods. For example, you may&#10;want to scale pods according to a metric of another cluster object, such as an Ingress&#10;object. The metric could be QPS as in listing 15.8, the average request latency, or&#10;something else completely. &#10; Unlike in the previous case, where the autoscaler needed to obtain the metric for&#10;all targeted pods and then use the average of those values, when you use an Object&#10;metric type, the autoscaler obtains a single metric from the single object. In the HPA&#10;Listing 15.8&#10;Referring to a custom pod metric in the HPA&#10;Defines a pod metric&#10;The name of &#10;the metric&#10;The target average value &#10;across all targeted pods&#10; &#10;"
    color "green"
  ]
  node [
    id 788
    label "482"
    title "Page_482"
    color "blue"
  ]
  node [
    id 789
    label "text_393"
    title "450&#10;CHAPTER 15&#10;Automatic scaling of pods and cluster nodes&#10;definition, you need to specify the target object and the target value. The following&#10;listing shows an example.&#10;...&#10;spec:&#10;  metrics:&#10;  - type: Object                   &#10;    resource:&#10;      metricName: latencyMillis           &#10;      target: &#10;        apiVersion: extensions/v1beta1     &#10;        kind: Ingress                      &#10;        name: frontend                     &#10;      targetValue: 20                   &#10;  scaleTargetRef:                          &#10;    apiVersion: extensions/v1beta1         &#10;    kind: Deployment                       &#10;    name: kubia                            &#10;...&#10;In this example, the HPA is configured to use the latencyMillis metric of the&#10;frontend Ingress object. The target value for the metric is 20. The horizontal pod&#10;autoscaler will monitor the Ingress&#8217; metric and if it rises too far above the target value,&#10;the autoscaler will scale the kubia Deployment resource. &#10;15.1.5 Determining which metrics are appropriate for autoscaling&#10;You need to understand that not all metrics are appropriate for use as the basis of&#10;autoscaling. As mentioned previously, the pods&#8217; containers&#8217; memory consumption isn&#8217;t&#10;a good metric for autoscaling. The autoscaler won&#8217;t function properly if increasing&#10;the number of replicas doesn&#8217;t result in a linear decrease of the average value of the&#10;observed metric (or at least close to linear). &#10; For example, if you have only a single pod instance and the value of the metric is X&#10;and the autoscaler scales up to two replicas, the metric needs to fall to somewhere&#10;close to X/2. An example of such a custom metric is Queries per Second (QPS),&#10;which in the case of web applications reports the number of requests the application&#10;is receiving per second. Increasing the number of replicas will always result in a pro-&#10;portionate decrease of QPS, because a greater number of pods will be handling the&#10;same total number of requests. &#10; Before you decide to base the autoscaler on your app&#8217;s own custom metric, be sure&#10;to think about how its value will behave when the number of pods increases or&#10;decreases.&#10;15.1.6 Scaling down to zero replicas&#10;The horizontal pod autoscaler currently doesn&#8217;t allow setting the minReplicas field&#10;to 0, so the autoscaler will never scale down to zero, even if the pods aren&#8217;t doing&#10;Listing 15.9&#10;Referring to a metric of a different object in the HPA&#10;Use metric of a &#10;specific object&#10;The name of &#10;the metric&#10;The specific object whose metric &#10;the autoscaler should obtain&#10;The&#10;Autoscaler&#10;should&#10;scale so&#10;the value&#10;of the&#10;metric&#10;stays close&#10;to this.&#10;The scalable resource the &#10;autoscaler will scale&#10; &#10;"
    color "green"
  ]
  node [
    id 790
    label "483"
    title "Page_483"
    color "blue"
  ]
  node [
    id 791
    label "text_394"
    title "451&#10;Vertical pod autoscaling&#10;anything. Allowing the number of pods to be scaled down to zero can dramatically&#10;increase the utilization of your hardware. When you run services that get requests only&#10;once every few hours or even days, it doesn&#8217;t make sense to have them running all the&#10;time, eating up resources that could be used by other pods. But you still want to have&#10;those services available immediately when a client request comes in. &#10; This is known as idling and un-idling. It allows pods that provide a certain service&#10;to be scaled down to zero. When a new request comes in, the request is blocked until&#10;the pod is brought up and then the request is finally forwarded to the pod. &#10; Kubernetes currently doesn&#8217;t provide this feature yet, but it will eventually. Check&#10;the documentation to see if idling has been implemented yet. &#10;15.2&#10;Vertical pod autoscaling&#10;Horizontal scaling is great, but not every application can be scaled horizontally. For&#10;such applications, the only option is to scale them vertically&#8212;give them more CPU&#10;and/or memory. Because a node usually has more resources than a single pod&#10;requests, it should almost always be possible to scale a pod vertically, right? &#10; Because a pod&#8217;s resource requests are configured through fields in the pod&#10;manifest, vertically scaling a pod would be performed by changing those fields. I&#10;say &#8220;would&#8221; because it&#8217;s currently not possible to change either resource requests&#10;or limits of existing pods. Before I started writing the book (well over a year ago), I&#10;was sure that by the time I wrote this chapter, Kubernetes would already support&#10;proper vertical pod autoscaling, so I included it in my proposal for the table of con-&#10;tents. Sadly, what seems like a lifetime later, vertical pod autoscaling is still not&#10;available yet. &#10;15.2.1 Automatically configuring resource requests&#10;An experimental feature sets the CPU and memory requests on newly created pods, if&#10;their containers don&#8217;t have them set explicitly. The feature is provided by an Admission&#10;Control plugin called InitialResources. When a new pod without resource requests is&#10;created, the plugin looks at historical resource usage data of the pod&#8217;s containers (per&#10;the underlying container image and tag) and sets the requests accordingly. &#10; You can deploy pods without specifying resource requests and rely on Kubernetes&#10;to eventually figure out what each container&#8217;s resource needs are. Effectively, Kuber-&#10;netes is vertically scaling the pod. For example, if a container keeps running out of&#10;memory, the next time a pod with that container image is created, its resource request&#10;for memory will be set higher automatically.&#10;15.2.2 Modifying resource requests while a pod is running&#10;Eventually, the same mechanism will be used to modify an existing pod&#8217;s resource&#10;requests, which means it will vertically scale the pod while it&#8217;s running. As I&#8217;m writing&#10;this, a new vertical pod autoscaling proposal is being finalized. Please refer to the&#10; &#10;"
    color "green"
  ]
  node [
    id 792
    label "484"
    title "Page_484"
    color "blue"
  ]
  node [
    id 793
    label "text_395"
    title "452&#10;CHAPTER 15&#10;Automatic scaling of pods and cluster nodes&#10;Kubernetes documentation to find out whether vertical pod autoscaling is already&#10;implemented or not.&#10;15.3&#10;Horizontal scaling of cluster nodes&#10;The Horizontal Pod Autoscaler creates additional pod instances when the need for&#10;them arises. But what about when all your nodes are at capacity and can&#8217;t run any&#10;more pods? Obviously, this problem isn&#8217;t limited only to when new pod instances are&#10;created by the Autoscaler. Even when creating pods manually, you may encounter the&#10;problem where none of the nodes can accept the new pods, because the node&#8217;s&#10;resources are used up by existing pods. &#10; In that case, you&#8217;d need to delete several of those existing pods, scale them down&#10;vertically, or add additional nodes to your cluster. If your Kubernetes cluster is run-&#10;ning on premises, you&#8217;d need to physically add a new machine and make it part of the&#10;Kubernetes cluster. But if your cluster is running on a cloud infrastructure, adding&#10;additional nodes is usually a matter of a few clicks or an API call to the cloud infra-&#10;structure. This can be done automatically, right?&#10; Kubernetes includes the feature to automatically request additional nodes from&#10;the cloud provider as soon as it detects additional nodes are needed. This is per-&#10;formed by the Cluster Autoscaler.&#10;15.3.1 Introducing the Cluster Autoscaler&#10;The Cluster Autoscaler takes care of automatically provisioning additional nodes&#10;when it notices a pod that can&#8217;t be scheduled to existing nodes because of a lack of&#10;resources on those nodes. It also de-provisions nodes when they&#8217;re underutilized for&#10;longer periods of time. &#10;REQUESTING ADDITIONAL NODES FROM THE CLOUD INFRASTRUCTURE&#10;A new node will be provisioned if, after a new pod is created, the Scheduler can&#8217;t&#10;schedule it to any of the existing nodes. The Cluster Autoscaler looks out for such&#10;pods and asks the cloud provider to start up an additional node. But before doing&#10;that, it checks whether the new node can even accommodate the pod. After all, if&#10;that&#8217;s not the case, it makes no sense to start up such a node.&#10; Cloud providers usually group nodes into groups (or pools) of same-sized nodes&#10;(or nodes having the same features). The Cluster Autoscaler thus can&#8217;t simply say&#10;&#8220;Give me an additional node.&#8221; It needs to also specify the node type.&#10; The Cluster Autoscaler does this by examining the available node groups to see if&#10;at least one node type would be able to fit the unscheduled pod. If exactly one such&#10;node group exists, the Autoscaler can increase the size of the node group to have the&#10;cloud provider add another node to the group. If more than one option is available,&#10;the Autoscaler must pick the best one. The exact meaning of &#8220;best&#8221; will obviously&#10;need to be configurable. In the worst case, it selects a random one. A simple overview&#10;of how the cluster Autoscaler reacts to an unschedulable pod is shown in figure 15.5.&#10; &#10;"
    color "green"
  ]
  node [
    id 794
    label "485"
    title "Page_485"
    color "blue"
  ]
  node [
    id 795
    label "text_396"
    title "453&#10;Horizontal scaling of cluster nodes&#10;When the new node starts up, the Kubelet on that node contacts the API server and&#10;registers the node by creating a Node resource. From then on, the node is part of the&#10;Kubernetes cluster and pods can be scheduled to it.&#10; Simple, right? What about scaling down?&#10;RELINQUISHING NODES&#10;The Cluster Autoscaler also needs to scale down the number of nodes when they&#10;aren&#8217;t being utilized enough. The Autoscaler does this by monitoring the requested&#10;CPU and memory on all the nodes. If the CPU and memory requests of all the pods&#10;running on a given node are below 50%, the node is considered unnecessary. &#10; That&#8217;s not the only determining factor in deciding whether to bring a node down.&#10;The Autoscaler also checks to see if any system pods are running (only) on that node&#10;(apart from those that are run on every node, because they&#8217;re deployed by a Daemon-&#10;Set, for example). If a system pod is running on a node, the node won&#8217;t be relinquished.&#10;The same is also true if an unmanaged pod or a pod with local storage is running on the&#10;node, because that would cause disruption to the service the pod is providing. In other&#10;words, a node will only be returned to the cloud provider if the Cluster Autoscaler&#10;knows the pods running on the node will be rescheduled to other nodes.&#10; When a node is selected to be shut down, the node is first marked as unschedula-&#10;ble and then all the pods running on the node are evicted. Because all those pods&#10;belong to ReplicaSets or other controllers, their replacements are created and sched-&#10;uled to the remaining nodes (that&#8217;s why the node that&#8217;s being shut down is first&#10;marked as unschedulable).&#10;Node group X&#10;Node X1&#10;1. Autoscaler notices a&#10;Pod can&#8217;t be scheduled&#10;to existing nodes&#10;3. Autoscaler scales up the&#10;node group selected in&#10;previous step&#10;2. Autoscaler determines which node&#10;type (if any) would be able to &#64257;t the&#10;pod. If multiple types could &#64257;t the&#10;pod, it selects one of them.&#10;Cluster&#10;Autoscaler&#10;Pods&#10;Node X2&#10;Pods&#10;Node group Y&#10;Node Y1&#10;Pods&#10;Unschedulable&#10;pod&#10;Figure 15.5&#10;The Cluster Autoscaler scales up when it finds a pod that can&#8217;t be scheduled to &#10;existing nodes.&#10; &#10;"
    color "green"
  ]
  node [
    id 796
    label "486"
    title "Page_486"
    color "blue"
  ]
  node [
    id 797
    label "text_397"
    title "454&#10;CHAPTER 15&#10;Automatic scaling of pods and cluster nodes&#10;15.3.2 Enabling the Cluster Autoscaler&#10;Cluster autoscaling is currently available on&#10;&#61601;Google Kubernetes Engine (GKE)&#10;&#61601;Google Compute Engine (GCE)&#10;&#61601;Amazon Web Services (AWS)&#10;&#61601;Microsoft Azure&#10;How you start the Autoscaler depends on where your Kubernetes cluster is running.&#10;For your kubia cluster running on GKE, you can enable the Cluster Autoscaler like&#10;this:&#10;$ gcloud container clusters update kubia --enable-autoscaling \&#10;  --min-nodes=3 --max-nodes=5&#10;If your cluster is running on GCE, you need to set three environment variables before&#10;running kube-up.sh: &#10;&#61601;&#10;KUBE_ENABLE_CLUSTER_AUTOSCALER=true&#10;&#61601;&#10;KUBE_AUTOSCALER_MIN_NODES=3&#10;&#61601;&#10;KUBE_AUTOSCALER_MAX_NODES=5&#10;Refer to the Cluster Autoscaler GitHub repo at https:/&#10;/github.com/kubernetes/auto-&#10;scaler/tree/master/cluster-autoscaler for information on how to enable it on other&#10;platforms. &#10;NOTE&#10;The Cluster Autoscaler publishes its status to the cluster-autoscaler-&#10;status ConfigMap in the kube-system namespace.&#10;15.3.3 Limiting service disruption during cluster scale-down&#10;When a node fails unexpectedly, nothing you can do will prevent its pods from becom-&#10;ing unavailable. But when a node is shut down voluntarily, either by the Cluster Auto-&#10;scaler or by a human operator, you can make sure the operation doesn&#8217;t disrupt the&#10;service provided by the pods running on that node through an additional feature.&#10;Manually cordoning and draining nodes&#10;A node can also be marked as unschedulable and drained manually. Without going&#10;into specifics, this is done with the following kubectl commands:&#10;&#61601;&#10;kubectl cordon <node> marks the node as unschedulable (but doesn&#8217;t do&#10;anything with pods running on that node).&#10;&#61601;&#10;kubectl drain <node> marks the node as unschedulable and then evicts all&#10;the pods from the node.&#10;In both cases, no new pods are scheduled to the node until you uncordon it again&#10;with kubectl uncordon <node>.&#10; &#10;"
    color "green"
  ]
  node [
    id 798
    label "487"
    title "Page_487"
    color "blue"
  ]
  node [
    id 799
    label "text_398"
    title "455&#10;Horizontal scaling of cluster nodes&#10; Certain services require that a minimum number of pods always keeps running;&#10;this is especially true for quorum-based clustered applications. For this reason, Kuber-&#10;netes provides a way of specifying the minimum number of pods that need to keep&#10;running while performing these types of operations. This is done by creating a Pod-&#10;DisruptionBudget resource.&#10; Even though the name of the resource sounds complex, it&#8217;s one of the simplest&#10;Kubernetes resources available. It contains only a pod label selector and a number&#10;specifying the minimum number of pods that must always be available or, starting&#10;from Kubernetes version 1.7, the maximum number of pods that can be unavailable.&#10;We&#8217;ll look at what a PodDisruptionBudget (PDB) resource manifest looks like, but&#10;instead of creating it from a YAML file, you&#8217;ll create it with kubectl create pod-&#10;disruptionbudget and then obtain and examine the YAML later.&#10; If you want to ensure three instances of your kubia pod are always running (they&#10;have the label app=kubia), create the PodDisruptionBudget resource like this:&#10;$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3&#10;poddisruptionbudget &#34;kubia-pdb&#34; created&#10;Simple, right? Now, retrieve the PDB&#8217;s YAML. It&#8217;s shown in the next listing.&#10;$ kubectl get pdb kubia-pdb -o yaml&#10;apiVersion: policy/v1beta1&#10;kind: PodDisruptionBudget&#10;metadata:&#10;  name: kubia-pdb&#10;spec:&#10;  minAvailable: 3         &#10;  selector:                &#10;    matchLabels:           &#10;      app: kubia           &#10;status:&#10;  ...&#10;You can also use a percentage instead of an absolute number in the minAvailable&#10;field. For example, you could state that 60% of all pods with the app=kubia label need&#10;to be running at all times.&#10;NOTE&#10;Starting with Kubernetes 1.7, the PodDisruptionBudget resource also&#10;supports the maxUnavailable field, which you can use instead of min-&#10;Available if you want to block evictions when more than that many pods are&#10;unavailable. &#10;We don&#8217;t have much more to say about this resource. As long as it exists, both the&#10;Cluster Autoscaler and the kubectl drain command will adhere to it and will never&#10;evict a pod with the app=kubia label if that would bring the number of such pods&#10;below three. &#10;Listing 15.10&#10;A PodDisruptionBudget definition&#10;How many pods should &#10;always be available&#10;The label selector that &#10;determines which pods &#10;this budget applies to&#10; &#10;"
    color "green"
  ]
  node [
    id 800
    label "488"
    title "Page_488"
    color "blue"
  ]
  node [
    id 801
    label "text_399"
    title "456&#10;CHAPTER 15&#10;Automatic scaling of pods and cluster nodes&#10; For example, if there were four pods altogether and minAvailable was set to three&#10;as in the example, the pod eviction process would evict pods one by one, waiting for&#10;the evicted pod to be replaced with a new one by the ReplicaSet controller, before&#10;evicting another pod. &#10;15.4&#10;Summary&#10;This chapter has shown you how Kubernetes can scale not only your pods, but also&#10;your nodes. You&#8217;ve learned that&#10;&#61601;Configuring the automatic horizontal scaling of pods is as easy as creating a&#10;HorizontalPodAutoscaler object and pointing it to a Deployment, ReplicaSet,&#10;or ReplicationController and specifying the target CPU utilization for the pods.&#10;&#61601;Besides having the Horizontal Pod Autoscaler perform scaling operations based&#10;on the pods&#8217; CPU utilization, you can also configure it to scale based on your&#10;own application-provided custom metrics or metrics related to other objects&#10;deployed in the cluster.&#10;&#61601;Vertical pod autoscaling isn&#8217;t possible yet.&#10;&#61601;Even cluster nodes can be scaled automatically if your Kubernetes cluster runs&#10;on a supported cloud provider.&#10;&#61601;You can run one-off processes in a pod and have the pod stopped and deleted&#10;automatically as soon you press CTRL+C by using kubectl run with the -it and&#10;--rm options.&#10;In the next chapter, you&#8217;ll explore advanced scheduling features, such as how to keep&#10;certain pods away from certain nodes and how to schedule pods either close together&#10;or apart.&#10; &#10;"
    color "green"
  ]
  node [
    id 802
    label "489"
    title "Page_489"
    color "blue"
  ]
  node [
    id 803
    label "text_400"
    title "457&#10;Advanced scheduling&#10;Kubernetes allows you to affect where pods are scheduled. Initially, this was only&#10;done by specifying a node selector in the pod specification, but additional mech-&#10;anisms were later added that expanded this functionality. They&#8217;re covered in this&#10;chapter.&#10;16.1&#10;Using taints and tolerations to repel pods from &#10;certain nodes&#10;The first two features related to advanced scheduling that we&#8217;ll explore here are&#10;the node taints and pods&#8217; tolerations of those taints. They&#8217;re used for restricting&#10;This chapter covers&#10;&#61601;Using node taints and pod tolerations to keep &#10;pods away from certain nodes&#10;&#61601;Defining node affinity rules as an alternative to &#10;node selectors&#10;&#61601;Co-locating pods using pod affinity &#10;&#61601;Keeping pods away from each other using pod &#10;anti-affinity&#10; &#10;"
    color "green"
  ]
  node [
    id 804
    label "490"
    title "Page_490"
    color "blue"
  ]
  node [
    id 805
    label "text_401"
    title "458&#10;CHAPTER 16&#10;Advanced scheduling&#10;which pods can use a certain node. A pod can only be scheduled to a node if it toler-&#10;ates the node&#8217;s taints.&#10; This is somewhat different from using node selectors and node affinity, which&#10;you&#8217;ll learn about later in this chapter. Node selectors and node affinity rules make&#10;it possible to select which nodes a pod can or can&#8217;t be scheduled to by specifically&#10;adding that information to the pod, whereas taints allow rejecting deployment of&#10;pods to certain nodes by only adding taints to the node without having to modify&#10;existing pods. Pods that you want deployed on a tainted node need to opt in to use&#10;the node, whereas with node selectors, pods explicitly specify which node(s) they&#10;want to be deployed to.&#10;16.1.1 Introducing taints and tolerations&#10;The best path to learn about node taints is to see an existing taint. Appendix B shows&#10;how to set up a multi-node cluster with the kubeadm tool. By default, the master node&#10;in such a cluster is tainted, so only Control Plane pods can be deployed on it. &#10;DISPLAYING A NODE&#8217;S TAINTS&#10;You can see the node&#8217;s taints using kubectl describe node, as shown in the follow-&#10;ing listing.&#10;$ kubectl describe node master.k8s&#10;Name:         master.k8s&#10;Role:&#10;Labels:       beta.kubernetes.io/arch=amd64&#10;              beta.kubernetes.io/os=linux&#10;              kubernetes.io/hostname=master.k8s&#10;              node-role.kubernetes.io/master=&#10;Annotations:  node.alpha.kubernetes.io/ttl=0&#10;              volumes.kubernetes.io/controller-managed-attach-detach=true&#10;Taints:       node-role.kubernetes.io/master:NoSchedule      &#10;...&#10;The master node has a single taint. Taints have a key, value, and an effect, and are repre-&#10;sented as <key>=<value>:<effect>. The master node&#8217;s taint shown in the previous&#10;listing has the key node-role.kubernetes.io/master, a null value (not shown in the&#10;taint), and the effect of NoSchedule. &#10; This taint prevents pods from being scheduled to the master node, unless those pods&#10;tolerate this taint. The pods that tolerate it are usually system pods (see figure 16.1).&#10; &#10; &#10; &#10; &#10;Listing 16.1&#10;Describing the master node in a cluster created with kubeadm&#10;The master node &#10;has one taint.&#10; &#10;"
    color "green"
  ]
  node [
    id 806
    label "491"
    title "Page_491"
    color "blue"
  ]
  node [
    id 807
    label "text_402"
    title "459&#10;Using taints and tolerations to repel pods from certain nodes&#10;DISPLAYING A POD&#8217;S TOLERATIONS&#10;In a cluster installed with kubeadm, the kube-proxy cluster component runs as a pod&#10;on every node, including the master node, because master components that run as&#10;pods may also need to access Kubernetes Services. To make sure the kube-proxy pod&#10;also runs on the master node, it includes the appropriate toleration. In total, the pod&#10;has three tolerations, which are shown in the following listing.&#10;$ kubectl describe po kube-proxy-80wqm -n kube-system&#10;...&#10;Tolerations:    node-role.kubernetes.io/master=:NoSchedule&#10;                node.alpha.kubernetes.io/notReady=:Exists:NoExecute&#10;                node.alpha.kubernetes.io/unreachable=:Exists:NoExecute&#10;...&#10;As you can see, the first toleration matches the master node&#8217;s taint, allowing this kube-&#10;proxy pod to be scheduled to the master node. &#10;NOTE&#10;Disregard the equal sign, which is shown in the pod&#8217;s tolerations, but&#10;not in the node&#8217;s taints. Kubectl apparently displays taints and tolerations dif-&#10;ferently when the taint&#8217;s/toleration&#8217;s value is null.&#10;UNDERSTANDING TAINT EFFECTS&#10;The two other tolerations on the kube-proxy pod define how long the pod is allowed&#10;to run on nodes that aren&#8217;t ready or are unreachable (the time in seconds isn&#8217;t shown,&#10;Listing 16.2&#10;A pod&#8217;s tolerations&#10;System pod may be&#10;scheduled to master&#10;node because its&#10;toleration matches&#10;the node&#8217;s taint.&#10;System pod&#10;Master node&#10;Taint:&#10;node-role.kubernetes.io&#10;/master:NoSchedule&#10;Toleration:&#10;node-role.kubernetes.io&#10;/master:NoSchedule&#10;Regular pod&#10;Regular node&#10;No taints&#10;No tolerations&#10;Pods with no tolerations&#10;may only be scheduled&#10;to nodes without taints.&#10;Figure 16.1&#10;A pod is only scheduled to a node if it tolerates the node&#8217;s taints.&#10; &#10;"
    color "green"
  ]
  node [
    id 808
    label "492"
    title "Page_492"
    color "blue"
  ]
  node [
    id 809
    label "text_403"
    title "460&#10;CHAPTER 16&#10;Advanced scheduling&#10;but can be seen in the pod&#8217;s YAML). Those two tolerations refer to the NoExecute&#10;instead of the NoSchedule effect. &#10; Each taint has an effect associated with it. Three possible effects exist:&#10;&#61601;&#10;NoSchedule, which means pods won&#8217;t be scheduled to the node if they don&#8217;t tol-&#10;erate the taint.&#10;&#61601;&#10;PreferNoSchedule is a soft version of NoSchedule, meaning the scheduler will&#10;try to avoid scheduling the pod to the node, but will schedule it to the node if it&#10;can&#8217;t schedule it somewhere else. &#10;&#61601;&#10;NoExecute, unlike NoSchedule and PreferNoSchedule that only affect schedul-&#10;ing, also affects pods already running on the node. If you add a NoExecute taint&#10;to a node, pods that are already running on that node and don&#8217;t tolerate the&#10;NoExecute taint will be evicted from the node. &#10;16.1.2 Adding custom taints to a node&#10;Imagine having a single Kubernetes cluster where you run both production and non-&#10;production workloads. It&#8217;s of the utmost importance that non-production pods never&#10;run on the production nodes. This can be achieved by adding a taint to your produc-&#10;tion nodes. To add a taint, you use the kubectl taint command:&#10;$ kubectl taint node node1.k8s node-type=production:NoSchedule&#10;node &#34;node1.k8s&#34; tainted&#10;This adds a taint with key node-type, value production and the NoSchedule effect. If&#10;you now deploy multiple replicas of a regular pod, you&#8217;ll see none of them are sched-&#10;uled to the node you tainted, as shown in the following listing.&#10;$ kubectl run test --image busybox --replicas 5 -- sleep 99999&#10;deployment &#34;test&#34; created&#10;$ kubectl get po -o wide&#10;NAME                READY  STATUS    RESTARTS   AGE   IP          NODE&#10;test-196686-46ngl   1/1    Running   0          12s   10.47.0.1   node2.k8s&#10;test-196686-73p89   1/1    Running   0          12s   10.47.0.7   node2.k8s&#10;test-196686-77280   1/1    Running   0          12s   10.47.0.6   node2.k8s&#10;test-196686-h9m8f   1/1    Running   0          12s   10.47.0.5   node2.k8s&#10;test-196686-p85ll   1/1    Running   0          12s   10.47.0.4   node2.k8s&#10;Now, no one can inadvertently deploy pods onto the production nodes. &#10;16.1.3 Adding tolerations to pods&#10;To deploy production pods to the production nodes, they need to tolerate the taint&#10;you added to the nodes. The manifests of your production pods need to include the&#10;YAML snippet shown in the following listing.&#10; &#10;Listing 16.3&#10;Deploying pods without a toleration&#10; &#10;"
    color "green"
  ]
  node [
    id 810
    label "493"
    title "Page_493"
    color "blue"
  ]
  node [
    id 811
    label "text_404"
    title "461&#10;Using taints and tolerations to repel pods from certain nodes&#10;apiVersion: extensions/v1beta1&#10;kind: Deployment&#10;metadata:&#10;  name: prod&#10;spec:&#10;  replicas: 5&#10;  template:&#10;    spec:&#10;      ...&#10;      tolerations:&#10;      - key: node-type         &#10;        Operator: Equal        &#10;        value: production      &#10;        effect: NoSchedule     &#10;If you deploy this Deployment, you&#8217;ll see its pods get deployed to the production&#10;node, as shown in the next listing.&#10;$ kubectl get po -o wide&#10;NAME                READY  STATUS    RESTARTS   AGE   IP          NODE&#10;prod-350605-1ph5h   0/1    Running   0          16s   10.44.0.3   node1.k8s&#10;prod-350605-ctqcr   1/1    Running   0          16s   10.47.0.4   node2.k8s&#10;prod-350605-f7pcc   0/1    Running   0          17s   10.44.0.6   node1.k8s&#10;prod-350605-k7c8g   1/1    Running   0          17s   10.47.0.9   node2.k8s&#10;prod-350605-rp1nv   0/1    Running   0          17s   10.44.0.4   node1.k8s&#10;As you can see in the listing, production pods were also deployed to node2, which isn&#8217;t&#10;a production node. To prevent that from happening, you&#8217;d also need to taint the non-&#10;production nodes with a taint such as node-type=non-production:NoSchedule. Then&#10;you&#8217;d also need to add the matching toleration to all your non-production pods.&#10;16.1.4 Understanding what taints and tolerations can be used for&#10;Nodes can have more than one taint and pods can have more than one toleration. As&#10;you&#8217;ve seen, taints can only have a key and an effect and don&#8217;t require a value. Tolera-&#10;tions can tolerate a specific value by specifying the Equal operator (that&#8217;s also the&#10;default operator if you don&#8217;t specify one), or they can tolerate any value for a specific&#10;taint key if you use the Exists operator.&#10;USING TAINTS AND TOLERATIONS DURING SCHEDULING&#10;Taints can be used to prevent scheduling of new pods (NoSchedule effect) and to&#10;define unpreferred nodes (PreferNoSchedule effect) and even evict existing pods&#10;from a node (NoExecute).&#10; You can set up taints and tolerations any way you see fit. For example, you could&#10;partition your cluster into multiple partitions, allowing your development teams to&#10;schedule pods only to their respective nodes. You can also use taints and tolerations&#10;Listing 16.4&#10;A production Deployment with a toleration: production-deployment.yaml&#10;Listing 16.5&#10;Pods with the toleration are deployed on production node1&#10;This toleration allows the &#10;pod to be scheduled to &#10;production nodes.&#10; &#10;"
    color "green"
  ]
  node [
    id 812
    label "494"
    title "Page_494"
    color "blue"
  ]
  node [
    id 813
    label "text_405"
    title "462&#10;CHAPTER 16&#10;Advanced scheduling&#10;when several of your nodes provide special hardware and only part of your pods need&#10;to use it.&#10;CONFIGURING HOW LONG AFTER A NODE FAILURE A POD IS RESCHEDULED&#10;You can also use a toleration to specify how long Kubernetes should wait before&#10;rescheduling a pod to another node if the node the pod is running on becomes&#10;unready or unreachable. If you look at the tolerations of one of your pods, you&#8217;ll see&#10;two tolerations, which are shown in the following listing.&#10;$ kubectl get po prod-350605-1ph5h -o yaml&#10;...&#10;  tolerations:&#10;  - effect: NoExecute                            &#10;    key: node.alpha.kubernetes.io/notReady       &#10;    operator: Exists                             &#10;    tolerationSeconds: 300                       &#10;  - effect: NoExecute                              &#10;    key: node.alpha.kubernetes.io/unreachable      &#10;    operator: Exists                               &#10;    tolerationSeconds: 300                         &#10;These two tolerations say that this pod tolerates a node being notReady or unreach-&#10;able for 300 seconds. The Kubernetes Control Plane, when it detects that a node is no&#10;longer ready or no longer reachable, will wait for 300 seconds before it deletes the&#10;pod and reschedules it to another node.&#10; These two tolerations are automatically added to pods that don&#8217;t define them. If&#10;that five-minute delay is too long for your pods, you can make the delay shorter by&#10;adding those two tolerations to the pod&#8217;s spec.&#10;NOTE&#10;This is currently an alpha feature, so it may change in future versions&#10;of Kubernetes. Taint-based evictions also aren&#8217;t enabled by default. You enable&#10;them by running the Controller Manager with the --feature-gates=Taint-&#10;BasedEvictions=true option.&#10;16.2&#10;Using node affinity to attract pods to certain nodes&#10;As you&#8217;ve learned, taints are used to keep pods away from certain nodes. Now you&#8217;ll&#10;learn about a newer mechanism called node affinity, which allows you to tell Kuberne-&#10;tes to schedule pods only to specific subsets of nodes.&#10;COMPARING NODE AFFINITY TO NODE SELECTORS&#10;The initial node affinity mechanism in early versions of Kubernetes was the node-&#10;Selector field in the pod specification. The node had to include all the labels speci-&#10;fied in that field to be eligible to become the target for the pod. &#10; Node selectors get the job done and are simple, but they don&#8217;t offer everything&#10;that you may need. Because of that, a more powerful mechanism was introduced.&#10;Listing 16.6&#10;Pod with default tolerations&#10;The pod tolerates the node being &#10;notReady for 300 seconds, before &#10;it needs to be rescheduled.&#10;The same applies to the &#10;node being unreachable.&#10; &#10;"
    color "green"
  ]
  node [
    id 814
    label "495"
    title "Page_495"
    color "blue"
  ]
  node [
    id 815
    label "text_406"
    title "463&#10;Using node affinity to attract pods to certain nodes&#10;Node selectors will eventually be deprecated, so it&#8217;s important you understand the&#10;new node affinity rules.&#10; Similar to node selectors, each pod can define its own node affinity rules. These&#10;allow you to specify either hard requirements or preferences. By specifying a prefer-&#10;ence, you tell Kubernetes which nodes you prefer for a specific pod, and Kubernetes&#10;will try to schedule the pod to one of those nodes. If that&#8217;s not possible, it will choose&#10;one of the other nodes. &#10;EXAMINING THE DEFAULT NODE LABELS&#10;Node affinity selects nodes based on their labels, the same way node selectors do.&#10;Before you see how to use node affinity, let&#8217;s examine the labels of one of the nodes in&#10;a Google Kubernetes Engine cluster (GKE) to see what the default node labels are.&#10;They&#8217;re shown in the following listing.&#10;$ kubectl describe node gke-kubia-default-pool-db274c5a-mjnf&#10;Name:     gke-kubia-default-pool-db274c5a-mjnf&#10;Role:&#10;Labels:   beta.kubernetes.io/arch=amd64&#10;          beta.kubernetes.io/fluentd-ds-ready=true&#10;          beta.kubernetes.io/instance-type=f1-micro&#10;          beta.kubernetes.io/os=linux&#10;          cloud.google.com/gke-nodepool=default-pool&#10;          failure-domain.beta.kubernetes.io/region=europe-west1         &#10;          failure-domain.beta.kubernetes.io/zone=europe-west1-d         &#10;          kubernetes.io/hostname=gke-kubia-default-pool-db274c5a-mjnf   &#10;The node has many labels, but the last three are the most important when it comes to&#10;node affinity and pod affinity, which you&#8217;ll learn about later. The meaning of those&#10;three labels is as follows:&#10;&#61601;&#10;failure-domain.beta.kubernetes.io/region specifies the geographical region&#10;the node is located in.&#10;&#61601;&#10;failure-domain.beta.kubernetes.io/zone specifies the availability zone the&#10;node is in.&#10;&#61601;&#10;kubernetes.io/hostname is obviously the node&#8217;s hostname.&#10;These and other labels can be used in pod affinity rules. In chapter 3, you already&#10;learned how you can add a custom label to nodes and use it in a pod&#8217;s node selector.&#10;You used the custom label to deploy pods only to nodes with that label by adding a node&#10;selector to the pods. Now, you&#8217;ll see how to do the same using node affinity rules.&#10;16.2.1 Specifying hard node affinity rules&#10;In the example in chapter 3, you used the node selector to deploy a pod that requires&#10;a GPU only to nodes that have a GPU. The pod spec included the nodeSelector field&#10;shown in the following listing.&#10;Listing 16.7&#10;Default labels of a node in GKE&#10;These three&#10;labels are the&#10;most important&#10;ones related to&#10;node affinity.&#10; &#10;"
    color "green"
  ]
  node [
    id 816
    label "496"
    title "Page_496"
    color "blue"
  ]
  node [
    id 817
    label "text_407"
    title "464&#10;CHAPTER 16&#10;Advanced scheduling&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: kubia-gpu&#10;spec:&#10;  nodeSelector:          &#10;    gpu: &#34;true&#34;          &#10;  ...&#10;The nodeSelector field specifies that the pod should only be deployed on nodes that&#10;include the gpu=true label. If you replace the node selector with a node affinity rule,&#10;the pod definition will look like the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: kubia-gpu&#10;spec:&#10;  affinity:&#10;    nodeAffinity:&#10;      requiredDuringSchedulingIgnoredDuringExecution:&#10;        nodeSelectorTerms:&#10;        - matchExpressions:&#10;          - key: gpu&#10;            operator: In&#10;            values:&#10;            - &#34;true&#34;&#10;The first thing you&#8217;ll notice is that this is much more complicated than a simple node&#10;selector. But that&#8217;s because it&#8217;s much more expressive. Let&#8217;s examine the rule in detail. &#10;MAKING SENSE OF THE LONG NODEAFFINITY ATTRIBUTE NAME&#10;As you can see, the pod&#8217;s spec section contains an affinity field that contains a node-&#10;Affinity field, which contains a field with an extremely long name, so let&#8217;s focus on&#10;that first.&#10; Let&#8217;s break it down into two parts and examine what they mean:&#10;&#61601;&#10;requiredDuringScheduling... means the rules defined under this field spec-&#10;ify the labels the node must have for the pod to be scheduled to the node.&#10;&#61601;&#10;...IgnoredDuringExecution means the rules defined under the field don&#8217;t&#10;affect pods already executing on the node. &#10;At this point, let me make things easier for you by letting you know that affinity cur-&#10;rently only affects pod scheduling and never causes a pod to be evicted from a node.&#10;That&#8217;s why all the rules right now always end with IgnoredDuringExecution. Eventu-&#10;ally, Kubernetes will also support RequiredDuringExecution, which means that if you&#10;Listing 16.8&#10;A pod using a node selector: kubia-gpu-nodeselector.yaml&#10;Listing 16.9&#10;A pod using a nodeAffinity rule: kubia-gpu-nodeaffinity.yaml&#10;This pod is only scheduled &#10;to nodes that have the &#10;gpu=true label.&#10; &#10;"
    color "green"
  ]
  node [
    id 818
    label "497"
    title "Page_497"
    color "blue"
  ]
  node [
    id 819
    label "text_408"
    title "465&#10;Using node affinity to attract pods to certain nodes&#10;remove a label from a node, pods that require the node to have that label will be&#10;evicted from such a node. As I&#8217;ve said, that&#8217;s not yet supported in Kubernetes, so let&#8217;s&#10;not concern ourselves with the second part of that long field any longer.&#10;UNDERSTANDING NODESELECTORTERMS&#10;By keeping what was explained in the previous section in mind, it&#8217;s easy to understand&#10;that the nodeSelectorTerms field and the matchExpressions field define which&#10;expressions the node&#8217;s labels must match for the pod to be scheduled to the node.&#10;The single expression in the example is simple to understand. The node must have a&#10;gpu label whose value is set to true. &#10; This pod will therefore only be scheduled to nodes that have the gpu=true label, as&#10;shown in figure 16.2.&#10;Now comes the more interesting part. Node also affinity allows you to prioritize nodes&#10;during scheduling. We&#8217;ll look at that next.&#10;16.2.2 Prioritizing nodes when scheduling a pod&#10;The biggest benefit of the newly introduced node affinity feature is the ability to spec-&#10;ify which nodes the Scheduler should prefer when scheduling a specific pod. This is&#10;done through the preferredDuringSchedulingIgnoredDuringExecution field.&#10; Imagine having multiple datacenters across different countries. Each datacenter&#10;represents a separate availability zone. In each zone, you have certain machines meant&#10;only for your own use and others that your partner companies can use. You now want&#10;to deploy a few pods and you&#8217;d prefer them to be scheduled to zone1 and to the&#10;Node with a GPU&#10;Pod&#10;Node af&#64257;nity&#10;Required label:&#10;gpu=true&#10;Pod&#10;No node af&#64257;nity&#10;gpu: true&#10;Node with a GPU&#10;Node without a GPU&#10;Node without a GPU&#10;gpu: true&#10;This pod may be scheduled only&#10;to nodes with gpu=true label&#10;This pod may be&#10;scheduled to any node&#10;Figure 16.2&#10;A pod&#8217;s node affinity specifies which labels a node must have for the pod to be &#10;scheduled to it.&#10; &#10;"
    color "green"
  ]
  node [
    id 820
    label "498"
    title "Page_498"
    color "blue"
  ]
  node [
    id 821
    label "text_409"
    title "466&#10;CHAPTER 16&#10;Advanced scheduling&#10;machines reserved for your company&#8217;s deployments. If those machines don&#8217;t have&#10;enough room for the pods or if other important reasons exist that prevent them from&#10;being scheduled there, you&#8217;re okay with them being scheduled to the machines your&#10;partners use and to the other zones. Node affinity allows you to do that.&#10;LABELING NODES&#10;First, the nodes need to be labeled appropriately. Each node needs to have a label that&#10;designates the availability zone the node belongs to and a label marking it as either a&#10;dedicated or a shared node.&#10; Appendix B explains how to set up a three-node cluster (one master and two&#10;worker nodes) in VMs running locally. In the following examples, I&#8217;ll use the two worker&#10;nodes in that cluster, but you can also use Google Kubernetes Engine or any other&#10;multi-node cluster. &#10;NOTE&#10;Minikube isn&#8217;t the best choice for running these examples, because it&#10;runs only one node.&#10;First, label the nodes, as shown in the next listing.&#10;$ kubectl label node node1.k8s availability-zone=zone1&#10;node &#34;node1.k8s&#34; labeled&#10;$ kubectl label node node1.k8s share-type=dedicated&#10;node &#34;node1.k8s&#34; labeled&#10;$ kubectl label node node2.k8s availability-zone=zone2&#10;node &#34;node2.k8s&#34; labeled&#10;$ kubectl label node node2.k8s share-type=shared&#10;node &#34;node2.k8s&#34; labeled&#10;$ kubectl get node -L availability-zone -L share-type&#10;NAME         STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE&#10;master.k8s   Ready     4d        v1.6.4    <none>              <none>&#10;node1.k8s    Ready     4d        v1.6.4    zone1               dedicated&#10;node2.k8s    Ready     4d        v1.6.4    zone2               shared&#10;SPECIFYING PREFERENTIAL NODE AFFINITY RULES&#10;With the node labels set up, you can now create a Deployment that prefers dedicated&#10;nodes in zone1. The following listing shows the Deployment manifest.&#10;apiVersion: extensions/v1beta1&#10;kind: Deployment&#10;metadata:&#10;  name: pref&#10;spec:&#10;  template:&#10;    ...&#10;    spec:&#10;      affinity:&#10;        nodeAffinity:&#10;Listing 16.10&#10;Labeling nodes&#10;Listing 16.11&#10;Deployment with preferred node affinity: preferred-deployment.yaml&#10; &#10;"
    color "green"
  ]
  node [
    id 822
    label "499"
    title "Page_499"
    color "blue"
  ]
  node [
    id 823
    label "text_410"
    title "467&#10;Using node affinity to attract pods to certain nodes&#10;          preferredDuringSchedulingIgnoredDuringExecution:    &#10;          - weight: 80                               &#10;            preference:                              &#10;              matchExpressions:                      &#10;              - key: availability-zone               &#10;                operator: In                         &#10;                values:                              &#10;                - zone1                              &#10;          - weight: 20                     &#10;            preference:                    &#10;              matchExpressions:            &#10;              - key: share-type            &#10;                operator: In               &#10;                values:                    &#10;                - dedicated                &#10;      ...&#10;Let&#8217;s examine the listing closely. You&#8217;re defining a node affinity preference, instead of&#10;a hard requirement. You want the pods scheduled to nodes that include the labels&#10;availability-zone=zone1 and share-type=dedicated. You&#8217;re saying that the first&#10;preference rule is important by setting its weight to 80, whereas the second one is&#10;much less important (weight is set to 20).&#10;UNDERSTANDING HOW NODE PREFERENCES WORK&#10;If your cluster had many nodes, when scheduling the pods of the Deployment in the&#10;previous listing, the nodes would be split into four groups, as shown in figure 16.3.&#10;Nodes whose availability-zone and share-type labels match the pod&#8217;s node affin-&#10;ity are ranked the highest. Then, because of how the weights in the pod&#8217;s node affinity&#10;rules are configured, next come the shared nodes in zone1, then come the dedicated&#10;nodes in the other zones, and at the lowest priority are all the other nodes.&#10;You&#8217;re&#10;specifying&#10;preferences,&#10;not hard&#10;requirements.&#10;You prefer the pod to be &#10;scheduled to zone1. This &#10;is your most important &#10;preference.&#10;You also prefer that your &#10;pods be scheduled to &#10;dedicated nodes, but this is &#10;four times less important &#10;than your zone preference.&#10;Node&#10;Top priority&#10;Availability zone 1&#10;Pod&#10;Priority: 2&#10;Priority: 3&#10;Priority: 4&#10;Node af&#64257;nity&#10;Preferred labels:&#10;avail-zone: zone1 (weight 80)&#10;share: dedicated (weight 20)&#10;avail-zone: zone1&#10;share: dedicated&#10;Node&#10;avail-zone: zone1&#10;share: shared&#10;Node&#10;Availability zone 2&#10;avail-zone: zone2&#10;share: dedicated&#10;Node&#10;avail-zone: zone2&#10;share: shared&#10;This pod may be scheduled to&#10;any node, but certain nodes are&#10;preferred based on their labels.&#10;Figure 16.3&#10;Prioritizing nodes based on a pod&#8217;s node affinity preferences&#10; &#10;"
    color "green"
  ]
  node [
    id 824
    label "500"
    title "Page_500"
    color "blue"
  ]
  node [
    id 825
    label "text_411"
    title "468&#10;CHAPTER 16&#10;Advanced scheduling&#10;DEPLOYING THE PODS IN THE TWO-NODE CLUSTER&#10;If you create this Deployment in your two-node cluster, you should see most (if not&#10;all) of your pods deployed to node1. Examine the following listing to see if that&#8217;s true.&#10;$ kubectl get po -o wide&#10;NAME                READY   STATUS    RESTARTS  AGE   IP          NODE&#10;pref-607515-1rnwv   1/1     Running   0         4m    10.47.0.1   node2.k8s&#10;pref-607515-27wp0   1/1     Running   0         4m    10.44.0.8   node1.k8s&#10;pref-607515-5xd0z   1/1     Running   0         4m    10.44.0.5   node1.k8s&#10;pref-607515-jx9wt   1/1     Running   0         4m    10.44.0.4   node1.k8s&#10;pref-607515-mlgqm   1/1     Running   0         4m    10.44.0.6   node1.k8s&#10;Out of the five pods that were created, four of them landed on node1 and only one&#10;landed on node2. Why did one of them land on node2 instead of node1? The reason is&#10;that besides the node affinity prioritization function, the Scheduler also uses other pri-&#10;oritization functions to decide where to schedule a pod. One of those is the Selector-&#10;SpreadPriority function, which makes sure pods belonging to the same ReplicaSet or&#10;Service are spread around different nodes so a node failure won&#8217;t bring the whole ser-&#10;vice down. That&#8217;s most likely what caused one of the pods to be scheduled to node2.&#10; You can try scaling the Deployment up to 20 or more and you&#8217;ll see the majority of&#10;pods will be scheduled to node1. In my test, only two out of the 20 were scheduled to&#10;node2. If you hadn&#8217;t defined any node affinity preferences, the pods would have been&#10;spread around the two nodes evenly.&#10;16.3&#10;Co-locating pods with pod affinity and anti-affinity&#10;You&#8217;ve seen how node affinity rules are used to influence which node a pod is scheduled&#10;to. But these rules only affect the affinity between a pod and a node, whereas sometimes&#10;you&#8217;d like to have the ability to specify the affinity between pods themselves. &#10; For example, imagine having a frontend and a backend pod. Having those pods&#10;deployed near to each other reduces latency and improves the performance of the&#10;app. You could use node affinity rules to ensure both are deployed to the same node,&#10;rack, or datacenter, but then you&#8217;d have to specify exactly which node, rack, or data-&#10;center to schedule them to, which is not the best solution. It&#8217;s better to let Kubernetes&#10;deploy your pods anywhere it sees fit, while keeping the frontend and backend pods&#10;close together. This can be achieved using pod affinity. Let&#8217;s learn more about it with&#10;an example.&#10;16.3.1 Using inter-pod affinity to deploy pods on the same node&#10;You&#8217;ll deploy a backend pod and five frontend pod replicas with pod affinity config-&#10;ured so that they&#8217;re all deployed on the same node as the backend pod.&#10; First, deploy the backend pod:&#10;$ kubectl run backend -l app=backend --image busybox -- sleep 999999&#10;deployment &#34;backend&#34; created&#10;Listing 16.12&#10;Seeing where pods were scheduled&#10; &#10;"
    color "green"
  ]
  node [
    id 826
    label "501"
    title "Page_501"
    color "blue"
  ]
  node [
    id 827
    label "text_412"
    title "469&#10;Co-locating pods with pod affinity and anti-affinity&#10;This Deployment is not special in any way. The only thing you need to note is the&#10;app=backend label you added to the pod using the -l option. This label is what you&#8217;ll&#10;use in the frontend pod&#8217;s podAffinity configuration. &#10;SPECIFYING POD AFFINITY IN A POD DEFINITION&#10;The frontend pod&#8217;s definition is shown in the following listing.&#10;apiVersion: extensions/v1beta1&#10;kind: Deployment&#10;metadata:&#10;  name: frontend&#10;spec:&#10;  replicas: 5&#10;  template:&#10;    ...&#10;    spec:&#10;      affinity:&#10;        podAffinity:                                 &#10;          requiredDuringSchedulingIgnoredDuringExecution:   &#10;          - topologyKey: kubernetes.io/hostname           &#10;            labelSelector:                                &#10;              matchLabels:                                &#10;                app: backend                              &#10;      ...&#10;The listing shows that this Deployment will create pods that have a hard requirement&#10;to be deployed on the same node (specified by the topologyKey field) as pods that&#10;have the app=backend label (see figure 16.4).&#10;Listing 16.13&#10;Pod using podAffinity: frontend-podaffinity-host.yaml&#10;Defining &#10;podAffinity rules&#10;Defining a hard &#10;requirement, not &#10;a preference&#10;The pods of this Deployment &#10;must be deployed on the &#10;same node as the pods that &#10;match the selector.&#10;All frontend pods will&#10;be scheduled only to&#10;the node the backend&#10;pod was scheduled to.&#10;Some node&#10;Other nodes&#10;Frontend pods&#10;Backend&#10;pod&#10;Pod af&#64257;nity&#10;Label selector: app=backend&#10;Topology key: hostname&#10;app: backend&#10;Figure 16.4&#10;Pod affinity allows scheduling pods to the node where other pods &#10;with a specific label are.&#10; &#10;"
    color "green"
  ]
  node [
    id 828
    label "502"
    title "Page_502"
    color "blue"
  ]
  node [
    id 829
    label "text_413"
    title "470&#10;CHAPTER 16&#10;Advanced scheduling&#10;NOTE&#10;Instead of the simpler matchLabels field, you could also use the more&#10;expressive matchExpressions field.&#10;DEPLOYING A POD WITH POD AFFINITY&#10;Before you create this Deployment, let&#8217;s see which node the backend pod was sched-&#10;uled to earlier:&#10;$ kubectl get po -o wide&#10;NAME                   READY  STATUS   RESTARTS  AGE  IP         NODE&#10;backend-257820-qhqj6   1/1    Running  0         8m   10.47.0.1  node2.k8s&#10;When you create the frontend pods, they should be deployed to node2 as well. You&#8217;re&#10;going to create the Deployment and see where the pods are deployed. This is shown&#10;in the next listing.&#10;$ kubectl create -f frontend-podaffinity-host.yaml&#10;deployment &#34;frontend&#34; created&#10;$ kubectl get po -o wide&#10;NAME                   READY  STATUS    RESTARTS  AGE  IP         NODE&#10;backend-257820-qhqj6   1/1    Running   0         8m   10.47.0.1  node2.k8s&#10;frontend-121895-2c1ts  1/1    Running   0         13s  10.47.0.6  node2.k8s&#10;frontend-121895-776m7  1/1    Running   0         13s  10.47.0.4  node2.k8s&#10;frontend-121895-7ffsm  1/1    Running   0         13s  10.47.0.8  node2.k8s&#10;frontend-121895-fpgm6  1/1    Running   0         13s  10.47.0.7  node2.k8s&#10;frontend-121895-vb9ll  1/1    Running   0         13s  10.47.0.5  node2.k8s&#10;All the frontend pods were indeed scheduled to the same node as the backend pod.&#10;When scheduling the frontend pod, the Scheduler first found all the pods that match&#10;the labelSelector defined in the frontend pod&#8217;s podAffinity configuration and&#10;then scheduled the frontend pod to the same node.&#10;UNDERSTANDING HOW THE SCHEDULER USES POD AFFINITY RULES&#10;What&#8217;s interesting is that if you now delete the backend pod, the Scheduler will sched-&#10;ule the pod to node2 even though it doesn&#8217;t define any pod affinity rules itself (the&#10;rules are only on the frontend pods). This makes sense, because otherwise if the back-&#10;end pod were to be deleted by accident and rescheduled to a different node, the fron-&#10;tend pods&#8217; affinity rules would be broken. &#10; You can confirm the Scheduler takes other pods&#8217; pod affinity rules into account, if&#10;you increase the Scheduler&#8217;s logging level and then check its log. The following listing&#10;shows the relevant log lines.&#10;... Attempting to schedule pod: default/backend-257820-qhqj6&#10;... ...&#10;... backend-qhqj6 -> node2.k8s: Taint Toleration Priority, Score: (10)&#10;Listing 16.14&#10;Deploying frontend pods and seeing which node they&#8217;re scheduled to&#10;Listing 16.15&#10;Scheduler log showing why the backend pod is scheduled to node2&#10; &#10;"
    color "green"
  ]
  node [
    id 830
    label "503"
    title "Page_503"
    color "blue"
  ]
  node [
    id 831
    label "text_414"
    title "471&#10;Co-locating pods with pod affinity and anti-affinity&#10;... backend-qhqj6 -> node1.k8s: Taint Toleration Priority, Score: (10)&#10;... backend-qhqj6 -> node2.k8s: InterPodAffinityPriority, Score: (10)&#10;... backend-qhqj6 -> node1.k8s: InterPodAffinityPriority, Score: (0)&#10;... backend-qhqj6 -> node2.k8s: SelectorSpreadPriority, Score: (10)&#10;... backend-qhqj6 -> node1.k8s: SelectorSpreadPriority, Score: (10)&#10;... backend-qhqj6 -> node2.k8s: NodeAffinityPriority, Score: (0)&#10;... backend-qhqj6 -> node1.k8s: NodeAffinityPriority, Score: (0)&#10;... Host node2.k8s => Score 100030&#10;... Host node1.k8s => Score 100022&#10;... Attempting to bind backend-257820-qhqj6 to node2.k8s&#10;If you focus on the two lines in bold, you&#8217;ll see that during the scheduling of the back-&#10;end pod, node2 received a higher score than node1 because of inter-pod affinity. &#10;16.3.2 Deploying pods in the same rack, availability zone, or &#10;geographic region&#10;In the previous example, you used podAffinity to deploy frontend pods onto the&#10;same node as the backend pods. You probably don&#8217;t want all your frontend pods to&#10;run on the same machine, but you&#8217;d still like to keep them close to the backend&#10;pod&#8212;for example, run them in the same availability zone. &#10;CO-LOCATING PODS IN THE SAME AVAILABILITY ZONE&#10;The cluster I&#8217;m using runs in three VMs on my local machine, so all the nodes are in&#10;the same availability zone, so to speak. But if the nodes were in different zones, all I&#8217;d&#10;need to do to run the frontend pods in the same zone as the backend pod would be to&#10;change the topologyKey property to failure-domain.beta.kubernetes.io/zone. &#10;CO-LOCATING PODS IN THE SAME GEOGRAPHICAL REGION&#10;To allow the pods to be deployed in the same region instead of the same zone (cloud&#10;providers usually have datacenters located in different geographical regions and split&#10;into multiple availability zones in each region), the topologyKey would be set to&#10;failure-domain.beta.kubernetes.io/region.&#10;UNDERSTANDING HOW TOPOLOGYKEY WORKS&#10;The way topologyKey works is simple. The three keys we&#8217;ve mentioned so far aren&#8217;t&#10;special. If you want, you can easily use your own topologyKey, such as rack, to have&#10;the pods scheduled to the same server rack. The only prerequisite is to add a rack&#10;label to your nodes. This scenario is shown in figure 16.5.&#10; For example, if you had 20 nodes, with 10 in each rack, you&#8217;d label the first ten as&#10;rack=rack1 and the others as rack=rack2. Then, when defining a pod&#8217;s podAffinity,&#10;you&#8217;d set the toplogyKey to rack. &#10; When the Scheduler is deciding where to deploy a pod, it checks the pod&#8217;s pod-&#10;Affinity config, finds the pods that match the label selector, and looks up the nodes&#10;they&#8217;re running on. Specifically, it looks up the nodes&#8217; label whose key matches the&#10;topologyKey field specified in podAffinity. Then it selects all the nodes whose label&#10; &#10;"
    color "green"
  ]
  node [
    id 832
    label "504"
    title "Page_504"
    color "blue"
  ]
  node [
    id 833
    label "text_415"
    title "472&#10;CHAPTER 16&#10;Advanced scheduling&#10;matches the values of the pods it found earlier. In figure 16.5, the label selector&#10;matched the backend pod, which runs on Node 12. The value of the rack label on&#10;that node equals rack2, so when scheduling a frontend pod, the Scheduler will only&#10;select among the nodes that have the rack=rack2 label.&#10;NOTE&#10;By default, the label selector only matches pods in the same name-&#10;space as the pod that&#8217;s being scheduled. But you can also select pods from&#10;other namespaces by adding a namespaces field at the same level as label-&#10;Selector.&#10;16.3.3 Expressing pod affinity preferences instead of hard requirements&#10;Earlier, when we talked about node affinity, you saw that nodeAffinity can be used to&#10;express a hard requirement, which means a pod is only scheduled to nodes that match&#10;the node affinity rules. It can also be used to specify node preferences, to instruct the&#10;Scheduler to schedule the pod to certain nodes, while allowing it to schedule it any-&#10;where else if those nodes can&#8217;t fit the pod for any reason.&#10; The same also applies to podAffinity. You can tell the Scheduler you&#8217;d prefer to&#10;have your frontend pods scheduled onto the same node as your backend pod, but if&#10;that&#8217;s not possible, you&#8217;re okay with them being scheduled elsewhere. An example of&#10;a Deployment using the preferredDuringSchedulingIgnoredDuringExecution pod&#10;affinity rule is shown in the next listing.&#10;Frontend pods will be&#10;scheduled to nodes in&#10;the same rack as the&#10;backend pod.&#10;Node 1&#10;Rack 1&#10;rack: rack1&#10;Node 2&#10;rack: rack1&#10;Node 3&#10;...&#10;rack: rack1&#10;Node 10&#10;rack: rack1&#10;Node 11&#10;Rack 2&#10;rack: rack2&#10;Node 12&#10;rack: rack2&#10;...&#10;Node 20&#10;rack: rack2&#10;Backend&#10;pod&#10;app: backend&#10;Frontend pods&#10;Pod af&#64257;nity (required)&#10;Label selector: app=backend&#10;Topology key: rack&#10;Figure 16.5&#10;The topologyKey in podAffinity determines the scope of where the pod &#10;should be scheduled to.&#10; &#10;"
    color "green"
  ]
  node [
    id 834
    label "505"
    title "Page_505"
    color "blue"
  ]
  node [
    id 835
    label "text_416"
    title "473&#10;Co-locating pods with pod affinity and anti-affinity&#10;apiVersion: extensions/v1beta1&#10;kind: Deployment&#10;metadata:&#10;  name: frontend&#10;spec:&#10;  replicas: 5&#10;  template:&#10;    ...&#10;    spec:&#10;      affinity:&#10;        podAffinity:&#10;          preferredDuringSchedulingIgnoredDuringExecution:  &#10;          - weight: 80                                        &#10;            podAffinityTerm:                                  &#10;              topologyKey: kubernetes.io/hostname             &#10;              labelSelector:                                  &#10;                matchLabels:                                  &#10;                  app: backend                                &#10;      containers: ...&#10;As in nodeAffinity preference rules, you need to define a weight for each rule. You&#10;also need to specify the topologyKey and labelSelector, as in the hard-requirement&#10;podAffinity rules. Figure 16.6 shows this scenario.&#10;Deploying this pod, as with your nodeAffinity example, deploys four pods on the same&#10;node as the backend pod, and one pod on the other node (see the following listing).&#10;Listing 16.16&#10;Pod affinity preference&#10;Preferred &#10;instead of &#10;Required&#10;A weight and a &#10;podAffinity term is &#10;specified as in the &#10;previous example&#10;The Scheduler will prefer&#10;Node 2 for frontend pods,&#10;but may schedule pods&#10;to Node 1 as well.&#10;Node 1&#10;Node 2&#10;Backend&#10;pod&#10;app: backend&#10;Frontend pod&#10;Pod af&#64257;nity (preferred)&#10;Label selector: app=backend&#10;Topology key: hostname&#10;hostname: node2&#10;hostname: node1&#10;Figure 16.6&#10;Pod affinity can be used to make the Scheduler prefer nodes where &#10;pods with a certain label are running. &#10; &#10;"
    color "green"
  ]
  node [
    id 836
    label "506"
    title "Page_506"
    color "blue"
  ]
  node [
    id 837
    label "text_417"
    title "474&#10;CHAPTER 16&#10;Advanced scheduling&#10;$ kubectl get po -o wide&#10;NAME                   READY  STATUS   RESTARTS  AGE  IP          NODE&#10;backend-257820-ssrgj   1/1    Running  0         1h   10.47.0.9   node2.k8s&#10;frontend-941083-3mff9  1/1    Running  0         8m   10.44.0.4   node1.k8s&#10;frontend-941083-7fp7d  1/1    Running  0         8m   10.47.0.6   node2.k8s&#10;frontend-941083-cq23b  1/1    Running  0         8m   10.47.0.1   node2.k8s&#10;frontend-941083-m70sw  1/1    Running  0         8m   10.47.0.5   node2.k8s&#10;frontend-941083-wsjv8  1/1    Running  0         8m   10.47.0.4   node2.k8s&#10;16.3.4 Scheduling pods away from each other with pod anti-affinity&#10;You&#8217;ve seen how to tell the Scheduler to co-locate pods, but sometimes you may want&#10;the exact opposite. You may want to keep pods away from each other. This is called&#10;pod anti-affinity. It&#8217;s specified the same way as pod affinity, except that you use the&#10;podAntiAffinity property instead of podAffinity, which results in the Scheduler&#10;never choosing nodes where pods matching the podAntiAffinity&#8217;s label selector are&#10;running, as shown in figure 16.7.&#10;An example of why you&#8217;d want to use pod anti-affinity is when two sets of pods inter-&#10;fere with each other&#8217;s performance if they run on the same node. In that case, you&#10;want to tell the Scheduler to never schedule those pods on the same node. Another&#10;example would be to force the Scheduler to spread pods of the same group across dif-&#10;ferent availability zones or regions, so that a failure of a whole zone (or region) never&#10;brings the service down completely. &#10;Listing 16.17&#10;Pods deployed with podAffinity preferences&#10;These pods will NOT be scheduled&#10;to the same node(s) where pods&#10;with app=foo label are running.&#10;Some node&#10;Other nodes&#10;Pods&#10;Pod: foo&#10;Pod&#10;(required)&#10;anti-af&#64257;nity&#10;Label selector: app=foo&#10;Topology key: hostname&#10;app: foo&#10;Figure 16.7&#10;Using pod anti-affinity to keep pods away from nodes that run pods &#10;with a certain label.&#10; &#10;"
    color "green"
  ]
  node [
    id 838
    label "507"
    title "Page_507"
    color "blue"
  ]
  node [
    id 839
    label "text_418"
    title "475&#10;Co-locating pods with pod affinity and anti-affinity&#10;USING ANTI-AFFINITY TO SPREAD APART PODS OF THE SAME DEPLOYMENT&#10;Let&#8217;s see how to force your frontend pods to be scheduled to different nodes. The fol-&#10;lowing listing shows how the pods&#8217; anti-affinity is configured.&#10;apiVersion: extensions/v1beta1&#10;kind: Deployment&#10;metadata:&#10;  name: frontend&#10;spec:&#10;  replicas: 5&#10;  template:&#10;    metadata:&#10;      labels:                  &#10;        app: frontend          &#10;    spec:&#10;      affinity:&#10;        podAntiAffinity:                                      &#10;          requiredDuringSchedulingIgnoredDuringExecution:     &#10;          - topologyKey: kubernetes.io/hostname            &#10;            labelSelector:                                 &#10;              matchLabels:                                 &#10;                app: frontend                              &#10;      containers: ...&#10;This time, you&#8217;re defining podAntiAffinity instead of podAffinity, and you&#8217;re mak-&#10;ing the labelSelector match the same pods that the Deployment creates. Let&#8217;s see&#10;what happens when you create this Deployment. The pods created by it are shown in&#10;the following listing.&#10;$ kubectl get po -l app=frontend -o wide&#10;NAME                    READY  STATUS   RESTARTS  AGE  IP         NODE&#10;frontend-286632-0lffz   0/1    Pending  0         1m   <none>&#10;frontend-286632-2rkcz   1/1    Running  0         1m   10.47.0.1  node2.k8s&#10;frontend-286632-4nwhp   0/1    Pending  0         1m   <none>&#10;frontend-286632-h4686   0/1    Pending  0         1m   <none>&#10;frontend-286632-st222   1/1    Running  0         1m   10.44.0.4  node1.k8s&#10;As you can see, only two pods were scheduled&#8212;one to node1, the other to node2. The&#10;three remaining pods are all Pending, because the Scheduler isn&#8217;t allowed to schedule&#10;them to the same nodes.&#10;USING PREFERENTIAL POD ANTI-AFFINITY&#10;In this case, you probably should have specified a soft requirement instead (using the&#10;preferredDuringSchedulingIgnoredDuringExecution property). After all, it&#8217;s not&#10;such a big problem if two frontend pods run on the same node. But in scenarios where&#10;that&#8217;s a problem, using requiredDuringScheduling is appropriate. &#10;Listing 16.18&#10;Pods with anti-affinity: frontend-podantiaffinity-host.yaml&#10;Listing 16.19&#10;Pods created by the Deployment&#10;The frontend pods have &#10;the app=frontend label.&#10;Defining hard-&#10;requirements for &#10;pod anti-affinity&#10;A frontend pod must not &#10;be scheduled to the same &#10;machine as a pod with &#10;app=frontend label.&#10; &#10;"
    color "green"
  ]
  node [
    id 840
    label "508"
    title "Page_508"
    color "blue"
  ]
  node [
    id 841
    label "text_419"
    title "476&#10;CHAPTER 16&#10;Advanced scheduling&#10; As with pod affinity, the topologyKey property determines the scope of where the&#10;pod shouldn&#8217;t be deployed to. You can use it to ensure pods aren&#8217;t deployed to the&#10;same rack, availability zone, region, or any custom scope you create using custom&#10;node labels.&#10;16.4&#10;Summary&#10;In this chapter, we looked at how to ensure pods aren&#8217;t scheduled to certain nodes or&#10;are only scheduled to specific nodes, either because of the node&#8217;s labels or because of&#10;the pods running on them.&#10; You learned that&#10;&#61601;If you add a taint to a node, pods won&#8217;t be scheduled to that node unless they&#10;tolerate that taint.&#10;&#61601;Three types of taints exist: NoSchedule completely prevents scheduling, Prefer-&#10;NoSchedule isn&#8217;t as strict, and NoExecute even evicts existing pods from a node.&#10;&#61601;The NoExecute taint is also used to specify how long the Control Plane should&#10;wait before rescheduling the pod when the node it runs on becomes unreach-&#10;able or unready.&#10;&#61601;Node affinity allows you to specify which nodes a pod should be scheduled to. It&#10;can be used to specify a hard requirement or to only express a node preference.&#10;&#61601;Pod affinity is used to make the Scheduler deploy pods to the same node where&#10;another pod is running (based on the pod&#8217;s labels). &#10;&#61601;Pod affinity&#8217;s topologyKey specifies how close the pod should be deployed to&#10;the other pod (onto the same node or onto a node in the same rack, availability&#10;zone, or availability region).&#10;&#61601;Pod anti-affinity can be used to keep certain pods away from each other. &#10;&#61601;Both pod affinity and anti-affinity, like node affinity, can either specify hard&#10;requirements or preferences.&#10;In the next chapter, you&#8217;ll learn about best practices for developing apps and how to&#10;make them run smoothly in a Kubernetes environment.&#10; &#10;"
    color "green"
  ]
  node [
    id 842
    label "509"
    title "Page_509"
    color "blue"
  ]
  node [
    id 843
    label "text_420"
    title "477&#10;Best practices&#10;for developing apps&#10;We&#8217;ve now covered most of what you need to know to run your apps in Kubernetes.&#10;We&#8217;ve explored what each individual resource does and how it&#8217;s used. Now we&#8217;ll see&#10;how to combine them in a typical application running on Kubernetes. We&#8217;ll also&#10;look at how to make an application run smoothly. After all, that&#8217;s the whole point&#10;of using Kubernetes, isn&#8217;t it? &#10; Hopefully, this chapter will help to clear up any misunderstandings and explain&#10;things that weren&#8217;t explained clearly yet. Along the way, we&#8217;ll also introduce a few&#10;additional concepts that haven&#8217;t been mentioned up to this point.&#10;This chapter covers&#10;&#61601;Understanding which Kubernetes resources &#10;appear in a typical application&#10;&#61601;Adding post-start and pre-stop pod lifecycle hooks&#10;&#61601;Properly terminating an app without breaking &#10;client requests&#10;&#61601;Making apps easy to manage in Kubernetes&#10;&#61601;Using init containers in a pod&#10;&#61601;Developing locally with Minikube&#10; &#10;"
    color "green"
  ]
  node [
    id 844
    label "510"
    title "Page_510"
    color "blue"
  ]
  node [
    id 845
    label "text_421"
    title "478&#10;CHAPTER 17&#10;Best practices for developing apps&#10;17.1&#10;Bringing everything together&#10;Let&#8217;s start by looking at what an actual application consists of. This will also give you a&#10;chance to see if you remember everything you&#8217;ve learned so far and look at the big&#10;picture. Figure 17.1 shows the Kubernetes components used in a typical application.&#10;A typical application manifest contains one or more Deployment and/or StatefulSet&#10;objects. Those include a pod template containing one or more containers, with a live-&#10;ness probe for each of them and a readiness probe for the service(s) the container&#10;provides (if any). Pods that provide services to others are exposed through one or&#10;more Services. When they need to be reachable from outside the cluster, the Services&#10;are either configured to be LoadBalancer or NodePort-type Services, or exposed&#10;through an Ingress resource. &#10; The pod templates (and the pods created from them) usually reference two types&#10;of Secrets&#8212;those for pulling container images from private image registries and those&#10;used directly by the process running inside the pods. The Secrets themselves are&#10;usually not part of the application manifest, because they aren&#8217;t configured by the&#10;application developers but by the operations team. Secrets are usually assigned to&#10;ServiceAccounts, which are assigned to individual pods. &#10;De&#64257;ned in the app manifest by the developer&#10;Pod template&#10;Deployment&#10;labels&#10;Pod(s)&#10;Label selector&#10;labels&#10;Created automatically at runtime&#10;Created by a cluster admin beforehand&#10;Container(s)&#10;Volume(s)&#10;ReplicaSet(s)&#10;Endpoints&#10;&#8226; Health probes&#10;&#8226; Environment variables&#10;&#8226; Volume mounts&#10;&#8226; Resource reqs/limits&#10;Horizontal&#10;PodAutoscaler&#10;StatefulSet&#10;DaemonSet&#10;Job&#10;CronJob&#10;Persistent&#10;Volume&#10;Con&#64257;gMap&#10;Service&#10;Persistent&#10;Volume&#10;Claim&#10;Secret(s)&#10;Service&#10;account&#10;Storage&#10;Class&#10;LimitRange&#10;ResourceQuota&#10;Ingress&#10;imagePullSecret&#10;Figure 17.1&#10;Resources in a typical application&#10; &#10;"
    color "green"
  ]
  node [
    id 846
    label "511"
    title "Page_511"
    color "blue"
  ]
  node [
    id 847
    label "text_422"
    title "479&#10;Understanding the pod&#8217;s lifecycle&#10; The application also contains one or more ConfigMaps, which are either used to&#10;initialize environment variables or mounted as a configMap volume in the pod. Cer-&#10;tain pods use additional volumes, such as an emptyDir or a gitRepo volume, whereas&#10;pods requiring persistent storage use persistentVolumeClaim volumes. The Persistent-&#10;VolumeClaims are also part of the application manifest, whereas StorageClasses refer-&#10;enced by them are created by system administrators upfront. &#10; In certain cases, an application also requires the use of Jobs or CronJobs. Daemon-&#10;Sets aren&#8217;t normally part of application deployments, but are usually created by sysad-&#10;mins to run system services on all or a subset of nodes. HorizontalPodAutoscalers&#10;are either included in the manifest by the developers or added to the system later by&#10;the ops team. The cluster administrator also creates LimitRange and ResourceQuota&#10;objects to keep compute resource usage of individual pods and all the pods (as a&#10;whole) under control.&#10; After the application is deployed, additional objects are created automatically by&#10;the various Kubernetes controllers. These include service Endpoints objects created&#10;by the Endpoints controller, ReplicaSets created by the Deployment controller, and&#10;the actual pods created by the ReplicaSet (or Job, CronJob, StatefulSet, or DaemonSet)&#10;controllers.&#10; Resources are often labeled with one or more labels to keep them organized. This&#10;doesn&#8217;t apply only to pods but to all other resources as well. In addition to labels, most&#10;resources also contain annotations that describe each resource, list the contact infor-&#10;mation of the person or team responsible for it, or provide additional metadata for&#10;management and other tools. &#10; At the center of all this is the Pod, which arguably is the most important Kuberne-&#10;tes resource. After all, each of your applications runs inside it. To make sure you know&#10;how to develop apps that make the most out of their environment, let&#8217;s take one last&#10;close look at pods&#8212;this time from the application&#8217;s perspective. &#10;17.2&#10;Understanding the pod&#8217;s lifecycle&#10;We&#8217;ve said that pods can be compared to VMs dedicated to running only a single&#10;application. Although an application running inside a pod is not unlike an application&#10;running in a VM, significant differences do exist. One example is that apps running in&#10;a pod can be killed any time, because Kubernetes needs to relocate the pod to&#10;another node for a reason or because of a scale-down request. We&#8217;ll explore this&#10;aspect next.&#10;17.2.1 Applications must expect to be killed and relocated&#10;Outside Kubernetes, apps running in VMs are seldom moved from one machine to&#10;another. When an operator moves the app, they can also reconfigure the app and&#10;manually check that the app is running fine in the new location. With Kubernetes,&#10;apps are relocated much more frequently and automatically&#8212;no human operator&#10; &#10;"
    color "green"
  ]
  node [
    id 848
    label "512"
    title "Page_512"
    color "blue"
  ]
  node [
    id 849
    label "text_423"
    title "480&#10;CHAPTER 17&#10;Best practices for developing apps&#10;reconfigures them and makes sure they still run properly after the move. This means&#10;application developers need to make sure their apps allow being moved relatively&#10;often. &#10;EXPECTING THE LOCAL IP AND HOSTNAME TO CHANGE&#10;When a pod is killed and run elsewhere (technically, it&#8217;s a new pod instance replac-&#10;ing the old one; the pod isn&#8217;t relocated), it not only has a new IP address but also a&#10;new name and hostname. Most stateless apps can usually handle this without any&#10;adverse effects, but stateful apps usually can&#8217;t. We&#8217;ve learned that stateful apps can&#10;be run through a StatefulSet, which ensures that when the app starts up on a new&#10;node after being rescheduled, it will still see the same host name and persistent state&#10;as before. The pod&#8217;s IP will change nevertheless. Apps need to be prepared for that&#10;to happen. The application developer therefore should never base membership in a&#10;clustered app on the member&#8217;s IP address, and if basing it on the hostname, should&#10;always use a StatefulSet.&#10;EXPECTING THE DATA WRITTEN TO DISK TO DISAPPEAR&#10;Another thing to keep in mind is that if the app writes data to disk, that data may not be&#10;available after the app is started inside a new pod, unless you mount persistent storage at&#10;the location the app is writing to. It should be clear this happens when the pod is&#10;rescheduled, but files written to disk will disappear even in scenarios that don&#8217;t involve&#10;any rescheduling. Even during the lifetime of a single pod, the files written to disk by&#10;the app running in the pod may disappear. Let me explain this with an example.&#10; Imagine an app that has a long and computationally intensive initial startup proce-&#10;dure. To help the app come up faster on subsequent startups, the developers make&#10;the app cache the results of the initial startup on disk (an example of this would be&#10;the scanning of all Java classes for annotations at startup and then writing the results&#10;to an index file). Because apps in Kubernetes run in containers by default, these files&#10;are written to the container&#8217;s filesystem. If the container is then restarted, they&#8217;re all&#10;lost, because the new container starts off with a completely new writable layer (see fig-&#10;ure 17.2).&#10; Don&#8217;t forget that individual containers may be restarted for several reasons, such&#10;as because the process crashes, because the liveness probe returned a failure, or&#10;because the node started running out of memory and the process was killed by the&#10;OOMKiller. When this happens, the pod is still the same, but the container itself is&#10;completely new. The Kubelet doesn&#8217;t run the same container again; it always creates a&#10;new container. &#10;USING VOLUMES TO PRESERVE DATA ACROSS CONTAINER RESTARTS&#10;When its container is restarted, the app in the example will need to perform the&#10;intensive startup procedure again. This may or may not be desired. To make sure data&#10;like this isn&#8217;t lost, you need to use at least a pod-scoped volume. Because volumes live&#10;and die together with the pod, the new container will be able to reuse the data written&#10;to the volume by the previous container (figure 17.3).&#10; &#10;"
    color "green"
  ]
  node [
    id 850
    label "513"
    title "Page_513"
    color "blue"
  ]
  node [
    id 851
    label "text_424"
    title "481&#10;Understanding the pod&#8217;s lifecycle&#10;Container&#10;Process&#10;Writes to&#10;Filesystem&#10;Writable layer&#10;Read-only layer&#10;Read-only layer&#10;Image layers&#10;Container crashes&#10;or is killed&#10;Pod&#10;New container&#10;New process&#10;Filesystem&#10;New writable layer&#10;Read-only layer&#10;Read-only layer&#10;Image layers&#10;New container started&#10;(part of the same pod)&#10;New container&#10;starts with new&#10;writeable layer:&#10;all &#64257;les are lost&#10;Figure 17.2&#10;Files written to the container&#8217;s filesystem are lost when the container is restarted.&#10;Container&#10;Process&#10;Writes to&#10;Can read&#10;the same &#64257;les&#10;Filesystem&#10;volumeMount&#10;Container crashes&#10;or is killed&#10;Pod&#10;New container&#10;New process&#10;Filesystem&#10;volumeMount&#10;New container started&#10;(part of the same pod)&#10;New process can&#10;use data preserved&#10;in the volume&#10;Volume&#10;Figure 17.3&#10;Using a volume to persist data across container restarts&#10; &#10;"
    color "green"
  ]
  node [
    id 852
    label "514"
    title "Page_514"
    color "blue"
  ]
  node [
    id 853
    label "text_425"
    title "482&#10;CHAPTER 17&#10;Best practices for developing apps&#10;Using a volume to preserve files across container restarts is a great idea sometimes,&#10;but not always. What if the data gets corrupted and causes the newly created process&#10;to crash again? This will result in a continuous crash loop (the pod will show the&#10;CrashLoopBackOff status). If you hadn&#8217;t used a volume, the new container would start&#10;from scratch and most likely not crash. Using volumes to preserve files across con-&#10;tainer restarts like this is a double-edged sword. You need to think carefully about&#10;whether to use them or not.&#10;17.2.2 Rescheduling of dead or partially dead pods&#10;If a pod&#8217;s container keeps crashing, the Kubelet will keep restarting it indefinitely.&#10;The time between restarts will be increased exponentially until it reaches five minutes.&#10;During those five minute intervals, the pod is essentially dead, because its container&#8217;s&#10;process isn&#8217;t running. To be fair, if it&#8217;s a multi-container pod, certain containers may&#10;be running normally, so the pod is only partially dead. But if a pod contains only a sin-&#10;gle container, the pod is effectively dead and completely useless, because no process is&#10;running in it anymore.&#10; You may find it surprising to learn that such pods aren&#8217;t automatically removed&#10;and rescheduled, even if they&#8217;re part of a ReplicaSet or similar controller. If you cre-&#10;ate a ReplicaSet with a desired replica count of three, and then one of the containers&#10;in one of those pods starts crashing, Kubernetes will not delete and replace the pod.&#10;The end result is a ReplicaSet with only two properly running replicas instead of the&#10;desired three (figure 17.4).&#10;You&#8217;d probably expect the pod to be deleted and replaced with another pod instance&#10;that might run successfully on another node. After all, the container may be crashing&#10;because of a node-related problem that doesn&#8217;t manifest itself on other nodes. Sadly,&#10;that isn&#8217;t the case. The ReplicaSet controller doesn&#8217;t care if the pods are dead&#8212;all it&#10;ReplicaSet&#10;Desired replicas: 3&#10;Actual replicas: 3&#10;Only two pods are actually&#10;performing their jobs&#10;Third pod&#8217;s status is Running,&#10;but its container keeps crashing,&#10;with signi&#64257;cant delays between&#10;restarts (CrashLoopBackOff)&#10;We want&#10;three pods&#10;Pod&#10;Running&#10;container&#10;Pod&#10;Running&#10;container&#10;Pod&#10;Dead&#10;container&#10;Figure 17.4&#10;A ReplicaSet controller doesn&#8217;t reschedule dead pods.&#10; &#10;"
    color "green"
  ]
  node [
    id 854
    label "515"
    title "Page_515"
    color "blue"
  ]
  node [
    id 855
    label "text_426"
    title "483&#10;Understanding the pod&#8217;s lifecycle&#10;cares about is that the number of pods matches the desired replica count, which in&#10;this case, it does.&#10; If you&#8217;d like to see for yourself, I&#8217;ve included a YAML manifest for a ReplicaSet&#10;whose pods will keep crashing (see file replicaset-crashingpods.yaml in the code&#10;archive). If you create the ReplicaSet and inspect the pods that are created, the follow-&#10;ing listing is what you&#8217;ll see.&#10;$ kubectl get po&#10;NAME                  READY     STATUS             RESTARTS   AGE&#10;crashing-pods-f1tcd   0/1       CrashLoopBackOff   5          6m     &#10;crashing-pods-k7l6k   0/1       CrashLoopBackOff   5          6m&#10;crashing-pods-z7l3v   0/1       CrashLoopBackOff   5          6m&#10;$ kubectl describe rs crashing-pods&#10;Name:           crashing-pods&#10;Replicas:       3 current / 3 desired                       &#10;Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed      &#10;$ kubectl describe po crashing-pods-f1tcd&#10;Name:           crashing-pods-f1tcd&#10;Namespace:      default&#10;Node:           minikube/192.168.99.102&#10;Start Time:     Thu, 02 Mar 2017 14:02:23 +0100&#10;Labels:         app=crashing-pods&#10;Status:         Running                      &#10;In a way, it&#8217;s understandable that Kubernetes behaves this way. The container will be&#10;restarted every five minutes in the hope that the underlying cause of the crash will be&#10;resolved. The rationale is that rescheduling the pod to another node most likely&#10;wouldn&#8217;t fix the problem anyway, because the app is running inside a container and&#10;all the nodes should be mostly equivalent. That&#8217;s not always the case, but it is most of&#10;the time. &#10;17.2.3 Starting pods in a specific order&#10;One other difference between apps running in pods and those managed manually is&#10;that the ops person deploying those apps knows about the dependencies between&#10;them. This allows them to start the apps in order. &#10;UNDERSTANDING HOW PODS ARE STARTED&#10;When you use Kubernetes to run your multi-pod applications, you don&#8217;t have a built-&#10;in way to tell Kubernetes to run certain pods first and the rest only when the first pods&#10;are already up and ready to serve. Sure, you could post the manifest for the first app&#10;and then wait for the pod(s) to be ready before you post the second manifest, but your&#10;Listing 17.1&#10;ReplicaSet and pods that keep crashing&#10;The pod&#8217;s status shows the Kubelet is&#10;delaying the restart because the&#10;container keeps crashing.&#10;No action taken &#10;by the controller, &#10;because current &#10;replicas match &#10;desired replicas&#10;Three &#10;replicas are &#10;shown as &#10;running.&#10;kubectl describe &#10;also shows pod&#8217;s &#10;status as running&#10; &#10;"
    color "green"
  ]
  node [
    id 856
    label "516"
    title "Page_516"
    color "blue"
  ]
  node [
    id 857
    label "text_427"
    title "484&#10;CHAPTER 17&#10;Best practices for developing apps&#10;whole system is usually defined in a single YAML or JSON containing multiple Pods,&#10;Services, and other objects. &#10; The Kubernetes API server does process the objects in the YAML/JSON in the&#10;order they&#8217;re listed, but this only means they&#8217;re written to etcd in that order. You have&#10;no guarantee that pods will also be started in that order. &#10; But you can prevent a pod&#8217;s main container from starting until a precondition is&#10;met. This is done by including an init containers in the pod. &#10;INTRODUCING INIT CONTAINERS&#10;In addition to regular containers, pods can also include init containers. As the name&#10;suggests, they can be used to initialize the pod&#8212;this often means writing data to the&#10;pod&#8217;s volumes, which are then mounted into the pod&#8217;s main container(s).&#10; A pod may have any number of init containers. They&#8217;re executed sequentially and&#10;only after the last one completes are the pod&#8217;s main containers started. This means&#10;init containers can also be used to delay the start of the pod&#8217;s main container(s)&#8212;for&#10;example, until a certain precondition is met. An init container could wait for a service&#10;required by the pod&#8217;s main container to be up and ready. When it is, the init container&#10;terminates and allows the main container(s) to be started. This way, the main con-&#10;tainer wouldn&#8217;t use the service before it&#8217;s ready.&#10; Let&#8217;s look at an example of a pod using an init container to delay the start of the&#10;main container. Remember the fortune pod you created in chapter 7? It&#8217;s a web&#10;server that returns a fortune quote as a response to client requests. Now, let&#8217;s imagine&#10;you have a fortune-client pod that requires the fortune Service to be up and run-&#10;ning before its main container starts. You can add an init container, which checks&#10;whether the Service is responding to requests. Until that&#8217;s the case, the init container&#10;keeps retrying. Once it gets a response, the init container terminates and lets the main&#10;container start.&#10;ADDING AN INIT CONTAINER TO A POD&#10;Init containers can be defined in the pod spec like main containers but through the&#10;spec.initContainers field. You&#8217;ll find the complete YAML for the fortune-client pod&#10;in the book&#8217;s code archive. The following listing shows the part where the init con-&#10;tainer is defined.&#10;spec:&#10;  initContainers:      &#10;  - name: init&#10;    image: busybox&#10;    command:&#10;    - sh&#10;    - -c&#10;    - 'while true; do echo &#34;Waiting for fortune service to come up...&#34;;  &#10;    &#10149; wget http://fortune -q -T 1 -O /dev/null >/dev/null 2>/dev/null   &#10;    &#10149; &#38;&#38; break; sleep 1; done; echo &#34;Service is up! Starting main       &#10;    &#10149; container.&#34;'&#10;Listing 17.2&#10;An init container defined in a pod: fortune-client.yaml&#10;You&#8217;re defining &#10;an init container, &#10;not a regular &#10;container.&#10;The init container runs a&#10;loop that runs until the&#10;fortune Service is up.&#10; &#10;"
    color "green"
  ]
  node [
    id 858
    label "517"
    title "Page_517"
    color "blue"
  ]
  node [
    id 859
    label "text_428"
    title "485&#10;Understanding the pod&#8217;s lifecycle&#10;When you deploy this pod, only its init container is started. This is shown in the pod&#8217;s&#10;status when you list pods with kubectl get:&#10;$ kubectl get po&#10;NAME             READY     STATUS     RESTARTS   AGE&#10;fortune-client   0/1       Init:0/1   0          1m&#10;The STATUS column shows that zero of one init containers have finished. You can see&#10;the log of the init container with kubectl logs:&#10;$ kubectl logs fortune-client -c init&#10;Waiting for fortune service to come up...&#10;When running the kubectl logs command, you need to specify the name of the init&#10;container with the -c switch (in the example, the name of the pod&#8217;s init container is&#10;init, as you can see in listing 17.2).&#10; The main container won&#8217;t run until you deploy the fortune Service and the&#10;fortune-server pod. You&#8217;ll find them in the fortune-server.yaml file. &#10;BEST PRACTICES FOR HANDLING INTER-POD DEPENDENCIES&#10;You&#8217;ve seen how an init container can be used to delay starting the pod&#8217;s main con-&#10;tainer(s) until a precondition is met (making sure the Service the pod depends on is&#10;ready, for example), but it&#8217;s much better to write apps that don&#8217;t require every service&#10;they rely on to be ready before the app starts up. After all, the service may also go&#10;offline later, while the app is already running.&#10; The application needs to handle internally the possibility that its dependencies&#10;aren&#8217;t ready. And don&#8217;t forget readiness probes. If an app can&#8217;t do its job because one&#10;of its dependencies is missing, it should signal that through its readiness probe, so&#10;Kubernetes knows it, too, isn&#8217;t ready. You&#8217;ll want to do this not only because it pre-&#10;vents the app from being added as a service endpoint, but also because the app&#8217;s read-&#10;iness is also used by the Deployment controller when performing a rolling update,&#10;thereby preventing a rollout of a bad version. &#10;17.2.4 Adding lifecycle hooks&#10;We&#8217;ve talked about how init containers can be used to hook into the startup of the&#10;pod, but pods also allow you to define two lifecycle hooks:&#10;&#61601;Post-start hooks&#10;&#61601;Pre-stop hooks&#10;These lifecycle hooks are specified per container, unlike init containers, which apply&#10;to the whole pod. As their names suggest, they&#8217;re executed when the container starts&#10;and before it stops. &#10; Lifecycle hooks are similar to liveness and readiness probes in that they can either&#10;&#61601;Execute a command inside the container&#10;&#61601;Perform an HTTP GET request against a URL&#10; &#10;"
    color "green"
  ]
  node [
    id 860
    label "518"
    title "Page_518"
    color "blue"
  ]
  node [
    id 861
    label "text_429"
    title "486&#10;CHAPTER 17&#10;Best practices for developing apps&#10;Let&#8217;s look at the two hooks individually to see what effect they have on the container&#10;lifecycle.&#10;USING A POST-START CONTAINER LIFECYCLE HOOK&#10;A post-start hook is executed immediately after the container&#8217;s main process is started.&#10;You use it to perform additional operations when the application starts. Sure, if you&#8217;re&#10;the author of the application running in the container, you can always perform those&#10;operations inside the application code itself. But when you&#8217;re running an application&#10;developed by someone else, you mostly don&#8217;t want to (or can&#8217;t) modify its source&#10;code. Post-start hooks allow you to run additional commands without having to touch&#10;the app. These may signal to an external listener that the app is starting, or they may&#10;initialize the application so it can start doing its job.&#10; The hook is run in parallel with the main process. The name might be somewhat&#10;misleading, because it doesn&#8217;t wait for the main process to start up fully (if the process&#10;has an initialization procedure, the Kubelet obviously can&#8217;t wait for the procedure to&#10;complete, because it has no way of knowing when that is). &#10; But even though the hook runs asynchronously, it does affect the container in two&#10;ways. Until the hook completes, the container will stay in the Waiting state with the&#10;reason ContainerCreating. Because of this, the pod&#8217;s status will be Pending instead of&#10;Running. If the hook fails to run or returns a non-zero exit code, the main container&#10;will be killed. &#10; A pod manifest containing a post-start hook looks like the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-with-poststart-hook&#10;spec:&#10;  containers:&#10;  - image: luksa/kubia&#10;    name: kubia&#10;    lifecycle:          &#10;      postStart:        &#10;        exec:                                                               &#10;          command:                                                          &#10;          - sh                                                              &#10;          - -c                                                              &#10;          - &#34;echo 'hook will fail with exit code 15'; sleep 5; exit 15&#34;     &#10;In the example, the echo, sleep, and exit commands are executed along with the&#10;container&#8217;s main process as soon as the container is created. Rather than run a com-&#10;mand like this, you&#8217;d typically run a shell script or a binary executable file stored in&#10;the container image. &#10; Sadly, if the process started by the hook logs to the standard output, you can&#8217;t see&#10;the output anywhere. This makes debugging lifecycle hooks painful. If the hook fails,&#10;Listing 17.3&#10;A pod with a post-start lifecycle hook: post-start-hook.yaml&#10;The hook is executed as &#10;the container starts.&#10;It executes the&#10;postStart.sh&#10;script in the /bin&#10;directory inside&#10;the container.&#10; &#10;"
    color "green"
  ]
  node [
    id 862
    label "519"
    title "Page_519"
    color "blue"
  ]
  node [
    id 863
    label "text_430"
    title "487&#10;Understanding the pod&#8217;s lifecycle&#10;you&#8217;ll only see a FailedPostStartHook warning among the pod&#8217;s events (you can see&#10;them using kubectl describe pod). A while later, you&#8217;ll see more information on why&#10;the hook failed, as shown in the following listing.&#10;FailedSync   Error syncing pod, skipping: failed to &#34;StartContainer&#34; for &#10;             &#34;kubia&#34; with PostStart handler: command 'sh -c echo 'hook &#10;             will fail with exit code 15'; sleep 5 ; exit 15' exited &#10;             with 15: : &#34;PostStart Hook Failed&#34; &#10;The number 15 in the last line is the exit code of the command. When using an HTTP&#10;GET hook handler, the reason may look like the following listing (you can try this by&#10;deploying the post-start-hook-httpget.yaml file from the book&#8217;s code archive).&#10;FailedSync   Error syncing pod, skipping: failed to &#34;StartContainer&#34; for &#10;             &#34;kubia&#34; with PostStart handler: Get &#10;             http://10.32.0.2:9090/postStart: dial tcp 10.32.0.2:9090: &#10;             getsockopt: connection refused: &#34;PostStart Hook Failed&#34; &#10;NOTE&#10;The post-start hook is intentionally misconfigured to use port 9090&#10;instead of the correct port 8080, to show what happens when the hook fails.&#10;The standard and error outputs of command-based post-start hooks aren&#8217;t logged any-&#10;where, so you may want to have the process the hook invokes log to a file in the con-&#10;tainer&#8217;s filesystem, which will allow you to examine the contents of the file with&#10;something like this:&#10;$ kubectl exec my-pod cat logfile.txt &#10;If the container gets restarted for whatever reason (including because the hook failed),&#10;the file may be gone before you can examine it. You can work around that by mount-&#10;ing an emptyDir volume into the container and having the hook write to it.&#10;USING A PRE-STOP CONTAINER LIFECYCLE HOOK&#10;A pre-stop hook is executed immediately before a container is terminated. When a&#10;container needs to be terminated, the Kubelet will run the pre-stop hook, if config-&#10;ured, and only then send a SIGTERM to the process (and later kill the process if it&#10;doesn&#8217;t terminate gracefully). &#10; A pre-stop hook can be used to initiate a graceful shutdown of the container, if it&#10;doesn&#8217;t shut down gracefully upon receipt of a SIGTERM signal. They can also be used&#10;to perform arbitrary operations before shutdown without having to implement those&#10;operations in the application itself (this is useful when you&#8217;re running a third-party&#10;app, whose source code you don&#8217;t have access to and/or can&#8217;t modify). &#10; Configuring a pre-stop hook in a pod manifest isn&#8217;t very different from adding a&#10;post-start hook. The previous example showed a post-start hook that executes a com-&#10;Listing 17.4&#10;Pod&#8217;s events showing the exit code of the failed command-based hook&#10;Listing 17.5&#10;Pod&#8217;s events showing the reason why an HTTP GET hook failed&#10; &#10;"
    color "green"
  ]
  node [
    id 864
    label "520"
    title "Page_520"
    color "blue"
  ]
  node [
    id 865
    label "text_431"
    title "488&#10;CHAPTER 17&#10;Best practices for developing apps&#10;mand, so we&#8217;ll look at a pre-stop hook that performs an HTTP GET request now. The&#10;following listing shows how to define a pre-stop HTTP GET hook in a pod.&#10;    lifecycle:&#10;      preStop:            &#10;        httpGet:          &#10;          port: 8080          &#10;          path: shutdown      &#10;The pre-stop hook defined in this listing performs an HTTP GET request to http:/&#10;/&#10;POD_IP:8080/shutdown as soon as the Kubelet starts terminating the container.&#10;Apart from the port and path shown in the listing, you can also set the fields scheme&#10;(HTTP or HTTPS) and host, as well as httpHeaders that should be sent in the&#10;request. The host field defaults to the pod IP. Be sure not to set it to localhost,&#10;because localhost would refer to the node, not the pod.&#10; In contrast to the post-start hook, the container will be terminated regardless of&#10;the result of the hook&#8212;an error HTTP response code or a non-zero exit code when&#10;using a command-based hook will not prevent the container from being terminated.&#10;If the pre-stop hook fails, you&#8217;ll see a FailedPreStopHook warning event among the&#10;pod&#8217;s events, but because the pod is deleted soon afterward (after all, the pod&#8217;s dele-&#10;tion is what triggered the pre-stop hook in the first place), you may not even notice&#10;that the pre-stop hook failed to run properly. &#10;TIP&#10;If the successful completion of the pre-stop hook is critical to the proper&#10;operation of your system, verify whether it&#8217;s being executed at all. I&#8217;ve wit-&#10;nessed situations where the pre-stop hook didn&#8217;t run and the developer&#10;wasn&#8217;t even aware of that.&#10;USING A PRE-STOP HOOK BECAUSE YOUR APP DOESN&#8217;T RECEIVE THE SIGTERM SIGNAL&#10;Many developers make the mistake of defining a pre-stop hook solely to send a SIGTERM&#10;signal to their apps in the pre-stop hook. They do this because they don&#8217;t see their appli-&#10;cation receive the SIGTERM signal sent by the Kubelet. The reason why the signal isn&#8217;t&#10;received by the application isn&#8217;t because Kubernetes isn&#8217;t sending it, but because the sig-&#10;nal isn&#8217;t being passed to the app process inside the container itself. If your container&#10;image is configured to run a shell, which in turn runs the app process, the signal may be&#10;eaten up by the shell itself, instead of being passed down to the child process.&#10; In such cases, instead of adding a pre-stop hook to send the signal directly to your&#10;app, the proper fix is to make sure the shell passes the signal to the app. This can be&#10;achieved by handling the signal in the shell script running as the main container pro-&#10;cess and then passing it on to the app. Or you could not configure the container image&#10;to run a shell at all and instead run the application binary directly. You do this by using&#10;the exec form of ENTRYPOINT or CMD in the Dockerfile: ENTRYPOINT [&#34;/mybinary&#34;]&#10;instead of ENTRYPOINT /mybinary.&#10;Listing 17.6&#10;A pre-stop hook YAML snippet: pre-stop-hook-httpget.yaml&#10;This is a pre-stop hook that &#10;performs an HTTP GET request.&#10;The request is sent to &#10;http://POD_IP:8080/shutdown.&#10; &#10;"
    color "green"
  ]
  node [
    id 866
    label "521"
    title "Page_521"
    color "blue"
  ]
  node [
    id 867
    label "text_432"
    title "489&#10;Understanding the pod&#8217;s lifecycle&#10; A container using the first form runs the mybinary executable as its main process,&#10;whereas the second form runs a shell as the main process with the mybinary process&#10;executed as a child of the shell process.&#10;UNDERSTANDING THAT LIFECYCLE HOOKS TARGET CONTAINERS, NOT PODS&#10;As a final thought on post-start and pre-stop hooks, let me emphasize that these lifecy-&#10;cle hooks relate to containers, not pods. You shouldn&#8217;t use a pre-stop hook for run-&#10;ning actions that need to be performed when the pod is terminating. The reason is&#10;that the pre-stop hook gets called when the container is being terminated (most likely&#10;because of a failed liveness probe). This may happen multiple times in the pod&#8217;s life-&#10;time, not only when the pod is in the process of being shut down. &#10;17.2.5 Understanding pod shutdown&#10;We&#8217;ve touched on the subject of pod termination, so let&#8217;s explore this subject in more&#10;detail and go over exactly what happens during pod shutdown. This is important for&#10;understanding how to cleanly shut down an application running in a pod.&#10; Let&#8217;s start at the beginning. A pod&#8217;s shut-down is triggered by the deletion of the&#10;Pod object through the API server. Upon receiving an HTTP DELETE request, the&#10;API server doesn&#8217;t delete the object yet, but only sets a deletionTimestamp field in it.&#10;Pods that have the deletionTimestamp field set are terminating. &#10; Once the Kubelet notices the pod needs to be terminated, it starts terminating&#10;each of the pod&#8217;s containers. It gives each container time to shut down gracefully, but&#10;the time is limited. That time is called the termination grace period and is configu-&#10;rable per pod. The timer starts as soon as the termination process starts. Then the fol-&#10;lowing sequence of events is performed:&#10;1&#10;Run the pre-stop hook, if one is configured, and wait for it to finish.&#10;2&#10;Send the SIGTERM signal to the main process of the container.&#10;3&#10;Wait until the container shuts down cleanly or until the termination grace&#10;period runs out.&#10;4&#10;Forcibly kill the process with SIGKILL, if it hasn&#8217;t terminated gracefully yet.&#10;The sequence of events is illustrated in figure 17.5.&#10;Pre-stop hook process&#10;Termination grace period&#10;Main container process&#10;Container shutdown&#10;initiated&#10;Container killed&#10;if still running&#10;Time&#10;SIGTERM&#10;SIGKILL&#10;Figure 17.5&#10;The container termination sequence&#10; &#10;"
    color "green"
  ]
  node [
    id 868
    label "522"
    title "Page_522"
    color "blue"
  ]
  node [
    id 869
    label "text_433"
    title "490&#10;CHAPTER 17&#10;Best practices for developing apps&#10;SPECIFYING THE TERMINATION GRACE PERIOD&#10;The termination grace period can be configured in the pod spec by setting the spec.&#10;terminationGracePeriodSeconds field. It defaults to 30, which means the pod&#8217;s con-&#10;tainers will be given 30 seconds to terminate gracefully before they&#8217;re killed forcibly. &#10;TIP&#10;You should set the grace period to long enough so your process can fin-&#10;ish cleaning up in that time. &#10;The grace period specified in the pod spec can also be overridden when deleting the&#10;pod like this:&#10;$ kubectl delete po mypod --grace-period=5&#10;This will make the Kubelet wait five seconds for the pod to shut down cleanly. When&#10;all the pod&#8217;s containers stop, the Kubelet notifies the API server and the Pod resource&#10;is finally deleted. You can force the API server to delete the resource immediately,&#10;without waiting for confirmation, by setting the grace period to zero and adding the&#10;--force option like this:&#10;$ kubectl delete po mypod --grace-period=0 --force&#10;Be careful when using this option, especially with pods of a StatefulSet. The Stateful-&#10;Set controller takes great care to never run two instances of the same pod at the same&#10;time (two pods with the same ordinal index and name and attached to the same&#10;PersistentVolume). By force-deleting a pod, you&#8217;ll cause the controller to create a&#10;replacement pod without waiting for the containers of the deleted pod to shut&#10;down. In other words, two instances of the same pod might be running at the same&#10;time, which may cause your stateful cluster to malfunction. Only delete stateful pods&#10;forcibly when you&#8217;re absolutely sure the pod isn&#8217;t running anymore or can&#8217;t talk to&#10;the other members of the cluster (you can be sure of this when you confirm that the&#10;node that hosted the pod has failed or has been disconnected from the network and&#10;can&#8217;t reconnect). &#10; Now that you understand how containers are shut down, let&#8217;s look at it from the&#10;application&#8217;s perspective and go over how applications should handle the shutdown&#10;procedure.&#10;IMPLEMENTING THE PROPER SHUTDOWN HANDLER IN YOUR APPLICATION&#10;Applications should react to a SIGTERM signal by starting their shut-down procedure&#10;and terminating when it finishes. Instead of handling the SIGTERM signal, the applica-&#10;tion can be notified to shut down through a pre-stop hook. In both cases, the app&#10;then only has a fixed amount of time to terminate cleanly. &#10; But what if you can&#8217;t predict how long the app will take to shut down cleanly? For&#10;example, imagine your app is a distributed data store. On scale-down, one of the pod&#10;instances will be deleted and therefore shut down. In the shut-down procedure, the&#10; &#10;"
    color "green"
  ]
  node [
    id 870
    label "523"
    title "Page_523"
    color "blue"
  ]
  node [
    id 871
    label "text_434"
    title "491&#10;Understanding the pod&#8217;s lifecycle&#10;pod needs to migrate all its data to the remaining pods to make sure it&#8217;s not lost.&#10;Should the pod start migrating the data upon receiving a termination signal (through&#10;either the SIGTERM signal or through a pre-stop hook)? &#10; Absolutely not! This is not recommended for at least the following two reasons:&#10;&#61601;A container terminating doesn&#8217;t necessarily mean the whole pod is being&#10;terminated.&#10;&#61601;You have no guarantee the shut-down procedure will finish before the process&#10;is killed.&#10;This second scenario doesn&#8217;t happen only when the grace period runs out before the&#10;application has finished shutting down gracefully, but also when the node running&#10;the pod fails in the middle of the container shut-down sequence. Even if the node&#10;then starts up again, the Kubelet will not restart the shut-down procedure (it won&#8217;t&#10;even start up the container again). There are absolutely no guarantees that the pod&#10;will be allowed to complete its whole shut-down procedure.&#10;REPLACING CRITICAL SHUT-DOWN PROCEDURES WITH DEDICATED SHUT-DOWN PROCEDURE PODS&#10;How do you ensure that a critical shut-down procedure that absolutely must run to&#10;completion does run to completion (for example, to ensure that a pod&#8217;s data is&#10;migrated to other pods)?&#10; One solution is for the app (upon receipt of a termination signal) to create a new&#10;Job resource that would run a new pod, whose sole job is to migrate the deleted pod&#8217;s&#10;data to the remaining pods. But if you&#8217;ve been paying attention, you&#8217;ll know that you&#10;have no guarantee the app will indeed manage to create the Job object every single&#10;time. What if the node fails exactly when the app tries to do that? &#10; The proper way to handle this problem is by having a dedicated, constantly run-&#10;ning pod that keeps checking for the existence of orphaned data. When this pod finds&#10;the orphaned data, it can migrate it to the remaining pods. Rather than a constantly&#10;running pod, you can also use a CronJob resource and run the pod periodically. &#10; You may think StatefulSets could help here, but they don&#8217;t. As you&#8217;ll remember,&#10;scaling down a StatefulSet leaves PersistentVolumeClaims orphaned, leaving the data&#10;stored on the PersistentVolume stranded. Yes, upon a subsequent scale-up, the Persistent-&#10;Volume will be reattached to the new pod instance, but what if that scale-up never&#10;happens (or happens after a long time)? For this reason, you may want to run a&#10;data-migrating pod also when using StatefulSets (this scenario is shown in figure 17.6).&#10;To prevent the migration from occurring during an application upgrade, the data-&#10;migrating pod could be configured to wait a while to give the stateful pod time to&#10;come up again before performing the migration.&#10; &#10; &#10; &#10;"
    color "green"
  ]
  node [
    id 872
    label "524"
    title "Page_524"
    color "blue"
  ]
  node [
    id 873
    label "text_435"
    title "492&#10;CHAPTER 17&#10;Best practices for developing apps&#10;17.3&#10;Ensuring all client requests are handled properly&#10;You now have a good sense of how to make pods shut down cleanly. Now, we&#8217;ll look at&#10;the pod&#8217;s lifecycle from the perspective of the pod&#8217;s clients (clients consuming the ser-&#10;vice the pod is providing). This is important to understand if you don&#8217;t want clients to&#10;run into problems when you scale pods up or down.&#10; It goes without saying that you want all client requests to be handled properly. You&#10;obviously don&#8217;t want to see broken connections when pods are starting up or shutting&#10;down. By itself, Kubernetes doesn&#8217;t prevent this from happening. Your app needs to&#10;follow a few rules to prevent broken connections. First, let&#8217;s focus on making sure all&#10;connections are handled properly when the pod starts up.&#10;17.3.1 Preventing broken client connections when a pod is starting up&#10;Ensuring each connection is handled properly at pod startup is simple if you under-&#10;stand how Services and service Endpoints work. When a pod is started, it&#8217;s added as an&#10;endpoint to all the Services, whose label selector matches the pod&#8217;s labels. As you may&#10;remember from chapter 5, the pod also needs to signal to Kubernetes that it&#8217;s ready.&#10;Until it is, it won&#8217;t become a service endpoint and therefore won&#8217;t receive any requests&#10;from clients. &#10; If you don&#8217;t specify a readiness probe in your pod spec, the pod is always considered&#10;ready. It will start receiving requests almost immediately&#8212;as soon as the first kube-proxy&#10;updates the iptables rules on its node and the first client pod tries to connect to the&#10;service. If your app isn&#8217;t ready to accept connections by then, clients will see &#8220;connec-&#10;tion refused&#8221; types of errors.&#10; All you need to do is make sure that your readiness probe returns success only&#10;when your app is ready to properly handle incoming requests. A good first step is to&#10;add an HTTP GET readiness probe and point it to the base URL of your app. In many&#10;Pod&#10;A-0&#10;Pod&#10;A-1&#10;StatefulSet A&#10;Replicas: 2&#10;Scale&#10;down&#10;PVC&#10;A-0&#10;PV&#10;PVC&#10;A-1&#10;PV&#10;Pod&#10;A-0&#10;StatefulSet A&#10;Replicas: 1&#10;Transfers data to&#10;remaining pod(s)&#10;Connects to&#10;orphaned PVC&#10;Data-migrating&#10;Pod&#10;Job&#10;PVC&#10;A-0&#10;PV&#10;PVC&#10;A-1&#10;PV&#10;Figure 17.6&#10;Using a dedicated pod to migrate data &#10; &#10;"
    color "green"
  ]
  node [
    id 874
    label "525"
    title "Page_525"
    color "blue"
  ]
  node [
    id 875
    label "text_436"
    title "493&#10;Ensuring all client requests are handled properly&#10;cases that gets you far enough and saves you from having to implement a special read-&#10;iness endpoint in your app. &#10;17.3.2 Preventing broken connections during pod shut-down&#10;Now let&#8217;s see what happens at the other end of a pod&#8217;s life&#8212;when the pod is deleted and&#10;its containers are terminated. We&#8217;ve already talked about how the pod&#8217;s containers&#10;should start shutting down cleanly as soon they receive the SIGTERM signal (or when its&#10;pre-stop hook is executed). But does that ensure all client requests are handled properly? &#10; How should the app behave when it receives a termination signal? Should it con-&#10;tinue to accept requests? What about requests that have already been received but&#10;haven&#8217;t completed yet? What about persistent HTTP connections, which may be in&#10;between requests, but are open (when no active request exists on the connection)?&#10;Before we can answer those questions, we need to take a detailed look at the chain of&#10;events that unfolds across the cluster when a Pod is deleted. &#10;UNDERSTANDING THE SEQUENCE OF EVENTS OCCURRING AT POD DELETION&#10;In chapter 11 we took an in-depth look at what components make up a Kubernetes clus-&#10;ter. You need to always keep in mind that those components run as separate processes on&#10;multiple machines. They aren&#8217;t all part of a single big monolithic process. It takes time&#10;for all the components to be on the same page regarding the state of the cluster. Let&#8217;s&#10;explore this fact by looking at what happens across the cluster when a Pod is deleted.&#10; When a request for a pod deletion is received by the API server, it first modifies the&#10;state in etcd and then notifies its watchers of the deletion. Among those watchers are&#10;the Kubelet and the Endpoints controller. The two sequences of events, which happen&#10;in parallel (marked with either A or B), are shown in figure 17.7.&#10;A2. Stop&#10;containers&#10;API server&#10;kube-proxy&#10;Kubelet&#10;Worker node&#10;Endpoints&#10;controller&#10;kube-proxy&#10;Pod&#10;(containers)&#10;Client&#10;Delete&#10;pod&#10;B1. Pod deletion&#10;noti&#64257;cation&#10;B2. Remove pod&#10;as endpoint&#10;A1. Pod deletion&#10;noti&#64257;cation&#10;B3. Endpoint&#10;modi&#64257;cation&#10;noti&#64257;cation&#10;B4. Remove pod&#10;from iptables&#10;B4. Remove pod&#10;from iptables&#10;iptables&#10;iptables&#10;Worker node&#10;Figure 17.7&#10;Sequence of events that occurs when a Pod is deleted&#10; &#10;"
    color "green"
  ]
  node [
    id 876
    label "526"
    title "Page_526"
    color "blue"
  ]
  node [
    id 877
    label "text_437"
    title "494&#10;CHAPTER 17&#10;Best practices for developing apps&#10;In the A sequence of events, you&#8217;ll see that as soon as the Kubelet receives the notifica-&#10;tion that the pod should be terminated, it initiates the shutdown sequence as explained&#10;in section 17.2.5 (run the pre-stop hook, send SIGTERM, wait for a period of time, and&#10;then forcibly kill the container if it hasn&#8217;t yet terminated on its own). If the app&#10;responds to the SIGTERM by immediately ceasing to receive client requests, any client&#10;trying to connect to it will receive a Connection Refused error. The time it takes for&#10;this to happen from the time the pod is deleted is relatively short because of the direct&#10;path from the API server to the Kubelet.&#10; Now, let&#8217;s look at what happens in the other sequence of events&#8212;the one leading&#10;up to the pod being removed from the iptables rules (sequence B in the figure).&#10;When the Endpoints controller (which runs in the Controller Manager in the Kuber-&#10;netes Control Plane) receives the notification of the Pod being deleted, it removes&#10;the pod as an endpoint in all services that the pod is a part of. It does this by modify-&#10;ing the Endpoints API object by sending a REST request to the API server. The API&#10;server then notifies all clients watching the Endpoints object. Among those watchers&#10;are all the kube-proxies running on the worker nodes. Each of these proxies then&#10;updates the iptables rules on its node, which is what prevents new connections&#10;from being forwarded to the terminating pod. An important detail here is that&#10;removing the iptables rules has no effect on existing connections&#8212;clients who are&#10;already connected to the pod will still send additional requests to the pod through&#10;those existing connections.&#10; Both of these sequences of events happen in parallel. Most likely, the time it takes&#10;to shut down the app&#8217;s process in the pod is slightly shorter than the time required for&#10;the iptables rules to be updated. The chain of events that leads to iptables rules&#10;being updated is considerably longer (see figure 17.8), because the event must first&#10;reach the Endpoints controller, which then sends a new request to the API server, and&#10;A2. Send&#10;SIGTERM&#10;API server&#10;API server&#10;Kubelet&#10;Endpoints&#10;controller&#10;Container(s)&#10;A1. Watch&#10;noti&#64257;cation&#10;(pod modi&#64257;ed)&#10;B1. Watch&#10;noti&#64257;cation&#10;(pod modi&#64257;ed)&#10;B2. Remove pod&#8217;s IP&#10;from endpoints&#10;kube-proxy&#10;B4. Update&#10;iptables&#10;rules&#10;iptables&#10;kube-proxy&#10;iptables&#10;Time&#10;B3. Watch noti&#64257;cation&#10;(endpoints changed)&#10;Figure 17.8&#10;Timeline of events when pod is deleted&#10; &#10;"
    color "green"
  ]
  node [
    id 878
    label "527"
    title "Page_527"
    color "blue"
  ]
  node [
    id 879
    label "text_438"
    title "495&#10;Ensuring all client requests are handled properly&#10;then the API server must notify the kube-proxy before the proxy finally modifies the&#10;iptables rules. A high probability exists that the SIGTERM signal will be sent well&#10;before the iptables rules are updated on all nodes.&#10; The end result is that the pod may still receive client requests after it was sent the&#10;termination signal. If the app closes the server socket and stops accepting connections&#10;immediately, this will cause clients to receive &#8220;Connection Refused&#8221; types of errors&#10;(similar to what happens at pod startup if your app isn&#8217;t capable of accepting connec-&#10;tions immediately and you don&#8217;t define a readiness probe for it). &#10;SOLVING THE PROBLEM&#10;Googling solutions to this problem makes it seem as though adding a readiness probe&#10;to your pod will solve the problem. Supposedly, all you need to do is make the readi-&#10;ness probe start failing as soon as the pod receives the SIGTERM. This is supposed to&#10;cause the pod to be removed as the endpoint of the service. But the removal would&#10;happen only after the readiness probe fails for a few consecutive times (this is configu-&#10;rable in the readiness probe spec). And, obviously, the removal then still needs to&#10;reach the kube-proxy before the pod is removed from iptables rules. &#10; In reality, the readiness probe has absolutely no bearing on the whole process at&#10;all. The Endpoints controller removes the pod from the service Endpoints as soon as&#10;it receives notice of the pod being deleted (when the deletionTimestamp field in the&#10;pod&#8217;s spec is no longer null). From that point on, the result of the readiness probe&#10;is irrelevant.&#10; What&#8217;s the proper solution to the problem? How can you make sure all requests&#10;are handled fully?&#10; It&#8217;s clear the pod needs to keep accepting connections even after it receives the ter-&#10;mination signal up until all the kube-proxies have finished updating the iptables&#10;rules. Well, it&#8217;s not only the kube-proxies. There may also be Ingress controllers or&#10;load balancers forwarding connections to the pod directly, without going through the&#10;Service (iptables). This also includes clients using client-side load-balancing. To&#10;ensure none of the clients experience broken connections, you&#8217;d have to wait until all&#10;of them somehow notify you they&#8217;ll no longer forward connections to the pod. &#10; That&#8217;s impossible, because all those components are distributed across many dif-&#10;ferent computers. Even if you knew the location of every one of them and could wait&#10;until all of them say it&#8217;s okay to shut down the pod, what do you do if one of them&#10;doesn&#8217;t respond? How long do you wait for the response? Remember, during that&#10;time, you&#8217;re holding up the shut-down process. &#10; The only reasonable thing you can do is wait for a long-enough time to ensure all&#10;the proxies have done their job. But how long is long enough? A few seconds should&#10;be enough in most situations, but there&#8217;s no guarantee it will suffice every time. When&#10;the API server or the Endpoints controller is overloaded, it may take longer for the&#10;notification to reach the kube-proxy. It&#8217;s important to understand that you can&#8217;t solve&#10;the problem perfectly, but even adding a 5- or 10-second delay should improve the&#10;user experience considerably. You can use a longer delay, but don&#8217;t go overboard,&#10; &#10;"
    color "green"
  ]
  node [
    id 880
    label "528"
    title "Page_528"
    color "blue"
  ]
  node [
    id 881
    label "text_439"
    title "496&#10;CHAPTER 17&#10;Best practices for developing apps&#10;because the delay will prevent the container from shutting down promptly and will&#10;cause the pod to be shown in lists long after it has been deleted, which is always frus-&#10;trating to the user deleting the pod.&#10;WRAPPING UP THIS SECTION&#10;To recap&#8212;properly shutting down an application includes these steps:&#10;&#61601;Wait for a few seconds, then stop accepting new connections. &#10;&#61601;Close all keep-alive connections not in the middle of a request.&#10;&#61601;Wait for all active requests to finish.&#10;&#61601;Then shut down completely.&#10;To understand what&#8217;s happening with the connections and requests during this pro-&#10;cess, examine figure 17.9 carefully.&#10;Not as simple as exiting the process immediately upon receiving the termination sig-&#10;nal, right? Is it worth going through all this? That&#8217;s for you to decide. But the least you&#10;can do is add a pre-stop hook that waits a few seconds, like the one in the following&#10;listing, perhaps.&#10;    lifecycle:                    &#10;      preStop:                    &#10;        exec:                     &#10;          command:                &#10;          - sh&#10;          - -c&#10;          - &#34;sleep 5&#34;&#10;Listing 17.7&#10;A pre-stop hook for preventing broken connections&#10;Delay (few seconds)&#10;Key:&#10;Connection&#10;Request&#10;iptables rules&#10;updated on all nodes&#10;(no new connections&#10;after this point)&#10;Stop&#10;accepting new&#10;connections&#10;Close inactive&#10;keep-alive&#10;connections&#10;and wait for&#10;active requests&#10;to &#64257;nish&#10;When last&#10;active request&#10;completes,&#10;shut down&#10;completely&#10;Time&#10;SIGTERM&#10;Figure 17.9&#10;Properly handling existing and new connections after receiving a termination signal&#10; &#10;"
    color "green"
  ]
  node [
    id 882
    label "529"
    title "Page_529"
    color "blue"
  ]
  node [
    id 883
    label "text_440"
    title "497&#10;Making your apps easy to run and manage in Kubernetes&#10;This way, you don&#8217;t need to modify the code of your app at all. If your app already&#10;ensures all in-flight requests are processed completely, this pre-stop delay may be all&#10;you need.&#10;17.4&#10;Making your apps easy to run and manage in Kubernetes&#10;I hope you now have a better sense of how to make your apps handle clients nicely.&#10;Now we&#8217;ll look at other aspects of how an app should be built to make it easier to man-&#10;age in Kubernetes.&#10;17.4.1 Making manageable container images&#10;When you package your app into an image, you can choose to include the app&#8217;s&#10;binary executable and any additional libraries it needs, or you can package up a whole&#10;OS filesystem along with the app. Way too many people do this, even though it&#8217;s usu-&#10;ally unnecessary.&#10; Do you need every single file from an OS distribution in your image? Probably not.&#10;Most of the files will never be used and will make your image larger than it needs to&#10;be. Sure, the layering of images makes sure each individual layer is downloaded only&#10;once, but even having to wait longer than necessary the first time a pod is scheduled&#10;to a node is undesirable.&#10; Deploying new pods and scaling them should be fast. This demands having small&#10;images without unnecessary cruft. If you&#8217;re building apps using the Go language, your&#10;images don&#8217;t need to include anything else apart from the app&#8217;s single binary execut-&#10;able file. This makes Go-based container images extremely small and perfect for&#10;Kubernetes.&#10;TIP&#10;Use the FROM scratch directive in the Dockerfile for these images.&#10;But in practice, you&#8217;ll soon see these minimal images are extremely difficult to debug.&#10;The first time you need to run a tool such as ping, dig, curl, or something similar&#10;inside the container, you&#8217;ll realize how important it is for container images to also&#10;include at least a limited set of these tools. I can&#8217;t tell you what to include and what&#10;not to include in your images, because it depends on how you do things, so you&#8217;ll&#10;need to find the sweet spot yourself.&#10;17.4.2 Properly tagging your images and using imagePullPolicy wisely&#10;You&#8217;ll also soon learn that referring to the latest image tag in your pod manifests will&#10;cause problems, because you can&#8217;t tell which version of the image each individual pod&#10;replica is running. Even if initially all your pod replicas run the same image version, if&#10;you push a new version of the image under the latest tag, and then pods are resched-&#10;uled (or you scale up your Deployment), the new pods will run the new version,&#10;whereas the old ones will still be running the old one. Also, using the latest tag&#10;makes it impossible to roll back to a previous version (unless you push the old version&#10;of the image again).&#10; &#10;"
    color "green"
  ]
  node [
    id 884
    label "530"
    title "Page_530"
    color "blue"
  ]
  node [
    id 885
    label "text_441"
    title "498&#10;CHAPTER 17&#10;Best practices for developing apps&#10; It&#8217;s almost mandatory to use tags containing a proper version designator instead&#10;of latest, except maybe in development. Keep in mind that if you use mutable tags&#10;(you push changes to the same tag), you&#8217;ll need to set the imagePullPolicy field in&#10;the pod spec to Always. But if you use that in production pods, be aware of the big&#10;caveat associated with it. If the image pull policy is set to Always, the container run-&#10;time will contact the image registry every time a new pod is deployed. This slows&#10;down pod startup a bit, because the node needs to check if the image has been mod-&#10;ified. Worse yet, this policy prevents the pod from starting up when the registry can-&#10;not be contacted.&#10;17.4.3 Using multi-dimensional instead of single-dimensional labels&#10;Don&#8217;t forget to label all your resources, not only Pods. Make sure you add multiple&#10;labels to each resource, so they can be selected across each individual dimension. You&#10;(or the ops team) will be grateful you did it when the number of resources increases.&#10; Labels may include things like&#10;&#61601;The name of the application (or perhaps microservice) the resource belongs to&#10;&#61601;Application tier (front-end, back-end, and so on)&#10;&#61601;Environment (development, QA, staging, production, and so on)&#10;&#61601;Version&#10;&#61601;Type of release (stable, canary, green or blue for green/blue deployments, and&#10;so on)&#10;&#61601;Tenant (if you&#8217;re running separate pods for each tenant instead of using name-&#10;spaces)&#10;&#61601;Shard for sharded systems&#10;This will allow you to manage resources in groups instead of individually and make it&#10;easy to see where each resource belongs.&#10;17.4.4 Describing each resource through annotations&#10;To add additional information to your resources use annotations. At the least,&#10;resources should contain an annotation describing the resource and an annotation&#10;with contact information of the person responsible for it. &#10; In a microservices architecture, pods could contain an annotation that lists the&#10;names of the other services the pod is using. This makes it possible to show dependen-&#10;cies between pods. Other annotations could include build and version information&#10;and metadata used by tooling or graphical user interfaces (icon names, and so on).&#10; Both labels and annotations make managing running applications much easier, but&#10;nothing is worse than when an application starts crashing and you don&#8217;t know why.&#10;17.4.5 Providing information on why the process terminated&#10;Nothing is more frustrating than having to figure out why a container terminated&#10;(or is even terminating continuously), especially if it happens at the worst possible&#10; &#10;"
    color "green"
  ]
  node [
    id 886
    label "531"
    title "Page_531"
    color "blue"
  ]
  node [
    id 887
    label "text_442"
    title "499&#10;Making your apps easy to run and manage in Kubernetes&#10;moment. Be nice to the ops people and make their lives easier by including all the&#10;necessary debug information in your log files. &#10; But to make triage even easier, you can use one other Kubernetes feature that&#10;makes it possible to show the reason why a container terminated in the pod&#8217;s status.&#10;You do this by having the process write a termination message to a specific file in the&#10;container&#8217;s filesystem. The contents of this file are read by the Kubelet when the con-&#10;tainer terminates and are shown in the output of kubectl describe pod. If an applica-&#10;tion uses this mechanism, an operator can quickly see why the app terminated without&#10;even having to look at the container logs. &#10; The default file the process needs to write the message to is /dev/termination-log,&#10;but it can be changed by setting the terminationMessagePath field in the container&#10;definition in the pod spec. &#10; You can see this in action by running a pod whose container dies immediately, as&#10;shown in the following listing.&#10;apiVersion: v1&#10;kind: Pod&#10;metadata:&#10;  name: pod-with-termination-message&#10;spec:&#10;  containers:&#10;  - image: busybox&#10;    name: main&#10;    terminationMessagePath: /var/termination-reason         &#10;    command:&#10;    - sh&#10;    - -c&#10;    - 'echo &#34;I''ve had enough&#34; > /var/termination-reason ; exit 1'   &#10;When running this pod, you&#8217;ll soon see the pod&#8217;s status shown as CrashLoopBackOff.&#10;If you then use kubectl describe, you can see why the container died, without having&#10;to dig down into its logs, as shown in the following listing.&#10;$ kubectl describe po&#10;Name:           pod-with-termination-message&#10;...&#10;Containers:&#10;...&#10;    State:      Waiting&#10;      Reason:   CrashLoopBackOff&#10;    Last State: Terminated&#10;      Reason:   Error&#10;      Message:  I've had enough          &#10;      Exit Code:        1&#10;      Started:          Tue, 21 Feb 2017 21:38:31 +0100&#10;      Finished:         Tue, 21 Feb 2017 21:38:31 +0100&#10;Listing 17.8&#10;Pod writing a termination message: termination-message.yaml&#10;Listing 17.9&#10;Seeing the container&#8217;s termination message with kubectl describe&#10;You&#8217;re overriding the &#10;default path of the &#10;termination message file.&#10;The container&#10;will write the&#10;message to&#10;the file just&#10;before exiting.&#10;You can see the reason &#10;why the container died &#10;without having to &#10;inspect its logs.&#10; &#10;"
    color "green"
  ]
  node [
    id 888
    label "532"
    title "Page_532"
    color "blue"
  ]
  node [
    id 889
    label "text_443"
    title "500&#10;CHAPTER 17&#10;Best practices for developing apps&#10;    Ready:              False&#10;    Restart Count:      6&#10;As you can see, the &#8220;I&#8217;ve had enough&#8221; message the process wrote to the file /var/ter-&#10;mination-reason is shown in the container&#8217;s Last State section. Note that this mecha-&#10;nism isn&#8217;t limited only to containers that crash. It can also be used in pods that run a&#10;completable task and terminate successfully (you&#8217;ll find an example in the file termi-&#10;nation-message-success.yaml). &#10; This mechanism is great for terminated containers, but you&#8217;ll probably agree that&#10;a similar mechanism would also be useful for showing app-specific status messages of&#10;running, not only terminated, containers. Kubernetes currently doesn&#8217;t provide any&#10;such functionality and I&#8217;m not aware of any plans to introduce it.&#10;NOTE&#10;If the container doesn&#8217;t write the message to any file, you can set the&#10;terminationMessagePolicy field to FallbackToLogsOnError. In that case,&#10;the last few lines of the container&#8217;s log are used as its termination message&#10;(but only when the container terminates unsuccessfully).&#10;17.4.6 Handling application logs&#10;While we&#8217;re on the subject of application logging, let&#8217;s reiterate that apps should write&#10;to the standard output instead of files. This makes it easy to view logs with the kubectl&#10;logs command. &#10;TIP&#10;If a container crashes and is replaced with a new one, you&#8217;ll see the new&#10;container&#8217;s log. To see the previous container&#8217;s logs, use the --previous&#10;option with kubectl logs.&#10;If the application logs to a file instead of the standard output, you can display the log&#10;file using an alternative approach: &#10;$ kubectl exec <pod> cat <logfile>&#10;This executes the cat command inside the container and streams the logs back to&#10;kubectl, which prints them out in your terminal. &#10;COPYING LOG AND OTHER FILES TO AND FROM A CONTAINER&#10;You can also copy the log file to your local machine using the kubectl cp command,&#10;which we haven&#8217;t looked at yet. It allows you to copy files from and into a container. For&#10;example, if a pod called foo-pod and its single container contains a file at /var/log/&#10;foo.log, you can transfer it to your local machine with the following command:&#10;$ kubectl cp foo-pod:/var/log/foo.log foo.log&#10;To copy a file from your local machine into the pod, specify the pod&#8217;s name in the sec-&#10;ond argument:&#10;$ kubectl cp localfile foo-pod:/etc/remotefile&#10; &#10;"
    color "green"
  ]
  node [
    id 890
    label "533"
    title "Page_533"
    color "blue"
  ]
  node [
    id 891
    label "text_444"
    title "501&#10;Making your apps easy to run and manage in Kubernetes&#10;This copies the file localfile to /etc/remotefile inside the pod&#8217;s container. If the pod has&#10;more than one container, you specify the container using the -c containerName option.&#10;USING CENTRALIZED LOGGING&#10;In a production system, you&#8217;ll want to use a centralized, cluster-wide logging solution,&#10;so all your logs are collected and (permanently) stored in a central location. This&#10;allows you to examine historical logs and analyze trends. Without such a system, a&#10;pod&#8217;s logs are only available while the pod exists. As soon as it&#8217;s deleted, its logs are&#10;deleted also. &#10; Kubernetes by itself doesn&#8217;t provide any kind of centralized logging. The compo-&#10;nents necessary for providing a centralized storage and analysis of all the container&#10;logs must be provided by additional components, which usually run as regular pods in&#10;the cluster. &#10; Deploying centralized logging solutions is easy. All you need to do is deploy a few&#10;YAML/JSON manifests and you&#8217;re good to go. On Google Kubernetes Engine, it&#8217;s&#10;even easier. Check the Enable Stackdriver Logging checkbox when setting up the clus-&#10;ter. Setting up centralized logging on an on-premises Kubernetes cluster is beyond the&#10;scope of this book, but I&#8217;ll give you a quick overview of how it&#8217;s usually done.&#10; You may have already heard of the ELK stack composed of ElasticSearch, Logstash,&#10;and Kibana. A slightly modified variation is the EFK stack, where Logstash is replaced&#10;with FluentD. &#10; When using the EFK stack for centralized logging, each Kubernetes cluster node&#10;runs a FluentD agent (usually as a pod deployed through a DaemonSet), which is&#10;responsible for gathering the logs from the containers, tagging them with pod-specific&#10;information, and delivering them to ElasticSearch, which stores them persistently.&#10;ElasticSearch is also deployed as a pod somewhere in the cluster. The logs can then be&#10;viewed and analyzed in a web browser through Kibana, which is a web tool for visualiz-&#10;ing ElasticSearch data. It also usually runs as a pod and is exposed through a Service.&#10;The three components of the EFK stack are shown in the following figure.&#10;NOTE&#10;In the next chapter, you&#8217;ll learn about Helm charts. You can use charts&#10;created by the Kubernetes community to deploy the EFK stack instead of cre-&#10;ating your own YAML manifests. &#10;Node 1&#10;Container logs&#10;Kibana&#10;Web&#10;browser&#10;FluentD&#10;Node 2&#10;Container logs&#10;FluentD&#10;Node 3&#10;Container logs&#10;FluentD&#10;ElasticSearch&#10;Figure 17.10&#10;Centralized logging with FluentD, ElasticSearch, and Kibana&#10; &#10;"
    color "green"
  ]
  node [
    id 892
    label "534"
    title "Page_534"
    color "blue"
  ]
  node [
    id 893
    label "text_445"
    title "502&#10;CHAPTER 17&#10;Best practices for developing apps&#10;HANDLING MULTI-LINE LOG STATEMENTS&#10;The FluentD agent stores each line of the log file as an entry in the ElasticSearch&#10;data store. There&#8217;s one problem with that. Log statements spanning multiple lines,&#10;such as exception stack traces in Java, appear as separate entries in the centralized&#10;logging system. &#10; To solve this problem, you can have the apps output JSON instead of plain text.&#10;This way, a multiline log statement can be stored and shown in Kibana as a single&#10;entry. But that makes viewing logs with kubectl logs much less human-friendly. &#10; The solution may be to keep outputting human-readable logs to standard output,&#10;while writing JSON logs to a file and having them processed by FluentD. This requires&#10;configuring the node-level FluentD agent appropriately or adding a logging sidecar&#10;container to every pod. &#10;17.5&#10;Best practices for development and testing&#10;We&#8217;ve talked about what to be mindful of when developing apps, but we haven&#8217;t&#10;talked about the development and testing workflows that will help you streamline&#10;those processes. I don&#8217;t want to go into too much detail here, because everyone needs&#10;to find what works best for them, but here are a few starting points.&#10;17.5.1 Running apps outside of Kubernetes during development&#10;When you&#8217;re developing an app that will run in a production Kubernetes cluster, does&#10;that mean you also need to run it in Kubernetes during development? Not really. Hav-&#10;ing to build the app after each minor change, then build the container image, push it&#10;to a registry, and then re-deploy the pods would make development slow and painful.&#10;Luckily, you don&#8217;t need to go through all that trouble.&#10; You can always develop and run apps on your local machine, the way you&#8217;re used&#10;to. After all, an app running in Kubernetes is a regular (although isolated) process&#10;running on one of the cluster nodes. If the app depends on certain features the&#10;Kubernetes environment provides, you can easily replicate that environment on your&#10;development machine.&#10; I&#8217;m not even talking about running the app in a container. Most of the time, you&#10;don&#8217;t need that&#8212;you can usually run the app directly from your IDE. &#10;CONNECTING TO BACKEND SERVICES&#10;In production, if the app connects to a backend Service and uses the BACKEND_SERVICE&#10;_HOST and BACKEND_SERVICE_PORT environment variables to find the Service&#8217;s coordi-&#10;nates, you can obviously set those environment variables on your local machine manu-&#10;ally and point them to the backend Service, regardless of if it&#8217;s running outside or&#10;inside a Kubernetes cluster. If it&#8217;s running inside Kubernetes, you can always (at least&#10;temporarily) make the Service accessible externally by changing it to a NodePort or a&#10;LoadBalancer-type Service. &#10; &#10;"
    color "green"
  ]
  node [
    id 894
    label "535"
    title "Page_535"
    color "blue"
  ]
  node [
    id 895
    label "text_446"
    title "503&#10;Best practices for development and testing&#10;CONNECTING TO THE API SERVER&#10;Similarly, if your app requires access to the Kubernetes API server when running&#10;inside a Kubernetes cluster, it can easily talk to the API server from outside the cluster&#10;during development. If it uses the ServiceAccount&#8217;s token to authenticate itself, you&#10;can always copy the ServiceAccount&#8217;s Secret&#8217;s files to your local machine with kubectl&#10;cp. The API server doesn&#8217;t care if the client accessing it is inside or outside the cluster. &#10; If the app uses an ambassador container like the one described in chapter 8, you&#10;don&#8217;t even need those Secret files. Run kubectl proxy on your local machine, run&#10;your app locally, and it should be ready to talk to your local kubectl proxy (as long as&#10;it and the ambassador container bind the proxy to the same port).&#10; In this case, you&#8217;ll need to make sure the user account your local kubectl is using&#10;has the same privileges as the ServiceAccount the app will run under.&#10;RUNNING INSIDE A CONTAINER EVEN DURING DEVELOPMENT&#10;When during development you absolutely have to run the app in a container for what-&#10;ever reason, there is a way of avoiding having to build the container image every time.&#10;Instead of baking the binaries into the image, you can always mount your local filesys-&#10;tem into the container through Docker volumes, for example. This way, after you&#10;build a new version of the app&#8217;s binaries, all you need to do is restart the container (or&#10;not even that, if hot-redeploy is supported). No need to rebuild the image.&#10;17.5.2 Using Minikube in development&#10;As you can see, nothing forces you to run your app inside Kubernetes during develop-&#10;ment. But you may do that anyway to see how the app behaves in a true Kubernetes&#10;environment.&#10; You may have used Minikube to run examples in this book. Although a Minikube&#10;cluster runs only a single worker node, it&#8217;s nevertheless a valuable method of trying&#10;out your app in Kubernetes (and, of course, developing all the resource manifests that&#10;make up your complete application). Minikube doesn&#8217;t offer everything that a proper&#10;multi-node Kubernetes cluster usually provides, but in most cases, that doesn&#8217;t matter.&#10;MOUNTING LOCAL FILES INTO THE MINIKUBE VM AND THEN INTO YOUR CONTAINERS&#10;When you&#8217;re developing with Minikube and you&#8217;d like to try out every change to your&#10;app in your Kubernetes cluster, you can mount your local filesystem into the Minikube&#10;VM using the minikube mount command and then mount it into your containers&#10;through a hostPath volume. You&#8217;ll find additional instructions on how to do that&#10;in the Minikube documentation at https:/&#10;/github.com/kubernetes/minikube/tree/&#10;master/docs.&#10;USING THE DOCKER DAEMON INSIDE THE MINIKUBE VM TO BUILD YOUR IMAGES&#10;If you&#8217;re developing your app with Minikube and planning to build the container&#10;image after every change, you can use the Docker daemon inside the Minikube VM to&#10;do the building, instead of having to build the image through your local Docker dae-&#10;mon, push it to a registry, and then have it pulled by the daemon in the VM. To use&#10; &#10;"
    color "green"
  ]
  node [
    id 896
    label "536"
    title "Page_536"
    color "blue"
  ]
  node [
    id 897
    label "text_447"
    title "504&#10;CHAPTER 17&#10;Best practices for developing apps&#10;Minikube&#8217;s Docker daemon, all you need to do is point your DOCKER_HOST environ-&#10;ment variable to it. Luckily, this is much easier than it sounds. All you need to do is&#10;run the following command on your local machine:&#10;$ eval $(minikube docker-env)&#10;This will set all the required environment variables for you. You then build your&#10;images the same way as if the Docker daemon was running on your local machine.&#10;After you build the image, you don&#8217;t need to push it anywhere, because it&#8217;s already&#10;stored locally on the Minikube VM, which means new pods can use the image immedi-&#10;ately. If your pods are already running, you either need to delete them or kill their&#10;containers so they&#8217;re restarted.&#10;BUILDING IMAGES LOCALLY AND COPYING THEM OVER TO THE MINIKUBE VM DIRECTLY&#10;If you can&#8217;t use the daemon inside the VM to build the images, you still have a way to&#10;avoid having to push the image to a registry and have the Kubelet running in the&#10;Minikube VM pull it. If you build the image on your local machine, you can copy it&#10;over to the Minikube VM with the following command:&#10;$ docker save <image> | (eval $(minikube docker-env) &#38;&#38; docker load)&#10;As before, the image is immediately ready to be used in a pod. But make sure the&#10;imagePullPolicy in your pod spec isn&#8217;t set to Always, because that would cause the&#10;image to be pulled from the external registry again and you&#8217;d lose the changes you&#8217;ve&#10;copied over.&#10;COMBINING MINIKUBE WITH A PROPER KUBERNETES CLUSTER&#10;You have virtually no limit when developing apps with Minikube. You can even com-&#10;bine a Minikube cluster with a proper Kubernetes cluster. I sometimes run my devel-&#10;opment workloads in my local Minikube cluster and have them talk to my other&#10;workloads that are deployed in a remote multi-node Kubernetes cluster thousands of&#10;miles away. &#10; Once I&#8217;m finished with development, I can move my local workloads to the remote&#10;cluster with no modifications and with absolutely no problems thanks to how Kuber-&#10;netes abstracts away the underlying infrastructure from the app.&#10;17.5.3 Versioning and auto-deploying resource manifests&#10;Because Kubernetes uses a declarative model, you never have to figure out the current&#10;state of your deployed resources and issue imperative commands to bring that state to&#10;what you desire. All you need to do is tell Kubernetes your desired state and it will take&#10;all the necessary actions to reconcile the cluster state with the desired state.&#10; You can store your collection of resource manifests in a Version Control System,&#10;enabling you to perform code reviews, keep an audit trail, and roll back changes&#10;whenever necessary. After each commit, you can run the kubectl apply command to&#10;have your changes reflected in your deployed resources. &#10; &#10;"
    color "green"
  ]
  node [
    id 898
    label "537"
    title "Page_537"
    color "blue"
  ]
  node [
    id 899
    label "text_448"
    title "505&#10;Best practices for development and testing&#10; If you run an agent that periodically (or when it detects a new commit) checks out&#10;your manifests from the Version Control System (VCS), and then runs the apply com-&#10;mand, you can manage your running apps simply by committing changes to the VCS&#10;without having to manually talk to the Kubernetes API server. Luckily, the people at&#10;Box (which coincidently was used to host this book&#8217;s manuscript and other materials)&#10;developed and released a tool called kube-applier, which does exactly what I described.&#10;You&#8217;ll find the tool&#8217;s source code at https:/&#10;/github.com/box/kube-applier.&#10; You can use multiple branches to deploy the manifests to a development, QA, stag-&#10;ing, and production cluster (or in different namespaces in the same cluster).&#10;17.5.4 Introducing Ksonnet as an alternative to writing YAML/JSON &#10;manifests&#10;We&#8217;ve seen a number of YAML manifests throughout the book. I don&#8217;t see writing&#10;YAML as too big of a problem, especially once you learn how to use kubectl explain&#10;to see the available options, but some people do. &#10; Just as I was finalizing the manuscript for this book, a new tool called Ksonnet was&#10;announced. It&#8217;s a library built on top of Jsonnet, which is a data templating language&#10;for building JSON data structures. Instead of writing the complete JSON by hand, it&#10;lets you define parameterized JSON fragments, give them a name, and then build a&#10;full JSON manifest by referencing those fragments by name, instead of repeating the&#10;same JSON code in multiple locations&#8212;much like you use functions or methods in a&#10;programming language. &#10; Ksonnet defines the fragments you&#8217;d find in Kubernetes resource manifests, allow-&#10;ing you to quickly build a complete Kubernetes resource JSON manifest with much&#10;less code. The following listing shows an example.&#10;local k = import &#34;../ksonnet-lib/ksonnet.beta.1/k.libsonnet&#34;;&#10;local container = k.core.v1.container;&#10;local deployment = k.apps.v1beta1.deployment;&#10;local kubiaContainer =                              &#10;  container.default(&#34;kubia&#34;, &#34;luksa/kubia:v1&#34;) +    &#10;  container.helpers.namedPort(&#34;http&#34;, 8080);        &#10;deployment.default(&#34;kubia&#34;, kubiaContainer) +    &#10;deployment.mixin.spec.replicas(3)                &#10;The kubia.ksonnet file shown in the listing is converted to a full JSON Deployment&#10;manifest when you run the following command:&#10;$ jsonnet kubia.ksonnet&#10;Listing 17.10&#10;The kubia Deployment written with Ksonnet: kubia.ksonnet&#10;This defines a container called kubia, &#10;which uses the luksa/kubia:v1 image &#10;and includes a port called http.&#10;This will be expanded into a full &#10;Deployment resource. The kubiaContainer &#10;defined here will be included in the &#10;Deployment&#8217;s pod template.&#10; &#10;"
    color "green"
  ]
  node [
    id 900
    label "538"
    title "Page_538"
    color "blue"
  ]
  node [
    id 901
    label "text_449"
    title "506&#10;CHAPTER 17&#10;Best practices for developing apps&#10;The power of Ksonnet and Jsonnet becomes apparent when you realize you can define&#10;your own higher-level fragments and make all your manifests consistent and duplica-&#10;tion-free. You&#8217;ll find more information on using and installing Ksonnet and Jsonnet at&#10;https:/&#10;/github.com/ksonnet/ksonnet-lib.&#10;17.5.5 Employing Continuous Integration and Continuous Delivery &#10;(CI/CD)&#10;We&#8217;ve touched on automating the deployment of Kubernetes resources two sections&#10;back, but you may want to set up a complete CI/CD pipeline for building your appli-&#10;cation binaries, container images, and resource manifests and then deploying them in&#10;one or more Kubernetes clusters.&#10; You&#8217;ll find many online resources talking about this subject. Here, I&#8217;d like to point&#10;you specifically to the Fabric8 project (http:/&#10;/fabric8.io), which is an integrated&#10;development platform for Kubernetes. It includes Jenkins, the well-known, open-&#10;source automation system, and various other tools to deliver a full CI/CD pipeline&#10;for DevOps-style development, deployment, and management of microservices on&#10;Kubernetes.&#10; If you&#8217;d like to build your own solution, I also suggest looking at one of the Google&#10;Cloud Platform&#8217;s online labs that talks about this subject. It&#8217;s available at https:/&#10;/&#10;github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.&#10;17.6&#10;Summary&#10;Hopefully, the information in this chapter has given you an even deeper insight into&#10;how Kubernetes works and will help you build apps that feel right at home when&#10;deployed to a Kubernetes cluster. The aim of this chapter was to&#10;&#61601;Show you how all the resources covered in this book come together to repre-&#10;sent a typical application running in Kubernetes.&#10;&#61601;Make you think about the difference between apps that are rarely moved&#10;between machines and apps running as pods, which are relocated much more&#10;frequently.&#10;&#61601;Help you understand that your multi-component apps (or microservices, if you&#10;will) shouldn&#8217;t rely on a specific start-up order.&#10;&#61601;Introduce init containers, which can be used to initialize a pod or delay the start&#10;of the pod&#8217;s main containers until a precondition is met.&#10;&#61601;Teach you about container lifecycle hooks and when to use them.&#10;&#61601;Gain a deeper insight into the consequences of the distributed nature of&#10;Kubernetes components and its eventual consistency model.&#10;&#61601;Learn how to make your apps shut down properly without breaking client&#10;connections.&#10; &#10;"
    color "green"
  ]
  node [
    id 902
    label "539"
    title "Page_539"
    color "blue"
  ]
  node [
    id 903
    label "text_450"
    title "507&#10;Summary&#10;&#61601;Give you a few small tips on how to make your apps easier to manage by keep-&#10;ing image sizes small, adding annotations and multi-dimensional labels to all&#10;your resources, and making it easier to see why an application terminated.&#10;&#61601;Teach you how to develop Kubernetes apps and run them locally or in Mini-&#10;kube before deploying them on a proper multi-node cluster.&#10;In the next and final chapter, we&#8217;ll learn how you can extend Kubernetes with your&#10;own custom API objects and controllers and how others have done it to create com-&#10;plete Platform-as-a-Service solutions on top of Kubernetes.&#10; &#10;"
    color "green"
  ]
  node [
    id 904
    label "540"
    title "Page_540"
    color "blue"
  ]
  node [
    id 905
    label "text_451"
    title "508&#10;Extending Kubernetes&#10;You&#8217;re almost done. To wrap up, we&#8217;ll look at how you can define your own API&#10;objects and create controllers for those objects. We&#8217;ll also look at how others have&#10;extended Kubernetes and built Platform-as-a-Service solutions on top of it.&#10;18.1&#10;Defining custom API objects&#10;Throughout the book, you&#8217;ve learned about the API objects that Kubernetes pro-&#10;vides and how they&#8217;re used to build application systems. Currently, Kubernetes&#10;users mostly use only these objects even though they represent relatively low-level,&#10;generic concepts. &#10;This chapter covers&#10;&#61601;Adding custom objects to Kubernetes&#10;&#61601;Creating a controller for the custom object&#10;&#61601;Adding custom API servers&#10;&#61601;Self-provisioning of services with the Kubernetes &#10;Service Catalog&#10;&#61601;Red Hat&#8217;s OpenShift Container Platform&#10;&#61601;Deis Workflow and Helm&#10; &#10;"
    color "green"
  ]
  node [
    id 906
    label "541"
    title "Page_541"
    color "blue"
  ]
  node [
    id 907
    label "text_452"
    title "509&#10;Defining custom API objects&#10; As the Kubernetes ecosystem evolves, you&#8217;ll see more and more high-level objects,&#10;which will be much more specialized than the resources Kubernetes supports today.&#10;Instead of dealing with Deployments, Services, ConfigMaps, and the like, you&#8217;ll create&#10;and manage objects that represent whole applications or software services. A custom&#10;controller will observe those high-level objects and create low-level objects based on&#10;them. For example, to run a messaging broker inside a Kubernetes cluster, all you&#8217;ll&#10;need to do is create an instance of a Queue resource and all the necessary Secrets,&#10;Deployments, and Services will be created by a custom Queue controller. Kubernetes&#10;already provides ways of adding custom resources like this. &#10;18.1.1 Introducing CustomResourceDefinitions&#10;To define a new resource type, all you need to do is post a CustomResourceDefinition&#10;object (CRD) to the Kubernetes API server. The CustomResourceDefinition object is&#10;the description of the custom resource type. Once the CRD is posted, users can then&#10;create instances of the custom resource by posting JSON or YAML manifests to the&#10;API server, the same as with any other Kubernetes resource.&#10;NOTE&#10;Prior to Kubernetes 1.7, custom resources were defined through Third-&#10;PartyResource objects, which were similar to CustomResourceDefinitions, but&#10;were removed in version 1.8.&#10;Creating a CRD so that users can create objects of the new type isn&#8217;t a useful feature if&#10;those objects don&#8217;t make something tangible happen in the cluster. Each CRD will&#10;usually also have an associated controller (an active component doing something&#10;based on the custom objects), the same way that all the core Kubernetes resources&#10;have an associated controller, as was explained in chapter 11. For this reason, to prop-&#10;erly show what CustomResourceDefinitions allow you to do other than adding&#10;instances of a custom object, a controller must be deployed as well. You&#8217;ll do that in&#10;the next example.&#10;INTRODUCING THE EXAMPLE CUSTOMRESOURCEDEFINITION&#10;Let&#8217;s imagine you want to allow users of your Kubernetes cluster to run static websites&#10;as easily as possible, without having to deal with Pods, Services, and other Kubernetes&#10;resources. What you want to achieve is for users to create objects of type Website that&#10;contain nothing more than the website&#8217;s name and the source from which the web-&#10;site&#8217;s files (HTML, CSS, PNG, and others) should be obtained. You&#8217;ll use a Git reposi-&#10;tory as the source of those files. When a user creates an instance of the Website&#10;resource, you want Kubernetes to spin up a new web server pod and expose it through&#10;a Service, as shown in figure 18.1.&#10; To create the Website resource, you want users to post manifests along the lines of&#10;the one shown in the following listing.&#10; &#10; &#10; &#10; &#10;"
    color "green"
  ]
  node [
    id 908
    label "542"
    title "Page_542"
    color "blue"
  ]
  node [
    id 909
    label "text_453"
    title "510&#10;CHAPTER 18&#10;Extending Kubernetes&#10;kind: Website        &#10;metadata:&#10;  name: kubia             &#10;spec:&#10;  gitRepo: https://github.com/luksa/kubia-website-example.git   &#10;Like all other resources, your resource contains a kind and a metadata.name field,&#10;and like most resources, it also contains a spec section. It contains a single field called&#10;gitRepo (you can choose the name)&#8212;it specifies the Git repository containing the&#10;website&#8217;s files. You&#8217;ll also need to include an apiVersion field, but you don&#8217;t know yet&#10;what its value must be for custom resources.&#10; If you try posting this resource to Kubernetes, you&#8217;ll receive an error because&#10;Kubernetes doesn&#8217;t know what a Website object is yet:&#10;$ kubectl create -f imaginary-kubia-website.yaml&#10;error: unable to recognize &#34;imaginary-kubia-website.yaml&#34;: no matches for &#10;&#10149; /, Kind=Website&#10;Before you can create instances of your custom object, you need to make Kubernetes&#10;recognize them.&#10;CREATING A CUSTOMRESOURCEDEFINITION OBJECT&#10;To make Kubernetes accept your custom Website resource instances, you need to post&#10;the CustomResourceDefinition shown in the following listing to the API server.&#10;apiVersion: apiextensions.k8s.io/v1beta1       &#10;kind: CustomResourceDefinition                 &#10;metadata:&#10;  name: websites.extensions.example.com      &#10;spec:&#10;  scope: Namespaced                          &#10;Listing 18.1&#10;An imaginary custom resource: imaginary-kubia-website.yaml&#10;Listing 18.2&#10;A CustomResourceDefinition manifest: website-crd.yaml&#10;Website&#10;kind: Website&#10;metadata:&#10;name: kubia&#10;spec:&#10;gitRepo:&#10;github.com/.../kubia.git&#10;Pod:&#10;kubia-website&#10;Service:&#10;kubia-website&#10;Figure 18.1&#10;Each Website object should result in the creation of a Service and an HTTP &#10;server Pod.&#10;A custom &#10;object kind&#10;The name of the website &#10;(used for naming the &#10;resulting Service and Pod)&#10;The Git &#10;repository &#10;holding the &#10;website&#8217;s files&#10;CustomResourceDefinitions belong &#10;to this API group and version.&#10;The full&#10;name of&#10;your&#10;custom&#10;object&#10;You want Website resources &#10;to be namespaced.&#10; &#10;"
    color "green"
  ]
  node [
    id 910
    label "543"
    title "Page_543"
    color "blue"
  ]
  node [
    id 911
    label "text_454"
    title "511&#10;Defining custom API objects&#10;  group: extensions.example.com                &#10;  version: v1                                  &#10;  names:                                    &#10;    kind: Website                           &#10;    singular: website                       &#10;    plural: websites                        &#10;After you post the descriptor to Kubernetes, it will allow you to create any number of&#10;instances of the custom Website resource. &#10; You can create the CRD from the website-crd.yaml file available in the code archive:&#10;$ kubectl create -f website-crd-definition.yaml&#10;customresourcedefinition &#34;websites.extensions.example.com&#34; created&#10;I&#8217;m sure you&#8217;re wondering about the long name of the CRD. Why not call it Website?&#10;The reason is to prevent name clashes. By adding a suffix to the name of the CRD&#10;(which will usually include the name of the organization that created the CRD), you&#10;keep CRD names unique. Luckily, the long name doesn&#8217;t mean you&#8217;ll need to create&#10;your Website resources with kind: websites.extensions.example.com, but as kind:&#10;Website, as specified in the names.kind property of the CRD. The extensions.exam-&#10;ple.com part is the API group of your resource. &#10; You&#8217;ve seen how creating Deployment objects requires you to set apiVersion to&#10;apps/v1beta1 instead of v1. The part before the slash is the API group (Deployments&#10;belong to the apps API group), and the part after it is the version name (v1beta1 in&#10;the case of Deployments). When creating instances of the custom Website resource,&#10;the apiVersion property will need to be set to extensions.example.com/v1.&#10;CREATING AN INSTANCE OF A CUSTOM RESOURCE&#10;Considering what you learned, you&#8217;ll now create a proper YAML for your Website&#10;resource instance. The YAML manifest is shown in the following listing.&#10;apiVersion: extensions.example.com/v1       &#10;kind: Website                               &#10;metadata:&#10;  name: kubia                                &#10;spec:&#10;  gitRepo: https://github.com/luksa/kubia-website-example.git&#10;The kind of your resource is Website, and the apiVersion is composed of the API&#10;group and the version number you defined in the CustomResourceDefinition.&#10; Create your Website object now:&#10;$ kubectl create -f kubia-website.yaml&#10;website &#34;kubia&#34; created&#10;Listing 18.3&#10;A custom Website resource: kubia-website.yaml&#10;Define an API group and version &#10;of the Website resource.&#10;You need to specify the various &#10;forms of the custom object&#8217;s name.&#10;Your custom API&#10;group and version&#10;This manifest &#10;describes a Website &#10;resource instance.&#10;The name of the &#10;Website instance&#10; &#10;"
    color "green"
  ]
  node [
    id 912
    label "544"
    title "Page_544"
    color "blue"
  ]
  node [
    id 913
    label "text_455"
    title "512&#10;CHAPTER 18&#10;Extending Kubernetes&#10;The response tells you that the API server has accepted and stored your custom&#10;Website object. Let&#8217;s see if you can now retrieve it. &#10;RETRIEVING INSTANCES OF A CUSTOM RESOURCE&#10;List all the websites in your cluster:&#10;$ kubectl get websites&#10;NAME      KIND&#10;kubia     Website.v1.extensions.example.com&#10;As with existing Kubernetes resources, you can create and then list instances of cus-&#10;tom resources. You can also use kubectl describe to see the details of your custom&#10;object, or retrieve the whole YAML with kubectl get, as in the following listing.&#10;$ kubectl get website kubia -o yaml&#10;apiVersion: extensions.example.com/v1&#10;kind: Website&#10;metadata:&#10;  creationTimestamp: 2017-02-26T15:53:21Z&#10;  name: kubia&#10;  namespace: default&#10;  resourceVersion: &#34;57047&#34;&#10;  selfLink: /apis/extensions.example.com/v1/.../default/websites/kubia&#10;  uid: b2eb6d99-fc3b-11e6-bd71-0800270a1c50&#10;spec:&#10;  gitRepo: https://github.com/luksa/kubia-website-example.git&#10;Note that the resource includes everything that was in the original YAML definition,&#10;and that Kubernetes has initialized additional metadata fields the way it does with all&#10;other resources. &#10;DELETING AN INSTANCE OF A CUSTOM OBJECT&#10;Obviously, in addition to creating and retrieving custom object instances, you can also&#10;delete them:&#10;$ kubectl delete website kubia&#10;website &#34;kubia&#34; deleted&#10;NOTE&#10;You&#8217;re deleting an instance of a Website, not the Website CRD&#10;resource. You could also delete the CRD object itself, but let&#8217;s hold off on that&#10;for a while, because you&#8217;ll be creating additional Website instances in the&#10;next section. &#10;Let&#8217;s go over everything you&#8217;ve done. By creating a CustomResourceDefinition object,&#10;you can now store, retrieve, and delete custom objects through the Kubernetes API&#10;server. These objects don&#8217;t do anything yet. You&#8217;ll need to create a controller to make&#10;them do something. &#10;Listing 18.4&#10;Full Website resource definition retrieved from the API server&#10; &#10;"
    color "green"
  ]
  node [
    id 914
    label "545"
    title "Page_545"
    color "blue"
  ]
  node [
    id 915
    label "text_456"
    title "513&#10;Defining custom API objects&#10; In general, the point of creating custom objects like this isn&#8217;t always to make some-&#10;thing happen when the object is created. Certain custom objects are used to store data&#10;instead of using a more generic mechanism such as a ConfigMap. Applications run-&#10;ning inside pods can query the API server for those objects and read whatever is&#10;stored in them. &#10; But in this case, we said you wanted the existence of a Website object to result in&#10;the spinning up of a web server serving the contents of the Git repository referenced&#10;in the object. We&#8217;ll look at how to do that next.&#10;18.1.2 Automating custom resources with custom controllers&#10;To make your Website objects run a web server pod exposed through a Service, you&#8217;ll&#10;need to build and deploy a Website controller, which will watch the API server for the&#10;creation of Website objects and then create the Service and the web server Pod for&#10;each of them. &#10; To make sure the Pod is managed and survives node failures, the controller will&#10;create a Deployment resource instead of an unmanaged Pod directly. The controller&#8217;s&#10;operation is summarized in figure 18.2.&#10;I&#8217;ve written a simple initial version of the controller, which works well enough to&#10;show CRDs and the controller in action, but it&#8217;s far from being production-ready,&#10;because it&#8217;s overly simplified. The container image is available at docker.io/luksa/&#10;website-controller:latest, and the source code is at https:/&#10;/github.com/luksa/k8s-&#10;website-controller. Instead of going through its source code, I&#8217;ll explain what the con-&#10;troller does.&#10;API server&#10;Websites&#10;Website:&#10;kubia&#10;Deployments&#10;Deployment:&#10;kubia-website&#10;Services&#10;Service:&#10;kubia-website&#10;Website&#10;controller&#10;Watches&#10;Creates&#10;Figure 18.2&#10;The Website controller &#10;watches for Website objects and &#10;creates a Deployment and a Service.&#10; &#10;"
    color "green"
  ]
  node [
    id 916
    label "546"
    title "Page_546"
    color "blue"
  ]
  node [
    id 917
    label "text_457"
    title "514&#10;CHAPTER 18&#10;Extending Kubernetes&#10;UNDERSTANDING WHAT THE WEBSITE CONTROLLER DOES&#10;Immediately upon startup, the controller starts to watch Website objects by requesting&#10;the following URL:&#10;http://localhost:8001/apis/extensions.example.com/v1/websites?watch=true&#10;You may recognize the hostname and port&#8212;the controller isn&#8217;t connecting to the&#10;API server directly, but is instead connecting to the kubectl proxy process, which&#10;runs in a sidecar container in the same pod and acts as the ambassador to the API&#10;server (we examined the ambassador pattern in chapter 8). The proxy forwards the&#10;request to the API server, taking care of both TLS encryption and authentication&#10;(see figure 18.3).&#10;Through the connection opened by this HTTP GET request, the API server will send&#10;watch events for every change to any Website object.&#10; The API server sends the ADDED watch event every time a new Website object is cre-&#10;ated. When the controller receives such an event, it extracts the Website&#8217;s name and&#10;the URL of the Git repository from the Website object it received in the watch event&#10;and creates a Deployment and a Service object by posting their JSON manifests to the&#10;API server. &#10; The Deployment resource contains a template for a pod with two containers&#10;(shown in figure 18.4): one running an nginx server and another one running a git-&#10;sync process, which keeps a local directory synced with the contents of a Git repo.&#10;The local directory is shared with the nginx container through an emptyDir volume&#10;(you did something similar to that in chapter 6, but instead of keeping the local&#10;directory synced with a Git repo, you used a gitRepo volume to download the Git&#10;repo&#8217;s contents at pod startup; the volume&#8217;s contents weren&#8217;t kept in sync with the&#10;Git repo afterward). The Service is a NodePort Service, which exposes your web&#10;server pod through a random port on each node (the same port is used on all&#10;nodes). When a pod is created by the Deployment object, clients can access the web-&#10;site through the node port.&#10;Pod: website-controller&#10;Container: main&#10;Website controller&#10;GET http://localhost:8001/apis/extensions.&#10;example.com/v1/websites?watch=true&#10;GET https://kubernetes:443/apis/extensions.&#10;example.com/v1/websites?watch=true&#10;Authorization: Bearer <token>&#10;Container: proxy&#10;kubectl proxy&#10;API server&#10;Figure 18.3&#10;The Website controller talks to the API server through a proxy (in the ambassador container).&#10; &#10;"
    color "green"
  ]
  node [
    id 918
    label "547"
    title "Page_547"
    color "blue"
  ]
  node [
    id 919
    label "text_458"
    title "515&#10;Defining custom API objects&#10;The API server also sends a DELETED watch event when a Website resource instance is&#10;deleted. Upon receiving the event, the controller deletes the Deployment and the Ser-&#10;vice resources it created earlier. As soon as a user deletes the Website instance, the&#10;controller will shut down and remove the web server serving that website.&#10;NOTE&#10;My oversimplified controller isn&#8217;t implemented properly. The way it&#10;watches the API objects doesn&#8217;t guarantee it won&#8217;t miss individual watch&#10;events. The proper way to watch objects through the API server is to not only&#10;watch them, but also periodically re-list all objects in case any watch events&#10;were missed. &#10;RUNNING THE CONTROLLER AS A POD&#10;During development, I ran the controller on my local development laptop and used a&#10;locally running kubectl proxy process (not running as a pod) as the ambassador to&#10;the Kubernetes API server. This allowed me to develop quickly, because I didn&#8217;t need&#10;to build a container image after every change to the source code and then run it&#10;inside Kubernetes. &#10; When I&#8217;m ready to deploy the controller into production, the best way is to run the&#10;controller inside Kubernetes itself, the way you do with all the other core controllers.&#10;To run the controller in Kubernetes, you can deploy it through a Deployment resource.&#10;The following listing shows an example of such a Deployment.&#10;apiVersion: apps/v1beta1&#10;kind: Deployment&#10;metadata:&#10;  name: website-controller&#10;spec:&#10;  replicas: 1                      &#10;  template:&#10;Listing 18.5&#10;A Website controller Deployment: website-controller.yaml&#10;Pod&#10;Webserver&#10;container&#10;Web client&#10;git-sync&#10;container&#10;Serves website to&#10;web client through&#10;a random port&#10;Clones Git repo&#10;into volume and&#10;keeps it synced&#10;emptyDir&#10;volume&#10;Figure 18.4&#10;The pod serving &#10;the website specified in the &#10;Website object&#10;You&#8217;ll run a single &#10;replica of the &#10;controller.&#10; &#10;"
    color "green"
  ]
  node [
    id 920
    label "548"
    title "Page_548"
    color "blue"
  ]
  node [
    id 921
    label "text_459"
    title "516&#10;CHAPTER 18&#10;Extending Kubernetes&#10;    metadata:&#10;      name: website-controller&#10;      labels:&#10;        app: website-controller&#10;    spec:&#10;      serviceAccountName: website-controller    &#10;      containers:                                    &#10;      - name: main                                   &#10;        image: luksa/website-controller              &#10;      - name: proxy                                  &#10;        image: luksa/kubectl-proxy:1.6.2             &#10;As you can see, the Deployment deploys a single replica of a two-container pod. One&#10;container runs your controller, whereas the other one is the ambassador container&#10;used for simpler communication with the API server. The pod runs under its own spe-&#10;cial ServiceAccount, so you&#8217;ll need to create it before you deploy the controller:&#10;$ kubectl create serviceaccount website-controller&#10;serviceaccount &#34;website-controller&#34; created&#10;If Role Based Access Control (RBAC) is enabled in your cluster, Kubernetes will not&#10;allow the controller to watch Website resources or create Deployments or Services. To&#10;allow it to do that, you&#8217;ll need to bind the website-controller ServiceAccount to the&#10;cluster-admin ClusterRole, by creating a ClusterRoleBinding like this:&#10;$ kubectl create clusterrolebinding website-controller &#10;&#10149; --clusterrole=cluster-admin &#10;&#10149; --serviceaccount=default:website-controller&#10;clusterrolebinding &#34;website-controller&#34; created&#10;Once you have the ServiceAccount and ClusterRoleBinding in place, you can deploy&#10;the controller&#8217;s Deployment. &#10;SEEING THE CONTROLLER IN ACTION&#10;With the controller now running, create the kubia Website resource again:&#10;$ kubectl create -f kubia-website.yaml&#10;website &#34;kubia&#34; created&#10;Now, let&#8217;s check the controller&#8217;s logs (shown in the following listing) to see if it has&#10;received the watch event.&#10;$ kubectl logs website-controller-2429717411-q43zs -c main&#10;2017/02/26 16:54:41 website-controller started.&#10;2017/02/26 16:54:47 Received watch event: ADDED: kubia: https://github.c...&#10;2017/02/26 16:54:47 Creating services with name kubia-website in namespa... &#10;2017/02/26 16:54:47 Response status: 201 Created&#10;2017/02/26 16:54:47 Creating deployments with name kubia-website in name... &#10;2017/02/26 16:54:47 Response status: 201 Created&#10;Listing 18.6&#10;Displaying logs of the Website controller&#10;It will run &#10;under a special &#10;ServiceAccount.&#10;Two containers: the &#10;main container and &#10;the proxy sidecar&#10; &#10;"
    color "green"
  ]
  node [
    id 922
    label "549"
    title "Page_549"
    color "blue"
  ]
  node [
    id 923
    label "text_460"
    title "517&#10;Defining custom API objects&#10;The logs show that the controller received the ADDED event and that it created a Service&#10;and a Deployment for the kubia-website Website. The API server responded with a&#10;201 Created response, which means the two resources should now exist. Let&#8217;s verify&#10;that the Deployment, Service and the resulting Pod were created. The following list-&#10;ing lists all Deployments, Services and Pods.&#10;$ kubectl get deploy,svc,po&#10;NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE  AGE&#10;deploy/kubia-website        1         1         1            1          4s&#10;deploy/website-controller   1         1         1            1          5m&#10;NAME                CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE&#10;svc/kubernetes      10.96.0.1      <none>        443/TCP        38d&#10;svc/kubia-website   10.101.48.23   <nodes>       80:32589/TCP   4s&#10;NAME                                     READY     STATUS    RESTARTS   AGE&#10;po/kubia-website-1029415133-rs715        2/2       Running   0          4s&#10;po/website-controller-1571685839-qzmg6   2/2       Running   1          5m&#10;There they are. The kubia-website Service, through which you can access your web-&#10;site, is available on port 32589 on all cluster nodes. You can access it with your browser.&#10;Awesome, right? &#10; Users of your Kubernetes cluster can now deploy static websites in seconds, with-&#10;out knowing anything about Pods, Services, or any other Kubernetes resources, except&#10;your custom Website resource. &#10; Obviously, you still have room for improvement. The controller could, for exam-&#10;ple, watch for Service objects and as soon as the node port is assigned, write the URL&#10;the website is accessible at into the status section of the Website resource instance&#10;itself. Or it could also create an Ingress object for each website. I&#8217;ll leave the imple-&#10;mentation of these additional features to you as an exercise.&#10;18.1.3 Validating custom objects&#10;You may have noticed that you didn&#8217;t specify any kind of validation schema in the Web-&#10;site CustomResourceDefinition. Users can include any field they want in the YAML of&#10;their Website object. The API server doesn&#8217;t validate the contents of the YAML (except&#10;the usual fields like apiVersion, kind, and metadata), so users can create invalid&#10;Website objects (without a gitRepo field, for example). &#10; Is it possible to add validation to the controller and prevent invalid objects from&#10;being accepted by the API server? It isn&#8217;t, because the API server first stores the object,&#10;then returns a success response to the client (kubectl), and only then notifies all the&#10;watchers (the controller is one of them). All the controller can really do is validate&#10;the object when it receives it in a watch event, and if the object is invalid, write the&#10;error message to the Website object (by updating the object through a new request to&#10;the API server). The user wouldn&#8217;t be notified of the error automatically. They&#8217;d have&#10;Listing 18.7&#10;The Deployment, Service, and Pod created for the kubia-website&#10; &#10;"
    color "green"
  ]
  node [
    id 924
    label "550"
    title "Page_550"
    color "blue"
  ]
  node [
    id 925
    label "text_461"
    title "518&#10;CHAPTER 18&#10;Extending Kubernetes&#10;to notice the error message by querying the API server for the Website object. Unless&#10;the user does this, they have no way of knowing whether the object is valid or not.&#10; This obviously isn&#8217;t ideal. You&#8217;d want the API server to validate the object and&#10;reject invalid objects immediately. Validation of custom objects was introduced in&#10;Kubernetes version 1.8 as an alpha feature. To have the API server validate your cus-&#10;tom objects, you need to enable the CustomResourceValidation feature gate in the&#10;API server and specify a JSON schema in the CRD.&#10;18.1.4 Providing a custom API server for your custom objects&#10;A better way of adding support for custom objects in Kubernetes is to implement your&#10;own API server and have the clients talk directly to it. &#10;INTRODUCING API SERVER AGGREGATION&#10;In Kubernetes version 1.7, you can integrate your custom API server with the main&#10;Kubernetes API server, through API server aggregation. Initially, the Kubernetes API&#10;server was a single monolithic component. From Kubernetes version 1.7, multiple&#10;aggregated API servers will be exposed at a single location. Clients can connect to the&#10;aggregated API and have their requests transparently forwarded to the appropriate&#10;API server. This way, the client wouldn&#8217;t even be aware that multiple API servers han-&#10;dle different objects behind the scenes. Even the core Kubernetes API server may&#10;eventually end up being split into multiple smaller API servers and exposed as a single&#10;server through the aggregator, as shown in figure 18.5.&#10;In your case, you could create an API server responsible for handling your Website&#10;objects. It could validate those objects the way the core Kubernetes API server validates&#10;them. You&#8217;d no longer need to create a CRD to represent those objects, because you&#8217;d&#10;implement the Website object type into the custom API server directly. &#10; Generally, each API server is responsible for storing their own resources. As shown&#10;in figure 18.5, it can either run its own instance of etcd (or a whole etcd cluster), or it&#10;Main&#10;API server&#10;Custom&#10;API server Y&#10;Custom&#10;API server X&#10;kubectl&#10;Uses its own etcd instance&#10;for storing its resources&#10;Uses CustomResourceDe&#64257;nitions&#10;in main API server as storage&#10;mechanism&#10;etcd&#10;etcd&#10;Figure 18.5&#10;API server aggregation&#10; &#10;"
    color "green"
  ]
  node [
    id 926
    label "551"
    title "Page_551"
    color "blue"
  ]
  node [
    id 927
    label "text_462"
    title "519&#10;Extending Kubernetes with the Kubernetes Service Catalog&#10;can store its resources in the core API server&#8217;s etcd store by creating CRD instances in&#10;the core API server. In that case, it needs to create a CRD object first, before creating&#10;instances of the CRD, the way you did in the example.&#10;REGISTERING A CUSTOM API SERVER&#10;To add a custom API server to your cluster, you&#8217;d deploy it as a pod and expose it&#10;through a Service. Then, to integrate it into the main API server, you&#8217;d deploy a YAML&#10;manifest describing an APIService resource like the one in the following listing.&#10;apiVersion: apiregistration.k8s.io/v1beta1   &#10;kind: APIService                             &#10;metadata:&#10;  name: v1alpha1.extensions.example.com&#10;spec:&#10;  group: extensions.example.com           &#10;  version: v1alpha1                      &#10;  priority: 150&#10;  service:                    &#10;    name: website-api         &#10;    namespace: default        &#10;After creating the APIService resource from the previous listing, client requests sent&#10;to the main API server that contain any resource from the extensions.example.com&#10;API group and version v1alpha1 would be forwarded to the custom API server pod(s)&#10;exposed through the website-api Service. &#10;CREATING CUSTOM CLIENTS&#10;While you can create custom resources from YAML files using the regular kubectl cli-&#10;ent, to make deployment of custom objects even easier, in addition to providing a cus-&#10;tom API server, you can also build a custom CLI tool. This will allow you to add&#10;dedicated commands for manipulating those objects, similar to how kubectl allows&#10;creating Secrets, Deployments, and other resources through resource-specific com-&#10;mands like kubectl create secret or kubectl create deployment.&#10; As I&#8217;ve already mentioned, custom API servers, API server aggregation, and other&#10;features related to extending Kubernetes are currently being worked on intensively, so&#10;they may change after the book is published. To get up-to-date information on the&#10;subject, refer to the Kubernetes GitHub repos at http:/&#10;/github.com/kubernetes.&#10;18.2&#10;Extending Kubernetes with the Kubernetes Service &#10;Catalog&#10;One of the first additional API servers that will be added to Kubernetes through API&#10;server aggregation is the Service Catalog API server. The Service Catalog is a hot topic&#10;in the Kubernetes community, so you may want to know about it. &#10; Currently, for a pod to consume a service (here I use the term generally, not in&#10;relation to Service resources; for example, a database service includes everything&#10;Listing 18.8&#10;An APIService YAML definition &#10;This is an APIService &#10;resource.&#10;The API group this API &#10;server is responsible for&#10;The supported API version&#10;The Service the custom API &#10;server is exposed through&#10; &#10;"
    color "green"
  ]
  node [
    id 928
    label "552"
    title "Page_552"
    color "blue"
  ]
  node [
    id 929
    label "text_463"
    title "520&#10;CHAPTER 18&#10;Extending Kubernetes&#10;required to allow users to use a database in their app), someone needs to deploy the&#10;pods providing the service, a Service resource, and possibly a Secret so the client pod&#10;can use it to authenticate with the service. That someone is usually the same user&#10;deploying the client pod or, if a team is dedicated to deploying these types of general&#10;services, the user needs to file a ticket and wait for the team to provision the service.&#10;This means the user needs to either create the manifests for all the components of the&#10;service, know where to find an existing set of manifests, know how to configure it&#10;properly, and deploy it manually, or wait for the other team to do it. &#10; But Kubernetes is supposed to be an easy-to-use, self-service system. Ideally, users&#10;whose apps require a certain service (for example, a web application requiring a back-&#10;end database), should be able to say to Kubernetes. &#8220;Hey, I need a PostgreSQL data-&#10;base. Please provision one and tell me where and how I can connect to it.&#8221; This will&#10;soon be possible through the Kubernetes Service Catalog. &#10;18.2.1 Introducing the Service Catalog&#10;As the name suggests, the Service Catalog is a catalog of services. Users can browse&#10;through the catalog and provision instances of the services listed in the catalog by&#10;themselves without having to deal with Pods, Services, ConfigMaps, and other resources&#10;required for the service to run. You&#8217;ll recognize that this is similar to what you did&#10;with the Website custom resource.&#10; Instead of adding custom resources to the API server for each type of service, the&#10;Service Catalog introduces the following four generic API resources:&#10;&#61601;A ClusterServiceBroker, which describes an (external) system that can provision&#10;services&#10;&#61601;A ClusterServiceClass, which describes a type of service that can be provisioned&#10;&#61601;A ServiceInstance, which is one instance of a service that has been provisioned&#10;&#61601;A ServiceBinding, which represents a binding between a set of clients (pods)&#10;and a ServiceInstance&#10;The relationships between those four resources are shown in the figure 18.6 and&#10;explained in the following paragraphs.&#10;In a nutshell, a cluster admin creates a ClusterServiceBroker resource for each service&#10;broker whose services they&#8217;d like to make available in the cluster. Kubernetes then asks&#10;the broker for a list of services that it can provide and creates a ClusterServiceClass&#10;resource for each of them. When a user requires a service to be provisioned, they create&#10;an ServiceInstance resource and then a ServiceBinding to bind that ServiceInstance to&#10;Client pods&#10;ServiceBinding&#10;ServiceInstance&#10;ClusterServiceClass(es)&#10;ClusterServiceBroker&#10;Figure 18.6&#10;The relationships between Service Catalog API resources. &#10; &#10;"
    color "green"
  ]
  node [
    id 930
    label "553"
    title "Page_553"
    color "blue"
  ]
  node [
    id 931
    label "text_464"
    title "521&#10;Extending Kubernetes with the Kubernetes Service Catalog&#10;their pods. Those pods are then injected with a Secret that holds all the necessary cre-&#10;dentials and other data required to connect to the provisioned ServiceInstance.&#10; The Service Catalog system architecture is shown in figure 18.7.&#10;The components shown in the figure are explained in the following sections.&#10;18.2.2 Introducing the Service Catalog API server and Controller &#10;Manager&#10;Similar to core Kubernetes, the Service Catalog is a distributed system composed of&#10;three components:&#10;&#61601;Service Catalog API Server&#10;&#61601;etcd as the storage&#10;&#61601;Controller Manager, where all the controllers run&#10;The four Service Catalog&#8211;related resources we introduced earlier are created by post-&#10;ing YAML/JSON manifests to the API server. It then stores them into its own etcd&#10;instance or uses CustomResourceDefinitions in the main API server as an alternative&#10;storage mechanism (in that case, no additional etcd instance is required). &#10; The controllers running in the Controller Manager are the ones doing some-&#10;thing with those resources. They obviously talk to the Service Catalog API server, the&#10;way other core Kubernetes controllers talk to the core API server. Those controllers&#10;don&#8217;t provision the requested services themselves. They leave that up to external&#10;service brokers, which are registered by creating ServiceBroker resources in the Ser-&#10;vice Catalog API.&#10;Kubernetes cluster&#10;External system(s)&#10;Kubernetes Service Catalog&#10;Client pods&#10;Provisioned&#10;services&#10;Broker A&#10;Broker B&#10;etcd&#10;Service&#10;Catalog&#10;API server&#10;Controller&#10;Manager&#10;kubectl&#10;Provisioned&#10;services&#10;Client pods use the&#10;provisioned services&#10;Figure 18.7&#10;The architecture of the Service Catalog&#10; &#10;"
    color "green"
  ]
  node [
    id 932
    label "554"
    title "Page_554"
    color "blue"
  ]
  node [
    id 933
    label "text_465"
    title "522&#10;CHAPTER 18&#10;Extending Kubernetes&#10;18.2.3 Introducing Service Brokers and the OpenServiceBroker API&#10;A cluster administrator can register one or more external ServiceBrokers in the Ser-&#10;vice Catalog. Every broker must implement the OpenServiceBroker API.&#10;INTRODUCING THE OPENSERVICEBROKER API&#10;The Service Catalog talks to the broker through that API. The API is relatively simple.&#10;It&#8217;s a REST API providing the following operations:&#10;&#61601;Retrieving the list of services with GET /v2/catalog&#10;&#61601;Provisioning a service instance (PUT /v2/service_instances/:id)&#10;&#61601;Updating a service instance (PATCH /v2/service_instances/:id)&#10;&#61601;Binding a service instance (PUT /v2/service_instances/:id/service_bind-&#10;ings/:binding_id)&#10;&#61601;Unbinding an instance (DELETE /v2/service_instances/:id/service_bind-&#10;ings/:binding_id)&#10;&#61601;Deprovisioning a service instance (DELETE /v2/service_instances/:id)&#10;You&#8217;ll find the OpenServiceBroker API spec at https:/&#10;/github.com/openservicebro-&#10;kerapi/servicebroker.&#10;REGISTERING BROKERS IN THE SERVICE CATALOG&#10;The cluster administrator registers a broker by posting a ServiceBroker resource man-&#10;ifest to the Service Catalog API, like the one shown in the following listing.&#10;apiVersion: servicecatalog.k8s.io/v1alpha1    &#10;kind: ClusterServiceBroker                                  &#10;metadata:&#10;  name: database-broker                          &#10;spec:&#10;  url: http://database-osbapi.myorganization.org  &#10;The listing describes an imaginary broker that can provision databases of different&#10;types. After the administrator creates the ClusterServiceBroker resource, a controller&#10;in the Service Catalog Controller Manager connects to the URL specified in the&#10;resource to retrieve the list of services this broker can provision.&#10; After the Service Catalog retrieves the list of services, it creates a ClusterService-&#10;Class resource for each of them. Each ClusterServiceClass resource describes a sin-&#10;gle type of service that can be provisioned (an example of a ClusterServiceClass is&#10;&#8220;PostgreSQL database&#8221;). Each ClusterServiceClass has one or more service plans asso-&#10;ciated with it. These allow the user to choose the level of service they need (for exam-&#10;ple, a database ClusterServiceClass could provide a &#8220;Free&#8221; plan, where the size of the&#10;Listing 18.9&#10;A ClusterServiceBroker manifest: database-broker.yaml&#10;The resource kind and &#10;the API group and version&#10;The name of this broker&#10;Where the Service Catalog&#10;can contact the broker&#10;(its OpenServiceBroker [OSB] API URL)&#10; &#10;"
    color "green"
  ]
  node [
    id 934
    label "555"
    title "Page_555"
    color "blue"
  ]
  node [
    id 935
    label "text_466"
    title "523&#10;Extending Kubernetes with the Kubernetes Service Catalog&#10;database is limited and the underlying storage is a spinning disk, and a &#8220;Premium&#8221;&#10;plan, with unlimited size and SSD storage). &#10;LISTING THE AVAILABLE SERVICES IN A CLUSTER&#10;Users of the Kubernetes cluster can retrieve a list of all services that can be provi-&#10;sioned in the cluster with kubectl get serviceclasses, as shown in the following&#10;listing.&#10;$ kubectl get clusterserviceclasses&#10;NAME                KIND&#10;postgres-database   ClusterServiceClass.v1alpha1.servicecatalog.k8s.io&#10;mysql-database      ServiceClass.v1alpha1.servicecatalog.k8s.io&#10;mongodb-database    ServiceClass.v1alpha1.servicecatalog.k8s.io&#10;The listing shows ClusterServiceClasses for services that your imaginary database bro-&#10;ker could provide. You can compare ClusterServiceClasses to StorageClasses, which we&#10;discussed in chapter 6. StorageClasses allow you to select the type of storage you&#8217;d like&#10;to use in your pods, while ClusterServiceClasses allow you to select the type of service.&#10; You can see details of one of the ClusterServiceClasses by retrieving its YAML. An&#10;example is shown in the following listing.&#10;$ kubectl get serviceclass postgres-database -o yaml&#10;apiVersion: servicecatalog.k8s.io/v1alpha1&#10;bindable: true&#10;brokerName: database-broker                     &#10;description: A PostgreSQL database&#10;kind: ClusterServiceClass&#10;metadata:&#10;  name: postgres-database&#10;  ...&#10;planUpdatable: false&#10;plans:&#10;- description: A free (but slow) PostgreSQL instance        &#10;  name: free                                                &#10;  osbFree: true                                             &#10;  ...&#10;- description: A paid (very fast) PostgreSQL instance      &#10;  name: premium                                            &#10;  osbFree: false                                           &#10;  ...&#10;The ClusterServiceClass in the listing contains two plans&#8212;a free plan, and a premium&#10;plan. You can see that this ClusterServiceClass is provided by the database-broker&#10;broker.&#10;Listing 18.10&#10;List of ClusterServiceClasses in a cluster&#10;Listing 18.11&#10;A ClusterServiceClass definition&#10;This ClusterServiceClass &#10;is provided by the &#10;database-broker.&#10;A free plan for &#10;this service&#10;A paid plan&#10; &#10;"
    color "green"
  ]
  node [
    id 936
    label "556"
    title "Page_556"
    color "blue"
  ]
  node [
    id 937
    label "text_467"
    title "524&#10;CHAPTER 18&#10;Extending Kubernetes&#10;18.2.4 Provisioning and using a service&#10;Let&#8217;s imagine the pods you&#8217;re deploying need to use a database. You&#8217;ve inspected the&#10;list of available ClusterServiceClasses and have chosen to use the free plan of the&#10;postgres-database ClusterServiceClass. &#10;PROVISIONING A SERVICEINSTANCE&#10;To have the database provisioned for you, all you need to do is create a Service-&#10;Instance resource, as shown in the following listing.&#10;apiVersion: servicecatalog.k8s.io/v1alpha1&#10;kind: ServiceInstance&#10;metadata:&#10;  name: my-postgres-db                     &#10;spec:&#10;  clusterServiceClassName: postgres-database        &#10;  clusterServicePlanName: free                             &#10;  parameters:&#10;    init-db-args: --data-checksums         &#10;You created a ServiceInstance called my-postgres-db (that will be the name of the&#10;resource you&#8217;re deploying) and specified the ClusterServiceClass and the chosen&#10;plan. You&#8217;re also specifying a parameter, which is specific for each broker and Cluster-&#10;ServiceClass. Let&#8217;s imagine you looked up the possible parameters in the broker&#8217;s doc-&#10;umentation.&#10; As soon as you create this resource, the Service Catalog will contact the broker the&#10;ClusterServiceClass belongs to and ask it to provision the service. It will pass on the&#10;chosen ClusterServiceClass and plan names, as well as all the parameters you specified.&#10; It&#8217;s then completely up to the broker to know what to do with this information. In&#10;your case, your database broker will probably spin up a new instance of a PostgreSQL&#10;database somewhere&#8212;not necessarily in the same Kubernetes cluster or even in&#10;Kubernetes at all. It could run a Virtual Machine and run the database in there. The&#10;Service Catalog doesn&#8217;t care, and neither does the user requesting the service. &#10; You can check if the service has been provisioned successfully by inspecting the&#10;status section of the my-postgres-db ServiceInstance you created, as shown in the&#10;following listing.&#10;$ kubectl get instance my-postgres-db -o yaml&#10;apiVersion: servicecatalog.k8s.io/v1alpha1&#10;kind: ServiceInstance&#10;...&#10;status:&#10;  asyncOpInProgress: false&#10;  conditions:&#10;Listing 18.12&#10;A ServiceInstance manifest: database-instance.yaml&#10;Listing 18.13&#10;Inspecting the status of a ServiceInstance&#10;You&#8217;re giving this &#10;Instance a name.&#10;The ServiceClass &#10;and Plan you want&#10;Additional parameters &#10;passed to the broker&#10; &#10;"
    color "green"
  ]
  node [
    id 938
    label "557"
    title "Page_557"
    color "blue"
  ]
  node [
    id 939
    label "text_468"
    title "525&#10;Extending Kubernetes with the Kubernetes Service Catalog&#10;  - lastTransitionTime: 2017-05-17T13:57:22Z&#10;    message: The instance was provisioned successfully    &#10;    reason: ProvisionedSuccessfully                       &#10;    status: &#34;True&#34;&#10;    type: Ready                   &#10;A database instance is now running somewhere, but how do you use it in your pods?&#10;To do that, you need to bind it.&#10;BINDING A SERVICEINSTANCE&#10;To use a provisioned ServiceInstance in your pods, you create a ServiceBinding&#10;resource, as shown in the following listing.&#10;apiVersion: servicecatalog.k8s.io/v1alpha1&#10;kind: ServiceBinding&#10;metadata:&#10;  name: my-postgres-db-binding&#10;spec:&#10;  instanceRef:                          &#10;    name: my-postgres-db                &#10;  secretName: postgres-secret           &#10;The listing shows that you&#8217;re defining a ServiceBinding resource called my-postgres-&#10;db-binding, in which you&#8217;re referencing the my-postgres-db service instance you&#10;created earlier. You&#8217;re also specifying a name of a Secret. You want the Service Catalog&#10;to put all the necessary credentials for accessing the service instance into a Secret&#10;called postgres-secret. But where are you binding the ServiceInstance to your pods?&#10;Nowhere, actually.&#10; Currently, the Service Catalog doesn&#8217;t yet make it possible to inject pods with the&#10;ServiceInstance&#8217;s credentials. This will be possible when a new Kubernetes feature&#10;called PodPresets is available. Until then, you can choose a name for the Secret&#10;where you want the credentials to be stored in and mount that Secret into your pods&#10;manually.&#10; When you submit the ServiceBinding resource from the previous listing to the Ser-&#10;vice Catalog API server, the controller will contact the Database broker once again&#10;and create a binding for the ServiceInstance you provisioned earlier. The broker&#10;responds with a list of credentials and other data necessary for connecting to the data-&#10;base. The Service Catalog creates a new Secret with the name you specified in the&#10;ServiceBinding resource and stores all that data in the Secret. &#10;USING THE NEWLY CREATED SECRET IN CLIENT PODS&#10;The Secret created by the Service Catalog system can be mounted into pods, so they&#10;can read its contents and use them to connect to the provisioned service instance (a&#10;PostgreSQL database in the example). The Secret could look like the one in the fol-&#10;lowing listing.&#10;Listing 18.14&#10;A ServiceBinding: my-postgres-db-binding.yaml&#10;The database was &#10;provisioned successfully.&#10;It&#8217;s ready to be used.&#10;You&#8217;re referencing the &#10;instance you created &#10;earlier.&#10;You&#8217;d like the credentials &#10;for accessing the service &#10;stored in this Secret.&#10; &#10;"
    color "green"
  ]
  node [
    id 940
    label "558"
    title "Page_558"
    color "blue"
  ]
  node [
    id 941
    label "text_469"
    title "526&#10;CHAPTER 18&#10;Extending Kubernetes&#10;$ kubectl get secret postgres-secret -o yaml&#10;apiVersion: v1&#10;data:&#10;  host: <base64-encoded hostname of the database>     &#10;  username: <base64-encoded username>                 &#10;  password: <base64-encoded password>                 &#10;kind: Secret&#10;metadata:&#10;  name: postgres-secret&#10;  namespace: default&#10;  ...&#10;type: Opaque&#10;Because you can choose the name of the Secret yourself, you can deploy pods before&#10;provisioning or binding the service. As you learned in chapter 7, the pods won&#8217;t be&#10;started until such a Secret exists. &#10; If necessary, multiple bindings can be created for different pods. The service bro-&#10;ker can choose to use the same set of credentials in every binding, but it&#8217;s better to&#10;create a new set of credentials for every binding instance. This way, pods can be pre-&#10;vented from using the service by deleting the ServiceBinding resource.&#10;18.2.5 Unbinding and deprovisioning&#10;Once you no longer need a ServiceBinding, you can delete it the way you delete other&#10;resources:&#10;$ kubectl delete servicebinding my-postgres-db-binding&#10;servicebinding &#34;my-postgres-db-binding&#34; deleted&#10;When you do this, the Service Catalog controller will delete the Secret and call the bro-&#10;ker to perform an unbinding operation. The service instance (in your case a PostgreSQL&#10;database) is still running. You can therefore create a new ServiceBinding if you want.&#10; But if you don&#8217;t need the database instance anymore, you should delete the Service-&#10;Instance resource also:&#10;$ kubectl delete serviceinstance my-postgres-db&#10;serviceinstance &#34;my-postgres-db &#34; deleted&#10;Deleting the ServiceInstance resource causes the Service Catalog to perform a depro-&#10;visioning operation on the service broker. Again, exactly what that means is up to the&#10;service broker, but in your case, the broker should shut down the PostgreSQL data-&#10;base instance that it created when we provisioned the service instance.&#10;18.2.6 Understanding what the Service Catalog brings&#10;As you&#8217;ve learned, the Service Catalog enables service providers make it possible to&#10;expose those services in any Kubernetes cluster by registering the broker in that cluster.&#10;Listing 18.15&#10;A Secret holding the credentials for connecting to the service instance&#10;This is what the pod &#10;should use to connect to &#10;the database service.&#10; &#10;"
    color "green"
  ]
  node [
    id 942
    label "559"
    title "Page_559"
    color "blue"
  ]
  node [
    id 943
    label "text_470"
    title "527&#10;Platforms built on top of Kubernetes&#10;For example, I&#8217;ve been involved with the Service Catalog since early on and have&#10;implemented a broker, which makes it trivial to provision messaging systems and&#10;expose them to pods in a Kubernetes cluster. Another team has implemented a broker&#10;that makes it easy to provision Amazon Web Services. &#10; In general, service brokers allow easy provisioning and exposing of services in&#10;Kubernetes and will make Kubernetes an even more awesome platform for deploying&#10;your applications. &#10;18.3&#10;Platforms built on top of Kubernetes&#10;I&#8217;m sure you&#8217;ll agree that Kubernetes is a great system by itself. Given that it&#8217;s easily&#10;extensible across all its components, it&#8217;s no wonder companies that had previously&#10;developed their own custom platforms are now re-implementing them on top of&#10;Kubernetes. Kubernetes is, in fact, becoming a widely accepted foundation for the&#10;new generation of Platform-as-a-Service offerings.&#10; Among the best-known PaaS systems built on Kubernetes are Deis Workflow and&#10;Red Hat&#8217;s OpenShift. We&#8217;ll do a quick overview of both systems to give you a sense of&#10;what they offer on top of all the awesome stuff Kubernetes already offers.&#10;18.3.1 Red Hat OpenShift Container Platform&#10;Red Hat OpenShift is a Platform-as-a-Service and as such, it has a strong focus on&#10;developer experience. Among its goals are enabling rapid development of applica-&#10;tions, as well as easy deployment, scaling, and long-term maintenance of those apps.&#10;OpenShift has been around much longer than Kubernetes. Versions 1 and 2 were&#10;built from the ground up and had nothing to do with Kubernetes, but when Kuberne-&#10;tes was announced, Red Hat decided to rebuild OpenShift version 3 from scratch&#8212;&#10;this time on top of Kubernetes. When a company such as Red Hat decides to throw&#10;away an old version of their software and build a new one on top of an existing tech-&#10;nology like Kubernetes, it should be clear to everyone how great Kubernetes is.&#10; Kubernetes automates rollouts and application scaling, whereas OpenShift also auto-&#10;mates the actual building of application images and their automatic deployment with-&#10;out requiring you to integrate a Continuous Integration solution into your cluster. &#10; OpenShift also provides user and group management, which allows you to run a&#10;properly secured multi-tenant Kubernetes cluster, where individual users are only&#10;allowed to access their own Kubernetes namespaces and the apps running in those&#10;namespaces are also fully network-isolated from each other by default. &#10;INTRODUCING ADDITIONAL RESOURCES AVAILABLE IN OPENSHIFT&#10;OpenShift provides some additional API objects in addition to all those available in&#10;Kubernetes. We&#8217;ll explain them in the next few paragraphs to give you a good over-&#10;view of what OpenShift does and what it provides.&#10; The additional resources include&#10;&#61601;Users &#38; Groups&#10;&#61601;Projects&#10; &#10;"
    color "green"
  ]
  node [
    id 944
    label "560"
    title "Page_560"
    color "blue"
  ]
  node [
    id 945
    label "text_471"
    title "528&#10;CHAPTER 18&#10;Extending Kubernetes&#10;&#61601;Templates&#10;&#61601;BuildConfigs&#10;&#61601;DeploymentConfigs&#10;&#61601;ImageStreams&#10;&#61601;Routes&#10;&#61601;And others&#10;UNDERSTANDING USERS, GROUPS, AND PROJECTS&#10;We&#8217;ve said that OpenShift provides a proper multi-tenant environment to its users.&#10;Unlike Kubernetes, which doesn&#8217;t have an API object for representing an individual&#10;user of the cluster (but does have ServiceAccounts that represent services running in&#10;it), OpenShift provides powerful user management features, which make it possible to&#10;specify what each user can do and what they cannot. These features pre-date the Role-&#10;Based Access Control, which is now the standard in vanilla Kubernetes.&#10; Each user has access to certain Projects, which are nothing more than Kubernetes&#10;Namespaces with additional annotations. Users can only act on resources that reside&#10;in the projects the user has access to. Access to the project is granted by a cluster&#10;administrator. &#10;INTRODUCING APPLICATION TEMPLATES&#10;Kubernetes makes it possible to deploy a set of resources through a single JSON or&#10;YAML manifest. OpenShift takes this a step further by allowing that manifest to be&#10;parameterizable. A parameterizable list in OpenShift is called a Template; it&#8217;s a list of&#10;objects whose definitions can include placeholders that get replaced with parameter&#10;values when you process and then instantiate a template (see figure 18.8).&#10;The template itself is a JSON or YAML file containing a list of parameters that are ref-&#10;erenced in resources defined in that same JSON/YAML. The template can be stored&#10;in the API server like any other object. Before a template can be instantiated, it needs&#10;Template&#10;Parameters&#10;APP_NAME=&#34;kubia&#34;&#10;VOL_CAPACITY=&#34;5 Gi&#34;&#10;...&#10;Pod&#10;name: $(APP_NAME)&#10;Service&#10;name: $(APP_NAME)&#10;Template&#10;Pod&#10;name: kubia&#10;Service&#10;name: kubia&#10;Pod&#10;name: kubia&#10;Service&#10;name: kubia&#10;Process&#10;Create&#10;Figure 18.8&#10;OpenShift templates&#10; &#10;"
    color "green"
  ]
  node [
    id 946
    label "561"
    title "Page_561"
    color "blue"
  ]
  node [
    id 947
    label "text_472"
    title "529&#10;Platforms built on top of Kubernetes&#10;to be processed. To process a template, you supply the values for the template&#8217;s&#10;parameters and then OpenShift replaces the references to the parameters with those&#10;values. The result is a processed template, which is exactly like a Kubernetes resource&#10;list that can then be created with a single POST request.&#10; OpenShift provides a long list of pre-fabricated templates that allow users to&#10;quickly run complex applications by specifying a few arguments (or none at all, if the&#10;template provides good defaults for those arguments). For example, a template can&#10;enable the creation of all the Kubernetes resources necessary to run a Java EE appli-&#10;cation inside an Application Server, which connects to a back-end database, also&#10;deployed as part of that same template. All those components can be deployed with a&#10;single command.&#10;BUILDING IMAGES FROM SOURCE USING BUILDCONFIGS&#10;One of the best features of OpenShift is the ability to have OpenShift build and imme-&#10;diately deploy an application in the OpenShift cluster by pointing it to a Git repository&#10;holding the application&#8217;s source code. You don&#8217;t need to build the container image at&#10;all&#8212;OpenShift does that for you. This is done by creating a resource called Build-&#10;Config, which can be configured to trigger builds of container images immediately&#10;after a change is committed to the source Git repository. &#10; Although OpenShift doesn&#8217;t monitor the Git repository itself, a hook in the repos-&#10;itory can notify OpenShift of the new commit. OpenShift will then pull the changes&#10;from the Git repository and start the build process. A build mechanism called Source&#10;To Image can detect what type of application is in the Git repository and run the&#10;proper build procedure for it. For example, if it detects a pom.xml file, which is used&#10;in Java Maven-formatted projects, it runs a Maven build. The resulting artifacts are&#10;packaged into an appropriate container image, and are then pushed to an internal&#10;container registry (provided by OpenShift). From there, they can be pulled and run&#10;in the cluster immediately. &#10; By creating a BuildConfig object, developers can thus point to a Git repo and not&#10;worry about building container images. Developers have almost no need to know&#10;anything about containers. Once the ops team deploys an OpenShift cluster and&#10;gives developers access to it, those developers can develop their code, commit, and&#10;push it to a Git repo, the same way they used to before we started packaging apps into&#10;containers. Then OpenShift takes care of building, deploying, and managing apps&#10;from that code.&#10;AUTOMATICALLY DEPLOYING NEWLY BUILT IMAGES WITH DEPLOYMENTCONFIGS&#10;Once a new container image is built, it can also automatically be deployed in the clus-&#10;ter. This is enabled by creating a DeploymentConfig object and pointing it to an&#10;ImageStream. As the name suggests, an ImageStream is a stream of images. When an&#10;image is built, it&#8217;s added to the ImageStream. This enables the DeploymentConfig to&#10;notice the newly built image and allows it to take action and initiate a rollout of the&#10;new image (see figure 18.9).&#10; &#10;"
    color "green"
  ]
  node [
    id 948
    label "562"
    title "Page_562"
    color "blue"
  ]
  node [
    id 949
    label "text_473"
    title "530&#10;CHAPTER 18&#10;Extending Kubernetes&#10;A DeploymentConfig is almost identical to the Deployment object in Kubernetes, but&#10;it pre-dates it. Like a Deployment object, it has a configurable strategy for transition-&#10;ing between Deployments. It contains a pod template used to create the actual pods,&#10;but it also allows you to configure pre- and post-deployment hooks. In contrast to a&#10;Kubernetes Deployment, it creates ReplicationControllers instead of ReplicaSets and&#10;provides a few additional features.&#10;EXPOSING SERVICES EXTERNALLY USING ROUTES&#10;Early on, Kubernetes didn&#8217;t provide Ingress objects. To expose Services to the outside&#10;world, you needed to use NodePort or LoadBalancer-type Services. But at that time,&#10;OpenShift already provided a better option through a Route resource. A Route is sim-&#10;ilar to an Ingress, but it provides additional configuration related to TLS termination&#10;and traffic splitting. &#10; Similar to an Ingress controller, a Route needs a Router, which is a controller that&#10;provides the load balancer or proxy. In contrast to Kubernetes, the Router is available&#10;out of the box in OpenShift. &#10;TRYING OUT OPENSHIFT&#10;If you&#8217;re interested in trying out OpenShift, you can start by using Minishift, which is&#10;the OpenShift equivalent of Minikube, or you can try OpenShift Online Starter at&#10;https:/&#10;/manage.openshift.com, which is a free multi-tenant, hosted solution provided&#10;to get you started with OpenShift. &#10;18.3.2 Deis Workflow and Helm&#10;A company called Deis, which has recently been acquired by Microsoft, also provides a&#10;PaaS called Workflow, which is also built on top of Kubernetes. Besides Workflow,&#10;Pods&#10;Builder pod&#10;Replication&#10;Controller&#10;BuildCon&#64257;g&#10;Git repo&#10;DeploymentCon&#64257;g&#10;ImageStream&#10;Build trigger&#10;Clones Git repo, builds new&#10;image from source, and adds&#10;it to the ImageStream&#10;Watches for new images in ImageStream&#10;and rolls out new version (similarly to a&#10;Deployment)&#10;Figure 18.9&#10;BuildConfigs and DeploymentConfigs in OpenShift&#10; &#10;"
    color "green"
  ]
  node [
    id 950
    label "563"
    title "Page_563"
    color "blue"
  ]
  node [
    id 951
    label "text_474"
    title "531&#10;Platforms built on top of Kubernetes&#10;they&#8217;ve also developed a tool called Helm, which is gaining traction in the Kubernetes&#10;community as a standard way of deploying existing apps in Kubernetes. We&#8217;ll take a&#10;brief look at both.&#10;INTRODUCING DEIS WORKFLOW&#10;You can deploy Deis Workflow to any existing Kubernetes cluster (unlike OpenShift,&#10;which is a complete cluster with a modified API server and other Kubernetes compo-&#10;nents). When you run Workflow, it creates a set of Services and ReplicationControllers,&#10;which then provide developers with a simple, developer-friendly environment. &#10; Deploying new versions of your app is triggered by pushing your changes with git&#10;push deis master and letting Workflow take care of the rest. Similar to OpenShift,&#10;Workflow also provides a source to image mechanism, application rollouts and roll-&#10;backs, edge routing, and also log aggregation, metrics, and alerting, which aren&#8217;t&#10;available in core Kubernetes. &#10; To run Workflow in your Kubernetes cluster, you first need to install the Deis Work-&#10;flow and Helm CLI tools and then install Workflow into your cluster. We won&#8217;t go into&#10;how to do that here, but if you&#8217;d like to learn more, visit the website at https:/&#10;/deis&#10;.com/workflow. What we&#8217;ll explore here is the Helm tool, which can be used without&#10;Workflow and has gained popularity in the community.&#10;DEPLOYING RESOURCES THROUGH HELM&#10;Helm is a package manager for Kubernetes (similar to OS package managers like yum&#10;or apt in Linux or homebrew in MacOS). &#10; Helm is comprised of two things:&#10;&#61601;A helm CLI tool (the client).&#10;&#61601;Tiller, a server component running as a Pod inside the Kubernetes cluster.&#10;Those two components are used to deploy and manage application packages in a&#10;Kubernetes cluster. Helm application packages are called Charts. They&#8217;re combined&#10;with a Config, which contains configuration information and is merged into a Chart&#10;to create a Release, which is a running instance of an application (a combined Chart&#10;and Config). You deploy and manage Releases using the helm CLI tool, which talks to&#10;the Tiller server, which is the component that creates all the necessary Kubernetes&#10;resources defined in the Chart, as shown in figure 18.10.&#10; You can create charts yourself and keep them on your local disk, or you can use&#10;any existing chart, which is available in the growing list of helm charts maintained by&#10;the community at https:/&#10;/github.com/kubernetes/charts. The list includes charts for&#10;applications such as PostgreSQL, MySQL, MariaDB, Magento, Memcached, MongoDB,&#10;OpenVPN, PHPBB, RabbitMQ, Redis, WordPress, and others.&#10; Similar to how you don&#8217;t build and install apps developed by other people to your&#10;Linux system manually, you probably don&#8217;t want to build and manage your own&#10;Kubernetes manifests for such applications, right? That&#8217;s why you&#8217;ll want to use Helm&#10;and the charts available in the GitHub repository I mentioned. &#10; &#10;"
    color "green"
  ]
  node [
    id 952
    label "564"
    title "Page_564"
    color "blue"
  ]
  node [
    id 953
    label "text_475"
    title "532&#10;CHAPTER 18&#10;Extending Kubernetes&#10;When you want to run a PostgreSQL or a MySQL database in your Kubernetes cluster,&#10;don&#8217;t start writing manifests for them. Instead, check if someone else has already gone&#10;through the trouble and prepared a Helm chart for it. &#10; Once someone prepares a Helm chart for a specific application and adds it to the&#10;Helm chart GitHub repo, installing the whole application takes a single one-line com-&#10;mand. For example, to run MySQL in your Kubernetes cluster, all you need to do is&#10;clone the charts Git repo to your local machine and run the following command (pro-&#10;vided you have Helm&#8217;s CLI tool and Tiller running in your cluster):&#10;$ helm install --name my-database stable/mysql&#10;This will create all the necessary Deployments, Services, Secrets, and PersistentVolu-&#10;meClaims needed to run MySQL in your cluster. You don&#8217;t need to concern yourself&#10;with what components you need and how to configure them to run MySQL properly.&#10;I&#8217;m sure you&#8217;ll agree this is awesome.&#10;TIP&#10;One of the most interesting charts available in the repo is an OpenVPN&#10;chart, which runs an OpenVPN server inside your Kubernetes cluster and&#10;allows you to enter the pod network through VPN and access Services as if&#10;your local machine was a pod in the cluster. This is useful when you&#8217;re devel-&#10;oping apps and running them locally.&#10;These were several examples of how Kubernetes can be extended and how companies&#10;like Red Hat and Deis (now Microsoft) have extended it. Now go and start riding the&#10;Kubernetes wave yourself!&#10;Kubernetes cluster&#10;Chart&#10;and&#10;Con&#64257;g&#10;Helm&#10;Charts&#10;(&#64257;les on&#10;local disk)&#10;Tiller&#10;(pod)&#10;Deployments,&#10;Services, and&#10;other objects&#10;helm&#10;CLI tool&#10;Manages&#10;charts&#10;Combines Chart and&#10;Con&#64257;g into a Release&#10;Creates Kubernetes objects&#10;de&#64257;ned in the Release&#10;Figure 18.10&#10;Overview of Helm&#10; &#10;"
    color "green"
  ]
  node [
    id 954
    label "565"
    title "Page_565"
    color "blue"
  ]
  node [
    id 955
    label "text_476"
    title "533&#10;Summary&#10;18.4&#10;Summary&#10;This final chapter has shown you how you can go beyond the existing functionalities&#10;Kubernetes provides and how companies like Dies and Red Hat have done it. You&#8217;ve&#10;learned how&#10;&#61601;Custom resources can be registered in the API server by creating a Custom-&#10;ResourceDefinition object.&#10;&#61601;Instances of custom objects can be stored, retrieved, updated, and deleted with-&#10;out having to change the API server code.&#10;&#61601;A custom controller can be implemented to bring those objects to life.&#10;&#61601;Kubernetes can be extended with custom API servers through API aggregation.&#10;&#61601;Kubernetes Service Catalog makes it possible to self-provision external services&#10;and expose them to pods running in the Kubernetes cluster.&#10;&#61601;Platforms-as-a-Service built on top of Kubernetes make it easy to build contain-&#10;erized applications inside the same Kubernetes cluster that then runs them. &#10;&#61601;A package manager called Helm makes deploying existing apps without requir-&#10;ing you to build resource manifests for them.&#10;Thank you for taking the time to read through this long book. I hope you&#8217;ve learned&#10;as much from reading it as I have from writing it.&#10; &#10;"
    color "green"
  ]
  node [
    id 956
    label "566"
    title "Page_566"
    color "blue"
  ]
  node [
    id 957
    label "text_477"
    title "534&#10;appendix A&#10;Using kubectl&#10;with multiple clusters&#10;A.1&#10;Switching between Minikube and Google Kubernetes &#10;Engine&#10;The examples in this book can either be run in a cluster created with Minikube, or&#10;one created with Google Kubernetes Engine (GKE). If you plan on using both, you&#10;need to know how to switch between them. A detailed explanation of how to use&#10;kubectl with multiple clusters is described in the next section. Here we look at how&#10;to switch between Minikube and GKE.&#10;SWITCHING TO MINIKUBE&#10;Luckily, every time you start up a Minikube cluster with minikube start, it also&#10;reconfigures kubectl to use it:&#10;$ minikube start&#10;Starting local Kubernetes cluster...&#10;...&#10;Setting up kubeconfig...                            &#10;Kubectl is now configured to use the cluster.       &#10;After switching from Minikube to GKE, you can switch back by stopping Minikube&#10;and starting it up again. kubectl will then be re-configured to use the Minikube clus-&#10;ter again.&#10;SWITCHING TO GKE&#10;To switch to using the GKE cluster, you can use the following command:&#10;$ gcloud container clusters get-credentials my-gke-cluster&#10;This will configure kubectl to use the GKE cluster called my-gke-cluster.&#10;Minikube sets up kubectl every &#10;time you start the cluster.&#10; &#10;"
    color "green"
  ]
  node [
    id 958
    label "567"
    title "Page_567"
    color "blue"
  ]
  node [
    id 959
    label "text_478"
    title "535&#10;Using kubectl with multiple clusters or namespaces&#10;GOING FURTHER&#10;These two methods should be enough to get you started quickly, but to understand&#10;the complete picture of using kubectl with multiple clusters, study the next section. &#10;A.2&#10;Using kubectl with multiple clusters or namespaces&#10;If you need to switch between different Kubernetes clusters, or if you want to work in a&#10;different namespace than the default and don&#8217;t want to specify the --namespace&#10;option every time you run kubectl, here&#8217;s how to do it.&#10;A.2.1&#10;Configuring the location of the kubeconfig file&#10;The config used by kubectl is usually stored in the ~/.kube/config file. If it&#8217;s stored&#10;somewhere else, the KUBECONFIG environment variable needs to point to its location. &#10;NOTE&#10;You can use multiple config files and have kubectl use them all at&#10;once by specifying all of them in the KUBECONFIG environment variable (sepa-&#10;rate them with a colon).&#10;A.2.2&#10;Understanding the contents of the kubeconfig file&#10;An example config file is shown in the following listing.&#10;apiVersion: v1&#10;clusters:&#10;- cluster:                                                 &#10;    certificate-authority: /home/luksa/.minikube/ca.crt    &#10;    server: https://192.168.99.100:8443                    &#10;  name: minikube                                           &#10;contexts:&#10;- context:                          &#10;    cluster: minikube               &#10;    user: minikube                  &#10;    namespace: default              &#10;  name: minikube                    &#10;current-context: minikube             &#10;kind: Config&#10;preferences: {}&#10;users:&#10;- name: minikube                                             &#10;  user:                                                      &#10;    client-certificate: /home/luksa/.minikube/apiserver.crt  &#10;    client-key: /home/luksa/.minikube/apiserver.key          &#10;The kubeconfig file consists of four sections:&#10;&#9632;&#10;A list of clusters&#10;&#9632;&#10;A list of users&#10;&#9632;&#10;A list of contexts&#10;&#9632;&#10;The name of the current context&#10;Listing A.1&#10;Example kubeconfig file&#10;Contains &#10;information about a &#10;Kubernetes cluster&#10;Defines a &#10;kubectl &#10;context&#10;The current context &#10;kubectl uses&#10;Contains &#10;a user&#8217;s &#10;credentials&#10; &#10;"
    color "green"
  ]
  edge [
    source 1
    target 3
  ]
  edge [
    source 2
    target 1
  ]
  edge [
    source 2
    target 4
  ]
  edge [
    source 2
    target 6
  ]
  edge [
    source 2
    target 8
  ]
  edge [
    source 2
    target 10
  ]
  edge [
    source 2
    target 12
  ]
  edge [
    source 2
    target 14
  ]
  edge [
    source 2
    target 16
  ]
  edge [
    source 2
    target 18
  ]
  edge [
    source 2
    target 20
  ]
  edge [
    source 2
    target 22
  ]
  edge [
    source 2
    target 24
  ]
  edge [
    source 2
    target 26
  ]
  edge [
    source 2
    target 28
  ]
  edge [
    source 2
    target 30
  ]
  edge [
    source 2
    target 32
  ]
  edge [
    source 2
    target 34
  ]
  edge [
    source 2
    target 36
  ]
  edge [
    source 2
    target 38
  ]
  edge [
    source 2
    target 40
  ]
  edge [
    source 2
    target 42
  ]
  edge [
    source 2
    target 44
  ]
  edge [
    source 2
    target 46
  ]
  edge [
    source 2
    target 48
  ]
  edge [
    source 2
    target 50
  ]
  edge [
    source 2
    target 52
  ]
  edge [
    source 2
    target 54
  ]
  edge [
    source 2
    target 56
  ]
  edge [
    source 2
    target 58
  ]
  edge [
    source 2
    target 60
  ]
  edge [
    source 2
    target 62
  ]
  edge [
    source 2
    target 64
  ]
  edge [
    source 2
    target 66
  ]
  edge [
    source 2
    target 68
  ]
  edge [
    source 2
    target 70
  ]
  edge [
    source 2
    target 72
  ]
  edge [
    source 2
    target 74
  ]
  edge [
    source 2
    target 76
  ]
  edge [
    source 2
    target 78
  ]
  edge [
    source 2
    target 80
  ]
  edge [
    source 2
    target 82
  ]
  edge [
    source 2
    target 84
  ]
  edge [
    source 2
    target 86
  ]
  edge [
    source 2
    target 88
  ]
  edge [
    source 2
    target 90
  ]
  edge [
    source 2
    target 92
  ]
  edge [
    source 2
    target 94
  ]
  edge [
    source 2
    target 96
  ]
  edge [
    source 2
    target 98
  ]
  edge [
    source 2
    target 100
  ]
  edge [
    source 2
    target 102
  ]
  edge [
    source 2
    target 104
  ]
  edge [
    source 2
    target 106
  ]
  edge [
    source 2
    target 108
  ]
  edge [
    source 2
    target 110
  ]
  edge [
    source 2
    target 112
  ]
  edge [
    source 2
    target 114
  ]
  edge [
    source 2
    target 116
  ]
  edge [
    source 2
    target 118
  ]
  edge [
    source 2
    target 120
  ]
  edge [
    source 2
    target 122
  ]
  edge [
    source 2
    target 124
  ]
  edge [
    source 2
    target 126
  ]
  edge [
    source 2
    target 128
  ]
  edge [
    source 2
    target 130
  ]
  edge [
    source 2
    target 132
  ]
  edge [
    source 2
    target 134
  ]
  edge [
    source 2
    target 136
  ]
  edge [
    source 2
    target 138
  ]
  edge [
    source 2
    target 140
  ]
  edge [
    source 2
    target 142
  ]
  edge [
    source 2
    target 144
  ]
  edge [
    source 2
    target 146
  ]
  edge [
    source 2
    target 148
  ]
  edge [
    source 2
    target 150
  ]
  edge [
    source 2
    target 152
  ]
  edge [
    source 2
    target 154
  ]
  edge [
    source 2
    target 156
  ]
  edge [
    source 2
    target 158
  ]
  edge [
    source 2
    target 160
  ]
  edge [
    source 2
    target 162
  ]
  edge [
    source 2
    target 164
  ]
  edge [
    source 2
    target 166
  ]
  edge [
    source 2
    target 168
  ]
  edge [
    source 2
    target 170
  ]
  edge [
    source 2
    target 172
  ]
  edge [
    source 2
    target 174
  ]
  edge [
    source 2
    target 176
  ]
  edge [
    source 2
    target 178
  ]
  edge [
    source 2
    target 180
  ]
  edge [
    source 2
    target 182
  ]
  edge [
    source 2
    target 184
  ]
  edge [
    source 2
    target 186
  ]
  edge [
    source 2
    target 188
  ]
  edge [
    source 2
    target 190
  ]
  edge [
    source 2
    target 192
  ]
  edge [
    source 2
    target 194
  ]
  edge [
    source 2
    target 196
  ]
  edge [
    source 2
    target 198
  ]
  edge [
    source 2
    target 200
  ]
  edge [
    source 2
    target 202
  ]
  edge [
    source 2
    target 204
  ]
  edge [
    source 2
    target 206
  ]
  edge [
    source 2
    target 208
  ]
  edge [
    source 2
    target 210
  ]
  edge [
    source 2
    target 212
  ]
  edge [
    source 2
    target 214
  ]
  edge [
    source 2
    target 216
  ]
  edge [
    source 2
    target 218
  ]
  edge [
    source 2
    target 220
  ]
  edge [
    source 2
    target 222
  ]
  edge [
    source 2
    target 224
  ]
  edge [
    source 2
    target 226
  ]
  edge [
    source 2
    target 228
  ]
  edge [
    source 2
    target 230
  ]
  edge [
    source 2
    target 232
  ]
  edge [
    source 2
    target 234
  ]
  edge [
    source 2
    target 236
  ]
  edge [
    source 2
    target 238
  ]
  edge [
    source 2
    target 240
  ]
  edge [
    source 2
    target 242
  ]
  edge [
    source 2
    target 244
  ]
  edge [
    source 2
    target 246
  ]
  edge [
    source 2
    target 248
  ]
  edge [
    source 2
    target 250
  ]
  edge [
    source 2
    target 252
  ]
  edge [
    source 2
    target 254
  ]
  edge [
    source 2
    target 256
  ]
  edge [
    source 2
    target 258
  ]
  edge [
    source 2
    target 260
  ]
  edge [
    source 2
    target 262
  ]
  edge [
    source 2
    target 264
  ]
  edge [
    source 2
    target 266
  ]
  edge [
    source 2
    target 268
  ]
  edge [
    source 2
    target 270
  ]
  edge [
    source 2
    target 272
  ]
  edge [
    source 2
    target 274
  ]
  edge [
    source 2
    target 276
  ]
  edge [
    source 2
    target 278
  ]
  edge [
    source 2
    target 280
  ]
  edge [
    source 2
    target 282
  ]
  edge [
    source 2
    target 284
  ]
  edge [
    source 2
    target 286
  ]
  edge [
    source 2
    target 288
  ]
  edge [
    source 2
    target 290
  ]
  edge [
    source 2
    target 292
  ]
  edge [
    source 2
    target 294
  ]
  edge [
    source 2
    target 296
  ]
  edge [
    source 2
    target 298
  ]
  edge [
    source 2
    target 300
  ]
  edge [
    source 2
    target 302
  ]
  edge [
    source 2
    target 304
  ]
  edge [
    source 2
    target 306
  ]
  edge [
    source 2
    target 308
  ]
  edge [
    source 2
    target 310
  ]
  edge [
    source 2
    target 312
  ]
  edge [
    source 2
    target 314
  ]
  edge [
    source 2
    target 316
  ]
  edge [
    source 2
    target 318
  ]
  edge [
    source 2
    target 320
  ]
  edge [
    source 2
    target 322
  ]
  edge [
    source 2
    target 324
  ]
  edge [
    source 2
    target 326
  ]
  edge [
    source 2
    target 328
  ]
  edge [
    source 2
    target 330
  ]
  edge [
    source 2
    target 332
  ]
  edge [
    source 2
    target 334
  ]
  edge [
    source 2
    target 336
  ]
  edge [
    source 2
    target 338
  ]
  edge [
    source 2
    target 340
  ]
  edge [
    source 2
    target 342
  ]
  edge [
    source 2
    target 344
  ]
  edge [
    source 2
    target 346
  ]
  edge [
    source 2
    target 348
  ]
  edge [
    source 2
    target 350
  ]
  edge [
    source 2
    target 352
  ]
  edge [
    source 2
    target 354
  ]
  edge [
    source 2
    target 356
  ]
  edge [
    source 2
    target 358
  ]
  edge [
    source 2
    target 360
  ]
  edge [
    source 2
    target 362
  ]
  edge [
    source 2
    target 364
  ]
  edge [
    source 2
    target 366
  ]
  edge [
    source 2
    target 368
  ]
  edge [
    source 2
    target 370
  ]
  edge [
    source 2
    target 372
  ]
  edge [
    source 2
    target 374
  ]
  edge [
    source 2
    target 376
  ]
  edge [
    source 2
    target 378
  ]
  edge [
    source 2
    target 380
  ]
  edge [
    source 2
    target 382
  ]
  edge [
    source 2
    target 384
  ]
  edge [
    source 2
    target 386
  ]
  edge [
    source 2
    target 388
  ]
  edge [
    source 2
    target 390
  ]
  edge [
    source 2
    target 392
  ]
  edge [
    source 2
    target 394
  ]
  edge [
    source 2
    target 396
  ]
  edge [
    source 2
    target 398
  ]
  edge [
    source 2
    target 400
  ]
  edge [
    source 2
    target 402
  ]
  edge [
    source 2
    target 404
  ]
  edge [
    source 2
    target 406
  ]
  edge [
    source 2
    target 408
  ]
  edge [
    source 2
    target 410
  ]
  edge [
    source 2
    target 412
  ]
  edge [
    source 2
    target 414
  ]
  edge [
    source 2
    target 416
  ]
  edge [
    source 2
    target 418
  ]
  edge [
    source 2
    target 420
  ]
  edge [
    source 2
    target 422
  ]
  edge [
    source 2
    target 424
  ]
  edge [
    source 2
    target 426
  ]
  edge [
    source 2
    target 428
  ]
  edge [
    source 2
    target 430
  ]
  edge [
    source 2
    target 432
  ]
  edge [
    source 2
    target 434
  ]
  edge [
    source 2
    target 436
  ]
  edge [
    source 2
    target 438
  ]
  edge [
    source 2
    target 440
  ]
  edge [
    source 2
    target 442
  ]
  edge [
    source 2
    target 444
  ]
  edge [
    source 2
    target 446
  ]
  edge [
    source 2
    target 448
  ]
  edge [
    source 2
    target 450
  ]
  edge [
    source 2
    target 452
  ]
  edge [
    source 2
    target 454
  ]
  edge [
    source 2
    target 456
  ]
  edge [
    source 2
    target 458
  ]
  edge [
    source 2
    target 460
  ]
  edge [
    source 2
    target 462
  ]
  edge [
    source 2
    target 464
  ]
  edge [
    source 2
    target 466
  ]
  edge [
    source 2
    target 468
  ]
  edge [
    source 2
    target 470
  ]
  edge [
    source 2
    target 472
  ]
  edge [
    source 2
    target 474
  ]
  edge [
    source 2
    target 476
  ]
  edge [
    source 2
    target 478
  ]
  edge [
    source 2
    target 480
  ]
  edge [
    source 2
    target 482
  ]
  edge [
    source 2
    target 484
  ]
  edge [
    source 2
    target 486
  ]
  edge [
    source 2
    target 488
  ]
  edge [
    source 2
    target 490
  ]
  edge [
    source 2
    target 492
  ]
  edge [
    source 2
    target 494
  ]
  edge [
    source 2
    target 496
  ]
  edge [
    source 2
    target 498
  ]
  edge [
    source 2
    target 500
  ]
  edge [
    source 2
    target 502
  ]
  edge [
    source 2
    target 504
  ]
  edge [
    source 2
    target 506
  ]
  edge [
    source 2
    target 508
  ]
  edge [
    source 2
    target 510
  ]
  edge [
    source 2
    target 512
  ]
  edge [
    source 2
    target 514
  ]
  edge [
    source 2
    target 516
  ]
  edge [
    source 2
    target 518
  ]
  edge [
    source 2
    target 520
  ]
  edge [
    source 2
    target 522
  ]
  edge [
    source 2
    target 524
  ]
  edge [
    source 2
    target 526
  ]
  edge [
    source 2
    target 528
  ]
  edge [
    source 2
    target 530
  ]
  edge [
    source 2
    target 532
  ]
  edge [
    source 2
    target 534
  ]
  edge [
    source 2
    target 536
  ]
  edge [
    source 2
    target 538
  ]
  edge [
    source 2
    target 540
  ]
  edge [
    source 2
    target 542
  ]
  edge [
    source 2
    target 544
  ]
  edge [
    source 2
    target 546
  ]
  edge [
    source 2
    target 548
  ]
  edge [
    source 2
    target 550
  ]
  edge [
    source 2
    target 552
  ]
  edge [
    source 2
    target 554
  ]
  edge [
    source 2
    target 556
  ]
  edge [
    source 2
    target 558
  ]
  edge [
    source 2
    target 560
  ]
  edge [
    source 2
    target 562
  ]
  edge [
    source 2
    target 564
  ]
  edge [
    source 2
    target 566
  ]
  edge [
    source 2
    target 568
  ]
  edge [
    source 2
    target 570
  ]
  edge [
    source 2
    target 572
  ]
  edge [
    source 2
    target 574
  ]
  edge [
    source 2
    target 576
  ]
  edge [
    source 2
    target 578
  ]
  edge [
    source 2
    target 580
  ]
  edge [
    source 2
    target 582
  ]
  edge [
    source 2
    target 584
  ]
  edge [
    source 2
    target 586
  ]
  edge [
    source 2
    target 588
  ]
  edge [
    source 2
    target 590
  ]
  edge [
    source 2
    target 592
  ]
  edge [
    source 2
    target 594
  ]
  edge [
    source 2
    target 596
  ]
  edge [
    source 2
    target 598
  ]
  edge [
    source 2
    target 600
  ]
  edge [
    source 2
    target 602
  ]
  edge [
    source 2
    target 604
  ]
  edge [
    source 2
    target 606
  ]
  edge [
    source 2
    target 608
  ]
  edge [
    source 2
    target 610
  ]
  edge [
    source 2
    target 612
  ]
  edge [
    source 2
    target 614
  ]
  edge [
    source 2
    target 616
  ]
  edge [
    source 2
    target 618
  ]
  edge [
    source 2
    target 620
  ]
  edge [
    source 2
    target 622
  ]
  edge [
    source 2
    target 624
  ]
  edge [
    source 2
    target 626
  ]
  edge [
    source 2
    target 628
  ]
  edge [
    source 2
    target 630
  ]
  edge [
    source 2
    target 632
  ]
  edge [
    source 2
    target 634
  ]
  edge [
    source 2
    target 636
  ]
  edge [
    source 2
    target 638
  ]
  edge [
    source 2
    target 640
  ]
  edge [
    source 2
    target 642
  ]
  edge [
    source 2
    target 644
  ]
  edge [
    source 2
    target 646
  ]
  edge [
    source 2
    target 648
  ]
  edge [
    source 2
    target 650
  ]
  edge [
    source 2
    target 652
  ]
  edge [
    source 2
    target 654
  ]
  edge [
    source 2
    target 656
  ]
  edge [
    source 2
    target 658
  ]
  edge [
    source 2
    target 660
  ]
  edge [
    source 2
    target 662
  ]
  edge [
    source 2
    target 664
  ]
  edge [
    source 2
    target 666
  ]
  edge [
    source 2
    target 668
  ]
  edge [
    source 2
    target 670
  ]
  edge [
    source 2
    target 672
  ]
  edge [
    source 2
    target 674
  ]
  edge [
    source 2
    target 676
  ]
  edge [
    source 2
    target 678
  ]
  edge [
    source 2
    target 680
  ]
  edge [
    source 2
    target 682
  ]
  edge [
    source 2
    target 684
  ]
  edge [
    source 2
    target 686
  ]
  edge [
    source 2
    target 688
  ]
  edge [
    source 2
    target 690
  ]
  edge [
    source 2
    target 692
  ]
  edge [
    source 2
    target 694
  ]
  edge [
    source 2
    target 696
  ]
  edge [
    source 2
    target 698
  ]
  edge [
    source 2
    target 700
  ]
  edge [
    source 2
    target 702
  ]
  edge [
    source 2
    target 704
  ]
  edge [
    source 2
    target 706
  ]
  edge [
    source 2
    target 708
  ]
  edge [
    source 2
    target 710
  ]
  edge [
    source 2
    target 712
  ]
  edge [
    source 2
    target 714
  ]
  edge [
    source 2
    target 716
  ]
  edge [
    source 2
    target 718
  ]
  edge [
    source 2
    target 720
  ]
  edge [
    source 2
    target 722
  ]
  edge [
    source 2
    target 724
  ]
  edge [
    source 2
    target 726
  ]
  edge [
    source 2
    target 728
  ]
  edge [
    source 2
    target 730
  ]
  edge [
    source 2
    target 732
  ]
  edge [
    source 2
    target 734
  ]
  edge [
    source 2
    target 736
  ]
  edge [
    source 2
    target 738
  ]
  edge [
    source 2
    target 740
  ]
  edge [
    source 2
    target 742
  ]
  edge [
    source 2
    target 744
  ]
  edge [
    source 2
    target 746
  ]
  edge [
    source 2
    target 748
  ]
  edge [
    source 2
    target 750
  ]
  edge [
    source 2
    target 752
  ]
  edge [
    source 2
    target 754
  ]
  edge [
    source 2
    target 756
  ]
  edge [
    source 2
    target 758
  ]
  edge [
    source 2
    target 760
  ]
  edge [
    source 2
    target 762
  ]
  edge [
    source 2
    target 764
  ]
  edge [
    source 2
    target 766
  ]
  edge [
    source 2
    target 768
  ]
  edge [
    source 2
    target 770
  ]
  edge [
    source 2
    target 772
  ]
  edge [
    source 2
    target 774
  ]
  edge [
    source 2
    target 776
  ]
  edge [
    source 2
    target 778
  ]
  edge [
    source 2
    target 780
  ]
  edge [
    source 2
    target 782
  ]
  edge [
    source 2
    target 784
  ]
  edge [
    source 2
    target 786
  ]
  edge [
    source 2
    target 788
  ]
  edge [
    source 2
    target 790
  ]
  edge [
    source 2
    target 792
  ]
  edge [
    source 2
    target 794
  ]
  edge [
    source 2
    target 796
  ]
  edge [
    source 2
    target 798
  ]
  edge [
    source 2
    target 800
  ]
  edge [
    source 2
    target 802
  ]
  edge [
    source 2
    target 804
  ]
  edge [
    source 2
    target 806
  ]
  edge [
    source 2
    target 808
  ]
  edge [
    source 2
    target 810
  ]
  edge [
    source 2
    target 812
  ]
  edge [
    source 2
    target 814
  ]
  edge [
    source 2
    target 816
  ]
  edge [
    source 2
    target 818
  ]
  edge [
    source 2
    target 820
  ]
  edge [
    source 2
    target 822
  ]
  edge [
    source 2
    target 824
  ]
  edge [
    source 2
    target 826
  ]
  edge [
    source 2
    target 828
  ]
  edge [
    source 2
    target 830
  ]
  edge [
    source 2
    target 832
  ]
  edge [
    source 2
    target 834
  ]
  edge [
    source 2
    target 836
  ]
  edge [
    source 2
    target 838
  ]
  edge [
    source 2
    target 840
  ]
  edge [
    source 2
    target 842
  ]
  edge [
    source 2
    target 844
  ]
  edge [
    source 2
    target 846
  ]
  edge [
    source 2
    target 848
  ]
  edge [
    source 2
    target 850
  ]
  edge [
    source 2
    target 852
  ]
  edge [
    source 2
    target 854
  ]
  edge [
    source 2
    target 856
  ]
  edge [
    source 2
    target 858
  ]
  edge [
    source 2
    target 860
  ]
  edge [
    source 2
    target 862
  ]
  edge [
    source 2
    target 864
  ]
  edge [
    source 2
    target 866
  ]
  edge [
    source 2
    target 868
  ]
  edge [
    source 2
    target 870
  ]
  edge [
    source 2
    target 872
  ]
  edge [
    source 2
    target 874
  ]
  edge [
    source 2
    target 876
  ]
  edge [
    source 2
    target 878
  ]
  edge [
    source 2
    target 880
  ]
  edge [
    source 2
    target 882
  ]
  edge [
    source 2
    target 884
  ]
  edge [
    source 2
    target 886
  ]
  edge [
    source 2
    target 888
  ]
  edge [
    source 2
    target 890
  ]
  edge [
    source 2
    target 892
  ]
  edge [
    source 2
    target 894
  ]
  edge [
    source 2
    target 896
  ]
  edge [
    source 2
    target 898
  ]
  edge [
    source 2
    target 900
  ]
  edge [
    source 2
    target 902
  ]
  edge [
    source 2
    target 904
  ]
  edge [
    source 2
    target 906
  ]
  edge [
    source 2
    target 908
  ]
  edge [
    source 2
    target 910
  ]
  edge [
    source 2
    target 912
  ]
  edge [
    source 2
    target 914
  ]
  edge [
    source 2
    target 916
  ]
  edge [
    source 2
    target 918
  ]
  edge [
    source 2
    target 920
  ]
  edge [
    source 2
    target 922
  ]
  edge [
    source 2
    target 924
  ]
  edge [
    source 2
    target 926
  ]
  edge [
    source 2
    target 928
  ]
  edge [
    source 2
    target 930
  ]
  edge [
    source 2
    target 932
  ]
  edge [
    source 2
    target 934
  ]
  edge [
    source 2
    target 936
  ]
  edge [
    source 2
    target 938
  ]
  edge [
    source 2
    target 940
  ]
  edge [
    source 2
    target 942
  ]
  edge [
    source 2
    target 944
  ]
  edge [
    source 2
    target 946
  ]
  edge [
    source 2
    target 948
  ]
  edge [
    source 2
    target 950
  ]
  edge [
    source 2
    target 952
  ]
  edge [
    source 2
    target 954
  ]
  edge [
    source 2
    target 956
  ]
  edge [
    source 2
    target 958
  ]
  edge [
    source 4
    target 5
  ]
  edge [
    source 6
    target 7
  ]
  edge [
    source 8
    target 9
  ]
  edge [
    source 10
    target 11
  ]
  edge [
    source 12
    target 13
  ]
  edge [
    source 14
    target 15
  ]
  edge [
    source 16
    target 17
  ]
  edge [
    source 18
    target 19
  ]
  edge [
    source 20
    target 21
  ]
  edge [
    source 22
    target 23
  ]
  edge [
    source 24
    target 25
  ]
  edge [
    source 26
    target 27
  ]
  edge [
    source 28
    target 29
  ]
  edge [
    source 30
    target 31
  ]
  edge [
    source 32
    target 33
  ]
  edge [
    source 34
    target 35
  ]
  edge [
    source 36
    target 37
  ]
  edge [
    source 38
    target 39
  ]
  edge [
    source 40
    target 41
  ]
  edge [
    source 42
    target 43
  ]
  edge [
    source 44
    target 45
  ]
  edge [
    source 46
    target 47
  ]
  edge [
    source 48
    target 49
  ]
  edge [
    source 50
    target 51
  ]
  edge [
    source 52
    target 53
  ]
  edge [
    source 54
    target 55
  ]
  edge [
    source 56
    target 57
  ]
  edge [
    source 58
    target 59
  ]
  edge [
    source 60
    target 61
  ]
  edge [
    source 62
    target 63
  ]
  edge [
    source 64
    target 65
  ]
  edge [
    source 66
    target 67
  ]
  edge [
    source 68
    target 69
  ]
  edge [
    source 70
    target 71
  ]
  edge [
    source 72
    target 73
  ]
  edge [
    source 74
    target 75
  ]
  edge [
    source 76
    target 77
  ]
  edge [
    source 78
    target 79
  ]
  edge [
    source 80
    target 81
  ]
  edge [
    source 82
    target 83
  ]
  edge [
    source 84
    target 85
  ]
  edge [
    source 86
    target 87
  ]
  edge [
    source 88
    target 89
  ]
  edge [
    source 90
    target 91
  ]
  edge [
    source 92
    target 93
  ]
  edge [
    source 94
    target 95
  ]
  edge [
    source 96
    target 97
  ]
  edge [
    source 98
    target 99
  ]
  edge [
    source 100
    target 101
  ]
  edge [
    source 102
    target 103
  ]
  edge [
    source 104
    target 105
  ]
  edge [
    source 106
    target 107
  ]
  edge [
    source 108
    target 109
  ]
  edge [
    source 110
    target 111
  ]
  edge [
    source 112
    target 113
  ]
  edge [
    source 114
    target 115
  ]
  edge [
    source 116
    target 117
  ]
  edge [
    source 118
    target 119
  ]
  edge [
    source 120
    target 121
  ]
  edge [
    source 122
    target 123
  ]
  edge [
    source 124
    target 125
  ]
  edge [
    source 126
    target 127
  ]
  edge [
    source 128
    target 129
  ]
  edge [
    source 130
    target 131
  ]
  edge [
    source 132
    target 133
  ]
  edge [
    source 134
    target 135
  ]
  edge [
    source 136
    target 137
  ]
  edge [
    source 138
    target 139
  ]
  edge [
    source 140
    target 141
  ]
  edge [
    source 142
    target 143
  ]
  edge [
    source 144
    target 145
  ]
  edge [
    source 146
    target 147
  ]
  edge [
    source 148
    target 149
  ]
  edge [
    source 150
    target 151
  ]
  edge [
    source 152
    target 153
  ]
  edge [
    source 154
    target 155
  ]
  edge [
    source 156
    target 157
  ]
  edge [
    source 158
    target 159
  ]
  edge [
    source 160
    target 161
  ]
  edge [
    source 162
    target 163
  ]
  edge [
    source 164
    target 165
  ]
  edge [
    source 166
    target 167
  ]
  edge [
    source 168
    target 169
  ]
  edge [
    source 170
    target 171
  ]
  edge [
    source 172
    target 173
  ]
  edge [
    source 174
    target 175
  ]
  edge [
    source 176
    target 177
  ]
  edge [
    source 178
    target 179
  ]
  edge [
    source 180
    target 181
  ]
  edge [
    source 182
    target 183
  ]
  edge [
    source 184
    target 185
  ]
  edge [
    source 186
    target 187
  ]
  edge [
    source 188
    target 189
  ]
  edge [
    source 190
    target 191
  ]
  edge [
    source 192
    target 193
  ]
  edge [
    source 194
    target 195
  ]
  edge [
    source 196
    target 197
  ]
  edge [
    source 198
    target 199
  ]
  edge [
    source 200
    target 201
  ]
  edge [
    source 202
    target 203
  ]
  edge [
    source 204
    target 205
  ]
  edge [
    source 206
    target 207
  ]
  edge [
    source 208
    target 209
  ]
  edge [
    source 210
    target 211
  ]
  edge [
    source 212
    target 213
  ]
  edge [
    source 214
    target 215
  ]
  edge [
    source 216
    target 217
  ]
  edge [
    source 218
    target 219
  ]
  edge [
    source 220
    target 221
  ]
  edge [
    source 222
    target 223
  ]
  edge [
    source 224
    target 225
  ]
  edge [
    source 226
    target 227
  ]
  edge [
    source 228
    target 229
  ]
  edge [
    source 230
    target 231
  ]
  edge [
    source 232
    target 233
  ]
  edge [
    source 234
    target 235
  ]
  edge [
    source 236
    target 237
  ]
  edge [
    source 238
    target 239
  ]
  edge [
    source 240
    target 241
  ]
  edge [
    source 242
    target 243
  ]
  edge [
    source 244
    target 245
  ]
  edge [
    source 246
    target 247
  ]
  edge [
    source 248
    target 249
  ]
  edge [
    source 250
    target 251
  ]
  edge [
    source 252
    target 253
  ]
  edge [
    source 254
    target 255
  ]
  edge [
    source 256
    target 257
  ]
  edge [
    source 258
    target 259
  ]
  edge [
    source 260
    target 261
  ]
  edge [
    source 262
    target 263
  ]
  edge [
    source 264
    target 265
  ]
  edge [
    source 266
    target 267
  ]
  edge [
    source 268
    target 269
  ]
  edge [
    source 270
    target 271
  ]
  edge [
    source 272
    target 273
  ]
  edge [
    source 274
    target 275
  ]
  edge [
    source 276
    target 277
  ]
  edge [
    source 278
    target 279
  ]
  edge [
    source 280
    target 281
  ]
  edge [
    source 282
    target 283
  ]
  edge [
    source 284
    target 285
  ]
  edge [
    source 286
    target 287
  ]
  edge [
    source 288
    target 289
  ]
  edge [
    source 290
    target 291
  ]
  edge [
    source 292
    target 293
  ]
  edge [
    source 294
    target 295
  ]
  edge [
    source 296
    target 297
  ]
  edge [
    source 298
    target 299
  ]
  edge [
    source 300
    target 301
  ]
  edge [
    source 302
    target 303
  ]
  edge [
    source 304
    target 305
  ]
  edge [
    source 306
    target 307
  ]
  edge [
    source 308
    target 309
  ]
  edge [
    source 310
    target 311
  ]
  edge [
    source 312
    target 313
  ]
  edge [
    source 314
    target 315
  ]
  edge [
    source 316
    target 317
  ]
  edge [
    source 318
    target 319
  ]
  edge [
    source 320
    target 321
  ]
  edge [
    source 322
    target 323
  ]
  edge [
    source 324
    target 325
  ]
  edge [
    source 326
    target 327
  ]
  edge [
    source 328
    target 329
  ]
  edge [
    source 330
    target 331
  ]
  edge [
    source 332
    target 333
  ]
  edge [
    source 334
    target 335
  ]
  edge [
    source 336
    target 337
  ]
  edge [
    source 338
    target 339
  ]
  edge [
    source 340
    target 341
  ]
  edge [
    source 342
    target 343
  ]
  edge [
    source 344
    target 345
  ]
  edge [
    source 346
    target 347
  ]
  edge [
    source 348
    target 349
  ]
  edge [
    source 350
    target 351
  ]
  edge [
    source 352
    target 353
  ]
  edge [
    source 354
    target 355
  ]
  edge [
    source 356
    target 357
  ]
  edge [
    source 358
    target 359
  ]
  edge [
    source 360
    target 361
  ]
  edge [
    source 362
    target 363
  ]
  edge [
    source 364
    target 365
  ]
  edge [
    source 366
    target 367
  ]
  edge [
    source 368
    target 369
  ]
  edge [
    source 370
    target 371
  ]
  edge [
    source 372
    target 373
  ]
  edge [
    source 374
    target 375
  ]
  edge [
    source 376
    target 377
  ]
  edge [
    source 378
    target 379
  ]
  edge [
    source 380
    target 381
  ]
  edge [
    source 382
    target 383
  ]
  edge [
    source 384
    target 385
  ]
  edge [
    source 386
    target 387
  ]
  edge [
    source 388
    target 389
  ]
  edge [
    source 390
    target 391
  ]
  edge [
    source 392
    target 393
  ]
  edge [
    source 394
    target 395
  ]
  edge [
    source 396
    target 397
  ]
  edge [
    source 398
    target 399
  ]
  edge [
    source 400
    target 401
  ]
  edge [
    source 402
    target 403
  ]
  edge [
    source 404
    target 405
  ]
  edge [
    source 406
    target 407
  ]
  edge [
    source 408
    target 409
  ]
  edge [
    source 410
    target 411
  ]
  edge [
    source 412
    target 413
  ]
  edge [
    source 414
    target 415
  ]
  edge [
    source 416
    target 417
  ]
  edge [
    source 418
    target 419
  ]
  edge [
    source 420
    target 421
  ]
  edge [
    source 422
    target 423
  ]
  edge [
    source 424
    target 425
  ]
  edge [
    source 426
    target 427
  ]
  edge [
    source 428
    target 429
  ]
  edge [
    source 430
    target 431
  ]
  edge [
    source 432
    target 433
  ]
  edge [
    source 434
    target 435
  ]
  edge [
    source 436
    target 437
  ]
  edge [
    source 438
    target 439
  ]
  edge [
    source 440
    target 441
  ]
  edge [
    source 442
    target 443
  ]
  edge [
    source 444
    target 445
  ]
  edge [
    source 446
    target 447
  ]
  edge [
    source 448
    target 449
  ]
  edge [
    source 450
    target 451
  ]
  edge [
    source 452
    target 453
  ]
  edge [
    source 454
    target 455
  ]
  edge [
    source 456
    target 457
  ]
  edge [
    source 458
    target 459
  ]
  edge [
    source 460
    target 461
  ]
  edge [
    source 462
    target 463
  ]
  edge [
    source 464
    target 465
  ]
  edge [
    source 466
    target 467
  ]
  edge [
    source 468
    target 469
  ]
  edge [
    source 470
    target 471
  ]
  edge [
    source 472
    target 473
  ]
  edge [
    source 474
    target 475
  ]
  edge [
    source 476
    target 477
  ]
  edge [
    source 478
    target 479
  ]
  edge [
    source 480
    target 481
  ]
  edge [
    source 482
    target 483
  ]
  edge [
    source 484
    target 485
  ]
  edge [
    source 486
    target 487
  ]
  edge [
    source 488
    target 489
  ]
  edge [
    source 490
    target 491
  ]
  edge [
    source 492
    target 493
  ]
  edge [
    source 494
    target 495
  ]
  edge [
    source 496
    target 497
  ]
  edge [
    source 498
    target 499
  ]
  edge [
    source 500
    target 501
  ]
  edge [
    source 502
    target 503
  ]
  edge [
    source 504
    target 505
  ]
  edge [
    source 506
    target 507
  ]
  edge [
    source 508
    target 509
  ]
  edge [
    source 510
    target 511
  ]
  edge [
    source 512
    target 513
  ]
  edge [
    source 514
    target 515
  ]
  edge [
    source 516
    target 517
  ]
  edge [
    source 518
    target 519
  ]
  edge [
    source 520
    target 521
  ]
  edge [
    source 522
    target 523
  ]
  edge [
    source 524
    target 525
  ]
  edge [
    source 526
    target 527
  ]
  edge [
    source 528
    target 529
  ]
  edge [
    source 530
    target 531
  ]
  edge [
    source 532
    target 533
  ]
  edge [
    source 534
    target 535
  ]
  edge [
    source 536
    target 537
  ]
  edge [
    source 538
    target 539
  ]
  edge [
    source 540
    target 541
  ]
  edge [
    source 542
    target 543
  ]
  edge [
    source 544
    target 545
  ]
  edge [
    source 546
    target 547
  ]
  edge [
    source 548
    target 549
  ]
  edge [
    source 550
    target 551
  ]
  edge [
    source 552
    target 553
  ]
  edge [
    source 554
    target 555
  ]
  edge [
    source 556
    target 557
  ]
  edge [
    source 558
    target 559
  ]
  edge [
    source 560
    target 561
  ]
  edge [
    source 562
    target 563
  ]
  edge [
    source 564
    target 565
  ]
  edge [
    source 566
    target 567
  ]
  edge [
    source 568
    target 569
  ]
  edge [
    source 570
    target 571
  ]
  edge [
    source 572
    target 573
  ]
  edge [
    source 574
    target 575
  ]
  edge [
    source 576
    target 577
  ]
  edge [
    source 578
    target 579
  ]
  edge [
    source 580
    target 581
  ]
  edge [
    source 582
    target 583
  ]
  edge [
    source 584
    target 585
  ]
  edge [
    source 586
    target 587
  ]
  edge [
    source 588
    target 589
  ]
  edge [
    source 590
    target 591
  ]
  edge [
    source 592
    target 593
  ]
  edge [
    source 594
    target 595
  ]
  edge [
    source 596
    target 597
  ]
  edge [
    source 598
    target 599
  ]
  edge [
    source 600
    target 601
  ]
  edge [
    source 602
    target 603
  ]
  edge [
    source 604
    target 605
  ]
  edge [
    source 606
    target 607
  ]
  edge [
    source 608
    target 609
  ]
  edge [
    source 610
    target 611
  ]
  edge [
    source 612
    target 613
  ]
  edge [
    source 614
    target 615
  ]
  edge [
    source 616
    target 617
  ]
  edge [
    source 618
    target 619
  ]
  edge [
    source 620
    target 621
  ]
  edge [
    source 622
    target 623
  ]
  edge [
    source 624
    target 625
  ]
  edge [
    source 626
    target 627
  ]
  edge [
    source 628
    target 629
  ]
  edge [
    source 630
    target 631
  ]
  edge [
    source 632
    target 633
  ]
  edge [
    source 634
    target 635
  ]
  edge [
    source 636
    target 637
  ]
  edge [
    source 638
    target 639
  ]
  edge [
    source 640
    target 641
  ]
  edge [
    source 642
    target 643
  ]
  edge [
    source 644
    target 645
  ]
  edge [
    source 646
    target 647
  ]
  edge [
    source 648
    target 649
  ]
  edge [
    source 650
    target 651
  ]
  edge [
    source 652
    target 653
  ]
  edge [
    source 654
    target 655
  ]
  edge [
    source 656
    target 657
  ]
  edge [
    source 658
    target 659
  ]
  edge [
    source 660
    target 661
  ]
  edge [
    source 662
    target 663
  ]
  edge [
    source 664
    target 665
  ]
  edge [
    source 666
    target 667
  ]
  edge [
    source 668
    target 669
  ]
  edge [
    source 670
    target 671
  ]
  edge [
    source 672
    target 673
  ]
  edge [
    source 674
    target 675
  ]
  edge [
    source 676
    target 677
  ]
  edge [
    source 678
    target 679
  ]
  edge [
    source 680
    target 681
  ]
  edge [
    source 682
    target 683
  ]
  edge [
    source 684
    target 685
  ]
  edge [
    source 686
    target 687
  ]
  edge [
    source 688
    target 689
  ]
  edge [
    source 690
    target 691
  ]
  edge [
    source 692
    target 693
  ]
  edge [
    source 694
    target 695
  ]
  edge [
    source 696
    target 697
  ]
  edge [
    source 698
    target 699
  ]
  edge [
    source 700
    target 701
  ]
  edge [
    source 702
    target 703
  ]
  edge [
    source 704
    target 705
  ]
  edge [
    source 706
    target 707
  ]
  edge [
    source 708
    target 709
  ]
  edge [
    source 710
    target 711
  ]
  edge [
    source 712
    target 713
  ]
  edge [
    source 714
    target 715
  ]
  edge [
    source 716
    target 717
  ]
  edge [
    source 718
    target 719
  ]
  edge [
    source 720
    target 721
  ]
  edge [
    source 722
    target 723
  ]
  edge [
    source 724
    target 725
  ]
  edge [
    source 726
    target 727
  ]
  edge [
    source 728
    target 729
  ]
  edge [
    source 730
    target 731
  ]
  edge [
    source 732
    target 733
  ]
  edge [
    source 734
    target 735
  ]
  edge [
    source 736
    target 737
  ]
  edge [
    source 738
    target 739
  ]
  edge [
    source 740
    target 741
  ]
  edge [
    source 742
    target 743
  ]
  edge [
    source 744
    target 745
  ]
  edge [
    source 746
    target 747
  ]
  edge [
    source 748
    target 749
  ]
  edge [
    source 750
    target 751
  ]
  edge [
    source 752
    target 753
  ]
  edge [
    source 754
    target 755
  ]
  edge [
    source 756
    target 757
  ]
  edge [
    source 758
    target 759
  ]
  edge [
    source 760
    target 761
  ]
  edge [
    source 762
    target 763
  ]
  edge [
    source 764
    target 765
  ]
  edge [
    source 766
    target 767
  ]
  edge [
    source 768
    target 769
  ]
  edge [
    source 770
    target 771
  ]
  edge [
    source 772
    target 773
  ]
  edge [
    source 774
    target 775
  ]
  edge [
    source 776
    target 777
  ]
  edge [
    source 778
    target 779
  ]
  edge [
    source 780
    target 781
  ]
  edge [
    source 782
    target 783
  ]
  edge [
    source 784
    target 785
  ]
  edge [
    source 786
    target 787
  ]
  edge [
    source 788
    target 789
  ]
  edge [
    source 790
    target 791
  ]
  edge [
    source 792
    target 793
  ]
  edge [
    source 794
    target 795
  ]
  edge [
    source 796
    target 797
  ]
  edge [
    source 798
    target 799
  ]
  edge [
    source 800
    target 801
  ]
  edge [
    source 802
    target 803
  ]
  edge [
    source 804
    target 805
  ]
  edge [
    source 806
    target 807
  ]
  edge [
    source 808
    target 809
  ]
  edge [
    source 810
    target 811
  ]
  edge [
    source 812
    target 813
  ]
  edge [
    source 814
    target 815
  ]
  edge [
    source 816
    target 817
  ]
  edge [
    source 818
    target 819
  ]
  edge [
    source 820
    target 821
  ]
  edge [
    source 822
    target 823
  ]
  edge [
    source 824
    target 825
  ]
  edge [
    source 826
    target 827
  ]
  edge [
    source 828
    target 829
  ]
  edge [
    source 830
    target 831
  ]
  edge [
    source 832
    target 833
  ]
  edge [
    source 834
    target 835
  ]
  edge [
    source 836
    target 837
  ]
  edge [
    source 838
    target 839
  ]
  edge [
    source 840
    target 841
  ]
  edge [
    source 842
    target 843
  ]
  edge [
    source 844
    target 845
  ]
  edge [
    source 846
    target 847
  ]
  edge [
    source 848
    target 849
  ]
  edge [
    source 850
    target 851
  ]
  edge [
    source 852
    target 853
  ]
  edge [
    source 854
    target 855
  ]
  edge [
    source 856
    target 857
  ]
  edge [
    source 858
    target 859
  ]
  edge [
    source 860
    target 861
  ]
  edge [
    source 862
    target 863
  ]
  edge [
    source 864
    target 865
  ]
  edge [
    source 866
    target 867
  ]
  edge [
    source 868
    target 869
  ]
  edge [
    source 870
    target 871
  ]
  edge [
    source 872
    target 873
  ]
  edge [
    source 874
    target 875
  ]
  edge [
    source 876
    target 877
  ]
  edge [
    source 878
    target 879
  ]
  edge [
    source 880
    target 881
  ]
  edge [
    source 882
    target 883
  ]
  edge [
    source 884
    target 885
  ]
  edge [
    source 886
    target 887
  ]
  edge [
    source 888
    target 889
  ]
  edge [
    source 890
    target 891
  ]
  edge [
    source 892
    target 893
  ]
  edge [
    source 894
    target 895
  ]
  edge [
    source 896
    target 897
  ]
  edge [
    source 898
    target 899
  ]
  edge [
    source 900
    target 901
  ]
  edge [
    source 902
    target 903
  ]
  edge [
    source 904
    target 905
  ]
  edge [
    source 906
    target 907
  ]
  edge [
    source 908
    target 909
  ]
  edge [
    source 910
    target 911
  ]
  edge [
    source 912
    target 913
  ]
  edge [
    source 914
    target 915
  ]
  edge [
    source 916
    target 917
  ]
  edge [
    source 918
    target 919
  ]
  edge [
    source 920
    target 921
  ]
  edge [
    source 922
    target 923
  ]
  edge [
    source 924
    target 925
  ]
  edge [
    source 926
    target 927
  ]
  edge [
    source 928
    target 929
  ]
  edge [
    source 930
    target 931
  ]
  edge [
    source 932
    target 933
  ]
  edge [
    source 934
    target 935
  ]
  edge [
    source 936
    target 937
  ]
  edge [
    source 938
    target 939
  ]
  edge [
    source 940
    target 941
  ]
  edge [
    source 942
    target 943
  ]
  edge [
    source 944
    target 945
  ]
  edge [
    source 946
    target 947
  ]
  edge [
    source 948
    target 949
  ]
  edge [
    source 950
    target 951
  ]
  edge [
    source 952
    target 953
  ]
  edge [
    source 954
    target 955
  ]
  edge [
    source 956
    target 957
  ]
  edge [
    source 958
    target 959
  ]
]
