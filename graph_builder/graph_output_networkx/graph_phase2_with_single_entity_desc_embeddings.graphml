<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d7" for="edge" attr.name="name" attr.type="string"/>
<key id="d6" for="edge" attr.name="width" attr.type="long"/>
<key id="d5" for="node" attr.name="title" attr.type="string"/>
<key id="d4" for="node" attr.name="size" attr.type="long"/>
<key id="d3" for="node" attr.name="category" attr.type="string"/>
<key id="d2" for="node" attr.name="description" attr.type="string"/>
<key id="d1" for="node" attr.name="color" attr.type="string"/>
<key id="d0" for="node" attr.name="label" attr.type="string"/>
<graph edgedefault="directed"><node id="Kubernetes_in_Action.pdf">
  <data key="d0">Kubernetes_in_Action.pdf</data>
  <data key="d1">red</data>
  <data key="d2">this is the name of the book</data>
  <data key="d3">Cloud_Computing</data>
  <data key="d4">10</data>
</node>
<node id="89">
  <data key="d0">Page_89</data>
  <data key="d5">Page_89</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="1">
  <data key="d4">10</data>
</node>
<node id="text_0">
  <data key="d0">57
Introducing pods
 Therefore, you need to run each process in its own container. That’s how Docker
and Kubernetes are meant to be used. 
3.1.2
Understanding pods
Because you’re not supposed to group multiple processes into a single container, it’s
obvious you need another higher-level construct that will allow you to bind containers
together and manage them as a single unit. This is the reasoning behind pods. 
 A pod of containers allows you to run closely related processes together and pro-
vide them with (almost) the same environment as if they were all running in a single
container, while keeping them somewhat isolated. This way, you get the best of both
worlds. You can take advantage of all the features containers provide, while at the
same time giving the processes the illusion of running together. 
UNDERSTANDING THE PARTIAL ISOLATION BETWEEN CONTAINERS OF THE SAME POD
In the previous chapter, you learned that containers are completely isolated from
each other, but now you see that you want to isolate groups of containers instead of
individual ones. You want containers inside each group to share certain resources,
although not all, so that they’re not fully isolated. Kubernetes achieves this by config-
uring Docker to have all containers of a pod share the same set of Linux namespaces
instead of each container having its own set. 
 Because all containers of a pod run under the same Network and UTS namespaces
(we’re talking about Linux namespaces here), they all share the same hostname and
network interfaces. Similarly, all containers of a pod run under the same IPC namespace
and can communicate through IPC. In the latest Kubernetes and Docker versions, they
can also share the same PID namespace, but that feature isn’t enabled by default. 
NOTE
When containers of the same pod use separate PID namespaces, you
only see the container’s own processes when running ps aux in the container.
But when it comes to the filesystem, things are a little different. Because most of the
container’s filesystem comes from the container image, by default, the filesystem of
each container is fully isolated from other containers. However, it’s possible to have
them share file directories using a Kubernetes concept called a Volume, which we’ll
talk about in chapter 6.
UNDERSTANDING HOW CONTAINERS SHARE THE SAME IP AND PORT SPACE
One thing to stress here is that because containers in a pod run in the same Network
namespace, they share the same IP address and port space. This means processes run-
ning in containers of the same pod need to take care not to bind to the same port
numbers or they’ll run into port conflicts. But this only concerns containers in the
same pod. Containers of different pods can never run into port conflicts, because
each pod has a separate port space. All the containers in a pod also have the same
loopback network interface, so a container can communicate with other containers in
the same pod through localhost.
 
</data>
  <data key="d5">57
Introducing pods
 Therefore, you need to run each process in its own container. That’s how Docker
and Kubernetes are meant to be used. 
3.1.2
Understanding pods
Because you’re not supposed to group multiple processes into a single container, it’s
obvious you need another higher-level construct that will allow you to bind containers
together and manage them as a single unit. This is the reasoning behind pods. 
 A pod of containers allows you to run closely related processes together and pro-
vide them with (almost) the same environment as if they were all running in a single
container, while keeping them somewhat isolated. This way, you get the best of both
worlds. You can take advantage of all the features containers provide, while at the
same time giving the processes the illusion of running together. 
UNDERSTANDING THE PARTIAL ISOLATION BETWEEN CONTAINERS OF THE SAME POD
In the previous chapter, you learned that containers are completely isolated from
each other, but now you see that you want to isolate groups of containers instead of
individual ones. You want containers inside each group to share certain resources,
although not all, so that they’re not fully isolated. Kubernetes achieves this by config-
uring Docker to have all containers of a pod share the same set of Linux namespaces
instead of each container having its own set. 
 Because all containers of a pod run under the same Network and UTS namespaces
(we’re talking about Linux namespaces here), they all share the same hostname and
network interfaces. Similarly, all containers of a pod run under the same IPC namespace
and can communicate through IPC. In the latest Kubernetes and Docker versions, they
can also share the same PID namespace, but that feature isn’t enabled by default. 
NOTE
When containers of the same pod use separate PID namespaces, you
only see the container’s own processes when running ps aux in the container.
But when it comes to the filesystem, things are a little different. Because most of the
container’s filesystem comes from the container image, by default, the filesystem of
each container is fully isolated from other containers. However, it’s possible to have
them share file directories using a Kubernetes concept called a Volume, which we’ll
talk about in chapter 6.
UNDERSTANDING HOW CONTAINERS SHARE THE SAME IP AND PORT SPACE
One thing to stress here is that because containers in a pod run in the same Network
namespace, they share the same IP address and port space. This means processes run-
ning in containers of the same pod need to take care not to bind to the same port
numbers or they’ll run into port conflicts. But this only concerns containers in the
same pod. Containers of different pods can never run into port conflicts, because
each pod has a separate port space. All the containers in a pod also have the same
loopback network interface, so a container can communicate with other containers in
the same pod through localhost.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="90">
  <data key="d0">Page_90</data>
  <data key="d5">Page_90</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_1">
  <data key="d0">58
CHAPTER 3
Pods: running containers in Kubernetes
INTRODUCING THE FLAT INTER-POD NETWORK
All pods in a Kubernetes cluster reside in a single flat, shared, network-address space
(shown in figure 3.2), which means every pod can access every other pod at the other
pod’s IP address. No NAT (Network Address Translation) gateways exist between them.
When two pods send network packets between each other, they’ll each see the actual
IP address of the other as the source IP in the packet.
Consequently, communication between pods is always simple. It doesn’t matter if two
pods are scheduled onto a single or onto different worker nodes; in both cases the
containers inside those pods can communicate with each other across the flat NAT-
less network, much like computers on a local area network (LAN), regardless of the
actual inter-node network topology. Like a computer on a LAN, each pod gets its own
IP address and is accessible from all other pods through this network established spe-
cifically for pods. This is usually achieved through an additional software-defined net-
work layered on top of the actual network.
 To sum up what’s been covered in this section: pods are logical hosts and behave
much like physical hosts or VMs in the non-container world. Processes running in the
same pod are like processes running on the same physical or virtual machine, except
that each process is encapsulated in a container. 
3.1.3
Organizing containers across pods properly
You should think of pods as separate machines, but where each one hosts only a cer-
tain app. Unlike the old days, when we used to cram all sorts of apps onto the same
host, we don’t do that with pods. Because pods are relatively lightweight, you can have
as many as you need without incurring almost any overhead. Instead of stuffing every-
thing into a single pod, you should organize apps into multiple pods, where each one
contains only tightly related components or processes.
Node 1
Pod A
IP: 10.1.1.6
Container 1
Container 2
Pod B
IP: 10.1.1.7
Container 1
Container 2
Node 2
Flat network
Pod C
IP: 10.1.2.5
Container 1
Container 2
Pod D
IP: 10.1.2.7
Container 1
Container 2
Figure 3.2
Each pod gets a routable IP address and all other pods see the pod under 
that IP address.
 
</data>
  <data key="d5">58
CHAPTER 3
Pods: running containers in Kubernetes
INTRODUCING THE FLAT INTER-POD NETWORK
All pods in a Kubernetes cluster reside in a single flat, shared, network-address space
(shown in figure 3.2), which means every pod can access every other pod at the other
pod’s IP address. No NAT (Network Address Translation) gateways exist between them.
When two pods send network packets between each other, they’ll each see the actual
IP address of the other as the source IP in the packet.
Consequently, communication between pods is always simple. It doesn’t matter if two
pods are scheduled onto a single or onto different worker nodes; in both cases the
containers inside those pods can communicate with each other across the flat NAT-
less network, much like computers on a local area network (LAN), regardless of the
actual inter-node network topology. Like a computer on a LAN, each pod gets its own
IP address and is accessible from all other pods through this network established spe-
cifically for pods. This is usually achieved through an additional software-defined net-
work layered on top of the actual network.
 To sum up what’s been covered in this section: pods are logical hosts and behave
much like physical hosts or VMs in the non-container world. Processes running in the
same pod are like processes running on the same physical or virtual machine, except
that each process is encapsulated in a container. 
3.1.3
Organizing containers across pods properly
You should think of pods as separate machines, but where each one hosts only a cer-
tain app. Unlike the old days, when we used to cram all sorts of apps onto the same
host, we don’t do that with pods. Because pods are relatively lightweight, you can have
as many as you need without incurring almost any overhead. Instead of stuffing every-
thing into a single pod, you should organize apps into multiple pods, where each one
contains only tightly related components or processes.
Node 1
Pod A
IP: 10.1.1.6
Container 1
Container 2
Pod B
IP: 10.1.1.7
Container 1
Container 2
Node 2
Flat network
Pod C
IP: 10.1.2.5
Container 1
Container 2
Pod D
IP: 10.1.2.7
Container 1
Container 2
Figure 3.2
Each pod gets a routable IP address and all other pods see the pod under 
that IP address.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="91">
  <data key="d0">Page_91</data>
  <data key="d5">Page_91</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_2">
  <data key="d0">59
Introducing pods
 Having said that, do you think a multi-tier application consisting of a frontend
application server and a backend database should be configured as a single pod or as
two pods?
SPLITTING MULTI-TIER APPS INTO MULTIPLE PODS
Although nothing is stopping you from running both the frontend server and the
database in a single pod with two containers, it isn’t the most appropriate way. We’ve
said that all containers of the same pod always run co-located, but do the web server
and the database really need to run on the same machine? The answer is obviously no,
so you don’t want to put them into a single pod. But is it wrong to do so regardless? In
a way, it is.
 If both the frontend and backend are in the same pod, then both will always be
run on the same machine. If you have a two-node Kubernetes cluster and only this sin-
gle pod, you’ll only be using a single worker node and not taking advantage of the
computational resources (CPU and memory) you have at your disposal on the second
node. Splitting the pod into two would allow Kubernetes to schedule the frontend to
one node and the backend to the other node, thereby improving the utilization of
your infrastructure.
SPLITTING INTO MULTIPLE PODS TO ENABLE INDIVIDUAL SCALING
Another reason why you shouldn’t put them both into a single pod is scaling. A pod is
also the basic unit of scaling. Kubernetes can’t horizontally scale individual contain-
ers; instead, it scales whole pods. If your pod consists of a frontend and a backend con-
tainer, when you scale up the number of instances of the pod to, let’s say, two, you end
up with two frontend containers and two backend containers. 
 Usually, frontend components have completely different scaling requirements
than the backends, so we tend to scale them individually. Not to mention the fact that
backends such as databases are usually much harder to scale compared to (stateless)
frontend web servers. If you need to scale a container individually, this is a clear indi-
cation that it needs to be deployed in a separate pod. 
UNDERSTANDING WHEN TO USE MULTIPLE CONTAINERS IN A POD
The main reason to put multiple containers into a single pod is when the application
consists of one main process and one or more complementary processes, as shown in
figure 3.3.
Pod
Main container
Supporting
container 1
Supporting
container 2
Volume
Figure 3.3
Pods should contain tightly coupled 
containers, usually a main container and containers 
that support the main one.
 
</data>
  <data key="d5">59
Introducing pods
 Having said that, do you think a multi-tier application consisting of a frontend
application server and a backend database should be configured as a single pod or as
two pods?
SPLITTING MULTI-TIER APPS INTO MULTIPLE PODS
Although nothing is stopping you from running both the frontend server and the
database in a single pod with two containers, it isn’t the most appropriate way. We’ve
said that all containers of the same pod always run co-located, but do the web server
and the database really need to run on the same machine? The answer is obviously no,
so you don’t want to put them into a single pod. But is it wrong to do so regardless? In
a way, it is.
 If both the frontend and backend are in the same pod, then both will always be
run on the same machine. If you have a two-node Kubernetes cluster and only this sin-
gle pod, you’ll only be using a single worker node and not taking advantage of the
computational resources (CPU and memory) you have at your disposal on the second
node. Splitting the pod into two would allow Kubernetes to schedule the frontend to
one node and the backend to the other node, thereby improving the utilization of
your infrastructure.
SPLITTING INTO MULTIPLE PODS TO ENABLE INDIVIDUAL SCALING
Another reason why you shouldn’t put them both into a single pod is scaling. A pod is
also the basic unit of scaling. Kubernetes can’t horizontally scale individual contain-
ers; instead, it scales whole pods. If your pod consists of a frontend and a backend con-
tainer, when you scale up the number of instances of the pod to, let’s say, two, you end
up with two frontend containers and two backend containers. 
 Usually, frontend components have completely different scaling requirements
than the backends, so we tend to scale them individually. Not to mention the fact that
backends such as databases are usually much harder to scale compared to (stateless)
frontend web servers. If you need to scale a container individually, this is a clear indi-
cation that it needs to be deployed in a separate pod. 
UNDERSTANDING WHEN TO USE MULTIPLE CONTAINERS IN A POD
The main reason to put multiple containers into a single pod is when the application
consists of one main process and one or more complementary processes, as shown in
figure 3.3.
Pod
Main container
Supporting
container 1
Supporting
container 2
Volume
Figure 3.3
Pods should contain tightly coupled 
containers, usually a main container and containers 
that support the main one.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="92">
  <data key="d0">Page_92</data>
  <data key="d5">Page_92</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_3">
  <data key="d0">60
CHAPTER 3
Pods: running containers in Kubernetes
For example, the main container in a pod could be a web server that serves files from
a certain file directory, while an additional container (a sidecar container) periodi-
cally downloads content from an external source and stores it in the web server’s
directory. In chapter 6 you’ll see that you need to use a Kubernetes Volume that you
mount into both containers. 
 Other examples of sidecar containers include log rotators and collectors, data pro-
cessors, communication adapters, and others.
DECIDING WHEN TO USE MULTIPLE CONTAINERS IN A POD
To recap how containers should be grouped into pods—when deciding whether to
put two containers into a single pod or into two separate pods, you always need to ask
yourself the following questions:
Do they need to be run together or can they run on different hosts?
Do they represent a single whole or are they independent components?
Must they be scaled together or individually? 
Basically, you should always gravitate toward running containers in separate pods,
unless a specific reason requires them to be part of the same pod. Figure 3.4 will help
you memorize this.
Although pods can contain multiple containers, to keep things simple for now, you’ll
only be dealing with single-container pods in this chapter. You’ll see how multiple
containers are used in the same pod later, in chapter 6. 
Pod
Frontend
process
Backend
process
Container
Pod
Frontend
process
Frontend
container
Frontend pod
Frontend
process
Frontend
container
Backend pod
Backend
process
Backend
container
Backend
process
Backend
container
Figure 3.4
A container shouldn’t run multiple processes. A pod shouldn’t contain multiple 
containers if they don’t need to run on the same machine.
 
</data>
  <data key="d5">60
CHAPTER 3
Pods: running containers in Kubernetes
For example, the main container in a pod could be a web server that serves files from
a certain file directory, while an additional container (a sidecar container) periodi-
cally downloads content from an external source and stores it in the web server’s
directory. In chapter 6 you’ll see that you need to use a Kubernetes Volume that you
mount into both containers. 
 Other examples of sidecar containers include log rotators and collectors, data pro-
cessors, communication adapters, and others.
DECIDING WHEN TO USE MULTIPLE CONTAINERS IN A POD
To recap how containers should be grouped into pods—when deciding whether to
put two containers into a single pod or into two separate pods, you always need to ask
yourself the following questions:
Do they need to be run together or can they run on different hosts?
Do they represent a single whole or are they independent components?
Must they be scaled together or individually? 
Basically, you should always gravitate toward running containers in separate pods,
unless a specific reason requires them to be part of the same pod. Figure 3.4 will help
you memorize this.
Although pods can contain multiple containers, to keep things simple for now, you’ll
only be dealing with single-container pods in this chapter. You’ll see how multiple
containers are used in the same pod later, in chapter 6. 
Pod
Frontend
process
Backend
process
Container
Pod
Frontend
process
Frontend
container
Frontend pod
Frontend
process
Frontend
container
Backend pod
Backend
process
Backend
container
Backend
process
Backend
container
Figure 3.4
A container shouldn’t run multiple processes. A pod shouldn’t contain multiple 
containers if they don’t need to run on the same machine.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="93">
  <data key="d0">Page_93</data>
  <data key="d5">Page_93</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_4">
  <data key="d0">61
Creating pods from YAML or JSON descriptors
3.2
Creating pods from YAML or JSON descriptors
Pods and other Kubernetes resources are usually created by posting a JSON or YAML
manifest to the Kubernetes REST API endpoint. Also, you can use other, simpler ways
of creating resources, such as the kubectl run command you used in the previous
chapter, but they usually only allow you to configure a limited set of properties, not
all. Additionally, defining all your Kubernetes objects from YAML files makes it possi-
ble to store them in a version control system, with all the benefits it brings.
 To configure all aspects of each type of resource, you’ll need to know and under-
stand the Kubernetes API object definitions. You’ll get to know most of them as you
learn about each resource type throughout this book. We won’t explain every single
property, so you should also refer to the Kubernetes API reference documentation at
http:/
/kubernetes.io/docs/reference/ when creating objects.
3.2.1
Examining a YAML descriptor of an existing pod
You already have some existing pods you created in the previous chapter, so let’s look
at what a YAML definition for one of those pods looks like. You’ll use the kubectl get
command with the -o yaml option to get the whole YAML definition of the pod, as
shown in the following listing.
$ kubectl get po kubia-zxzij -o yaml
apiVersion: v1                         
kind: Pod                                       
metadata:                                                 
  annotations:                                            
    kubernetes.io/created-by: ...                         
  creationTimestamp: 2016-03-18T12:37:50Z                 
  generateName: kubia-                                    
  labels:                                                 
    run: kubia                                            
  name: kubia-zxzij                                       
  namespace: default                                      
  resourceVersion: "294"                                  
  selfLink: /api/v1/namespaces/default/pods/kubia-zxzij   
  uid: 3a564dc0-ed06-11e5-ba3b-42010af00004               
spec:                                                   
  containers:                                           
  - image: luksa/kubia                                  
    imagePullPolicy: IfNotPresent                       
    name: kubia                                         
    ports:                                              
    - containerPort: 8080                               
      protocol: TCP                                     
    resources:                                          
      requests:                                         
        cpu: 100m                                       
Listing 3.1
Full YAML of a deployed pod
Kubernetes API version used 
in this YAML descriptor
Type of Kubernetes 
object/resource
Pod metadata (name, 
labels, annotations, 
and so on)
Pod specification/
contents (list of 
pod’s containers, 
volumes, and so on)
 
</data>
  <data key="d5">61
Creating pods from YAML or JSON descriptors
3.2
Creating pods from YAML or JSON descriptors
Pods and other Kubernetes resources are usually created by posting a JSON or YAML
manifest to the Kubernetes REST API endpoint. Also, you can use other, simpler ways
of creating resources, such as the kubectl run command you used in the previous
chapter, but they usually only allow you to configure a limited set of properties, not
all. Additionally, defining all your Kubernetes objects from YAML files makes it possi-
ble to store them in a version control system, with all the benefits it brings.
 To configure all aspects of each type of resource, you’ll need to know and under-
stand the Kubernetes API object definitions. You’ll get to know most of them as you
learn about each resource type throughout this book. We won’t explain every single
property, so you should also refer to the Kubernetes API reference documentation at
http:/
/kubernetes.io/docs/reference/ when creating objects.
3.2.1
Examining a YAML descriptor of an existing pod
You already have some existing pods you created in the previous chapter, so let’s look
at what a YAML definition for one of those pods looks like. You’ll use the kubectl get
command with the -o yaml option to get the whole YAML definition of the pod, as
shown in the following listing.
$ kubectl get po kubia-zxzij -o yaml
apiVersion: v1                         
kind: Pod                                       
metadata:                                                 
  annotations:                                            
    kubernetes.io/created-by: ...                         
  creationTimestamp: 2016-03-18T12:37:50Z                 
  generateName: kubia-                                    
  labels:                                                 
    run: kubia                                            
  name: kubia-zxzij                                       
  namespace: default                                      
  resourceVersion: "294"                                  
  selfLink: /api/v1/namespaces/default/pods/kubia-zxzij   
  uid: 3a564dc0-ed06-11e5-ba3b-42010af00004               
spec:                                                   
  containers:                                           
  - image: luksa/kubia                                  
    imagePullPolicy: IfNotPresent                       
    name: kubia                                         
    ports:                                              
    - containerPort: 8080                               
      protocol: TCP                                     
    resources:                                          
      requests:                                         
        cpu: 100m                                       
Listing 3.1
Full YAML of a deployed pod
Kubernetes API version used 
in this YAML descriptor
Type of Kubernetes 
object/resource
Pod metadata (name, 
labels, annotations, 
and so on)
Pod specification/
contents (list of 
pod’s containers, 
volumes, and so on)
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="94">
  <data key="d0">Page_94</data>
  <data key="d5">Page_94</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_5">
  <data key="d0">62
CHAPTER 3
Pods: running containers in Kubernetes
    terminationMessagePath: /dev/termination-log      
    volumeMounts:                                     
    - mountPath: /var/run/secrets/k8s.io/servacc      
      name: default-token-kvcqa                       
      readOnly: true                                  
  dnsPolicy: ClusterFirst                             
  nodeName: gke-kubia-e8fe08b8-node-txje              
  restartPolicy: Always                               
  serviceAccount: default                             
  serviceAccountName: default                         
  terminationGracePeriodSeconds: 30                   
  volumes:                                            
  - name: default-token-kvcqa                         
    secret:                                           
      secretName: default-token-kvcqa                 
status:                                                   
  conditions:                                             
  - lastProbeTime: null                                   
    lastTransitionTime: null                              
    status: "True"                                        
    type: Ready                                           
  containerStatuses:                                      
  - containerID: docker://f0276994322d247ba...            
    image: luksa/kubia                                    
    imageID: docker://4c325bcc6b40c110226b89fe...         
    lastState: {}                                         
    name: kubia                                           
    ready: true                                           
    restartCount: 0                                       
    state:                                                
      running:                                            
        startedAt: 2016-03-18T12:46:05Z                   
  hostIP: 10.132.0.4                                      
  phase: Running                                          
  podIP: 10.0.2.3                                         
  startTime: 2016-03-18T12:44:32Z                         
I know this looks complicated, but it becomes simple once you understand the basics
and know how to distinguish between the important parts and the minor details. Also,
you can take comfort in the fact that when creating a new pod, the YAML you need to
write is much shorter, as you’ll see later.
INTRODUCING THE MAIN PARTS OF A POD DEFINITION
The pod definition consists of a few parts. First, there’s the Kubernetes API version
used in the YAML and the type of resource the YAML is describing. Then, three
important sections are found in almost all Kubernetes resources:
Metadata includes the name, namespace, labels, and other information about
the pod.
Spec contains the actual description of the pod’s contents, such as the pod’s con-
tainers, volumes, and other data.
Pod specification/
contents (list of 
pod’s containers, 
volumes, and so on)
Detailed status 
of the pod and 
its containers
 
</data>
  <data key="d5">62
CHAPTER 3
Pods: running containers in Kubernetes
    terminationMessagePath: /dev/termination-log      
    volumeMounts:                                     
    - mountPath: /var/run/secrets/k8s.io/servacc      
      name: default-token-kvcqa                       
      readOnly: true                                  
  dnsPolicy: ClusterFirst                             
  nodeName: gke-kubia-e8fe08b8-node-txje              
  restartPolicy: Always                               
  serviceAccount: default                             
  serviceAccountName: default                         
  terminationGracePeriodSeconds: 30                   
  volumes:                                            
  - name: default-token-kvcqa                         
    secret:                                           
      secretName: default-token-kvcqa                 
status:                                                   
  conditions:                                             
  - lastProbeTime: null                                   
    lastTransitionTime: null                              
    status: "True"                                        
    type: Ready                                           
  containerStatuses:                                      
  - containerID: docker://f0276994322d247ba...            
    image: luksa/kubia                                    
    imageID: docker://4c325bcc6b40c110226b89fe...         
    lastState: {}                                         
    name: kubia                                           
    ready: true                                           
    restartCount: 0                                       
    state:                                                
      running:                                            
        startedAt: 2016-03-18T12:46:05Z                   
  hostIP: 10.132.0.4                                      
  phase: Running                                          
  podIP: 10.0.2.3                                         
  startTime: 2016-03-18T12:44:32Z                         
I know this looks complicated, but it becomes simple once you understand the basics
and know how to distinguish between the important parts and the minor details. Also,
you can take comfort in the fact that when creating a new pod, the YAML you need to
write is much shorter, as you’ll see later.
INTRODUCING THE MAIN PARTS OF A POD DEFINITION
The pod definition consists of a few parts. First, there’s the Kubernetes API version
used in the YAML and the type of resource the YAML is describing. Then, three
important sections are found in almost all Kubernetes resources:
Metadata includes the name, namespace, labels, and other information about
the pod.
Spec contains the actual description of the pod’s contents, such as the pod’s con-
tainers, volumes, and other data.
Pod specification/
contents (list of 
pod’s containers, 
volumes, and so on)
Detailed status 
of the pod and 
its containers
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="95">
  <data key="d0">Page_95</data>
  <data key="d5">Page_95</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_6">
  <data key="d0">63
Creating pods from YAML or JSON descriptors
Status contains the current information about the running pod, such as what
condition the pod is in, the description and status of each container, and the
pod’s internal IP and other basic info.
Listing 3.1 showed a full description of a running pod, including its status. The status
part contains read-only runtime data that shows the state of the resource at a given
moment. When creating a new pod, you never need to provide the status part. 
 The three parts described previously show the typical structure of a Kubernetes
API object. As you’ll see throughout the book, all other objects have the same anat-
omy. This makes understanding new objects relatively easy.
 Going through all the individual properties in the previous YAML doesn’t make
much sense, so, instead, let’s see what the most basic YAML for creating a pod looks
like. 
3.2.2
Creating a simple YAML descriptor for a pod
You’re going to create a file called kubia-manual.yaml (you can create it in any
directory you want), or download the book’s code archive, where you’ll find the
file inside the Chapter03 directory. The following listing shows the entire contents
of the file.
apiVersion: v1         
kind: Pod                             
metadata:     
  name: kubia-manual         
spec: 
  containers: 
  - image: luksa/kubia          
    name: kubia         
    ports: 
    - containerPort: 8080     
      protocol: TCP
I’m sure you’ll agree this is much simpler than the definition in listing 3.1. Let’s exam-
ine this descriptor in detail. It conforms to the v1 version of the Kubernetes API. The
type of resource you’re describing is a pod, with the name kubia-manual. The pod
consists of a single container based on the luksa/kubia image. You’ve also given a
name to the container and indicated that it’s listening on port 8080. 
SPECIFYING CONTAINER PORTS
Specifying ports in the pod definition is purely informational. Omitting them has no
effect on whether clients can connect to the pod through the port or not. If the con-
Listing 3.2
A basic pod manifest: kubia-manual.yaml
Descriptor conforms
to version v1 of
Kubernetes API
You’re 
describing a pod.
The name 
of the pod
Container image to create 
the container from
Name of the container
The port the app 
is listening on
 
</data>
  <data key="d5">63
Creating pods from YAML or JSON descriptors
Status contains the current information about the running pod, such as what
condition the pod is in, the description and status of each container, and the
pod’s internal IP and other basic info.
Listing 3.1 showed a full description of a running pod, including its status. The status
part contains read-only runtime data that shows the state of the resource at a given
moment. When creating a new pod, you never need to provide the status part. 
 The three parts described previously show the typical structure of a Kubernetes
API object. As you’ll see throughout the book, all other objects have the same anat-
omy. This makes understanding new objects relatively easy.
 Going through all the individual properties in the previous YAML doesn’t make
much sense, so, instead, let’s see what the most basic YAML for creating a pod looks
like. 
3.2.2
Creating a simple YAML descriptor for a pod
You’re going to create a file called kubia-manual.yaml (you can create it in any
directory you want), or download the book’s code archive, where you’ll find the
file inside the Chapter03 directory. The following listing shows the entire contents
of the file.
apiVersion: v1         
kind: Pod                             
metadata:     
  name: kubia-manual         
spec: 
  containers: 
  - image: luksa/kubia          
    name: kubia         
    ports: 
    - containerPort: 8080     
      protocol: TCP
I’m sure you’ll agree this is much simpler than the definition in listing 3.1. Let’s exam-
ine this descriptor in detail. It conforms to the v1 version of the Kubernetes API. The
type of resource you’re describing is a pod, with the name kubia-manual. The pod
consists of a single container based on the luksa/kubia image. You’ve also given a
name to the container and indicated that it’s listening on port 8080. 
SPECIFYING CONTAINER PORTS
Specifying ports in the pod definition is purely informational. Omitting them has no
effect on whether clients can connect to the pod through the port or not. If the con-
Listing 3.2
A basic pod manifest: kubia-manual.yaml
Descriptor conforms
to version v1 of
Kubernetes API
You’re 
describing a pod.
The name 
of the pod
Container image to create 
the container from
Name of the container
The port the app 
is listening on
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="96">
  <data key="d0">Page_96</data>
  <data key="d5">Page_96</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_7">
  <data key="d0">64
CHAPTER 3
Pods: running containers in Kubernetes
tainer is accepting connections through a port bound to the 0.0.0.0 address, other
pods can always connect to it, even if the port isn’t listed in the pod spec explicitly. But
it makes sense to define the ports explicitly so that everyone using your cluster can
quickly see what ports each pod exposes. Explicitly defining ports also allows you to
assign a name to each port, which can come in handy, as you’ll see later in the book.
Using kubectl explain to discover possible API object fields
When preparing a manifest, you can either turn to the Kubernetes reference
documentation at http:/
/kubernetes.io/docs/api to see which attributes are
supported by each API object, or you can use the kubectl explain command.
For example, when creating a pod manifest from scratch, you can start by asking
kubectl to explain pods:
$ kubectl explain pods
DESCRIPTION:
Pod is a collection of containers that can run on a host. This resource 
is created by clients and scheduled onto hosts.
FIELDS:
   kind      &lt;string&gt;
     Kind is a string value representing the REST resource this object
     represents...
   metadata  &lt;Object&gt;
     Standard object's metadata...
   spec      &lt;Object&gt;
     Specification of the desired behavior of the pod...
   status    &lt;Object&gt;
     Most recently observed status of the pod. This data may not be up to
     date...
Kubectl prints out the explanation of the object and lists the attributes the object
can contain. You can then drill deeper to find out more about each attribute. For
example, you can examine the spec attribute like this:
$ kubectl explain pod.spec
RESOURCE: spec &lt;Object&gt;
DESCRIPTION:
    Specification of the desired behavior of the pod...
    podSpec is a description of a pod.
FIELDS:
   hostPID   &lt;boolean&gt;
     Use the host's pid namespace. Optional: Default to false.
   ...
   volumes   &lt;[]Object&gt;
     List of volumes that can be mounted by containers belonging to the
     pod.
 
</data>
  <data key="d5">64
CHAPTER 3
Pods: running containers in Kubernetes
tainer is accepting connections through a port bound to the 0.0.0.0 address, other
pods can always connect to it, even if the port isn’t listed in the pod spec explicitly. But
it makes sense to define the ports explicitly so that everyone using your cluster can
quickly see what ports each pod exposes. Explicitly defining ports also allows you to
assign a name to each port, which can come in handy, as you’ll see later in the book.
Using kubectl explain to discover possible API object fields
When preparing a manifest, you can either turn to the Kubernetes reference
documentation at http:/
/kubernetes.io/docs/api to see which attributes are
supported by each API object, or you can use the kubectl explain command.
For example, when creating a pod manifest from scratch, you can start by asking
kubectl to explain pods:
$ kubectl explain pods
DESCRIPTION:
Pod is a collection of containers that can run on a host. This resource 
is created by clients and scheduled onto hosts.
FIELDS:
   kind      &lt;string&gt;
     Kind is a string value representing the REST resource this object
     represents...
   metadata  &lt;Object&gt;
     Standard object's metadata...
   spec      &lt;Object&gt;
     Specification of the desired behavior of the pod...
   status    &lt;Object&gt;
     Most recently observed status of the pod. This data may not be up to
     date...
Kubectl prints out the explanation of the object and lists the attributes the object
can contain. You can then drill deeper to find out more about each attribute. For
example, you can examine the spec attribute like this:
$ kubectl explain pod.spec
RESOURCE: spec &lt;Object&gt;
DESCRIPTION:
    Specification of the desired behavior of the pod...
    podSpec is a description of a pod.
FIELDS:
   hostPID   &lt;boolean&gt;
     Use the host's pid namespace. Optional: Default to false.
   ...
   volumes   &lt;[]Object&gt;
     List of volumes that can be mounted by containers belonging to the
     pod.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="97">
  <data key="d0">Page_97</data>
  <data key="d5">Page_97</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_8">
  <data key="d0">65
Creating pods from YAML or JSON descriptors
3.2.3
Using kubectl create to create the pod
To create the pod from your YAML file, use the kubectl create command:
$ kubectl create -f kubia-manual.yaml
pod "kubia-manual" created
The kubectl create -f command is used for creating any resource (not only pods)
from a YAML or JSON file. 
RETRIEVING THE WHOLE DEFINITION OF A RUNNING POD
After creating the pod, you can ask Kubernetes for the full YAML of the pod. You’ll
see it’s similar to the YAML you saw earlier. You’ll learn about the additional fields
appearing in the returned definition in the next sections. Go ahead and use the fol-
lowing command to see the full descriptor of the pod:
$ kubectl get po kubia-manual -o yaml
If you’re more into JSON, you can also tell kubectl to return JSON instead of YAML
like this (this works even if you used YAML to create the pod):
$ kubectl get po kubia-manual -o json
SEEING YOUR NEWLY CREATED POD IN THE LIST OF PODS
Your pod has been created, but how do you know if it’s running? Let’s list pods to see
their statuses:
$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
kubia-manual    1/1     Running   0          32s
kubia-zxzij     1/1     Running   0          1d    
There’s your kubia-manual pod. Its status shows that it’s running. If you’re like me,
you’ll probably want to confirm that’s true by talking to the pod. You’ll do that in a
minute. First, you’ll look at the app’s log to check for any errors.
3.2.4
Viewing application logs
Your little Node.js application logs to the process’s standard output. Containerized
applications usually log to the standard output and standard error stream instead of
   Containers  &lt;[]Object&gt; -required-
     List of containers belonging to the pod. Containers cannot currently
     Be added or removed. There must be at least one container in a pod.
     Cannot be updated. More info:
     http://releases.k8s.io/release-1.4/docs/user-guide/containers.md
 
</data>
  <data key="d5">65
Creating pods from YAML or JSON descriptors
3.2.3
Using kubectl create to create the pod
To create the pod from your YAML file, use the kubectl create command:
$ kubectl create -f kubia-manual.yaml
pod "kubia-manual" created
The kubectl create -f command is used for creating any resource (not only pods)
from a YAML or JSON file. 
RETRIEVING THE WHOLE DEFINITION OF A RUNNING POD
After creating the pod, you can ask Kubernetes for the full YAML of the pod. You’ll
see it’s similar to the YAML you saw earlier. You’ll learn about the additional fields
appearing in the returned definition in the next sections. Go ahead and use the fol-
lowing command to see the full descriptor of the pod:
$ kubectl get po kubia-manual -o yaml
If you’re more into JSON, you can also tell kubectl to return JSON instead of YAML
like this (this works even if you used YAML to create the pod):
$ kubectl get po kubia-manual -o json
SEEING YOUR NEWLY CREATED POD IN THE LIST OF PODS
Your pod has been created, but how do you know if it’s running? Let’s list pods to see
their statuses:
$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
kubia-manual    1/1     Running   0          32s
kubia-zxzij     1/1     Running   0          1d    
There’s your kubia-manual pod. Its status shows that it’s running. If you’re like me,
you’ll probably want to confirm that’s true by talking to the pod. You’ll do that in a
minute. First, you’ll look at the app’s log to check for any errors.
3.2.4
Viewing application logs
Your little Node.js application logs to the process’s standard output. Containerized
applications usually log to the standard output and standard error stream instead of
   Containers  &lt;[]Object&gt; -required-
     List of containers belonging to the pod. Containers cannot currently
     Be added or removed. There must be at least one container in a pod.
     Cannot be updated. More info:
     http://releases.k8s.io/release-1.4/docs/user-guide/containers.md
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="98">
  <data key="d0">Page_98</data>
  <data key="d5">Page_98</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_9">
  <data key="d0">66
CHAPTER 3
Pods: running containers in Kubernetes
writing their logs to files. This is to allow users to view logs of different applications in
a simple, standard way. 
 The container runtime (Docker in your case) redirects those streams to files and
allows you to get the container’s log by running
$ docker logs &lt;container id&gt;
You could use ssh to log into the node where your pod is running and retrieve its logs
with docker logs, but Kubernetes provides an easier way. 
RETRIEVING A POD’S LOG WITH KUBECTL LOGS
To see your pod’s log (more precisely, the container’s log) you run the following com-
mand on your local machine (no need to ssh anywhere):
$ kubectl logs kubia-manual
Kubia server starting...
You haven’t sent any web requests to your Node.js app, so the log only shows a single
log statement about the server starting up. As you can see, retrieving logs of an appli-
cation running in Kubernetes is incredibly simple if the pod only contains a single
container. 
NOTE
Container logs are automatically rotated daily and every time the log file
reaches 10MB in size. The kubectl logs command only shows the log entries
from the last rotation.
SPECIFYING THE CONTAINER NAME WHEN GETTING LOGS OF A MULTI-CONTAINER POD
If your pod includes multiple containers, you have to explicitly specify the container
name by including the -c &lt;container name&gt; option when running kubectl logs. In
your kubia-manual pod, you set the container’s name to kubia, so if additional con-
tainers exist in the pod, you’d have to get its logs like this:
$ kubectl logs kubia-manual -c kubia
Kubia server starting...
Note that you can only retrieve container logs of pods that are still in existence. When
a pod is deleted, its logs are also deleted. To make a pod’s logs available even after the
pod is deleted, you need to set up centralized, cluster-wide logging, which stores all
the logs into a central store. Chapter 17 explains how centralized logging works.
3.2.5
Sending requests to the pod
The pod is now running—at least that’s what kubectl get and your app’s log say. But
how do you see it in action? In the previous chapter, you used the kubectl expose
command to create a service to gain access to the pod externally. You’re not going to
do that now, because a whole chapter is dedicated to services, and you have other ways
of connecting to a pod for testing and debugging purposes. One of them is through
port forwarding.
 
</data>
  <data key="d5">66
CHAPTER 3
Pods: running containers in Kubernetes
writing their logs to files. This is to allow users to view logs of different applications in
a simple, standard way. 
 The container runtime (Docker in your case) redirects those streams to files and
allows you to get the container’s log by running
$ docker logs &lt;container id&gt;
You could use ssh to log into the node where your pod is running and retrieve its logs
with docker logs, but Kubernetes provides an easier way. 
RETRIEVING A POD’S LOG WITH KUBECTL LOGS
To see your pod’s log (more precisely, the container’s log) you run the following com-
mand on your local machine (no need to ssh anywhere):
$ kubectl logs kubia-manual
Kubia server starting...
You haven’t sent any web requests to your Node.js app, so the log only shows a single
log statement about the server starting up. As you can see, retrieving logs of an appli-
cation running in Kubernetes is incredibly simple if the pod only contains a single
container. 
NOTE
Container logs are automatically rotated daily and every time the log file
reaches 10MB in size. The kubectl logs command only shows the log entries
from the last rotation.
SPECIFYING THE CONTAINER NAME WHEN GETTING LOGS OF A MULTI-CONTAINER POD
If your pod includes multiple containers, you have to explicitly specify the container
name by including the -c &lt;container name&gt; option when running kubectl logs. In
your kubia-manual pod, you set the container’s name to kubia, so if additional con-
tainers exist in the pod, you’d have to get its logs like this:
$ kubectl logs kubia-manual -c kubia
Kubia server starting...
Note that you can only retrieve container logs of pods that are still in existence. When
a pod is deleted, its logs are also deleted. To make a pod’s logs available even after the
pod is deleted, you need to set up centralized, cluster-wide logging, which stores all
the logs into a central store. Chapter 17 explains how centralized logging works.
3.2.5
Sending requests to the pod
The pod is now running—at least that’s what kubectl get and your app’s log say. But
how do you see it in action? In the previous chapter, you used the kubectl expose
command to create a service to gain access to the pod externally. You’re not going to
do that now, because a whole chapter is dedicated to services, and you have other ways
of connecting to a pod for testing and debugging purposes. One of them is through
port forwarding.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="99">
  <data key="d0">Page_99</data>
  <data key="d5">Page_99</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_10">
  <data key="d0">67
Organizing pods with labels
FORWARDING A LOCAL NETWORK PORT TO A PORT IN THE POD
When you want to talk to a specific pod without going through a service (for debug-
ging or other reasons), Kubernetes allows you to configure port forwarding to the
pod. This is done through the kubectl port-forward command. The following
command will forward your machine’s local port 8888 to port 8080 of your kubia-
manual pod:
$ kubectl port-forward kubia-manual 8888:8080
... Forwarding from 127.0.0.1:8888 -&gt; 8080
... Forwarding from [::1]:8888 -&gt; 8080
The port forwarder is running and you can now connect to your pod through the
local port. 
CONNECTING TO THE POD THROUGH THE PORT FORWARDER
In a different terminal, you can now use curl to send an HTTP request to your pod
through the kubectl port-forward proxy running on localhost:8888:
$ curl localhost:8888
You’ve hit kubia-manual
Figure 3.5 shows an overly simplified view of what happens when you send the request.
In reality, a couple of additional components sit between the kubectl process and the
pod, but they aren’t relevant right now.
Using port forwarding like this is an effective way to test an individual pod. You’ll
learn about other similar methods throughout the book. 
3.3
Organizing pods with labels
At this point, you have two pods running in your cluster. When deploying actual
applications, most users will end up running many more pods. As the number of
pods increases, the need for categorizing them into subsets becomes more and
more evident.
 For example, with microservices architectures, the number of deployed microser-
vices can easily exceed 20 or more. Those components will probably be replicated
Kubernetes cluster
Port
8080
Local machine
kubectl
port-forward
process
curl
Port
8888
Pod:
kubia-manual
Figure 3.5
A simplified view of what happens when you use curl with kubectl port-forward
 
</data>
  <data key="d5">67
Organizing pods with labels
FORWARDING A LOCAL NETWORK PORT TO A PORT IN THE POD
When you want to talk to a specific pod without going through a service (for debug-
ging or other reasons), Kubernetes allows you to configure port forwarding to the
pod. This is done through the kubectl port-forward command. The following
command will forward your machine’s local port 8888 to port 8080 of your kubia-
manual pod:
$ kubectl port-forward kubia-manual 8888:8080
... Forwarding from 127.0.0.1:8888 -&gt; 8080
... Forwarding from [::1]:8888 -&gt; 8080
The port forwarder is running and you can now connect to your pod through the
local port. 
CONNECTING TO THE POD THROUGH THE PORT FORWARDER
In a different terminal, you can now use curl to send an HTTP request to your pod
through the kubectl port-forward proxy running on localhost:8888:
$ curl localhost:8888
You’ve hit kubia-manual
Figure 3.5 shows an overly simplified view of what happens when you send the request.
In reality, a couple of additional components sit between the kubectl process and the
pod, but they aren’t relevant right now.
Using port forwarding like this is an effective way to test an individual pod. You’ll
learn about other similar methods throughout the book. 
3.3
Organizing pods with labels
At this point, you have two pods running in your cluster. When deploying actual
applications, most users will end up running many more pods. As the number of
pods increases, the need for categorizing them into subsets becomes more and
more evident.
 For example, with microservices architectures, the number of deployed microser-
vices can easily exceed 20 or more. Those components will probably be replicated
Kubernetes cluster
Port
8080
Local machine
kubectl
port-forward
process
curl
Port
8888
Pod:
kubia-manual
Figure 3.5
A simplified view of what happens when you use curl with kubectl port-forward
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="100">
  <data key="d0">Page_100</data>
  <data key="d5">100</data>
  <data key="d1">cyan</data>
  <data key="d2">the value of an integer variable stored in the ConfigMap</data>
  <data key="d3">integer</data>
  <data key="d4">10</data>
</node>
<node id="text_11">
  <data key="d0">68
CHAPTER 3
Pods: running containers in Kubernetes
(multiple copies of the same component will be deployed) and multiple versions or
releases (stable, beta, canary, and so on) will run concurrently. This can lead to hun-
dreds of pods in the system. Without a mechanism for organizing them, you end up
with a big, incomprehensible mess, such as the one shown in figure 3.6. The figure
shows pods of multiple microservices, with several running multiple replicas, and others
running different releases of the same microservice.
It’s evident you need a way of organizing them into smaller groups based on arbitrary
criteria, so every developer and system administrator dealing with your system can eas-
ily see which pod is which. And you’ll want to operate on every pod belonging to a cer-
tain group with a single action instead of having to perform the action for each pod
individually. 
 Organizing pods and all other Kubernetes objects is done through labels.
3.3.1
Introducing labels
Labels are a simple, yet incredibly powerful, Kubernetes feature for organizing not
only pods, but all other Kubernetes resources. A label is an arbitrary key-value pair you
attach to a resource, which is then utilized when selecting resources using label selectors
(resources are filtered based on whether they include the label specified in the selec-
tor). A resource can have more than one label, as long as the keys of those labels are
unique within that resource. You usually attach labels to resources when you create
them, but you can also add additional labels or even modify the values of existing
labels later without having to recreate the resource. 
UI pod
UI pod
UI pod
Account
Service
pod
Product
Catalog
pod
Product
Catalog
pod
Product
Catalog
pod
Shopping
Cart
pod
Shopping
Cart
pod
Order
Service
pod
UI pod
UI pod
Product
Catalog
pod
Product
Catalog
pod
Order
Service
pod
Account
Service
pod
Product
Catalog
pod
Product
Catalog
pod
Order
Service
pod
Figure 3.6
Uncategorized pods in a microservices architecture
 
</data>
  <data key="d5">68
CHAPTER 3
Pods: running containers in Kubernetes
(multiple copies of the same component will be deployed) and multiple versions or
releases (stable, beta, canary, and so on) will run concurrently. This can lead to hun-
dreds of pods in the system. Without a mechanism for organizing them, you end up
with a big, incomprehensible mess, such as the one shown in figure 3.6. The figure
shows pods of multiple microservices, with several running multiple replicas, and others
running different releases of the same microservice.
It’s evident you need a way of organizing them into smaller groups based on arbitrary
criteria, so every developer and system administrator dealing with your system can eas-
ily see which pod is which. And you’ll want to operate on every pod belonging to a cer-
tain group with a single action instead of having to perform the action for each pod
individually. 
 Organizing pods and all other Kubernetes objects is done through labels.
3.3.1
Introducing labels
Labels are a simple, yet incredibly powerful, Kubernetes feature for organizing not
only pods, but all other Kubernetes resources. A label is an arbitrary key-value pair you
attach to a resource, which is then utilized when selecting resources using label selectors
(resources are filtered based on whether they include the label specified in the selec-
tor). A resource can have more than one label, as long as the keys of those labels are
unique within that resource. You usually attach labels to resources when you create
them, but you can also add additional labels or even modify the values of existing
labels later without having to recreate the resource. 
UI pod
UI pod
UI pod
Account
Service
pod
Product
Catalog
pod
Product
Catalog
pod
Product
Catalog
pod
Shopping
Cart
pod
Shopping
Cart
pod
Order
Service
pod
UI pod
UI pod
Product
Catalog
pod
Product
Catalog
pod
Order
Service
pod
Account
Service
pod
Product
Catalog
pod
Product
Catalog
pod
Order
Service
pod
Figure 3.6
Uncategorized pods in a microservices architecture
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="101">
  <data key="d0">Page_101</data>
  <data key="d5">Page_101</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_12">
  <data key="d0">69
Organizing pods with labels
 Let’s turn back to the microservices example from figure 3.6. By adding labels to
those pods, you get a much-better-organized system that everyone can easily make
sense of. Each pod is labeled with two labels:

app, which specifies which app, component, or microservice the pod belongs to. 

rel, which shows whether the application running in the pod is a stable, beta,
or a canary release.
DEFINITION
A canary release is when you deploy a new version of an applica-
tion next to the stable version, and only let a small fraction of users hit the
new version to see how it behaves before rolling it out to all users. This pre-
vents bad releases from being exposed to too many users.
By adding these two labels, you’ve essentially organized your pods into two dimen-
sions (horizontally by app and vertically by release), as shown in figure 3.7.
Every developer or ops person with access to your cluster can now easily see the sys-
tem’s structure and where each pod fits in by looking at the pod’s labels.
3.3.2
Specifying labels when creating a pod
Now, you’ll see labels in action by creating a new pod with two labels. Create a new file
called kubia-manual-with-labels.yaml with the contents of the following listing.
apiVersion: v1                                         
kind: Pod                                              
metadata:                                              
  name: kubia-manual-v2
Listing 3.3
A pod with labels: kubia-manual-with-labels.yaml
UI pod
app: ui
rel: stable
rel=stable
app=ui
Account
Service
pod
app: as
rel: stable
app=as
app: pc
rel: stable
app=pc
app: sc
rel: stable
app=sc
app: os
rel: stable
app=os
Product
Catalog
pod
Shopping
Cart
pod
Order
Service
pod
UI pod
app: ui
rel: beta
rel=beta
app: pc
rel: beta
app: os
rel: beta
Product
Catalog
pod
Order
Service
pod
rel=canary
Account
Service
pod
app: as
rel: canary
app: pc
rel: canary
app: os
rel: canary
Product
Catalog
pod
Order
Service
pod
Figure 3.7
Organizing pods in a microservices architecture with pod labels
 
</data>
  <data key="d5">69
Organizing pods with labels
 Let’s turn back to the microservices example from figure 3.6. By adding labels to
those pods, you get a much-better-organized system that everyone can easily make
sense of. Each pod is labeled with two labels:

app, which specifies which app, component, or microservice the pod belongs to. 

rel, which shows whether the application running in the pod is a stable, beta,
or a canary release.
DEFINITION
A canary release is when you deploy a new version of an applica-
tion next to the stable version, and only let a small fraction of users hit the
new version to see how it behaves before rolling it out to all users. This pre-
vents bad releases from being exposed to too many users.
By adding these two labels, you’ve essentially organized your pods into two dimen-
sions (horizontally by app and vertically by release), as shown in figure 3.7.
Every developer or ops person with access to your cluster can now easily see the sys-
tem’s structure and where each pod fits in by looking at the pod’s labels.
3.3.2
Specifying labels when creating a pod
Now, you’ll see labels in action by creating a new pod with two labels. Create a new file
called kubia-manual-with-labels.yaml with the contents of the following listing.
apiVersion: v1                                         
kind: Pod                                              
metadata:                                              
  name: kubia-manual-v2
Listing 3.3
A pod with labels: kubia-manual-with-labels.yaml
UI pod
app: ui
rel: stable
rel=stable
app=ui
Account
Service
pod
app: as
rel: stable
app=as
app: pc
rel: stable
app=pc
app: sc
rel: stable
app=sc
app: os
rel: stable
app=os
Product
Catalog
pod
Shopping
Cart
pod
Order
Service
pod
UI pod
app: ui
rel: beta
rel=beta
app: pc
rel: beta
app: os
rel: beta
Product
Catalog
pod
Order
Service
pod
rel=canary
Account
Service
pod
app: as
rel: canary
app: pc
rel: canary
app: os
rel: canary
Product
Catalog
pod
Order
Service
pod
Figure 3.7
Organizing pods in a microservices architecture with pod labels
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="102">
  <data key="d0">Page_102</data>
  <data key="d5">Page_102</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_13">
  <data key="d0">70
CHAPTER 3
Pods: running containers in Kubernetes
  labels:    
    creation_method: manual          
    env: prod                        
spec: 
  containers: 
  - image: luksa/kubia
    name: kubia
    ports: 
    - containerPort: 8080
      protocol: TCP
You’ve included the labels creation_method=manual and env=data.labels section.
You’ll create this pod now:
$ kubectl create -f kubia-manual-with-labels.yaml
pod "kubia-manual-v2" created
The kubectl get pods command doesn’t list any labels by default, but you can see
them by using the --show-labels switch:
$ kubectl get po --show-labels
NAME            READY  STATUS   RESTARTS  AGE LABELS
kubia-manual    1/1    Running  0         16m &lt;none&gt;
kubia-manual-v2 1/1    Running  0         2m  creat_method=manual,env=prod
kubia-zxzij     1/1    Running  0         1d  run=kubia
Instead of listing all labels, if you’re only interested in certain labels, you can specify
them with the -L switch and have each displayed in its own column. List pods again
and show the columns for the two labels you’ve attached to your kubia-manual-v2 pod:
$ kubectl get po -L creation_method,env
NAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV
kubia-manual    1/1     Running   0          16m   &lt;none&gt;            &lt;none&gt;
kubia-manual-v2 1/1     Running   0          2m    manual            prod
kubia-zxzij     1/1     Running   0          1d    &lt;none&gt;            &lt;none&gt;
3.3.3
Modifying labels of existing pods
Labels can also be added to and modified on existing pods. Because the kubia-man-
ual pod was also created manually, let’s add the creation_method=manual label to it: 
$ kubectl label po kubia-manual creation_method=manual
pod "kubia-manual" labeled
Now, let’s also change the env=prod label to env=debug on the kubia-manual-v2 pod,
to see how existing labels can be changed.
NOTE
You need to use the --overwrite option when changing existing labels.
$ kubectl label po kubia-manual-v2 env=debug --overwrite
pod "kubia-manual-v2" labeled
Two labels are 
attached to the pod.
 
</data>
  <data key="d5">70
CHAPTER 3
Pods: running containers in Kubernetes
  labels:    
    creation_method: manual          
    env: prod                        
spec: 
  containers: 
  - image: luksa/kubia
    name: kubia
    ports: 
    - containerPort: 8080
      protocol: TCP
You’ve included the labels creation_method=manual and env=data.labels section.
You’ll create this pod now:
$ kubectl create -f kubia-manual-with-labels.yaml
pod "kubia-manual-v2" created
The kubectl get pods command doesn’t list any labels by default, but you can see
them by using the --show-labels switch:
$ kubectl get po --show-labels
NAME            READY  STATUS   RESTARTS  AGE LABELS
kubia-manual    1/1    Running  0         16m &lt;none&gt;
kubia-manual-v2 1/1    Running  0         2m  creat_method=manual,env=prod
kubia-zxzij     1/1    Running  0         1d  run=kubia
Instead of listing all labels, if you’re only interested in certain labels, you can specify
them with the -L switch and have each displayed in its own column. List pods again
and show the columns for the two labels you’ve attached to your kubia-manual-v2 pod:
$ kubectl get po -L creation_method,env
NAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV
kubia-manual    1/1     Running   0          16m   &lt;none&gt;            &lt;none&gt;
kubia-manual-v2 1/1     Running   0          2m    manual            prod
kubia-zxzij     1/1     Running   0          1d    &lt;none&gt;            &lt;none&gt;
3.3.3
Modifying labels of existing pods
Labels can also be added to and modified on existing pods. Because the kubia-man-
ual pod was also created manually, let’s add the creation_method=manual label to it: 
$ kubectl label po kubia-manual creation_method=manual
pod "kubia-manual" labeled
Now, let’s also change the env=prod label to env=debug on the kubia-manual-v2 pod,
to see how existing labels can be changed.
NOTE
You need to use the --overwrite option when changing existing labels.
$ kubectl label po kubia-manual-v2 env=debug --overwrite
pod "kubia-manual-v2" labeled
Two labels are 
attached to the pod.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="103">
  <data key="d0">Page_103</data>
  <data key="d5">Page_103</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_14">
  <data key="d0">71
Listing subsets of pods through label selectors
List the pods again to see the updated labels:
$ kubectl get po -L creation_method,env
NAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV
kubia-manual    1/1     Running   0          16m   manual            &lt;none&gt;
kubia-manual-v2 1/1     Running   0          2m    manual            debug
kubia-zxzij     1/1     Running   0          1d    &lt;none&gt;            &lt;none&gt;
As you can see, attaching labels to resources is trivial, and so is changing them on
existing resources. It may not be evident right now, but this is an incredibly powerful
feature, as you’ll see in the next chapter. But first, let’s see what you can do with these
labels, in addition to displaying them when listing pods.
3.4
Listing subsets of pods through label selectors
Attaching labels to resources so you can see the labels next to each resource when list-
ing them isn’t that interesting. But labels go hand in hand with label selectors. Label
selectors allow you to select a subset of pods tagged with certain labels and perform an
operation on those pods. A label selector is a criterion, which filters resources based
on whether they include a certain label with a certain value. 
 A label selector can select resources based on whether the resource
Contains (or doesn’t contain) a label with a certain key
Contains a label with a certain key and value
Contains a label with a certain key, but with a value not equal to the one you
specify
3.4.1
Listing pods using a label selector
Let’s use label selectors on the pods you’ve created so far. To see all pods you created
manually (you labeled them with creation_method=manual), do the following:
$ kubectl get po -l creation_method=manual
NAME              READY     STATUS    RESTARTS   AGE
kubia-manual      1/1       Running   0          51m
kubia-manual-v2   1/1       Running   0          37m
To list all pods that include the env label, whatever its value is:
$ kubectl get po -l env
NAME              READY     STATUS    RESTARTS   AGE
kubia-manual-v2   1/1       Running   0          37m
And those that don’t have the env label:
$ kubectl get po -l '!env'
NAME           READY     STATUS    RESTARTS   AGE
kubia-manual   1/1       Running   0          51m
kubia-zxzij    1/1       Running   0          10d
 
</data>
  <data key="d5">71
Listing subsets of pods through label selectors
List the pods again to see the updated labels:
$ kubectl get po -L creation_method,env
NAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV
kubia-manual    1/1     Running   0          16m   manual            &lt;none&gt;
kubia-manual-v2 1/1     Running   0          2m    manual            debug
kubia-zxzij     1/1     Running   0          1d    &lt;none&gt;            &lt;none&gt;
As you can see, attaching labels to resources is trivial, and so is changing them on
existing resources. It may not be evident right now, but this is an incredibly powerful
feature, as you’ll see in the next chapter. But first, let’s see what you can do with these
labels, in addition to displaying them when listing pods.
3.4
Listing subsets of pods through label selectors
Attaching labels to resources so you can see the labels next to each resource when list-
ing them isn’t that interesting. But labels go hand in hand with label selectors. Label
selectors allow you to select a subset of pods tagged with certain labels and perform an
operation on those pods. A label selector is a criterion, which filters resources based
on whether they include a certain label with a certain value. 
 A label selector can select resources based on whether the resource
Contains (or doesn’t contain) a label with a certain key
Contains a label with a certain key and value
Contains a label with a certain key, but with a value not equal to the one you
specify
3.4.1
Listing pods using a label selector
Let’s use label selectors on the pods you’ve created so far. To see all pods you created
manually (you labeled them with creation_method=manual), do the following:
$ kubectl get po -l creation_method=manual
NAME              READY     STATUS    RESTARTS   AGE
kubia-manual      1/1       Running   0          51m
kubia-manual-v2   1/1       Running   0          37m
To list all pods that include the env label, whatever its value is:
$ kubectl get po -l env
NAME              READY     STATUS    RESTARTS   AGE
kubia-manual-v2   1/1       Running   0          37m
And those that don’t have the env label:
$ kubectl get po -l '!env'
NAME           READY     STATUS    RESTARTS   AGE
kubia-manual   1/1       Running   0          51m
kubia-zxzij    1/1       Running   0          10d
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="104">
  <data key="d0">Page_104</data>
  <data key="d5">Page_104</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_15">
  <data key="d0">72
CHAPTER 3
Pods: running containers in Kubernetes
NOTE
Make sure to use single quotes around !env, so the bash shell doesn’t
evaluate the exclamation mark.
Similarly, you could also match pods with the following label selectors:

creation_method!=manual to select pods with the creation_method label with
any value other than manual

env in (prod,devel) to select pods with the env label set to either prod or
devel

env notin (prod,devel) to select pods with the env label set to any value other
than prod or devel
Turning back to the pods in the microservices-oriented architecture example, you
could select all pods that are part of the product catalog microservice by using the
app=pc label selector (shown in the following figure).
3.4.2
Using multiple conditions in a label selector
A selector can also include multiple comma-separated criteria. Resources need to
match all of them to match the selector. If, for example, you want to select only pods
running the beta release of the product catalog microservice, you’d use the following
selector: app=pc,rel=beta (visualized in figure 3.9).
 Label selectors aren’t useful only for listing pods, but also for performing actions
on a subset of all pods. For example, later in the chapter, you’ll see how to use label
selectors to delete multiple pods at once. But label selectors aren’t used only by
kubectl. They’re also used internally, as you’ll see next.
UI pod
app: ui
rel: stable
rel=stable
app=ui
Account
Service
pod
app: as
rel: stable
app=as
app: pc
rel: stable
app=pc
app: sc
rel: stable
app=sc
app: os
rel: stable
app=os
Product
Catalog
pod
Shopping
Cart
pod
Order
Service
pod
UI pod
app: ui
rel: beta
rel=beta
app: pc
rel: beta
app: os
rel: beta
Product
Catalog
pod
Order
Service
pod
rel=canary
Account
Service
pod
app: as
rel: canary
app: pc
rel: canary
app: os
rel: canary
Product
Catalog
pod
Order
Service
pod
Figure 3.8
Selecting the product catalog microservice pods using the “app=pc” label selector
 
</data>
  <data key="d5">72
CHAPTER 3
Pods: running containers in Kubernetes
NOTE
Make sure to use single quotes around !env, so the bash shell doesn’t
evaluate the exclamation mark.
Similarly, you could also match pods with the following label selectors:

creation_method!=manual to select pods with the creation_method label with
any value other than manual

env in (prod,devel) to select pods with the env label set to either prod or
devel

env notin (prod,devel) to select pods with the env label set to any value other
than prod or devel
Turning back to the pods in the microservices-oriented architecture example, you
could select all pods that are part of the product catalog microservice by using the
app=pc label selector (shown in the following figure).
3.4.2
Using multiple conditions in a label selector
A selector can also include multiple comma-separated criteria. Resources need to
match all of them to match the selector. If, for example, you want to select only pods
running the beta release of the product catalog microservice, you’d use the following
selector: app=pc,rel=beta (visualized in figure 3.9).
 Label selectors aren’t useful only for listing pods, but also for performing actions
on a subset of all pods. For example, later in the chapter, you’ll see how to use label
selectors to delete multiple pods at once. But label selectors aren’t used only by
kubectl. They’re also used internally, as you’ll see next.
UI pod
app: ui
rel: stable
rel=stable
app=ui
Account
Service
pod
app: as
rel: stable
app=as
app: pc
rel: stable
app=pc
app: sc
rel: stable
app=sc
app: os
rel: stable
app=os
Product
Catalog
pod
Shopping
Cart
pod
Order
Service
pod
UI pod
app: ui
rel: beta
rel=beta
app: pc
rel: beta
app: os
rel: beta
Product
Catalog
pod
Order
Service
pod
rel=canary
Account
Service
pod
app: as
rel: canary
app: pc
rel: canary
app: os
rel: canary
Product
Catalog
pod
Order
Service
pod
Figure 3.8
Selecting the product catalog microservice pods using the “app=pc” label selector
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="105">
  <data key="d0">Page_105</data>
  <data key="d5">Page_105</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_16">
  <data key="d0">73
Using labels and selectors to constrain pod scheduling
3.5
Using labels and selectors to constrain pod scheduling
All the pods you’ve created so far have been scheduled pretty much randomly across
your worker nodes. As I’ve mentioned in the previous chapter, this is the proper way
of working in a Kubernetes cluster. Because Kubernetes exposes all the nodes in the
cluster as a single, large deployment platform, it shouldn’t matter to you what node a
pod is scheduled to. Because each pod gets the exact amount of computational
resources it requests (CPU, memory, and so on) and its accessibility from other pods
isn’t at all affected by the node the pod is scheduled to, usually there shouldn’t be any
need for you to tell Kubernetes exactly where to schedule your pods. 
 Certain cases exist, however, where you’ll want to have at least a little say in where
a pod should be scheduled. A good example is when your hardware infrastructure
isn’t homogenous. If part of your worker nodes have spinning hard drives, whereas
others have SSDs, you may want to schedule certain pods to one group of nodes and
the rest to the other. Another example is when you need to schedule pods perform-
ing intensive GPU-based computation only to nodes that provide the required GPU
acceleration. 
 You never want to say specifically what node a pod should be scheduled to, because
that would couple the application to the infrastructure, whereas the whole idea of
Kubernetes is hiding the actual infrastructure from the apps that run on it. But if you
want to have a say in where a pod should be scheduled, instead of specifying an exact
node, you should describe the node requirements and then let Kubernetes select a
node that matches those requirements. This can be done through node labels and
node label selectors. 
UI pod
app: ui
rel: stable
rel=stable
app=ui
Account
Service
pod
app: as
rel: stable
app=as
app: pc
rel: stable
app=pc
app: sc
rel: stable
app=sc
app: os
rel: stable
app=os
Product
Catalog
pod
Shopping
Cart
pod
Order
Service
pod
UI pod
app: ui
rel: beta
rel=beta
app: pc
rel: beta
app: os
rel: beta
Product
Catalog
pod
Order
Service
pod
rel=canary
Account
Service
pod
app: as
rel: canary
app: pc
rel: canary
app: os
rel: canary
Product
Catalog
pod
Order
Service
pod
Figure 3.9
Selecting pods with multiple label selectors
 
</data>
  <data key="d5">73
Using labels and selectors to constrain pod scheduling
3.5
Using labels and selectors to constrain pod scheduling
All the pods you’ve created so far have been scheduled pretty much randomly across
your worker nodes. As I’ve mentioned in the previous chapter, this is the proper way
of working in a Kubernetes cluster. Because Kubernetes exposes all the nodes in the
cluster as a single, large deployment platform, it shouldn’t matter to you what node a
pod is scheduled to. Because each pod gets the exact amount of computational
resources it requests (CPU, memory, and so on) and its accessibility from other pods
isn’t at all affected by the node the pod is scheduled to, usually there shouldn’t be any
need for you to tell Kubernetes exactly where to schedule your pods. 
 Certain cases exist, however, where you’ll want to have at least a little say in where
a pod should be scheduled. A good example is when your hardware infrastructure
isn’t homogenous. If part of your worker nodes have spinning hard drives, whereas
others have SSDs, you may want to schedule certain pods to one group of nodes and
the rest to the other. Another example is when you need to schedule pods perform-
ing intensive GPU-based computation only to nodes that provide the required GPU
acceleration. 
 You never want to say specifically what node a pod should be scheduled to, because
that would couple the application to the infrastructure, whereas the whole idea of
Kubernetes is hiding the actual infrastructure from the apps that run on it. But if you
want to have a say in where a pod should be scheduled, instead of specifying an exact
node, you should describe the node requirements and then let Kubernetes select a
node that matches those requirements. This can be done through node labels and
node label selectors. 
UI pod
app: ui
rel: stable
rel=stable
app=ui
Account
Service
pod
app: as
rel: stable
app=as
app: pc
rel: stable
app=pc
app: sc
rel: stable
app=sc
app: os
rel: stable
app=os
Product
Catalog
pod
Shopping
Cart
pod
Order
Service
pod
UI pod
app: ui
rel: beta
rel=beta
app: pc
rel: beta
app: os
rel: beta
Product
Catalog
pod
Order
Service
pod
rel=canary
Account
Service
pod
app: as
rel: canary
app: pc
rel: canary
app: os
rel: canary
Product
Catalog
pod
Order
Service
pod
Figure 3.9
Selecting pods with multiple label selectors
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="106">
  <data key="d0">Page_106</data>
  <data key="d5">Page_106</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_17">
  <data key="d0">74
CHAPTER 3
Pods: running containers in Kubernetes
3.5.1
Using labels for categorizing worker nodes
As you learned earlier, pods aren’t the only Kubernetes resource type that you can
attach a label to. Labels can be attached to any Kubernetes object, including nodes.
Usually, when the ops team adds a new node to the cluster, they’ll categorize the node
by attaching labels specifying the type of hardware the node provides or anything else
that may come in handy when scheduling pods. 
 Let’s imagine one of the nodes in your cluster contains a GPU meant to be used
for general-purpose GPU computing. You want to add a label to the node showing this
feature. You’re going to add the label gpu=true to one of your nodes (pick one out of
the list returned by kubectl get nodes):
$ kubectl label node gke-kubia-85f6-node-0rrx gpu=true
node "gke-kubia-85f6-node-0rrx" labeled
Now you can use a label selector when listing the nodes, like you did before with pods.
List only nodes that include the label gpu=true:
$ kubectl get nodes -l gpu=true
NAME                      STATUS AGE
gke-kubia-85f6-node-0rrx  Ready  1d
As expected, only one node has this label. You can also try listing all the nodes and tell
kubectl to display an additional column showing the values of each node’s gpu label
(kubectl get nodes -L gpu).
3.5.2
Scheduling pods to specific nodes
Now imagine you want to deploy a new pod that needs a GPU to perform its work.
To ask the scheduler to only choose among the nodes that provide a GPU, you’ll
add a node selector to the pod’s YAML. Create a file called kubia-gpu.yaml with the
following listing’s contents and then use kubectl create -f kubia-gpu.yaml to cre-
ate the pod.
apiVersion: v1                                         
kind: Pod                                              
metadata:                                              
  name: kubia-gpu
spec: 
  nodeSelector:               
    gpu: "true"               
  containers: 
  - image: luksa/kubia
    name: kubia
Listing 3.4
Using a label selector to schedule a pod to a specific node: kubia-gpu.yaml
nodeSelector tells Kubernetes 
to deploy this pod only to 
nodes containing the 
gpu=true label.
 
</data>
  <data key="d5">74
CHAPTER 3
Pods: running containers in Kubernetes
3.5.1
Using labels for categorizing worker nodes
As you learned earlier, pods aren’t the only Kubernetes resource type that you can
attach a label to. Labels can be attached to any Kubernetes object, including nodes.
Usually, when the ops team adds a new node to the cluster, they’ll categorize the node
by attaching labels specifying the type of hardware the node provides or anything else
that may come in handy when scheduling pods. 
 Let’s imagine one of the nodes in your cluster contains a GPU meant to be used
for general-purpose GPU computing. You want to add a label to the node showing this
feature. You’re going to add the label gpu=true to one of your nodes (pick one out of
the list returned by kubectl get nodes):
$ kubectl label node gke-kubia-85f6-node-0rrx gpu=true
node "gke-kubia-85f6-node-0rrx" labeled
Now you can use a label selector when listing the nodes, like you did before with pods.
List only nodes that include the label gpu=true:
$ kubectl get nodes -l gpu=true
NAME                      STATUS AGE
gke-kubia-85f6-node-0rrx  Ready  1d
As expected, only one node has this label. You can also try listing all the nodes and tell
kubectl to display an additional column showing the values of each node’s gpu label
(kubectl get nodes -L gpu).
3.5.2
Scheduling pods to specific nodes
Now imagine you want to deploy a new pod that needs a GPU to perform its work.
To ask the scheduler to only choose among the nodes that provide a GPU, you’ll
add a node selector to the pod’s YAML. Create a file called kubia-gpu.yaml with the
following listing’s contents and then use kubectl create -f kubia-gpu.yaml to cre-
ate the pod.
apiVersion: v1                                         
kind: Pod                                              
metadata:                                              
  name: kubia-gpu
spec: 
  nodeSelector:               
    gpu: "true"               
  containers: 
  - image: luksa/kubia
    name: kubia
Listing 3.4
Using a label selector to schedule a pod to a specific node: kubia-gpu.yaml
nodeSelector tells Kubernetes 
to deploy this pod only to 
nodes containing the 
gpu=true label.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="107">
  <data key="d0">Page_107</data>
  <data key="d5">Page_107</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_18">
  <data key="d0">75
Annotating pods
You’ve added a nodeSelector field under the spec section. When you create the pod,
the scheduler will only choose among the nodes that contain the gpu=true label
(which is only a single node in your case). 
3.5.3
Scheduling to one specific node
Similarly, you could also schedule a pod to an exact node, because each node also has
a unique label with the key kubernetes.io/hostname and value set to the actual host-
name of the node. But setting the nodeSelector to a specific node by the hostname
label may lead to the pod being unschedulable if the node is offline. You shouldn’t
think in terms of individual nodes. Always think about logical groups of nodes that sat-
isfy certain criteria specified through label selectors.
 This was a quick demonstration of how labels and label selectors work and how
they can be used to influence the operation of Kubernetes. The importance and use-
fulness of label selectors will become even more evident when we talk about Replication-
Controllers and Services in the next two chapters. 
NOTE
Additional ways of influencing which node a pod is scheduled to are
covered in chapter 16.
3.6
Annotating pods
In addition to labels, pods and other objects can also contain annotations. Annotations
are also key-value pairs, so in essence, they’re similar to labels, but they aren’t meant to
hold identifying information. They can’t be used to group objects the way labels can.
While objects can be selected through label selectors, there’s no such thing as an
annotation selector. 
 On the other hand, annotations can hold much larger pieces of information and
are primarily meant to be used by tools. Certain annotations are automatically added
to objects by Kubernetes, but others are added by users manually.
 Annotations are also commonly used when introducing new features to Kuberne-
tes. Usually, alpha and beta versions of new features don’t introduce any new fields to
API objects. Annotations are used instead of fields, and then once the required API
changes have become clear and been agreed upon by the Kubernetes developers, new
fields are introduced and the related annotations deprecated.
 A great use of annotations is adding descriptions for each pod or other API object,
so that everyone using the cluster can quickly look up information about each individ-
ual object. For example, an annotation used to specify the name of the person who
created the object can make collaboration between everyone working on the cluster
much easier.
3.6.1
Looking up an object’s annotations
Let’s see an example of an annotation that Kubernetes added automatically to the
pod you created in the previous chapter. To see the annotations, you’ll need to
 
</data>
  <data key="d5">75
Annotating pods
You’ve added a nodeSelector field under the spec section. When you create the pod,
the scheduler will only choose among the nodes that contain the gpu=true label
(which is only a single node in your case). 
3.5.3
Scheduling to one specific node
Similarly, you could also schedule a pod to an exact node, because each node also has
a unique label with the key kubernetes.io/hostname and value set to the actual host-
name of the node. But setting the nodeSelector to a specific node by the hostname
label may lead to the pod being unschedulable if the node is offline. You shouldn’t
think in terms of individual nodes. Always think about logical groups of nodes that sat-
isfy certain criteria specified through label selectors.
 This was a quick demonstration of how labels and label selectors work and how
they can be used to influence the operation of Kubernetes. The importance and use-
fulness of label selectors will become even more evident when we talk about Replication-
Controllers and Services in the next two chapters. 
NOTE
Additional ways of influencing which node a pod is scheduled to are
covered in chapter 16.
3.6
Annotating pods
In addition to labels, pods and other objects can also contain annotations. Annotations
are also key-value pairs, so in essence, they’re similar to labels, but they aren’t meant to
hold identifying information. They can’t be used to group objects the way labels can.
While objects can be selected through label selectors, there’s no such thing as an
annotation selector. 
 On the other hand, annotations can hold much larger pieces of information and
are primarily meant to be used by tools. Certain annotations are automatically added
to objects by Kubernetes, but others are added by users manually.
 Annotations are also commonly used when introducing new features to Kuberne-
tes. Usually, alpha and beta versions of new features don’t introduce any new fields to
API objects. Annotations are used instead of fields, and then once the required API
changes have become clear and been agreed upon by the Kubernetes developers, new
fields are introduced and the related annotations deprecated.
 A great use of annotations is adding descriptions for each pod or other API object,
so that everyone using the cluster can quickly look up information about each individ-
ual object. For example, an annotation used to specify the name of the person who
created the object can make collaboration between everyone working on the cluster
much easier.
3.6.1
Looking up an object’s annotations
Let’s see an example of an annotation that Kubernetes added automatically to the
pod you created in the previous chapter. To see the annotations, you’ll need to
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="108">
  <data key="d0">Page_108</data>
  <data key="d5">Page_108</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_19">
  <data key="d0">76
CHAPTER 3
Pods: running containers in Kubernetes
request the full YAML of the pod or use the kubectl describe command. You’ll use the
first option in the following listing.
$ kubectl get po kubia-zxzij -o yaml
apiVersion: v1
kind: pod
metadata:
  annotations:
    kubernetes.io/created-by: |
      {"kind":"SerializedReference", "apiVersion":"v1", 
      "reference":{"kind":"ReplicationController", "namespace":"default", ...
Without going into too many details, as you can see, the kubernetes.io/created-by
annotation holds JSON data about the object that created the pod. That’s not some-
thing you’d want to put into a label. Labels should be short, whereas annotations can
contain relatively large blobs of data (up to 256 KB in total).
NOTE
The kubernetes.io/created-by annotations was deprecated in ver-
sion 1.8 and will be removed in 1.9, so you will no longer see it in the YAML.
3.6.2
Adding and modifying annotations
Annotations can obviously be added to pods at creation time, the same way labels can.
They can also be added to or modified on existing pods later. The simplest way to add
an annotation to an existing object is through the kubectl annotate command. 
 You’ll try adding an annotation to your kubia-manual pod now:
$ kubectl annotate pod kubia-manual mycompany.com/someannotation="foo bar"
pod "kubia-manual" annotated
You added the annotation mycompany.com/someannotation with the value foo bar.
It’s a good idea to use this format for annotation keys to prevent key collisions. When
different tools or libraries add annotations to objects, they may accidentally override
each other’s annotations if they don’t use unique prefixes like you did here.
 You can use kubectl describe to see the annotation you added:
$ kubectl describe pod kubia-manual
...
Annotations:    mycompany.com/someannotation=foo bar
...
3.7
Using namespaces to group resources
Let’s turn back to labels for a moment. We’ve seen how they organize pods and other
objects into groups. Because each object can have multiple labels, those groups of
objects can overlap. Plus, when working with the cluster (through kubectl for example),
if you don’t explicitly specify a label selector, you’ll always see all objects. 
Listing 3.5
A pod’s annotations
 
</data>
  <data key="d5">76
CHAPTER 3
Pods: running containers in Kubernetes
request the full YAML of the pod or use the kubectl describe command. You’ll use the
first option in the following listing.
$ kubectl get po kubia-zxzij -o yaml
apiVersion: v1
kind: pod
metadata:
  annotations:
    kubernetes.io/created-by: |
      {"kind":"SerializedReference", "apiVersion":"v1", 
      "reference":{"kind":"ReplicationController", "namespace":"default", ...
Without going into too many details, as you can see, the kubernetes.io/created-by
annotation holds JSON data about the object that created the pod. That’s not some-
thing you’d want to put into a label. Labels should be short, whereas annotations can
contain relatively large blobs of data (up to 256 KB in total).
NOTE
The kubernetes.io/created-by annotations was deprecated in ver-
sion 1.8 and will be removed in 1.9, so you will no longer see it in the YAML.
3.6.2
Adding and modifying annotations
Annotations can obviously be added to pods at creation time, the same way labels can.
They can also be added to or modified on existing pods later. The simplest way to add
an annotation to an existing object is through the kubectl annotate command. 
 You’ll try adding an annotation to your kubia-manual pod now:
$ kubectl annotate pod kubia-manual mycompany.com/someannotation="foo bar"
pod "kubia-manual" annotated
You added the annotation mycompany.com/someannotation with the value foo bar.
It’s a good idea to use this format for annotation keys to prevent key collisions. When
different tools or libraries add annotations to objects, they may accidentally override
each other’s annotations if they don’t use unique prefixes like you did here.
 You can use kubectl describe to see the annotation you added:
$ kubectl describe pod kubia-manual
...
Annotations:    mycompany.com/someannotation=foo bar
...
3.7
Using namespaces to group resources
Let’s turn back to labels for a moment. We’ve seen how they organize pods and other
objects into groups. Because each object can have multiple labels, those groups of
objects can overlap. Plus, when working with the cluster (through kubectl for example),
if you don’t explicitly specify a label selector, you’ll always see all objects. 
Listing 3.5
A pod’s annotations
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="109">
  <data key="d0">Page_109</data>
  <data key="d5">Page_109</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_20">
  <data key="d0">77
Using namespaces to group resources
 But what about times when you want to split objects into separate, non-overlapping
groups? You may want to only operate inside one group at a time. For this and other
reasons, Kubernetes also groups objects into namespaces. These aren’t the Linux
namespaces we talked about in chapter 2, which are used to isolate processes from
each other. Kubernetes namespaces provide a scope for objects names. Instead of hav-
ing all your resources in one single namespace, you can split them into multiple name-
spaces, which also allows you to use the same resource names multiple times (across
different namespaces).
3.7.1
Understanding the need for namespaces
Using multiple namespaces allows you to split complex systems with numerous com-
ponents into smaller distinct groups. They can also be used for separating resources
in a multi-tenant environment, splitting up resources into production, development,
and QA environments, or in any other way you may need. Resource names only need
to be unique within a namespace. Two different namespaces can contain resources of
the same name. But, while most types of resources are namespaced, a few aren’t. One
of them is the Node resource, which is global and not tied to a single namespace.
You’ll learn about other cluster-level resources in later chapters.
 Let’s see how to use namespaces now.
3.7.2
Discovering other namespaces and their pods
First, let’s list all namespaces in your cluster:
$ kubectl get ns
NAME          LABELS    STATUS    AGE
default       &lt;none&gt;    Active    1h
kube-public   &lt;none&gt;    Active    1h
kube-system   &lt;none&gt;    Active    1h
Up to this point, you’ve operated only in the default namespace. When listing resources
with the kubectl get command, you’ve never specified the namespace explicitly, so
kubectl always defaulted to the default namespace, showing you only the objects in
that namespace. But as you can see from the list, the kube-public and the kube-system
namespaces also exist. Let’s look at the pods that belong to the kube-system name-
space, by telling kubectl to list pods in that namespace only:
$ kubectl get po --namespace kube-system
NAME                                 READY     STATUS    RESTARTS   AGE
fluentd-cloud-kubia-e8fe-node-txje   1/1       Running   0          1h
heapster-v11-fz1ge                   1/1       Running   0          1h
kube-dns-v9-p8a4t                    0/4       Pending   0          1h
kube-ui-v4-kdlai                     1/1       Running   0          1h
l7-lb-controller-v0.5.2-bue96        2/2       Running   92         1h
TIP
You can also use -n instead of --namespace.
 
</data>
  <data key="d5">77
Using namespaces to group resources
 But what about times when you want to split objects into separate, non-overlapping
groups? You may want to only operate inside one group at a time. For this and other
reasons, Kubernetes also groups objects into namespaces. These aren’t the Linux
namespaces we talked about in chapter 2, which are used to isolate processes from
each other. Kubernetes namespaces provide a scope for objects names. Instead of hav-
ing all your resources in one single namespace, you can split them into multiple name-
spaces, which also allows you to use the same resource names multiple times (across
different namespaces).
3.7.1
Understanding the need for namespaces
Using multiple namespaces allows you to split complex systems with numerous com-
ponents into smaller distinct groups. They can also be used for separating resources
in a multi-tenant environment, splitting up resources into production, development,
and QA environments, or in any other way you may need. Resource names only need
to be unique within a namespace. Two different namespaces can contain resources of
the same name. But, while most types of resources are namespaced, a few aren’t. One
of them is the Node resource, which is global and not tied to a single namespace.
You’ll learn about other cluster-level resources in later chapters.
 Let’s see how to use namespaces now.
3.7.2
Discovering other namespaces and their pods
First, let’s list all namespaces in your cluster:
$ kubectl get ns
NAME          LABELS    STATUS    AGE
default       &lt;none&gt;    Active    1h
kube-public   &lt;none&gt;    Active    1h
kube-system   &lt;none&gt;    Active    1h
Up to this point, you’ve operated only in the default namespace. When listing resources
with the kubectl get command, you’ve never specified the namespace explicitly, so
kubectl always defaulted to the default namespace, showing you only the objects in
that namespace. But as you can see from the list, the kube-public and the kube-system
namespaces also exist. Let’s look at the pods that belong to the kube-system name-
space, by telling kubectl to list pods in that namespace only:
$ kubectl get po --namespace kube-system
NAME                                 READY     STATUS    RESTARTS   AGE
fluentd-cloud-kubia-e8fe-node-txje   1/1       Running   0          1h
heapster-v11-fz1ge                   1/1       Running   0          1h
kube-dns-v9-p8a4t                    0/4       Pending   0          1h
kube-ui-v4-kdlai                     1/1       Running   0          1h
l7-lb-controller-v0.5.2-bue96        2/2       Running   92         1h
TIP
You can also use -n instead of --namespace.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="110">
  <data key="d0">Page_110</data>
  <data key="d5">Page_110</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_21">
  <data key="d0">78
CHAPTER 3
Pods: running containers in Kubernetes
You’ll learn about these pods later in the book (don’t worry if the pods shown here
don’t match the ones on your system exactly). It’s clear from the name of the name-
space that these are resources related to the Kubernetes system itself. By having
them in this separate namespace, it keeps everything nicely organized. If they were
all in the default namespace, mixed in with the resources you create yourself, you’d
have a hard time seeing what belongs where, and you might inadvertently delete sys-
tem resources. 
 Namespaces enable you to separate resources that don’t belong together into non-
overlapping groups. If several users or groups of users are using the same Kubernetes
cluster, and they each manage their own distinct set of resources, they should each use
their own namespace. This way, they don’t need to take any special care not to inad-
vertently modify or delete the other users’ resources and don’t need to concern them-
selves with name conflicts, because namespaces provide a scope for resource names,
as has already been mentioned.
  Besides isolating resources, namespaces are also used for allowing only certain users
access to particular resources and even for limiting the amount of computational
resources available to individual users. You’ll learn about this in chapters 12 through 14.
3.7.3
Creating a namespace
A namespace is a Kubernetes resource like any other, so you can create it by posting a
YAML file to the Kubernetes API server. Let’s see how to do this now. 
CREATING A NAMESPACE FROM A YAML FILE
First, create a custom-namespace.yaml file with the following listing’s contents (you’ll
find the file in the book’s code archive).
apiVersion: v1
kind: Namespace         
metadata:
  name: custom-namespace  
Now, use kubectl to post the file to the Kubernetes API server:
$ kubectl create -f custom-namespace.yaml
namespace "custom-namespace" created
CREATING A NAMESPACE WITH KUBECTL CREATE NAMESPACE
Although writing a file like the previous one isn’t a big deal, it’s still a hassle. Luckily,
you can also create namespaces with the dedicated kubectl create namespace com-
mand, which is quicker than writing a YAML file. By having you create a YAML mani-
fest for the namespace, I wanted to reinforce the idea that everything in Kubernetes
Listing 3.6
A YAML definition of a namespace: custom-namespace.yaml
This says you’re 
defining a namespace.
This is the name 
of the namespace.
 
</data>
  <data key="d5">78
CHAPTER 3
Pods: running containers in Kubernetes
You’ll learn about these pods later in the book (don’t worry if the pods shown here
don’t match the ones on your system exactly). It’s clear from the name of the name-
space that these are resources related to the Kubernetes system itself. By having
them in this separate namespace, it keeps everything nicely organized. If they were
all in the default namespace, mixed in with the resources you create yourself, you’d
have a hard time seeing what belongs where, and you might inadvertently delete sys-
tem resources. 
 Namespaces enable you to separate resources that don’t belong together into non-
overlapping groups. If several users or groups of users are using the same Kubernetes
cluster, and they each manage their own distinct set of resources, they should each use
their own namespace. This way, they don’t need to take any special care not to inad-
vertently modify or delete the other users’ resources and don’t need to concern them-
selves with name conflicts, because namespaces provide a scope for resource names,
as has already been mentioned.
  Besides isolating resources, namespaces are also used for allowing only certain users
access to particular resources and even for limiting the amount of computational
resources available to individual users. You’ll learn about this in chapters 12 through 14.
3.7.3
Creating a namespace
A namespace is a Kubernetes resource like any other, so you can create it by posting a
YAML file to the Kubernetes API server. Let’s see how to do this now. 
CREATING A NAMESPACE FROM A YAML FILE
First, create a custom-namespace.yaml file with the following listing’s contents (you’ll
find the file in the book’s code archive).
apiVersion: v1
kind: Namespace         
metadata:
  name: custom-namespace  
Now, use kubectl to post the file to the Kubernetes API server:
$ kubectl create -f custom-namespace.yaml
namespace "custom-namespace" created
CREATING A NAMESPACE WITH KUBECTL CREATE NAMESPACE
Although writing a file like the previous one isn’t a big deal, it’s still a hassle. Luckily,
you can also create namespaces with the dedicated kubectl create namespace com-
mand, which is quicker than writing a YAML file. By having you create a YAML mani-
fest for the namespace, I wanted to reinforce the idea that everything in Kubernetes
Listing 3.6
A YAML definition of a namespace: custom-namespace.yaml
This says you’re 
defining a namespace.
This is the name 
of the namespace.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="111">
  <data key="d0">Page_111</data>
  <data key="d5">Page_111</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_22">
  <data key="d0">79
Using namespaces to group resources
has a corresponding API object that you can create, read, update, and delete by post-
ing a YAML manifest to the API server.
 You could have created the namespace like this:
$ kubectl create namespace custom-namespace
namespace "custom-namespace" created
NOTE
Although most objects’ names must conform to the naming conven-
tions specified in RFC 1035 (Domain names), which means they may contain
only letters, digits, dashes, and dots, namespaces (and a few others) aren’t
allowed to contain dots. 
3.7.4
Managing objects in other namespaces
To create resources in the namespace you’ve created, either add a namespace: custom-
namespace entry to the metadata section, or specify the namespace when creating the
resource with the kubectl create command:
$ kubectl create -f kubia-manual.yaml -n custom-namespace
pod "kubia-manual" created
You now have two pods with the same name (kubia-manual). One is in the default
namespace, and the other is in your custom-namespace.
 When listing, describing, modifying, or deleting objects in other namespaces, you
need to pass the --namespace (or -n) flag to kubectl. If you don’t specify the name-
space, kubectl performs the action in the default namespace configured in the cur-
rent kubectl context. The current context’s namespace and the current context itself
can be changed through kubectl config commands. To learn more about managing
kubectl contexts, refer to appendix A. 
TIP
To quickly switch to a different namespace, you can set up the following
alias: alias kcd='kubectl config set-context $(kubectl config current-
context) --namespace '. You can then switch between namespaces using kcd
some-namespace.
3.7.5
Understanding the isolation provided by namespaces
To wrap up this section about namespaces, let me explain what namespaces don’t pro-
vide—at least not out of the box. Although namespaces allow you to isolate objects
into distinct groups, which allows you to operate only on those belonging to the speci-
fied namespace, they don’t provide any kind of isolation of running objects. 
 For example, you may think that when different users deploy pods across different
namespaces, those pods are isolated from each other and can’t communicate, but that’s
not necessarily the case. Whether namespaces provide network isolation depends on
which networking solution is deployed with Kubernetes. When the solution doesn’t
provide inter-namespace network isolation, if a pod in namespace foo knows the IP
 
</data>
  <data key="d5">79
Using namespaces to group resources
has a corresponding API object that you can create, read, update, and delete by post-
ing a YAML manifest to the API server.
 You could have created the namespace like this:
$ kubectl create namespace custom-namespace
namespace "custom-namespace" created
NOTE
Although most objects’ names must conform to the naming conven-
tions specified in RFC 1035 (Domain names), which means they may contain
only letters, digits, dashes, and dots, namespaces (and a few others) aren’t
allowed to contain dots. 
3.7.4
Managing objects in other namespaces
To create resources in the namespace you’ve created, either add a namespace: custom-
namespace entry to the metadata section, or specify the namespace when creating the
resource with the kubectl create command:
$ kubectl create -f kubia-manual.yaml -n custom-namespace
pod "kubia-manual" created
You now have two pods with the same name (kubia-manual). One is in the default
namespace, and the other is in your custom-namespace.
 When listing, describing, modifying, or deleting objects in other namespaces, you
need to pass the --namespace (or -n) flag to kubectl. If you don’t specify the name-
space, kubectl performs the action in the default namespace configured in the cur-
rent kubectl context. The current context’s namespace and the current context itself
can be changed through kubectl config commands. To learn more about managing
kubectl contexts, refer to appendix A. 
TIP
To quickly switch to a different namespace, you can set up the following
alias: alias kcd='kubectl config set-context $(kubectl config current-
context) --namespace '. You can then switch between namespaces using kcd
some-namespace.
3.7.5
Understanding the isolation provided by namespaces
To wrap up this section about namespaces, let me explain what namespaces don’t pro-
vide—at least not out of the box. Although namespaces allow you to isolate objects
into distinct groups, which allows you to operate only on those belonging to the speci-
fied namespace, they don’t provide any kind of isolation of running objects. 
 For example, you may think that when different users deploy pods across different
namespaces, those pods are isolated from each other and can’t communicate, but that’s
not necessarily the case. Whether namespaces provide network isolation depends on
which networking solution is deployed with Kubernetes. When the solution doesn’t
provide inter-namespace network isolation, if a pod in namespace foo knows the IP
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="112">
  <data key="d0">Page_112</data>
  <data key="d5">Page_112</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_23">
  <data key="d0">80
CHAPTER 3
Pods: running containers in Kubernetes
address of a pod in namespace bar, there is nothing preventing it from sending traffic,
such as HTTP requests, to the other pod. 
3.8
Stopping and removing pods
You’ve created a number of pods, which should all still be running. You have four
pods running in the default namespace and one pod in custom-namespace. You’re
going to stop all of them now, because you don’t need them anymore.
3.8.1
Deleting a pod by name
First, delete the kubia-gpu pod by name:
$ kubectl delete po kubia-gpu
pod "kubia-gpu" deleted
By deleting a pod, you’re instructing Kubernetes to terminate all the containers that are
part of that pod. Kubernetes sends a SIGTERM signal to the process and waits a certain
number of seconds (30 by default) for it to shut down gracefully. If it doesn’t shut down
in time, the process is then killed through SIGKILL. To make sure your processes are
always shut down gracefully, they need to handle the SIGTERM signal properly. 
TIP
You can also delete more than one pod by specifying multiple, space-sep-
arated names (for example, kubectl delete po pod1 pod2).
3.8.2
Deleting pods using label selectors
Instead of specifying each pod to delete by name, you’ll now use what you’ve learned
about label selectors to stop both the kubia-manual and the kubia-manual-v2 pod.
Both pods include the creation_method=manual label, so you can delete them by
using a label selector:
$ kubectl delete po -l creation_method=manual
pod "kubia-manual" deleted
pod "kubia-manual-v2" deleted 
In the earlier microservices example, where you had tens (or possibly hundreds) of
pods, you could, for instance, delete all canary pods at once by specifying the
rel=canary label selector (visualized in figure 3.10):
$ kubectl delete po -l rel=canary
3.8.3
Deleting pods by deleting the whole namespace
Okay, back to your real pods. What about the pod in the custom-namespace? You no
longer need either the pods in that namespace, or the namespace itself. You can
 
</data>
  <data key="d5">80
CHAPTER 3
Pods: running containers in Kubernetes
address of a pod in namespace bar, there is nothing preventing it from sending traffic,
such as HTTP requests, to the other pod. 
3.8
Stopping and removing pods
You’ve created a number of pods, which should all still be running. You have four
pods running in the default namespace and one pod in custom-namespace. You’re
going to stop all of them now, because you don’t need them anymore.
3.8.1
Deleting a pod by name
First, delete the kubia-gpu pod by name:
$ kubectl delete po kubia-gpu
pod "kubia-gpu" deleted
By deleting a pod, you’re instructing Kubernetes to terminate all the containers that are
part of that pod. Kubernetes sends a SIGTERM signal to the process and waits a certain
number of seconds (30 by default) for it to shut down gracefully. If it doesn’t shut down
in time, the process is then killed through SIGKILL. To make sure your processes are
always shut down gracefully, they need to handle the SIGTERM signal properly. 
TIP
You can also delete more than one pod by specifying multiple, space-sep-
arated names (for example, kubectl delete po pod1 pod2).
3.8.2
Deleting pods using label selectors
Instead of specifying each pod to delete by name, you’ll now use what you’ve learned
about label selectors to stop both the kubia-manual and the kubia-manual-v2 pod.
Both pods include the creation_method=manual label, so you can delete them by
using a label selector:
$ kubectl delete po -l creation_method=manual
pod "kubia-manual" deleted
pod "kubia-manual-v2" deleted 
In the earlier microservices example, where you had tens (or possibly hundreds) of
pods, you could, for instance, delete all canary pods at once by specifying the
rel=canary label selector (visualized in figure 3.10):
$ kubectl delete po -l rel=canary
3.8.3
Deleting pods by deleting the whole namespace
Okay, back to your real pods. What about the pod in the custom-namespace? You no
longer need either the pods in that namespace, or the namespace itself. You can
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="113">
  <data key="d0">Page_113</data>
  <data key="d5">Page_113</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_24">
  <data key="d0">81
Stopping and removing pods
delete the whole namespace (the pods will be deleted along with the namespace auto-
matically), using the following command:
$ kubectl delete ns custom-namespace
namespace "custom-namespace" deleted
3.8.4
Deleting all pods in a namespace, while keeping the namespace
You’ve now cleaned up almost everything. But what about the pod you created with
the kubectl run command in chapter 2? That one is still running:
$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
kubia-zxzij     1/1     Running   0          1d    
This time, instead of deleting the specific pod, tell Kubernetes to delete all pods in the
current namespace by using the --all option:
$ kubectl delete po --all
pod "kubia-zxzij" deleted
Now, double check that no pods were left running:
$ kubectl get pods
NAME            READY   STATUS        RESTARTS   AGE
kubia-09as0     1/1     Running       0          1d    
kubia-zxzij     1/1     Terminating   0          1d    
UI pod
app: ui
rel: stable
rel=stable
app=ui
Account
Service
pod
app: as
rel: stable
app=as
app: pc
rel: stable
app=pc
app: sc
rel: stable
app=sc
app: os
rel: stable
app=os
Product
Catalog
pod
Shopping
Cart
pod
Order
Service
pod
UI pod
app: ui
rel: beta
rel=beta
app: pc
rel: beta
app: os
rel: beta
Product
Catalog
pod
Order
Service
pod
rel=canary
Account
Service
pod
app: as
rel: canary
app: pc
rel: canary
app: os
rel: canary
Product
Catalog
pod
Order
Service
pod
Figure 3.10
Selecting and deleting all canary pods through the rel=canary label selector
 
</data>
  <data key="d5">81
Stopping and removing pods
delete the whole namespace (the pods will be deleted along with the namespace auto-
matically), using the following command:
$ kubectl delete ns custom-namespace
namespace "custom-namespace" deleted
3.8.4
Deleting all pods in a namespace, while keeping the namespace
You’ve now cleaned up almost everything. But what about the pod you created with
the kubectl run command in chapter 2? That one is still running:
$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
kubia-zxzij     1/1     Running   0          1d    
This time, instead of deleting the specific pod, tell Kubernetes to delete all pods in the
current namespace by using the --all option:
$ kubectl delete po --all
pod "kubia-zxzij" deleted
Now, double check that no pods were left running:
$ kubectl get pods
NAME            READY   STATUS        RESTARTS   AGE
kubia-09as0     1/1     Running       0          1d    
kubia-zxzij     1/1     Terminating   0          1d    
UI pod
app: ui
rel: stable
rel=stable
app=ui
Account
Service
pod
app: as
rel: stable
app=as
app: pc
rel: stable
app=pc
app: sc
rel: stable
app=sc
app: os
rel: stable
app=os
Product
Catalog
pod
Shopping
Cart
pod
Order
Service
pod
UI pod
app: ui
rel: beta
rel=beta
app: pc
rel: beta
app: os
rel: beta
Product
Catalog
pod
Order
Service
pod
rel=canary
Account
Service
pod
app: as
rel: canary
app: pc
rel: canary
app: os
rel: canary
Product
Catalog
pod
Order
Service
pod
Figure 3.10
Selecting and deleting all canary pods through the rel=canary label selector
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="114">
  <data key="d0">Page_114</data>
  <data key="d5">Page_114</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_25">
  <data key="d0">82
CHAPTER 3
Pods: running containers in Kubernetes
Wait, what!?! The kubia-zxzij pod is terminating, but a new pod called kubia-09as0,
which wasn’t there before, has appeared. No matter how many times you delete all
pods, a new pod called kubia-something will emerge. 
 You may remember you created your first pod with the kubectl run command. In
chapter 2, I mentioned that this doesn’t create a pod directly, but instead creates a
ReplicationController, which then creates the pod. As soon as you delete a pod cre-
ated by the ReplicationController, it immediately creates a new one. To delete the
pod, you also need to delete the ReplicationController. 
3.8.5
Deleting (almost) all resources in a namespace
You can delete the ReplicationController and the pods, as well as all the Services
you’ve created, by deleting all resources in the current namespace with a single
command:
$ kubectl delete all --all
pod "kubia-09as0" deleted
replicationcontroller "kubia" deleted
service "kubernetes" deleted
service "kubia-http" deleted
The first all in the command specifies that you’re deleting resources of all types, and
the --all option specifies that you’re deleting all resource instances instead of speci-
fying them by name (you already used this option when you ran the previous delete
command).
NOTE
Deleting everything with the all keyword doesn’t delete absolutely
everything. Certain resources (like Secrets, which we’ll introduce in chapter 7)
are preserved and need to be deleted explicitly.
As it deletes resources, kubectl will print the name of every resource it deletes. In the
list, you should see the kubia ReplicationController and the kubia-http Service you
created in chapter 2. 
NOTE
The kubectl delete all --all command also deletes the kubernetes
Service, but it should be recreated automatically in a few moments.
3.9
Summary
After reading this chapter, you should now have a decent knowledge of the central
building block in Kubernetes. Every other concept you’ll learn about in the next few
chapters is directly related to pods. 
 In this chapter, you’ve learned
How to decide whether certain containers should be grouped together in a pod
or not.
 
</data>
  <data key="d5">82
CHAPTER 3
Pods: running containers in Kubernetes
Wait, what!?! The kubia-zxzij pod is terminating, but a new pod called kubia-09as0,
which wasn’t there before, has appeared. No matter how many times you delete all
pods, a new pod called kubia-something will emerge. 
 You may remember you created your first pod with the kubectl run command. In
chapter 2, I mentioned that this doesn’t create a pod directly, but instead creates a
ReplicationController, which then creates the pod. As soon as you delete a pod cre-
ated by the ReplicationController, it immediately creates a new one. To delete the
pod, you also need to delete the ReplicationController. 
3.8.5
Deleting (almost) all resources in a namespace
You can delete the ReplicationController and the pods, as well as all the Services
you’ve created, by deleting all resources in the current namespace with a single
command:
$ kubectl delete all --all
pod "kubia-09as0" deleted
replicationcontroller "kubia" deleted
service "kubernetes" deleted
service "kubia-http" deleted
The first all in the command specifies that you’re deleting resources of all types, and
the --all option specifies that you’re deleting all resource instances instead of speci-
fying them by name (you already used this option when you ran the previous delete
command).
NOTE
Deleting everything with the all keyword doesn’t delete absolutely
everything. Certain resources (like Secrets, which we’ll introduce in chapter 7)
are preserved and need to be deleted explicitly.
As it deletes resources, kubectl will print the name of every resource it deletes. In the
list, you should see the kubia ReplicationController and the kubia-http Service you
created in chapter 2. 
NOTE
The kubectl delete all --all command also deletes the kubernetes
Service, but it should be recreated automatically in a few moments.
3.9
Summary
After reading this chapter, you should now have a decent knowledge of the central
building block in Kubernetes. Every other concept you’ll learn about in the next few
chapters is directly related to pods. 
 In this chapter, you’ve learned
How to decide whether certain containers should be grouped together in a pod
or not.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="115">
  <data key="d0">Page_115</data>
  <data key="d5">Page_115</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_26">
  <data key="d0">83
Summary
Pods can run multiple processes and are similar to physical hosts in the non-
container world.
YAML or JSON descriptors can be written and used to create pods and then
examined to see the specification of a pod and its current state.
Labels and label selectors should be used to organize pods and easily perform
operations on multiple pods at once.
You can use node labels and selectors to schedule pods only to nodes that have
certain features.
Annotations allow attaching larger blobs of data to pods either by people or
tools and libraries.
Namespaces can be used to allow different teams to use the same cluster as
though they were using separate Kubernetes clusters.
How to use the kubectl explain command to quickly look up the information
on any Kubernetes resource. 
In the next chapter, you’ll learn about ReplicationControllers and other resources
that manage pods.
 
</data>
  <data key="d5">83
Summary
Pods can run multiple processes and are similar to physical hosts in the non-
container world.
YAML or JSON descriptors can be written and used to create pods and then
examined to see the specification of a pod and its current state.
Labels and label selectors should be used to organize pods and easily perform
operations on multiple pods at once.
You can use node labels and selectors to schedule pods only to nodes that have
certain features.
Annotations allow attaching larger blobs of data to pods either by people or
tools and libraries.
Namespaces can be used to allow different teams to use the same cluster as
though they were using separate Kubernetes clusters.
How to use the kubectl explain command to quickly look up the information
on any Kubernetes resource. 
In the next chapter, you’ll learn about ReplicationControllers and other resources
that manage pods.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="116">
  <data key="d0">Page_116</data>
  <data key="d5">Page_116</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_27">
  <data key="d0">84
Replication and other
controllers: deploying
managed pods
As you’ve learned so far, pods represent the basic deployable unit in Kubernetes.
You know how to create, supervise, and manage them manually. But in real-world
use cases, you want your deployments to stay up and running automatically and
remain healthy without any manual intervention. To do this, you almost never cre-
ate pods directly. Instead, you create other types of resources, such as Replication-
Controllers or Deployments, which then create and manage the actual pods.
 When you create unmanaged pods (such as the ones you created in the previ-
ous chapter), a cluster node is selected to run the pod and then its containers are
run on that node. In this chapter, you’ll learn that Kubernetes then monitors
This chapter covers
Keeping pods healthy
Running multiple instances of the same pod
Automatically rescheduling pods after a node fails
Scaling pods horizontally
Running system-level pods on each cluster node
Running batch jobs
Scheduling jobs to run periodically or once in 
the future
 
</data>
  <data key="d5">84
Replication and other
controllers: deploying
managed pods
As you’ve learned so far, pods represent the basic deployable unit in Kubernetes.
You know how to create, supervise, and manage them manually. But in real-world
use cases, you want your deployments to stay up and running automatically and
remain healthy without any manual intervention. To do this, you almost never cre-
ate pods directly. Instead, you create other types of resources, such as Replication-
Controllers or Deployments, which then create and manage the actual pods.
 When you create unmanaged pods (such as the ones you created in the previ-
ous chapter), a cluster node is selected to run the pod and then its containers are
run on that node. In this chapter, you’ll learn that Kubernetes then monitors
This chapter covers
Keeping pods healthy
Running multiple instances of the same pod
Automatically rescheduling pods after a node fails
Scaling pods horizontally
Running system-level pods on each cluster node
Running batch jobs
Scheduling jobs to run periodically or once in 
the future
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="117">
  <data key="d0">Page_117</data>
  <data key="d5">Page_117</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_28">
  <data key="d0">85
Keeping pods healthy
those containers and automatically restarts them if they fail. But if the whole node
fails, the pods on the node are lost and will not be replaced with new ones, unless
those pods are managed by the previously mentioned ReplicationControllers or simi-
lar. In this chapter, you’ll learn how Kubernetes checks if a container is still alive and
restarts it if it isn’t. You’ll also learn how to run managed pods—both those that run
indefinitely and those that perform a single task and then stop. 
4.1
Keeping pods healthy
One of the main benefits of using Kubernetes is the ability to give it a list of contain-
ers and let it keep those containers running somewhere in the cluster. You do this by
creating a Pod resource and letting Kubernetes pick a worker node for it and run
the pod’s containers on that node. But what if one of those containers dies? What if
all containers of a pod die? 
 As soon as a pod is scheduled to a node, the Kubelet on that node will run its con-
tainers and, from then on, keep them running as long as the pod exists. If the con-
tainer’s main process crashes, the Kubelet will restart the container. If your
application has a bug that causes it to crash every once in a while, Kubernetes will
restart it automatically, so even without doing anything special in the app itself, run-
ning the app in Kubernetes automatically gives it the ability to heal itself. 
 But sometimes apps stop working without their process crashing. For example, a
Java app with a memory leak will start throwing OutOfMemoryErrors, but the JVM
process will keep running. It would be great to have a way for an app to signal to
Kubernetes that it’s no longer functioning properly and have Kubernetes restart it. 
 We’ve said that a container that crashes is restarted automatically, so maybe you’re
thinking you could catch these types of errors in the app and exit the process when
they occur. You can certainly do that, but it still doesn’t solve all your problems. 
 For example, what about those situations when your app stops responding because
it falls into an infinite loop or a deadlock? To make sure applications are restarted in
such cases, you must check an application’s health from the outside and not depend
on the app doing it internally. 
4.1.1
Introducing liveness probes
Kubernetes can check if a container is still alive through liveness probes. You can specify
a liveness probe for each container in the pod’s specification. Kubernetes will periodi-
cally execute the probe and restart the container if the probe fails. 
NOTE
Kubernetes also supports readiness probes, which we’ll learn about in the
next chapter. Be sure not to confuse the two. They’re used for two different
things.
Kubernetes can probe a container using one of the three mechanisms:
An HTTP GET probe performs an HTTP GET request on the container’s IP
address, a port and path you specify. If the probe receives a response, and the
 
</data>
  <data key="d5">85
Keeping pods healthy
those containers and automatically restarts them if they fail. But if the whole node
fails, the pods on the node are lost and will not be replaced with new ones, unless
those pods are managed by the previously mentioned ReplicationControllers or simi-
lar. In this chapter, you’ll learn how Kubernetes checks if a container is still alive and
restarts it if it isn’t. You’ll also learn how to run managed pods—both those that run
indefinitely and those that perform a single task and then stop. 
4.1
Keeping pods healthy
One of the main benefits of using Kubernetes is the ability to give it a list of contain-
ers and let it keep those containers running somewhere in the cluster. You do this by
creating a Pod resource and letting Kubernetes pick a worker node for it and run
the pod’s containers on that node. But what if one of those containers dies? What if
all containers of a pod die? 
 As soon as a pod is scheduled to a node, the Kubelet on that node will run its con-
tainers and, from then on, keep them running as long as the pod exists. If the con-
tainer’s main process crashes, the Kubelet will restart the container. If your
application has a bug that causes it to crash every once in a while, Kubernetes will
restart it automatically, so even without doing anything special in the app itself, run-
ning the app in Kubernetes automatically gives it the ability to heal itself. 
 But sometimes apps stop working without their process crashing. For example, a
Java app with a memory leak will start throwing OutOfMemoryErrors, but the JVM
process will keep running. It would be great to have a way for an app to signal to
Kubernetes that it’s no longer functioning properly and have Kubernetes restart it. 
 We’ve said that a container that crashes is restarted automatically, so maybe you’re
thinking you could catch these types of errors in the app and exit the process when
they occur. You can certainly do that, but it still doesn’t solve all your problems. 
 For example, what about those situations when your app stops responding because
it falls into an infinite loop or a deadlock? To make sure applications are restarted in
such cases, you must check an application’s health from the outside and not depend
on the app doing it internally. 
4.1.1
Introducing liveness probes
Kubernetes can check if a container is still alive through liveness probes. You can specify
a liveness probe for each container in the pod’s specification. Kubernetes will periodi-
cally execute the probe and restart the container if the probe fails. 
NOTE
Kubernetes also supports readiness probes, which we’ll learn about in the
next chapter. Be sure not to confuse the two. They’re used for two different
things.
Kubernetes can probe a container using one of the three mechanisms:
An HTTP GET probe performs an HTTP GET request on the container’s IP
address, a port and path you specify. If the probe receives a response, and the
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="118">
  <data key="d0">Page_118</data>
  <data key="d5">Page_118</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_29">
  <data key="d0">86
CHAPTER 4
Replication and other controllers: deploying managed pods
response code doesn’t represent an error (in other words, if the HTTP response
code is 2xx or 3xx), the probe is considered successful. If the server returns an
error response code or if it doesn’t respond at all, the probe is considered a fail-
ure and the container will be restarted as a result.
A TCP Socket probe tries to open a TCP connection to the specified port of the
container. If the connection is established successfully, the probe is successful.
Otherwise, the container is restarted.
An Exec probe executes an arbitrary command inside the container and checks
the command’s exit status code. If the status code is 0, the probe is successful.
All other codes are considered failures. 
4.1.2
Creating an HTTP-based liveness probe
Let’s see how to add a liveness probe to your Node.js app. Because it’s a web app, it
makes sense to add a liveness probe that will check whether its web server is serving
requests. But because this particular Node.js app is too simple to ever fail, you’ll need
to make the app fail artificially. 
 To properly demo liveness probes, you’ll modify the app slightly and make it
return a 500 Internal Server Error HTTP status code for each request after the fifth
one—your app will handle the first five client requests properly and then return an
error on every subsequent request. Thanks to the liveness probe, it should be restarted
when that happens, allowing it to properly handle client requests again.
 You can find the code of the new app in the book’s code archive (in the folder
Chapter04/kubia-unhealthy). I’ve pushed the container image to Docker Hub, so you
don’t need to build it yourself. 
 You’ll create a new pod that includes an HTTP GET liveness probe. The following
listing shows the YAML for the pod.
apiVersion: v1
kind: pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy   
    name: kubia
    livenessProbe:                 
      httpGet:                     
        path: /                     
        port: 8080       
Listing 4.1
Adding a liveness probe to a pod: kubia-liveness-probe.yaml
This is the image 
containing the 
(somewhat) 
broken app.
A liveness probe that will 
perform an HTTP GET
The path to 
request in the 
HTTP request
The network port
the probe should
connect to
 
</data>
  <data key="d5">86
CHAPTER 4
Replication and other controllers: deploying managed pods
response code doesn’t represent an error (in other words, if the HTTP response
code is 2xx or 3xx), the probe is considered successful. If the server returns an
error response code or if it doesn’t respond at all, the probe is considered a fail-
ure and the container will be restarted as a result.
A TCP Socket probe tries to open a TCP connection to the specified port of the
container. If the connection is established successfully, the probe is successful.
Otherwise, the container is restarted.
An Exec probe executes an arbitrary command inside the container and checks
the command’s exit status code. If the status code is 0, the probe is successful.
All other codes are considered failures. 
4.1.2
Creating an HTTP-based liveness probe
Let’s see how to add a liveness probe to your Node.js app. Because it’s a web app, it
makes sense to add a liveness probe that will check whether its web server is serving
requests. But because this particular Node.js app is too simple to ever fail, you’ll need
to make the app fail artificially. 
 To properly demo liveness probes, you’ll modify the app slightly and make it
return a 500 Internal Server Error HTTP status code for each request after the fifth
one—your app will handle the first five client requests properly and then return an
error on every subsequent request. Thanks to the liveness probe, it should be restarted
when that happens, allowing it to properly handle client requests again.
 You can find the code of the new app in the book’s code archive (in the folder
Chapter04/kubia-unhealthy). I’ve pushed the container image to Docker Hub, so you
don’t need to build it yourself. 
 You’ll create a new pod that includes an HTTP GET liveness probe. The following
listing shows the YAML for the pod.
apiVersion: v1
kind: pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy   
    name: kubia
    livenessProbe:                 
      httpGet:                     
        path: /                     
        port: 8080       
Listing 4.1
Adding a liveness probe to a pod: kubia-liveness-probe.yaml
This is the image 
containing the 
(somewhat) 
broken app.
A liveness probe that will 
perform an HTTP GET
The path to 
request in the 
HTTP request
The network port
the probe should
connect to
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="119">
  <data key="d0">Page_119</data>
  <data key="d5">Page_119</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_30">
  <data key="d0">87
Keeping pods healthy
The pod descriptor defines an httpGet liveness probe, which tells Kubernetes to peri-
odically perform HTTP GET requests on path / on port 8080 to determine if the con-
tainer is still healthy. These requests start as soon as the container is run.
 After five such requests (or actual client requests), your app starts returning
HTTP status code 500, which Kubernetes will treat as a probe failure, and will thus
restart the container. 
4.1.3
Seeing a liveness probe in action
To see what the liveness probe does, try creating the pod now. After about a minute and
a half, the container will be restarted. You can see that by running kubectl get:
$ kubectl get po kubia-liveness
NAME             READY     STATUS    RESTARTS   AGE
kubia-liveness   1/1       Running   1          2m
The RESTARTS column shows that the pod’s container has been restarted once (if you
wait another minute and a half, it gets restarted again, and then the cycle continues
indefinitely).
You can see why the container had to be restarted by looking at what kubectl describe
prints out, as shown in the following listing.
$ kubectl describe po kubia-liveness
Name:           kubia-liveness
...
Containers:
  kubia:
    Container ID:       docker://480986f8
    Image:              luksa/kubia-unhealthy
    Image ID:           docker://sha256:2b208508
    Port:
    State:              Running                            
      Started:          Sun, 14 May 2017 11:41:40 +0200    
Obtaining the application log of a crashed container
In the previous chapter, you learned how to print the application’s log with kubectl
logs. If your container is restarted, the kubectl logs command will show the log of
the current container. 
When you want to figure out why the previous container terminated, you’ll want to
see those logs instead of the current container’s logs. This can be done by using
the --previous option:
$ kubectl logs mypod --previous
Listing 4.2
A pod’s description after its container is restarted
The container is 
currently running.
 
</data>
  <data key="d5">87
Keeping pods healthy
The pod descriptor defines an httpGet liveness probe, which tells Kubernetes to peri-
odically perform HTTP GET requests on path / on port 8080 to determine if the con-
tainer is still healthy. These requests start as soon as the container is run.
 After five such requests (or actual client requests), your app starts returning
HTTP status code 500, which Kubernetes will treat as a probe failure, and will thus
restart the container. 
4.1.3
Seeing a liveness probe in action
To see what the liveness probe does, try creating the pod now. After about a minute and
a half, the container will be restarted. You can see that by running kubectl get:
$ kubectl get po kubia-liveness
NAME             READY     STATUS    RESTARTS   AGE
kubia-liveness   1/1       Running   1          2m
The RESTARTS column shows that the pod’s container has been restarted once (if you
wait another minute and a half, it gets restarted again, and then the cycle continues
indefinitely).
You can see why the container had to be restarted by looking at what kubectl describe
prints out, as shown in the following listing.
$ kubectl describe po kubia-liveness
Name:           kubia-liveness
...
Containers:
  kubia:
    Container ID:       docker://480986f8
    Image:              luksa/kubia-unhealthy
    Image ID:           docker://sha256:2b208508
    Port:
    State:              Running                            
      Started:          Sun, 14 May 2017 11:41:40 +0200    
Obtaining the application log of a crashed container
In the previous chapter, you learned how to print the application’s log with kubectl
logs. If your container is restarted, the kubectl logs command will show the log of
the current container. 
When you want to figure out why the previous container terminated, you’ll want to
see those logs instead of the current container’s logs. This can be done by using
the --previous option:
$ kubectl logs mypod --previous
Listing 4.2
A pod’s description after its container is restarted
The container is 
currently running.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="120">
  <data key="d0">Page_120</data>
  <data key="d5">Page_120</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_31">
  <data key="d0">88
CHAPTER 4
Replication and other controllers: deploying managed pods
    Last State:         Terminated                         
      Reason:           Error                              
      Exit Code:        137                                
      Started:          Mon, 01 Jan 0001 00:00:00 +0000    
      Finished:         Sun, 14 May 2017 11:41:38 +0200    
    Ready:              True
    Restart Count:      1                                 
    Liveness:           http-get http://:8080/ delay=0s timeout=1s
                        period=10s #success=1 #failure=3
    ...
Events:
... Killing container with id docker://95246981:pod "kubia-liveness ..."
    container "kubia" is unhealthy, it will be killed and re-created.
You can see that the container is currently running, but it previously terminated
because of an error. The exit code was 137, which has a special meaning—it denotes
that the process was terminated by an external signal. The number 137 is a sum of two
numbers: 128+x, where x is the signal number sent to the process that caused it to ter-
minate. In the example, x equals 9, which is the number of the SIGKILL signal, mean-
ing the process was killed forcibly.
 The events listed at the bottom show why the container was killed—Kubernetes
detected the container was unhealthy, so it killed and re-created it. 
NOTE
When a container is killed, a completely new container is created—it’s
not the same container being restarted again.
4.1.4
Configuring additional properties of the liveness probe
You may have noticed that kubectl describe also displays additional information
about the liveness probe:
Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 
          ➥ #failure=3
Beside the liveness probe options you specified explicitly, you can also see additional
properties, such as delay, timeout, period, and so on. The delay=0s part shows that
the probing begins immediately after the container is started. The timeout is set to
only 1 second, so the container must return a response in 1 second or the probe is
counted as failed. The container is probed every 10 seconds (period=10s) and the
container is restarted after the probe fails three consecutive times (#failure=3). 
 These additional parameters can be customized when defining the probe. For
example, to set the initial delay, add the initialDelaySeconds property to the live-
ness probe as shown in the following listing.
   livenessProbe:          
     httpGet:              
       path: /             
Listing 4.3
A liveness probe with an initial delay: kubia-liveness-probe-initial-delay.yaml
The previous 
container terminated 
with an error and 
exited with code 137.
The container 
has been 
restarted once.
 
</data>
  <data key="d5">88
CHAPTER 4
Replication and other controllers: deploying managed pods
    Last State:         Terminated                         
      Reason:           Error                              
      Exit Code:        137                                
      Started:          Mon, 01 Jan 0001 00:00:00 +0000    
      Finished:         Sun, 14 May 2017 11:41:38 +0200    
    Ready:              True
    Restart Count:      1                                 
    Liveness:           http-get http://:8080/ delay=0s timeout=1s
                        period=10s #success=1 #failure=3
    ...
Events:
... Killing container with id docker://95246981:pod "kubia-liveness ..."
    container "kubia" is unhealthy, it will be killed and re-created.
You can see that the container is currently running, but it previously terminated
because of an error. The exit code was 137, which has a special meaning—it denotes
that the process was terminated by an external signal. The number 137 is a sum of two
numbers: 128+x, where x is the signal number sent to the process that caused it to ter-
minate. In the example, x equals 9, which is the number of the SIGKILL signal, mean-
ing the process was killed forcibly.
 The events listed at the bottom show why the container was killed—Kubernetes
detected the container was unhealthy, so it killed and re-created it. 
NOTE
When a container is killed, a completely new container is created—it’s
not the same container being restarted again.
4.1.4
Configuring additional properties of the liveness probe
You may have noticed that kubectl describe also displays additional information
about the liveness probe:
Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 
          ➥ #failure=3
Beside the liveness probe options you specified explicitly, you can also see additional
properties, such as delay, timeout, period, and so on. The delay=0s part shows that
the probing begins immediately after the container is started. The timeout is set to
only 1 second, so the container must return a response in 1 second or the probe is
counted as failed. The container is probed every 10 seconds (period=10s) and the
container is restarted after the probe fails three consecutive times (#failure=3). 
 These additional parameters can be customized when defining the probe. For
example, to set the initial delay, add the initialDelaySeconds property to the live-
ness probe as shown in the following listing.
   livenessProbe:          
     httpGet:              
       path: /             
Listing 4.3
A liveness probe with an initial delay: kubia-liveness-probe-initial-delay.yaml
The previous 
container terminated 
with an error and 
exited with code 137.
The container 
has been 
restarted once.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="121">
  <data key="d0">Page_121</data>
  <data key="d5">Page_121</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_32">
  <data key="d0">89
Keeping pods healthy
       port: 8080          
     initialDelaySeconds: 15   
If you don’t set the initial delay, the prober will start probing the container as soon as
it starts, which usually leads to the probe failing, because the app isn’t ready to start
receiving requests. If the number of failures exceeds the failure threshold, the con-
tainer is restarted before it’s even able to start responding to requests properly. 
TIP
Always remember to set an initial delay to account for your app’s startup
time.
I’ve seen this on many occasions and users were confused why their container was
being restarted. But if they’d used kubectl describe, they’d have seen that the con-
tainer terminated with exit code 137 or 143, telling them that the pod was terminated
externally. Additionally, the listing of the pod’s events would show that the container
was killed because of a failed liveness probe. If you see this happening at pod startup,
it’s because you failed to set initialDelaySeconds appropriately.
NOTE
Exit code 137 signals that the process was killed by an external signal
(exit code is 128 + 9 (SIGKILL). Likewise, exit code 143 corresponds to 128 +
15 (SIGTERM).
4.1.5
Creating effective liveness probes
For pods running in production, you should always define a liveness probe. Without
one, Kubernetes has no way of knowing whether your app is still alive or not. As long
as the process is still running, Kubernetes will consider the container to be healthy. 
WHAT A LIVENESS PROBE SHOULD CHECK
Your simplistic liveness probe simply checks if the server is responding. While this may
seem overly simple, even a liveness probe like this does wonders, because it causes the
container to be restarted if the web server running within the container stops
responding to HTTP requests. Compared to having no liveness probe, this is a major
improvement, and may be sufficient in most cases.
 But for a better liveness check, you’d configure the probe to perform requests on a
specific URL path (/health, for example) and have the app perform an internal sta-
tus check of all the vital components running inside the app to ensure none of them
has died or is unresponsive. 
TIP
Make sure the /health HTTP endpoint doesn’t require authentication;
otherwise the probe will always fail, causing your container to be restarted
indefinitely.
Be sure to check only the internals of the app and nothing influenced by an external
factor. For example, a frontend web server’s liveness probe shouldn’t return a failure
when the server can’t connect to the backend database. If the underlying cause is in
the database itself, restarting the web server container will not fix the problem.
Kubernetes will wait 15 seconds 
before executing the first probe.
 
</data>
  <data key="d5">89
Keeping pods healthy
       port: 8080          
     initialDelaySeconds: 15   
If you don’t set the initial delay, the prober will start probing the container as soon as
it starts, which usually leads to the probe failing, because the app isn’t ready to start
receiving requests. If the number of failures exceeds the failure threshold, the con-
tainer is restarted before it’s even able to start responding to requests properly. 
TIP
Always remember to set an initial delay to account for your app’s startup
time.
I’ve seen this on many occasions and users were confused why their container was
being restarted. But if they’d used kubectl describe, they’d have seen that the con-
tainer terminated with exit code 137 or 143, telling them that the pod was terminated
externally. Additionally, the listing of the pod’s events would show that the container
was killed because of a failed liveness probe. If you see this happening at pod startup,
it’s because you failed to set initialDelaySeconds appropriately.
NOTE
Exit code 137 signals that the process was killed by an external signal
(exit code is 128 + 9 (SIGKILL). Likewise, exit code 143 corresponds to 128 +
15 (SIGTERM).
4.1.5
Creating effective liveness probes
For pods running in production, you should always define a liveness probe. Without
one, Kubernetes has no way of knowing whether your app is still alive or not. As long
as the process is still running, Kubernetes will consider the container to be healthy. 
WHAT A LIVENESS PROBE SHOULD CHECK
Your simplistic liveness probe simply checks if the server is responding. While this may
seem overly simple, even a liveness probe like this does wonders, because it causes the
container to be restarted if the web server running within the container stops
responding to HTTP requests. Compared to having no liveness probe, this is a major
improvement, and may be sufficient in most cases.
 But for a better liveness check, you’d configure the probe to perform requests on a
specific URL path (/health, for example) and have the app perform an internal sta-
tus check of all the vital components running inside the app to ensure none of them
has died or is unresponsive. 
TIP
Make sure the /health HTTP endpoint doesn’t require authentication;
otherwise the probe will always fail, causing your container to be restarted
indefinitely.
Be sure to check only the internals of the app and nothing influenced by an external
factor. For example, a frontend web server’s liveness probe shouldn’t return a failure
when the server can’t connect to the backend database. If the underlying cause is in
the database itself, restarting the web server container will not fix the problem.
Kubernetes will wait 15 seconds 
before executing the first probe.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="122">
  <data key="d0">Page_122</data>
  <data key="d5">Page_122</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_33">
  <data key="d0">90
CHAPTER 4
Replication and other controllers: deploying managed pods
Because the liveness probe will fail again, you’ll end up with the container restarting
repeatedly until the database becomes accessible again. 
KEEPING PROBES LIGHT
Liveness probes shouldn’t use too many computational resources and shouldn’t take
too long to complete. By default, the probes are executed relatively often and are
only allowed one second to complete. Having a probe that does heavy lifting can slow
down your container considerably. Later in the book, you’ll also learn about how to
limit CPU time available to a container. The probe’s CPU time is counted in the con-
tainer’s CPU time quota, so having a heavyweight liveness probe will reduce the CPU
time available to the main application processes.
TIP
If you’re running a Java app in your container, be sure to use an HTTP
GET liveness probe instead of an Exec probe, where you spin up a whole new
JVM to get the liveness information. The same goes for any JVM-based or sim-
ilar applications, whose start-up procedure requires considerable computa-
tional resources.
DON’T BOTHER IMPLEMENTING RETRY LOOPS IN YOUR PROBES
You’ve already seen that the failure threshold for the probe is configurable and usu-
ally the probe must fail multiple times before the container is killed. But even if you
set the failure threshold to 1, Kubernetes will retry the probe several times before con-
sidering it a single failed attempt. Therefore, implementing your own retry loop into
the probe is wasted effort.
LIVENESS PROBE WRAP-UP
You now understand that Kubernetes keeps your containers running by restarting
them if they crash or if their liveness probes fail. This job is performed by the Kubelet
on the node hosting the pod—the Kubernetes Control Plane components running on
the master(s) have no part in this process. 
 But if the node itself crashes, it’s the Control Plane that must create replacements for
all the pods that went down with the node. It doesn’t do that for pods that you create
directly. Those pods aren’t managed by anything except by the Kubelet, but because the
Kubelet runs on the node itself, it can’t do anything if the node fails. 
 To make sure your app is restarted on another node, you need to have the pod
managed by a ReplicationController or similar mechanism, which we’ll discuss in the
rest of this chapter. 
4.2
Introducing ReplicationControllers
A ReplicationController is a Kubernetes resource that ensures its pods are always
kept running. If the pod disappears for any reason, such as in the event of a node
disappearing from the cluster or because the pod was evicted from the node, the
ReplicationController notices the missing pod and creates a replacement pod. 
 Figure 4.1 shows what happens when a node goes down and takes two pods with it.
Pod A was created directly and is therefore an unmanaged pod, while pod B is managed
 
</data>
  <data key="d5">90
CHAPTER 4
Replication and other controllers: deploying managed pods
Because the liveness probe will fail again, you’ll end up with the container restarting
repeatedly until the database becomes accessible again. 
KEEPING PROBES LIGHT
Liveness probes shouldn’t use too many computational resources and shouldn’t take
too long to complete. By default, the probes are executed relatively often and are
only allowed one second to complete. Having a probe that does heavy lifting can slow
down your container considerably. Later in the book, you’ll also learn about how to
limit CPU time available to a container. The probe’s CPU time is counted in the con-
tainer’s CPU time quota, so having a heavyweight liveness probe will reduce the CPU
time available to the main application processes.
TIP
If you’re running a Java app in your container, be sure to use an HTTP
GET liveness probe instead of an Exec probe, where you spin up a whole new
JVM to get the liveness information. The same goes for any JVM-based or sim-
ilar applications, whose start-up procedure requires considerable computa-
tional resources.
DON’T BOTHER IMPLEMENTING RETRY LOOPS IN YOUR PROBES
You’ve already seen that the failure threshold for the probe is configurable and usu-
ally the probe must fail multiple times before the container is killed. But even if you
set the failure threshold to 1, Kubernetes will retry the probe several times before con-
sidering it a single failed attempt. Therefore, implementing your own retry loop into
the probe is wasted effort.
LIVENESS PROBE WRAP-UP
You now understand that Kubernetes keeps your containers running by restarting
them if they crash or if their liveness probes fail. This job is performed by the Kubelet
on the node hosting the pod—the Kubernetes Control Plane components running on
the master(s) have no part in this process. 
 But if the node itself crashes, it’s the Control Plane that must create replacements for
all the pods that went down with the node. It doesn’t do that for pods that you create
directly. Those pods aren’t managed by anything except by the Kubelet, but because the
Kubelet runs on the node itself, it can’t do anything if the node fails. 
 To make sure your app is restarted on another node, you need to have the pod
managed by a ReplicationController or similar mechanism, which we’ll discuss in the
rest of this chapter. 
4.2
Introducing ReplicationControllers
A ReplicationController is a Kubernetes resource that ensures its pods are always
kept running. If the pod disappears for any reason, such as in the event of a node
disappearing from the cluster or because the pod was evicted from the node, the
ReplicationController notices the missing pod and creates a replacement pod. 
 Figure 4.1 shows what happens when a node goes down and takes two pods with it.
Pod A was created directly and is therefore an unmanaged pod, while pod B is managed
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="123">
  <data key="d0">Page_123</data>
  <data key="d5">Page_123</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_34">
  <data key="d0">91
Introducing ReplicationControllers
by a ReplicationController. After the node fails, the ReplicationController creates a
new pod (pod B2) to replace the missing pod B, whereas pod A is lost completely—
nothing will ever recreate it.
 The ReplicationController in the figure manages only a single pod, but Replication-
Controllers, in general, are meant to create and manage multiple copies (replicas) of a
pod. That’s where ReplicationControllers got their name from. 
4.2.1
The operation of a ReplicationController
A ReplicationController constantly monitors the list of running pods and makes sure
the actual number of pods of a “type” always matches the desired number. If too few
such pods are running, it creates new replicas from a pod template. If too many such
pods are running, it removes the excess replicas. 
 You might be wondering how there can be more than the desired number of repli-
cas. This can happen for a few reasons: 
Someone creates a pod of the same type manually.
Someone changes an existing pod’s “type.”
Someone decreases the desired number of pods, and so on.
Node 1
Node 1 fails
Pod A
Pod B
Node 2
Various
other pods
Creates and
manages
Node 1
Pod A
Pod B
Node 2
Various
other pods
ReplicationController
ReplicationController
Pod A goes down with Node 1 and is
not recreated, because there is no
ReplicationController overseeing it.
RC notices pod B is
missing and creates
a new pod instance.
Pod B2
Figure 4.1
When a node fails, only pods backed by a ReplicationController are recreated.
 
</data>
  <data key="d5">91
Introducing ReplicationControllers
by a ReplicationController. After the node fails, the ReplicationController creates a
new pod (pod B2) to replace the missing pod B, whereas pod A is lost completely—
nothing will ever recreate it.
 The ReplicationController in the figure manages only a single pod, but Replication-
Controllers, in general, are meant to create and manage multiple copies (replicas) of a
pod. That’s where ReplicationControllers got their name from. 
4.2.1
The operation of a ReplicationController
A ReplicationController constantly monitors the list of running pods and makes sure
the actual number of pods of a “type” always matches the desired number. If too few
such pods are running, it creates new replicas from a pod template. If too many such
pods are running, it removes the excess replicas. 
 You might be wondering how there can be more than the desired number of repli-
cas. This can happen for a few reasons: 
Someone creates a pod of the same type manually.
Someone changes an existing pod’s “type.”
Someone decreases the desired number of pods, and so on.
Node 1
Node 1 fails
Pod A
Pod B
Node 2
Various
other pods
Creates and
manages
Node 1
Pod A
Pod B
Node 2
Various
other pods
ReplicationController
ReplicationController
Pod A goes down with Node 1 and is
not recreated, because there is no
ReplicationController overseeing it.
RC notices pod B is
missing and creates
a new pod instance.
Pod B2
Figure 4.1
When a node fails, only pods backed by a ReplicationController are recreated.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="124">
  <data key="d0">Page_124</data>
  <data key="d5">Page_124</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_35">
  <data key="d0">92
CHAPTER 4
Replication and other controllers: deploying managed pods
I’ve used the term pod “type” a few times. But no such thing exists. Replication-
Controllers don’t operate on pod types, but on sets of pods that match a certain label
selector (you learned about them in the previous chapter). 
INTRODUCING THE CONTROLLER’S RECONCILIATION LOOP
A ReplicationController’s job is to make sure that an exact number of pods always
matches its label selector. If it doesn’t, the ReplicationController takes the appropriate
action to reconcile the actual with the desired number. The operation of a Replication-
Controller is shown in figure 4.2.
UNDERSTANDING THE THREE PARTS OF A REPLICATIONCONTROLLER
A ReplicationController has three essential parts (also shown in figure 4.3):
A label selector, which determines what pods are in the ReplicationController’s scope
A replica count, which specifies the desired number of pods that should be running
A pod template, which is used when creating new pod replicas
Start
Compare
matched vs.
desired pod
count
Find pods
matching the
label selector
Create additional
pod(s) from
current template
Delete the
excess pod(s)
Too many
Just enough
Too few
Figure 4.2
A ReplicationController’s reconciliation loop
app: kubia
Pod
Pod template
ReplicationController: kubia
Pod selector:
app=kubia
Replicas: 3
Figure 4.3
The three key parts of a 
ReplicationController (pod selector, 
replica count, and pod template)
 
</data>
  <data key="d5">92
CHAPTER 4
Replication and other controllers: deploying managed pods
I’ve used the term pod “type” a few times. But no such thing exists. Replication-
Controllers don’t operate on pod types, but on sets of pods that match a certain label
selector (you learned about them in the previous chapter). 
INTRODUCING THE CONTROLLER’S RECONCILIATION LOOP
A ReplicationController’s job is to make sure that an exact number of pods always
matches its label selector. If it doesn’t, the ReplicationController takes the appropriate
action to reconcile the actual with the desired number. The operation of a Replication-
Controller is shown in figure 4.2.
UNDERSTANDING THE THREE PARTS OF A REPLICATIONCONTROLLER
A ReplicationController has three essential parts (also shown in figure 4.3):
A label selector, which determines what pods are in the ReplicationController’s scope
A replica count, which specifies the desired number of pods that should be running
A pod template, which is used when creating new pod replicas
Start
Compare
matched vs.
desired pod
count
Find pods
matching the
label selector
Create additional
pod(s) from
current template
Delete the
excess pod(s)
Too many
Just enough
Too few
Figure 4.2
A ReplicationController’s reconciliation loop
app: kubia
Pod
Pod template
ReplicationController: kubia
Pod selector:
app=kubia
Replicas: 3
Figure 4.3
The three key parts of a 
ReplicationController (pod selector, 
replica count, and pod template)
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="125">
  <data key="d0">Page_125</data>
  <data key="d5">Page_125</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_36">
  <data key="d0">93
Introducing ReplicationControllers
A ReplicationController’s replica count, the label selector, and even the pod tem-
plate can all be modified at any time, but only changes to the replica count affect
existing pods. 
UNDERSTANDING THE EFFECT OF CHANGING THE CONTROLLER’S LABEL SELECTOR OR POD TEMPLATE
Changes to the label selector and the pod template have no effect on existing pods.
Changing the label selector makes the existing pods fall out of the scope of the
ReplicationController, so the controller stops caring about them. ReplicationCon-
trollers also don’t care about the actual “contents” of its pods (the container images,
environment variables, and other things) after they create the pod. The template
therefore only affects new pods created by this ReplicationController. You can think
of it as a cookie cutter for cutting out new pods.
UNDERSTANDING THE BENEFITS OF USING A REPLICATIONCONTROLLER
Like many things in Kubernetes, a ReplicationController, although an incredibly sim-
ple concept, provides or enables the following powerful features:
It makes sure a pod (or multiple pod replicas) is always running by starting a
new pod when an existing one goes missing.
When a cluster node fails, it creates replacement replicas for all the pods that
were running on the failed node (those that were under the Replication-
Controller’s control).
It enables easy horizontal scaling of pods—both manual and automatic (see
horizontal pod auto-scaling in chapter 15).
NOTE
A pod instance is never relocated to another node. Instead, the
ReplicationController creates a completely new pod instance that has no rela-
tion to the instance it’s replacing. 
4.2.2
Creating a ReplicationController
Let’s look at how to create a ReplicationController and then see how it keeps your
pods running. Like pods and other Kubernetes resources, you create a Replication-
Controller by posting a JSON or YAML descriptor to the Kubernetes API server.
 You’re going to create a YAML file called kubia-rc.yaml for your Replication-
Controller, as shown in the following listing.
apiVersion: v1
kind: ReplicationController     
metadata:
  name: kubia                      
spec:
  replicas: 3                     
  selector:              
    app: kubia           
Listing 4.4
A YAML definition of a ReplicationController: kubia-rc.yaml
This manifest defines a 
ReplicationController (RC)
The name of this 
ReplicationController
The desired number 
of pod instances
The pod selector determining 
what pods the RC is operating on
 
</data>
  <data key="d5">93
Introducing ReplicationControllers
A ReplicationController’s replica count, the label selector, and even the pod tem-
plate can all be modified at any time, but only changes to the replica count affect
existing pods. 
UNDERSTANDING THE EFFECT OF CHANGING THE CONTROLLER’S LABEL SELECTOR OR POD TEMPLATE
Changes to the label selector and the pod template have no effect on existing pods.
Changing the label selector makes the existing pods fall out of the scope of the
ReplicationController, so the controller stops caring about them. ReplicationCon-
trollers also don’t care about the actual “contents” of its pods (the container images,
environment variables, and other things) after they create the pod. The template
therefore only affects new pods created by this ReplicationController. You can think
of it as a cookie cutter for cutting out new pods.
UNDERSTANDING THE BENEFITS OF USING A REPLICATIONCONTROLLER
Like many things in Kubernetes, a ReplicationController, although an incredibly sim-
ple concept, provides or enables the following powerful features:
It makes sure a pod (or multiple pod replicas) is always running by starting a
new pod when an existing one goes missing.
When a cluster node fails, it creates replacement replicas for all the pods that
were running on the failed node (those that were under the Replication-
Controller’s control).
It enables easy horizontal scaling of pods—both manual and automatic (see
horizontal pod auto-scaling in chapter 15).
NOTE
A pod instance is never relocated to another node. Instead, the
ReplicationController creates a completely new pod instance that has no rela-
tion to the instance it’s replacing. 
4.2.2
Creating a ReplicationController
Let’s look at how to create a ReplicationController and then see how it keeps your
pods running. Like pods and other Kubernetes resources, you create a Replication-
Controller by posting a JSON or YAML descriptor to the Kubernetes API server.
 You’re going to create a YAML file called kubia-rc.yaml for your Replication-
Controller, as shown in the following listing.
apiVersion: v1
kind: ReplicationController     
metadata:
  name: kubia                      
spec:
  replicas: 3                     
  selector:              
    app: kubia           
Listing 4.4
A YAML definition of a ReplicationController: kubia-rc.yaml
This manifest defines a 
ReplicationController (RC)
The name of this 
ReplicationController
The desired number 
of pod instances
The pod selector determining 
what pods the RC is operating on
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="126">
  <data key="d0">Page_126</data>
  <data key="d5">Page_126</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_37">
  <data key="d0">94
CHAPTER 4
Replication and other controllers: deploying managed pods
  template:                        
    metadata:                      
      labels:                      
        app: kubia                 
    spec:                          
      containers:                  
      - name: kubia                
        image: luksa/kubia         
        ports:                     
        - containerPort: 8080      
When you post the file to the API server, Kubernetes creates a new Replication-
Controller named kubia, which makes sure three pod instances always match the
label selector app=kubia. When there aren’t enough pods, new pods will be created
from the provided pod template. The contents of the template are almost identical to
the pod definition you created in the previous chapter. 
 The pod labels in the template must obviously match the label selector of the
ReplicationController; otherwise the controller would create new pods indefinitely,
because spinning up a new pod wouldn’t bring the actual replica count any closer to
the desired number of replicas. To prevent such scenarios, the API server verifies the
ReplicationController definition and will not accept it if it’s misconfigured.
 Not specifying the selector at all is also an option. In that case, it will be configured
automatically from the labels in the pod template. 
TIP
Don’t specify a pod selector when defining a ReplicationController. Let
Kubernetes extract it from the pod template. This will keep your YAML
shorter and simpler.
To create the ReplicationController, use the kubectl create command, which you
already know:
$ kubectl create -f kubia-rc.yaml
replicationcontroller "kubia" created
As soon as the ReplicationController is created, it goes to work. Let’s see what
it does.
4.2.3
Seeing the ReplicationController in action
Because no pods exist with the app=kubia label, the ReplicationController should
spin up three new pods from the pod template. List the pods to see if the Replication-
Controller has done what it’s supposed to:
$ kubectl get pods
NAME          READY     STATUS              RESTARTS   AGE
kubia-53thy   0/1       ContainerCreating   0          2s
kubia-k0xz6   0/1       ContainerCreating   0          2s
kubia-q3vkg   0/1       ContainerCreating   0          2s
The pod template 
for creating new 
pods
 
</data>
  <data key="d5">94
CHAPTER 4
Replication and other controllers: deploying managed pods
  template:                        
    metadata:                      
      labels:                      
        app: kubia                 
    spec:                          
      containers:                  
      - name: kubia                
        image: luksa/kubia         
        ports:                     
        - containerPort: 8080      
When you post the file to the API server, Kubernetes creates a new Replication-
Controller named kubia, which makes sure three pod instances always match the
label selector app=kubia. When there aren’t enough pods, new pods will be created
from the provided pod template. The contents of the template are almost identical to
the pod definition you created in the previous chapter. 
 The pod labels in the template must obviously match the label selector of the
ReplicationController; otherwise the controller would create new pods indefinitely,
because spinning up a new pod wouldn’t bring the actual replica count any closer to
the desired number of replicas. To prevent such scenarios, the API server verifies the
ReplicationController definition and will not accept it if it’s misconfigured.
 Not specifying the selector at all is also an option. In that case, it will be configured
automatically from the labels in the pod template. 
TIP
Don’t specify a pod selector when defining a ReplicationController. Let
Kubernetes extract it from the pod template. This will keep your YAML
shorter and simpler.
To create the ReplicationController, use the kubectl create command, which you
already know:
$ kubectl create -f kubia-rc.yaml
replicationcontroller "kubia" created
As soon as the ReplicationController is created, it goes to work. Let’s see what
it does.
4.2.3
Seeing the ReplicationController in action
Because no pods exist with the app=kubia label, the ReplicationController should
spin up three new pods from the pod template. List the pods to see if the Replication-
Controller has done what it’s supposed to:
$ kubectl get pods
NAME          READY     STATUS              RESTARTS   AGE
kubia-53thy   0/1       ContainerCreating   0          2s
kubia-k0xz6   0/1       ContainerCreating   0          2s
kubia-q3vkg   0/1       ContainerCreating   0          2s
The pod template 
for creating new 
pods
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="127">
  <data key="d0">Page_127</data>
  <data key="d5">Page_127</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_38">
  <data key="d0">95
Introducing ReplicationControllers
Indeed, it has! You wanted three pods, and it created three pods. It’s now managing
those three pods. Next you’ll mess with them a little to see how the Replication-
Controller responds. 
SEEING THE REPLICATIONCONTROLLER RESPOND TO A DELETED POD
First, you’ll delete one of the pods manually to see how the ReplicationController spins
up a new one immediately, bringing the number of matching pods back to three:
$ kubectl delete pod kubia-53thy
pod "kubia-53thy" deleted
Listing the pods again shows four of them, because the one you deleted is terminat-
ing, and a new pod has already been created:
$ kubectl get pods
NAME          READY     STATUS              RESTARTS   AGE
kubia-53thy   1/1       Terminating         0          3m
kubia-oini2   0/1       ContainerCreating   0          2s
kubia-k0xz6   1/1       Running             0          3m
kubia-q3vkg   1/1       Running             0          3m
The ReplicationController has done its job again. It’s a nice little helper, isn’t it?
GETTING INFORMATION ABOUT A REPLICATIONCONTROLLER
Now, let’s see what information the kubectl get command shows for Replication-
Controllers:
$ kubectl get rc
NAME      DESIRED   CURRENT   READY     AGE
kubia     3         3         2         3m
NOTE
We’re using rc as a shorthand for replicationcontroller.
You see three columns showing the desired number of pods, the actual number of
pods, and how many of them are ready (you’ll learn what that means in the next chap-
ter, when we talk about readiness probes).
 You can see additional information about your ReplicationController with the
kubectl describe command, as shown in the following listing.
$ kubectl describe rc kubia
Name:           kubia
Namespace:      default
Selector:       app=kubia
Labels:         app=kubia
Annotations:    &lt;none&gt;
Replicas:       3 current / 3 desired               
Pods Status:    4 Running / 0 Waiting / 0 Succeeded / 0 Failed  
Pod Template:
  Labels:       app=kubia
  Containers:   ...
Listing 4.5
Displaying details of a ReplicationController with kubectl describe
The actual vs. the 
desired number of 
pod instances
Number of 
pod instances 
per pod 
status
 
</data>
  <data key="d5">95
Introducing ReplicationControllers
Indeed, it has! You wanted three pods, and it created three pods. It’s now managing
those three pods. Next you’ll mess with them a little to see how the Replication-
Controller responds. 
SEEING THE REPLICATIONCONTROLLER RESPOND TO A DELETED POD
First, you’ll delete one of the pods manually to see how the ReplicationController spins
up a new one immediately, bringing the number of matching pods back to three:
$ kubectl delete pod kubia-53thy
pod "kubia-53thy" deleted
Listing the pods again shows four of them, because the one you deleted is terminat-
ing, and a new pod has already been created:
$ kubectl get pods
NAME          READY     STATUS              RESTARTS   AGE
kubia-53thy   1/1       Terminating         0          3m
kubia-oini2   0/1       ContainerCreating   0          2s
kubia-k0xz6   1/1       Running             0          3m
kubia-q3vkg   1/1       Running             0          3m
The ReplicationController has done its job again. It’s a nice little helper, isn’t it?
GETTING INFORMATION ABOUT A REPLICATIONCONTROLLER
Now, let’s see what information the kubectl get command shows for Replication-
Controllers:
$ kubectl get rc
NAME      DESIRED   CURRENT   READY     AGE
kubia     3         3         2         3m
NOTE
We’re using rc as a shorthand for replicationcontroller.
You see three columns showing the desired number of pods, the actual number of
pods, and how many of them are ready (you’ll learn what that means in the next chap-
ter, when we talk about readiness probes).
 You can see additional information about your ReplicationController with the
kubectl describe command, as shown in the following listing.
$ kubectl describe rc kubia
Name:           kubia
Namespace:      default
Selector:       app=kubia
Labels:         app=kubia
Annotations:    &lt;none&gt;
Replicas:       3 current / 3 desired               
Pods Status:    4 Running / 0 Waiting / 0 Succeeded / 0 Failed  
Pod Template:
  Labels:       app=kubia
  Containers:   ...
Listing 4.5
Displaying details of a ReplicationController with kubectl describe
The actual vs. the 
desired number of 
pod instances
Number of 
pod instances 
per pod 
status
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="128">
  <data key="d0">Page_128</data>
  <data key="d5">Page_128</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_39">
  <data key="d0">96
CHAPTER 4
Replication and other controllers: deploying managed pods
  Volumes:      &lt;none&gt;
Events:                                                   
From                    Type      Reason           Message
----                    -------  ------            -------
replication-controller  Normal   SuccessfulCreate  Created pod: kubia-53thy
replication-controller  Normal   SuccessfulCreate  Created pod: kubia-k0xz6
replication-controller  Normal   SuccessfulCreate  Created pod: kubia-q3vkg
replication-controller  Normal   SuccessfulCreate  Created pod: kubia-oini2
The current number of replicas matches the desired number, because the controller
has already created a new pod. It shows four running pods because a pod that’s termi-
nating is still considered running, although it isn’t counted in the current replica count. 
 The list of events at the bottom shows the actions taken by the Replication-
Controller—it has created four pods so far.
UNDERSTANDING EXACTLY WHAT CAUSED THE CONTROLLER TO CREATE A NEW POD
The controller is responding to the deletion of a pod by creating a new replacement
pod (see figure 4.4). Well, technically, it isn’t responding to the deletion itself, but the
resulting state—the inadequate number of pods.
 While a ReplicationController is immediately notified about a pod being deleted
(the API server allows clients to watch for changes to resources and resource lists), that’s
not what causes it to create a replacement pod. The notification triggers the controller
to check the actual number of pods and take appropriate action.
The events 
related to this 
ReplicationController
Before deletion
After deletion
ReplicationController: kubia
Replicas: 3
Selector: app=kubia
app: kubia
Pod:
kubia-q3vkg
app: kubia
Pod:
kubia-oini2
[ContainerCreating]
[Terminating]
app: kubia
Pod:
kubia-k0xz6
app: kubia
Pod:
kubia-53thy
ReplicationController: kubia
Replicas: 3
Selector: app=kubia
app: kubia
Pod:
kubia-q3vkg
app: kubia
Pod:
kubia-k0xz6
app: kubia
Pod:
kubia-53thy
Delete kubia-53thy
Figure 4.4
If a pod disappears, the ReplicationController sees too few pods and creates a new replacement pod.
 
</data>
  <data key="d5">96
CHAPTER 4
Replication and other controllers: deploying managed pods
  Volumes:      &lt;none&gt;
Events:                                                   
From                    Type      Reason           Message
----                    -------  ------            -------
replication-controller  Normal   SuccessfulCreate  Created pod: kubia-53thy
replication-controller  Normal   SuccessfulCreate  Created pod: kubia-k0xz6
replication-controller  Normal   SuccessfulCreate  Created pod: kubia-q3vkg
replication-controller  Normal   SuccessfulCreate  Created pod: kubia-oini2
The current number of replicas matches the desired number, because the controller
has already created a new pod. It shows four running pods because a pod that’s termi-
nating is still considered running, although it isn’t counted in the current replica count. 
 The list of events at the bottom shows the actions taken by the Replication-
Controller—it has created four pods so far.
UNDERSTANDING EXACTLY WHAT CAUSED THE CONTROLLER TO CREATE A NEW POD
The controller is responding to the deletion of a pod by creating a new replacement
pod (see figure 4.4). Well, technically, it isn’t responding to the deletion itself, but the
resulting state—the inadequate number of pods.
 While a ReplicationController is immediately notified about a pod being deleted
(the API server allows clients to watch for changes to resources and resource lists), that’s
not what causes it to create a replacement pod. The notification triggers the controller
to check the actual number of pods and take appropriate action.
The events 
related to this 
ReplicationController
Before deletion
After deletion
ReplicationController: kubia
Replicas: 3
Selector: app=kubia
app: kubia
Pod:
kubia-q3vkg
app: kubia
Pod:
kubia-oini2
[ContainerCreating]
[Terminating]
app: kubia
Pod:
kubia-k0xz6
app: kubia
Pod:
kubia-53thy
ReplicationController: kubia
Replicas: 3
Selector: app=kubia
app: kubia
Pod:
kubia-q3vkg
app: kubia
Pod:
kubia-k0xz6
app: kubia
Pod:
kubia-53thy
Delete kubia-53thy
Figure 4.4
If a pod disappears, the ReplicationController sees too few pods and creates a new replacement pod.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="129">
  <data key="d0">Page_129</data>
  <data key="d5">Page_129</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_40">
  <data key="d0">97
Introducing ReplicationControllers
RESPONDING TO A NODE FAILURE
Seeing the ReplicationController respond to the manual deletion of a pod isn’t too
interesting, so let’s look at a better example. If you’re using Google Kubernetes Engine
to run these examples, you have a three-node Kubernetes cluster. You’re going to dis-
connect one of the nodes from the network to simulate a node failure.
NOTE
If you’re using Minikube, you can’t do this exercise, because you only
have one node that acts both as a master and a worker node.
If a node fails in the non-Kubernetes world, the ops team would need to migrate the
applications running on that node to other machines manually. Kubernetes, on the
other hand, does that automatically. Soon after the ReplicationController detects that
its pods are down, it will spin up new pods to replace them. 
 Let’s see this in action. You need to ssh into one of the nodes with the gcloud
compute ssh command and then shut down its network interface with sudo ifconfig
eth0 down, as shown in the following listing.
NOTE
Choose a node that runs at least one of your pods by listing pods with
the -o wide option.
$ gcloud compute ssh gke-kubia-default-pool-b46381f1-zwko
Enter passphrase for key '/home/luksa/.ssh/google_compute_engine':
Welcome to Kubernetes v1.6.4!
...
luksa@gke-kubia-default-pool-b46381f1-zwko ~ $ sudo ifconfig eth0 down
When you shut down the network interface, the ssh session will stop responding, so
you need to open up another terminal or hard-exit from the ssh session. In the new
terminal you can list the nodes to see if Kubernetes has detected that the node is
down. This takes a minute or so. Then, the node’s status is shown as NotReady:
$ kubectl get node
NAME                                   STATUS     AGE
gke-kubia-default-pool-b46381f1-opc5   Ready      5h
gke-kubia-default-pool-b46381f1-s8gj   Ready      5h
gke-kubia-default-pool-b46381f1-zwko   NotReady   5h    
If you list the pods now, you’ll still see the same three pods as before, because Kuber-
netes waits a while before rescheduling pods (in case the node is unreachable because
of a temporary network glitch or because the Kubelet is restarting). If the node stays
unreachable for several minutes, the status of the pods that were scheduled to that
node changes to Unknown. At that point, the ReplicationController will immediately
spin up a new pod. You can see this by listing the pods again:
Listing 4.6
Simulating a node failure by shutting down its network interface
Node isn’t ready, 
because it’s 
disconnected from 
the network
 
</data>
  <data key="d5">97
Introducing ReplicationControllers
RESPONDING TO A NODE FAILURE
Seeing the ReplicationController respond to the manual deletion of a pod isn’t too
interesting, so let’s look at a better example. If you’re using Google Kubernetes Engine
to run these examples, you have a three-node Kubernetes cluster. You’re going to dis-
connect one of the nodes from the network to simulate a node failure.
NOTE
If you’re using Minikube, you can’t do this exercise, because you only
have one node that acts both as a master and a worker node.
If a node fails in the non-Kubernetes world, the ops team would need to migrate the
applications running on that node to other machines manually. Kubernetes, on the
other hand, does that automatically. Soon after the ReplicationController detects that
its pods are down, it will spin up new pods to replace them. 
 Let’s see this in action. You need to ssh into one of the nodes with the gcloud
compute ssh command and then shut down its network interface with sudo ifconfig
eth0 down, as shown in the following listing.
NOTE
Choose a node that runs at least one of your pods by listing pods with
the -o wide option.
$ gcloud compute ssh gke-kubia-default-pool-b46381f1-zwko
Enter passphrase for key '/home/luksa/.ssh/google_compute_engine':
Welcome to Kubernetes v1.6.4!
...
luksa@gke-kubia-default-pool-b46381f1-zwko ~ $ sudo ifconfig eth0 down
When you shut down the network interface, the ssh session will stop responding, so
you need to open up another terminal or hard-exit from the ssh session. In the new
terminal you can list the nodes to see if Kubernetes has detected that the node is
down. This takes a minute or so. Then, the node’s status is shown as NotReady:
$ kubectl get node
NAME                                   STATUS     AGE
gke-kubia-default-pool-b46381f1-opc5   Ready      5h
gke-kubia-default-pool-b46381f1-s8gj   Ready      5h
gke-kubia-default-pool-b46381f1-zwko   NotReady   5h    
If you list the pods now, you’ll still see the same three pods as before, because Kuber-
netes waits a while before rescheduling pods (in case the node is unreachable because
of a temporary network glitch or because the Kubelet is restarting). If the node stays
unreachable for several minutes, the status of the pods that were scheduled to that
node changes to Unknown. At that point, the ReplicationController will immediately
spin up a new pod. You can see this by listing the pods again:
Listing 4.6
Simulating a node failure by shutting down its network interface
Node isn’t ready, 
because it’s 
disconnected from 
the network
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="130">
  <data key="d0">Page_130</data>
  <data key="d5">Page_130</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_41">
  <data key="d0">98
CHAPTER 4
Replication and other controllers: deploying managed pods
$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-oini2   1/1     Running   0          10m
kubia-k0xz6   1/1     Running   0          10m
kubia-q3vkg   1/1     Unknown   0          10m    
kubia-dmdck   1/1     Running   0          5s    
Looking at the age of the pods, you see that the kubia-dmdck pod is new. You again
have three pod instances running, which means the ReplicationController has again
done its job of bringing the actual state of the system to the desired state. 
 The same thing happens if a node fails (either breaks down or becomes unreach-
able). No immediate human intervention is necessary. The system heals itself
automatically. 
 To bring the node back, you need to reset it with the following command:
$ gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko
When the node boots up again, its status should return to Ready, and the pod whose
status was Unknown will be deleted.
4.2.4
Moving pods in and out of the scope of a ReplicationController
Pods created by a ReplicationController aren’t tied to the ReplicationController in
any way. At any moment, a ReplicationController manages pods that match its label
selector. By changing a pod’s labels, it can be removed from or added to the scope
of a ReplicationController. It can even be moved from one ReplicationController to
another.
TIP
Although a pod isn’t tied to a ReplicationController, the pod does refer-
ence it in the metadata.ownerReferences field, which you can use to easily
find which ReplicationController a pod belongs to.
If you change a pod’s labels so they no longer match a ReplicationController’s label
selector, the pod becomes like any other manually created pod. It’s no longer man-
aged by anything. If the node running the pod fails, the pod is obviously not resched-
uled. But keep in mind that when you changed the pod’s labels, the replication
controller noticed one pod was missing and spun up a new pod to replace it.
 Let’s try this with your pods. Because your ReplicationController manages pods
that have the app=kubia label, you need to either remove this label or change its value
to move the pod out of the ReplicationController’s scope. Adding another label will
have no effect, because the ReplicationController doesn’t care if the pod has any addi-
tional labels. It only cares whether the pod has all the labels referenced in the label
selector. 
This pod’s status is 
unknown, because its 
node is unreachable.
This pod was created 
five seconds ago.
 
</data>
  <data key="d5">98
CHAPTER 4
Replication and other controllers: deploying managed pods
$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-oini2   1/1     Running   0          10m
kubia-k0xz6   1/1     Running   0          10m
kubia-q3vkg   1/1     Unknown   0          10m    
kubia-dmdck   1/1     Running   0          5s    
Looking at the age of the pods, you see that the kubia-dmdck pod is new. You again
have three pod instances running, which means the ReplicationController has again
done its job of bringing the actual state of the system to the desired state. 
 The same thing happens if a node fails (either breaks down or becomes unreach-
able). No immediate human intervention is necessary. The system heals itself
automatically. 
 To bring the node back, you need to reset it with the following command:
$ gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko
When the node boots up again, its status should return to Ready, and the pod whose
status was Unknown will be deleted.
4.2.4
Moving pods in and out of the scope of a ReplicationController
Pods created by a ReplicationController aren’t tied to the ReplicationController in
any way. At any moment, a ReplicationController manages pods that match its label
selector. By changing a pod’s labels, it can be removed from or added to the scope
of a ReplicationController. It can even be moved from one ReplicationController to
another.
TIP
Although a pod isn’t tied to a ReplicationController, the pod does refer-
ence it in the metadata.ownerReferences field, which you can use to easily
find which ReplicationController a pod belongs to.
If you change a pod’s labels so they no longer match a ReplicationController’s label
selector, the pod becomes like any other manually created pod. It’s no longer man-
aged by anything. If the node running the pod fails, the pod is obviously not resched-
uled. But keep in mind that when you changed the pod’s labels, the replication
controller noticed one pod was missing and spun up a new pod to replace it.
 Let’s try this with your pods. Because your ReplicationController manages pods
that have the app=kubia label, you need to either remove this label or change its value
to move the pod out of the ReplicationController’s scope. Adding another label will
have no effect, because the ReplicationController doesn’t care if the pod has any addi-
tional labels. It only cares whether the pod has all the labels referenced in the label
selector. 
This pod’s status is 
unknown, because its 
node is unreachable.
This pod was created 
five seconds ago.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="131">
  <data key="d0">Page_131</data>
  <data key="d5">Page_131</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_42">
  <data key="d0">99
Introducing ReplicationControllers
ADDING LABELS TO PODS MANAGED BY A REPLICATIONCONTROLLER
Let’s confirm that a ReplicationController doesn’t care if you add additional labels to
its managed pods:
$ kubectl label pod kubia-dmdck type=special
pod "kubia-dmdck" labeled
$ kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-oini2   1/1     Running   0          11m   app=kubia
kubia-k0xz6   1/1     Running   0          11m   app=kubia
kubia-dmdck   1/1     Running   0          1m    app=kubia,type=special
You’ve added the type=special label to one of the pods. Listing all pods again shows
the same three pods as before, because no change occurred as far as the Replication-
Controller is concerned.
CHANGING THE LABELS OF A MANAGED POD
Now, you’ll change the app=kubia label to something else. This will make the pod no
longer match the ReplicationController’s label selector, leaving it to only match two
pods. The ReplicationController should therefore start a new pod to bring the num-
ber back to three:
$ kubectl label pod kubia-dmdck app=foo --overwrite
pod "kubia-dmdck" labeled
The --overwrite argument is necessary; otherwise kubectl will only print out a warn-
ing and won’t change the label, to prevent you from inadvertently changing an exist-
ing label’s value when your intent is to add a new one. 
 Listing all the pods again should now show four pods: 
$ kubectl get pods -L app
NAME         READY  STATUS             RESTARTS  AGE  APP
kubia-2qneh  0/1    ContainerCreating  0         2s   kubia   
kubia-oini2  1/1    Running            0         20m  kubia
kubia-k0xz6  1/1    Running            0         20m  kubia
kubia-dmdck  1/1    Running            0         10m  foo    
NOTE
You’re using the -L app option to display the app label in a column.
There, you now have four pods altogether: one that isn’t managed by your Replication-
Controller and three that are. Among them is the newly created pod.
 Figure 4.5 illustrates what happened when you changed the pod’s labels so they no
longer matched the ReplicationController’s pod selector. You can see your three pods
and your ReplicationController. After you change the pod’s label from app=kubia to
app=foo, the ReplicationController no longer cares about the pod. Because the con-
troller’s replica count is set to 3 and only two pods match the label selector, the
Newly created pod that replaces
the pod you removed from the
scope of the ReplicationController
Pod no longer 
managed by the 
ReplicationController
 
</data>
  <data key="d5">99
Introducing ReplicationControllers
ADDING LABELS TO PODS MANAGED BY A REPLICATIONCONTROLLER
Let’s confirm that a ReplicationController doesn’t care if you add additional labels to
its managed pods:
$ kubectl label pod kubia-dmdck type=special
pod "kubia-dmdck" labeled
$ kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-oini2   1/1     Running   0          11m   app=kubia
kubia-k0xz6   1/1     Running   0          11m   app=kubia
kubia-dmdck   1/1     Running   0          1m    app=kubia,type=special
You’ve added the type=special label to one of the pods. Listing all pods again shows
the same three pods as before, because no change occurred as far as the Replication-
Controller is concerned.
CHANGING THE LABELS OF A MANAGED POD
Now, you’ll change the app=kubia label to something else. This will make the pod no
longer match the ReplicationController’s label selector, leaving it to only match two
pods. The ReplicationController should therefore start a new pod to bring the num-
ber back to three:
$ kubectl label pod kubia-dmdck app=foo --overwrite
pod "kubia-dmdck" labeled
The --overwrite argument is necessary; otherwise kubectl will only print out a warn-
ing and won’t change the label, to prevent you from inadvertently changing an exist-
ing label’s value when your intent is to add a new one. 
 Listing all the pods again should now show four pods: 
$ kubectl get pods -L app
NAME         READY  STATUS             RESTARTS  AGE  APP
kubia-2qneh  0/1    ContainerCreating  0         2s   kubia   
kubia-oini2  1/1    Running            0         20m  kubia
kubia-k0xz6  1/1    Running            0         20m  kubia
kubia-dmdck  1/1    Running            0         10m  foo    
NOTE
You’re using the -L app option to display the app label in a column.
There, you now have four pods altogether: one that isn’t managed by your Replication-
Controller and three that are. Among them is the newly created pod.
 Figure 4.5 illustrates what happened when you changed the pod’s labels so they no
longer matched the ReplicationController’s pod selector. You can see your three pods
and your ReplicationController. After you change the pod’s label from app=kubia to
app=foo, the ReplicationController no longer cares about the pod. Because the con-
troller’s replica count is set to 3 and only two pods match the label selector, the
Newly created pod that replaces
the pod you removed from the
scope of the ReplicationController
Pod no longer 
managed by the 
ReplicationController
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="132">
  <data key="d0">Page_132</data>
  <data key="d5">Page_132</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_43">
  <data key="d0">100
CHAPTER 4
Replication and other controllers: deploying managed pods
ReplicationController spins up pod kubia-2qneh to bring the number back up to
three. Pod kubia-dmdck is now completely independent and will keep running until
you delete it manually (you can do that now, because you don’t need it anymore).
REMOVING PODS FROM CONTROLLERS IN PRACTICE
Removing a pod from the scope of the ReplicationController comes in handy when
you want to perform actions on a specific pod. For example, you might have a bug
that causes your pod to start behaving badly after a specific amount of time or a spe-
cific event. If you know a pod is malfunctioning, you can take it out of the Replication-
Controller’s scope, let the controller replace it with a new one, and then debug or
play with the pod in any way you want. Once you’re done, you delete the pod. 
CHANGING THE REPLICATIONCONTROLLER’S LABEL SELECTOR
As an exercise to see if you fully understand ReplicationControllers, what do you
think would happen if instead of changing the labels of a pod, you modified the
ReplicationController’s label selector? 
 If your answer is that it would make all the pods fall out of the scope of the
ReplicationController, which would result in it creating three new pods, you’re abso-
lutely right. And it shows that you understand how ReplicationControllers work. 
 Kubernetes does allow you to change a ReplicationController’s label selector, but
that’s not the case for the other resources that are covered in the second half of this
Initial state
After re-labelling
Re-label kubia-dmdck
app: kubia
Pod:
kubia-oini2
app: kubia
Pod:
kubia-2qneh
[ContainerCreating]
Pod:
kubia-dmdck
app: kubia
Pod:
kubia-k0xz6
app: kubia
type: special
type: special
app: foo
app: kubia
Pod:
kubia-dmdck
app: kubia
Pod:
kubia-k0xz6
ReplicationController: kubia
Replicas: 3
Selector: app=kubia
ReplicationController: kubia
Replicas: 3
Selector: app=kubia
Pod:
kubia-oini2
Figure 4.5
Removing a pod from the scope of a ReplicationController by changing its labels 
 
</data>
  <data key="d5">100
CHAPTER 4
Replication and other controllers: deploying managed pods
ReplicationController spins up pod kubia-2qneh to bring the number back up to
three. Pod kubia-dmdck is now completely independent and will keep running until
you delete it manually (you can do that now, because you don’t need it anymore).
REMOVING PODS FROM CONTROLLERS IN PRACTICE
Removing a pod from the scope of the ReplicationController comes in handy when
you want to perform actions on a specific pod. For example, you might have a bug
that causes your pod to start behaving badly after a specific amount of time or a spe-
cific event. If you know a pod is malfunctioning, you can take it out of the Replication-
Controller’s scope, let the controller replace it with a new one, and then debug or
play with the pod in any way you want. Once you’re done, you delete the pod. 
CHANGING THE REPLICATIONCONTROLLER’S LABEL SELECTOR
As an exercise to see if you fully understand ReplicationControllers, what do you
think would happen if instead of changing the labels of a pod, you modified the
ReplicationController’s label selector? 
 If your answer is that it would make all the pods fall out of the scope of the
ReplicationController, which would result in it creating three new pods, you’re abso-
lutely right. And it shows that you understand how ReplicationControllers work. 
 Kubernetes does allow you to change a ReplicationController’s label selector, but
that’s not the case for the other resources that are covered in the second half of this
Initial state
After re-labelling
Re-label kubia-dmdck
app: kubia
Pod:
kubia-oini2
app: kubia
Pod:
kubia-2qneh
[ContainerCreating]
Pod:
kubia-dmdck
app: kubia
Pod:
kubia-k0xz6
app: kubia
type: special
type: special
app: foo
app: kubia
Pod:
kubia-dmdck
app: kubia
Pod:
kubia-k0xz6
ReplicationController: kubia
Replicas: 3
Selector: app=kubia
ReplicationController: kubia
Replicas: 3
Selector: app=kubia
Pod:
kubia-oini2
Figure 4.5
Removing a pod from the scope of a ReplicationController by changing its labels 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="133">
  <data key="d0">Page_133</data>
  <data key="d5">Page_133</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_44">
  <data key="d0">101
Introducing ReplicationControllers
chapter and which are also used for managing pods. You’ll never change a controller’s
label selector, but you’ll regularly change its pod template. Let’s take a look at that.
4.2.5
Changing the pod template
A ReplicationController’s pod template can be modified at any time. Changing the pod
template is like replacing a cookie cutter with another one. It will only affect the cookies
you cut out afterward and will have no effect on the ones you’ve already cut (see figure
4.6). To modify the old pods, you’d need to delete them and let the Replication-
Controller replace them with new ones based on the new template.
As an exercise, you can try editing the ReplicationController and adding a label to the
pod template. You can edit the ReplicationController with the following command:
$ kubectl edit rc kubia
This will open the ReplicationController’s YAML definition in your default text editor.
Find the pod template section and add an additional label to the metadata. After you
save your changes and exit the editor, kubectl will update the ReplicationController
and print the following message:
replicationcontroller "kubia" edited
You can now list pods and their labels again and confirm that they haven’t changed.
But if you delete the pods and wait for their replacements to be created, you’ll see the
new label.
 Editing a ReplicationController like this to change the container image in the pod
template, deleting the existing pods, and letting them be replaced with new ones from
the new template could be used for upgrading pods, but you’ll learn a better way of
doing that in chapter 9. 
Replication
Controller
Replicas: 3
Template:
A
B
C
Replication
Controller
Replicas: 3
Template:
A
Replication
Controller
Replicas: 3
Template:
A
Replication
Controller
Replicas: 3
Template:
D
A
B
C
A
B
C
A
B
Change
template
Delete
a pod
RC creates
new pod
Figure 4.6
Changing a ReplicationController’s pod template only affects pods created afterward and has no 
effect on existing pods.
 
</data>
  <data key="d5">101
Introducing ReplicationControllers
chapter and which are also used for managing pods. You’ll never change a controller’s
label selector, but you’ll regularly change its pod template. Let’s take a look at that.
4.2.5
Changing the pod template
A ReplicationController’s pod template can be modified at any time. Changing the pod
template is like replacing a cookie cutter with another one. It will only affect the cookies
you cut out afterward and will have no effect on the ones you’ve already cut (see figure
4.6). To modify the old pods, you’d need to delete them and let the Replication-
Controller replace them with new ones based on the new template.
As an exercise, you can try editing the ReplicationController and adding a label to the
pod template. You can edit the ReplicationController with the following command:
$ kubectl edit rc kubia
This will open the ReplicationController’s YAML definition in your default text editor.
Find the pod template section and add an additional label to the metadata. After you
save your changes and exit the editor, kubectl will update the ReplicationController
and print the following message:
replicationcontroller "kubia" edited
You can now list pods and their labels again and confirm that they haven’t changed.
But if you delete the pods and wait for their replacements to be created, you’ll see the
new label.
 Editing a ReplicationController like this to change the container image in the pod
template, deleting the existing pods, and letting them be replaced with new ones from
the new template could be used for upgrading pods, but you’ll learn a better way of
doing that in chapter 9. 
Replication
Controller
Replicas: 3
Template:
A
B
C
Replication
Controller
Replicas: 3
Template:
A
Replication
Controller
Replicas: 3
Template:
A
Replication
Controller
Replicas: 3
Template:
D
A
B
C
A
B
C
A
B
Change
template
Delete
a pod
RC creates
new pod
Figure 4.6
Changing a ReplicationController’s pod template only affects pods created afterward and has no 
effect on existing pods.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="134">
  <data key="d0">Page_134</data>
  <data key="d5">Page_134</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_45">
  <data key="d0">102
CHAPTER 4
Replication and other controllers: deploying managed pods
4.2.6
Horizontally scaling pods
You’ve seen how ReplicationControllers make sure a specific number of pod instances
is always running. Because it’s incredibly simple to change the desired number of rep-
licas, this also means scaling pods horizontally is trivial. 
 Scaling the number of pods up or down is as easy as changing the value of the rep-
licas field in the ReplicationController resource. After the change, the Replication-
Controller will either see too many pods exist (when scaling down) and delete part of
them, or see too few of them (when scaling up) and create additional pods. 
SCALING UP A REPLICATIONCONTROLLER
Your ReplicationController has been keeping three instances of your pod running.
You’re going to scale that number up to 10 now. As you may remember, you’ve
already scaled a ReplicationController in chapter 2. You could use the same com-
mand as before:
$ kubectl scale rc kubia --replicas=10
But you’ll do it differently this time. 
SCALING A REPLICATIONCONTROLLER BY EDITING ITS DEFINITION
Instead of using the kubectl scale command, you’re going to scale it in a declarative
way by editing the ReplicationController’s definition:
$ kubectl edit rc kubia
When the text editor opens, find the spec.replicas field and change its value to 10,
as shown in the following listing.
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving 
# this file will be reopened with the relevant failures.
apiVersion: v1
kind: ReplicationController
Configuring kubectl edit to use a different text editor
You can tell kubectl to use a text editor of your choice by setting the KUBE_EDITOR
environment variable. For example, if you’d like to use nano for editing Kubernetes
resources, execute the following command (or put it into your ~/.bashrc or an
equivalent file):
export KUBE_EDITOR="/usr/bin/nano"
If the KUBE_EDITOR environment variable isn’t set, kubectl edit falls back to using
the default editor, usually configured through the EDITOR environment variable.
Listing 4.7
Editing the RC in a text editor by running kubectl edit
 
</data>
  <data key="d5">102
CHAPTER 4
Replication and other controllers: deploying managed pods
4.2.6
Horizontally scaling pods
You’ve seen how ReplicationControllers make sure a specific number of pod instances
is always running. Because it’s incredibly simple to change the desired number of rep-
licas, this also means scaling pods horizontally is trivial. 
 Scaling the number of pods up or down is as easy as changing the value of the rep-
licas field in the ReplicationController resource. After the change, the Replication-
Controller will either see too many pods exist (when scaling down) and delete part of
them, or see too few of them (when scaling up) and create additional pods. 
SCALING UP A REPLICATIONCONTROLLER
Your ReplicationController has been keeping three instances of your pod running.
You’re going to scale that number up to 10 now. As you may remember, you’ve
already scaled a ReplicationController in chapter 2. You could use the same com-
mand as before:
$ kubectl scale rc kubia --replicas=10
But you’ll do it differently this time. 
SCALING A REPLICATIONCONTROLLER BY EDITING ITS DEFINITION
Instead of using the kubectl scale command, you’re going to scale it in a declarative
way by editing the ReplicationController’s definition:
$ kubectl edit rc kubia
When the text editor opens, find the spec.replicas field and change its value to 10,
as shown in the following listing.
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving 
# this file will be reopened with the relevant failures.
apiVersion: v1
kind: ReplicationController
Configuring kubectl edit to use a different text editor
You can tell kubectl to use a text editor of your choice by setting the KUBE_EDITOR
environment variable. For example, if you’d like to use nano for editing Kubernetes
resources, execute the following command (or put it into your ~/.bashrc or an
equivalent file):
export KUBE_EDITOR="/usr/bin/nano"
If the KUBE_EDITOR environment variable isn’t set, kubectl edit falls back to using
the default editor, usually configured through the EDITOR environment variable.
Listing 4.7
Editing the RC in a text editor by running kubectl edit
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="135">
  <data key="d0">Page_135</data>
  <data key="d5">Page_135</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_46">
  <data key="d0">103
Introducing ReplicationControllers
metadata:
  ...
spec:
  replicas: 3        
  selector:
    app: kubia
  ...
When you save the file and close the editor, the ReplicationController is updated and
it immediately scales the number of pods to 10:
$ kubectl get rc
NAME      DESIRED   CURRENT   READY     AGE
kubia     10        10        4         21m
There you go. If the kubectl scale command makes it look as though you’re telling
Kubernetes exactly what to do, it’s now much clearer that you’re making a declarative
change to the desired state of the ReplicationController and not telling Kubernetes to
do something.
SCALING DOWN WITH THE KUBECTL SCALE COMMAND
Now scale back down to 3. You can use the kubectl scale command:
$ kubectl scale rc kubia --replicas=3
All this command does is modify the spec.replicas field of the ReplicationController’s
definition—like when you changed it through kubectl edit. 
UNDERSTANDING THE DECLARATIVE APPROACH TO SCALING
Horizontally scaling pods in Kubernetes is a matter of stating your desire: “I want to
have x number of instances running.” You’re not telling Kubernetes what or how to do
it. You’re just specifying the desired state. 
 This declarative approach makes interacting with a Kubernetes cluster easy. Imag-
ine if you had to manually determine the current number of running instances and
then explicitly tell Kubernetes how many additional instances to run. That’s more
work and is much more error-prone. Changing a simple number is much easier, and
in chapter 15, you’ll learn that even that can be done by Kubernetes itself if you
enable horizontal pod auto-scaling. 
4.2.7
Deleting a ReplicationController
When you delete a ReplicationController through kubectl delete, the pods are also
deleted. But because pods created by a ReplicationController aren’t an integral part
of the ReplicationController, and are only managed by it, you can delete only the
ReplicationController and leave the pods running, as shown in figure 4.7.
 This may be useful when you initially have a set of pods managed by a Replication-
Controller, and then decide to replace the ReplicationController with a ReplicaSet,
for example (you’ll learn about them next.). You can do this without affecting the
Change the number 3 
to number 10 in 
this line.
 
</data>
  <data key="d5">103
Introducing ReplicationControllers
metadata:
  ...
spec:
  replicas: 3        
  selector:
    app: kubia
  ...
When you save the file and close the editor, the ReplicationController is updated and
it immediately scales the number of pods to 10:
$ kubectl get rc
NAME      DESIRED   CURRENT   READY     AGE
kubia     10        10        4         21m
There you go. If the kubectl scale command makes it look as though you’re telling
Kubernetes exactly what to do, it’s now much clearer that you’re making a declarative
change to the desired state of the ReplicationController and not telling Kubernetes to
do something.
SCALING DOWN WITH THE KUBECTL SCALE COMMAND
Now scale back down to 3. You can use the kubectl scale command:
$ kubectl scale rc kubia --replicas=3
All this command does is modify the spec.replicas field of the ReplicationController’s
definition—like when you changed it through kubectl edit. 
UNDERSTANDING THE DECLARATIVE APPROACH TO SCALING
Horizontally scaling pods in Kubernetes is a matter of stating your desire: “I want to
have x number of instances running.” You’re not telling Kubernetes what or how to do
it. You’re just specifying the desired state. 
 This declarative approach makes interacting with a Kubernetes cluster easy. Imag-
ine if you had to manually determine the current number of running instances and
then explicitly tell Kubernetes how many additional instances to run. That’s more
work and is much more error-prone. Changing a simple number is much easier, and
in chapter 15, you’ll learn that even that can be done by Kubernetes itself if you
enable horizontal pod auto-scaling. 
4.2.7
Deleting a ReplicationController
When you delete a ReplicationController through kubectl delete, the pods are also
deleted. But because pods created by a ReplicationController aren’t an integral part
of the ReplicationController, and are only managed by it, you can delete only the
ReplicationController and leave the pods running, as shown in figure 4.7.
 This may be useful when you initially have a set of pods managed by a Replication-
Controller, and then decide to replace the ReplicationController with a ReplicaSet,
for example (you’ll learn about them next.). You can do this without affecting the
Change the number 3 
to number 10 in 
this line.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="136">
  <data key="d0">Page_136</data>
  <data key="d5">Page_136</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_47">
  <data key="d0">104
CHAPTER 4
Replication and other controllers: deploying managed pods
pods and keep them running without interruption while you replace the Replication-
Controller that manages them. 
 When deleting a ReplicationController with kubectl delete, you can keep its
pods running by passing the --cascade=false option to the command. Try that now:
$ kubectl delete rc kubia --cascade=false
replicationcontroller "kubia" deleted
You’ve deleted the ReplicationController so the pods are on their own. They are no
longer managed. But you can always create a new ReplicationController with the
proper label selector and make them managed again.
4.3
Using ReplicaSets instead of ReplicationControllers
Initially, ReplicationControllers were the only Kubernetes component for replicating
pods and rescheduling them when nodes failed. Later, a similar resource called a
ReplicaSet was introduced. It’s a new generation of ReplicationController and
replaces it completely (ReplicationControllers will eventually be deprecated). 
 You could have started this chapter by creating a ReplicaSet instead of a Replication-
Controller, but I felt it would be a good idea to start with what was initially available in
Kubernetes. Plus, you’ll still see ReplicationControllers used in the wild, so it’s good
for you to know about them. That said, you should always create ReplicaSets instead
of ReplicationControllers from now on. They’re almost identical, so you shouldn’t
have any trouble using them instead. 
Before the RC deletion
After the RC deletion
Delete RC
Pod:
kubia-q3vkg
Pod:
kubia-53thy
Pod:
kubia-k0xz6
Pod:
kubia-q3vkg
Pod:
kubia-53thy
Pod:
kubia-k0xz6
ReplicationController: kubia
Replicas: 3
Selector: app=kubia
app: kubia
app: kubia
app: kubia
app: kubia
app: kubia
app: kubia
Figure 4.7
Deleting a replication controller with --cascade=false leaves pods unmanaged.
 
</data>
  <data key="d5">104
CHAPTER 4
Replication and other controllers: deploying managed pods
pods and keep them running without interruption while you replace the Replication-
Controller that manages them. 
 When deleting a ReplicationController with kubectl delete, you can keep its
pods running by passing the --cascade=false option to the command. Try that now:
$ kubectl delete rc kubia --cascade=false
replicationcontroller "kubia" deleted
You’ve deleted the ReplicationController so the pods are on their own. They are no
longer managed. But you can always create a new ReplicationController with the
proper label selector and make them managed again.
4.3
Using ReplicaSets instead of ReplicationControllers
Initially, ReplicationControllers were the only Kubernetes component for replicating
pods and rescheduling them when nodes failed. Later, a similar resource called a
ReplicaSet was introduced. It’s a new generation of ReplicationController and
replaces it completely (ReplicationControllers will eventually be deprecated). 
 You could have started this chapter by creating a ReplicaSet instead of a Replication-
Controller, but I felt it would be a good idea to start with what was initially available in
Kubernetes. Plus, you’ll still see ReplicationControllers used in the wild, so it’s good
for you to know about them. That said, you should always create ReplicaSets instead
of ReplicationControllers from now on. They’re almost identical, so you shouldn’t
have any trouble using them instead. 
Before the RC deletion
After the RC deletion
Delete RC
Pod:
kubia-q3vkg
Pod:
kubia-53thy
Pod:
kubia-k0xz6
Pod:
kubia-q3vkg
Pod:
kubia-53thy
Pod:
kubia-k0xz6
ReplicationController: kubia
Replicas: 3
Selector: app=kubia
app: kubia
app: kubia
app: kubia
app: kubia
app: kubia
app: kubia
Figure 4.7
Deleting a replication controller with --cascade=false leaves pods unmanaged.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="137">
  <data key="d0">Page_137</data>
  <data key="d5">Page_137</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_48">
  <data key="d0">105
Using ReplicaSets instead of ReplicationControllers
 You usually won’t create them directly, but instead have them created automati-
cally when you create the higher-level Deployment resource, which you’ll learn about
in chapter 9. In any case, you should understand ReplicaSets, so let’s see how they dif-
fer from ReplicationControllers.
4.3.1
Comparing a ReplicaSet to a ReplicationController
A ReplicaSet behaves exactly like a ReplicationController, but it has more expressive
pod selectors. Whereas a ReplicationController’s label selector only allows matching
pods that include a certain label, a ReplicaSet’s selector also allows matching pods
that lack a certain label or pods that include a certain label key, regardless of
its value.
 Also, for example, a single ReplicationController can’t match pods with the label
env=production and those with the label env=devel at the same time. It can only match
either pods with the env=production label or pods with the env=devel label. But a sin-
gle ReplicaSet can match both sets of pods and treat them as a single group. 
 Similarly, a ReplicationController can’t match pods based merely on the presence
of a label key, regardless of its value, whereas a ReplicaSet can. For example, a Replica-
Set can match all pods that include a label with the key env, whatever its actual value is
(you can think of it as env=*).
4.3.2
Defining a ReplicaSet
You’re going to create a ReplicaSet now to see how the orphaned pods that were cre-
ated by your ReplicationController and then abandoned earlier can now be adopted
by a ReplicaSet. First, you’ll rewrite your ReplicationController into a ReplicaSet by
creating a new file called kubia-replicaset.yaml with the contents in the following
listing.
apiVersion: apps/v1beta2      
kind: ReplicaSet                    
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:                 
      app: kubia                 
  template:                        
    metadata:                      
      labels:                      
        app: kubia                 
    spec:                          
      containers:                  
      - name: kubia                
        image: luksa/kubia         
Listing 4.8
A YAML definition of a ReplicaSet: kubia-replicaset.yaml
ReplicaSets aren’t part of the v1 
API, but belong to the apps API 
group and version v1beta2.
You’re using the simpler matchLabels 
selector here, which is much like a 
ReplicationController’s selector.
The template is 
the same as in the 
ReplicationController.
 
</data>
  <data key="d5">105
Using ReplicaSets instead of ReplicationControllers
 You usually won’t create them directly, but instead have them created automati-
cally when you create the higher-level Deployment resource, which you’ll learn about
in chapter 9. In any case, you should understand ReplicaSets, so let’s see how they dif-
fer from ReplicationControllers.
4.3.1
Comparing a ReplicaSet to a ReplicationController
A ReplicaSet behaves exactly like a ReplicationController, but it has more expressive
pod selectors. Whereas a ReplicationController’s label selector only allows matching
pods that include a certain label, a ReplicaSet’s selector also allows matching pods
that lack a certain label or pods that include a certain label key, regardless of
its value.
 Also, for example, a single ReplicationController can’t match pods with the label
env=production and those with the label env=devel at the same time. It can only match
either pods with the env=production label or pods with the env=devel label. But a sin-
gle ReplicaSet can match both sets of pods and treat them as a single group. 
 Similarly, a ReplicationController can’t match pods based merely on the presence
of a label key, regardless of its value, whereas a ReplicaSet can. For example, a Replica-
Set can match all pods that include a label with the key env, whatever its actual value is
(you can think of it as env=*).
4.3.2
Defining a ReplicaSet
You’re going to create a ReplicaSet now to see how the orphaned pods that were cre-
ated by your ReplicationController and then abandoned earlier can now be adopted
by a ReplicaSet. First, you’ll rewrite your ReplicationController into a ReplicaSet by
creating a new file called kubia-replicaset.yaml with the contents in the following
listing.
apiVersion: apps/v1beta2      
kind: ReplicaSet                    
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:                 
      app: kubia                 
  template:                        
    metadata:                      
      labels:                      
        app: kubia                 
    spec:                          
      containers:                  
      - name: kubia                
        image: luksa/kubia         
Listing 4.8
A YAML definition of a ReplicaSet: kubia-replicaset.yaml
ReplicaSets aren’t part of the v1 
API, but belong to the apps API 
group and version v1beta2.
You’re using the simpler matchLabels 
selector here, which is much like a 
ReplicationController’s selector.
The template is 
the same as in the 
ReplicationController.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="138">
  <data key="d0">Page_138</data>
  <data key="d5">Page_138</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_49">
  <data key="d0">106
CHAPTER 4
Replication and other controllers: deploying managed pods
The first thing to note is that ReplicaSets aren’t part of the v1 API, so you need to
ensure you specify the proper apiVersion when creating the resource. You’re creating a
resource of type ReplicaSet which has much the same contents as the Replication-
Controller you created earlier. 
 The only difference is in the selector. Instead of listing labels the pods need to
have directly under the selector property, you’re specifying them under selector
.matchLabels. This is the simpler (and less expressive) way of defining label selectors
in a ReplicaSet. Later, you’ll look at the more expressive option, as well.
Because you still have three pods matching the app=kubia selector running from ear-
lier, creating this ReplicaSet will not cause any new pods to be created. The ReplicaSet
will take those existing three pods under its wing. 
4.3.3
Creating and examining a ReplicaSet
Create the ReplicaSet from the YAML file with the kubectl create command. After
that, you can examine the ReplicaSet with kubectl get and kubectl describe:
$ kubectl get rs
NAME      DESIRED   CURRENT   READY     AGE
kubia     3         3         3         3s
TIP
Use rs shorthand, which stands for replicaset.
$ kubectl describe rs
Name:           kubia
Namespace:      default
Selector:       app=kubia
Labels:         app=kubia
Annotations:    &lt;none&gt;
Replicas:       3 current / 3 desired
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=kubia
About the API version attribute
This is your first opportunity to see that the apiVersion property specifies two things:
The API group (which is apps in this case)
The actual API version (v1beta2)
You’ll see throughout the book that certain Kubernetes resources are in what’s called
the core API group, which doesn’t need to be specified in the apiVersion field (you
just specify the version—for example, you’ve been using apiVersion: v1 when
defining Pod resources). Other resources, which were introduced in later Kubernetes
versions, are categorized into several API groups. Look at the inside of the book’s
covers to see all resources and their respective API groups.
 
</data>
  <data key="d5">106
CHAPTER 4
Replication and other controllers: deploying managed pods
The first thing to note is that ReplicaSets aren’t part of the v1 API, so you need to
ensure you specify the proper apiVersion when creating the resource. You’re creating a
resource of type ReplicaSet which has much the same contents as the Replication-
Controller you created earlier. 
 The only difference is in the selector. Instead of listing labels the pods need to
have directly under the selector property, you’re specifying them under selector
.matchLabels. This is the simpler (and less expressive) way of defining label selectors
in a ReplicaSet. Later, you’ll look at the more expressive option, as well.
Because you still have three pods matching the app=kubia selector running from ear-
lier, creating this ReplicaSet will not cause any new pods to be created. The ReplicaSet
will take those existing three pods under its wing. 
4.3.3
Creating and examining a ReplicaSet
Create the ReplicaSet from the YAML file with the kubectl create command. After
that, you can examine the ReplicaSet with kubectl get and kubectl describe:
$ kubectl get rs
NAME      DESIRED   CURRENT   READY     AGE
kubia     3         3         3         3s
TIP
Use rs shorthand, which stands for replicaset.
$ kubectl describe rs
Name:           kubia
Namespace:      default
Selector:       app=kubia
Labels:         app=kubia
Annotations:    &lt;none&gt;
Replicas:       3 current / 3 desired
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=kubia
About the API version attribute
This is your first opportunity to see that the apiVersion property specifies two things:
The API group (which is apps in this case)
The actual API version (v1beta2)
You’ll see throughout the book that certain Kubernetes resources are in what’s called
the core API group, which doesn’t need to be specified in the apiVersion field (you
just specify the version—for example, you’ve been using apiVersion: v1 when
defining Pod resources). Other resources, which were introduced in later Kubernetes
versions, are categorized into several API groups. Look at the inside of the book’s
covers to see all resources and their respective API groups.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="139">
  <data key="d0">Page_139</data>
  <data key="d5">Page_139</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_50">
  <data key="d0">107
Using ReplicaSets instead of ReplicationControllers
  Containers:   ...
  Volumes:      &lt;none&gt;
Events:         &lt;none&gt;
As you can see, the ReplicaSet isn’t any different from a ReplicationController. It’s
showing it has three replicas matching the selector. If you list all the pods, you’ll see
they’re still the same three pods you had before. The ReplicaSet didn’t create any new
ones. 
4.3.4
Using the ReplicaSet’s more expressive label selectors
The main improvements of ReplicaSets over ReplicationControllers are their more
expressive label selectors. You intentionally used the simpler matchLabels selector in
the first ReplicaSet example to see that ReplicaSets are no different from Replication-
Controllers. Now, you’ll rewrite the selector to use the more powerful matchExpressions
property, as shown in the following listing.
 selector:
   matchExpressions:                 
     - key: app           
       operator: In                  
       values:                       
         - kubia                     
NOTE
Only the selector is shown. You’ll find the whole ReplicaSet definition
in the book’s code archive.
You can add additional expressions to the selector. As in the example, each expression
must contain a key, an operator, and possibly (depending on the operator) a list of
values. You’ll see four valid operators:

In—Label’s value must match one of the specified values.

NotIn—Label’s value must not match any of the specified values.

Exists—Pod must include a label with the specified key (the value isn’t import-
ant). When using this operator, you shouldn’t specify the values field.

DoesNotExist—Pod must not include a label with the specified key. The values
property must not be specified.
If you specify multiple expressions, all those expressions must evaluate to true for the
selector to match a pod. If you specify both matchLabels and matchExpressions, all
the labels must match and all the expressions must evaluate to true for the pod to
match the selector.
Listing 4.9
A matchExpressions selector: kubia-replicaset-matchexpressions.yaml
This selector requires the pod to 
contain a label with the “app” key.
The label’s value 
must be “kubia”.
 
</data>
  <data key="d5">107
Using ReplicaSets instead of ReplicationControllers
  Containers:   ...
  Volumes:      &lt;none&gt;
Events:         &lt;none&gt;
As you can see, the ReplicaSet isn’t any different from a ReplicationController. It’s
showing it has three replicas matching the selector. If you list all the pods, you’ll see
they’re still the same three pods you had before. The ReplicaSet didn’t create any new
ones. 
4.3.4
Using the ReplicaSet’s more expressive label selectors
The main improvements of ReplicaSets over ReplicationControllers are their more
expressive label selectors. You intentionally used the simpler matchLabels selector in
the first ReplicaSet example to see that ReplicaSets are no different from Replication-
Controllers. Now, you’ll rewrite the selector to use the more powerful matchExpressions
property, as shown in the following listing.
 selector:
   matchExpressions:                 
     - key: app           
       operator: In                  
       values:                       
         - kubia                     
NOTE
Only the selector is shown. You’ll find the whole ReplicaSet definition
in the book’s code archive.
You can add additional expressions to the selector. As in the example, each expression
must contain a key, an operator, and possibly (depending on the operator) a list of
values. You’ll see four valid operators:

In—Label’s value must match one of the specified values.

NotIn—Label’s value must not match any of the specified values.

Exists—Pod must include a label with the specified key (the value isn’t import-
ant). When using this operator, you shouldn’t specify the values field.

DoesNotExist—Pod must not include a label with the specified key. The values
property must not be specified.
If you specify multiple expressions, all those expressions must evaluate to true for the
selector to match a pod. If you specify both matchLabels and matchExpressions, all
the labels must match and all the expressions must evaluate to true for the pod to
match the selector.
Listing 4.9
A matchExpressions selector: kubia-replicaset-matchexpressions.yaml
This selector requires the pod to 
contain a label with the “app” key.
The label’s value 
must be “kubia”.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="140">
  <data key="d0">Page_140</data>
  <data key="d5">Page_140</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_51">
  <data key="d0">108
CHAPTER 4
Replication and other controllers: deploying managed pods
4.3.5
Wrapping up ReplicaSets
This was a quick introduction to ReplicaSets as an alternative to ReplicationControllers.
Remember, always use them instead of ReplicationControllers, but you may still find
ReplicationControllers in other people’s deployments.
 Now, delete the ReplicaSet to clean up your cluster a little. You can delete the
ReplicaSet the same way you’d delete a ReplicationController:
$ kubectl delete rs kubia
replicaset "kubia" deleted
Deleting the ReplicaSet should delete all the pods. List the pods to confirm that’s
the case. 
4.4
Running exactly one pod on each node with 
DaemonSets
Both ReplicationControllers and ReplicaSets are used for running a specific number
of pods deployed anywhere in the Kubernetes cluster. But certain cases exist when you
want a pod to run on each and every node in the cluster (and each node needs to run
exactly one instance of the pod, as shown in figure 4.8).
 Those cases include infrastructure-related pods that perform system-level opera-
tions. For example, you’ll want to run a log collector and a resource monitor on every
node. Another good example is Kubernetes’ own kube-proxy process, which needs to
run on all nodes to make services work.
Node 1
Pod
Pod
Pod
ReplicaSet
Replicas: 5
Node 2
Pod
Pod
Node 3
Pod
DaemonSet
Exactly one replica
on each node
Node 4
Pod
Pod
Pod
Figure 4.8
DaemonSets run only a single pod replica on each node, whereas ReplicaSets 
scatter them around the whole cluster randomly. 
 
</data>
  <data key="d5">108
CHAPTER 4
Replication and other controllers: deploying managed pods
4.3.5
Wrapping up ReplicaSets
This was a quick introduction to ReplicaSets as an alternative to ReplicationControllers.
Remember, always use them instead of ReplicationControllers, but you may still find
ReplicationControllers in other people’s deployments.
 Now, delete the ReplicaSet to clean up your cluster a little. You can delete the
ReplicaSet the same way you’d delete a ReplicationController:
$ kubectl delete rs kubia
replicaset "kubia" deleted
Deleting the ReplicaSet should delete all the pods. List the pods to confirm that’s
the case. 
4.4
Running exactly one pod on each node with 
DaemonSets
Both ReplicationControllers and ReplicaSets are used for running a specific number
of pods deployed anywhere in the Kubernetes cluster. But certain cases exist when you
want a pod to run on each and every node in the cluster (and each node needs to run
exactly one instance of the pod, as shown in figure 4.8).
 Those cases include infrastructure-related pods that perform system-level opera-
tions. For example, you’ll want to run a log collector and a resource monitor on every
node. Another good example is Kubernetes’ own kube-proxy process, which needs to
run on all nodes to make services work.
Node 1
Pod
Pod
Pod
ReplicaSet
Replicas: 5
Node 2
Pod
Pod
Node 3
Pod
DaemonSet
Exactly one replica
on each node
Node 4
Pod
Pod
Pod
Figure 4.8
DaemonSets run only a single pod replica on each node, whereas ReplicaSets 
scatter them around the whole cluster randomly. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="141">
  <data key="d0">Page_141</data>
  <data key="d5">Page_141</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_52">
  <data key="d0">109
Running exactly one pod on each node with DaemonSets
Outside of Kubernetes, such processes would usually be started through system init
scripts or the systemd daemon during node boot up. On Kubernetes nodes, you can
still use systemd to run your system processes, but then you can’t take advantage of all
the features Kubernetes provides. 
4.4.1
Using a DaemonSet to run a pod on every node
To run a pod on all cluster nodes, you create a DaemonSet object, which is much
like a ReplicationController or a ReplicaSet, except that pods created by a Daemon-
Set already have a target node specified and skip the Kubernetes Scheduler. They
aren’t scattered around the cluster randomly. 
 A DaemonSet makes sure it creates as many pods as there are nodes and deploys
each one on its own node, as shown in figure 4.8.
 Whereas a ReplicaSet (or ReplicationController) makes sure that a desired num-
ber of pod replicas exist in the cluster, a DaemonSet doesn’t have any notion of a
desired replica count. It doesn’t need it because its job is to ensure that a pod match-
ing its pod selector is running on each node. 
 If a node goes down, the DaemonSet doesn’t cause the pod to be created else-
where. But when a new node is added to the cluster, the DaemonSet immediately
deploys a new pod instance to it. It also does the same if someone inadvertently
deletes one of the pods, leaving the node without the DaemonSet’s pod. Like a Replica-
Set, a DaemonSet creates the pod from the pod template configured in it.
4.4.2
Using a DaemonSet to run pods only on certain nodes
A DaemonSet deploys pods to all nodes in the cluster, unless you specify that the pods
should only run on a subset of all the nodes. This is done by specifying the node-
Selector property in the pod template, which is part of the DaemonSet definition
(similar to the pod template in a ReplicaSet or ReplicationController). 
 You’ve already used node selectors to deploy a pod onto specific nodes in chapter 3.
A node selector in a DaemonSet is similar—it defines the nodes the DaemonSet must
deploy its pods to. 
NOTE
Later in the book, you’ll learn that nodes can be made unschedulable,
preventing pods from being deployed to them. A DaemonSet will deploy pods
even to such nodes, because the unschedulable attribute is only used by the
Scheduler, whereas pods managed by a DaemonSet bypass the Scheduler
completely. This is usually desirable, because DaemonSets are meant to run
system services, which usually need to run even on unschedulable nodes.
EXPLAINING DAEMONSETS WITH AN EXAMPLE
Let’s imagine having a daemon called ssd-monitor that needs to run on all nodes
that contain a solid-state drive (SSD). You’ll create a DaemonSet that runs this dae-
mon on all nodes that are marked as having an SSD. The cluster administrators have
added the disk=ssd label to all such nodes, so you’ll create the DaemonSet with a
node selector that only selects nodes with that label, as shown in figure 4.9.
 
</data>
  <data key="d5">109
Running exactly one pod on each node with DaemonSets
Outside of Kubernetes, such processes would usually be started through system init
scripts or the systemd daemon during node boot up. On Kubernetes nodes, you can
still use systemd to run your system processes, but then you can’t take advantage of all
the features Kubernetes provides. 
4.4.1
Using a DaemonSet to run a pod on every node
To run a pod on all cluster nodes, you create a DaemonSet object, which is much
like a ReplicationController or a ReplicaSet, except that pods created by a Daemon-
Set already have a target node specified and skip the Kubernetes Scheduler. They
aren’t scattered around the cluster randomly. 
 A DaemonSet makes sure it creates as many pods as there are nodes and deploys
each one on its own node, as shown in figure 4.8.
 Whereas a ReplicaSet (or ReplicationController) makes sure that a desired num-
ber of pod replicas exist in the cluster, a DaemonSet doesn’t have any notion of a
desired replica count. It doesn’t need it because its job is to ensure that a pod match-
ing its pod selector is running on each node. 
 If a node goes down, the DaemonSet doesn’t cause the pod to be created else-
where. But when a new node is added to the cluster, the DaemonSet immediately
deploys a new pod instance to it. It also does the same if someone inadvertently
deletes one of the pods, leaving the node without the DaemonSet’s pod. Like a Replica-
Set, a DaemonSet creates the pod from the pod template configured in it.
4.4.2
Using a DaemonSet to run pods only on certain nodes
A DaemonSet deploys pods to all nodes in the cluster, unless you specify that the pods
should only run on a subset of all the nodes. This is done by specifying the node-
Selector property in the pod template, which is part of the DaemonSet definition
(similar to the pod template in a ReplicaSet or ReplicationController). 
 You’ve already used node selectors to deploy a pod onto specific nodes in chapter 3.
A node selector in a DaemonSet is similar—it defines the nodes the DaemonSet must
deploy its pods to. 
NOTE
Later in the book, you’ll learn that nodes can be made unschedulable,
preventing pods from being deployed to them. A DaemonSet will deploy pods
even to such nodes, because the unschedulable attribute is only used by the
Scheduler, whereas pods managed by a DaemonSet bypass the Scheduler
completely. This is usually desirable, because DaemonSets are meant to run
system services, which usually need to run even on unschedulable nodes.
EXPLAINING DAEMONSETS WITH AN EXAMPLE
Let’s imagine having a daemon called ssd-monitor that needs to run on all nodes
that contain a solid-state drive (SSD). You’ll create a DaemonSet that runs this dae-
mon on all nodes that are marked as having an SSD. The cluster administrators have
added the disk=ssd label to all such nodes, so you’ll create the DaemonSet with a
node selector that only selects nodes with that label, as shown in figure 4.9.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="142">
  <data key="d0">Page_142</data>
  <data key="d5">Page_142</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_53">
  <data key="d0">110
CHAPTER 4
Replication and other controllers: deploying managed pods
CREATING A DAEMONSET YAML DEFINITION
You’ll create a DaemonSet that runs a mock ssd-monitor process, which prints
“SSD OK” to the standard output every five seconds. I’ve already prepared the mock
container image and pushed it to Docker Hub, so you can use it instead of building
your own. Create the YAML for the DaemonSet, as shown in the following listing.
apiVersion: apps/v1beta2      
kind: DaemonSet                     
metadata:
  name: ssd-monitor
spec:                            
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:                
        disk: ssd                  
      containers:
      - name: main
        image: luksa/ssd-monitor
You’re defining a DaemonSet that will run a pod with a single container based on the
luksa/ssd-monitor container image. An instance of this pod will be created for each
node that has the disk=ssd label.
Listing 4.10
A YAML for a DaemonSet: ssd-monitor-daemonset.yaml
Node 1
Pod:
ssd-monitor
Node 2
Node 3
DaemonSet:
sssd-monitor
Node selector:
disk=ssd
Node 4
disk: ssd
disk: ssd
disk: ssd
Unschedulable
Pod:
ssd-monitor
Pod:
ssd-monitor
Figure 4.9
Using a DaemonSet with a node selector to deploy system pods only on certain 
nodes
DaemonSets are in the 
apps API group, 
version v1beta2.
The pod template includes a 
node selector, which selects 
nodes with the disk=ssd label.
 
</data>
  <data key="d5">110
CHAPTER 4
Replication and other controllers: deploying managed pods
CREATING A DAEMONSET YAML DEFINITION
You’ll create a DaemonSet that runs a mock ssd-monitor process, which prints
“SSD OK” to the standard output every five seconds. I’ve already prepared the mock
container image and pushed it to Docker Hub, so you can use it instead of building
your own. Create the YAML for the DaemonSet, as shown in the following listing.
apiVersion: apps/v1beta2      
kind: DaemonSet                     
metadata:
  name: ssd-monitor
spec:                            
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:                
        disk: ssd                  
      containers:
      - name: main
        image: luksa/ssd-monitor
You’re defining a DaemonSet that will run a pod with a single container based on the
luksa/ssd-monitor container image. An instance of this pod will be created for each
node that has the disk=ssd label.
Listing 4.10
A YAML for a DaemonSet: ssd-monitor-daemonset.yaml
Node 1
Pod:
ssd-monitor
Node 2
Node 3
DaemonSet:
sssd-monitor
Node selector:
disk=ssd
Node 4
disk: ssd
disk: ssd
disk: ssd
Unschedulable
Pod:
ssd-monitor
Pod:
ssd-monitor
Figure 4.9
Using a DaemonSet with a node selector to deploy system pods only on certain 
nodes
DaemonSets are in the 
apps API group, 
version v1beta2.
The pod template includes a 
node selector, which selects 
nodes with the disk=ssd label.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="143">
  <data key="d0">Page_143</data>
  <data key="d5">Page_143</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_54">
  <data key="d0">111
Running exactly one pod on each node with DaemonSets
CREATING THE DAEMONSET
You’ll create the DaemonSet like you always create resources from a YAML file:
$ kubectl create -f ssd-monitor-daemonset.yaml
daemonset "ssd-monitor" created
Let’s see the created DaemonSet:
$ kubectl get ds
NAME          DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE-SELECTOR  
ssd-monitor   0        0        0      0           0          disk=ssd
Those zeroes look strange. Didn’t the DaemonSet deploy any pods? List the pods:
$ kubectl get po
No resources found.
Where are the pods? Do you know what’s going on? Yes, you forgot to label your nodes
with the disk=ssd label. No problem—you can do that now. The DaemonSet should
detect that the nodes’ labels have changed and deploy the pod to all nodes with a
matching label. Let’s see if that’s true. 
ADDING THE REQUIRED LABEL TO YOUR NODE(S)
Regardless if you’re using Minikube, GKE, or another multi-node cluster, you’ll need
to list the nodes first, because you’ll need to know the node’s name when labeling it:
$ kubectl get node
NAME       STATUS    AGE       VERSION
minikube   Ready     4d        v1.6.0
Now, add the disk=ssd label to one of your nodes like this:
$ kubectl label node minikube disk=ssd
node "minikube" labeled
NOTE
Replace minikube with the name of one of your nodes if you’re not
using Minikube.
The DaemonSet should have created one pod now. Let’s see:
$ kubectl get po
NAME                READY     STATUS    RESTARTS   AGE
ssd-monitor-hgxwq   1/1       Running   0          35s
Okay; so far so good. If you have multiple nodes and you add the same label to further
nodes, you’ll see the DaemonSet spin up pods for each of them. 
REMOVING THE REQUIRED LABEL FROM THE NODE
Now, imagine you’ve made a mistake and have mislabeled one of the nodes. It has a
spinning disk drive, not an SSD. What happens if you change the node’s label?
$ kubectl label node minikube disk=hdd --overwrite
node "minikube" labeled
 
</data>
  <data key="d5">111
Running exactly one pod on each node with DaemonSets
CREATING THE DAEMONSET
You’ll create the DaemonSet like you always create resources from a YAML file:
$ kubectl create -f ssd-monitor-daemonset.yaml
daemonset "ssd-monitor" created
Let’s see the created DaemonSet:
$ kubectl get ds
NAME          DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE-SELECTOR  
ssd-monitor   0        0        0      0           0          disk=ssd
Those zeroes look strange. Didn’t the DaemonSet deploy any pods? List the pods:
$ kubectl get po
No resources found.
Where are the pods? Do you know what’s going on? Yes, you forgot to label your nodes
with the disk=ssd label. No problem—you can do that now. The DaemonSet should
detect that the nodes’ labels have changed and deploy the pod to all nodes with a
matching label. Let’s see if that’s true. 
ADDING THE REQUIRED LABEL TO YOUR NODE(S)
Regardless if you’re using Minikube, GKE, or another multi-node cluster, you’ll need
to list the nodes first, because you’ll need to know the node’s name when labeling it:
$ kubectl get node
NAME       STATUS    AGE       VERSION
minikube   Ready     4d        v1.6.0
Now, add the disk=ssd label to one of your nodes like this:
$ kubectl label node minikube disk=ssd
node "minikube" labeled
NOTE
Replace minikube with the name of one of your nodes if you’re not
using Minikube.
The DaemonSet should have created one pod now. Let’s see:
$ kubectl get po
NAME                READY     STATUS    RESTARTS   AGE
ssd-monitor-hgxwq   1/1       Running   0          35s
Okay; so far so good. If you have multiple nodes and you add the same label to further
nodes, you’ll see the DaemonSet spin up pods for each of them. 
REMOVING THE REQUIRED LABEL FROM THE NODE
Now, imagine you’ve made a mistake and have mislabeled one of the nodes. It has a
spinning disk drive, not an SSD. What happens if you change the node’s label?
$ kubectl label node minikube disk=hdd --overwrite
node "minikube" labeled
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="144">
  <data key="d0">Page_144</data>
  <data key="d5">Page_144</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_55">
  <data key="d0">112
CHAPTER 4
Replication and other controllers: deploying managed pods
Let’s see if the change has any effect on the pod that was running on that node:
$ kubectl get po
NAME                READY     STATUS        RESTARTS   AGE
ssd-monitor-hgxwq   1/1       Terminating   0          4m
The pod is being terminated. But you knew that was going to happen, right? This
wraps up your exploration of DaemonSets, so you may want to delete your ssd-monitor
DaemonSet. If you still have any other daemon pods running, you’ll see that deleting
the DaemonSet deletes those pods as well. 
4.5
Running pods that perform a single completable task 
Up to now, we’ve only talked about pods than need to run continuously. You’ll have
cases where you only want to run a task that terminates after completing its work.
ReplicationControllers, ReplicaSets, and DaemonSets run continuous tasks that are
never considered completed. Processes in such pods are restarted when they exit. But
in a completable task, after its process terminates, it should not be restarted again. 
4.5.1
Introducing the Job resource
Kubernetes includes support for this through the Job resource, which is similar to the
other resources we’ve discussed in this chapter, but it allows you to run a pod whose
container isn’t restarted when the process running inside finishes successfully. Once it
does, the pod is considered complete. 
 In the event of a node failure, the pods on that node that are managed by a Job will
be rescheduled to other nodes the way ReplicaSet pods are. In the event of a failure of
the process itself (when the process returns an error exit code), the Job can be config-
ured to either restart the container or not.
 Figure 4.10 shows how a pod created by a Job is rescheduled to a new node if the
node it was initially scheduled to fails. The figure also shows both a managed pod,
which isn’t rescheduled, and a pod backed by a ReplicaSet, which is.
 For example, Jobs are useful for ad hoc tasks, where it’s crucial that the task fin-
ishes properly. You could run the task in an unmanaged pod and wait for it to finish,
but in the event of a node failing or the pod being evicted from the node while it is
performing its task, you’d need to manually recreate it. Doing this manually doesn’t
make sense—especially if the job takes hours to complete. 
 An example of such a job would be if you had data stored somewhere and you
needed to transform and export it somewhere. You’re going to emulate this by run-
ning a container image built on top of the busybox image, which invokes the sleep
command for two minutes. I’ve already built the image and pushed it to Docker Hub,
but you can peek into its Dockerfile in the book’s code archive.
 
</data>
  <data key="d5">112
CHAPTER 4
Replication and other controllers: deploying managed pods
Let’s see if the change has any effect on the pod that was running on that node:
$ kubectl get po
NAME                READY     STATUS        RESTARTS   AGE
ssd-monitor-hgxwq   1/1       Terminating   0          4m
The pod is being terminated. But you knew that was going to happen, right? This
wraps up your exploration of DaemonSets, so you may want to delete your ssd-monitor
DaemonSet. If you still have any other daemon pods running, you’ll see that deleting
the DaemonSet deletes those pods as well. 
4.5
Running pods that perform a single completable task 
Up to now, we’ve only talked about pods than need to run continuously. You’ll have
cases where you only want to run a task that terminates after completing its work.
ReplicationControllers, ReplicaSets, and DaemonSets run continuous tasks that are
never considered completed. Processes in such pods are restarted when they exit. But
in a completable task, after its process terminates, it should not be restarted again. 
4.5.1
Introducing the Job resource
Kubernetes includes support for this through the Job resource, which is similar to the
other resources we’ve discussed in this chapter, but it allows you to run a pod whose
container isn’t restarted when the process running inside finishes successfully. Once it
does, the pod is considered complete. 
 In the event of a node failure, the pods on that node that are managed by a Job will
be rescheduled to other nodes the way ReplicaSet pods are. In the event of a failure of
the process itself (when the process returns an error exit code), the Job can be config-
ured to either restart the container or not.
 Figure 4.10 shows how a pod created by a Job is rescheduled to a new node if the
node it was initially scheduled to fails. The figure also shows both a managed pod,
which isn’t rescheduled, and a pod backed by a ReplicaSet, which is.
 For example, Jobs are useful for ad hoc tasks, where it’s crucial that the task fin-
ishes properly. You could run the task in an unmanaged pod and wait for it to finish,
but in the event of a node failing or the pod being evicted from the node while it is
performing its task, you’d need to manually recreate it. Doing this manually doesn’t
make sense—especially if the job takes hours to complete. 
 An example of such a job would be if you had data stored somewhere and you
needed to transform and export it somewhere. You’re going to emulate this by run-
ning a container image built on top of the busybox image, which invokes the sleep
command for two minutes. I’ve already built the image and pushed it to Docker Hub,
but you can peek into its Dockerfile in the book’s code archive.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="145">
  <data key="d0">Page_145</data>
  <data key="d5">Page_145</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_56">
  <data key="d0">113
Running pods that perform a single completable task
4.5.2
Defining a Job resource
Create the Job manifest as in the following listing.
apiVersion: batch/v1        
kind: Job                   
metadata:
  name: batch-job
spec:                                
  template: 
    metadata:
      labels:                        
        app: batch-job               
    spec:
      restartPolicy: OnFailure         
      containers:
      - name: main
        image: luksa/batch-job
Jobs are part of the batch API group and v1 API version. The YAML defines a
resource of type Job that will run the luksa/batch-job image, which invokes a pro-
cess that runs for exactly 120 seconds and then exits. 
 In a pod’s specification, you can specify what Kubernetes should do when the
processes running in the container finish. This is done through the restartPolicy
Listing 4.11
A YAML definition of a Job: exporter.yaml
Node 1
Pod A (unmanaged)
Pod B (managed by a ReplicaSet)
Pod C (managed by a Job)
Node 2
Node 1 fails
Job C2 ﬁnishes
Time
Pod B2 (managed by a ReplicaSet)
Pod C2 (managed by a Job)
Pod A isn’t rescheduled,
because there is nothing
managing it.
Figure 4.10
Pods managed by Jobs are rescheduled until they finish successfully.
Jobs are in the batch 
API group, version v1.
You’re not specifying a pod 
selector (it will be created 
based on the labels in the 
pod template).
Jobs can’t use the 
default restart policy, 
which is Always.
 
</data>
  <data key="d5">113
Running pods that perform a single completable task
4.5.2
Defining a Job resource
Create the Job manifest as in the following listing.
apiVersion: batch/v1        
kind: Job                   
metadata:
  name: batch-job
spec:                                
  template: 
    metadata:
      labels:                        
        app: batch-job               
    spec:
      restartPolicy: OnFailure         
      containers:
      - name: main
        image: luksa/batch-job
Jobs are part of the batch API group and v1 API version. The YAML defines a
resource of type Job that will run the luksa/batch-job image, which invokes a pro-
cess that runs for exactly 120 seconds and then exits. 
 In a pod’s specification, you can specify what Kubernetes should do when the
processes running in the container finish. This is done through the restartPolicy
Listing 4.11
A YAML definition of a Job: exporter.yaml
Node 1
Pod A (unmanaged)
Pod B (managed by a ReplicaSet)
Pod C (managed by a Job)
Node 2
Node 1 fails
Job C2 ﬁnishes
Time
Pod B2 (managed by a ReplicaSet)
Pod C2 (managed by a Job)
Pod A isn’t rescheduled,
because there is nothing
managing it.
Figure 4.10
Pods managed by Jobs are rescheduled until they finish successfully.
Jobs are in the batch 
API group, version v1.
You’re not specifying a pod 
selector (it will be created 
based on the labels in the 
pod template).
Jobs can’t use the 
default restart policy, 
which is Always.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="146">
  <data key="d0">Page_146</data>
  <data key="d5">Page_146</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_57">
  <data key="d0">114
CHAPTER 4
Replication and other controllers: deploying managed pods
pod spec property, which defaults to Always. Job pods can’t use the default policy,
because they’re not meant to run indefinitely. Therefore, you need to explicitly set
the restart policy to either OnFailure or Never. This setting is what prevents the con-
tainer from being restarted when it finishes (not the fact that the pod is being man-
aged by a Job resource).
4.5.3
Seeing a Job run a pod
After you create this Job with the kubectl create command, you should see it start up
a pod immediately:
$ kubectl get jobs
NAME        DESIRED   SUCCESSFUL   AGE
batch-job   1         0            2s
$ kubectl get po
NAME              READY     STATUS    RESTARTS   AGE
batch-job-28qf4   1/1       Running   0          4s
After the two minutes have passed, the pod will no longer show up in the pod list and
the Job will be marked as completed. By default, completed pods aren’t shown when
you list pods, unless you use the --show-all (or -a) switch:
$ kubectl get po -a
NAME              READY     STATUS      RESTARTS   AGE
batch-job-28qf4   0/1       Completed   0          2m
The reason the pod isn’t deleted when it completes is to allow you to examine its logs;
for example:
$ kubectl logs batch-job-28qf4
Fri Apr 29 09:58:22 UTC 2016 Batch job starting
Fri Apr 29 10:00:22 UTC 2016 Finished succesfully
The pod will be deleted when you delete it or the Job that created it. Before you do
that, let’s look at the Job resource again:
$ kubectl get job
NAME        DESIRED   SUCCESSFUL   AGE
batch-job   1         1            9m
The Job is shown as having completed successfully. But why is that piece of informa-
tion shown as a number instead of as yes or true? And what does the DESIRED column
indicate? 
4.5.4
Running multiple pod instances in a Job
Jobs may be configured to create more than one pod instance and run them in paral-
lel or sequentially. This is done by setting the completions and the parallelism prop-
erties in the Job spec.
 
</data>
  <data key="d5">114
CHAPTER 4
Replication and other controllers: deploying managed pods
pod spec property, which defaults to Always. Job pods can’t use the default policy,
because they’re not meant to run indefinitely. Therefore, you need to explicitly set
the restart policy to either OnFailure or Never. This setting is what prevents the con-
tainer from being restarted when it finishes (not the fact that the pod is being man-
aged by a Job resource).
4.5.3
Seeing a Job run a pod
After you create this Job with the kubectl create command, you should see it start up
a pod immediately:
$ kubectl get jobs
NAME        DESIRED   SUCCESSFUL   AGE
batch-job   1         0            2s
$ kubectl get po
NAME              READY     STATUS    RESTARTS   AGE
batch-job-28qf4   1/1       Running   0          4s
After the two minutes have passed, the pod will no longer show up in the pod list and
the Job will be marked as completed. By default, completed pods aren’t shown when
you list pods, unless you use the --show-all (or -a) switch:
$ kubectl get po -a
NAME              READY     STATUS      RESTARTS   AGE
batch-job-28qf4   0/1       Completed   0          2m
The reason the pod isn’t deleted when it completes is to allow you to examine its logs;
for example:
$ kubectl logs batch-job-28qf4
Fri Apr 29 09:58:22 UTC 2016 Batch job starting
Fri Apr 29 10:00:22 UTC 2016 Finished succesfully
The pod will be deleted when you delete it or the Job that created it. Before you do
that, let’s look at the Job resource again:
$ kubectl get job
NAME        DESIRED   SUCCESSFUL   AGE
batch-job   1         1            9m
The Job is shown as having completed successfully. But why is that piece of informa-
tion shown as a number instead of as yes or true? And what does the DESIRED column
indicate? 
4.5.4
Running multiple pod instances in a Job
Jobs may be configured to create more than one pod instance and run them in paral-
lel or sequentially. This is done by setting the completions and the parallelism prop-
erties in the Job spec.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="147">
  <data key="d0">Page_147</data>
  <data key="d5">Page_147</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_58">
  <data key="d0">115
Running pods that perform a single completable task
RUNNING JOB PODS SEQUENTIALLY
If you need a Job to run more than once, you set completions to how many times you
want the Job’s pod to run. The following listing shows an example.
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5                  
  template:
    &lt;template is the same as in listing 4.11&gt;
This Job will run five pods one after the other. It initially creates one pod, and when
the pod’s container finishes, it creates the second pod, and so on, until five pods com-
plete successfully. If one of the pods fails, the Job creates a new pod, so the Job may
create more than five pods overall.
RUNNING JOB PODS IN PARALLEL
Instead of running single Job pods one after the other, you can also make the Job run
multiple pods in parallel. You specify how many pods are allowed to run in parallel
with the parallelism  Job spec property, as shown in the following listing.
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5                    
  parallelism: 2                    
  template:
    &lt;same as in listing 4.11&gt;
By setting parallelism to 2, the Job creates two pods and runs them in parallel:
$ kubectl get po
NAME                               READY   STATUS     RESTARTS   AGE
multi-completion-batch-job-lmmnk   1/1     Running    0          21s
multi-completion-batch-job-qx4nq   1/1     Running    0          21s
As soon as one of them finishes, the Job will run the next pod, until five pods finish
successfully.
Listing 4.12
A Job requiring multiple completions: multi-completion-batch-job.yaml
Listing 4.13
Running Job pods in parallel: multi-completion-parallel-batch-job.yaml
Setting completions to 
5 makes this Job run 
five pods sequentially.
This job must ensure 
five pods complete 
successfully.
Up to two pods 
can run in parallel.
 
</data>
  <data key="d5">115
Running pods that perform a single completable task
RUNNING JOB PODS SEQUENTIALLY
If you need a Job to run more than once, you set completions to how many times you
want the Job’s pod to run. The following listing shows an example.
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5                  
  template:
    &lt;template is the same as in listing 4.11&gt;
This Job will run five pods one after the other. It initially creates one pod, and when
the pod’s container finishes, it creates the second pod, and so on, until five pods com-
plete successfully. If one of the pods fails, the Job creates a new pod, so the Job may
create more than five pods overall.
RUNNING JOB PODS IN PARALLEL
Instead of running single Job pods one after the other, you can also make the Job run
multiple pods in parallel. You specify how many pods are allowed to run in parallel
with the parallelism  Job spec property, as shown in the following listing.
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5                    
  parallelism: 2                    
  template:
    &lt;same as in listing 4.11&gt;
By setting parallelism to 2, the Job creates two pods and runs them in parallel:
$ kubectl get po
NAME                               READY   STATUS     RESTARTS   AGE
multi-completion-batch-job-lmmnk   1/1     Running    0          21s
multi-completion-batch-job-qx4nq   1/1     Running    0          21s
As soon as one of them finishes, the Job will run the next pod, until five pods finish
successfully.
Listing 4.12
A Job requiring multiple completions: multi-completion-batch-job.yaml
Listing 4.13
Running Job pods in parallel: multi-completion-parallel-batch-job.yaml
Setting completions to 
5 makes this Job run 
five pods sequentially.
This job must ensure 
five pods complete 
successfully.
Up to two pods 
can run in parallel.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="148">
  <data key="d0">Page_148</data>
  <data key="d5">Page_148</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_59">
  <data key="d0">116
CHAPTER 4
Replication and other controllers: deploying managed pods
SCALING A JOB
You can even change a Job’s parallelism property while the Job is running. This is
similar to scaling a ReplicaSet or ReplicationController, and can be done with the
kubectl scale command:
$ kubectl scale job multi-completion-batch-job --replicas 3
job "multi-completion-batch-job" scaled
Because you’ve increased parallelism from 2 to 3, another pod is immediately spun
up, so three pods are now running.
4.5.5
Limiting the time allowed for a Job pod to complete
We need to discuss one final thing about Jobs. How long should the Job wait for a pod
to finish? What if the pod gets stuck and can’t finish at all (or it can’t finish fast
enough)?
 A pod’s time can be limited by setting the activeDeadlineSeconds property in the
pod spec. If the pod runs longer than that, the system will try to terminate it and will
mark the Job as failed. 
NOTE
You can configure how many times a Job can be retried before it is
marked as failed by specifying the spec.backoffLimit field in the Job mani-
fest. If you don't explicitly specify it, it defaults to 6.
4.6
Scheduling Jobs to run periodically or once 
in the future
Job resources run their pods immediately when you create the Job resource. But many
batch jobs need to be run at a specific time in the future or repeatedly in the specified
interval. In Linux- and UNIX-like operating systems, these jobs are better known as
cron jobs. Kubernetes supports them, too.
 A cron job in Kubernetes is configured by creating a CronJob resource. The
schedule for running the job is specified in the well-known cron format, so if you’re
familiar with regular cron jobs, you’ll understand Kubernetes’ CronJobs in a matter
of seconds.
 At the configured time, Kubernetes will create a Job resource according to the Job
template configured in the CronJob object. When the Job resource is created, one or
more pod replicas will be created and started according to the Job’s pod template, as
you learned in the previous section. There’s nothing more to it.
 Let’s look at how to create CronJobs. 
4.6.1
Creating a CronJob
Imagine you need to run the batch job from your previous example every 15 minutes.
To do that, create a CronJob resource with the following specification.
 
 
</data>
  <data key="d5">116
CHAPTER 4
Replication and other controllers: deploying managed pods
SCALING A JOB
You can even change a Job’s parallelism property while the Job is running. This is
similar to scaling a ReplicaSet or ReplicationController, and can be done with the
kubectl scale command:
$ kubectl scale job multi-completion-batch-job --replicas 3
job "multi-completion-batch-job" scaled
Because you’ve increased parallelism from 2 to 3, another pod is immediately spun
up, so three pods are now running.
4.5.5
Limiting the time allowed for a Job pod to complete
We need to discuss one final thing about Jobs. How long should the Job wait for a pod
to finish? What if the pod gets stuck and can’t finish at all (or it can’t finish fast
enough)?
 A pod’s time can be limited by setting the activeDeadlineSeconds property in the
pod spec. If the pod runs longer than that, the system will try to terminate it and will
mark the Job as failed. 
NOTE
You can configure how many times a Job can be retried before it is
marked as failed by specifying the spec.backoffLimit field in the Job mani-
fest. If you don't explicitly specify it, it defaults to 6.
4.6
Scheduling Jobs to run periodically or once 
in the future
Job resources run their pods immediately when you create the Job resource. But many
batch jobs need to be run at a specific time in the future or repeatedly in the specified
interval. In Linux- and UNIX-like operating systems, these jobs are better known as
cron jobs. Kubernetes supports them, too.
 A cron job in Kubernetes is configured by creating a CronJob resource. The
schedule for running the job is specified in the well-known cron format, so if you’re
familiar with regular cron jobs, you’ll understand Kubernetes’ CronJobs in a matter
of seconds.
 At the configured time, Kubernetes will create a Job resource according to the Job
template configured in the CronJob object. When the Job resource is created, one or
more pod replicas will be created and started according to the Job’s pod template, as
you learned in the previous section. There’s nothing more to it.
 Let’s look at how to create CronJobs. 
4.6.1
Creating a CronJob
Imagine you need to run the batch job from your previous example every 15 minutes.
To do that, create a CronJob resource with the following specification.
 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="149">
  <data key="d0">Page_149</data>
  <data key="d5">Page_149</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_60">
  <data key="d0">117
Scheduling Jobs to run periodically or once in the future
apiVersion: batch/v1beta1               
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  schedule: "0,15,30,45 * * * *"           
  jobTemplate:
    spec:
      template:                            
        metadata:                          
          labels:                          
            app: periodic-batch-job        
        spec:                              
          restartPolicy: OnFailure         
          containers:                      
          - name: main                     
            image: luksa/batch-job         
As you can see, it’s not too complicated. You’ve specified a schedule and a template
from which the Job objects will be created. 
CONFIGURING THE SCHEDULE
If you’re unfamiliar with the cron schedule format, you’ll find great tutorials and
explanations online, but as a quick introduction, from left to right, the schedule con-
tains the following five entries:
Minute
Hour
Day of month
Month
Day of week.
In the example, you want to run the job every 15 minutes, so the schedule needs to be
"0,15,30,45 * * * *", which means at the 0, 15, 30 and 45 minutes mark of every hour
(first asterisk), of every day of the month (second asterisk), of every month (third
asterisk) and on every day of the week (fourth asterisk). 
 If, instead, you wanted it to run every 30 minutes, but only on the first day of the
month, you’d set the schedule to "0,30 * 1 * *", and if you want it to run at 3AM every
Sunday, you’d set it to "0 3 * * 0" (the last zero stands for Sunday).
CONFIGURING THE JOB TEMPLATE
A CronJob creates Job resources from the jobTemplate property configured in the
CronJob spec, so refer to section 4.5 for more information on how to configure it.
4.6.2
Understanding how scheduled jobs are run
Job resources will be created from the CronJob resource at approximately the sched-
uled time. The Job then creates the pods. 
Listing 4.14
YAML for a CronJob resource: cronjob.yaml
API group is batch, 
version is v1beta1
This job should run at the 
0, 15, 30 and 45 minutes of 
every hour, every day.
The template for the 
Job resources that 
will be created by 
this CronJob
 
</data>
  <data key="d5">117
Scheduling Jobs to run periodically or once in the future
apiVersion: batch/v1beta1               
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  schedule: "0,15,30,45 * * * *"           
  jobTemplate:
    spec:
      template:                            
        metadata:                          
          labels:                          
            app: periodic-batch-job        
        spec:                              
          restartPolicy: OnFailure         
          containers:                      
          - name: main                     
            image: luksa/batch-job         
As you can see, it’s not too complicated. You’ve specified a schedule and a template
from which the Job objects will be created. 
CONFIGURING THE SCHEDULE
If you’re unfamiliar with the cron schedule format, you’ll find great tutorials and
explanations online, but as a quick introduction, from left to right, the schedule con-
tains the following five entries:
Minute
Hour
Day of month
Month
Day of week.
In the example, you want to run the job every 15 minutes, so the schedule needs to be
"0,15,30,45 * * * *", which means at the 0, 15, 30 and 45 minutes mark of every hour
(first asterisk), of every day of the month (second asterisk), of every month (third
asterisk) and on every day of the week (fourth asterisk). 
 If, instead, you wanted it to run every 30 minutes, but only on the first day of the
month, you’d set the schedule to "0,30 * 1 * *", and if you want it to run at 3AM every
Sunday, you’d set it to "0 3 * * 0" (the last zero stands for Sunday).
CONFIGURING THE JOB TEMPLATE
A CronJob creates Job resources from the jobTemplate property configured in the
CronJob spec, so refer to section 4.5 for more information on how to configure it.
4.6.2
Understanding how scheduled jobs are run
Job resources will be created from the CronJob resource at approximately the sched-
uled time. The Job then creates the pods. 
Listing 4.14
YAML for a CronJob resource: cronjob.yaml
API group is batch, 
version is v1beta1
This job should run at the 
0, 15, 30 and 45 minutes of 
every hour, every day.
The template for the 
Job resources that 
will be created by 
this CronJob
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="150">
  <data key="d0">Page_150</data>
  <data key="d5">Page_150</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_61">
  <data key="d0">118
CHAPTER 4
Replication and other controllers: deploying managed pods
 It may happen that the Job or pod is created and run relatively late. You may have
a hard requirement for the job to not be started too far over the scheduled time. In
that case, you can specify a deadline by specifying the startingDeadlineSeconds field
in the CronJob specification as shown in the following listing.
apiVersion: batch/v1beta1
kind: CronJob
spec:
  schedule: "0,15,30,45 * * * *"
  startingDeadlineSeconds: 15    
  ...
In the example in listing 4.15, one of the times the job is supposed to run is 10:30:00.
If it doesn’t start by 10:30:15 for whatever reason, the job will not run and will be
shown as Failed. 
 In normal circumstances, a CronJob always creates only a single Job for each exe-
cution configured in the schedule, but it may happen that two Jobs are created at the
same time, or none at all. To combat the first problem, your jobs should be idempo-
tent (running them multiple times instead of once shouldn’t lead to unwanted
results). For the second problem, make sure that the next job run performs any work
that should have been done by the previous (missed) run.
4.7
Summary
You’ve now learned how to keep pods running and have them rescheduled in the
event of node failures. You should now know that
You can specify a liveness probe to have Kubernetes restart your container as
soon as it’s no longer healthy (where the app defines what’s considered
healthy).
Pods shouldn’t be created directly, because they will not be re-created if they’re
deleted by mistake, if the node they’re running on fails, or if they’re evicted
from the node.
ReplicationControllers always keep the desired number of pod replicas
running.
Scaling pods horizontally is as easy as changing the desired replica count on a
ReplicationController.
Pods aren’t owned by the ReplicationControllers and can be moved between
them if necessary.
A ReplicationController creates new pods from a pod template. Changing the
template has no effect on existing pods.
Listing 4.15
Specifying a startingDeadlineSeconds for a CronJob
At the latest, the pod must 
start running at 15 seconds 
past the scheduled time.
 
</data>
  <data key="d5">118
CHAPTER 4
Replication and other controllers: deploying managed pods
 It may happen that the Job or pod is created and run relatively late. You may have
a hard requirement for the job to not be started too far over the scheduled time. In
that case, you can specify a deadline by specifying the startingDeadlineSeconds field
in the CronJob specification as shown in the following listing.
apiVersion: batch/v1beta1
kind: CronJob
spec:
  schedule: "0,15,30,45 * * * *"
  startingDeadlineSeconds: 15    
  ...
In the example in listing 4.15, one of the times the job is supposed to run is 10:30:00.
If it doesn’t start by 10:30:15 for whatever reason, the job will not run and will be
shown as Failed. 
 In normal circumstances, a CronJob always creates only a single Job for each exe-
cution configured in the schedule, but it may happen that two Jobs are created at the
same time, or none at all. To combat the first problem, your jobs should be idempo-
tent (running them multiple times instead of once shouldn’t lead to unwanted
results). For the second problem, make sure that the next job run performs any work
that should have been done by the previous (missed) run.
4.7
Summary
You’ve now learned how to keep pods running and have them rescheduled in the
event of node failures. You should now know that
You can specify a liveness probe to have Kubernetes restart your container as
soon as it’s no longer healthy (where the app defines what’s considered
healthy).
Pods shouldn’t be created directly, because they will not be re-created if they’re
deleted by mistake, if the node they’re running on fails, or if they’re evicted
from the node.
ReplicationControllers always keep the desired number of pod replicas
running.
Scaling pods horizontally is as easy as changing the desired replica count on a
ReplicationController.
Pods aren’t owned by the ReplicationControllers and can be moved between
them if necessary.
A ReplicationController creates new pods from a pod template. Changing the
template has no effect on existing pods.
Listing 4.15
Specifying a startingDeadlineSeconds for a CronJob
At the latest, the pod must 
start running at 15 seconds 
past the scheduled time.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="151">
  <data key="d0">Page_151</data>
  <data key="d5">Page_151</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_62">
  <data key="d0">119
Summary
ReplicationControllers should be replaced with ReplicaSets and Deployments,
which provide the same functionality, but with additional powerful features.
ReplicationControllers and ReplicaSets schedule pods to random cluster nodes,
whereas DaemonSets make sure every node runs a single instance of a pod
defined in the DaemonSet.
Pods that perform a batch task should be created through a Kubernetes Job
resource, not directly or through a ReplicationController or similar object.
Jobs that need to run sometime in the future can be created through CronJob
resources. 
 
</data>
  <data key="d5">119
Summary
ReplicationControllers should be replaced with ReplicaSets and Deployments,
which provide the same functionality, but with additional powerful features.
ReplicationControllers and ReplicaSets schedule pods to random cluster nodes,
whereas DaemonSets make sure every node runs a single instance of a pod
defined in the DaemonSet.
Pods that perform a batch task should be created through a Kubernetes Job
resource, not directly or through a ReplicationController or similar object.
Jobs that need to run sometime in the future can be created through CronJob
resources. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="152">
  <data key="d0">Page_152</data>
  <data key="d5">Page_152</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_63">
  <data key="d0">120
Services: enabling
clients to discover
and talk to pods
You’ve learned about pods and how to deploy them through ReplicaSets and similar
resources to ensure they keep running. Although certain pods can do their work
independently of an external stimulus, many applications these days are meant to
respond to external requests. For example, in the case of microservices, pods will
usually respond to HTTP requests coming either from other pods inside the cluster
or from clients outside the cluster. 
 Pods need a way of finding other pods if they want to consume the services they
provide. Unlike in the non-Kubernetes world, where a sysadmin would configure
This chapter covers
Creating Service resources to expose a group of 
pods at a single address
Discovering services in the cluster
Exposing services to external clients
Connecting to external services from inside the 
cluster
Controlling whether a pod is ready to be part of 
the service or not
Troubleshooting services
 
</data>
  <data key="d5">120
Services: enabling
clients to discover
and talk to pods
You’ve learned about pods and how to deploy them through ReplicaSets and similar
resources to ensure they keep running. Although certain pods can do their work
independently of an external stimulus, many applications these days are meant to
respond to external requests. For example, in the case of microservices, pods will
usually respond to HTTP requests coming either from other pods inside the cluster
or from clients outside the cluster. 
 Pods need a way of finding other pods if they want to consume the services they
provide. Unlike in the non-Kubernetes world, where a sysadmin would configure
This chapter covers
Creating Service resources to expose a group of 
pods at a single address
Discovering services in the cluster
Exposing services to external clients
Connecting to external services from inside the 
cluster
Controlling whether a pod is ready to be part of 
the service or not
Troubleshooting services
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="153">
  <data key="d0">Page_153</data>
  <data key="d5">Page_153</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_64">
  <data key="d0">121
Introducing services
each client app by specifying the exact IP address or hostname of the server providing
the service in the client’s configuration files, doing the same in Kubernetes wouldn’t
work, because
Pods are ephemeral—They may come and go at any time, whether it’s because a
pod is removed from a node to make room for other pods, because someone
scaled down the number of pods, or because a cluster node has failed.
Kubernetes assigns an IP address to a pod after the pod has been scheduled to a node
and before it’s started—Clients thus can’t know the IP address of the server pod
up front.
Horizontal scaling means multiple pods may provide the same service—Each of those
pods has its own IP address. Clients shouldn’t care how many pods are backing
the service and what their IPs are. They shouldn’t have to keep a list of all the
individual IPs of pods. Instead, all those pods should be accessible through a
single IP address.
To solve these problems, Kubernetes also provides another resource type—Services—
that we’ll discuss in this chapter.
5.1
Introducing services
A Kubernetes Service is a resource you create to make a single, constant point of
entry to a group of pods providing the same service. Each service has an IP address
and port that never change while the service exists. Clients can open connections to
that IP and port, and those connections are then routed to one of the pods backing
that service. This way, clients of a service don’t need to know the location of individ-
ual pods providing the service, allowing those pods to be moved around the cluster
at any time. 
EXPLAINING SERVICES WITH AN EXAMPLE
Let’s revisit the example where you have a frontend web server and a backend data-
base server. There may be multiple pods that all act as the frontend, but there may
only be a single backend database pod. You need to solve two problems to make the
system function:
External clients need to connect to the frontend pods without caring if there’s
only a single web server or hundreds.
The frontend pods need to connect to the backend database. Because the data-
base runs inside a pod, it may be moved around the cluster over time, causing
its IP address to change. You don’t want to reconfigure the frontend pods every
time the backend database is moved.
By creating a service for the frontend pods and configuring it to be accessible from
outside the cluster, you expose a single, constant IP address through which external
clients can connect to the pods. Similarly, by also creating a service for the backend
pod, you create a stable address for the backend pod. The service address doesn’t
 
</data>
  <data key="d5">121
Introducing services
each client app by specifying the exact IP address or hostname of the server providing
the service in the client’s configuration files, doing the same in Kubernetes wouldn’t
work, because
Pods are ephemeral—They may come and go at any time, whether it’s because a
pod is removed from a node to make room for other pods, because someone
scaled down the number of pods, or because a cluster node has failed.
Kubernetes assigns an IP address to a pod after the pod has been scheduled to a node
and before it’s started—Clients thus can’t know the IP address of the server pod
up front.
Horizontal scaling means multiple pods may provide the same service—Each of those
pods has its own IP address. Clients shouldn’t care how many pods are backing
the service and what their IPs are. They shouldn’t have to keep a list of all the
individual IPs of pods. Instead, all those pods should be accessible through a
single IP address.
To solve these problems, Kubernetes also provides another resource type—Services—
that we’ll discuss in this chapter.
5.1
Introducing services
A Kubernetes Service is a resource you create to make a single, constant point of
entry to a group of pods providing the same service. Each service has an IP address
and port that never change while the service exists. Clients can open connections to
that IP and port, and those connections are then routed to one of the pods backing
that service. This way, clients of a service don’t need to know the location of individ-
ual pods providing the service, allowing those pods to be moved around the cluster
at any time. 
EXPLAINING SERVICES WITH AN EXAMPLE
Let’s revisit the example where you have a frontend web server and a backend data-
base server. There may be multiple pods that all act as the frontend, but there may
only be a single backend database pod. You need to solve two problems to make the
system function:
External clients need to connect to the frontend pods without caring if there’s
only a single web server or hundreds.
The frontend pods need to connect to the backend database. Because the data-
base runs inside a pod, it may be moved around the cluster over time, causing
its IP address to change. You don’t want to reconfigure the frontend pods every
time the backend database is moved.
By creating a service for the frontend pods and configuring it to be accessible from
outside the cluster, you expose a single, constant IP address through which external
clients can connect to the pods. Similarly, by also creating a service for the backend
pod, you create a stable address for the backend pod. The service address doesn’t
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="154">
  <data key="d0">Page_154</data>
  <data key="d5">Page_154</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_65">
  <data key="d0">122
CHAPTER 5
Services: enabling clients to discover and talk to pods
change even if the pod’s IP address changes. Additionally, by creating the service, you
also enable the frontend pods to easily find the backend service by its name through
either environment variables or DNS. All the components of your system (the two ser-
vices, the two sets of pods backing those services, and the interdependencies between
them) are shown in figure 5.1.
You now understand the basic idea behind services. Now, let’s dig deeper by first see-
ing how they can be created.
5.1.1
Creating services
As you’ve seen, a service can be backed by more than one pod. Connections to the ser-
vice are load-balanced across all the backing pods. But how exactly do you define
which pods are part of the service and which aren’t? 
 You probably remember label selectors and how they’re used in Replication-
Controllers and other pod controllers to specify which pods belong to the same set.
The same mechanism is used by services in the same way, as you can see in figure 5.2.
 In the previous chapter, you created a ReplicationController which then ran three
instances of the pod containing the Node.js app. Create the ReplicationController
again and verify three pod instances are up and running. After that, you’ll create a
Service for those three pods. 
Frontend pod 1
IP: 2.1.1.1
External client
Frontend pod 2
IP: 2.1.1.2
Frontend pod 3
IP: 2.1.1.3
Backend pod
IP: 2.1.1.4
Frontend service
IP: 1.1.1.1
Backend service
IP: 1.1.1.2
Frontend components
Backend components
Figure 5.1
Both internal and external clients usually connect to pods through services.
 
</data>
  <data key="d5">122
CHAPTER 5
Services: enabling clients to discover and talk to pods
change even if the pod’s IP address changes. Additionally, by creating the service, you
also enable the frontend pods to easily find the backend service by its name through
either environment variables or DNS. All the components of your system (the two ser-
vices, the two sets of pods backing those services, and the interdependencies between
them) are shown in figure 5.1.
You now understand the basic idea behind services. Now, let’s dig deeper by first see-
ing how they can be created.
5.1.1
Creating services
As you’ve seen, a service can be backed by more than one pod. Connections to the ser-
vice are load-balanced across all the backing pods. But how exactly do you define
which pods are part of the service and which aren’t? 
 You probably remember label selectors and how they’re used in Replication-
Controllers and other pod controllers to specify which pods belong to the same set.
The same mechanism is used by services in the same way, as you can see in figure 5.2.
 In the previous chapter, you created a ReplicationController which then ran three
instances of the pod containing the Node.js app. Create the ReplicationController
again and verify three pod instances are up and running. After that, you’ll create a
Service for those three pods. 
Frontend pod 1
IP: 2.1.1.1
External client
Frontend pod 2
IP: 2.1.1.2
Frontend pod 3
IP: 2.1.1.3
Backend pod
IP: 2.1.1.4
Frontend service
IP: 1.1.1.1
Backend service
IP: 1.1.1.2
Frontend components
Backend components
Figure 5.1
Both internal and external clients usually connect to pods through services.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="155">
  <data key="d0">Page_155</data>
  <data key="d5">Page_155</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_66">
  <data key="d0">123
Introducing services
CREATING A SERVICE THROUGH KUBECTL EXPOSE
The easiest way to create a service is through kubectl expose, which you’ve already
used in chapter 2 to expose the ReplicationController you created earlier. The
expose command created a Service resource with the same pod selector as the one
used by the ReplicationController, thereby exposing all its pods through a single IP
address and port. 
 Now, instead of using the expose command, you’ll create a service manually by
posting a YAML to the Kubernetes API server. 
CREATING A SERVICE THROUGH A YAML DESCRIPTOR
Create a file called kubia-svc.yaml with the following listing’s contents.
apiVersion: v1
kind: Service             
metadata:
  name: kubia              
spec:
  ports:
  - port: 80              
    targetPort: 8080       
  selector:                 
    app: kubia              
You’re defining a service called kubia, which will accept connections on port 80 and
route each connection to port 8080 of one of the pods matching the app=kubia
label selector. 
 Go ahead and create the service by posting the file using kubectl create.
Listing 5.1
A definition of a service: kubia-svc.yaml
app: kubia
Pod: kubia-q3vkg
Pod: kubia-k0xz6
Pod: kubia-53thy
Client
Service: kubia
Selector: app=kubia
app: kubia
app: kubia
Figure 5.2
Label selectors 
determine which pods belong 
to the Service.
The port this service 
will be available on
The container port the 
service will forward to
All pods with the app=kubia 
label will be part of this service.
 
</data>
  <data key="d5">123
Introducing services
CREATING A SERVICE THROUGH KUBECTL EXPOSE
The easiest way to create a service is through kubectl expose, which you’ve already
used in chapter 2 to expose the ReplicationController you created earlier. The
expose command created a Service resource with the same pod selector as the one
used by the ReplicationController, thereby exposing all its pods through a single IP
address and port. 
 Now, instead of using the expose command, you’ll create a service manually by
posting a YAML to the Kubernetes API server. 
CREATING A SERVICE THROUGH A YAML DESCRIPTOR
Create a file called kubia-svc.yaml with the following listing’s contents.
apiVersion: v1
kind: Service             
metadata:
  name: kubia              
spec:
  ports:
  - port: 80              
    targetPort: 8080       
  selector:                 
    app: kubia              
You’re defining a service called kubia, which will accept connections on port 80 and
route each connection to port 8080 of one of the pods matching the app=kubia
label selector. 
 Go ahead and create the service by posting the file using kubectl create.
Listing 5.1
A definition of a service: kubia-svc.yaml
app: kubia
Pod: kubia-q3vkg
Pod: kubia-k0xz6
Pod: kubia-53thy
Client
Service: kubia
Selector: app=kubia
app: kubia
app: kubia
Figure 5.2
Label selectors 
determine which pods belong 
to the Service.
The port this service 
will be available on
The container port the 
service will forward to
All pods with the app=kubia 
label will be part of this service.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="156">
  <data key="d0">Page_156</data>
  <data key="d5">Page_156</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_67">
  <data key="d0">124
CHAPTER 5
Services: enabling clients to discover and talk to pods
EXAMINING YOUR NEW SERVICE
After posting the YAML, you can list all Service resources in your namespace and see
that an internal cluster IP has been assigned to your service:
$ kubectl get svc
NAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.111.240.1     &lt;none&gt;        443/TCP   30d
kubia        10.111.249.153   &lt;none&gt;        80/TCP    6m     
The list shows that the IP address assigned to the service is 10.111.249.153. Because
this is the cluster IP, it’s only accessible from inside the cluster. The primary purpose
of services is exposing groups of pods to other pods in the cluster, but you’ll usually
also want to expose services externally. You’ll see how to do that later. For now, let’s
use your service from inside the cluster and see what it does.
TESTING YOUR SERVICE FROM WITHIN THE CLUSTER
You can send requests to your service from within the cluster in a few ways:
The obvious way is to create a pod that will send the request to the service’s
cluster IP and log the response. You can then examine the pod’s log to see
what the service’s response was.
You can ssh into one of the Kubernetes nodes and use the curl command.
You can execute the curl command inside one of your existing pods through
the kubectl exec command.
Let’s go for the last option, so you also learn how to run commands in existing pods. 
REMOTELY EXECUTING COMMANDS IN RUNNING CONTAINERS
The kubectl exec command allows you to remotely run arbitrary commands inside
an existing container of a pod. This comes in handy when you want to examine the
contents, state, and/or environment of a container. List the pods with the kubectl
get pods command and choose one as your target for the exec command (in the fol-
lowing example, I’ve chosen the kubia-7nog1 pod as the target). You’ll also need to
obtain the cluster IP of your service (using kubectl get svc, for example). When run-
ning the following commands yourself, be sure to replace the pod name and the ser-
vice IP with your own: 
$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153
You’ve hit kubia-gzwli
If you’ve used ssh to execute commands on a remote system before, you’ll recognize
that kubectl exec isn’t much different.
 
 
 
 
Here’s your 
service.
 
</data>
  <data key="d5">124
CHAPTER 5
Services: enabling clients to discover and talk to pods
EXAMINING YOUR NEW SERVICE
After posting the YAML, you can list all Service resources in your namespace and see
that an internal cluster IP has been assigned to your service:
$ kubectl get svc
NAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.111.240.1     &lt;none&gt;        443/TCP   30d
kubia        10.111.249.153   &lt;none&gt;        80/TCP    6m     
The list shows that the IP address assigned to the service is 10.111.249.153. Because
this is the cluster IP, it’s only accessible from inside the cluster. The primary purpose
of services is exposing groups of pods to other pods in the cluster, but you’ll usually
also want to expose services externally. You’ll see how to do that later. For now, let’s
use your service from inside the cluster and see what it does.
TESTING YOUR SERVICE FROM WITHIN THE CLUSTER
You can send requests to your service from within the cluster in a few ways:
The obvious way is to create a pod that will send the request to the service’s
cluster IP and log the response. You can then examine the pod’s log to see
what the service’s response was.
You can ssh into one of the Kubernetes nodes and use the curl command.
You can execute the curl command inside one of your existing pods through
the kubectl exec command.
Let’s go for the last option, so you also learn how to run commands in existing pods. 
REMOTELY EXECUTING COMMANDS IN RUNNING CONTAINERS
The kubectl exec command allows you to remotely run arbitrary commands inside
an existing container of a pod. This comes in handy when you want to examine the
contents, state, and/or environment of a container. List the pods with the kubectl
get pods command and choose one as your target for the exec command (in the fol-
lowing example, I’ve chosen the kubia-7nog1 pod as the target). You’ll also need to
obtain the cluster IP of your service (using kubectl get svc, for example). When run-
ning the following commands yourself, be sure to replace the pod name and the ser-
vice IP with your own: 
$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153
You’ve hit kubia-gzwli
If you’ve used ssh to execute commands on a remote system before, you’ll recognize
that kubectl exec isn’t much different.
 
 
 
 
Here’s your 
service.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="157">
  <data key="d0">Page_157</data>
  <data key="d5">Page_157</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_68">
  <data key="d0">125
Introducing services
Let’s go over what transpired when you ran the command. Figure 5.3 shows the
sequence of events. You instructed Kubernetes to execute the curl command inside the
container of one of your pods. Curl sent an HTTP request to the service IP, which is
backed by three pods. The Kubernetes service proxy intercepted the connection,
selected a random pod among the three pods, and forwarded the request to it. Node.js
running inside that pod then handled the request and returned an HTTP response con-
taining the pod’s name. Curl then printed the response to the standard output, which
was intercepted and printed to its standard output on your local machine by kubectl.
Why the double dash?
The double dash (--) in the command signals the end of command options for
kubectl. Everything after the double dash is the command that should be executed
inside the pod. Using the double dash isn’t necessary if the command has no
arguments that start with a dash. But in your case, if you don’t use the double dash
there, the -s option would be interpreted as an option for kubectl exec and would
result in the following strange and highly misleading error:
$ kubectl exec kubia-7nog1 curl -s http://10.111.249.153
The connection to the server 10.111.249.153 was refused – did you 
specify the right host or port?
This has nothing to do with your service refusing the connection. It’s because
kubectl is not able to connect to an API server at 10.111.249.153 (the -s option
is used to tell kubectl to connect to a different API server than the default).
3. Curl sends HTTP
GET request
4. Service redirects HTTP
connection to a randomly
selected pod
2. Curl is executed
inside the container
running node.js
6. The output of the
command is sent
curl
back to kubectl and
printed by it
5. HTTP response is
sent back to curl
Pod: kubia-7nog1
Container
node.js
curl http://
10.111.249.153
Pod: kubia-gzwli
Container
node.js
Pod: kubia-5fje3
Container
node.js
1. kubectl exec
Service: kubia
10.111.249.153:80
Figure 5.3
Using kubectl exec to test out a connection to the service by running curl in one of the pods
 
</data>
  <data key="d5">125
Introducing services
Let’s go over what transpired when you ran the command. Figure 5.3 shows the
sequence of events. You instructed Kubernetes to execute the curl command inside the
container of one of your pods. Curl sent an HTTP request to the service IP, which is
backed by three pods. The Kubernetes service proxy intercepted the connection,
selected a random pod among the three pods, and forwarded the request to it. Node.js
running inside that pod then handled the request and returned an HTTP response con-
taining the pod’s name. Curl then printed the response to the standard output, which
was intercepted and printed to its standard output on your local machine by kubectl.
Why the double dash?
The double dash (--) in the command signals the end of command options for
kubectl. Everything after the double dash is the command that should be executed
inside the pod. Using the double dash isn’t necessary if the command has no
arguments that start with a dash. But in your case, if you don’t use the double dash
there, the -s option would be interpreted as an option for kubectl exec and would
result in the following strange and highly misleading error:
$ kubectl exec kubia-7nog1 curl -s http://10.111.249.153
The connection to the server 10.111.249.153 was refused – did you 
specify the right host or port?
This has nothing to do with your service refusing the connection. It’s because
kubectl is not able to connect to an API server at 10.111.249.153 (the -s option
is used to tell kubectl to connect to a different API server than the default).
3. Curl sends HTTP
GET request
4. Service redirects HTTP
connection to a randomly
selected pod
2. Curl is executed
inside the container
running node.js
6. The output of the
command is sent
curl
back to kubectl and
printed by it
5. HTTP response is
sent back to curl
Pod: kubia-7nog1
Container
node.js
curl http://
10.111.249.153
Pod: kubia-gzwli
Container
node.js
Pod: kubia-5fje3
Container
node.js
1. kubectl exec
Service: kubia
10.111.249.153:80
Figure 5.3
Using kubectl exec to test out a connection to the service by running curl in one of the pods
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="158">
  <data key="d0">Page_158</data>
  <data key="d5">Page_158</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_69">
  <data key="d0">126
CHAPTER 5
Services: enabling clients to discover and talk to pods
In the previous example, you executed the curl command as a separate process, but
inside the pod’s main container. This isn’t much different from the actual main pro-
cess in the container talking to the service.
CONFIGURING SESSION AFFINITY ON THE SERVICE
If you execute the same command a few more times, you should hit a different pod
with every invocation, because the service proxy normally forwards each connection
to a randomly selected backing pod, even if the connections are coming from the
same client. 
 If, on the other hand, you want all requests made by a certain client to be redi-
rected to the same pod every time, you can set the service’s sessionAffinity property
to ClientIP (instead of None, which is the default), as shown in the following listing.
apiVersion: v1
kind: Service             
spec:
  sessionAffinity: ClientIP
  ...
This makes the service proxy redirect all requests originating from the same client IP
to the same pod. As an exercise, you can create an additional service with session affin-
ity set to ClientIP and try sending requests to it.
 Kubernetes supports only two types of service session affinity: None and ClientIP.
You may be surprised it doesn’t have a cookie-based session affinity option, but you
need to understand that Kubernetes services don’t operate at the HTTP level. Services
deal with TCP and UDP packets and don’t care about the payload they carry. Because
cookies are a construct of the HTTP protocol, services don’t know about them, which
explains why session affinity cannot be based on cookies. 
EXPOSING MULTIPLE PORTS IN THE SAME SERVICE
Your service exposes only a single port, but services can also support multiple ports. For
example, if your pods listened on two ports—let’s say 8080 for HTTP and 8443 for
HTTPS—you could use a single service to forward both port 80 and 443 to the pod’s
ports 8080 and 8443. You don’t need to create two different services in such cases. Using
a single, multi-port service exposes all the service’s ports through a single cluster IP.
NOTE
When creating a service with multiple ports, you must specify a name
for each port.
The spec for a multi-port service is shown in the following listing.
apiVersion: v1
kind: Service             
metadata:
  name: kubia              
Listing 5.2
A example of a service with ClientIP session affinity configured
Listing 5.3
Specifying multiple ports in a service definition
 
</data>
  <data key="d5">126
CHAPTER 5
Services: enabling clients to discover and talk to pods
In the previous example, you executed the curl command as a separate process, but
inside the pod’s main container. This isn’t much different from the actual main pro-
cess in the container talking to the service.
CONFIGURING SESSION AFFINITY ON THE SERVICE
If you execute the same command a few more times, you should hit a different pod
with every invocation, because the service proxy normally forwards each connection
to a randomly selected backing pod, even if the connections are coming from the
same client. 
 If, on the other hand, you want all requests made by a certain client to be redi-
rected to the same pod every time, you can set the service’s sessionAffinity property
to ClientIP (instead of None, which is the default), as shown in the following listing.
apiVersion: v1
kind: Service             
spec:
  sessionAffinity: ClientIP
  ...
This makes the service proxy redirect all requests originating from the same client IP
to the same pod. As an exercise, you can create an additional service with session affin-
ity set to ClientIP and try sending requests to it.
 Kubernetes supports only two types of service session affinity: None and ClientIP.
You may be surprised it doesn’t have a cookie-based session affinity option, but you
need to understand that Kubernetes services don’t operate at the HTTP level. Services
deal with TCP and UDP packets and don’t care about the payload they carry. Because
cookies are a construct of the HTTP protocol, services don’t know about them, which
explains why session affinity cannot be based on cookies. 
EXPOSING MULTIPLE PORTS IN THE SAME SERVICE
Your service exposes only a single port, but services can also support multiple ports. For
example, if your pods listened on two ports—let’s say 8080 for HTTP and 8443 for
HTTPS—you could use a single service to forward both port 80 and 443 to the pod’s
ports 8080 and 8443. You don’t need to create two different services in such cases. Using
a single, multi-port service exposes all the service’s ports through a single cluster IP.
NOTE
When creating a service with multiple ports, you must specify a name
for each port.
The spec for a multi-port service is shown in the following listing.
apiVersion: v1
kind: Service             
metadata:
  name: kubia              
Listing 5.2
A example of a service with ClientIP session affinity configured
Listing 5.3
Specifying multiple ports in a service definition
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="159">
  <data key="d0">Page_159</data>
  <data key="d5">Page_159</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_70">
  <data key="d0">127
Introducing services
spec:
  ports:
  - name: http              
    port: 80                
    targetPort: 8080        
  - name: https             
    port: 443               
    targetPort: 8443        
  selector:                 
    app: kubia              
NOTE
The label selector applies to the service as a whole—it can’t be config-
ured for each port individually. If you want different ports to map to different
subsets of pods, you need to create two services.
Because your kubia pods don’t listen on multiple ports, creating a multi-port service
and a multi-port pod is left as an exercise to you.
USING NAMED PORTS
In all these examples, you’ve referred to the target port by its number, but you can also
give a name to each pod’s port and refer to it by name in the service spec. This makes
the service spec slightly clearer, especially if the port numbers aren’t well-known.
 For example, suppose your pod defines names for its ports as shown in the follow-
ing listing.
kind: Pod
spec:
  containers:
  - name: kubia
    ports:
    - name: http               
      containerPort: 8080      
    - name: https              
      containerPort: 8443      
You can then refer to those ports by name in the service spec, as shown in the follow-
ing listing.
apiVersion: v1
kind: Service             
spec:
  ports:
  - name: http              
    port: 80                
    targetPort: http        
  - name: https             
    port: 443               
    targetPort: https       
Listing 5.4
Specifying port names in a pod definition
Listing 5.5
Referring to named ports in a service
Port 80 is mapped to 
the pods’ port 8080.
Port 443 is mapped to 
pods’ port 8443.
The label selector always 
applies to the whole service.
Container’s port 
8080 is called http
Port 8443 is called https.
Port 80 is mapped to the 
container’s port called http.
Port 443 is mapped to the container’s 
port, whose name is https.
 
</data>
  <data key="d5">127
Introducing services
spec:
  ports:
  - name: http              
    port: 80                
    targetPort: 8080        
  - name: https             
    port: 443               
    targetPort: 8443        
  selector:                 
    app: kubia              
NOTE
The label selector applies to the service as a whole—it can’t be config-
ured for each port individually. If you want different ports to map to different
subsets of pods, you need to create two services.
Because your kubia pods don’t listen on multiple ports, creating a multi-port service
and a multi-port pod is left as an exercise to you.
USING NAMED PORTS
In all these examples, you’ve referred to the target port by its number, but you can also
give a name to each pod’s port and refer to it by name in the service spec. This makes
the service spec slightly clearer, especially if the port numbers aren’t well-known.
 For example, suppose your pod defines names for its ports as shown in the follow-
ing listing.
kind: Pod
spec:
  containers:
  - name: kubia
    ports:
    - name: http               
      containerPort: 8080      
    - name: https              
      containerPort: 8443      
You can then refer to those ports by name in the service spec, as shown in the follow-
ing listing.
apiVersion: v1
kind: Service             
spec:
  ports:
  - name: http              
    port: 80                
    targetPort: http        
  - name: https             
    port: 443               
    targetPort: https       
Listing 5.4
Specifying port names in a pod definition
Listing 5.5
Referring to named ports in a service
Port 80 is mapped to 
the pods’ port 8080.
Port 443 is mapped to 
pods’ port 8443.
The label selector always 
applies to the whole service.
Container’s port 
8080 is called http
Port 8443 is called https.
Port 80 is mapped to the 
container’s port called http.
Port 443 is mapped to the container’s 
port, whose name is https.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="160">
  <data key="d0">Page_160</data>
  <data key="d5">Page_160</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_71">
  <data key="d0">128
CHAPTER 5
Services: enabling clients to discover and talk to pods
But why should you even bother with naming ports? The biggest benefit of doing so is
that it enables you to change port numbers later without having to change the service
spec. Your pod currently uses port 8080 for http, but what if you later decide you’d
like to move that to port 80? 
 If you’re using named ports, all you need to do is change the port number in the
pod spec (while keeping the port’s name unchanged). As you spin up pods with the
new ports, client connections will be forwarded to the appropriate port numbers,
depending on the pod receiving the connection (port 8080 on old pods and port 80
on the new ones).
5.1.2
Discovering services
By creating a service, you now have a single and stable IP address and port that you
can hit to access your pods. This address will remain unchanged throughout the
whole lifetime of the service. Pods behind this service may come and go, their IPs may
change, their number can go up or down, but they’ll always be accessible through the
service’s single and constant IP address. 
 But how do the client pods know the IP and port of a service? Do you need to cre-
ate the service first, then manually look up its IP address and pass the IP to the config-
uration options of the client pod? Not really. Kubernetes also provides ways for client
pods to discover a service’s IP and port.
DISCOVERING SERVICES THROUGH ENVIRONMENT VARIABLES
When a pod is started, Kubernetes initializes a set of environment variables pointing
to each service that exists at that moment. If you create the service before creating the
client pods, processes in those pods can get the IP address and port of the service by
inspecting their environment variables. 
 Let’s see what those environment variables look like by examining the environment
of one of your running pods. You’ve already learned that you can use the kubectl exec
command to run a command in the pod, but because you created the service only
after your pods had been created, the environment variables for the service couldn’t
have been set yet. You’ll need to address that first.
 Before you can see environment variables for your service, you first need to delete
all the pods and let the ReplicationController create new ones. You may remember
you can delete all pods without specifying their names like this:
$ kubectl delete po --all
pod "kubia-7nog1" deleted
pod "kubia-bf50t" deleted
pod "kubia-gzwli" deleted
Now you can list the new pods (I’m sure you know how to do that) and pick one as
your target for the kubectl exec command. Once you’ve selected your target pod,
you can list environment variables by running the env command inside the container,
as shown in the following listing.
 
</data>
  <data key="d5">128
CHAPTER 5
Services: enabling clients to discover and talk to pods
But why should you even bother with naming ports? The biggest benefit of doing so is
that it enables you to change port numbers later without having to change the service
spec. Your pod currently uses port 8080 for http, but what if you later decide you’d
like to move that to port 80? 
 If you’re using named ports, all you need to do is change the port number in the
pod spec (while keeping the port’s name unchanged). As you spin up pods with the
new ports, client connections will be forwarded to the appropriate port numbers,
depending on the pod receiving the connection (port 8080 on old pods and port 80
on the new ones).
5.1.2
Discovering services
By creating a service, you now have a single and stable IP address and port that you
can hit to access your pods. This address will remain unchanged throughout the
whole lifetime of the service. Pods behind this service may come and go, their IPs may
change, their number can go up or down, but they’ll always be accessible through the
service’s single and constant IP address. 
 But how do the client pods know the IP and port of a service? Do you need to cre-
ate the service first, then manually look up its IP address and pass the IP to the config-
uration options of the client pod? Not really. Kubernetes also provides ways for client
pods to discover a service’s IP and port.
DISCOVERING SERVICES THROUGH ENVIRONMENT VARIABLES
When a pod is started, Kubernetes initializes a set of environment variables pointing
to each service that exists at that moment. If you create the service before creating the
client pods, processes in those pods can get the IP address and port of the service by
inspecting their environment variables. 
 Let’s see what those environment variables look like by examining the environment
of one of your running pods. You’ve already learned that you can use the kubectl exec
command to run a command in the pod, but because you created the service only
after your pods had been created, the environment variables for the service couldn’t
have been set yet. You’ll need to address that first.
 Before you can see environment variables for your service, you first need to delete
all the pods and let the ReplicationController create new ones. You may remember
you can delete all pods without specifying their names like this:
$ kubectl delete po --all
pod "kubia-7nog1" deleted
pod "kubia-bf50t" deleted
pod "kubia-gzwli" deleted
Now you can list the new pods (I’m sure you know how to do that) and pick one as
your target for the kubectl exec command. Once you’ve selected your target pod,
you can list environment variables by running the env command inside the container,
as shown in the following listing.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="161">
  <data key="d0">Page_161</data>
  <data key="d5">Page_161</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_72">
  <data key="d0">129
Introducing services
$ kubectl exec kubia-3inly env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-3inly
KUBERNETES_SERVICE_HOST=10.111.240.1
KUBERNETES_SERVICE_PORT=443
...
KUBIA_SERVICE_HOST=10.111.249.153             
KUBIA_SERVICE_PORT=80                            
...
Two services are defined in your cluster: the kubernetes and the kubia service (you
saw this earlier with the kubectl get svc command); consequently, two sets of service-
related environment variables are in the list. Among the variables that pertain to the
kubia service you created at the beginning of the chapter, you’ll see the KUBIA_SERVICE
_HOST and the KUBIA_SERVICE_PORT environment variables, which hold the IP address
and port of the kubia service, respectively. 
 Turning back to the frontend-backend example we started this chapter with, when
you have a frontend pod that requires the use of a backend database server pod, you
can expose the backend pod through a service called backend-database and then
have the frontend pod look up its IP address and port through the environment vari-
ables BACKEND_DATABASE_SERVICE_HOST and BACKEND_DATABASE_SERVICE_PORT.
NOTE
Dashes in the service name are converted to underscores and all let-
ters are uppercased when the service name is used as the prefix in the envi-
ronment variable’s name. 
Environment variables are one way of looking up the IP and port of a service, but isn’t
this usually the domain of DNS? Why doesn’t Kubernetes include a DNS server and
allow you to look up service IPs through DNS instead? As it turns out, it does!
DISCOVERING SERVICES THROUGH DNS
Remember in chapter 3 when you listed pods in the kube-system namespace? One of
the pods was called kube-dns. The kube-system namespace also includes a corre-
sponding service with the same name.
 As the name suggests, the pod runs a DNS server, which all other pods running in
the cluster are automatically configured to use (Kubernetes does that by modifying
each container’s /etc/resolv.conf file). Any DNS query performed by a process run-
ning in a pod will be handled by Kubernetes’ own DNS server, which knows all the ser-
vices running in your system. 
NOTE
Whether a pod uses the internal DNS server or not is configurable
through the dnsPolicy property in each pod’s spec.
Each service gets a DNS entry in the internal DNS server, and client pods that know
the name of the service can access it through its fully qualified domain name (FQDN)
instead of resorting to environment variables. 
Listing 5.6
Service-related environment variables in a container
Here’s the cluster 
IP of the service.
And here’s the port the 
service is available on.
 
</data>
  <data key="d5">129
Introducing services
$ kubectl exec kubia-3inly env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-3inly
KUBERNETES_SERVICE_HOST=10.111.240.1
KUBERNETES_SERVICE_PORT=443
...
KUBIA_SERVICE_HOST=10.111.249.153             
KUBIA_SERVICE_PORT=80                            
...
Two services are defined in your cluster: the kubernetes and the kubia service (you
saw this earlier with the kubectl get svc command); consequently, two sets of service-
related environment variables are in the list. Among the variables that pertain to the
kubia service you created at the beginning of the chapter, you’ll see the KUBIA_SERVICE
_HOST and the KUBIA_SERVICE_PORT environment variables, which hold the IP address
and port of the kubia service, respectively. 
 Turning back to the frontend-backend example we started this chapter with, when
you have a frontend pod that requires the use of a backend database server pod, you
can expose the backend pod through a service called backend-database and then
have the frontend pod look up its IP address and port through the environment vari-
ables BACKEND_DATABASE_SERVICE_HOST and BACKEND_DATABASE_SERVICE_PORT.
NOTE
Dashes in the service name are converted to underscores and all let-
ters are uppercased when the service name is used as the prefix in the envi-
ronment variable’s name. 
Environment variables are one way of looking up the IP and port of a service, but isn’t
this usually the domain of DNS? Why doesn’t Kubernetes include a DNS server and
allow you to look up service IPs through DNS instead? As it turns out, it does!
DISCOVERING SERVICES THROUGH DNS
Remember in chapter 3 when you listed pods in the kube-system namespace? One of
the pods was called kube-dns. The kube-system namespace also includes a corre-
sponding service with the same name.
 As the name suggests, the pod runs a DNS server, which all other pods running in
the cluster are automatically configured to use (Kubernetes does that by modifying
each container’s /etc/resolv.conf file). Any DNS query performed by a process run-
ning in a pod will be handled by Kubernetes’ own DNS server, which knows all the ser-
vices running in your system. 
NOTE
Whether a pod uses the internal DNS server or not is configurable
through the dnsPolicy property in each pod’s spec.
Each service gets a DNS entry in the internal DNS server, and client pods that know
the name of the service can access it through its fully qualified domain name (FQDN)
instead of resorting to environment variables. 
Listing 5.6
Service-related environment variables in a container
Here’s the cluster 
IP of the service.
And here’s the port the 
service is available on.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="162">
  <data key="d0">Page_162</data>
  <data key="d5">Page_162</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_73">
  <data key="d0">130
CHAPTER 5
Services: enabling clients to discover and talk to pods
CONNECTING TO THE SERVICE THROUGH ITS FQDN
To revisit the frontend-backend example, a frontend pod can connect to the backend-
database service by opening a connection to the following FQDN:
backend-database.default.svc.cluster.local
backend-database corresponds to the service name, default stands for the name-
space the service is defined in, and svc.cluster.local is a configurable cluster
domain suffix used in all cluster local service names. 
NOTE
The client must still know the service’s port number. If the service is
using a standard port (for example, 80 for HTTP or 5432 for Postgres), that
shouldn’t be a problem. If not, the client can get the port number from the
environment variable.
Connecting to a service can be even simpler than that. You can omit the svc.cluster
.local suffix and even the namespace, when the frontend pod is in the same name-
space as the database pod. You can thus refer to the service simply as backend-
database. That’s incredibly simple, right?
 Let’s try this. You’ll try to access the kubia service through its FQDN instead of its
IP. Again, you’ll need to do that inside an existing pod. You already know how to use
kubectl exec to run a single command in a pod’s container, but this time, instead of
running the curl command directly, you’ll run the bash shell instead, so you can then
run multiple commands in the container. This is similar to what you did in chapter 2
when you entered the container you ran with Docker by using the docker exec -it
bash command. 
RUNNING A SHELL IN A POD’S CONTAINER
You can use the kubectl exec command to run bash (or any other shell) inside a
pod’s container. This way you’re free to explore the container as long as you want,
without having to perform a kubectl exec for every command you want to run.
NOTE
The shell’s binary executable must be available in the container image
for this to work.
To use the shell properly, you need to pass the -it option to kubectl exec:
$ kubectl exec -it kubia-3inly bash
root@kubia-3inly:/# 
You’re now inside the container. You can use the curl command to access the kubia
service in any of the following ways:
root@kubia-3inly:/# curl http://kubia.default.svc.cluster.local
You’ve hit kubia-5asi2
root@kubia-3inly:/# curl http://kubia.default
You’ve hit kubia-3inly
 
</data>
  <data key="d5">130
CHAPTER 5
Services: enabling clients to discover and talk to pods
CONNECTING TO THE SERVICE THROUGH ITS FQDN
To revisit the frontend-backend example, a frontend pod can connect to the backend-
database service by opening a connection to the following FQDN:
backend-database.default.svc.cluster.local
backend-database corresponds to the service name, default stands for the name-
space the service is defined in, and svc.cluster.local is a configurable cluster
domain suffix used in all cluster local service names. 
NOTE
The client must still know the service’s port number. If the service is
using a standard port (for example, 80 for HTTP or 5432 for Postgres), that
shouldn’t be a problem. If not, the client can get the port number from the
environment variable.
Connecting to a service can be even simpler than that. You can omit the svc.cluster
.local suffix and even the namespace, when the frontend pod is in the same name-
space as the database pod. You can thus refer to the service simply as backend-
database. That’s incredibly simple, right?
 Let’s try this. You’ll try to access the kubia service through its FQDN instead of its
IP. Again, you’ll need to do that inside an existing pod. You already know how to use
kubectl exec to run a single command in a pod’s container, but this time, instead of
running the curl command directly, you’ll run the bash shell instead, so you can then
run multiple commands in the container. This is similar to what you did in chapter 2
when you entered the container you ran with Docker by using the docker exec -it
bash command. 
RUNNING A SHELL IN A POD’S CONTAINER
You can use the kubectl exec command to run bash (or any other shell) inside a
pod’s container. This way you’re free to explore the container as long as you want,
without having to perform a kubectl exec for every command you want to run.
NOTE
The shell’s binary executable must be available in the container image
for this to work.
To use the shell properly, you need to pass the -it option to kubectl exec:
$ kubectl exec -it kubia-3inly bash
root@kubia-3inly:/# 
You’re now inside the container. You can use the curl command to access the kubia
service in any of the following ways:
root@kubia-3inly:/# curl http://kubia.default.svc.cluster.local
You’ve hit kubia-5asi2
root@kubia-3inly:/# curl http://kubia.default
You’ve hit kubia-3inly
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="163">
  <data key="d0">Page_163</data>
  <data key="d5">Page_163</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_74">
  <data key="d0">131
Connecting to services living outside the cluster
root@kubia-3inly:/# curl http://kubia
You’ve hit kubia-8awf3
You can hit your service by using the service’s name as the hostname in the requested
URL. You can omit the namespace and the svc.cluster.local suffix because of how
the DNS resolver inside each pod’s container is configured. Look at the /etc/resolv.conf
file in the container and you’ll understand:
root@kubia-3inly:/# cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local ...
UNDERSTANDING WHY YOU CAN’T PING A SERVICE IP
One last thing before we move on. You know how to create services now, so you’ll soon
create your own. But what if, for whatever reason, you can’t access your service?
 You’ll probably try to figure out what’s wrong by entering an existing pod and try-
ing to access the service like you did in the last example. Then, if you still can’t access
the service with a simple curl command, maybe you’ll try to ping the service IP to see
if it’s up. Let’s try that now:
root@kubia-3inly:/# ping kubia
PING kubia.default.svc.cluster.local (10.111.249.153): 56 data bytes
^C--- kubia.default.svc.cluster.local ping statistics ---
54 packets transmitted, 0 packets received, 100% packet loss
Hmm. curl-ing the service works, but pinging it doesn’t. That’s because the service’s
cluster IP is a virtual IP, and only has meaning when combined with the service port.
We’ll explain what that means and how services work in chapter 11. I wanted to men-
tion that here because it’s the first thing users do when they try to debug a broken
service and it catches most of them off guard.
5.2
Connecting to services living outside the cluster
Up to now, we’ve talked about services backed by one or more pods running inside
the cluster. But cases exist when you’d like to expose external services through the
Kubernetes services feature. Instead of having the service redirect connections to
pods in the cluster, you want it to redirect to external IP(s) and port(s). 
 This allows you to take advantage of both service load balancing and service discov-
ery. Client pods running in the cluster can connect to the external service like they
connect to internal services.
5.2.1
Introducing service endpoints
Before going into how to do this, let me first shed more light on services. Services
don’t link to pods directly. Instead, a resource sits in between—the Endpoints
resource. You may have already noticed endpoints if you used the kubectl describe
command on your service, as shown in the following listing.
 
</data>
  <data key="d5">131
Connecting to services living outside the cluster
root@kubia-3inly:/# curl http://kubia
You’ve hit kubia-8awf3
You can hit your service by using the service’s name as the hostname in the requested
URL. You can omit the namespace and the svc.cluster.local suffix because of how
the DNS resolver inside each pod’s container is configured. Look at the /etc/resolv.conf
file in the container and you’ll understand:
root@kubia-3inly:/# cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local ...
UNDERSTANDING WHY YOU CAN’T PING A SERVICE IP
One last thing before we move on. You know how to create services now, so you’ll soon
create your own. But what if, for whatever reason, you can’t access your service?
 You’ll probably try to figure out what’s wrong by entering an existing pod and try-
ing to access the service like you did in the last example. Then, if you still can’t access
the service with a simple curl command, maybe you’ll try to ping the service IP to see
if it’s up. Let’s try that now:
root@kubia-3inly:/# ping kubia
PING kubia.default.svc.cluster.local (10.111.249.153): 56 data bytes
^C--- kubia.default.svc.cluster.local ping statistics ---
54 packets transmitted, 0 packets received, 100% packet loss
Hmm. curl-ing the service works, but pinging it doesn’t. That’s because the service’s
cluster IP is a virtual IP, and only has meaning when combined with the service port.
We’ll explain what that means and how services work in chapter 11. I wanted to men-
tion that here because it’s the first thing users do when they try to debug a broken
service and it catches most of them off guard.
5.2
Connecting to services living outside the cluster
Up to now, we’ve talked about services backed by one or more pods running inside
the cluster. But cases exist when you’d like to expose external services through the
Kubernetes services feature. Instead of having the service redirect connections to
pods in the cluster, you want it to redirect to external IP(s) and port(s). 
 This allows you to take advantage of both service load balancing and service discov-
ery. Client pods running in the cluster can connect to the external service like they
connect to internal services.
5.2.1
Introducing service endpoints
Before going into how to do this, let me first shed more light on services. Services
don’t link to pods directly. Instead, a resource sits in between—the Endpoints
resource. You may have already noticed endpoints if you used the kubectl describe
command on your service, as shown in the following listing.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="164">
  <data key="d0">Page_164</data>
  <data key="d5">Page_164</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_75">
  <data key="d0">132
CHAPTER 5
Services: enabling clients to discover and talk to pods
$ kubectl describe svc kubia
Name:                kubia
Namespace:           default
Labels:              &lt;none&gt;
Selector:            app=kubia         
Type:                ClusterIP
IP:                  10.111.249.153
Port:                &lt;unset&gt; 80/TCP
Endpoints:           10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   
Session Affinity:    None
No events.
An Endpoints resource (yes, plural) is a list of IP addresses and ports exposing a ser-
vice. The Endpoints resource is like any other Kubernetes resource, so you can display
its basic info with kubectl get:
$ kubectl get endpoints kubia
NAME    ENDPOINTS                                         AGE
kubia   10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h
Although the pod selector is defined in the service spec, it’s not used directly when
redirecting incoming connections. Instead, the selector is used to build a list of IPs
and ports, which is then stored in the Endpoints resource. When a client connects to a
service, the service proxy selects one of those IP and port pairs and redirects the
incoming connection to the server listening at that location.
5.2.2
Manually configuring service endpoints
You may have probably realized this already, but having the service’s endpoints decou-
pled from the service allows them to be configured and updated manually. 
 If you create a service without a pod selector, Kubernetes won’t even create the
Endpoints resource (after all, without a selector, it can’t know which pods to include
in the service). It’s up to you to create the Endpoints resource to specify the list of
endpoints for the service.
 To create a service with manually managed endpoints, you need to create both a
Service and an Endpoints resource. 
CREATING A SERVICE WITHOUT A SELECTOR
You’ll first create the YAML for the service itself, as shown in the following listing.
apiVersion: v1
kind: Service
metadata:
  name: external-service     
spec:                       
  ports:
  - port: 80                  
Listing 5.7
Full details of a service displayed with kubectl describe
Listing 5.8
A service without a pod selector: external-service.yaml
The service’s pod 
selector is used to 
create the list of 
endpoints.
The list of pod
IPs and ports
that represent
the endpoints of
this service
The name of the service must 
match the name of the Endpoints 
object (see next listing).
This service has no 
selector defined.
 
</data>
  <data key="d5">132
CHAPTER 5
Services: enabling clients to discover and talk to pods
$ kubectl describe svc kubia
Name:                kubia
Namespace:           default
Labels:              &lt;none&gt;
Selector:            app=kubia         
Type:                ClusterIP
IP:                  10.111.249.153
Port:                &lt;unset&gt; 80/TCP
Endpoints:           10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   
Session Affinity:    None
No events.
An Endpoints resource (yes, plural) is a list of IP addresses and ports exposing a ser-
vice. The Endpoints resource is like any other Kubernetes resource, so you can display
its basic info with kubectl get:
$ kubectl get endpoints kubia
NAME    ENDPOINTS                                         AGE
kubia   10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h
Although the pod selector is defined in the service spec, it’s not used directly when
redirecting incoming connections. Instead, the selector is used to build a list of IPs
and ports, which is then stored in the Endpoints resource. When a client connects to a
service, the service proxy selects one of those IP and port pairs and redirects the
incoming connection to the server listening at that location.
5.2.2
Manually configuring service endpoints
You may have probably realized this already, but having the service’s endpoints decou-
pled from the service allows them to be configured and updated manually. 
 If you create a service without a pod selector, Kubernetes won’t even create the
Endpoints resource (after all, without a selector, it can’t know which pods to include
in the service). It’s up to you to create the Endpoints resource to specify the list of
endpoints for the service.
 To create a service with manually managed endpoints, you need to create both a
Service and an Endpoints resource. 
CREATING A SERVICE WITHOUT A SELECTOR
You’ll first create the YAML for the service itself, as shown in the following listing.
apiVersion: v1
kind: Service
metadata:
  name: external-service     
spec:                       
  ports:
  - port: 80                  
Listing 5.7
Full details of a service displayed with kubectl describe
Listing 5.8
A service without a pod selector: external-service.yaml
The service’s pod 
selector is used to 
create the list of 
endpoints.
The list of pod
IPs and ports
that represent
the endpoints of
this service
The name of the service must 
match the name of the Endpoints 
object (see next listing).
This service has no 
selector defined.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="165">
  <data key="d0">Page_165</data>
  <data key="d5">Page_165</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_76">
  <data key="d0">133
Connecting to services living outside the cluster
You’re defining a service called external-service that will accept incoming connec-
tions on port 80. You didn’t define a pod selector for the service.
CREATING AN ENDPOINTS RESOURCE FOR A SERVICE WITHOUT A SELECTOR
Endpoints are a separate resource and not an attribute of a service. Because you cre-
ated the service without a selector, the corresponding Endpoints resource hasn’t been
created automatically, so it’s up to you to create it. The following listing shows its
YAML manifest.
apiVersion: v1
kind: Endpoints
metadata:
  name: external-service      
subsets:
  - addresses:
    - ip: 11.11.11.11         
    - ip: 22.22.22.22         
    ports:
    - port: 80      
The Endpoints object needs to have the same name as the service and contain the list
of target IP addresses and ports for the service. After both the Service and the End-
points resource are posted to the server, the service is ready to be used like any regular
service with a pod selector. Containers created after the service is created will include
the environment variables for the service, and all connections to its IP:port pair will be
load balanced between the service’s endpoints. 
 Figure 5.4 shows three pods connecting to the service with external endpoints.
If you later decide to migrate the external service to pods running inside Kubernetes,
you can add a selector to the service, thereby making its Endpoints managed automat-
ically. The same is also true in reverse—by removing the selector from a Service,
Listing 5.9
A manually created Endpoints resource: external-service-endpoints.yaml
The name of the Endpoints object 
must match the name of the 
service (see previous listing).
The IPs of the endpoints that the 
service will forward connections to
The target port of the endpoints
Pod
Pod
Pod
External server 1
IP: 11.11.11.11:80
External server 2
IP: 22.22.22.22:80
Service
10.111.249.214:80
Kubernetes cluster
Internet
Figure 5.4
Pods consuming a service with two external endpoints.
 
</data>
  <data key="d5">133
Connecting to services living outside the cluster
You’re defining a service called external-service that will accept incoming connec-
tions on port 80. You didn’t define a pod selector for the service.
CREATING AN ENDPOINTS RESOURCE FOR A SERVICE WITHOUT A SELECTOR
Endpoints are a separate resource and not an attribute of a service. Because you cre-
ated the service without a selector, the corresponding Endpoints resource hasn’t been
created automatically, so it’s up to you to create it. The following listing shows its
YAML manifest.
apiVersion: v1
kind: Endpoints
metadata:
  name: external-service      
subsets:
  - addresses:
    - ip: 11.11.11.11         
    - ip: 22.22.22.22         
    ports:
    - port: 80      
The Endpoints object needs to have the same name as the service and contain the list
of target IP addresses and ports for the service. After both the Service and the End-
points resource are posted to the server, the service is ready to be used like any regular
service with a pod selector. Containers created after the service is created will include
the environment variables for the service, and all connections to its IP:port pair will be
load balanced between the service’s endpoints. 
 Figure 5.4 shows three pods connecting to the service with external endpoints.
If you later decide to migrate the external service to pods running inside Kubernetes,
you can add a selector to the service, thereby making its Endpoints managed automat-
ically. The same is also true in reverse—by removing the selector from a Service,
Listing 5.9
A manually created Endpoints resource: external-service-endpoints.yaml
The name of the Endpoints object 
must match the name of the 
service (see previous listing).
The IPs of the endpoints that the 
service will forward connections to
The target port of the endpoints
Pod
Pod
Pod
External server 1
IP: 11.11.11.11:80
External server 2
IP: 22.22.22.22:80
Service
10.111.249.214:80
Kubernetes cluster
Internet
Figure 5.4
Pods consuming a service with two external endpoints.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="166">
  <data key="d0">Page_166</data>
  <data key="d5">Page_166</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_77">
  <data key="d0">134
CHAPTER 5
Services: enabling clients to discover and talk to pods
Kubernetes stops updating its Endpoints. This means a service IP address can remain
constant while the actual implementation of the service is changed. 
5.2.3
Creating an alias for an external service
Instead of exposing an external service by manually configuring the service’s End-
points, a simpler method allows you to refer to an external service by its fully qualified
domain name (FQDN).
CREATING AN EXTERNALNAME SERVICE
To create a service that serves as an alias for an external service, you create a Service
resource with the type field set to ExternalName. For example, let’s imagine there’s a
public API available at api.somecompany.com. You can define a service that points to
it as shown in the following listing.
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName                       
  externalName: someapi.somecompany.com     
  ports:
  - port: 80
After the service is created, pods can connect to the external service through the
external-service.default.svc.cluster.local domain name (or even external-
service) instead of using the service’s actual FQDN. This hides the actual service
name and its location from pods consuming the service, allowing you to modify the
service definition and point it to a different service any time later, by only changing
the externalName attribute or by changing the type back to ClusterIP and creating
an Endpoints object for the service—either manually or by specifying a label selector
on the service and having it created automatically.
 ExternalName services are implemented solely at the DNS level—a simple CNAME
DNS record is created for the service. Therefore, clients connecting to the service will
connect to the external service directly, bypassing the service proxy completely. For
this reason, these types of services don’t even get a cluster IP. 
NOTE
A CNAME record points to a fully qualified domain name instead of a
numeric IP address.
5.3
Exposing services to external clients
Up to now, we’ve only talked about how services can be consumed by pods from inside
the cluster. But you’ll also want to expose certain services, such as frontend webserv-
ers, to the outside, so external clients can access them, as depicted in figure 5.5.
Listing 5.10
An ExternalName-type service: external-service-externalname.yaml
Service type is set 
to ExternalName
The fully qualified domain 
name of the actual service
 
</data>
  <data key="d5">134
CHAPTER 5
Services: enabling clients to discover and talk to pods
Kubernetes stops updating its Endpoints. This means a service IP address can remain
constant while the actual implementation of the service is changed. 
5.2.3
Creating an alias for an external service
Instead of exposing an external service by manually configuring the service’s End-
points, a simpler method allows you to refer to an external service by its fully qualified
domain name (FQDN).
CREATING AN EXTERNALNAME SERVICE
To create a service that serves as an alias for an external service, you create a Service
resource with the type field set to ExternalName. For example, let’s imagine there’s a
public API available at api.somecompany.com. You can define a service that points to
it as shown in the following listing.
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName                       
  externalName: someapi.somecompany.com     
  ports:
  - port: 80
After the service is created, pods can connect to the external service through the
external-service.default.svc.cluster.local domain name (or even external-
service) instead of using the service’s actual FQDN. This hides the actual service
name and its location from pods consuming the service, allowing you to modify the
service definition and point it to a different service any time later, by only changing
the externalName attribute or by changing the type back to ClusterIP and creating
an Endpoints object for the service—either manually or by specifying a label selector
on the service and having it created automatically.
 ExternalName services are implemented solely at the DNS level—a simple CNAME
DNS record is created for the service. Therefore, clients connecting to the service will
connect to the external service directly, bypassing the service proxy completely. For
this reason, these types of services don’t even get a cluster IP. 
NOTE
A CNAME record points to a fully qualified domain name instead of a
numeric IP address.
5.3
Exposing services to external clients
Up to now, we’ve only talked about how services can be consumed by pods from inside
the cluster. But you’ll also want to expose certain services, such as frontend webserv-
ers, to the outside, so external clients can access them, as depicted in figure 5.5.
Listing 5.10
An ExternalName-type service: external-service-externalname.yaml
Service type is set 
to ExternalName
The fully qualified domain 
name of the actual service
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="167">
  <data key="d0">Page_167</data>
  <data key="d5">Page_167</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_78">
  <data key="d0">135
Exposing services to external clients
You have a few ways to make a service accessible externally:
Setting the service type to NodePort—For a NodePort service, each cluster node
opens a port on the node itself (hence the name) and redirects traffic received
on that port to the underlying service. The service isn’t accessible only at the
internal cluster IP and port, but also through a dedicated port on all nodes. 
Setting the service type to LoadBalancer, an extension of the NodePort type—This
makes the service accessible through a dedicated load balancer, provisioned
from the cloud infrastructure Kubernetes is running on. The load balancer redi-
rects traffic to the node port across all the nodes. Clients connect to the service
through the load balancer’s IP.
Creating an Ingress resource, a radically different mechanism for exposing multiple ser-
vices through a single IP address—It operates at the HTTP level (network layer 7)
and can thus offer more features than layer 4 services can. We’ll explain Ingress
resources in section 5.4. 
5.3.1
Using a NodePort service
The first method of exposing a set of pods to external clients is by creating a service
and setting its type to NodePort. By creating a NodePort service, you make Kubernetes
reserve a port on all its nodes (the same port number is used across all of them) and
forward incoming connections to the pods that are part of the service. 
 This is similar to a regular service (their actual type is ClusterIP), but a NodePort
service can be accessed not only through the service’s internal cluster IP, but also
through any node’s IP and the reserved node port. 
 This will make more sense when you try interacting with a NodePort service.
CREATING A NODEPORT SERVICE
You’ll now create a NodePort service to see how you can use it. The following listing
shows the YAML for the service.
 
Kubernetes cluster
External client
Service
Pod
Pod
Pod
Figure 5.5
Exposing a service to external clients
 
</data>
  <data key="d5">135
Exposing services to external clients
You have a few ways to make a service accessible externally:
Setting the service type to NodePort—For a NodePort service, each cluster node
opens a port on the node itself (hence the name) and redirects traffic received
on that port to the underlying service. The service isn’t accessible only at the
internal cluster IP and port, but also through a dedicated port on all nodes. 
Setting the service type to LoadBalancer, an extension of the NodePort type—This
makes the service accessible through a dedicated load balancer, provisioned
from the cloud infrastructure Kubernetes is running on. The load balancer redi-
rects traffic to the node port across all the nodes. Clients connect to the service
through the load balancer’s IP.
Creating an Ingress resource, a radically different mechanism for exposing multiple ser-
vices through a single IP address—It operates at the HTTP level (network layer 7)
and can thus offer more features than layer 4 services can. We’ll explain Ingress
resources in section 5.4. 
5.3.1
Using a NodePort service
The first method of exposing a set of pods to external clients is by creating a service
and setting its type to NodePort. By creating a NodePort service, you make Kubernetes
reserve a port on all its nodes (the same port number is used across all of them) and
forward incoming connections to the pods that are part of the service. 
 This is similar to a regular service (their actual type is ClusterIP), but a NodePort
service can be accessed not only through the service’s internal cluster IP, but also
through any node’s IP and the reserved node port. 
 This will make more sense when you try interacting with a NodePort service.
CREATING A NODEPORT SERVICE
You’ll now create a NodePort service to see how you can use it. The following listing
shows the YAML for the service.
 
Kubernetes cluster
External client
Service
Pod
Pod
Pod
Figure 5.5
Exposing a service to external clients
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="168">
  <data key="d0">Page_168</data>
  <data key="d5">Page_168</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_79">
  <data key="d0">136
CHAPTER 5
Services: enabling clients to discover and talk to pods
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort            
  ports:
  - port: 80                 
    targetPort: 8080        
    nodePort: 30123        
  selector:
    app: kubia
You set the type to NodePort and specify the node port this service should be bound to
across all cluster nodes. Specifying the port isn’t mandatory; Kubernetes will choose a
random port if you omit it. 
NOTE
When you create the service in GKE, kubectl prints out a warning
about having to configure firewall rules. We’ll see how to do that soon. 
EXAMINING YOUR NODEPORT SERVICE
Let’s see the basic information of your service to learn more about it:
$ kubectl get svc kubia-nodeport
NAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubia-nodeport   10.111.254.223   &lt;nodes&gt;       80:30123/TCP   2m
Look at the EXTERNAL-IP column. It shows &lt;nodes&gt;, indicating the service is accessible
through the IP address of any cluster node. The PORT(S) column shows both the
internal port of the cluster IP (80) and the node port (30123). The service is accessi-
ble at the following addresses:

10.11.254.223:80

&lt;1st node’s IP&gt;:30123

&lt;2nd node’s IP&gt;:30123, and so on.
Figure 5.6 shows your service exposed on port 30123 of both of your cluster nodes
(this applies if you’re running this on GKE; Minikube only has a single node, but the
principle is the same). An incoming connection to one of those ports will be redi-
rected to a randomly selected pod, which may or may not be the one running on the
node the connection is being made to. 
 
 
 
Listing 5.11
A NodePort service definition: kubia-svc-nodeport.yaml
Set the service 
type to NodePort.
This is the port of the 
service’s internal cluster IP.
This is the target port 
of the backing pods.
The service will be accessible 
through port 30123 of each of 
your cluster nodes.
 
</data>
  <data key="d5">136
CHAPTER 5
Services: enabling clients to discover and talk to pods
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort            
  ports:
  - port: 80                 
    targetPort: 8080        
    nodePort: 30123        
  selector:
    app: kubia
You set the type to NodePort and specify the node port this service should be bound to
across all cluster nodes. Specifying the port isn’t mandatory; Kubernetes will choose a
random port if you omit it. 
NOTE
When you create the service in GKE, kubectl prints out a warning
about having to configure firewall rules. We’ll see how to do that soon. 
EXAMINING YOUR NODEPORT SERVICE
Let’s see the basic information of your service to learn more about it:
$ kubectl get svc kubia-nodeport
NAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubia-nodeport   10.111.254.223   &lt;nodes&gt;       80:30123/TCP   2m
Look at the EXTERNAL-IP column. It shows &lt;nodes&gt;, indicating the service is accessible
through the IP address of any cluster node. The PORT(S) column shows both the
internal port of the cluster IP (80) and the node port (30123). The service is accessi-
ble at the following addresses:

10.11.254.223:80

&lt;1st node’s IP&gt;:30123

&lt;2nd node’s IP&gt;:30123, and so on.
Figure 5.6 shows your service exposed on port 30123 of both of your cluster nodes
(this applies if you’re running this on GKE; Minikube only has a single node, but the
principle is the same). An incoming connection to one of those ports will be redi-
rected to a randomly selected pod, which may or may not be the one running on the
node the connection is being made to. 
 
 
 
Listing 5.11
A NodePort service definition: kubia-svc-nodeport.yaml
Set the service 
type to NodePort.
This is the port of the 
service’s internal cluster IP.
This is the target port 
of the backing pods.
The service will be accessible 
through port 30123 of each of 
your cluster nodes.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="169">
  <data key="d0">Page_169</data>
  <data key="d5">Page_169</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_80">
  <data key="d0">137
Exposing services to external clients
A connection received on port 30123 of the first node might be forwarded either to
the pod running on the first node or to one of the pods running on the second node.
CHANGING FIREWALL RULES TO LET EXTERNAL CLIENTS ACCESS OUR NODEPORT SERVICE
As I’ve mentioned previously, before you can access your service through the node
port, you need to configure the Google Cloud Platform’s firewalls to allow external
connections to your nodes on that port. You’ll do this now:
$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123
Created [https://www.googleapis.com/compute/v1/projects/kubia-
1295/global/firewalls/kubia-svc-rule].
NAME            NETWORK  SRC_RANGES  RULES      SRC_TAGS  TARGET_TAGS
kubia-svc-rule  default  0.0.0.0/0   tcp:30123
You can access your service through port 30123 of one of the node’s IPs. But you need
to figure out the IP of a node first. Refer to the sidebar on how to do that.
 
 
 
Kubernetes cluster
External client
Pod
Node 2
IP: 130.211.99.206
Node 1
IP: 130.211.97.55
Port 30123
Port 8080
Pod
Port 8080
Pod
Port 30123
Port 8080
Service
Figure 5.6
An external client connecting to a NodePort service either through Node 1 or 2
 
</data>
  <data key="d5">137
Exposing services to external clients
A connection received on port 30123 of the first node might be forwarded either to
the pod running on the first node or to one of the pods running on the second node.
CHANGING FIREWALL RULES TO LET EXTERNAL CLIENTS ACCESS OUR NODEPORT SERVICE
As I’ve mentioned previously, before you can access your service through the node
port, you need to configure the Google Cloud Platform’s firewalls to allow external
connections to your nodes on that port. You’ll do this now:
$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123
Created [https://www.googleapis.com/compute/v1/projects/kubia-
1295/global/firewalls/kubia-svc-rule].
NAME            NETWORK  SRC_RANGES  RULES      SRC_TAGS  TARGET_TAGS
kubia-svc-rule  default  0.0.0.0/0   tcp:30123
You can access your service through port 30123 of one of the node’s IPs. But you need
to figure out the IP of a node first. Refer to the sidebar on how to do that.
 
 
 
Kubernetes cluster
External client
Pod
Node 2
IP: 130.211.99.206
Node 1
IP: 130.211.97.55
Port 30123
Port 8080
Pod
Port 8080
Pod
Port 30123
Port 8080
Service
Figure 5.6
An external client connecting to a NodePort service either through Node 1 or 2
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="170">
  <data key="d0">Page_170</data>
  <data key="d5">Page_170</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_81">
  <data key="d0">138
CHAPTER 5
Services: enabling clients to discover and talk to pods
Once you know the IPs of your nodes, you can try accessing your service through them:
$ curl http://130.211.97.55:30123
You've hit kubia-ym8or
$ curl http://130.211.99.206:30123
You've hit kubia-xueq1
TIP
When using Minikube, you can easily access your NodePort services
through your browser by running minikube service &lt;service-name&gt; [-n
&lt;namespace&gt;].
As you can see, your pods are now accessible to the whole internet through port 30123
on any of your nodes. It doesn’t matter what node a client sends the request to. But if
you only point your clients to the first node, when that node fails, your clients can’t
access the service anymore. That’s why it makes sense to put a load balancer in front
of the nodes to make sure you’re spreading requests across all healthy nodes and
never sending them to a node that’s offline at that moment. 
 If your Kubernetes cluster supports it (which is mostly true when Kubernetes is
deployed on cloud infrastructure), the load balancer can be provisioned automati-
cally by creating a LoadBalancer instead of a NodePort service. We’ll look at this next.
5.3.2
Exposing a service through an external load balancer
Kubernetes clusters running on cloud providers usually support the automatic provi-
sion of a load balancer from the cloud infrastructure. All you need to do is set the
Using JSONPath to get the IPs of all your nodes 
You can find the IP in the JSON or YAML descriptors of the nodes. But instead of
sifting through the relatively large JSON, you can tell kubectl to print out only the
node IP instead of the whole service definition: 
$ kubectl get nodes -o jsonpath='{.items[*].status.
➥ addresses[?(@.type=="ExternalIP")].address}'
130.211.97.55 130.211.99.206
You’re telling kubectl to only output the information you want by specifying a
JSONPath. You’re probably familiar with XPath and how it’s used with XML. JSONPath
is basically XPath for JSON. The JSONPath in the previous example instructs kubectl
to do the following:
Go through all the elements in the items attribute.
For each element, enter the status attribute.
Filter elements of the addresses attribute, taking only those that have the
type attribute set to ExternalIP.
Finally, print the address attribute of the filtered elements.
To learn more about how to use JSONPath with kubectl, refer to the documentation
at http:/
/kubernetes.io/docs/user-guide/jsonpath. 
 
</data>
  <data key="d5">138
CHAPTER 5
Services: enabling clients to discover and talk to pods
Once you know the IPs of your nodes, you can try accessing your service through them:
$ curl http://130.211.97.55:30123
You've hit kubia-ym8or
$ curl http://130.211.99.206:30123
You've hit kubia-xueq1
TIP
When using Minikube, you can easily access your NodePort services
through your browser by running minikube service &lt;service-name&gt; [-n
&lt;namespace&gt;].
As you can see, your pods are now accessible to the whole internet through port 30123
on any of your nodes. It doesn’t matter what node a client sends the request to. But if
you only point your clients to the first node, when that node fails, your clients can’t
access the service anymore. That’s why it makes sense to put a load balancer in front
of the nodes to make sure you’re spreading requests across all healthy nodes and
never sending them to a node that’s offline at that moment. 
 If your Kubernetes cluster supports it (which is mostly true when Kubernetes is
deployed on cloud infrastructure), the load balancer can be provisioned automati-
cally by creating a LoadBalancer instead of a NodePort service. We’ll look at this next.
5.3.2
Exposing a service through an external load balancer
Kubernetes clusters running on cloud providers usually support the automatic provi-
sion of a load balancer from the cloud infrastructure. All you need to do is set the
Using JSONPath to get the IPs of all your nodes 
You can find the IP in the JSON or YAML descriptors of the nodes. But instead of
sifting through the relatively large JSON, you can tell kubectl to print out only the
node IP instead of the whole service definition: 
$ kubectl get nodes -o jsonpath='{.items[*].status.
➥ addresses[?(@.type=="ExternalIP")].address}'
130.211.97.55 130.211.99.206
You’re telling kubectl to only output the information you want by specifying a
JSONPath. You’re probably familiar with XPath and how it’s used with XML. JSONPath
is basically XPath for JSON. The JSONPath in the previous example instructs kubectl
to do the following:
Go through all the elements in the items attribute.
For each element, enter the status attribute.
Filter elements of the addresses attribute, taking only those that have the
type attribute set to ExternalIP.
Finally, print the address attribute of the filtered elements.
To learn more about how to use JSONPath with kubectl, refer to the documentation
at http:/
/kubernetes.io/docs/user-guide/jsonpath. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="171">
  <data key="d0">Page_171</data>
  <data key="d5">Page_171</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_82">
  <data key="d0">139
Exposing services to external clients
service’s type to LoadBalancer instead of NodePort. The load balancer will have its
own unique, publicly accessible IP address and will redirect all connections to your
service. You can thus access your service through the load balancer’s IP address. 
 If Kubernetes is running in an environment that doesn’t support LoadBalancer
services, the load balancer will not be provisioned, but the service will still behave like
a NodePort service. That’s because a LoadBalancer service is an extension of a Node-
Port service. You’ll run this example on Google Kubernetes Engine, which supports
LoadBalancer services. Minikube doesn’t, at least not as of this writing. 
CREATING A LOADBALANCER SERVICE
To create a service with a load balancer in front, create the service from the following
YAML manifest, as shown in the following listing.
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer          
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
The service type is set to LoadBalancer instead of NodePort. You’re not specifying a spe-
cific node port, although you could (you’re letting Kubernetes choose one instead). 
CONNECTING TO THE SERVICE THROUGH THE LOAD BALANCER
After you create the service, it takes time for the cloud infrastructure to create the
load balancer and write its IP address into the Service object. Once it does that, the IP
address will be listed as the external IP address of your service:
$ kubectl get svc kubia-loadbalancer
NAME                 CLUSTER-IP       EXTERNAL-IP      PORT(S)         AGE
kubia-loadbalancer   10.111.241.153   130.211.53.173   80:32143/TCP    1m
In this case, the load balancer is available at IP 130.211.53.173, so you can now access
the service at that IP address:
$ curl http://130.211.53.173
You've hit kubia-xueq1
Success! As you may have noticed, this time you didn’t need to mess with firewalls the
way you had to before with the NodePort service.
Listing 5.12
A LoadBalancer-type service: kubia-svc-loadbalancer.yaml
This type of service obtains 
a load balancer from the 
infrastructure hosting the 
Kubernetes cluster.
 
</data>
  <data key="d5">139
Exposing services to external clients
service’s type to LoadBalancer instead of NodePort. The load balancer will have its
own unique, publicly accessible IP address and will redirect all connections to your
service. You can thus access your service through the load balancer’s IP address. 
 If Kubernetes is running in an environment that doesn’t support LoadBalancer
services, the load balancer will not be provisioned, but the service will still behave like
a NodePort service. That’s because a LoadBalancer service is an extension of a Node-
Port service. You’ll run this example on Google Kubernetes Engine, which supports
LoadBalancer services. Minikube doesn’t, at least not as of this writing. 
CREATING A LOADBALANCER SERVICE
To create a service with a load balancer in front, create the service from the following
YAML manifest, as shown in the following listing.
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer          
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
The service type is set to LoadBalancer instead of NodePort. You’re not specifying a spe-
cific node port, although you could (you’re letting Kubernetes choose one instead). 
CONNECTING TO THE SERVICE THROUGH THE LOAD BALANCER
After you create the service, it takes time for the cloud infrastructure to create the
load balancer and write its IP address into the Service object. Once it does that, the IP
address will be listed as the external IP address of your service:
$ kubectl get svc kubia-loadbalancer
NAME                 CLUSTER-IP       EXTERNAL-IP      PORT(S)         AGE
kubia-loadbalancer   10.111.241.153   130.211.53.173   80:32143/TCP    1m
In this case, the load balancer is available at IP 130.211.53.173, so you can now access
the service at that IP address:
$ curl http://130.211.53.173
You've hit kubia-xueq1
Success! As you may have noticed, this time you didn’t need to mess with firewalls the
way you had to before with the NodePort service.
Listing 5.12
A LoadBalancer-type service: kubia-svc-loadbalancer.yaml
This type of service obtains 
a load balancer from the 
infrastructure hosting the 
Kubernetes cluster.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="172">
  <data key="d0">Page_172</data>
  <data key="d5">Page_172</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_83">
  <data key="d0">140
CHAPTER 5
Services: enabling clients to discover and talk to pods
See figure 5.7 to see how HTTP requests are delivered to the pod. External clients
(curl in your case) connect to port 80 of the load balancer and get routed to the
Session affinity and web browsers
Because your service is now exposed externally, you may try accessing it with your
web browser. You’ll see something that may strike you as odd—the browser will hit
the exact same pod every time. Did the service’s session affinity change in the
meantime? With kubectl explain, you can double-check that the service’s session
affinity is still set to None, so why don’t different browser requests hit different
pods, as is the case when using curl?
Let me explain what’s happening. The browser is using keep-alive connections and
sends all its requests through a single connection, whereas curl opens a new
connection every time. Services work at the connection level, so when a connection to a
service is first opened, a random pod is selected and then all network packets belonging
to that connection are all sent to that single pod. Even if session affinity is set to None,
users will always hit the same pod (until the connection is closed).
Kubernetes cluster
External client
Load balancer
IP: 130.211.53.173:80
Pod
Node 2
IP: 130.211.99.206
Node 1
IP: 130.211.97.55
Port 32143
Port 8080
Pod
Port 8080
Pod
Port 32143
Port 8080
Service
Figure 5.7
An external client connecting to a LoadBalancer service
 
</data>
  <data key="d5">140
CHAPTER 5
Services: enabling clients to discover and talk to pods
See figure 5.7 to see how HTTP requests are delivered to the pod. External clients
(curl in your case) connect to port 80 of the load balancer and get routed to the
Session affinity and web browsers
Because your service is now exposed externally, you may try accessing it with your
web browser. You’ll see something that may strike you as odd—the browser will hit
the exact same pod every time. Did the service’s session affinity change in the
meantime? With kubectl explain, you can double-check that the service’s session
affinity is still set to None, so why don’t different browser requests hit different
pods, as is the case when using curl?
Let me explain what’s happening. The browser is using keep-alive connections and
sends all its requests through a single connection, whereas curl opens a new
connection every time. Services work at the connection level, so when a connection to a
service is first opened, a random pod is selected and then all network packets belonging
to that connection are all sent to that single pod. Even if session affinity is set to None,
users will always hit the same pod (until the connection is closed).
Kubernetes cluster
External client
Load balancer
IP: 130.211.53.173:80
Pod
Node 2
IP: 130.211.99.206
Node 1
IP: 130.211.97.55
Port 32143
Port 8080
Pod
Port 8080
Pod
Port 32143
Port 8080
Service
Figure 5.7
An external client connecting to a LoadBalancer service
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="173">
  <data key="d0">Page_173</data>
  <data key="d5">Page_173</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_84">
  <data key="d0">141
Exposing services to external clients
implicitly assigned node port on one of the nodes. From there, the connection is for-
warded to one of the pod instances.
 As already mentioned, a LoadBalancer-type service is a NodePort service with an
additional infrastructure-provided load balancer. If you use kubectl describe to dis-
play additional info about the service, you’ll see that a node port has been selected for
the service. If you were to open the firewall for this port, the way you did in the previ-
ous section about NodePort services, you could access the service through the node
IPs as well.
TIP
If you’re using Minikube, even though the load balancer will never be
provisioned, you can still access the service through the node port (at the
Minikube VM’s IP address).
5.3.3
Understanding the peculiarities of external connections
You must be aware of several things related to externally originating connections to
services. 
UNDERSTANDING AND PREVENTING UNNECESSARY NETWORK HOPS
When an external client connects to a service through the node port (this also
includes cases when it goes through the load balancer first), the randomly chosen
pod may or may not be running on the same node that received the connection. An
additional network hop is required to reach the pod, but this may not always be
desirable. 
 You can prevent this additional hop by configuring the service to redirect external
traffic only to pods running on the node that received the connection. This is done by
setting the externalTrafficPolicy field in the service’s spec section:
spec:
  externalTrafficPolicy: Local
  ...
If a service definition includes this setting and an external connection is opened
through the service’s node port, the service proxy will choose a locally running pod. If
no local pods exist, the connection will hang (it won’t be forwarded to a random
global pod, the way connections are when not using the annotation). You therefore
need to ensure the load balancer forwards connections only to nodes that have at
least one such pod.
 Using this annotation also has other drawbacks. Normally, connections are spread
evenly across all the pods, but when using this annotation, that’s no longer the case.
 Imagine having two nodes and three pods. Let’s say node A runs one pod and
node B runs the other two. If the load balancer spreads connections evenly across the
two nodes, the pod on node A will receive 50% of all connections, but the two pods on
node B will only receive 25% each, as shown in figure 5.8.
 
</data>
  <data key="d5">141
Exposing services to external clients
implicitly assigned node port on one of the nodes. From there, the connection is for-
warded to one of the pod instances.
 As already mentioned, a LoadBalancer-type service is a NodePort service with an
additional infrastructure-provided load balancer. If you use kubectl describe to dis-
play additional info about the service, you’ll see that a node port has been selected for
the service. If you were to open the firewall for this port, the way you did in the previ-
ous section about NodePort services, you could access the service through the node
IPs as well.
TIP
If you’re using Minikube, even though the load balancer will never be
provisioned, you can still access the service through the node port (at the
Minikube VM’s IP address).
5.3.3
Understanding the peculiarities of external connections
You must be aware of several things related to externally originating connections to
services. 
UNDERSTANDING AND PREVENTING UNNECESSARY NETWORK HOPS
When an external client connects to a service through the node port (this also
includes cases when it goes through the load balancer first), the randomly chosen
pod may or may not be running on the same node that received the connection. An
additional network hop is required to reach the pod, but this may not always be
desirable. 
 You can prevent this additional hop by configuring the service to redirect external
traffic only to pods running on the node that received the connection. This is done by
setting the externalTrafficPolicy field in the service’s spec section:
spec:
  externalTrafficPolicy: Local
  ...
If a service definition includes this setting and an external connection is opened
through the service’s node port, the service proxy will choose a locally running pod. If
no local pods exist, the connection will hang (it won’t be forwarded to a random
global pod, the way connections are when not using the annotation). You therefore
need to ensure the load balancer forwards connections only to nodes that have at
least one such pod.
 Using this annotation also has other drawbacks. Normally, connections are spread
evenly across all the pods, but when using this annotation, that’s no longer the case.
 Imagine having two nodes and three pods. Let’s say node A runs one pod and
node B runs the other two. If the load balancer spreads connections evenly across the
two nodes, the pod on node A will receive 50% of all connections, but the two pods on
node B will only receive 25% each, as shown in figure 5.8.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="174">
  <data key="d0">Page_174</data>
  <data key="d5">Page_174</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_85">
  <data key="d0">142
CHAPTER 5
Services: enabling clients to discover and talk to pods
BEING AWARE OF THE NON-PRESERVATION OF THE CLIENT’S IP
Usually, when clients inside the cluster connect to a service, the pods backing the ser-
vice can obtain the client’s IP address. But when the connection is received through a
node port, the packets’ source IP is changed, because Source Network Address Trans-
lation (SNAT) is performed on the packets. 
 The backing pod can’t see the actual client’s IP, which may be a problem for some
applications that need to know the client’s IP. In the case of a web server, for example,
this means the access log won’t show the browser’s IP.
 The Local external traffic policy described in the previous section affects the pres-
ervation of the client’s IP, because there’s no additional hop between the node receiv-
ing the connection and the node hosting the target pod (SNAT isn’t performed).
5.4
Exposing services externally through an Ingress 
resource
You’ve now seen two ways of exposing a service to clients outside the cluster, but
another method exists—creating an Ingress resource.
DEFINITION
Ingress (noun)—The act of going in or entering; the right to
enter; a means or place of entering; entryway. 
Let me first explain why you need another way to access Kubernetes services from the
outside. 
UNDERSTANDING WHY INGRESSES ARE NEEDED
One important reason is that each LoadBalancer service requires its own load bal-
ancer with its own public IP address, whereas an Ingress only requires one, even when
providing access to dozens of services. When a client sends an HTTP request to the
Ingress, the host and path in the request determine which service the request is for-
warded to, as shown in figure 5.9.
 
50%
50%
50%
25%
25%
Node A
Pod
Node B
Pod
Pod
Load balancer
Figure 5.8
A Service using 
the Local external traffic 
policy may lead to uneven 
load distribution across pods.
 
</data>
  <data key="d5">142
CHAPTER 5
Services: enabling clients to discover and talk to pods
BEING AWARE OF THE NON-PRESERVATION OF THE CLIENT’S IP
Usually, when clients inside the cluster connect to a service, the pods backing the ser-
vice can obtain the client’s IP address. But when the connection is received through a
node port, the packets’ source IP is changed, because Source Network Address Trans-
lation (SNAT) is performed on the packets. 
 The backing pod can’t see the actual client’s IP, which may be a problem for some
applications that need to know the client’s IP. In the case of a web server, for example,
this means the access log won’t show the browser’s IP.
 The Local external traffic policy described in the previous section affects the pres-
ervation of the client’s IP, because there’s no additional hop between the node receiv-
ing the connection and the node hosting the target pod (SNAT isn’t performed).
5.4
Exposing services externally through an Ingress 
resource
You’ve now seen two ways of exposing a service to clients outside the cluster, but
another method exists—creating an Ingress resource.
DEFINITION
Ingress (noun)—The act of going in or entering; the right to
enter; a means or place of entering; entryway. 
Let me first explain why you need another way to access Kubernetes services from the
outside. 
UNDERSTANDING WHY INGRESSES ARE NEEDED
One important reason is that each LoadBalancer service requires its own load bal-
ancer with its own public IP address, whereas an Ingress only requires one, even when
providing access to dozens of services. When a client sends an HTTP request to the
Ingress, the host and path in the request determine which service the request is for-
warded to, as shown in figure 5.9.
 
50%
50%
50%
25%
25%
Node A
Pod
Node B
Pod
Pod
Load balancer
Figure 5.8
A Service using 
the Local external traffic 
policy may lead to uneven 
load distribution across pods.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="175">
  <data key="d0">Page_175</data>
  <data key="d5">Page_175</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_86">
  <data key="d0">143
Exposing services externally through an Ingress resource
Ingresses operate at the application layer of the network stack (HTTP) and can pro-
vide features such as cookie-based session affinity and the like, which services can’t.
UNDERSTANDING THAT AN INGRESS CONTROLLER IS REQUIRED
Before we go into the features an Ingress object provides, let me emphasize that to
make Ingress resources work, an Ingress controller needs to be running in the cluster.
Different Kubernetes environments use different implementations of the controller,
but several don’t provide a default controller at all. 
 For example, Google Kubernetes Engine uses Google Cloud Platform’s own HTTP
load-balancing features to provide the Ingress functionality. Initially, Minikube didn’t
provide a controller out of the box, but it now includes an add-on that can be enabled
to let you try out the Ingress functionality. Follow the instructions in the following
sidebar to ensure it’s enabled.
Enabling the Ingress add-on in Minikube
If you’re using Minikube to run the examples in this book, you’ll need to ensure the
Ingress add-on is enabled. You can check whether it is by listing all the add-ons:
$ minikube addons list
- default-storageclass: enabled
- kube-dns: enabled
- heapster: disabled
- ingress: disabled               
- registry-creds: disabled
- addon-manager: enabled
- dashboard: enabled
You’ll learn about what these add-ons are throughout the book, but it should be
pretty clear what the dashboard and the kube-dns add-ons do. Enable the Ingress
add-on so you can see Ingresses in action:
$ minikube addons enable ingress
ingress was successfully enabled
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Ingress
Client
Service
kubia.example.com/kubia
foo.example.com
kubia.example.com/foo
Service
bar.example.com
Service
Service
Figure 5.9
Multiple services can be exposed through a single Ingress.
The Ingress add-on 
isn’t enabled.
 
</data>
  <data key="d5">143
Exposing services externally through an Ingress resource
Ingresses operate at the application layer of the network stack (HTTP) and can pro-
vide features such as cookie-based session affinity and the like, which services can’t.
UNDERSTANDING THAT AN INGRESS CONTROLLER IS REQUIRED
Before we go into the features an Ingress object provides, let me emphasize that to
make Ingress resources work, an Ingress controller needs to be running in the cluster.
Different Kubernetes environments use different implementations of the controller,
but several don’t provide a default controller at all. 
 For example, Google Kubernetes Engine uses Google Cloud Platform’s own HTTP
load-balancing features to provide the Ingress functionality. Initially, Minikube didn’t
provide a controller out of the box, but it now includes an add-on that can be enabled
to let you try out the Ingress functionality. Follow the instructions in the following
sidebar to ensure it’s enabled.
Enabling the Ingress add-on in Minikube
If you’re using Minikube to run the examples in this book, you’ll need to ensure the
Ingress add-on is enabled. You can check whether it is by listing all the add-ons:
$ minikube addons list
- default-storageclass: enabled
- kube-dns: enabled
- heapster: disabled
- ingress: disabled               
- registry-creds: disabled
- addon-manager: enabled
- dashboard: enabled
You’ll learn about what these add-ons are throughout the book, but it should be
pretty clear what the dashboard and the kube-dns add-ons do. Enable the Ingress
add-on so you can see Ingresses in action:
$ minikube addons enable ingress
ingress was successfully enabled
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Pod
Ingress
Client
Service
kubia.example.com/kubia
foo.example.com
kubia.example.com/foo
Service
bar.example.com
Service
Service
Figure 5.9
Multiple services can be exposed through a single Ingress.
The Ingress add-on 
isn’t enabled.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="176">
  <data key="d0">Page_176</data>
  <data key="d5">Page_176</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_87">
  <data key="d0">144
CHAPTER 5
Services: enabling clients to discover and talk to pods
TIP
The --all-namespaces option mentioned in the sidebar is handy when
you don’t know what namespace your pod (or other type of resource) is in, or
if you want to list resources across all namespaces.
5.4.1
Creating an Ingress resource
You’ve confirmed there’s an Ingress controller running in your cluster, so you can
now create an Ingress resource. The following listing shows what the YAML manifest
for the Ingress looks like.
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com             
    http:
      paths:
      - path: /                           
        backend:
          serviceName: kubia-nodeport     
          servicePort: 80                 
This defines an Ingress with a single rule, which makes sure all HTTP requests received
by the Ingress controller, in which the host kubia.example.com is requested, will be
sent to the kubia-nodeport service on port 80. 
(continued)
This should have spun up an Ingress controller as another pod. Most likely, the
controller pod will be in the kube-system namespace, but not necessarily, so list all
the running pods across all namespaces by using the --all-namespaces option:
$ kubectl get po --all-namespaces
NAMESPACE    NAME                            READY  STATUS    RESTARTS AGE
default      kubia-rsv5m                     1/1    Running   0        13h
default      kubia-fe4ad                     1/1    Running   0        13h
default      kubia-ke823                     1/1    Running   0        13h
kube-system  default-http-backend-5wb0h      1/1    Running   0        18m
kube-system  kube-addon-manager-minikube     1/1    Running   3        6d
kube-system  kube-dns-v20-101vq              3/3    Running   9        6d
kube-system  kubernetes-dashboard-jxd9l      1/1    Running   3        6d
kube-system  nginx-ingress-controller-gdts0  1/1    Running   0        18m
At the bottom of the output, you see the Ingress controller pod. The name suggests
that Nginx (an open-source HTTP server and reverse proxy) is used to provide the
Ingress functionality.
Listing 5.13
An Ingress resource definition: kubia-ingress.yaml
This Ingress maps the 
kubia.example.com domain 
name to your service.
All requests will be sent to 
port 80 of the kubia-
nodeport service.
 
</data>
  <data key="d5">144
CHAPTER 5
Services: enabling clients to discover and talk to pods
TIP
The --all-namespaces option mentioned in the sidebar is handy when
you don’t know what namespace your pod (or other type of resource) is in, or
if you want to list resources across all namespaces.
5.4.1
Creating an Ingress resource
You’ve confirmed there’s an Ingress controller running in your cluster, so you can
now create an Ingress resource. The following listing shows what the YAML manifest
for the Ingress looks like.
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com             
    http:
      paths:
      - path: /                           
        backend:
          serviceName: kubia-nodeport     
          servicePort: 80                 
This defines an Ingress with a single rule, which makes sure all HTTP requests received
by the Ingress controller, in which the host kubia.example.com is requested, will be
sent to the kubia-nodeport service on port 80. 
(continued)
This should have spun up an Ingress controller as another pod. Most likely, the
controller pod will be in the kube-system namespace, but not necessarily, so list all
the running pods across all namespaces by using the --all-namespaces option:
$ kubectl get po --all-namespaces
NAMESPACE    NAME                            READY  STATUS    RESTARTS AGE
default      kubia-rsv5m                     1/1    Running   0        13h
default      kubia-fe4ad                     1/1    Running   0        13h
default      kubia-ke823                     1/1    Running   0        13h
kube-system  default-http-backend-5wb0h      1/1    Running   0        18m
kube-system  kube-addon-manager-minikube     1/1    Running   3        6d
kube-system  kube-dns-v20-101vq              3/3    Running   9        6d
kube-system  kubernetes-dashboard-jxd9l      1/1    Running   3        6d
kube-system  nginx-ingress-controller-gdts0  1/1    Running   0        18m
At the bottom of the output, you see the Ingress controller pod. The name suggests
that Nginx (an open-source HTTP server and reverse proxy) is used to provide the
Ingress functionality.
Listing 5.13
An Ingress resource definition: kubia-ingress.yaml
This Ingress maps the 
kubia.example.com domain 
name to your service.
All requests will be sent to 
port 80 of the kubia-
nodeport service.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="177">
  <data key="d0">Page_177</data>
  <data key="d5">Page_177</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_88">
  <data key="d0">145
Exposing services externally through an Ingress resource
NOTE
Ingress controllers on cloud providers (in GKE, for example) require
the Ingress to point to a NodePort service. But that’s not a requirement of
Kubernetes itself.
5.4.2
Accessing the service through the Ingress
To access your service through http:/
/kubia.example.com, you’ll need to make sure
the domain name resolves to the IP of the Ingress controller. 
OBTAINING THE IP ADDRESS OF THE INGRESS
To look up the IP, you need to list Ingresses:
$ kubectl get ingresses
NAME      HOSTS               ADDRESS          PORTS     AGE
kubia     kubia.example.com   192.168.99.100   80        29m
NOTE
When running on cloud providers, the address may take time to appear,
because the Ingress controller provisions a load balancer behind the scenes.
The IP is shown in the ADDRESS column. 
ENSURING THE HOST CONFIGURED IN THE INGRESS POINTS TO THE INGRESS’ IP ADDRESS
Once you know the IP, you can then either configure your DNS servers to resolve
kubia.example.com to that IP or you can add the following line to /etc/hosts (or
C:\windows\system32\drivers\etc\hosts on Windows):
192.168.99.100    kubia.example.com
ACCESSING PODS THROUGH THE INGRESS
Everything is now set up, so you can access the service at http:/
/kubia.example.com
(using a browser or curl):
$ curl http://kubia.example.com
You've hit kubia-ke823
You’ve successfully accessed the service through an Ingress. Let’s take a better look at
how that unfolded.
UNDERSTANDING HOW INGRESSES WORK
Figure 5.10 shows how the client connected to one of the pods through the Ingress
controller. The client first performed a DNS lookup of kubia.example.com, and the
DNS server (or the local operating system) returned the IP of the Ingress controller.
The client then sent an HTTP request to the Ingress controller and specified
kubia.example.com in the Host header. From that header, the controller determined
which service the client is trying to access, looked up the pod IPs through the End-
points object associated with the service, and forwarded the client’s request to one of
the pods.
 As you can see, the Ingress controller didn’t forward the request to the service. It
only used it to select a pod. Most, if not all, controllers work like this. 
 
</data>
  <data key="d5">145
Exposing services externally through an Ingress resource
NOTE
Ingress controllers on cloud providers (in GKE, for example) require
the Ingress to point to a NodePort service. But that’s not a requirement of
Kubernetes itself.
5.4.2
Accessing the service through the Ingress
To access your service through http:/
/kubia.example.com, you’ll need to make sure
the domain name resolves to the IP of the Ingress controller. 
OBTAINING THE IP ADDRESS OF THE INGRESS
To look up the IP, you need to list Ingresses:
$ kubectl get ingresses
NAME      HOSTS               ADDRESS          PORTS     AGE
kubia     kubia.example.com   192.168.99.100   80        29m
NOTE
When running on cloud providers, the address may take time to appear,
because the Ingress controller provisions a load balancer behind the scenes.
The IP is shown in the ADDRESS column. 
ENSURING THE HOST CONFIGURED IN THE INGRESS POINTS TO THE INGRESS’ IP ADDRESS
Once you know the IP, you can then either configure your DNS servers to resolve
kubia.example.com to that IP or you can add the following line to /etc/hosts (or
C:\windows\system32\drivers\etc\hosts on Windows):
192.168.99.100    kubia.example.com
ACCESSING PODS THROUGH THE INGRESS
Everything is now set up, so you can access the service at http:/
/kubia.example.com
(using a browser or curl):
$ curl http://kubia.example.com
You've hit kubia-ke823
You’ve successfully accessed the service through an Ingress. Let’s take a better look at
how that unfolded.
UNDERSTANDING HOW INGRESSES WORK
Figure 5.10 shows how the client connected to one of the pods through the Ingress
controller. The client first performed a DNS lookup of kubia.example.com, and the
DNS server (or the local operating system) returned the IP of the Ingress controller.
The client then sent an HTTP request to the Ingress controller and specified
kubia.example.com in the Host header. From that header, the controller determined
which service the client is trying to access, looked up the pod IPs through the End-
points object associated with the service, and forwarded the client’s request to one of
the pods.
 As you can see, the Ingress controller didn’t forward the request to the service. It
only used it to select a pod. Most, if not all, controllers work like this. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="178">
  <data key="d0">Page_178</data>
  <data key="d5">Page_178</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_89">
  <data key="d0">146
CHAPTER 5
Services: enabling clients to discover and talk to pods
5.4.3
Exposing multiple services through the same Ingress
If you look at the Ingress spec closely, you’ll see that both rules and paths are arrays,
so they can contain multiple items. An Ingress can map multiple hosts and paths to
multiple services, as you’ll see next. Let’s focus on paths first. 
MAPPING DIFFERENT SERVICES TO DIFFERENT PATHS OF THE SAME HOST
You can map multiple paths on the same host to different services, as shown in the
following listing.
...
  - host: kubia.example.com
    http:
      paths:
      - path: /kubia                
        backend:                    
          serviceName: kubia        
          servicePort: 80           
      - path: /foo                
        backend:                  
          serviceName: bar        
          servicePort: 80         
In this case, requests will be sent to two different services, depending on the path in
the requested URL. Clients can therefore reach two different services through a single
IP address (that of the Ingress controller).
Listing 5.14
Ingress exposing multiple services on same host, but different paths
Node A
Pod
Node B
Pod
Pod
Ingress
controller
Endpoints
Service
Ingress
Client
2. Client sends HTTP GET
request with header
Host: kubia.example.com
3. Controller sends
request to one of
the pods.
1. Client looks up
kubia.example.com
DNS
Figure 5.10
Accessing pods through an Ingress
Requests to kubia.example.com/kubia 
will be routed to the kubia service.
Requests to kubia.example.com/bar 
will be routed to the bar service.
 
</data>
  <data key="d5">146
CHAPTER 5
Services: enabling clients to discover and talk to pods
5.4.3
Exposing multiple services through the same Ingress
If you look at the Ingress spec closely, you’ll see that both rules and paths are arrays,
so they can contain multiple items. An Ingress can map multiple hosts and paths to
multiple services, as you’ll see next. Let’s focus on paths first. 
MAPPING DIFFERENT SERVICES TO DIFFERENT PATHS OF THE SAME HOST
You can map multiple paths on the same host to different services, as shown in the
following listing.
...
  - host: kubia.example.com
    http:
      paths:
      - path: /kubia                
        backend:                    
          serviceName: kubia        
          servicePort: 80           
      - path: /foo                
        backend:                  
          serviceName: bar        
          servicePort: 80         
In this case, requests will be sent to two different services, depending on the path in
the requested URL. Clients can therefore reach two different services through a single
IP address (that of the Ingress controller).
Listing 5.14
Ingress exposing multiple services on same host, but different paths
Node A
Pod
Node B
Pod
Pod
Ingress
controller
Endpoints
Service
Ingress
Client
2. Client sends HTTP GET
request with header
Host: kubia.example.com
3. Controller sends
request to one of
the pods.
1. Client looks up
kubia.example.com
DNS
Figure 5.10
Accessing pods through an Ingress
Requests to kubia.example.com/kubia 
will be routed to the kubia service.
Requests to kubia.example.com/bar 
will be routed to the bar service.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="179">
  <data key="d0">Page_179</data>
  <data key="d5">Page_179</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_90">
  <data key="d0">147
Exposing services externally through an Ingress resource
MAPPING DIFFERENT SERVICES TO DIFFERENT HOSTS
Similarly, you can use an Ingress to map to different services based on the host in the
HTTP request instead of (only) the path, as shown in the next listing.
spec:
  rules:
  - host: foo.example.com          
    http:
      paths:
      - path: / 
        backend:
          serviceName: foo         
          servicePort: 80
  - host: bar.example.com          
    http:
      paths:
      - path: /
        backend:
          serviceName: bar         
          servicePort: 80
Requests received by the controller will be forwarded to either service foo or bar,
depending on the Host header in the request (the way virtual hosts are handled in
web servers). DNS needs to point both the foo.example.com and the bar.exam-
ple.com domain names to the Ingress controller’s IP address. 
5.4.4
Configuring Ingress to handle TLS traffic
You’ve seen how an Ingress forwards HTTP traffic. But what about HTTPS? Let’s take
a quick look at how to configure Ingress to support TLS. 
CREATING A TLS CERTIFICATE FOR THE INGRESS
When a client opens a TLS connection to an Ingress controller, the controller termi-
nates the TLS connection. The communication between the client and the controller
is encrypted, whereas the communication between the controller and the backend
pod isn’t. The application running in the pod doesn’t need to support TLS. For exam-
ple, if the pod runs a web server, it can accept only HTTP traffic and let the Ingress
controller take care of everything related to TLS. To enable the controller to do that,
you need to attach a certificate and a private key to the Ingress. The two need to be
stored in a Kubernetes resource called a Secret, which is then referenced in the
Ingress manifest. We’ll explain Secrets in detail in chapter 7. For now, you’ll create the
Secret without paying too much attention to it.
 First, you need to create the private key and certificate:
$ openssl genrsa -out tls.key 2048
$ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj 
➥ /CN=kubia.example.com
Listing 5.15
Ingress exposing multiple services on different hosts
Requests for 
foo.example.com will be 
routed to service foo.
Requests for 
bar.example.com will be 
routed to service bar.
 
</data>
  <data key="d5">147
Exposing services externally through an Ingress resource
MAPPING DIFFERENT SERVICES TO DIFFERENT HOSTS
Similarly, you can use an Ingress to map to different services based on the host in the
HTTP request instead of (only) the path, as shown in the next listing.
spec:
  rules:
  - host: foo.example.com          
    http:
      paths:
      - path: / 
        backend:
          serviceName: foo         
          servicePort: 80
  - host: bar.example.com          
    http:
      paths:
      - path: /
        backend:
          serviceName: bar         
          servicePort: 80
Requests received by the controller will be forwarded to either service foo or bar,
depending on the Host header in the request (the way virtual hosts are handled in
web servers). DNS needs to point both the foo.example.com and the bar.exam-
ple.com domain names to the Ingress controller’s IP address. 
5.4.4
Configuring Ingress to handle TLS traffic
You’ve seen how an Ingress forwards HTTP traffic. But what about HTTPS? Let’s take
a quick look at how to configure Ingress to support TLS. 
CREATING A TLS CERTIFICATE FOR THE INGRESS
When a client opens a TLS connection to an Ingress controller, the controller termi-
nates the TLS connection. The communication between the client and the controller
is encrypted, whereas the communication between the controller and the backend
pod isn’t. The application running in the pod doesn’t need to support TLS. For exam-
ple, if the pod runs a web server, it can accept only HTTP traffic and let the Ingress
controller take care of everything related to TLS. To enable the controller to do that,
you need to attach a certificate and a private key to the Ingress. The two need to be
stored in a Kubernetes resource called a Secret, which is then referenced in the
Ingress manifest. We’ll explain Secrets in detail in chapter 7. For now, you’ll create the
Secret without paying too much attention to it.
 First, you need to create the private key and certificate:
$ openssl genrsa -out tls.key 2048
$ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj 
➥ /CN=kubia.example.com
Listing 5.15
Ingress exposing multiple services on different hosts
Requests for 
foo.example.com will be 
routed to service foo.
Requests for 
bar.example.com will be 
routed to service bar.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="180">
  <data key="d0">Page_180</data>
  <data key="d5">Page_180</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_91">
  <data key="d0">148
CHAPTER 5
Services: enabling clients to discover and talk to pods
Then you create the Secret from the two files like this:
$ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key
secret "tls-secret" created
The private key and the certificate are now stored in the Secret called tls-secret.
Now, you can update your Ingress object so it will also accept HTTPS requests for
kubia.example.com. The Ingress manifest should now look like the following listing.
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  tls:                           
  - hosts:                        
    - kubia.example.com           
    secretName: tls-secret       
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80
TIP
Instead of deleting the Ingress and re-creating it from the new file, you
can invoke kubectl apply -f kubia-ingress-tls.yaml, which updates the
Ingress resource with what’s specified in the file.
Signing certificates through the CertificateSigningRequest resource
Instead of signing the certificate ourselves, you can get the certificate signed by
creating a CertificateSigningRequest (CSR) resource. Users or their applications
can create a regular certificate request, put it into a CSR, and then either a human
operator or an automated process can approve the request like this:
$ kubectl certificate approve &lt;name of the CSR&gt; 
The signed certificate can then be retrieved from the CSR’s status.certificate
field. 
Note that a certificate signer component must be running in the cluster; otherwise
creating CertificateSigningRequest and approving or denying them won’t have
any effect.
Listing 5.16
Ingress handling TLS traffic: kubia-ingress-tls.yaml
The whole TLS configuration 
is under this attribute.
TLS connections will be accepted for 
the kubia.example.com hostname.
The private key and the certificate 
should be obtained from the tls-
secret you created previously.
 
</data>
  <data key="d5">148
CHAPTER 5
Services: enabling clients to discover and talk to pods
Then you create the Secret from the two files like this:
$ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key
secret "tls-secret" created
The private key and the certificate are now stored in the Secret called tls-secret.
Now, you can update your Ingress object so it will also accept HTTPS requests for
kubia.example.com. The Ingress manifest should now look like the following listing.
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  tls:                           
  - hosts:                        
    - kubia.example.com           
    secretName: tls-secret       
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80
TIP
Instead of deleting the Ingress and re-creating it from the new file, you
can invoke kubectl apply -f kubia-ingress-tls.yaml, which updates the
Ingress resource with what’s specified in the file.
Signing certificates through the CertificateSigningRequest resource
Instead of signing the certificate ourselves, you can get the certificate signed by
creating a CertificateSigningRequest (CSR) resource. Users or their applications
can create a regular certificate request, put it into a CSR, and then either a human
operator or an automated process can approve the request like this:
$ kubectl certificate approve &lt;name of the CSR&gt; 
The signed certificate can then be retrieved from the CSR’s status.certificate
field. 
Note that a certificate signer component must be running in the cluster; otherwise
creating CertificateSigningRequest and approving or denying them won’t have
any effect.
Listing 5.16
Ingress handling TLS traffic: kubia-ingress-tls.yaml
The whole TLS configuration 
is under this attribute.
TLS connections will be accepted for 
the kubia.example.com hostname.
The private key and the certificate 
should be obtained from the tls-
secret you created previously.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="181">
  <data key="d0">Page_181</data>
  <data key="d5">Page_181</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_92">
  <data key="d0">149
Signaling when a pod is ready to accept connections
You can now use HTTPS to access your service through the Ingress:
$ curl -k -v https://kubia.example.com/kubia
* About to connect() to kubia.example.com port 443 (#0)
...
* Server certificate:
*   subject: CN=kubia.example.com
...
&gt; GET /kubia HTTP/1.1
&gt; ...
You've hit kubia-xueq1
The command’s output shows the response from the app, as well as the server certifi-
cate you configured the Ingress with.
NOTE
Support for Ingress features varies between the different Ingress con-
troller implementations, so check the implementation-specific documenta-
tion to see what’s supported. 
Ingresses are a relatively new Kubernetes feature, so you can expect to see many
improvements and new features in the future. Although they currently support only
L7 (HTTP/HTTPS) load balancing, support for L4 load balancing is also planned.
5.5
Signaling when a pod is ready to accept connections
There’s one more thing we need to cover regarding both Services and Ingresses.
You’ve already learned that pods are included as endpoints of a service if their labels
match the service’s pod selector. As soon as a new pod with proper labels is created, it
becomes part of the service and requests start to be redirected to the pod. But what if
the pod isn’t ready to start serving requests immediately? 
 The pod may need time to load either configuration or data, or it may need to per-
form a warm-up procedure to prevent the first user request from taking too long and
affecting the user experience. In such cases you don’t want the pod to start receiving
requests immediately, especially when the already-running instances can process
requests properly and quickly. It makes sense to not forward requests to a pod that’s in
the process of starting up until it’s fully ready.
5.5.1
Introducing readiness probes
In the previous chapter you learned about liveness probes and how they help keep
your apps healthy by ensuring unhealthy containers are restarted automatically.
Similar to liveness probes, Kubernetes allows you to also define a readiness probe
for your pod.
 The readiness probe is invoked periodically and determines whether the specific
pod should receive client requests or not. When a container’s readiness probe returns
success, it’s signaling that the container is ready to accept requests. 
 This notion of being ready is obviously something that’s specific to each container.
Kubernetes can merely check if the app running in the container responds to a simple
 
</data>
  <data key="d5">149
Signaling when a pod is ready to accept connections
You can now use HTTPS to access your service through the Ingress:
$ curl -k -v https://kubia.example.com/kubia
* About to connect() to kubia.example.com port 443 (#0)
...
* Server certificate:
*   subject: CN=kubia.example.com
...
&gt; GET /kubia HTTP/1.1
&gt; ...
You've hit kubia-xueq1
The command’s output shows the response from the app, as well as the server certifi-
cate you configured the Ingress with.
NOTE
Support for Ingress features varies between the different Ingress con-
troller implementations, so check the implementation-specific documenta-
tion to see what’s supported. 
Ingresses are a relatively new Kubernetes feature, so you can expect to see many
improvements and new features in the future. Although they currently support only
L7 (HTTP/HTTPS) load balancing, support for L4 load balancing is also planned.
5.5
Signaling when a pod is ready to accept connections
There’s one more thing we need to cover regarding both Services and Ingresses.
You’ve already learned that pods are included as endpoints of a service if their labels
match the service’s pod selector. As soon as a new pod with proper labels is created, it
becomes part of the service and requests start to be redirected to the pod. But what if
the pod isn’t ready to start serving requests immediately? 
 The pod may need time to load either configuration or data, or it may need to per-
form a warm-up procedure to prevent the first user request from taking too long and
affecting the user experience. In such cases you don’t want the pod to start receiving
requests immediately, especially when the already-running instances can process
requests properly and quickly. It makes sense to not forward requests to a pod that’s in
the process of starting up until it’s fully ready.
5.5.1
Introducing readiness probes
In the previous chapter you learned about liveness probes and how they help keep
your apps healthy by ensuring unhealthy containers are restarted automatically.
Similar to liveness probes, Kubernetes allows you to also define a readiness probe
for your pod.
 The readiness probe is invoked periodically and determines whether the specific
pod should receive client requests or not. When a container’s readiness probe returns
success, it’s signaling that the container is ready to accept requests. 
 This notion of being ready is obviously something that’s specific to each container.
Kubernetes can merely check if the app running in the container responds to a simple
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="182">
  <data key="d0">Page_182</data>
  <data key="d5">Page_182</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_93">
  <data key="d0">150
CHAPTER 5
Services: enabling clients to discover and talk to pods
GET / request or it can hit a specific URL path, which causes the app to perform a
whole list of checks to determine if it’s ready. Such a detailed readiness probe, which
takes the app’s specifics into account, is the app developer’s responsibility. 
TYPES OF READINESS PROBES
Like liveness probes, three types of readiness probes exist:
An Exec probe, where a process is executed. The container’s status is deter-
mined by the process’ exit status code.
An HTTP GET probe, which sends an HTTP GET request to the container and
the HTTP status code of the response determines whether the container is
ready or not.
A TCP Socket probe, which opens a TCP connection to a specified port of the
container. If the connection is established, the container is considered ready.
UNDERSTANDING THE OPERATION OF READINESS PROBES
When a container is started, Kubernetes can be configured to wait for a configurable
amount of time to pass before performing the first readiness check. After that, it
invokes the probe periodically and acts based on the result of the readiness probe. If a
pod reports that it’s not ready, it’s removed from the service. If the pod then becomes
ready again, it’s re-added. 
 Unlike liveness probes, if a container fails the readiness check, it won’t be killed or
restarted. This is an important distinction between liveness and readiness probes.
Liveness probes keep pods healthy by killing off unhealthy containers and replacing
them with new, healthy ones, whereas readiness probes make sure that only pods that
are ready to serve requests receive them. This is mostly necessary during container
start up, but it’s also useful after the container has been running for a while. 
 As you can see in figure 5.11, if a pod’s readiness probe fails, the pod is removed
from the Endpoints object. Clients connecting to the service will not be redirected to
the pod. The effect is the same as when the pod doesn’t match the service’s label
selector at all.
Endpoints
Service
Selector: app=kubia
app: kubia
Pod: kubia-q3vkg
app: kubia
Pod: kubia-k0xz6
app: kubia
Pod: kubia-53thy
Not ready
This pod is no longer
an endpoint, because its
readiness probe has failed.
Figure 5.11
A pod whose readiness probe fails is removed as an endpoint of a service.
 
</data>
  <data key="d5">150
CHAPTER 5
Services: enabling clients to discover and talk to pods
GET / request or it can hit a specific URL path, which causes the app to perform a
whole list of checks to determine if it’s ready. Such a detailed readiness probe, which
takes the app’s specifics into account, is the app developer’s responsibility. 
TYPES OF READINESS PROBES
Like liveness probes, three types of readiness probes exist:
An Exec probe, where a process is executed. The container’s status is deter-
mined by the process’ exit status code.
An HTTP GET probe, which sends an HTTP GET request to the container and
the HTTP status code of the response determines whether the container is
ready or not.
A TCP Socket probe, which opens a TCP connection to a specified port of the
container. If the connection is established, the container is considered ready.
UNDERSTANDING THE OPERATION OF READINESS PROBES
When a container is started, Kubernetes can be configured to wait for a configurable
amount of time to pass before performing the first readiness check. After that, it
invokes the probe periodically and acts based on the result of the readiness probe. If a
pod reports that it’s not ready, it’s removed from the service. If the pod then becomes
ready again, it’s re-added. 
 Unlike liveness probes, if a container fails the readiness check, it won’t be killed or
restarted. This is an important distinction between liveness and readiness probes.
Liveness probes keep pods healthy by killing off unhealthy containers and replacing
them with new, healthy ones, whereas readiness probes make sure that only pods that
are ready to serve requests receive them. This is mostly necessary during container
start up, but it’s also useful after the container has been running for a while. 
 As you can see in figure 5.11, if a pod’s readiness probe fails, the pod is removed
from the Endpoints object. Clients connecting to the service will not be redirected to
the pod. The effect is the same as when the pod doesn’t match the service’s label
selector at all.
Endpoints
Service
Selector: app=kubia
app: kubia
Pod: kubia-q3vkg
app: kubia
Pod: kubia-k0xz6
app: kubia
Pod: kubia-53thy
Not ready
This pod is no longer
an endpoint, because its
readiness probe has failed.
Figure 5.11
A pod whose readiness probe fails is removed as an endpoint of a service.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="183">
  <data key="d0">Page_183</data>
  <data key="d5">Page_183</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_94">
  <data key="d0">151
Signaling when a pod is ready to accept connections
UNDERSTANDING WHY READINESS PROBES ARE IMPORTANT
Imagine that a group of pods (for example, pods running application servers)
depends on a service provided by another pod (a backend database, for example). If
at any point one of the frontend pods experiences connectivity problems and can’t
reach the database anymore, it may be wise for its readiness probe to signal to Kuber-
netes that the pod isn’t ready to serve any requests at that time. If other pod instances
aren’t experiencing the same type of connectivity issues, they can serve requests nor-
mally. A readiness probe makes sure clients only talk to those healthy pods and never
notice there’s anything wrong with the system.
5.5.2
Adding a readiness probe to a pod
Next you’ll add a readiness probe to your existing pods by modifying the Replication-
Controller’s pod template. 
ADDING A READINESS PROBE TO THE POD TEMPLATE
You’ll use the kubectl edit command to add the probe to the pod template in your
existing ReplicationController:
$ kubectl edit rc kubia
When the ReplicationController’s YAML opens in the text editor, find the container
specification in the pod template and add the following readiness probe definition to
the first container under spec.template.spec.containers. The YAML should look
like the following listing.
apiVersion: v1
kind: ReplicationController
...
spec:
  ...
  template:
    ...
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        readinessProbe:       
          exec:               
            command:          
            - ls              
            - /var/ready      
        ...
The readiness probe will periodically perform the command ls /var/ready inside the
container. The ls command returns exit code zero if the file exists, or a non-zero exit
code otherwise. If the file exists, the readiness probe will succeed; otherwise, it will fail. 
Listing 5.17
RC creating a pod with a readiness probe: kubia-rc-readinessprobe.yaml
A readinessProbe may 
be defined for each 
container in the pod.
 
</data>
  <data key="d5">151
Signaling when a pod is ready to accept connections
UNDERSTANDING WHY READINESS PROBES ARE IMPORTANT
Imagine that a group of pods (for example, pods running application servers)
depends on a service provided by another pod (a backend database, for example). If
at any point one of the frontend pods experiences connectivity problems and can’t
reach the database anymore, it may be wise for its readiness probe to signal to Kuber-
netes that the pod isn’t ready to serve any requests at that time. If other pod instances
aren’t experiencing the same type of connectivity issues, they can serve requests nor-
mally. A readiness probe makes sure clients only talk to those healthy pods and never
notice there’s anything wrong with the system.
5.5.2
Adding a readiness probe to a pod
Next you’ll add a readiness probe to your existing pods by modifying the Replication-
Controller’s pod template. 
ADDING A READINESS PROBE TO THE POD TEMPLATE
You’ll use the kubectl edit command to add the probe to the pod template in your
existing ReplicationController:
$ kubectl edit rc kubia
When the ReplicationController’s YAML opens in the text editor, find the container
specification in the pod template and add the following readiness probe definition to
the first container under spec.template.spec.containers. The YAML should look
like the following listing.
apiVersion: v1
kind: ReplicationController
...
spec:
  ...
  template:
    ...
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        readinessProbe:       
          exec:               
            command:          
            - ls              
            - /var/ready      
        ...
The readiness probe will periodically perform the command ls /var/ready inside the
container. The ls command returns exit code zero if the file exists, or a non-zero exit
code otherwise. If the file exists, the readiness probe will succeed; otherwise, it will fail. 
Listing 5.17
RC creating a pod with a readiness probe: kubia-rc-readinessprobe.yaml
A readinessProbe may 
be defined for each 
container in the pod.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="184">
  <data key="d0">Page_184</data>
  <data key="d5">Page_184</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_95">
  <data key="d0">152
CHAPTER 5
Services: enabling clients to discover and talk to pods
 The reason you’re defining such a strange readiness probe is so you can toggle its
result by creating or removing the file in question. The file doesn’t exist yet, so all the
pods should now report not being ready, right? Well, not exactly. As you may remem-
ber from the previous chapter, changing a ReplicationController’s pod template has
no effect on existing pods. 
 In other words, all your existing pods still have no readiness probe defined. You
can see this by listing the pods with kubectl get pods and looking at the READY col-
umn. You need to delete the pods and have them re-created by the Replication-
Controller. The new pods will fail the readiness check and won’t be included as
endpoints of the service until you create the /var/ready file in each of them. 
OBSERVING AND MODIFYING THE PODS’ READINESS STATUS
List the pods again and inspect whether they’re ready or not:
$ kubectl get po
NAME          READY     STATUS    RESTARTS   AGE
kubia-2r1qb   0/1       Running   0          1m
kubia-3rax1   0/1       Running   0          1m
kubia-3yw4s   0/1       Running   0          1m
The READY column shows that none of the containers are ready. Now make the readi-
ness probe of one of them start returning success by creating the /var/ready file,
whose existence makes your mock readiness probe succeed:
$ kubectl exec kubia-2r1qb -- touch /var/ready
You’ve used the kubectl exec command to execute the touch command inside the
container of the kubia-2r1qb pod. The touch command creates the file if it doesn’t
yet exist. The pod’s readiness probe command should now exit with status code 0,
which means the probe is successful, and the pod should now be shown as ready. Let’s
see if it is:
$ kubectl get po kubia-2r1qb
NAME          READY     STATUS    RESTARTS   AGE
kubia-2r1qb   0/1       Running   0          2m
The pod still isn’t ready. Is there something wrong or is this the expected result? Take
a more detailed look at the pod with kubectl describe. The output should contain
the following line:
Readiness: exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1
➥ #failure=3
The readiness probe is checked periodically—every 10 seconds by default. The pod
isn’t ready because the readiness probe hasn’t been invoked yet. But in 10 seconds at
the latest, the pod should become ready and its IP should be listed as the only end-
point of the service (run kubectl get endpoints kubia-loadbalancer to confirm). 
 
</data>
  <data key="d5">152
CHAPTER 5
Services: enabling clients to discover and talk to pods
 The reason you’re defining such a strange readiness probe is so you can toggle its
result by creating or removing the file in question. The file doesn’t exist yet, so all the
pods should now report not being ready, right? Well, not exactly. As you may remem-
ber from the previous chapter, changing a ReplicationController’s pod template has
no effect on existing pods. 
 In other words, all your existing pods still have no readiness probe defined. You
can see this by listing the pods with kubectl get pods and looking at the READY col-
umn. You need to delete the pods and have them re-created by the Replication-
Controller. The new pods will fail the readiness check and won’t be included as
endpoints of the service until you create the /var/ready file in each of them. 
OBSERVING AND MODIFYING THE PODS’ READINESS STATUS
List the pods again and inspect whether they’re ready or not:
$ kubectl get po
NAME          READY     STATUS    RESTARTS   AGE
kubia-2r1qb   0/1       Running   0          1m
kubia-3rax1   0/1       Running   0          1m
kubia-3yw4s   0/1       Running   0          1m
The READY column shows that none of the containers are ready. Now make the readi-
ness probe of one of them start returning success by creating the /var/ready file,
whose existence makes your mock readiness probe succeed:
$ kubectl exec kubia-2r1qb -- touch /var/ready
You’ve used the kubectl exec command to execute the touch command inside the
container of the kubia-2r1qb pod. The touch command creates the file if it doesn’t
yet exist. The pod’s readiness probe command should now exit with status code 0,
which means the probe is successful, and the pod should now be shown as ready. Let’s
see if it is:
$ kubectl get po kubia-2r1qb
NAME          READY     STATUS    RESTARTS   AGE
kubia-2r1qb   0/1       Running   0          2m
The pod still isn’t ready. Is there something wrong or is this the expected result? Take
a more detailed look at the pod with kubectl describe. The output should contain
the following line:
Readiness: exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1
➥ #failure=3
The readiness probe is checked periodically—every 10 seconds by default. The pod
isn’t ready because the readiness probe hasn’t been invoked yet. But in 10 seconds at
the latest, the pod should become ready and its IP should be listed as the only end-
point of the service (run kubectl get endpoints kubia-loadbalancer to confirm). 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="185">
  <data key="d0">Page_185</data>
  <data key="d5">Page_185</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_96">
  <data key="d0">153
Signaling when a pod is ready to accept connections
HITTING THE SERVICE WITH THE SINGLE READY POD
You can now hit the service URL a few times to see that each and every request is redi-
rected to this one pod:
$ curl http://130.211.53.173
You’ve hit kubia-2r1qb
$ curl http://130.211.53.173
You’ve hit kubia-2r1qb
...
$ curl http://130.211.53.173
You’ve hit kubia-2r1qb
Even though there are three pods running, only a single pod is reporting as being
ready and is therefore the only pod receiving requests. If you now delete the file, the
pod will be removed from the service again. 
5.5.3
Understanding what real-world readiness probes should do
This mock readiness probe is useful only for demonstrating what readiness probes do.
In the real world, the readiness probe should return success or failure depending on
whether the app can (and wants to) receive client requests or not. 
 Manually removing pods from services should be performed by either deleting the
pod or changing the pod’s labels instead of manually flipping a switch in the probe. 
TIP
If you want to add or remove a pod from a service manually, add
enabled=true as a label to your pod and to the label selector of your service.
Remove the label when you want to remove the pod from the service.
ALWAYS DEFINE A READINESS PROBE
Before we conclude this section, there are two final notes about readiness probes that
I need to emphasize. First, if you don’t add a readiness probe to your pods, they’ll
become service endpoints almost immediately. If your application takes too long to
start listening for incoming connections, client requests hitting the service will be for-
warded to the pod while it’s still starting up and not ready to accept incoming connec-
tions. Clients will therefore see “Connection refused” types of errors. 
TIP
You should always define a readiness probe, even if it’s as simple as send-
ing an HTTP request to the base URL. 
DON’T INCLUDE POD SHUTDOWN LOGIC INTO YOUR READINESS PROBES
The other thing I need to mention applies to the other end of the pod’s life (pod
shutdown) and is also related to clients experiencing connection errors. 
 When a pod is being shut down, the app running in it usually stops accepting con-
nections as soon as it receives the termination signal. Because of this, you might think
you need to make your readiness probe start failing as soon as the shutdown proce-
dure is initiated, ensuring the pod is removed from all services it’s part of. But that’s
not necessary, because Kubernetes removes the pod from all services as soon as you
delete the pod.
 
</data>
  <data key="d5">153
Signaling when a pod is ready to accept connections
HITTING THE SERVICE WITH THE SINGLE READY POD
You can now hit the service URL a few times to see that each and every request is redi-
rected to this one pod:
$ curl http://130.211.53.173
You’ve hit kubia-2r1qb
$ curl http://130.211.53.173
You’ve hit kubia-2r1qb
...
$ curl http://130.211.53.173
You’ve hit kubia-2r1qb
Even though there are three pods running, only a single pod is reporting as being
ready and is therefore the only pod receiving requests. If you now delete the file, the
pod will be removed from the service again. 
5.5.3
Understanding what real-world readiness probes should do
This mock readiness probe is useful only for demonstrating what readiness probes do.
In the real world, the readiness probe should return success or failure depending on
whether the app can (and wants to) receive client requests or not. 
 Manually removing pods from services should be performed by either deleting the
pod or changing the pod’s labels instead of manually flipping a switch in the probe. 
TIP
If you want to add or remove a pod from a service manually, add
enabled=true as a label to your pod and to the label selector of your service.
Remove the label when you want to remove the pod from the service.
ALWAYS DEFINE A READINESS PROBE
Before we conclude this section, there are two final notes about readiness probes that
I need to emphasize. First, if you don’t add a readiness probe to your pods, they’ll
become service endpoints almost immediately. If your application takes too long to
start listening for incoming connections, client requests hitting the service will be for-
warded to the pod while it’s still starting up and not ready to accept incoming connec-
tions. Clients will therefore see “Connection refused” types of errors. 
TIP
You should always define a readiness probe, even if it’s as simple as send-
ing an HTTP request to the base URL. 
DON’T INCLUDE POD SHUTDOWN LOGIC INTO YOUR READINESS PROBES
The other thing I need to mention applies to the other end of the pod’s life (pod
shutdown) and is also related to clients experiencing connection errors. 
 When a pod is being shut down, the app running in it usually stops accepting con-
nections as soon as it receives the termination signal. Because of this, you might think
you need to make your readiness probe start failing as soon as the shutdown proce-
dure is initiated, ensuring the pod is removed from all services it’s part of. But that’s
not necessary, because Kubernetes removes the pod from all services as soon as you
delete the pod.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="186">
  <data key="d0">Page_186</data>
  <data key="d5">Page_186</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_97">
  <data key="d0">154
CHAPTER 5
Services: enabling clients to discover and talk to pods
5.6
Using a headless service for discovering individual pods
You’ve seen how services can be used to provide a stable IP address allowing clients to
connect to pods (or other endpoints) backing each service. Each connection to the
service is forwarded to one randomly selected backing pod. But what if the client
needs to connect to all of those pods? What if the backing pods themselves need to
each connect to all the other backing pods? Connecting through the service clearly
isn’t the way to do this. What is?
 For a client to connect to all pods, it needs to figure out the the IP of each individ-
ual pod. One option is to have the client call the Kubernetes API server and get the
list of pods and their IP addresses through an API call, but because you should always
strive to keep your apps Kubernetes-agnostic, using the API server isn’t ideal. 
 Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually,
when you perform a DNS lookup for a service, the DNS server returns a single IP—the
service’s cluster IP. But if you tell Kubernetes you don’t need a cluster IP for your service
(you do this by setting the clusterIP field to None in the service specification), the DNS
server will return the pod IPs instead of the single service IP.
 Instead of returning a single DNS A record, the DNS server will return multiple A
records for the service, each pointing to the IP of an individual pod backing the ser-
vice at that moment. Clients can therefore do a simple DNS A record lookup and get
the IPs of all the pods that are part of the service. The client can then use that infor-
mation to connect to one, many, or all of them.
5.6.1
Creating a headless service
Setting the clusterIP field in a service spec to None makes the service headless, as
Kubernetes won’t assign it a cluster IP through which clients could connect to the
pods backing it. 
 You’ll create a headless service called kubia-headless now. The following listing
shows its definition.
apiVersion: v1
kind: Service
metadata:
  name: kubia-headless
spec:
  clusterIP: None       
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
After you create the service with kubectl create, you can inspect it with kubectl get
and kubectl describe. You’ll see it has no cluster IP and its endpoints include (part of)
Listing 5.18
A headless service: kubia-svc-headless.yaml
This makes the 
service headless.
 
</data>
  <data key="d5">154
CHAPTER 5
Services: enabling clients to discover and talk to pods
5.6
Using a headless service for discovering individual pods
You’ve seen how services can be used to provide a stable IP address allowing clients to
connect to pods (or other endpoints) backing each service. Each connection to the
service is forwarded to one randomly selected backing pod. But what if the client
needs to connect to all of those pods? What if the backing pods themselves need to
each connect to all the other backing pods? Connecting through the service clearly
isn’t the way to do this. What is?
 For a client to connect to all pods, it needs to figure out the the IP of each individ-
ual pod. One option is to have the client call the Kubernetes API server and get the
list of pods and their IP addresses through an API call, but because you should always
strive to keep your apps Kubernetes-agnostic, using the API server isn’t ideal. 
 Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually,
when you perform a DNS lookup for a service, the DNS server returns a single IP—the
service’s cluster IP. But if you tell Kubernetes you don’t need a cluster IP for your service
(you do this by setting the clusterIP field to None in the service specification), the DNS
server will return the pod IPs instead of the single service IP.
 Instead of returning a single DNS A record, the DNS server will return multiple A
records for the service, each pointing to the IP of an individual pod backing the ser-
vice at that moment. Clients can therefore do a simple DNS A record lookup and get
the IPs of all the pods that are part of the service. The client can then use that infor-
mation to connect to one, many, or all of them.
5.6.1
Creating a headless service
Setting the clusterIP field in a service spec to None makes the service headless, as
Kubernetes won’t assign it a cluster IP through which clients could connect to the
pods backing it. 
 You’ll create a headless service called kubia-headless now. The following listing
shows its definition.
apiVersion: v1
kind: Service
metadata:
  name: kubia-headless
spec:
  clusterIP: None       
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
After you create the service with kubectl create, you can inspect it with kubectl get
and kubectl describe. You’ll see it has no cluster IP and its endpoints include (part of)
Listing 5.18
A headless service: kubia-svc-headless.yaml
This makes the 
service headless.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="187">
  <data key="d0">Page_187</data>
  <data key="d5">Page_187</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_98">
  <data key="d0">155
Using a headless service for discovering individual pods
the pods matching its pod selector. I say “part of” because your pods contain a readi-
ness probe, so only pods that are ready will be listed as endpoints of the service.
Before continuing, please make sure at least two pods report being ready, by creating
the /var/ready file, as in the previous example:
$ kubectl exec &lt;pod name&gt; -- touch /var/ready
5.6.2
Discovering pods through DNS
With your pods ready, you can now try performing a DNS lookup to see if you get the
actual pod IPs or not. You’ll need to perform the lookup from inside one of the pods.
Unfortunately, your kubia container image doesn’t include the nslookup (or the dig)
binary, so you can’t use it to perform the DNS lookup.
 All you’re trying to do is perform a DNS lookup from inside a pod running in the
cluster. Why not run a new pod based on an image that contains the binaries you
need? To perform DNS-related actions, you can use the tutum/dnsutils container
image, which is available on Docker Hub and contains both the nslookup and the dig
binaries. To run the pod, you can go through the whole process of creating a YAML
manifest for it and passing it to kubectl create, but that’s too much work, right?
Luckily, there’s a faster way.
RUNNING A POD WITHOUT WRITING A YAML MANIFEST
In chapter 1, you already created pods without writing a YAML manifest by using the
kubectl run command. But this time you want to create only a pod—you don’t need
to create a ReplicationController to manage the pod. You can do that like this:
$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1
➥ --command -- sleep infinity
pod "dnsutils" created
The trick is in the --generator=run-pod/v1 option, which tells kubectl to create the
pod directly, without any kind of ReplicationController or similar behind it. 
UNDERSTANDING DNS A RECORDS RETURNED FOR A HEADLESS SERVICE
Let’s use the newly created pod to perform a DNS lookup:
$ kubectl exec dnsutils nslookup kubia-headless
...
Name:    kubia-headless.default.svc.cluster.local
Address: 10.108.1.4 
Name:    kubia-headless.default.svc.cluster.local
Address: 10.108.2.5 
The DNS server returns two different IPs for the kubia-headless.default.svc
.cluster.local FQDN. Those are the IPs of the two pods that are reporting being
ready. You can confirm this by listing pods with kubectl get pods -o wide, which
shows the pods’ IPs. 
 
</data>
  <data key="d5">155
Using a headless service for discovering individual pods
the pods matching its pod selector. I say “part of” because your pods contain a readi-
ness probe, so only pods that are ready will be listed as endpoints of the service.
Before continuing, please make sure at least two pods report being ready, by creating
the /var/ready file, as in the previous example:
$ kubectl exec &lt;pod name&gt; -- touch /var/ready
5.6.2
Discovering pods through DNS
With your pods ready, you can now try performing a DNS lookup to see if you get the
actual pod IPs or not. You’ll need to perform the lookup from inside one of the pods.
Unfortunately, your kubia container image doesn’t include the nslookup (or the dig)
binary, so you can’t use it to perform the DNS lookup.
 All you’re trying to do is perform a DNS lookup from inside a pod running in the
cluster. Why not run a new pod based on an image that contains the binaries you
need? To perform DNS-related actions, you can use the tutum/dnsutils container
image, which is available on Docker Hub and contains both the nslookup and the dig
binaries. To run the pod, you can go through the whole process of creating a YAML
manifest for it and passing it to kubectl create, but that’s too much work, right?
Luckily, there’s a faster way.
RUNNING A POD WITHOUT WRITING A YAML MANIFEST
In chapter 1, you already created pods without writing a YAML manifest by using the
kubectl run command. But this time you want to create only a pod—you don’t need
to create a ReplicationController to manage the pod. You can do that like this:
$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1
➥ --command -- sleep infinity
pod "dnsutils" created
The trick is in the --generator=run-pod/v1 option, which tells kubectl to create the
pod directly, without any kind of ReplicationController or similar behind it. 
UNDERSTANDING DNS A RECORDS RETURNED FOR A HEADLESS SERVICE
Let’s use the newly created pod to perform a DNS lookup:
$ kubectl exec dnsutils nslookup kubia-headless
...
Name:    kubia-headless.default.svc.cluster.local
Address: 10.108.1.4 
Name:    kubia-headless.default.svc.cluster.local
Address: 10.108.2.5 
The DNS server returns two different IPs for the kubia-headless.default.svc
.cluster.local FQDN. Those are the IPs of the two pods that are reporting being
ready. You can confirm this by listing pods with kubectl get pods -o wide, which
shows the pods’ IPs. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="188">
  <data key="d0">Page_188</data>
  <data key="d5">Page_188</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_99">
  <data key="d0">156
CHAPTER 5
Services: enabling clients to discover and talk to pods
 This is different from what DNS returns for regular (non-headless) services, such
as for your kubia service, where the returned IP is the service’s cluster IP:
$ kubectl exec dnsutils nslookup kubia
...
Name:    kubia.default.svc.cluster.local
Address: 10.111.249.153
Although headless services may seem different from regular services, they aren’t that
different from the clients’ perspective. Even with a headless service, clients can con-
nect to its pods by connecting to the service’s DNS name, as they can with regular ser-
vices. But with headless services, because DNS returns the pods’ IPs, clients connect
directly to the pods, instead of through the service proxy. 
NOTE
A headless services still provides load balancing across pods, but through
the DNS round-robin mechanism instead of through the service proxy.
5.6.3
Discovering all pods—even those that aren’t ready
You’ve seen that only pods that are ready become endpoints of services. But some-
times you want to use the service discovery mechanism to find all pods matching the
service’s label selector, even those that aren’t ready. 
 Luckily, you don’t have to resort to querying the Kubernetes API server. You can
use the DNS lookup mechanism to find even those unready pods. To tell Kubernetes
you want all pods added to a service, regardless of the pod’s readiness status, you must
add the following annotation to the service:
kind: Service
metadata:
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
WARNING
As the annotation name suggests, as I’m writing this, this is an alpha
feature. The Kubernetes Service API already supports a new service spec field
called publishNotReadyAddresses, which will replace the tolerate-unready-
endpoints annotation. In Kubernetes version 1.9.0, the field is not honored yet
(the annotation is what determines whether unready endpoints are included in
the DNS or not). Check the documentation to see whether that’s changed.
5.7
Troubleshooting services
Services are a crucial Kubernetes concept and the source of frustration for many
developers. I’ve seen many developers lose heaps of time figuring out why they can’t
connect to their pods through the service IP or FQDN. For this reason, a short look at
how to troubleshoot services is in order.
 When you’re unable to access your pods through the service, you should start by
going through the following list:
 
</data>
  <data key="d5">156
CHAPTER 5
Services: enabling clients to discover and talk to pods
 This is different from what DNS returns for regular (non-headless) services, such
as for your kubia service, where the returned IP is the service’s cluster IP:
$ kubectl exec dnsutils nslookup kubia
...
Name:    kubia.default.svc.cluster.local
Address: 10.111.249.153
Although headless services may seem different from regular services, they aren’t that
different from the clients’ perspective. Even with a headless service, clients can con-
nect to its pods by connecting to the service’s DNS name, as they can with regular ser-
vices. But with headless services, because DNS returns the pods’ IPs, clients connect
directly to the pods, instead of through the service proxy. 
NOTE
A headless services still provides load balancing across pods, but through
the DNS round-robin mechanism instead of through the service proxy.
5.6.3
Discovering all pods—even those that aren’t ready
You’ve seen that only pods that are ready become endpoints of services. But some-
times you want to use the service discovery mechanism to find all pods matching the
service’s label selector, even those that aren’t ready. 
 Luckily, you don’t have to resort to querying the Kubernetes API server. You can
use the DNS lookup mechanism to find even those unready pods. To tell Kubernetes
you want all pods added to a service, regardless of the pod’s readiness status, you must
add the following annotation to the service:
kind: Service
metadata:
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
WARNING
As the annotation name suggests, as I’m writing this, this is an alpha
feature. The Kubernetes Service API already supports a new service spec field
called publishNotReadyAddresses, which will replace the tolerate-unready-
endpoints annotation. In Kubernetes version 1.9.0, the field is not honored yet
(the annotation is what determines whether unready endpoints are included in
the DNS or not). Check the documentation to see whether that’s changed.
5.7
Troubleshooting services
Services are a crucial Kubernetes concept and the source of frustration for many
developers. I’ve seen many developers lose heaps of time figuring out why they can’t
connect to their pods through the service IP or FQDN. For this reason, a short look at
how to troubleshoot services is in order.
 When you’re unable to access your pods through the service, you should start by
going through the following list:
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="189">
  <data key="d0">Page_189</data>
  <data key="d5">Page_189</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_100">
  <data key="d0">157
Summary
First, make sure you’re connecting to the service’s cluster IP from within the
cluster, not from the outside.
Don’t bother pinging the service IP to figure out if the service is accessible
(remember, the service’s cluster IP is a virtual IP and pinging it will never work).
If you’ve defined a readiness probe, make sure it’s succeeding; otherwise the
pod won’t be part of the service.
To confirm that a pod is part of the service, examine the corresponding End-
points object with kubectl get endpoints.
If you’re trying to access the service through its FQDN or a part of it (for exam-
ple, myservice.mynamespace.svc.cluster.local or myservice.mynamespace) and
it doesn’t work, see if you can access it using its cluster IP instead of the FQDN.
Check whether you’re connecting to the port exposed by the service and not
the target port.
Try connecting to the pod IP directly to confirm your pod is accepting connec-
tions on the correct port.
If you can’t even access your app through the pod’s IP, make sure your app isn’t
only binding to localhost.
This should help you resolve most of your service-related problems. You’ll learn much
more about how services work in chapter 11. By understanding exactly how they’re
implemented, it should be much easier for you to troubleshoot them.
5.8
Summary
In this chapter, you’ve learned how to create Kubernetes Service resources to expose
the services available in your application, regardless of how many pod instances are
providing each service. You’ve learned how Kubernetes
Exposes multiple pods that match a certain label selector under a single, stable
IP address and port
Makes services accessible from inside the cluster by default, but allows you to
make the service accessible from outside the cluster by setting its type to either
NodePort or LoadBalancer
Enables pods to discover services together with their IP addresses and ports by
looking up environment variables
Allows discovery of and communication with services residing outside the
cluster by creating a Service resource without specifying a selector, by creating
an associated Endpoints resource instead
Provides a DNS CNAME alias for external services with the ExternalName ser-
vice type
Exposes multiple HTTP services through a single Ingress (consuming a sin-
gle IP)
 
</data>
  <data key="d5">157
Summary
First, make sure you’re connecting to the service’s cluster IP from within the
cluster, not from the outside.
Don’t bother pinging the service IP to figure out if the service is accessible
(remember, the service’s cluster IP is a virtual IP and pinging it will never work).
If you’ve defined a readiness probe, make sure it’s succeeding; otherwise the
pod won’t be part of the service.
To confirm that a pod is part of the service, examine the corresponding End-
points object with kubectl get endpoints.
If you’re trying to access the service through its FQDN or a part of it (for exam-
ple, myservice.mynamespace.svc.cluster.local or myservice.mynamespace) and
it doesn’t work, see if you can access it using its cluster IP instead of the FQDN.
Check whether you’re connecting to the port exposed by the service and not
the target port.
Try connecting to the pod IP directly to confirm your pod is accepting connec-
tions on the correct port.
If you can’t even access your app through the pod’s IP, make sure your app isn’t
only binding to localhost.
This should help you resolve most of your service-related problems. You’ll learn much
more about how services work in chapter 11. By understanding exactly how they’re
implemented, it should be much easier for you to troubleshoot them.
5.8
Summary
In this chapter, you’ve learned how to create Kubernetes Service resources to expose
the services available in your application, regardless of how many pod instances are
providing each service. You’ve learned how Kubernetes
Exposes multiple pods that match a certain label selector under a single, stable
IP address and port
Makes services accessible from inside the cluster by default, but allows you to
make the service accessible from outside the cluster by setting its type to either
NodePort or LoadBalancer
Enables pods to discover services together with their IP addresses and ports by
looking up environment variables
Allows discovery of and communication with services residing outside the
cluster by creating a Service resource without specifying a selector, by creating
an associated Endpoints resource instead
Provides a DNS CNAME alias for external services with the ExternalName ser-
vice type
Exposes multiple HTTP services through a single Ingress (consuming a sin-
gle IP)
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="190">
  <data key="d0">Page_190</data>
  <data key="d5">Page_190</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_101">
  <data key="d0">158
CHAPTER 5
Services: enabling clients to discover and talk to pods
Uses a pod container’s readiness probe to determine whether a pod should or
shouldn’t be included as a service endpoint
Enables discovery of pod IPs through DNS when you create a headless service
Along with getting a better understanding of services, you’ve also learned how to
Troubleshoot them
Modify firewall rules in Google Kubernetes/Compute Engine
Execute commands in pod containers through kubectl exec 
Run a bash shell in an existing pod’s container
Modify Kubernetes resources through the kubectl apply command
Run an unmanaged ad hoc pod with kubectl run --generator=run-pod/v1
 
</data>
  <data key="d5">158
CHAPTER 5
Services: enabling clients to discover and talk to pods
Uses a pod container’s readiness probe to determine whether a pod should or
shouldn’t be included as a service endpoint
Enables discovery of pod IPs through DNS when you create a headless service
Along with getting a better understanding of services, you’ve also learned how to
Troubleshoot them
Modify firewall rules in Google Kubernetes/Compute Engine
Execute commands in pod containers through kubectl exec 
Run a bash shell in an existing pod’s container
Modify Kubernetes resources through the kubectl apply command
Run an unmanaged ad hoc pod with kubectl run --generator=run-pod/v1
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="191">
  <data key="d0">Page_191</data>
  <data key="d5">Page_191</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_102">
  <data key="d0">159
Volumes: attaching
disk storage to containers
In the previous three chapters, we introduced pods and other Kubernetes resources
that interact with them, namely ReplicationControllers, ReplicaSets, DaemonSets,
Jobs, and Services. Now, we’re going back inside the pod to learn how its containers
can access external disk storage and/or share storage between them.
 We’ve said that pods are similar to logical hosts where processes running inside
them share resources such as CPU, RAM, network interfaces, and others. One
would expect the processes to also share disks, but that’s not the case. You’ll remem-
ber that each container in a pod has its own isolated filesystem, because the file-
system comes from the container’s image.
This chapter covers
Creating multi-container pods
Creating a volume to share disk storage between 
containers
Using a Git repository inside a pod
Attaching persistent storage such as a GCE 
Persistent Disk to pods
Using pre-provisioned persistent storage
Dynamic provisioning of persistent storage
 
</data>
  <data key="d5">159
Volumes: attaching
disk storage to containers
In the previous three chapters, we introduced pods and other Kubernetes resources
that interact with them, namely ReplicationControllers, ReplicaSets, DaemonSets,
Jobs, and Services. Now, we’re going back inside the pod to learn how its containers
can access external disk storage and/or share storage between them.
 We’ve said that pods are similar to logical hosts where processes running inside
them share resources such as CPU, RAM, network interfaces, and others. One
would expect the processes to also share disks, but that’s not the case. You’ll remem-
ber that each container in a pod has its own isolated filesystem, because the file-
system comes from the container’s image.
This chapter covers
Creating multi-container pods
Creating a volume to share disk storage between 
containers
Using a Git repository inside a pod
Attaching persistent storage such as a GCE 
Persistent Disk to pods
Using pre-provisioned persistent storage
Dynamic provisioning of persistent storage
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="192">
  <data key="d0">Page_192</data>
  <data key="d5">Page_192</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_103">
  <data key="d0">160
CHAPTER 6
Volumes: attaching disk storage to containers
 Every new container starts off with the exact set of files that was added to the image
at build time. Combine this with the fact that containers in a pod get restarted (either
because the process died or because the liveness probe signaled to Kubernetes that
the container wasn’t healthy anymore) and you’ll realize that the new container will
not see anything that was written to the filesystem by the previous container, even
though the newly started container runs in the same pod.
 In certain scenarios you want the new container to continue where the last one fin-
ished, such as when restarting a process on a physical machine. You may not need (or
want) the whole filesystem to be persisted, but you do want to preserve the directories
that hold actual data.
 Kubernetes provides this by defining storage volumes. They aren’t top-level resources
like pods, but are instead defined as a part of a pod and share the same lifecycle as the
pod. This means a volume is created when the pod is started and is destroyed when
the pod is deleted. Because of this, a volume’s contents will persist across container
restarts. After a container is restarted, the new container can see all the files that were
written to the volume by the previous container. Also, if a pod contains multiple con-
tainers, the volume can be used by all of them at once. 
6.1
Introducing volumes
Kubernetes volumes are a component of a pod and are thus defined in the pod’s spec-
ification—much like containers. They aren’t a standalone Kubernetes object and can-
not be created or deleted on their own. A volume is available to all containers in the
pod, but it must be mounted in each container that needs to access it. In each con-
tainer, you can mount the volume in any location of its filesystem.
6.1.1
Explaining volumes in an example
Imagine you have a pod with three containers (shown in figure 6.1). One container
runs a web server that serves HTML pages from the /var/htdocs directory and stores
the access log to /var/logs. The second container runs an agent that creates HTML
files and stores them in /var/html. The third container processes the logs it finds in
the /var/logs directory (rotates them, compresses them, analyzes them, or whatever).
 Each container has a nicely defined single responsibility, but on its own each con-
tainer wouldn’t be of much use. Creating a pod with these three containers without
them sharing disk storage doesn’t make any sense, because the content generator
would write the generated HTML files inside its own container and the web server
couldn’t access those files, as it runs in a separate isolated container. Instead, it would
serve an empty directory or whatever you put in the /var/htdocs directory in its con-
tainer image. Similarly, the log rotator would never have anything to do, because its
/var/logs directory would always remain empty with nothing writing logs there. A pod
with these three containers and no volumes basically does nothing.
 But if you add two volumes to the pod and mount them at appropriate paths inside
the three containers, as shown in figure 6.2, you’ve created a system that’s much more
 
</data>
  <data key="d5">160
CHAPTER 6
Volumes: attaching disk storage to containers
 Every new container starts off with the exact set of files that was added to the image
at build time. Combine this with the fact that containers in a pod get restarted (either
because the process died or because the liveness probe signaled to Kubernetes that
the container wasn’t healthy anymore) and you’ll realize that the new container will
not see anything that was written to the filesystem by the previous container, even
though the newly started container runs in the same pod.
 In certain scenarios you want the new container to continue where the last one fin-
ished, such as when restarting a process on a physical machine. You may not need (or
want) the whole filesystem to be persisted, but you do want to preserve the directories
that hold actual data.
 Kubernetes provides this by defining storage volumes. They aren’t top-level resources
like pods, but are instead defined as a part of a pod and share the same lifecycle as the
pod. This means a volume is created when the pod is started and is destroyed when
the pod is deleted. Because of this, a volume’s contents will persist across container
restarts. After a container is restarted, the new container can see all the files that were
written to the volume by the previous container. Also, if a pod contains multiple con-
tainers, the volume can be used by all of them at once. 
6.1
Introducing volumes
Kubernetes volumes are a component of a pod and are thus defined in the pod’s spec-
ification—much like containers. They aren’t a standalone Kubernetes object and can-
not be created or deleted on their own. A volume is available to all containers in the
pod, but it must be mounted in each container that needs to access it. In each con-
tainer, you can mount the volume in any location of its filesystem.
6.1.1
Explaining volumes in an example
Imagine you have a pod with three containers (shown in figure 6.1). One container
runs a web server that serves HTML pages from the /var/htdocs directory and stores
the access log to /var/logs. The second container runs an agent that creates HTML
files and stores them in /var/html. The third container processes the logs it finds in
the /var/logs directory (rotates them, compresses them, analyzes them, or whatever).
 Each container has a nicely defined single responsibility, but on its own each con-
tainer wouldn’t be of much use. Creating a pod with these three containers without
them sharing disk storage doesn’t make any sense, because the content generator
would write the generated HTML files inside its own container and the web server
couldn’t access those files, as it runs in a separate isolated container. Instead, it would
serve an empty directory or whatever you put in the /var/htdocs directory in its con-
tainer image. Similarly, the log rotator would never have anything to do, because its
/var/logs directory would always remain empty with nothing writing logs there. A pod
with these three containers and no volumes basically does nothing.
 But if you add two volumes to the pod and mount them at appropriate paths inside
the three containers, as shown in figure 6.2, you’ve created a system that’s much more
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="193">
  <data key="d0">Page_193</data>
  <data key="d5">Page_193</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_104">
  <data key="d0">161
Introducing volumes
Pod
Container: WebServer
Filesystem
Webserver
process
Writes
Reads
/
var/
htdocs/
logs/
Container: ContentAgent
Filesystem
ContentAgent
process
Writes
/
var/
html/
Container: LogRotator
Filesystem
LogRotator
process
Reads
/
var/
logs/
Figure 6.1
Three containers of the 
same pod without shared storage
Pod
Container: WebServer
Filesystem
/
var/
htdocs/
logs/
Container: ContentAgent
Filesystem
/
var/
html/
Container: LogRotator
Filesystem
/
var/
logs/
Volume:
publicHtml
Volume:
logVol
Figure 6.2
Three containers sharing two 
volumes mounted at various mount paths
 
</data>
  <data key="d5">161
Introducing volumes
Pod
Container: WebServer
Filesystem
Webserver
process
Writes
Reads
/
var/
htdocs/
logs/
Container: ContentAgent
Filesystem
ContentAgent
process
Writes
/
var/
html/
Container: LogRotator
Filesystem
LogRotator
process
Reads
/
var/
logs/
Figure 6.1
Three containers of the 
same pod without shared storage
Pod
Container: WebServer
Filesystem
/
var/
htdocs/
logs/
Container: ContentAgent
Filesystem
/
var/
html/
Container: LogRotator
Filesystem
/
var/
logs/
Volume:
publicHtml
Volume:
logVol
Figure 6.2
Three containers sharing two 
volumes mounted at various mount paths
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="194">
  <data key="d0">Page_194</data>
  <data key="d5">Page_194</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_105">
  <data key="d0">162
CHAPTER 6
Volumes: attaching disk storage to containers
than the sum of its parts. Linux allows you to mount a filesystem at arbitrary locations
in the file tree. When you do that, the contents of the mounted filesystem are accessi-
ble in the directory it’s mounted into. By mounting the same volume into two contain-
ers, they can operate on the same files. In your case, you’re mounting two volumes in
three containers. By doing this, your three containers can work together and do some-
thing useful. Let me explain how.
 First, the pod has a volume called publicHtml. This volume is mounted in the Web-
Server container at /var/htdocs, because that’s the directory the web server serves
files from. The same volume is also mounted in the ContentAgent container, but at
/var/html, because that’s where the agent writes the files to. By mounting this single vol-
ume like that, the web server will now serve the content generated by the content agent.
 Similarly, the pod also has a volume called logVol for storing logs. This volume is
mounted at /var/logs in both the WebServer and the LogRotator containers. Note
that it isn’t mounted in the ContentAgent container. The container cannot access its
files, even though the container and the volume are part of the same pod. It’s not
enough to define a volume in the pod; you need to define a VolumeMount inside the
container’s spec also, if you want the container to be able to access it.
 The two volumes in this example can both initially be empty, so you can use a type
of volume called emptyDir. Kubernetes also supports other types of volumes that are
either populated during initialization of the volume from an external source, or an
existing directory is mounted inside the volume. This process of populating or mount-
ing a volume is performed before the pod’s containers are started. 
 A volume is bound to the lifecycle of a pod and will stay in existence only while the
pod exists, but depending on the volume type, the volume’s files may remain intact
even after the pod and volume disappear, and can later be mounted into a new vol-
ume. Let’s see what types of volumes exist.
6.1.2
Introducing available volume types
A wide variety of volume types is available. Several are generic, while others are spe-
cific to the actual storage technologies used underneath. Don’t worry if you’ve never
heard of those technologies—I hadn’t heard of at least half of them. You’ll probably
only use volume types for the technologies you already know and use. Here’s a list of
several of the available volume types:

emptyDir—A simple empty directory used for storing transient data.

hostPath—Used for mounting directories from the worker node’s filesystem
into the pod.

gitRepo—A volume initialized by checking out the contents of a Git repository.

nfs—An NFS share mounted into the pod.

gcePersistentDisk (Google Compute Engine Persistent Disk), awsElastic-
BlockStore (Amazon Web Services Elastic Block Store Volume), azureDisk
(Microsoft Azure Disk Volume)—Used for mounting cloud provider-specific
storage.
 
</data>
  <data key="d5">162
CHAPTER 6
Volumes: attaching disk storage to containers
than the sum of its parts. Linux allows you to mount a filesystem at arbitrary locations
in the file tree. When you do that, the contents of the mounted filesystem are accessi-
ble in the directory it’s mounted into. By mounting the same volume into two contain-
ers, they can operate on the same files. In your case, you’re mounting two volumes in
three containers. By doing this, your three containers can work together and do some-
thing useful. Let me explain how.
 First, the pod has a volume called publicHtml. This volume is mounted in the Web-
Server container at /var/htdocs, because that’s the directory the web server serves
files from. The same volume is also mounted in the ContentAgent container, but at
/var/html, because that’s where the agent writes the files to. By mounting this single vol-
ume like that, the web server will now serve the content generated by the content agent.
 Similarly, the pod also has a volume called logVol for storing logs. This volume is
mounted at /var/logs in both the WebServer and the LogRotator containers. Note
that it isn’t mounted in the ContentAgent container. The container cannot access its
files, even though the container and the volume are part of the same pod. It’s not
enough to define a volume in the pod; you need to define a VolumeMount inside the
container’s spec also, if you want the container to be able to access it.
 The two volumes in this example can both initially be empty, so you can use a type
of volume called emptyDir. Kubernetes also supports other types of volumes that are
either populated during initialization of the volume from an external source, or an
existing directory is mounted inside the volume. This process of populating or mount-
ing a volume is performed before the pod’s containers are started. 
 A volume is bound to the lifecycle of a pod and will stay in existence only while the
pod exists, but depending on the volume type, the volume’s files may remain intact
even after the pod and volume disappear, and can later be mounted into a new vol-
ume. Let’s see what types of volumes exist.
6.1.2
Introducing available volume types
A wide variety of volume types is available. Several are generic, while others are spe-
cific to the actual storage technologies used underneath. Don’t worry if you’ve never
heard of those technologies—I hadn’t heard of at least half of them. You’ll probably
only use volume types for the technologies you already know and use. Here’s a list of
several of the available volume types:

emptyDir—A simple empty directory used for storing transient data.

hostPath—Used for mounting directories from the worker node’s filesystem
into the pod.

gitRepo—A volume initialized by checking out the contents of a Git repository.

nfs—An NFS share mounted into the pod.

gcePersistentDisk (Google Compute Engine Persistent Disk), awsElastic-
BlockStore (Amazon Web Services Elastic Block Store Volume), azureDisk
(Microsoft Azure Disk Volume)—Used for mounting cloud provider-specific
storage.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="195">
  <data key="d0">Page_195</data>
  <data key="d5">Page_195</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_106">
  <data key="d0">163
Using volumes to share data between containers

cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphere-
Volume, photonPersistentDisk, scaleIO—Used for mounting other types of
network storage.

configMap, secret, downwardAPI—Special types of volumes used to expose cer-
tain Kubernetes resources and cluster information to the pod.

persistentVolumeClaim—A way to use a pre- or dynamically provisioned per-
sistent storage. (We’ll talk about them in the last section of this chapter.)
These volume types serve various purposes. You’ll learn about some of them in the
following sections. Special types of volumes (secret, downwardAPI, configMap) are
covered in the next two chapters, because they aren’t used for storing data, but for
exposing Kubernetes metadata to apps running in the pod. 
 A single pod can use multiple volumes of different types at the same time, and, as
we’ve mentioned before, each of the pod’s containers can either have the volume
mounted or not.
6.2
Using volumes to share data between containers
Although a volume can prove useful even when used by a single container, let’s first
focus on how it’s used for sharing data between multiple containers in a pod.
6.2.1
Using an emptyDir volume
The simplest volume type is the emptyDir volume, so let’s look at it in the first exam-
ple of how to define a volume in a pod. As the name suggests, the volume starts out as
an empty directory. The app running inside the pod can then write any files it needs
to it. Because the volume’s lifetime is tied to that of the pod, the volume’s contents are
lost when the pod is deleted.
 An emptyDir volume is especially useful for sharing files between containers
running in the same pod. But it can also be used by a single container for when a con-
tainer needs to write data to disk temporarily, such as when performing a sort
operation on a large dataset, which can’t fit into the available memory. The data could
also be written to the container’s filesystem itself (remember the top read-write layer
in a container?), but subtle differences exist between the two options. A container’s
filesystem may not even be writable (we’ll talk about this toward the end of the book),
so writing to a mounted volume might be the only option. 
USING AN EMPTYDIR VOLUME IN A POD
Let’s revisit the previous example where a web server, a content agent, and a log rota-
tor share two volumes, but let’s simplify a bit. You’ll build a pod with only the web
server container and the content agent and a single volume for the HTML. 
 You’ll use Nginx as the web server and the UNIX fortune command to generate
the HTML content. The fortune command prints out a random quote every time you
run it. You’ll create a script that invokes the fortune command every 10 seconds and
stores its output in index.html. You’ll find an existing Nginx image available on
 
</data>
  <data key="d5">163
Using volumes to share data between containers

cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphere-
Volume, photonPersistentDisk, scaleIO—Used for mounting other types of
network storage.

configMap, secret, downwardAPI—Special types of volumes used to expose cer-
tain Kubernetes resources and cluster information to the pod.

persistentVolumeClaim—A way to use a pre- or dynamically provisioned per-
sistent storage. (We’ll talk about them in the last section of this chapter.)
These volume types serve various purposes. You’ll learn about some of them in the
following sections. Special types of volumes (secret, downwardAPI, configMap) are
covered in the next two chapters, because they aren’t used for storing data, but for
exposing Kubernetes metadata to apps running in the pod. 
 A single pod can use multiple volumes of different types at the same time, and, as
we’ve mentioned before, each of the pod’s containers can either have the volume
mounted or not.
6.2
Using volumes to share data between containers
Although a volume can prove useful even when used by a single container, let’s first
focus on how it’s used for sharing data between multiple containers in a pod.
6.2.1
Using an emptyDir volume
The simplest volume type is the emptyDir volume, so let’s look at it in the first exam-
ple of how to define a volume in a pod. As the name suggests, the volume starts out as
an empty directory. The app running inside the pod can then write any files it needs
to it. Because the volume’s lifetime is tied to that of the pod, the volume’s contents are
lost when the pod is deleted.
 An emptyDir volume is especially useful for sharing files between containers
running in the same pod. But it can also be used by a single container for when a con-
tainer needs to write data to disk temporarily, such as when performing a sort
operation on a large dataset, which can’t fit into the available memory. The data could
also be written to the container’s filesystem itself (remember the top read-write layer
in a container?), but subtle differences exist between the two options. A container’s
filesystem may not even be writable (we’ll talk about this toward the end of the book),
so writing to a mounted volume might be the only option. 
USING AN EMPTYDIR VOLUME IN A POD
Let’s revisit the previous example where a web server, a content agent, and a log rota-
tor share two volumes, but let’s simplify a bit. You’ll build a pod with only the web
server container and the content agent and a single volume for the HTML. 
 You’ll use Nginx as the web server and the UNIX fortune command to generate
the HTML content. The fortune command prints out a random quote every time you
run it. You’ll create a script that invokes the fortune command every 10 seconds and
stores its output in index.html. You’ll find an existing Nginx image available on
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="196">
  <data key="d0">Page_196</data>
  <data key="d5">Page_196</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_107">
  <data key="d0">164
CHAPTER 6
Volumes: attaching disk storage to containers
Docker Hub, but you’ll need to either create the fortune image yourself or use the
one I’ve already built and pushed to Docker Hub under luksa/fortune. If you want a
refresher on how to build Docker images, refer to the sidebar.
CREATING THE POD
Now that you have the two images required to run your pod, it’s time to create the pod
manifest. Create a file called fortune-pod.yaml with the contents shown in the follow-
ing listing.
apiVersion: v1
kind: Pod
metadata:
  name: fortune
spec:
  containers:
Building the fortune container image
Here’s how to build the image. Create a new directory called fortune and then inside
it, create a fortuneloop.sh shell script with the following contents:
#!/bin/bash
trap "exit" SIGINT
mkdir /var/htdocs
while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune &gt; /var/htdocs/index.html
  sleep 10
done
Then, in the same directory, create a file called Dockerfile containing the following:
FROM ubuntu:latest
RUN apt-get update ; apt-get -y install fortune
ADD fortuneloop.sh /bin/fortuneloop.sh
ENTRYPOINT /bin/fortuneloop.sh
The image is based on the ubuntu:latest image, which doesn’t include the fortune
binary by default. That’s why in the second line of the Dockerfile you install it with
apt-get. After that, you add the fortuneloop.sh script to the image’s /bin folder.
In the last line of the Dockerfile, you specify that the fortuneloop.sh script should
be executed when the image is run.
After preparing both files, build and upload the image to Docker Hub with the following
two commands (replace luksa with your own Docker Hub user ID):
$ docker build -t luksa/fortune .
$ docker push luksa/fortune
Listing 6.1
A pod with two containers sharing the same volume: fortune-pod.yaml
 
</data>
  <data key="d5">164
CHAPTER 6
Volumes: attaching disk storage to containers
Docker Hub, but you’ll need to either create the fortune image yourself or use the
one I’ve already built and pushed to Docker Hub under luksa/fortune. If you want a
refresher on how to build Docker images, refer to the sidebar.
CREATING THE POD
Now that you have the two images required to run your pod, it’s time to create the pod
manifest. Create a file called fortune-pod.yaml with the contents shown in the follow-
ing listing.
apiVersion: v1
kind: Pod
metadata:
  name: fortune
spec:
  containers:
Building the fortune container image
Here’s how to build the image. Create a new directory called fortune and then inside
it, create a fortuneloop.sh shell script with the following contents:
#!/bin/bash
trap "exit" SIGINT
mkdir /var/htdocs
while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune &gt; /var/htdocs/index.html
  sleep 10
done
Then, in the same directory, create a file called Dockerfile containing the following:
FROM ubuntu:latest
RUN apt-get update ; apt-get -y install fortune
ADD fortuneloop.sh /bin/fortuneloop.sh
ENTRYPOINT /bin/fortuneloop.sh
The image is based on the ubuntu:latest image, which doesn’t include the fortune
binary by default. That’s why in the second line of the Dockerfile you install it with
apt-get. After that, you add the fortuneloop.sh script to the image’s /bin folder.
In the last line of the Dockerfile, you specify that the fortuneloop.sh script should
be executed when the image is run.
After preparing both files, build and upload the image to Docker Hub with the following
two commands (replace luksa with your own Docker Hub user ID):
$ docker build -t luksa/fortune .
$ docker push luksa/fortune
Listing 6.1
A pod with two containers sharing the same volume: fortune-pod.yaml
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="197">
  <data key="d0">Page_197</data>
  <data key="d5">Page_197</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_108">
  <data key="d0">165
Using volumes to share data between containers
  - image: luksa/fortune                   
    name: html-generator                   
    volumeMounts:                          
    - name: html                           
      mountPath: /var/htdocs               
  - image: nginx:alpine                   
    name: web-server                      
    volumeMounts:                         
    - name: html                          
      mountPath: /usr/share/nginx/html    
      readOnly: true                      
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:                 
  - name: html             
    emptyDir: {}           
The pod contains two containers and a single volume that’s mounted in both of
them, yet at different paths. When the html-generator container starts, it starts writ-
ing the output of the fortune command to the /var/htdocs/index.html file every 10
seconds. Because the volume is mounted at /var/htdocs, the index.html file is writ-
ten to the volume instead of the container’s top layer. As soon as the web-server con-
tainer starts, it starts serving whatever HTML files are in the /usr/share/nginx/html
directory (this is the default directory Nginx serves files from). Because you mounted
the volume in that exact location, Nginx will serve the index.html file written there
by the container running the fortune loop. The end effect is that a client sending an
HTTP request to the pod on port 80 will receive the current fortune message as
the response. 
SEEING THE POD IN ACTION
To see the fortune message, you need to enable access to the pod. You’ll do that by
forwarding a port from your local machine to the pod:
$ kubectl port-forward fortune 8080:80
Forwarding from 127.0.0.1:8080 -&gt; 80
Forwarding from [::1]:8080 -&gt; 80
NOTE
As an exercise, you can also expose the pod through a service instead
of using port forwarding.
Now you can access the Nginx server through port 8080 of your local machine. Use
curl to do that:
$ curl http://localhost:8080
Beware of a tall blond man with one black shoe.
If you wait a few seconds and send another request, you should receive a different
message. By combining two containers, you created a simple app to see how a volume
can glue together two containers and enhance what each of them does.
The first container is called html-generator 
and runs the luksa/fortune image.
The volume called html is mounted 
at /var/htdocs in the container.
The second container is called web-server 
and runs the nginx:alpine image.
The same volume as above is 
mounted at /usr/share/nginx/html 
as read-only.
A single emptyDir volume 
called html that’s mounted 
in the two containers above
 
</data>
  <data key="d5">165
Using volumes to share data between containers
  - image: luksa/fortune                   
    name: html-generator                   
    volumeMounts:                          
    - name: html                           
      mountPath: /var/htdocs               
  - image: nginx:alpine                   
    name: web-server                      
    volumeMounts:                         
    - name: html                          
      mountPath: /usr/share/nginx/html    
      readOnly: true                      
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:                 
  - name: html             
    emptyDir: {}           
The pod contains two containers and a single volume that’s mounted in both of
them, yet at different paths. When the html-generator container starts, it starts writ-
ing the output of the fortune command to the /var/htdocs/index.html file every 10
seconds. Because the volume is mounted at /var/htdocs, the index.html file is writ-
ten to the volume instead of the container’s top layer. As soon as the web-server con-
tainer starts, it starts serving whatever HTML files are in the /usr/share/nginx/html
directory (this is the default directory Nginx serves files from). Because you mounted
the volume in that exact location, Nginx will serve the index.html file written there
by the container running the fortune loop. The end effect is that a client sending an
HTTP request to the pod on port 80 will receive the current fortune message as
the response. 
SEEING THE POD IN ACTION
To see the fortune message, you need to enable access to the pod. You’ll do that by
forwarding a port from your local machine to the pod:
$ kubectl port-forward fortune 8080:80
Forwarding from 127.0.0.1:8080 -&gt; 80
Forwarding from [::1]:8080 -&gt; 80
NOTE
As an exercise, you can also expose the pod through a service instead
of using port forwarding.
Now you can access the Nginx server through port 8080 of your local machine. Use
curl to do that:
$ curl http://localhost:8080
Beware of a tall blond man with one black shoe.
If you wait a few seconds and send another request, you should receive a different
message. By combining two containers, you created a simple app to see how a volume
can glue together two containers and enhance what each of them does.
The first container is called html-generator 
and runs the luksa/fortune image.
The volume called html is mounted 
at /var/htdocs in the container.
The second container is called web-server 
and runs the nginx:alpine image.
The same volume as above is 
mounted at /usr/share/nginx/html 
as read-only.
A single emptyDir volume 
called html that’s mounted 
in the two containers above
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="198">
  <data key="d0">Page_198</data>
  <data key="d5">Page_198</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_109">
  <data key="d0">166
CHAPTER 6
Volumes: attaching disk storage to containers
SPECIFYING THE MEDIUM TO USE FOR THE EMPTYDIR
The emptyDir you used as the volume was created on the actual disk of the worker
node hosting your pod, so its performance depends on the type of the node’s disks.
But you can tell Kubernetes to create the emptyDir on a tmpfs filesystem (in memory
instead of on disk). To do this, set the emptyDir’s medium to Memory like this:
volumes:
  - name: html
    emptyDir:
      medium: Memory    
An emptyDir volume is the simplest type of volume, but other types build upon it.
After the empty directory is created, they populate it with data. One such volume type
is the gitRepo volume type, which we’ll introduce next.
6.2.2
Using a Git repository as the starting point for a volume 
A gitRepo volume is basically an emptyDir volume that gets populated by cloning a
Git repository and checking out a specific revision when the pod is starting up (but
before its containers are created). Figure 6.3 shows how this unfolds.
NOTE
After the gitRepo volume is created, it isn’t kept in sync with the repo
it’s referencing. The files in the volume will not be updated when you push
additional commits to the Git repository. However, if your pod is managed by
a ReplicationController, deleting the pod will result in a new pod being cre-
ated and this new pod’s volume will then contain the latest commits. 
For example, you can use a Git repository to store static HTML files of your website
and create a pod containing a web server container and a gitRepo volume. Every time
the pod is created, it pulls the latest version of your website and starts serving it. The
This emptyDir’s 
files should be 
stored in memory.
Pod
Container
User
gitRepo
volume
1. User (or a replication
controller) creates pod
with gitRepo volume
2. Kubernetes creates
an empty directory and
clones the speciﬁed Git
repository into it
3. The pod’s container is started
(with the volume mounted at
the mount path)
Repository
Figure 6.3
A gitRepo volume is an emptyDir volume initially populated with the contents of a 
Git repository.
 
</data>
  <data key="d5">166
CHAPTER 6
Volumes: attaching disk storage to containers
SPECIFYING THE MEDIUM TO USE FOR THE EMPTYDIR
The emptyDir you used as the volume was created on the actual disk of the worker
node hosting your pod, so its performance depends on the type of the node’s disks.
But you can tell Kubernetes to create the emptyDir on a tmpfs filesystem (in memory
instead of on disk). To do this, set the emptyDir’s medium to Memory like this:
volumes:
  - name: html
    emptyDir:
      medium: Memory    
An emptyDir volume is the simplest type of volume, but other types build upon it.
After the empty directory is created, they populate it with data. One such volume type
is the gitRepo volume type, which we’ll introduce next.
6.2.2
Using a Git repository as the starting point for a volume 
A gitRepo volume is basically an emptyDir volume that gets populated by cloning a
Git repository and checking out a specific revision when the pod is starting up (but
before its containers are created). Figure 6.3 shows how this unfolds.
NOTE
After the gitRepo volume is created, it isn’t kept in sync with the repo
it’s referencing. The files in the volume will not be updated when you push
additional commits to the Git repository. However, if your pod is managed by
a ReplicationController, deleting the pod will result in a new pod being cre-
ated and this new pod’s volume will then contain the latest commits. 
For example, you can use a Git repository to store static HTML files of your website
and create a pod containing a web server container and a gitRepo volume. Every time
the pod is created, it pulls the latest version of your website and starts serving it. The
This emptyDir’s 
files should be 
stored in memory.
Pod
Container
User
gitRepo
volume
1. User (or a replication
controller) creates pod
with gitRepo volume
2. Kubernetes creates
an empty directory and
clones the speciﬁed Git
repository into it
3. The pod’s container is started
(with the volume mounted at
the mount path)
Repository
Figure 6.3
A gitRepo volume is an emptyDir volume initially populated with the contents of a 
Git repository.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="199">
  <data key="d0">Page_199</data>
  <data key="d5">Page_199</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_110">
  <data key="d0">167
Using volumes to share data between containers
only drawback to this is that you need to delete the pod every time you push changes
to the gitRepo and want to start serving the new version of the website. 
 Let’s do this right now. It’s not that different from what you did before. 
RUNNING A WEB SERVER POD SERVING FILES FROM A CLONED GIT REPOSITORY
Before you create your pod, you’ll need an actual Git repository with HTML files in it.
I’ve created a repo on GitHub at https:/
/github.com/luksa/kubia-website-example.git.
You’ll need to fork it (create your own copy of the repo on GitHub) so you can push
changes to it later. 
 Once you’ve created your fork, you can move on to creating the pod. This time,
you’ll only need a single Nginx container and a single gitRepo volume in the pod (be
sure to point the gitRepo volume to your own fork of my repository), as shown in the
following listing.
apiVersion: v1
kind: Pod
metadata:
  name: gitrepo-volume-pod
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    gitRepo:                     
      repository: https://github.com/luksa/kubia-website-example.git   
      revision: master                     
      directory: .      
When you create the pod, the volume is first initialized as an empty directory and then
the specified Git repository is cloned into it. If you hadn’t set the directory to . (dot),
the repository would have been cloned into the kubia-website-example subdirectory,
which isn’t what you want. You want the repo to be cloned into the root directory of
your volume. Along with the repository, you also specified you want Kubernetes to
check out whatever revision the master branch is pointing to at the time the volume
is created. 
 With the pod running, you can try hitting it through port forwarding, a service, or by
executing the curl command from within the pod (or any other pod inside the cluster). 
Listing 6.2
A pod using a gitRepo volume: gitrepo-volume-pod.yaml
You’re creating a 
gitRepo volume.
The volume will clone
this Git repository.
The master branch 
will be checked out.
You want the repo to 
be cloned into the root 
dir of the volume.
 
</data>
  <data key="d5">167
Using volumes to share data between containers
only drawback to this is that you need to delete the pod every time you push changes
to the gitRepo and want to start serving the new version of the website. 
 Let’s do this right now. It’s not that different from what you did before. 
RUNNING A WEB SERVER POD SERVING FILES FROM A CLONED GIT REPOSITORY
Before you create your pod, you’ll need an actual Git repository with HTML files in it.
I’ve created a repo on GitHub at https:/
/github.com/luksa/kubia-website-example.git.
You’ll need to fork it (create your own copy of the repo on GitHub) so you can push
changes to it later. 
 Once you’ve created your fork, you can move on to creating the pod. This time,
you’ll only need a single Nginx container and a single gitRepo volume in the pod (be
sure to point the gitRepo volume to your own fork of my repository), as shown in the
following listing.
apiVersion: v1
kind: Pod
metadata:
  name: gitrepo-volume-pod
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    gitRepo:                     
      repository: https://github.com/luksa/kubia-website-example.git   
      revision: master                     
      directory: .      
When you create the pod, the volume is first initialized as an empty directory and then
the specified Git repository is cloned into it. If you hadn’t set the directory to . (dot),
the repository would have been cloned into the kubia-website-example subdirectory,
which isn’t what you want. You want the repo to be cloned into the root directory of
your volume. Along with the repository, you also specified you want Kubernetes to
check out whatever revision the master branch is pointing to at the time the volume
is created. 
 With the pod running, you can try hitting it through port forwarding, a service, or by
executing the curl command from within the pod (or any other pod inside the cluster). 
Listing 6.2
A pod using a gitRepo volume: gitrepo-volume-pod.yaml
You’re creating a 
gitRepo volume.
The volume will clone
this Git repository.
The master branch 
will be checked out.
You want the repo to 
be cloned into the root 
dir of the volume.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="200">
  <data key="d0">Page_200</data>
  <data key="d5">Page_200</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_111">
  <data key="d0">168
CHAPTER 6
Volumes: attaching disk storage to containers
CONFIRMING THE FILES AREN’T KEPT IN SYNC WITH THE GIT REPO
Now you’ll make changes to the index.html file in your GitHub repository. If you
don’t use Git locally, you can edit the file on GitHub directly—click on the file in your
GitHub repository to open it and then click on the pencil icon to start editing it.
Change the text and then commit the changes by clicking the button at the bottom.
 The master branch of the Git repository now includes the changes you made to the
HTML file. These changes will not be visible on your Nginx web server yet, because
the gitRepo volume isn’t kept in sync with the Git repository. You can confirm this by
hitting the pod again. 
 To see the new version of the website, you need to delete the pod and create
it again. Instead of having to delete the pod every time you make changes, you could
run an additional process, which keeps your volume in sync with the Git repository.
I won’t explain in detail how to do this. Instead, try doing this yourself as an exer-
cise, but here are a few pointers.
INTRODUCING SIDECAR CONTAINERS
The Git sync process shouldn’t run in the same container as the Nginx web server, but
in a second container: a sidecar container. A sidecar container is a container that aug-
ments the operation of the main container of the pod. You add a sidecar to a pod so
you can use an existing container image instead of cramming additional logic into the
main app’s code, which would make it overly complex and less reusable. 
 To find an existing container image, which keeps a local directory synchronized
with a Git repository, go to Docker Hub and search for “git sync.” You’ll find many
images that do that. Then use the image in a new container in the pod from the previ-
ous example, mount the pod’s existing gitRepo volume in the new container, and
configure the Git sync container to keep the files in sync with your Git repo. If you set
everything up correctly, you should see that the files the web server is serving are kept
in sync with your GitHub repo. 
NOTE
An example in chapter 18 includes using a Git sync container like the
one explained here, so you can wait until you reach chapter 18 and follow the
step-by-step instructions then instead of doing this exercise on your own now. 
USING A GITREPO VOLUME WITH PRIVATE GIT REPOSITORIES
There’s one other reason for having to resort to Git sync sidecar containers. We
haven’t talked about whether you can use a gitRepo volume with a private Git repo. It
turns out you can’t. The current consensus among Kubernetes developers is to keep
the gitRepo volume simple and not add any support for cloning private repositories
through the SSH protocol, because that would require adding additional config
options to the gitRepo volume. 
 If you want to clone a private Git repo into your container, you should use a git-
sync sidecar or a similar method instead of a gitRepo volume.
 
</data>
  <data key="d5">168
CHAPTER 6
Volumes: attaching disk storage to containers
CONFIRMING THE FILES AREN’T KEPT IN SYNC WITH THE GIT REPO
Now you’ll make changes to the index.html file in your GitHub repository. If you
don’t use Git locally, you can edit the file on GitHub directly—click on the file in your
GitHub repository to open it and then click on the pencil icon to start editing it.
Change the text and then commit the changes by clicking the button at the bottom.
 The master branch of the Git repository now includes the changes you made to the
HTML file. These changes will not be visible on your Nginx web server yet, because
the gitRepo volume isn’t kept in sync with the Git repository. You can confirm this by
hitting the pod again. 
 To see the new version of the website, you need to delete the pod and create
it again. Instead of having to delete the pod every time you make changes, you could
run an additional process, which keeps your volume in sync with the Git repository.
I won’t explain in detail how to do this. Instead, try doing this yourself as an exer-
cise, but here are a few pointers.
INTRODUCING SIDECAR CONTAINERS
The Git sync process shouldn’t run in the same container as the Nginx web server, but
in a second container: a sidecar container. A sidecar container is a container that aug-
ments the operation of the main container of the pod. You add a sidecar to a pod so
you can use an existing container image instead of cramming additional logic into the
main app’s code, which would make it overly complex and less reusable. 
 To find an existing container image, which keeps a local directory synchronized
with a Git repository, go to Docker Hub and search for “git sync.” You’ll find many
images that do that. Then use the image in a new container in the pod from the previ-
ous example, mount the pod’s existing gitRepo volume in the new container, and
configure the Git sync container to keep the files in sync with your Git repo. If you set
everything up correctly, you should see that the files the web server is serving are kept
in sync with your GitHub repo. 
NOTE
An example in chapter 18 includes using a Git sync container like the
one explained here, so you can wait until you reach chapter 18 and follow the
step-by-step instructions then instead of doing this exercise on your own now. 
USING A GITREPO VOLUME WITH PRIVATE GIT REPOSITORIES
There’s one other reason for having to resort to Git sync sidecar containers. We
haven’t talked about whether you can use a gitRepo volume with a private Git repo. It
turns out you can’t. The current consensus among Kubernetes developers is to keep
the gitRepo volume simple and not add any support for cloning private repositories
through the SSH protocol, because that would require adding additional config
options to the gitRepo volume. 
 If you want to clone a private Git repo into your container, you should use a git-
sync sidecar or a similar method instead of a gitRepo volume.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="201">
  <data key="d0">Page_201</data>
  <data key="d5">Page_201</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_112">
  <data key="d0">169
Accessing files on the worker node’s filesystem
WRAPPING UP THE GITREPO VOLUME
A gitRepo volume, like the emptyDir volume, is basically a dedicated directory cre-
ated specifically for, and used exclusively by, the pod that contains the volume. When
the pod is deleted, the volume and its contents are deleted. Other types of volumes,
however, don’t create a new directory, but instead mount an existing external direc-
tory into the pod’s container’s filesystem. The contents of that volume can survive
multiple pod instantiations. We’ll learn about those types of volumes next.
6.3
Accessing files on the worker node’s filesystem
Most  pods should be oblivious of their host node, so they shouldn’t access any files on
the node’s filesystem. But certain system-level pods (remember, these will usually be
managed by a DaemonSet) do need to either read the node’s files or use the node’s
filesystem to access the node’s devices through the filesystem. Kubernetes makes this
possible through a hostPath volume. 
6.3.1
Introducing the hostPath volume
A hostPath volume points to a specific file or directory on the node’s filesystem (see
figure 6.4). Pods running on the same node and using the same path in their host-
Path volume see the same files.
hostPath volumes are the first type of persistent storage we’re introducing, because
both the gitRepo and emptyDir volumes’ contents get deleted when a pod is torn
down, whereas a hostPath volume’s contents don’t. If a pod is deleted and the next
pod uses a hostPath volume pointing to the same path on the host, the new pod will
see whatever was left behind by the previous pod, but only if it’s scheduled to the same
node as the first pod.
Node 1
Pod
hostPath
volume
Pod
hostPath
volume
Node 2
Pod
hostPath
volume
/some/path/on/host
/some/path/on/host
Figure 6.4
A hostPath volume mounts a file or directory on the worker node into 
the container’s filesystem.
 
</data>
  <data key="d5">169
Accessing files on the worker node’s filesystem
WRAPPING UP THE GITREPO VOLUME
A gitRepo volume, like the emptyDir volume, is basically a dedicated directory cre-
ated specifically for, and used exclusively by, the pod that contains the volume. When
the pod is deleted, the volume and its contents are deleted. Other types of volumes,
however, don’t create a new directory, but instead mount an existing external direc-
tory into the pod’s container’s filesystem. The contents of that volume can survive
multiple pod instantiations. We’ll learn about those types of volumes next.
6.3
Accessing files on the worker node’s filesystem
Most  pods should be oblivious of their host node, so they shouldn’t access any files on
the node’s filesystem. But certain system-level pods (remember, these will usually be
managed by a DaemonSet) do need to either read the node’s files or use the node’s
filesystem to access the node’s devices through the filesystem. Kubernetes makes this
possible through a hostPath volume. 
6.3.1
Introducing the hostPath volume
A hostPath volume points to a specific file or directory on the node’s filesystem (see
figure 6.4). Pods running on the same node and using the same path in their host-
Path volume see the same files.
hostPath volumes are the first type of persistent storage we’re introducing, because
both the gitRepo and emptyDir volumes’ contents get deleted when a pod is torn
down, whereas a hostPath volume’s contents don’t. If a pod is deleted and the next
pod uses a hostPath volume pointing to the same path on the host, the new pod will
see whatever was left behind by the previous pod, but only if it’s scheduled to the same
node as the first pod.
Node 1
Pod
hostPath
volume
Pod
hostPath
volume
Node 2
Pod
hostPath
volume
/some/path/on/host
/some/path/on/host
Figure 6.4
A hostPath volume mounts a file or directory on the worker node into 
the container’s filesystem.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="202">
  <data key="d0">Page_202</data>
  <data key="d5">Page_202</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_113">
  <data key="d0">170
CHAPTER 6
Volumes: attaching disk storage to containers
 If you’re thinking of using a hostPath volume as the place to store a database’s
data directory, think again. Because the volume’s contents are stored on a specific
node’s filesystem, when the database pod gets rescheduled to another node, it will no
longer see the data. This explains why it’s not a good idea to use a hostPath volume
for regular pods, because it makes the pod sensitive to what node it’s scheduled to.
6.3.2
Examining system pods that use hostPath volumes
Let’s see how a hostPath volume can be used properly. Instead of creating a new pod,
let’s see if any existing system-wide pods are already using this type of volume. As you
may remember from one of the previous chapters, several such pods are running in
the kube-system namespace. Let’s list them again:
$ kubectl get pod s --namespace kube-system
NAME                          READY     STATUS    RESTARTS   AGE
fluentd-kubia-4ebc2f1e-9a3e   1/1       Running   1          4d
fluentd-kubia-4ebc2f1e-e2vz   1/1       Running   1          31d
...
Pick the first one and see what kinds of volumes it uses (shown in the following listing).
$ kubectl describe po fluentd-kubia-4ebc2f1e-9a3e --namespace kube-system
Name:           fluentd-cloud-logging-gke-kubia-default-pool-4ebc2f1e-9a3e
Namespace:      kube-system
...
Volumes:
  varlog:
    Type:       HostPath (bare host directory volume)
    Path:       /var/log
  varlibdockercontainers:
    Type:       HostPath (bare host directory volume)
    Path:       /var/lib/docker/containers
TIP
If you’re using Minikube, try the kube-addon-manager-minikube pod.
Aha! The pod uses two hostPath volumes to gain access to the node’s /var/log and
the /var/lib/docker/containers directories. You’d think you were lucky to find a pod
using a hostPath volume on the first try, but not really (at least not on GKE). Check
the other pods, and you’ll see most use this type of volume either to access the node’s
log files, kubeconfig (the Kubernetes config file), or the CA certificates.
 If you inspect the other pods, you’ll see none of them uses the hostPath volume
for storing their own data. They all use it to get access to the node’s data. But as we’ll
see later in the chapter, hostPath volumes are often used for trying out persistent stor-
age in single-node clusters, such as the one created by Minikube. Read on to learn
about the types of volumes you should use for storing persistent data properly even in
a multi-node cluster.
Listing 6.3
 A pod using hostPath volumes to access the node’s logs
 
</data>
  <data key="d5">170
CHAPTER 6
Volumes: attaching disk storage to containers
 If you’re thinking of using a hostPath volume as the place to store a database’s
data directory, think again. Because the volume’s contents are stored on a specific
node’s filesystem, when the database pod gets rescheduled to another node, it will no
longer see the data. This explains why it’s not a good idea to use a hostPath volume
for regular pods, because it makes the pod sensitive to what node it’s scheduled to.
6.3.2
Examining system pods that use hostPath volumes
Let’s see how a hostPath volume can be used properly. Instead of creating a new pod,
let’s see if any existing system-wide pods are already using this type of volume. As you
may remember from one of the previous chapters, several such pods are running in
the kube-system namespace. Let’s list them again:
$ kubectl get pod s --namespace kube-system
NAME                          READY     STATUS    RESTARTS   AGE
fluentd-kubia-4ebc2f1e-9a3e   1/1       Running   1          4d
fluentd-kubia-4ebc2f1e-e2vz   1/1       Running   1          31d
...
Pick the first one and see what kinds of volumes it uses (shown in the following listing).
$ kubectl describe po fluentd-kubia-4ebc2f1e-9a3e --namespace kube-system
Name:           fluentd-cloud-logging-gke-kubia-default-pool-4ebc2f1e-9a3e
Namespace:      kube-system
...
Volumes:
  varlog:
    Type:       HostPath (bare host directory volume)
    Path:       /var/log
  varlibdockercontainers:
    Type:       HostPath (bare host directory volume)
    Path:       /var/lib/docker/containers
TIP
If you’re using Minikube, try the kube-addon-manager-minikube pod.
Aha! The pod uses two hostPath volumes to gain access to the node’s /var/log and
the /var/lib/docker/containers directories. You’d think you were lucky to find a pod
using a hostPath volume on the first try, but not really (at least not on GKE). Check
the other pods, and you’ll see most use this type of volume either to access the node’s
log files, kubeconfig (the Kubernetes config file), or the CA certificates.
 If you inspect the other pods, you’ll see none of them uses the hostPath volume
for storing their own data. They all use it to get access to the node’s data. But as we’ll
see later in the chapter, hostPath volumes are often used for trying out persistent stor-
age in single-node clusters, such as the one created by Minikube. Read on to learn
about the types of volumes you should use for storing persistent data properly even in
a multi-node cluster.
Listing 6.3
 A pod using hostPath volumes to access the node’s logs
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="203">
  <data key="d0">Page_203</data>
  <data key="d5">Page_203</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_114">
  <data key="d0">171
Using persistent storage
TIP
Remember to use hostPath volumes only if you need to read or write sys-
tem files on the node. Never use them to persist data across pods. 
6.4
Using persistent storage
When an application running in a pod needs to persist data to disk and have that
same data available even when the pod is rescheduled to another node, you can’t use
any of the volume types we’ve mentioned so far. Because this data needs to be accessi-
ble from any cluster node, it must be stored on some type of network-attached stor-
age (NAS).
 To learn about volumes that allow persisting data, you’ll create a pod that will run
the MongoDB document-oriented NoSQL database. Running a database pod without
a volume or with a non-persistent volume doesn’t make sense, except for testing
purposes, so you’ll add an appropriate type of volume to the pod and mount it in the
MongoDB container. 
6.4.1
Using a GCE Persistent Disk in a pod volume
If you’ve been running these examples on Google Kubernetes Engine, which runs
your cluster nodes on Google Compute Engine (GCE), you’ll use a GCE Persistent
Disk as your underlying storage mechanism. 
 In the early versions, Kubernetes didn’t provision the underlying storage automati-
cally—you had to do that manually. Automatic provisioning is now possible, and you’ll
learn about it later in the chapter, but first, you’ll start by provisioning the storage
manually. It will give you a chance to learn exactly what’s going on underneath. 
CREATING A GCE PERSISTENT DISK
You’ll start by creating the GCE persistent disk first. You need to create it in the same
zone as your Kubernetes cluster. If you don’t remember what zone you created the
cluster in, you can see it by listing your Kubernetes clusters with the gcloud command
like this:
$ gcloud container clusters list
NAME   ZONE            MASTER_VERSION  MASTER_IP       ...
kubia  europe-west1-b  1.2.5           104.155.84.137  ...
This shows you’ve created your cluster in zone europe-west1-b, so you need to create
the GCE persistent disk in the same zone as well. You create the disk like this:
$ gcloud compute disks create --size=1GiB --zone=europe-west1-b mongodb
WARNING: You have selected a disk size of under [200GB]. This may result in 
poor I/O performance. For more information, see: 
https://developers.google.com/compute/docs/disks#pdperformance.
Created [https://www.googleapis.com/compute/v1/projects/rapid-pivot-
136513/zones/europe-west1-b/disks/mongodb].
NAME     ZONE            SIZE_GB  TYPE         STATUS
mongodb  europe-west1-b  1        pd-standard  READY
 
</data>
  <data key="d5">171
Using persistent storage
TIP
Remember to use hostPath volumes only if you need to read or write sys-
tem files on the node. Never use them to persist data across pods. 
6.4
Using persistent storage
When an application running in a pod needs to persist data to disk and have that
same data available even when the pod is rescheduled to another node, you can’t use
any of the volume types we’ve mentioned so far. Because this data needs to be accessi-
ble from any cluster node, it must be stored on some type of network-attached stor-
age (NAS).
 To learn about volumes that allow persisting data, you’ll create a pod that will run
the MongoDB document-oriented NoSQL database. Running a database pod without
a volume or with a non-persistent volume doesn’t make sense, except for testing
purposes, so you’ll add an appropriate type of volume to the pod and mount it in the
MongoDB container. 
6.4.1
Using a GCE Persistent Disk in a pod volume
If you’ve been running these examples on Google Kubernetes Engine, which runs
your cluster nodes on Google Compute Engine (GCE), you’ll use a GCE Persistent
Disk as your underlying storage mechanism. 
 In the early versions, Kubernetes didn’t provision the underlying storage automati-
cally—you had to do that manually. Automatic provisioning is now possible, and you’ll
learn about it later in the chapter, but first, you’ll start by provisioning the storage
manually. It will give you a chance to learn exactly what’s going on underneath. 
CREATING A GCE PERSISTENT DISK
You’ll start by creating the GCE persistent disk first. You need to create it in the same
zone as your Kubernetes cluster. If you don’t remember what zone you created the
cluster in, you can see it by listing your Kubernetes clusters with the gcloud command
like this:
$ gcloud container clusters list
NAME   ZONE            MASTER_VERSION  MASTER_IP       ...
kubia  europe-west1-b  1.2.5           104.155.84.137  ...
This shows you’ve created your cluster in zone europe-west1-b, so you need to create
the GCE persistent disk in the same zone as well. You create the disk like this:
$ gcloud compute disks create --size=1GiB --zone=europe-west1-b mongodb
WARNING: You have selected a disk size of under [200GB]. This may result in 
poor I/O performance. For more information, see: 
https://developers.google.com/compute/docs/disks#pdperformance.
Created [https://www.googleapis.com/compute/v1/projects/rapid-pivot-
136513/zones/europe-west1-b/disks/mongodb].
NAME     ZONE            SIZE_GB  TYPE         STATUS
mongodb  europe-west1-b  1        pd-standard  READY
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="204">
  <data key="d0">Page_204</data>
  <data key="d5">Page_204</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_115">
  <data key="d0">172
CHAPTER 6
Volumes: attaching disk storage to containers
This command creates a 1 GiB large GCE persistent disk called mongodb. You can
ignore the warning about the disk size, because you don’t care about the disk’s perfor-
mance for the tests you’re about to run.
CREATING A POD USING A GCEPERSISTENTDISK VOLUME
Now that you have your physical storage properly set up, you can use it in a volume
inside your MongoDB pod. You’re going to prepare the YAML for the pod, which is
shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  volumes:
  - name: mongodb-data          
    gcePersistentDisk:           
      pdName: mongodb            
      fsType: ext4             
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:                
    - name: mongodb-data         
      mountPath: /data/db      
    ports:
    - containerPort: 27017
      protocol: TCP
NOTE
If you’re using Minikube, you can’t use a GCE Persistent Disk, but you
can deploy mongodb-pod-hostpath.yaml, which uses a hostPath volume
instead of a GCE PD.
The pod contains a single container and a single volume backed by the GCE Per-
sistent Disk you’ve created (as shown in figure 6.5). You’re mounting the volume
inside the container at /data/db, because that’s where MongoDB stores its data.
Listing 6.4
A pod using a gcePersistentDisk volume: mongodb-pod-gcepd.yaml
The name
of the
volume
(also
referenced
when
mounting
the volume)
The type of the volume 
is a GCE Persistent Disk.
The name of the persistent 
disk must match the actual 
PD you created earlier.
The filesystem type is EXT4 
(a type of Linux filesystem).
The path where MongoDB 
stores its data
Pod: mongodb
Container: mongodb
volumeMounts:
name: mongodb-data
mountPath: /data/db
gcePersistentDisk:
pdName: mongodb
GCE
Persistent Disk:
mongodb
Volume:
mongodb
Figure 6.5
A pod with a single container running MongoDB, which mounts a volume referencing an 
external GCE Persistent Disk
 
</data>
  <data key="d5">172
CHAPTER 6
Volumes: attaching disk storage to containers
This command creates a 1 GiB large GCE persistent disk called mongodb. You can
ignore the warning about the disk size, because you don’t care about the disk’s perfor-
mance for the tests you’re about to run.
CREATING A POD USING A GCEPERSISTENTDISK VOLUME
Now that you have your physical storage properly set up, you can use it in a volume
inside your MongoDB pod. You’re going to prepare the YAML for the pod, which is
shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  volumes:
  - name: mongodb-data          
    gcePersistentDisk:           
      pdName: mongodb            
      fsType: ext4             
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:                
    - name: mongodb-data         
      mountPath: /data/db      
    ports:
    - containerPort: 27017
      protocol: TCP
NOTE
If you’re using Minikube, you can’t use a GCE Persistent Disk, but you
can deploy mongodb-pod-hostpath.yaml, which uses a hostPath volume
instead of a GCE PD.
The pod contains a single container and a single volume backed by the GCE Per-
sistent Disk you’ve created (as shown in figure 6.5). You’re mounting the volume
inside the container at /data/db, because that’s where MongoDB stores its data.
Listing 6.4
A pod using a gcePersistentDisk volume: mongodb-pod-gcepd.yaml
The name
of the
volume
(also
referenced
when
mounting
the volume)
The type of the volume 
is a GCE Persistent Disk.
The name of the persistent 
disk must match the actual 
PD you created earlier.
The filesystem type is EXT4 
(a type of Linux filesystem).
The path where MongoDB 
stores its data
Pod: mongodb
Container: mongodb
volumeMounts:
name: mongodb-data
mountPath: /data/db
gcePersistentDisk:
pdName: mongodb
GCE
Persistent Disk:
mongodb
Volume:
mongodb
Figure 6.5
A pod with a single container running MongoDB, which mounts a volume referencing an 
external GCE Persistent Disk
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="205">
  <data key="d0">Page_205</data>
  <data key="d5">Page_205</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_116">
  <data key="d0">173
Using persistent storage
WRITING DATA TO THE PERSISTENT STORAGE BY ADDING DOCUMENTS TO YOUR MONGODB DATABASE
Now that you’ve created the pod and the container has been started, you can run the
MongoDB shell inside the container and use it to write some data to the data store.
 You’ll run the shell as shown in the following listing.
$ kubectl exec -it mongodb mongo
MongoDB shell version: 3.2.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
For interactive help, type "help".
For more comprehensive documentation, see
    http://docs.mongodb.org/
Questions? Try the support group
    http://groups.google.com/group/mongodb-user
...
&gt; 
MongoDB allows storing JSON documents, so you’ll store one to see if it’s stored per-
sistently and can be retrieved after the pod is re-created. Insert a new JSON document
with the following commands: 
&gt; use mystore
switched to db mystore
&gt; db.foo.insert({name:'foo'})
WriteResult({ "nInserted" : 1 })
You’ve inserted a simple JSON document with a single property (name: ’foo’). Now,
use the find() command to see the document you inserted:
&gt; db.foo.find()
{ "_id" : ObjectId("57a61eb9de0cfd512374cc75"), "name" : "foo" }
There it is. The document should be stored in your GCE persistent disk now. 
RE-CREATING THE POD AND VERIFYING THAT IT CAN READ THE DATA PERSISTED BY THE PREVIOUS POD
You can now exit the mongodb shell (type exit and press Enter), and then delete the
pod and recreate it:
$ kubectl delete pod mongodb
pod "mongodb" deleted
$ kubectl create -f mongodb-pod-gcepd.yaml
pod "mongodb" created
The new pod uses the exact same GCE persistent disk as the previous pod, so the
MongoDB container running inside it should see the exact same data, even if the pod
is scheduled to a different node.
TIP
You can see what node a pod is scheduled to by running kubectl get po
-o wide.
Listing 6.5
Entering the MongoDB shell inside the mongodb pod
 
</data>
  <data key="d5">173
Using persistent storage
WRITING DATA TO THE PERSISTENT STORAGE BY ADDING DOCUMENTS TO YOUR MONGODB DATABASE
Now that you’ve created the pod and the container has been started, you can run the
MongoDB shell inside the container and use it to write some data to the data store.
 You’ll run the shell as shown in the following listing.
$ kubectl exec -it mongodb mongo
MongoDB shell version: 3.2.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
For interactive help, type "help".
For more comprehensive documentation, see
    http://docs.mongodb.org/
Questions? Try the support group
    http://groups.google.com/group/mongodb-user
...
&gt; 
MongoDB allows storing JSON documents, so you’ll store one to see if it’s stored per-
sistently and can be retrieved after the pod is re-created. Insert a new JSON document
with the following commands: 
&gt; use mystore
switched to db mystore
&gt; db.foo.insert({name:'foo'})
WriteResult({ "nInserted" : 1 })
You’ve inserted a simple JSON document with a single property (name: ’foo’). Now,
use the find() command to see the document you inserted:
&gt; db.foo.find()
{ "_id" : ObjectId("57a61eb9de0cfd512374cc75"), "name" : "foo" }
There it is. The document should be stored in your GCE persistent disk now. 
RE-CREATING THE POD AND VERIFYING THAT IT CAN READ THE DATA PERSISTED BY THE PREVIOUS POD
You can now exit the mongodb shell (type exit and press Enter), and then delete the
pod and recreate it:
$ kubectl delete pod mongodb
pod "mongodb" deleted
$ kubectl create -f mongodb-pod-gcepd.yaml
pod "mongodb" created
The new pod uses the exact same GCE persistent disk as the previous pod, so the
MongoDB container running inside it should see the exact same data, even if the pod
is scheduled to a different node.
TIP
You can see what node a pod is scheduled to by running kubectl get po
-o wide.
Listing 6.5
Entering the MongoDB shell inside the mongodb pod
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="206">
  <data key="d0">Page_206</data>
  <data key="d5">Page_206</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_117">
  <data key="d0">174
CHAPTER 6
Volumes: attaching disk storage to containers
Once the container is up, you can again run the MongoDB shell and check to see if the
document you stored earlier can still be retrieved, as shown in the following listing.
$ kubectl exec -it mongodb mongo
MongoDB shell version: 3.2.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
...
&gt; use mystore
switched to db mystore
&gt; db.foo.find()
{ "_id" : ObjectId("57a61eb9de0cfd512374cc75"), "name" : "foo" }
As expected, the data is still there, even though you deleted the pod and re-created it.
This confirms you can use a GCE persistent disk to persist data across multiple pod
instances. 
 You’re done playing with the MongoDB pod, so go ahead and delete it again, but
hold off on deleting the underlying GCE persistent disk. You’ll use it again later in
the chapter.
6.4.2
Using other types of volumes with underlying persistent storage
The reason you created the GCE Persistent Disk volume is because your Kubernetes
cluster runs on Google Kubernetes Engine. When you run your cluster elsewhere, you
should use other types of volumes, depending on the underlying infrastructure.
 If your Kubernetes cluster is running on Amazon’s AWS EC2, for example, you can
use an awsElasticBlockStore volume to provide persistent storage for your pods. If
your cluster runs on Microsoft Azure, you can use the azureFile or the azureDisk
volume. We won’t go into detail on how to do that here, but it’s virtually the same as in
the previous example. First, you need to create the actual underlying storage, and
then set the appropriate properties in the volume definition.
USING AN AWS ELASTIC BLOCK STORE VOLUME
For example, to use an AWS elastic block store instead of the GCE Persistent Disk,
you’d only need to change the volume definition as shown in the following listing (see
those lines printed in bold).
apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  volumes:                       
  - name: mongodb-data           
    awsElasticBlockStore:          
Listing 6.6
Retrieving MongoDB’s persisted data in a new pod
Listing 6.7
A pod using an awsElasticBlockStore volume: mongodb-pod-aws.yaml
Using awsElasticBlockStore 
instead of gcePersistentDisk
 
</data>
  <data key="d5">174
CHAPTER 6
Volumes: attaching disk storage to containers
Once the container is up, you can again run the MongoDB shell and check to see if the
document you stored earlier can still be retrieved, as shown in the following listing.
$ kubectl exec -it mongodb mongo
MongoDB shell version: 3.2.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
...
&gt; use mystore
switched to db mystore
&gt; db.foo.find()
{ "_id" : ObjectId("57a61eb9de0cfd512374cc75"), "name" : "foo" }
As expected, the data is still there, even though you deleted the pod and re-created it.
This confirms you can use a GCE persistent disk to persist data across multiple pod
instances. 
 You’re done playing with the MongoDB pod, so go ahead and delete it again, but
hold off on deleting the underlying GCE persistent disk. You’ll use it again later in
the chapter.
6.4.2
Using other types of volumes with underlying persistent storage
The reason you created the GCE Persistent Disk volume is because your Kubernetes
cluster runs on Google Kubernetes Engine. When you run your cluster elsewhere, you
should use other types of volumes, depending on the underlying infrastructure.
 If your Kubernetes cluster is running on Amazon’s AWS EC2, for example, you can
use an awsElasticBlockStore volume to provide persistent storage for your pods. If
your cluster runs on Microsoft Azure, you can use the azureFile or the azureDisk
volume. We won’t go into detail on how to do that here, but it’s virtually the same as in
the previous example. First, you need to create the actual underlying storage, and
then set the appropriate properties in the volume definition.
USING AN AWS ELASTIC BLOCK STORE VOLUME
For example, to use an AWS elastic block store instead of the GCE Persistent Disk,
you’d only need to change the volume definition as shown in the following listing (see
those lines printed in bold).
apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  volumes:                       
  - name: mongodb-data           
    awsElasticBlockStore:          
Listing 6.6
Retrieving MongoDB’s persisted data in a new pod
Listing 6.7
A pod using an awsElasticBlockStore volume: mongodb-pod-aws.yaml
Using awsElasticBlockStore 
instead of gcePersistentDisk
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="207">
  <data key="d0">Page_207</data>
  <data key="d5">Page_207</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_118">
  <data key="d0">175
Using persistent storage
      volumeId: my-volume          
      fsType: ext4       
  containers:
  - ...
USING AN NFS VOLUME
If your cluster is running on your own set of servers, you have a vast array of other sup-
ported options for mounting external storage inside your volume. For example, to
mount a simple NFS share, you only need to specify the NFS server and the path
exported by the server, as shown in the following listing.
  volumes:                       
  - name: mongodb-data           
    nfs:                     
      server: 1.2.3.4         
      path: /some/path     
USING OTHER STORAGE TECHNOLOGIES
Other supported options include iscsi for mounting an ISCSI disk resource, glusterfs
for a GlusterFS mount, rbd for a RADOS Block Device, flexVolume, cinder, cephfs,
flocker, fc (Fibre Channel), and others. You don’t need to know all of them if you’re
not using them. They’re mentioned here to show you that Kubernetes supports a
broad range of storage technologies and you can use whichever you prefer and are
used to.
 To see details on what properties you need to set for each of these volume types,
you can either turn to the Kubernetes API definitions in the Kubernetes API refer-
ence or look up the information through kubectl explain, as shown in chapter 3. If
you’re already familiar with a particular storage technology, using the explain com-
mand should allow you to easily figure out how to mount a volume of the proper type
and use it in your pods.
 But does a developer need to know all this stuff? Should a developer, when creat-
ing a pod, have to deal with infrastructure-related storage details, or should that be
left to the cluster administrator? 
 Having a pod’s volumes refer to the actual underlying infrastructure isn’t what
Kubernetes is about, is it? For example, for a developer to have to specify the host-
name of the NFS server feels wrong. And that’s not even the worst thing about it. 
 Including this type of infrastructure-related information into a pod definition
means the pod definition is pretty much tied to a specific Kubernetes cluster. You
can’t use the same pod definition in another one. That’s why using volumes like this
isn’t the best way to attach persistent storage to your pods. You’ll learn how to improve
on this in the next section.
Listing 6.8
A pod using an nfs volume: mongodb-pod-nfs.yaml
Specify the ID of the EBS 
volume you created.
The filesystem type 
is EXT4 as before.
This volume is backed 
by an NFS share.
The IP of the 
NFS server
The path exported 
by the server
 
</data>
  <data key="d5">175
Using persistent storage
      volumeId: my-volume          
      fsType: ext4       
  containers:
  - ...
USING AN NFS VOLUME
If your cluster is running on your own set of servers, you have a vast array of other sup-
ported options for mounting external storage inside your volume. For example, to
mount a simple NFS share, you only need to specify the NFS server and the path
exported by the server, as shown in the following listing.
  volumes:                       
  - name: mongodb-data           
    nfs:                     
      server: 1.2.3.4         
      path: /some/path     
USING OTHER STORAGE TECHNOLOGIES
Other supported options include iscsi for mounting an ISCSI disk resource, glusterfs
for a GlusterFS mount, rbd for a RADOS Block Device, flexVolume, cinder, cephfs,
flocker, fc (Fibre Channel), and others. You don’t need to know all of them if you’re
not using them. They’re mentioned here to show you that Kubernetes supports a
broad range of storage technologies and you can use whichever you prefer and are
used to.
 To see details on what properties you need to set for each of these volume types,
you can either turn to the Kubernetes API definitions in the Kubernetes API refer-
ence or look up the information through kubectl explain, as shown in chapter 3. If
you’re already familiar with a particular storage technology, using the explain com-
mand should allow you to easily figure out how to mount a volume of the proper type
and use it in your pods.
 But does a developer need to know all this stuff? Should a developer, when creat-
ing a pod, have to deal with infrastructure-related storage details, or should that be
left to the cluster administrator? 
 Having a pod’s volumes refer to the actual underlying infrastructure isn’t what
Kubernetes is about, is it? For example, for a developer to have to specify the host-
name of the NFS server feels wrong. And that’s not even the worst thing about it. 
 Including this type of infrastructure-related information into a pod definition
means the pod definition is pretty much tied to a specific Kubernetes cluster. You
can’t use the same pod definition in another one. That’s why using volumes like this
isn’t the best way to attach persistent storage to your pods. You’ll learn how to improve
on this in the next section.
Listing 6.8
A pod using an nfs volume: mongodb-pod-nfs.yaml
Specify the ID of the EBS 
volume you created.
The filesystem type 
is EXT4 as before.
This volume is backed 
by an NFS share.
The IP of the 
NFS server
The path exported 
by the server
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="208">
  <data key="d0">Page_208</data>
  <data key="d5">Page_208</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_119">
  <data key="d0">176
CHAPTER 6
Volumes: attaching disk storage to containers
6.5
Decoupling pods from the underlying storage technology
All the persistent volume types we’ve explored so far have required the developer of the
pod to have knowledge of the actual network storage infrastructure available in the clus-
ter. For example, to create a NFS-backed volume, the developer has to know the actual
server the NFS export is located on. This is against the basic idea of Kubernetes, which
aims to hide the actual infrastructure from both the application and its developer, leav-
ing them free from worrying about the specifics of the infrastructure and making apps
portable across a wide array of cloud providers and on-premises datacenters.
 Ideally, a developer deploying their apps on Kubernetes should never have to
know what kind of storage technology is used underneath, the same way they don’t
have to know what type of physical servers are being used to run their pods. Infrastruc-
ture-related dealings should be the sole domain of the cluster administrator.
 When a developer needs a certain amount of persistent storage for their applica-
tion, they can request it from Kubernetes, the same way they can request CPU, mem-
ory, and other resources when creating a pod. The system administrator can configure
the cluster so it can give the apps what they request.
6.5.1
Introducing PersistentVolumes and PersistentVolumeClaims
To enable apps to request storage in a Kubernetes cluster without having to deal with
infrastructure specifics, two new resources were introduced. They are Persistent-
Volumes and PersistentVolumeClaims. The names may be a bit misleading, because as
you’ve seen in the previous few sections, even regular Kubernetes volumes can be
used to store persistent data. 
 Using a PersistentVolume inside a pod is a little more complex than using a regular
pod volume, so let’s illustrate how pods, PersistentVolumeClaims, PersistentVolumes,
and the actual underlying storage relate to each other in figure 6.6.
Pod
Admin
Volume
1. Cluster admin sets up some type of
network storage (NFS export or similar)
2. Admin then creates a PersistentVolume (PV)
by posting a PV descriptor to the Kubernetes API
NFS
export
Persistent
Volume
User
Persistent
VolumeClaim
3. User creates a
PersistentVolumeClaim (PVC)
4. Kubernetes ﬁnds a PV of
adequate size and access
mode and binds the PVC
to the PV
5. User creates a
pod with a volume
referencing the PVC
Figure 6.6
PersistentVolumes are provisioned by cluster admins and consumed by pods 
through PersistentVolumeClaims.
 
</data>
  <data key="d5">176
CHAPTER 6
Volumes: attaching disk storage to containers
6.5
Decoupling pods from the underlying storage technology
All the persistent volume types we’ve explored so far have required the developer of the
pod to have knowledge of the actual network storage infrastructure available in the clus-
ter. For example, to create a NFS-backed volume, the developer has to know the actual
server the NFS export is located on. This is against the basic idea of Kubernetes, which
aims to hide the actual infrastructure from both the application and its developer, leav-
ing them free from worrying about the specifics of the infrastructure and making apps
portable across a wide array of cloud providers and on-premises datacenters.
 Ideally, a developer deploying their apps on Kubernetes should never have to
know what kind of storage technology is used underneath, the same way they don’t
have to know what type of physical servers are being used to run their pods. Infrastruc-
ture-related dealings should be the sole domain of the cluster administrator.
 When a developer needs a certain amount of persistent storage for their applica-
tion, they can request it from Kubernetes, the same way they can request CPU, mem-
ory, and other resources when creating a pod. The system administrator can configure
the cluster so it can give the apps what they request.
6.5.1
Introducing PersistentVolumes and PersistentVolumeClaims
To enable apps to request storage in a Kubernetes cluster without having to deal with
infrastructure specifics, two new resources were introduced. They are Persistent-
Volumes and PersistentVolumeClaims. The names may be a bit misleading, because as
you’ve seen in the previous few sections, even regular Kubernetes volumes can be
used to store persistent data. 
 Using a PersistentVolume inside a pod is a little more complex than using a regular
pod volume, so let’s illustrate how pods, PersistentVolumeClaims, PersistentVolumes,
and the actual underlying storage relate to each other in figure 6.6.
Pod
Admin
Volume
1. Cluster admin sets up some type of
network storage (NFS export or similar)
2. Admin then creates a PersistentVolume (PV)
by posting a PV descriptor to the Kubernetes API
NFS
export
Persistent
Volume
User
Persistent
VolumeClaim
3. User creates a
PersistentVolumeClaim (PVC)
4. Kubernetes ﬁnds a PV of
adequate size and access
mode and binds the PVC
to the PV
5. User creates a
pod with a volume
referencing the PVC
Figure 6.6
PersistentVolumes are provisioned by cluster admins and consumed by pods 
through PersistentVolumeClaims.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="209">
  <data key="d0">Page_209</data>
  <data key="d5">Page_209</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_120">
  <data key="d0">177
Decoupling pods from the underlying storage technology
Instead of the developer adding a technology-specific volume to their pod, it’s the
cluster administrator who sets up the underlying storage and then registers it in
Kubernetes by creating a PersistentVolume resource through the Kubernetes API
server. When creating the PersistentVolume, the admin specifies its size and the access
modes it supports. 
 When a cluster user needs to use persistent storage in one of their pods, they first
create a PersistentVolumeClaim manifest, specifying the minimum size and the access
mode they require. The user then submits the PersistentVolumeClaim manifest to the
Kubernetes API server, and Kubernetes finds the appropriate PersistentVolume and
binds the volume to the claim. 
 The PersistentVolumeClaim can then be used as one of the volumes inside a pod.
Other users cannot use the same PersistentVolume until it has been released by delet-
ing the bound PersistentVolumeClaim.
6.5.2
Creating a PersistentVolume
Let’s revisit the MongoDB example, but unlike before, you won’t reference the GCE
Persistent Disk in the pod directly. Instead, you’ll first assume the role of a cluster
administrator and create a PersistentVolume backed by the GCE Persistent Disk. Then
you’ll assume the role of the application developer and first claim the PersistentVol-
ume and then use it inside your pod.
 In section 6.4.1 you set up the physical storage by provisioning the GCE Persistent
Disk, so you don’t need to do that again. All you need to do is create the Persistent-
Volume resource in Kubernetes by preparing the manifest shown in the following list-
ing and posting it to the API server.
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity:                  
    storage: 1Gi             
  accessModes:                              
  - ReadWriteOnce                           
  - ReadOnlyMany                            
  persistentVolumeReclaimPolicy: Retain    
  gcePersistentDisk:                      
    pdName: mongodb                       
    fsType: ext4                          
Listing 6.9
A gcePersistentDisk PersistentVolume: mongodb-pv-gcepd.yaml
Defining the 
PersistentVolume’s size
It can either be mounted by a single 
client for reading and writing or by 
multiple clients for reading only.
After the claim is released, 
the PersistentVolume 
should be retained (not 
erased or deleted).
The PersistentVolume is 
backed by the GCE Persistent 
Disk you created earlier.
 
</data>
  <data key="d5">177
Decoupling pods from the underlying storage technology
Instead of the developer adding a technology-specific volume to their pod, it’s the
cluster administrator who sets up the underlying storage and then registers it in
Kubernetes by creating a PersistentVolume resource through the Kubernetes API
server. When creating the PersistentVolume, the admin specifies its size and the access
modes it supports. 
 When a cluster user needs to use persistent storage in one of their pods, they first
create a PersistentVolumeClaim manifest, specifying the minimum size and the access
mode they require. The user then submits the PersistentVolumeClaim manifest to the
Kubernetes API server, and Kubernetes finds the appropriate PersistentVolume and
binds the volume to the claim. 
 The PersistentVolumeClaim can then be used as one of the volumes inside a pod.
Other users cannot use the same PersistentVolume until it has been released by delet-
ing the bound PersistentVolumeClaim.
6.5.2
Creating a PersistentVolume
Let’s revisit the MongoDB example, but unlike before, you won’t reference the GCE
Persistent Disk in the pod directly. Instead, you’ll first assume the role of a cluster
administrator and create a PersistentVolume backed by the GCE Persistent Disk. Then
you’ll assume the role of the application developer and first claim the PersistentVol-
ume and then use it inside your pod.
 In section 6.4.1 you set up the physical storage by provisioning the GCE Persistent
Disk, so you don’t need to do that again. All you need to do is create the Persistent-
Volume resource in Kubernetes by preparing the manifest shown in the following list-
ing and posting it to the API server.
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity:                  
    storage: 1Gi             
  accessModes:                              
  - ReadWriteOnce                           
  - ReadOnlyMany                            
  persistentVolumeReclaimPolicy: Retain    
  gcePersistentDisk:                      
    pdName: mongodb                       
    fsType: ext4                          
Listing 6.9
A gcePersistentDisk PersistentVolume: mongodb-pv-gcepd.yaml
Defining the 
PersistentVolume’s size
It can either be mounted by a single 
client for reading and writing or by 
multiple clients for reading only.
After the claim is released, 
the PersistentVolume 
should be retained (not 
erased or deleted).
The PersistentVolume is 
backed by the GCE Persistent 
Disk you created earlier.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="210">
  <data key="d0">Page_210</data>
  <data key="d5">Page_210</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_121">
  <data key="d0">178
CHAPTER 6
Volumes: attaching disk storage to containers
NOTE
If you’re using Minikube, create the PV using the mongodb-pv-host-
path.yaml file.
When creating a PersistentVolume, the administrator needs to tell Kubernetes what its
capacity is and whether it can be read from and/or written to by a single node or by
multiple nodes at the same time. They also need to tell Kubernetes what to do with the
PersistentVolume when it’s released (when the PersistentVolumeClaim it’s bound to is
deleted). And last, but certainly not least, they need to specify the type, location, and
other properties of the actual storage this PersistentVolume is backed by. If you look
closely, this last part is exactly the same as earlier, when you referenced the GCE Per-
sistent Disk in the pod volume directly (shown again in the following listing).
spec:
  volumes:                       
  - name: mongodb-data           
    gcePersistentDisk:           
      pdName: mongodb            
      fsType: ext4               
  ...
After you create the PersistentVolume with the kubectl create command, it should
be ready to be claimed. See if it is by listing all PersistentVolumes:
$ kubectl get pv
NAME         CAPACITY   RECLAIMPOLICY   ACCESSMODES   STATUS      CLAIM
mongodb-pv   1Gi        Retain          RWO,ROX       Available   
NOTE
Several columns are omitted. Also, pv is used as a shorthand for
persistentvolume.
As expected, the PersistentVolume is shown as Available, because you haven’t yet cre-
ated the PersistentVolumeClaim. 
NOTE
PersistentVolumes don’t belong to any namespace (see figure 6.7).
They’re cluster-level resources like nodes.
Listing 6.10
Referencing a GCE PD in a pod’s volume
 
</data>
  <data key="d5">178
CHAPTER 6
Volumes: attaching disk storage to containers
NOTE
If you’re using Minikube, create the PV using the mongodb-pv-host-
path.yaml file.
When creating a PersistentVolume, the administrator needs to tell Kubernetes what its
capacity is and whether it can be read from and/or written to by a single node or by
multiple nodes at the same time. They also need to tell Kubernetes what to do with the
PersistentVolume when it’s released (when the PersistentVolumeClaim it’s bound to is
deleted). And last, but certainly not least, they need to specify the type, location, and
other properties of the actual storage this PersistentVolume is backed by. If you look
closely, this last part is exactly the same as earlier, when you referenced the GCE Per-
sistent Disk in the pod volume directly (shown again in the following listing).
spec:
  volumes:                       
  - name: mongodb-data           
    gcePersistentDisk:           
      pdName: mongodb            
      fsType: ext4               
  ...
After you create the PersistentVolume with the kubectl create command, it should
be ready to be claimed. See if it is by listing all PersistentVolumes:
$ kubectl get pv
NAME         CAPACITY   RECLAIMPOLICY   ACCESSMODES   STATUS      CLAIM
mongodb-pv   1Gi        Retain          RWO,ROX       Available   
NOTE
Several columns are omitted. Also, pv is used as a shorthand for
persistentvolume.
As expected, the PersistentVolume is shown as Available, because you haven’t yet cre-
ated the PersistentVolumeClaim. 
NOTE
PersistentVolumes don’t belong to any namespace (see figure 6.7).
They’re cluster-level resources like nodes.
Listing 6.10
Referencing a GCE PD in a pod’s volume
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="211">
  <data key="d0">Page_211</data>
  <data key="d5">Page_211</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_122">
  <data key="d0">179
Decoupling pods from the underlying storage technology
6.5.3
Claiming a PersistentVolume by creating a 
PersistentVolumeClaim
Now let’s lay down our admin hats and put our developer hats back on. Say you need
to deploy a pod that requires persistent storage. You’ll use the PersistentVolume you
created earlier. But you can’t use it directly in the pod. You need to claim it first.
 Claiming a PersistentVolume is a completely separate process from creating a pod,
because you want the same PersistentVolumeClaim to stay available even if the pod is
rescheduled (remember, rescheduling means the previous pod is deleted and a new
one is created). 
CREATING A PERSISTENTVOLUMECLAIM
You’ll create the claim now. You need to prepare a PersistentVolumeClaim manifest
like the one shown in the following listing and post it to the Kubernetes API through
kubectl create.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc          
Listing 6.11
A PersistentVolumeClaim: mongodb-pvc.yaml
Pod(s)
Pod(s)
Persistent
Volume
Persistent
Volume
Persistent
Volume
Persistent
Volume
...
User A
Persistent
Volume
Claim(s)
Persistent
Volume
Claim(s)
Namespace A
User B
Namespace B
Node
Node
Node
Node
Node
Node
Persistent
Volume
Figure 6.7
PersistentVolumes, like cluster Nodes, don’t belong to any namespace, unlike pods and 
PersistentVolumeClaims.
The name of your claim—you’ll 
need this later when using the 
claim as the pod’s volume.
 
</data>
  <data key="d5">179
Decoupling pods from the underlying storage technology
6.5.3
Claiming a PersistentVolume by creating a 
PersistentVolumeClaim
Now let’s lay down our admin hats and put our developer hats back on. Say you need
to deploy a pod that requires persistent storage. You’ll use the PersistentVolume you
created earlier. But you can’t use it directly in the pod. You need to claim it first.
 Claiming a PersistentVolume is a completely separate process from creating a pod,
because you want the same PersistentVolumeClaim to stay available even if the pod is
rescheduled (remember, rescheduling means the previous pod is deleted and a new
one is created). 
CREATING A PERSISTENTVOLUMECLAIM
You’ll create the claim now. You need to prepare a PersistentVolumeClaim manifest
like the one shown in the following listing and post it to the Kubernetes API through
kubectl create.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc          
Listing 6.11
A PersistentVolumeClaim: mongodb-pvc.yaml
Pod(s)
Pod(s)
Persistent
Volume
Persistent
Volume
Persistent
Volume
Persistent
Volume
...
User A
Persistent
Volume
Claim(s)
Persistent
Volume
Claim(s)
Namespace A
User B
Namespace B
Node
Node
Node
Node
Node
Node
Persistent
Volume
Figure 6.7
PersistentVolumes, like cluster Nodes, don’t belong to any namespace, unlike pods and 
PersistentVolumeClaims.
The name of your claim—you’ll 
need this later when using the 
claim as the pod’s volume.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="212">
  <data key="d0">Page_212</data>
  <data key="d5">Page_212</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_123">
  <data key="d0">180
CHAPTER 6
Volumes: attaching disk storage to containers
spec:
  resources:
    requests:                
      storage: 1Gi           
  accessModes:              
  - ReadWriteOnce           
  storageClassName: ""     
As soon as you create the claim, Kubernetes finds the appropriate PersistentVolume
and binds it to the claim. The PersistentVolume’s capacity must be large enough to
accommodate what the claim requests. Additionally, the volume’s access modes must
include the access modes requested by the claim. In your case, the claim requests 1 GiB
of storage and a ReadWriteOnce access mode. The PersistentVolume you created ear-
lier matches those two requirements so it is bound to your claim. You can see this by
inspecting the claim.
LISTING PERSISTENTVOLUMECLAIMS
List all PersistentVolumeClaims to see the state of your PVC:
$ kubectl get pvc
NAME          STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE
mongodb-pvc   Bound     mongodb-pv   1Gi        RWO,ROX       3s
NOTE
We’re using pvc as a shorthand for persistentvolumeclaim.
The claim is shown as Bound to PersistentVolume mongodb-pv. Note the abbreviations
used for the access modes:

RWO—ReadWriteOnce—Only a single node can mount the volume for reading
and writing.

ROX—ReadOnlyMany—Multiple nodes can mount the volume for reading.

RWX—ReadWriteMany—Multiple nodes can mount the volume for both reading
and writing.
NOTE
RWO, ROX, and RWX pertain to the number of worker nodes that can use
the volume at the same time, not to the number of pods!
LISTING PERSISTENTVOLUMES
You can also see that the PersistentVolume is now Bound and no longer Available by
inspecting it with kubectl get:
$ kubectl get pv
NAME         CAPACITY   ACCESSMODES   STATUS   CLAIM                 AGE
mongodb-pv   1Gi        RWO,ROX       Bound    default/mongodb-pvc   1m
The PersistentVolume shows it’s bound to claim default/mongodb-pvc. The default
part is the namespace the claim resides in (you created the claim in the default
Requesting 1 GiB of storage
You want the storage to support a single 
client (performing both reads and writes).
You’ll learn about this in the section 
about dynamic provisioning.
 
</data>
  <data key="d5">180
CHAPTER 6
Volumes: attaching disk storage to containers
spec:
  resources:
    requests:                
      storage: 1Gi           
  accessModes:              
  - ReadWriteOnce           
  storageClassName: ""     
As soon as you create the claim, Kubernetes finds the appropriate PersistentVolume
and binds it to the claim. The PersistentVolume’s capacity must be large enough to
accommodate what the claim requests. Additionally, the volume’s access modes must
include the access modes requested by the claim. In your case, the claim requests 1 GiB
of storage and a ReadWriteOnce access mode. The PersistentVolume you created ear-
lier matches those two requirements so it is bound to your claim. You can see this by
inspecting the claim.
LISTING PERSISTENTVOLUMECLAIMS
List all PersistentVolumeClaims to see the state of your PVC:
$ kubectl get pvc
NAME          STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE
mongodb-pvc   Bound     mongodb-pv   1Gi        RWO,ROX       3s
NOTE
We’re using pvc as a shorthand for persistentvolumeclaim.
The claim is shown as Bound to PersistentVolume mongodb-pv. Note the abbreviations
used for the access modes:

RWO—ReadWriteOnce—Only a single node can mount the volume for reading
and writing.

ROX—ReadOnlyMany—Multiple nodes can mount the volume for reading.

RWX—ReadWriteMany—Multiple nodes can mount the volume for both reading
and writing.
NOTE
RWO, ROX, and RWX pertain to the number of worker nodes that can use
the volume at the same time, not to the number of pods!
LISTING PERSISTENTVOLUMES
You can also see that the PersistentVolume is now Bound and no longer Available by
inspecting it with kubectl get:
$ kubectl get pv
NAME         CAPACITY   ACCESSMODES   STATUS   CLAIM                 AGE
mongodb-pv   1Gi        RWO,ROX       Bound    default/mongodb-pvc   1m
The PersistentVolume shows it’s bound to claim default/mongodb-pvc. The default
part is the namespace the claim resides in (you created the claim in the default
Requesting 1 GiB of storage
You want the storage to support a single 
client (performing both reads and writes).
You’ll learn about this in the section 
about dynamic provisioning.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="213">
  <data key="d0">Page_213</data>
  <data key="d5">Page_213</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_124">
  <data key="d0">181
Decoupling pods from the underlying storage technology
namespace). We’ve already said that PersistentVolume resources are cluster-scoped
and thus cannot be created in a specific namespace, but PersistentVolumeClaims can
only be created in a specific namespace. They can then only be used by pods in the
same namespace.
6.5.4
Using a PersistentVolumeClaim in a pod
The PersistentVolume is now yours to use. Nobody else can claim the same volume
until you release it. To use it inside a pod, you need to reference the Persistent-
VolumeClaim by name inside the pod’s volume (yes, the PersistentVolumeClaim, not
the PersistentVolume directly!), as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
  volumes:
  - name: mongodb-data
    persistentVolumeClaim:       
      claimName: mongodb-pvc     
Go ahead and create the pod. Now, check to see if the pod is indeed using the same
PersistentVolume and its underlying GCE PD. You should see the data you stored ear-
lier by running the MongoDB shell again, as shown in the following listing.
$ kubectl exec -it mongodb mongo
MongoDB shell version: 3.2.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
...
&gt; use mystore
switched to db mystore
&gt; db.foo.find()
{ "_id" : ObjectId("57a61eb9de0cfd512374cc75"), "name" : "foo" }
And there it is. You‘re able to retrieve the document you stored into MongoDB
previously.
Listing 6.12
A pod using a PersistentVolumeClaim volume: mongodb-pod-pvc.yaml
Listing 6.13
Retrieving MongoDB’s persisted data in the pod using the PVC and PV
Referencing the PersistentVolumeClaim 
by name in the pod volume
 
</data>
  <data key="d5">181
Decoupling pods from the underlying storage technology
namespace). We’ve already said that PersistentVolume resources are cluster-scoped
and thus cannot be created in a specific namespace, but PersistentVolumeClaims can
only be created in a specific namespace. They can then only be used by pods in the
same namespace.
6.5.4
Using a PersistentVolumeClaim in a pod
The PersistentVolume is now yours to use. Nobody else can claim the same volume
until you release it. To use it inside a pod, you need to reference the Persistent-
VolumeClaim by name inside the pod’s volume (yes, the PersistentVolumeClaim, not
the PersistentVolume directly!), as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
  volumes:
  - name: mongodb-data
    persistentVolumeClaim:       
      claimName: mongodb-pvc     
Go ahead and create the pod. Now, check to see if the pod is indeed using the same
PersistentVolume and its underlying GCE PD. You should see the data you stored ear-
lier by running the MongoDB shell again, as shown in the following listing.
$ kubectl exec -it mongodb mongo
MongoDB shell version: 3.2.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
...
&gt; use mystore
switched to db mystore
&gt; db.foo.find()
{ "_id" : ObjectId("57a61eb9de0cfd512374cc75"), "name" : "foo" }
And there it is. You‘re able to retrieve the document you stored into MongoDB
previously.
Listing 6.12
A pod using a PersistentVolumeClaim volume: mongodb-pod-pvc.yaml
Listing 6.13
Retrieving MongoDB’s persisted data in the pod using the PVC and PV
Referencing the PersistentVolumeClaim 
by name in the pod volume
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="214">
  <data key="d0">Page_214</data>
  <data key="d5">Page_214</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_125">
  <data key="d0">182
CHAPTER 6
Volumes: attaching disk storage to containers
6.5.5
Understanding the benefits of using PersistentVolumes and claims
Examine figure 6.8, which shows both ways a pod can use a GCE Persistent Disk—
directly or through a PersistentVolume and claim.
Consider how using this indirect method of obtaining storage from the infrastructure
is much simpler for the application developer (or cluster user). Yes, it does require
the additional steps of creating the PersistentVolume and the PersistentVolumeClaim,
but the developer doesn’t have to know anything about the actual storage technology
used underneath. 
 Additionally, the same pod and claim manifests can now be used on many different
Kubernetes clusters, because they don’t refer to anything infrastructure-specific. The
claim states, “I need x amount of storage and I need to be able to read and write to it
by a single client at once,” and then the pod references the claim by name in one of
its volumes.
Pod: mongodb
Container: mongodb
volumeMounts:
name: mongodb-data
mountPath: /data/db
gcePersistentDisk:
pdName: mongodb
GCE
Persistent Disk:
mongodb
Volume:
mongodb
Pod: mongodb
Container: mongodb
volumeMounts:
name: mongodb-data
mountPath: /data/db
persistentVolumeClaim:
claimName: mongodb-pvc
gcePersistentDisk:
pdName: mongodb
GCE
Persistent Disk:
mongodb
PersistentVolume:
mongodb-pv
(1 Gi, RWO, RWX)
Volume:
mongodb
Claim lists
1Gi and
ReadWriteOnce
access
PersistentVolumeClaim:
mongodb-pvc
Figure 6.8
Using the GCE Persistent Disk directly or through a PVC and PV
 
</data>
  <data key="d5">182
CHAPTER 6
Volumes: attaching disk storage to containers
6.5.5
Understanding the benefits of using PersistentVolumes and claims
Examine figure 6.8, which shows both ways a pod can use a GCE Persistent Disk—
directly or through a PersistentVolume and claim.
Consider how using this indirect method of obtaining storage from the infrastructure
is much simpler for the application developer (or cluster user). Yes, it does require
the additional steps of creating the PersistentVolume and the PersistentVolumeClaim,
but the developer doesn’t have to know anything about the actual storage technology
used underneath. 
 Additionally, the same pod and claim manifests can now be used on many different
Kubernetes clusters, because they don’t refer to anything infrastructure-specific. The
claim states, “I need x amount of storage and I need to be able to read and write to it
by a single client at once,” and then the pod references the claim by name in one of
its volumes.
Pod: mongodb
Container: mongodb
volumeMounts:
name: mongodb-data
mountPath: /data/db
gcePersistentDisk:
pdName: mongodb
GCE
Persistent Disk:
mongodb
Volume:
mongodb
Pod: mongodb
Container: mongodb
volumeMounts:
name: mongodb-data
mountPath: /data/db
persistentVolumeClaim:
claimName: mongodb-pvc
gcePersistentDisk:
pdName: mongodb
GCE
Persistent Disk:
mongodb
PersistentVolume:
mongodb-pv
(1 Gi, RWO, RWX)
Volume:
mongodb
Claim lists
1Gi and
ReadWriteOnce
access
PersistentVolumeClaim:
mongodb-pvc
Figure 6.8
Using the GCE Persistent Disk directly or through a PVC and PV
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="215">
  <data key="d0">Page_215</data>
  <data key="d5">Page_215</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_126">
  <data key="d0">183
Decoupling pods from the underlying storage technology
6.5.6
Recycling PersistentVolumes
Before you wrap up this section on PersistentVolumes, let’s do one last quick experi-
ment. Delete the pod and the PersistentVolumeClaim:
$ kubectl delete pod mongodb
pod "mongodb" deleted
$ kubectl delete pvc mongodb-pvc
persistentvolumeclaim "mongodb-pvc" deleted
What if you create the PersistentVolumeClaim again? Will it be bound to the Persistent-
Volume or not? After you create the claim, what does kubectl get pvc show?
$ kubectl get pvc
NAME           STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE
mongodb-pvc    Pending                                         13s
The claim’s status is shown as Pending. Interesting. When you created the claim ear-
lier, it was immediately bound to the PersistentVolume, so why wasn’t it bound now?
Maybe listing the PersistentVolumes can shed more light on this:
$ kubectl get pv
NAME        CAPACITY  ACCESSMODES  STATUS    CLAIM               REASON AGE
mongodb-pv  1Gi       RWO,ROX      Released  default/mongodb-pvc        5m
The STATUS column shows the PersistentVolume as Released, not Available like
before. Because you’ve already used the volume, it may contain data and shouldn’t be
bound to a completely new claim without giving the cluster admin a chance to clean it
up. Without this, a new pod using the same PersistentVolume could read the data
stored there by the previous pod, even if the claim and pod were created in a different
namespace (and thus likely belong to a different cluster tenant).
RECLAIMING PERSISTENTVOLUMES MANUALLY
You told Kubernetes you wanted your PersistentVolume to behave like this when you
created it—by setting its persistentVolumeReclaimPolicy to Retain. You wanted
Kubernetes to retain the volume and its contents after it’s released from its claim. As
far as I’m aware, the only way to manually recycle the PersistentVolume to make it
available again is to delete and recreate the PersistentVolume resource. As you do
that, it’s your decision what to do with the files on the underlying storage: you can
either delete them or leave them alone so they can be reused by the next  pod.
RECLAIMING PERSISTENTVOLUMES AUTOMATICALLY
Two other possible reclaim policies exist: Recycle and Delete. The first one deletes
the volume’s contents and makes the volume available to be claimed again. This way,
the PersistentVolume can be reused multiple times by different PersistentVolume-
Claims and different pods, as you can see in figure 6.9.
 The Delete policy, on the other hand, deletes the underlying storage. Note that
the Recycle option is currently not available for GCE Persistent Disks. This type of
 
</data>
  <data key="d5">183
Decoupling pods from the underlying storage technology
6.5.6
Recycling PersistentVolumes
Before you wrap up this section on PersistentVolumes, let’s do one last quick experi-
ment. Delete the pod and the PersistentVolumeClaim:
$ kubectl delete pod mongodb
pod "mongodb" deleted
$ kubectl delete pvc mongodb-pvc
persistentvolumeclaim "mongodb-pvc" deleted
What if you create the PersistentVolumeClaim again? Will it be bound to the Persistent-
Volume or not? After you create the claim, what does kubectl get pvc show?
$ kubectl get pvc
NAME           STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE
mongodb-pvc    Pending                                         13s
The claim’s status is shown as Pending. Interesting. When you created the claim ear-
lier, it was immediately bound to the PersistentVolume, so why wasn’t it bound now?
Maybe listing the PersistentVolumes can shed more light on this:
$ kubectl get pv
NAME        CAPACITY  ACCESSMODES  STATUS    CLAIM               REASON AGE
mongodb-pv  1Gi       RWO,ROX      Released  default/mongodb-pvc        5m
The STATUS column shows the PersistentVolume as Released, not Available like
before. Because you’ve already used the volume, it may contain data and shouldn’t be
bound to a completely new claim without giving the cluster admin a chance to clean it
up. Without this, a new pod using the same PersistentVolume could read the data
stored there by the previous pod, even if the claim and pod were created in a different
namespace (and thus likely belong to a different cluster tenant).
RECLAIMING PERSISTENTVOLUMES MANUALLY
You told Kubernetes you wanted your PersistentVolume to behave like this when you
created it—by setting its persistentVolumeReclaimPolicy to Retain. You wanted
Kubernetes to retain the volume and its contents after it’s released from its claim. As
far as I’m aware, the only way to manually recycle the PersistentVolume to make it
available again is to delete and recreate the PersistentVolume resource. As you do
that, it’s your decision what to do with the files on the underlying storage: you can
either delete them or leave them alone so they can be reused by the next  pod.
RECLAIMING PERSISTENTVOLUMES AUTOMATICALLY
Two other possible reclaim policies exist: Recycle and Delete. The first one deletes
the volume’s contents and makes the volume available to be claimed again. This way,
the PersistentVolume can be reused multiple times by different PersistentVolume-
Claims and different pods, as you can see in figure 6.9.
 The Delete policy, on the other hand, deletes the underlying storage. Note that
the Recycle option is currently not available for GCE Persistent Disks. This type of
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="216">
  <data key="d0">Page_216</data>
  <data key="d5">Page_216</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_127">
  <data key="d0">184
CHAPTER 6
Volumes: attaching disk storage to containers
A PersistentVolume only supports the Retain or Delete policies. Other Persistent-
Volume types may or may not support each of these options, so before creating your
own PersistentVolume, be sure to check what reclaim policies are supported for the
specific underlying storage you’ll use in the volume.
TIP
You can change the PersistentVolume reclaim policy on an existing
PersistentVolume. For example, if it’s initially set to Delete, you can easily
change it to Retain to prevent losing valuable data.
6.6
Dynamic provisioning of PersistentVolumes
You’ve seen how using PersistentVolumes and PersistentVolumeClaims makes it easy
to obtain persistent storage without the developer having to deal with the actual stor-
age technology used underneath. But this still requires a cluster administrator to pro-
vision the actual storage up front. Luckily, Kubernetes can also perform this job
automatically through dynamic provisioning of PersistentVolumes.
 The cluster admin, instead of creating PersistentVolumes, can deploy a Persistent-
Volume provisioner and define one or more StorageClass objects to let users choose
what type of PersistentVolume they want. The users can refer to the StorageClass in
their PersistentVolumeClaims and the provisioner will take that into account when
provisioning the persistent storage. 
NOTE
Similar to PersistentVolumes, StorageClass resources aren’t namespaced.
Kubernetes includes provisioners for the most popular cloud providers, so the admin-
istrator doesn’t always need to deploy a provisioner. But if Kubernetes is deployed
on-premises, a custom provisioner needs to be deployed.
PersistentVolume
PersistentVolumeClaim 1
Pod 1
Pod 2
PersistentVolumeClaim 2
Pod 3
PVC is deleted;
PV is automatically
recycled and ready
to be claimed and
re-used again
User creates
PersistentVolumeClaim
Pod 2
unmounts
PVC
Pod 2
mounts
PVC
Pod 1
mounts
PVC
Pod 1
unmounts
PVC
Admin deletes
PersistentVolume
Admin creates
PersistentVolume
Time
Figure 6.9
The lifespan of a PersistentVolume, PersistentVolumeClaims, and pods using them
 
</data>
  <data key="d5">184
CHAPTER 6
Volumes: attaching disk storage to containers
A PersistentVolume only supports the Retain or Delete policies. Other Persistent-
Volume types may or may not support each of these options, so before creating your
own PersistentVolume, be sure to check what reclaim policies are supported for the
specific underlying storage you’ll use in the volume.
TIP
You can change the PersistentVolume reclaim policy on an existing
PersistentVolume. For example, if it’s initially set to Delete, you can easily
change it to Retain to prevent losing valuable data.
6.6
Dynamic provisioning of PersistentVolumes
You’ve seen how using PersistentVolumes and PersistentVolumeClaims makes it easy
to obtain persistent storage without the developer having to deal with the actual stor-
age technology used underneath. But this still requires a cluster administrator to pro-
vision the actual storage up front. Luckily, Kubernetes can also perform this job
automatically through dynamic provisioning of PersistentVolumes.
 The cluster admin, instead of creating PersistentVolumes, can deploy a Persistent-
Volume provisioner and define one or more StorageClass objects to let users choose
what type of PersistentVolume they want. The users can refer to the StorageClass in
their PersistentVolumeClaims and the provisioner will take that into account when
provisioning the persistent storage. 
NOTE
Similar to PersistentVolumes, StorageClass resources aren’t namespaced.
Kubernetes includes provisioners for the most popular cloud providers, so the admin-
istrator doesn’t always need to deploy a provisioner. But if Kubernetes is deployed
on-premises, a custom provisioner needs to be deployed.
PersistentVolume
PersistentVolumeClaim 1
Pod 1
Pod 2
PersistentVolumeClaim 2
Pod 3
PVC is deleted;
PV is automatically
recycled and ready
to be claimed and
re-used again
User creates
PersistentVolumeClaim
Pod 2
unmounts
PVC
Pod 2
mounts
PVC
Pod 1
mounts
PVC
Pod 1
unmounts
PVC
Admin deletes
PersistentVolume
Admin creates
PersistentVolume
Time
Figure 6.9
The lifespan of a PersistentVolume, PersistentVolumeClaims, and pods using them
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="217">
  <data key="d0">Page_217</data>
  <data key="d5">Page_217</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_128">
  <data key="d0">185
Dynamic provisioning of PersistentVolumes
 Instead of the administrator pre-provisioning a bunch of PersistentVolumes, they
need to define one or two (or more) StorageClasses and let the system create a new
PersistentVolume each time one is requested through a PersistentVolumeClaim. The
great thing about this is that it’s impossible to run out of PersistentVolumes (obviously,
you can run out of storage space). 
6.6.1
Defining the available storage types through StorageClass 
resources
Before a user can create a PersistentVolumeClaim, which will result in a new Persistent-
Volume being provisioned, an admin needs to create one or more StorageClass
resources. Let’s look at an example of one in the following listing.
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd       
parameters:
  type: pd-ssd                     
  zone: europe-west1-b             
NOTE
If using Minikube, deploy the file storageclass-fast-hostpath.yaml.
The StorageClass resource specifies which provisioner should be used for provision-
ing the PersistentVolume when a PersistentVolumeClaim requests this StorageClass.
The parameters defined in the StorageClass definition are passed to the provisioner
and are specific to each provisioner plugin. 
 The StorageClass uses the Google Compute Engine (GCE) Persistent Disk (PD)
provisioner, which means it can be used when Kubernetes is running in GCE. For
other cloud providers, other provisioners need to be used.
6.6.2
Requesting the storage class in a PersistentVolumeClaim
After the StorageClass resource is created, users can refer to the storage class by name
in their PersistentVolumeClaims. 
CREATING A PVC DEFINITION REQUESTING A SPECIFIC STORAGE CLASS
You can modify your mongodb-pvc to use dynamic provisioning. The following listing
shows the updated YAML definition of the PVC.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc 
Listing 6.14
A StorageClass definition: storageclass-fast-gcepd.yaml
Listing 6.15
A PVC with dynamic provisioning: mongodb-pvc-dp.yaml
The volume plugin to 
use for provisioning 
the PersistentVolume
The parameters passed 
to the provisioner
 
</data>
  <data key="d5">185
Dynamic provisioning of PersistentVolumes
 Instead of the administrator pre-provisioning a bunch of PersistentVolumes, they
need to define one or two (or more) StorageClasses and let the system create a new
PersistentVolume each time one is requested through a PersistentVolumeClaim. The
great thing about this is that it’s impossible to run out of PersistentVolumes (obviously,
you can run out of storage space). 
6.6.1
Defining the available storage types through StorageClass 
resources
Before a user can create a PersistentVolumeClaim, which will result in a new Persistent-
Volume being provisioned, an admin needs to create one or more StorageClass
resources. Let’s look at an example of one in the following listing.
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd       
parameters:
  type: pd-ssd                     
  zone: europe-west1-b             
NOTE
If using Minikube, deploy the file storageclass-fast-hostpath.yaml.
The StorageClass resource specifies which provisioner should be used for provision-
ing the PersistentVolume when a PersistentVolumeClaim requests this StorageClass.
The parameters defined in the StorageClass definition are passed to the provisioner
and are specific to each provisioner plugin. 
 The StorageClass uses the Google Compute Engine (GCE) Persistent Disk (PD)
provisioner, which means it can be used when Kubernetes is running in GCE. For
other cloud providers, other provisioners need to be used.
6.6.2
Requesting the storage class in a PersistentVolumeClaim
After the StorageClass resource is created, users can refer to the storage class by name
in their PersistentVolumeClaims. 
CREATING A PVC DEFINITION REQUESTING A SPECIFIC STORAGE CLASS
You can modify your mongodb-pvc to use dynamic provisioning. The following listing
shows the updated YAML definition of the PVC.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc 
Listing 6.14
A StorageClass definition: storageclass-fast-gcepd.yaml
Listing 6.15
A PVC with dynamic provisioning: mongodb-pvc-dp.yaml
The volume plugin to 
use for provisioning 
the PersistentVolume
The parameters passed 
to the provisioner
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="218">
  <data key="d0">Page_218</data>
  <data key="d5">Page_218</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_129">
  <data key="d0">186
CHAPTER 6
Volumes: attaching disk storage to containers
spec:
  storageClassName: fast     
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
Apart from specifying the size and access modes, your PersistentVolumeClaim now
also specifies the class of storage you want to use. When you create the claim, the
PersistentVolume is created by the provisioner referenced in the fast StorageClass
resource. The provisioner is used even if an existing manually provisioned Persistent-
Volume matches the PersistentVolumeClaim. 
NOTE
If you reference a non-existing storage class in a PVC, the provisioning
of the PV will fail (you’ll see a ProvisioningFailed event when you use
kubectl describe on the PVC).
EXAMINING THE CREATED PVC AND THE DYNAMICALLY PROVISIONED PV
Next you’ll create the PVC and then use kubectl get to see it:
$ kubectl get pvc mongodb-pvc
NAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS
mongodb-pvc   Bound    pvc-1e6bc048   1Gi        RWO           fast 
The VOLUME column shows the PersistentVolume that’s bound to this claim (the actual
name is longer than what’s shown above). You can try listing PersistentVolumes now to
see that a new PV has indeed been created automatically:
$ kubectl get pv
NAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   
mongodb-pv     1Gi       RWO,ROX      Retain         Released 
pvc-1e6bc048   1Gi       RWO          Delete         Bound     fast
NOTE
Only pertinent columns are shown.
You can see the dynamically provisioned PersistentVolume. Its capacity and access
modes are what you requested in the PVC. Its reclaim policy is Delete, which means
the PersistentVolume will be deleted when the PVC is deleted. Beside the PV, the pro-
visioner also provisioned the actual storage. Your fast StorageClass is configured to
use the kubernetes.io/gce-pd provisioner, which provisions GCE Persistent Disks.
You can see the disk with the following command:
$ gcloud compute disks list
NAME                          ZONE            SIZE_GB  TYPE         STATUS
gke-kubia-dyn-pvc-1e6bc048    europe-west1-d  1        pd-ssd       READY
gke-kubia-default-pool-71df   europe-west1-d  100      pd-standard  READY
gke-kubia-default-pool-79cd   europe-west1-d  100      pd-standard  READY
gke-kubia-default-pool-blc4   europe-west1-d  100      pd-standard  READY
mongodb                       europe-west1-d  1        pd-standard  READY
This PVC requests the 
custom storage class.
 
</data>
  <data key="d5">186
CHAPTER 6
Volumes: attaching disk storage to containers
spec:
  storageClassName: fast     
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
Apart from specifying the size and access modes, your PersistentVolumeClaim now
also specifies the class of storage you want to use. When you create the claim, the
PersistentVolume is created by the provisioner referenced in the fast StorageClass
resource. The provisioner is used even if an existing manually provisioned Persistent-
Volume matches the PersistentVolumeClaim. 
NOTE
If you reference a non-existing storage class in a PVC, the provisioning
of the PV will fail (you’ll see a ProvisioningFailed event when you use
kubectl describe on the PVC).
EXAMINING THE CREATED PVC AND THE DYNAMICALLY PROVISIONED PV
Next you’ll create the PVC and then use kubectl get to see it:
$ kubectl get pvc mongodb-pvc
NAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS
mongodb-pvc   Bound    pvc-1e6bc048   1Gi        RWO           fast 
The VOLUME column shows the PersistentVolume that’s bound to this claim (the actual
name is longer than what’s shown above). You can try listing PersistentVolumes now to
see that a new PV has indeed been created automatically:
$ kubectl get pv
NAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   
mongodb-pv     1Gi       RWO,ROX      Retain         Released 
pvc-1e6bc048   1Gi       RWO          Delete         Bound     fast
NOTE
Only pertinent columns are shown.
You can see the dynamically provisioned PersistentVolume. Its capacity and access
modes are what you requested in the PVC. Its reclaim policy is Delete, which means
the PersistentVolume will be deleted when the PVC is deleted. Beside the PV, the pro-
visioner also provisioned the actual storage. Your fast StorageClass is configured to
use the kubernetes.io/gce-pd provisioner, which provisions GCE Persistent Disks.
You can see the disk with the following command:
$ gcloud compute disks list
NAME                          ZONE            SIZE_GB  TYPE         STATUS
gke-kubia-dyn-pvc-1e6bc048    europe-west1-d  1        pd-ssd       READY
gke-kubia-default-pool-71df   europe-west1-d  100      pd-standard  READY
gke-kubia-default-pool-79cd   europe-west1-d  100      pd-standard  READY
gke-kubia-default-pool-blc4   europe-west1-d  100      pd-standard  READY
mongodb                       europe-west1-d  1        pd-standard  READY
This PVC requests the 
custom storage class.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="219">
  <data key="d0">Page_219</data>
  <data key="d5">Page_219</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_130">
  <data key="d0">187
Dynamic provisioning of PersistentVolumes
As you can see, the first persistent disk’s name suggests it was provisioned dynamically
and its type shows it’s an SSD, as specified in the storage class you created earlier. 
UNDERSTANDING HOW TO USE STORAGE CLASSES
The cluster admin can create multiple storage classes with different performance or
other characteristics. The developer then decides which one is most appropriate for
each claim they create. 
 The nice thing about StorageClasses is the fact that claims refer to them by
name. The PVC definitions are therefore portable across different clusters, as long
as the StorageClass names are the same across all of them. To see this portability
yourself, you can try running the same example on Minikube, if you’ve been using
GKE up to this point. As a cluster admin, you’ll have to create a different storage
class (but with the same name). The storage class defined in the storageclass-fast-
hostpath.yaml file is tailor-made for use in Minikube. Then, once you deploy the stor-
age class, you as a cluster user can deploy the exact same PVC manifest and the exact
same pod manifest as before. This shows how the pods and PVCs are portable across
different clusters.
6.6.3
Dynamic provisioning without specifying a storage class
As we’ve progressed through this chapter, attaching persistent storage to pods has
become ever simpler. The sections in this chapter reflect how provisioning of storage
has evolved from early Kubernetes versions to now. In this final section, we’ll look at
the latest and simplest way of attaching a PersistentVolume to a pod. 
LISTING STORAGE CLASSES
When you created your custom storage class called fast, you didn’t check if any exist-
ing storage classes were already defined in your cluster. Why don’t you do that now?
Here are the storage classes available in GKE:
$ kubectl get sc
NAME                 TYPE
fast                 kubernetes.io/gce-pd
standard (default)   kubernetes.io/gce-pd
NOTE
We’re using sc as shorthand for storageclass.
Beside the fast storage class, which you created yourself, a standard storage class
exists and is marked as default. You’ll learn what that means in a moment. Let’s list the
storage classes available in Minikube, so we can compare:
$ kubectl get sc
NAME                 TYPE
fast                 k8s.io/minikube-hostpath
standard (default)   k8s.io/minikube-hostpath
Again, the fast storage class was created by you and a default standard storage class
exists here as well. Comparing the TYPE columns in the two listings, you see GKE is
 
</data>
  <data key="d5">187
Dynamic provisioning of PersistentVolumes
As you can see, the first persistent disk’s name suggests it was provisioned dynamically
and its type shows it’s an SSD, as specified in the storage class you created earlier. 
UNDERSTANDING HOW TO USE STORAGE CLASSES
The cluster admin can create multiple storage classes with different performance or
other characteristics. The developer then decides which one is most appropriate for
each claim they create. 
 The nice thing about StorageClasses is the fact that claims refer to them by
name. The PVC definitions are therefore portable across different clusters, as long
as the StorageClass names are the same across all of them. To see this portability
yourself, you can try running the same example on Minikube, if you’ve been using
GKE up to this point. As a cluster admin, you’ll have to create a different storage
class (but with the same name). The storage class defined in the storageclass-fast-
hostpath.yaml file is tailor-made for use in Minikube. Then, once you deploy the stor-
age class, you as a cluster user can deploy the exact same PVC manifest and the exact
same pod manifest as before. This shows how the pods and PVCs are portable across
different clusters.
6.6.3
Dynamic provisioning without specifying a storage class
As we’ve progressed through this chapter, attaching persistent storage to pods has
become ever simpler. The sections in this chapter reflect how provisioning of storage
has evolved from early Kubernetes versions to now. In this final section, we’ll look at
the latest and simplest way of attaching a PersistentVolume to a pod. 
LISTING STORAGE CLASSES
When you created your custom storage class called fast, you didn’t check if any exist-
ing storage classes were already defined in your cluster. Why don’t you do that now?
Here are the storage classes available in GKE:
$ kubectl get sc
NAME                 TYPE
fast                 kubernetes.io/gce-pd
standard (default)   kubernetes.io/gce-pd
NOTE
We’re using sc as shorthand for storageclass.
Beside the fast storage class, which you created yourself, a standard storage class
exists and is marked as default. You’ll learn what that means in a moment. Let’s list the
storage classes available in Minikube, so we can compare:
$ kubectl get sc
NAME                 TYPE
fast                 k8s.io/minikube-hostpath
standard (default)   k8s.io/minikube-hostpath
Again, the fast storage class was created by you and a default standard storage class
exists here as well. Comparing the TYPE columns in the two listings, you see GKE is
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="220">
  <data key="d0">Page_220</data>
  <data key="d5">Page_220</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_131">
  <data key="d0">188
CHAPTER 6
Volumes: attaching disk storage to containers
using the kubernetes.io/gce-pd provisioner, whereas Minikube is using k8s.io/
minikube-hostpath. 
EXAMINING THE DEFAULT STORAGE CLASS
You’re going to use kubectl get to see more info about the standard storage class in a
GKE cluster, as shown in the following listing.
$ kubectl get sc standard -o yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"   
  creationTimestamp: 2017-05-16T15:24:11Z
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
    kubernetes.io/cluster-service: "true"
  name: standard
  resourceVersion: "180"
  selfLink: /apis/storage.k8s.io/v1/storageclassesstandard
  uid: b6498511-3a4b-11e7-ba2c-42010a840014
parameters:                                    
  type: pd-standard                            
provisioner: kubernetes.io/gce-pd      
If you look closely toward the top of the listing, the storage class definition includes an
annotation, which makes this the default storage class. The default storage class is
what’s used to dynamically provision a PersistentVolume if the PersistentVolumeClaim
doesn’t explicitly say which storage class to use. 
CREATING A PERSISTENTVOLUMECLAIM WITHOUT SPECIFYING A STORAGE CLASS
You can create a PVC without specifying the storageClassName attribute and (on
Google Kubernetes Engine) a GCE Persistent Disk of type pd-standard will be provi-
sioned for you. Try this by creating a claim from the YAML in the following listing.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc2
spec:                        
  resources:                 
    requests:                
      storage: 100Mi         
  accessModes:               
    - ReadWriteOnce          
Listing 6.16
The definition of the standard storage class on GKE
Listing 6.17
PVC with no storage class defined: mongodb-pvc-dp-nostorageclass.yaml
This annotation 
marks the storage 
class as default.
The type parameter is used by the provisioner 
to know what type of GCE PD to create.
The GCE Persistent Disk provisioner 
is used to provision PVs of this class.
You’re not specifying 
the storageClassName 
attribute (unlike earlier 
examples).
 
</data>
  <data key="d5">188
CHAPTER 6
Volumes: attaching disk storage to containers
using the kubernetes.io/gce-pd provisioner, whereas Minikube is using k8s.io/
minikube-hostpath. 
EXAMINING THE DEFAULT STORAGE CLASS
You’re going to use kubectl get to see more info about the standard storage class in a
GKE cluster, as shown in the following listing.
$ kubectl get sc standard -o yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"   
  creationTimestamp: 2017-05-16T15:24:11Z
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
    kubernetes.io/cluster-service: "true"
  name: standard
  resourceVersion: "180"
  selfLink: /apis/storage.k8s.io/v1/storageclassesstandard
  uid: b6498511-3a4b-11e7-ba2c-42010a840014
parameters:                                    
  type: pd-standard                            
provisioner: kubernetes.io/gce-pd      
If you look closely toward the top of the listing, the storage class definition includes an
annotation, which makes this the default storage class. The default storage class is
what’s used to dynamically provision a PersistentVolume if the PersistentVolumeClaim
doesn’t explicitly say which storage class to use. 
CREATING A PERSISTENTVOLUMECLAIM WITHOUT SPECIFYING A STORAGE CLASS
You can create a PVC without specifying the storageClassName attribute and (on
Google Kubernetes Engine) a GCE Persistent Disk of type pd-standard will be provi-
sioned for you. Try this by creating a claim from the YAML in the following listing.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc2
spec:                        
  resources:                 
    requests:                
      storage: 100Mi         
  accessModes:               
    - ReadWriteOnce          
Listing 6.16
The definition of the standard storage class on GKE
Listing 6.17
PVC with no storage class defined: mongodb-pvc-dp-nostorageclass.yaml
This annotation 
marks the storage 
class as default.
The type parameter is used by the provisioner 
to know what type of GCE PD to create.
The GCE Persistent Disk provisioner 
is used to provision PVs of this class.
You’re not specifying 
the storageClassName 
attribute (unlike earlier 
examples).
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="221">
  <data key="d0">Page_221</data>
  <data key="d5">Page_221</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_132">
  <data key="d0">189
Dynamic provisioning of PersistentVolumes
This PVC definition includes only the storage size request and the desired access
modes, but no storage class. When you create the PVC, whatever storage class is
marked as default will be used. You can confirm that’s the case:
$ kubectl get pvc mongodb-pvc2
NAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS
mongodb-pvc2  Bound    pvc-95a5ec12   1Gi        RWO           standard
$ kubectl get pv pvc-95a5ec12
NAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   
pvc-95a5ec12   1Gi       RWO          Delete         Bound     standard
$ gcloud compute disks list
NAME                          ZONE            SIZE_GB  TYPE         STATUS
gke-kubia-dyn-pvc-95a5ec12    europe-west1-d  1        pd-standard  READY
...
FORCING A PERSISTENTVOLUMECLAIM TO BE BOUND TO ONE OF THE PRE-PROVISIONED 
PERSISTENTVOLUMES
This finally brings us to why you set storageClassName to an empty string in listing 6.11
(when you wanted the PVC to bind to the PV you’d provisioned manually). Let me
repeat the relevant lines of that PVC definition here:
kind: PersistentVolumeClaim
spec:
  storageClassName: ""       
If you hadn’t set the storageClassName attribute to an empty string, the dynamic vol-
ume provisioner would have provisioned a new PersistentVolume, despite there being
an appropriate pre-provisioned PersistentVolume. At that point, I wanted to demon-
strate how a claim gets bound to a manually pre-provisioned PersistentVolume. I didn’t
want the dynamic provisioner to interfere. 
TIP
Explicitly set storageClassName to "" if you want the PVC to use a pre-
provisioned PersistentVolume.
UNDERSTANDING THE COMPLETE PICTURE OF DYNAMIC PERSISTENTVOLUME PROVISIONING
This brings us to the end of this chapter. To summarize, the best way to attach per-
sistent storage to a pod is to only create the PVC (with an explicitly specified storage-
ClassName if necessary) and the pod (which refers to the PVC by name). Everything
else is taken care of by the dynamic PersistentVolume provisioner.
 To get a complete picture of the steps involved in getting a dynamically provi-
sioned PersistentVolume, examine figure 6.10.
 
 
 
Specifying an empty string as the storage class 
name ensures the PVC binds to a pre-provisioned 
PV instead of dynamically provisioning a new one.
 
</data>
  <data key="d5">189
Dynamic provisioning of PersistentVolumes
This PVC definition includes only the storage size request and the desired access
modes, but no storage class. When you create the PVC, whatever storage class is
marked as default will be used. You can confirm that’s the case:
$ kubectl get pvc mongodb-pvc2
NAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS
mongodb-pvc2  Bound    pvc-95a5ec12   1Gi        RWO           standard
$ kubectl get pv pvc-95a5ec12
NAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   
pvc-95a5ec12   1Gi       RWO          Delete         Bound     standard
$ gcloud compute disks list
NAME                          ZONE            SIZE_GB  TYPE         STATUS
gke-kubia-dyn-pvc-95a5ec12    europe-west1-d  1        pd-standard  READY
...
FORCING A PERSISTENTVOLUMECLAIM TO BE BOUND TO ONE OF THE PRE-PROVISIONED 
PERSISTENTVOLUMES
This finally brings us to why you set storageClassName to an empty string in listing 6.11
(when you wanted the PVC to bind to the PV you’d provisioned manually). Let me
repeat the relevant lines of that PVC definition here:
kind: PersistentVolumeClaim
spec:
  storageClassName: ""       
If you hadn’t set the storageClassName attribute to an empty string, the dynamic vol-
ume provisioner would have provisioned a new PersistentVolume, despite there being
an appropriate pre-provisioned PersistentVolume. At that point, I wanted to demon-
strate how a claim gets bound to a manually pre-provisioned PersistentVolume. I didn’t
want the dynamic provisioner to interfere. 
TIP
Explicitly set storageClassName to "" if you want the PVC to use a pre-
provisioned PersistentVolume.
UNDERSTANDING THE COMPLETE PICTURE OF DYNAMIC PERSISTENTVOLUME PROVISIONING
This brings us to the end of this chapter. To summarize, the best way to attach per-
sistent storage to a pod is to only create the PVC (with an explicitly specified storage-
ClassName if necessary) and the pod (which refers to the PVC by name). Everything
else is taken care of by the dynamic PersistentVolume provisioner.
 To get a complete picture of the steps involved in getting a dynamically provi-
sioned PersistentVolume, examine figure 6.10.
 
 
 
Specifying an empty string as the storage class 
name ensures the PVC binds to a pre-provisioned 
PV instead of dynamically provisioning a new one.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="222">
  <data key="d0">Page_222</data>
  <data key="d5">Page_222</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_133">
  <data key="d0">190
CHAPTER 6
Volumes: attaching disk storage to containers
6.7
Summary
This chapter has shown you how volumes are used to provide either temporary or per-
sistent storage to a pod’s containers. You’ve learned how to
Create a multi-container pod and have the pod’s containers operate on the
same files by adding a volume to the pod and mounting it in each container
Use the emptyDir volume to store temporary, non-persistent data
Use the gitRepo volume to easily populate a directory with the contents of a Git
repository at pod startup
Use the hostPath volume to access files from the host node
Mount external storage in a volume to persist pod data across pod restarts
Decouple the pod from the storage infrastructure by using PersistentVolumes
and PersistentVolumeClaims
Have PersistentVolumes of the desired (or the default) storage class dynami-
cally provisioned for each PersistentVolumeClaim
Prevent the dynamic provisioner from interfering when you want the Persistent-
VolumeClaim to be bound to a pre-provisioned PersistentVolume
In the next chapter, you’ll see what mechanisms Kubernetes provides to deliver con-
figuration data, secret information, and metadata about the pod and container to the
processes running inside a pod. This is done with the special types of volumes we’ve
mentioned in this chapter, but not yet explored.
Pod
Admin
Volume
1. Cluster admin sets up a PersistentVolume
provisioner (if one’s not already deployed)
2. Admin creates one or
more StorageClasses
and marks one as the
default (it may already
exist)
Actual
storage
Persistent
Volume
User
Persistent
Volume
provisioner
Persistent
VolumeClaim
Storage
Class
3. User creates a PVC referencing one of the
StorageClasses (or none to use the default)
6. User creates a pod with
a volume referencing the
PVC by name
4. Kubernetes looks up the
StorageClass and the provisioner
referenced in it and asks the provisioner
to provision a new PV based on the
PVC’s requested access mode and
storage size and the parameters
in the StorageClass
5. Provisioner provisions the
actual storage, creates
a PersistentVolume, and
binds it to the PVC
Figure 6.10
The complete picture of dynamic provisioning of PersistentVolumes
 
</data>
  <data key="d5">190
CHAPTER 6
Volumes: attaching disk storage to containers
6.7
Summary
This chapter has shown you how volumes are used to provide either temporary or per-
sistent storage to a pod’s containers. You’ve learned how to
Create a multi-container pod and have the pod’s containers operate on the
same files by adding a volume to the pod and mounting it in each container
Use the emptyDir volume to store temporary, non-persistent data
Use the gitRepo volume to easily populate a directory with the contents of a Git
repository at pod startup
Use the hostPath volume to access files from the host node
Mount external storage in a volume to persist pod data across pod restarts
Decouple the pod from the storage infrastructure by using PersistentVolumes
and PersistentVolumeClaims
Have PersistentVolumes of the desired (or the default) storage class dynami-
cally provisioned for each PersistentVolumeClaim
Prevent the dynamic provisioner from interfering when you want the Persistent-
VolumeClaim to be bound to a pre-provisioned PersistentVolume
In the next chapter, you’ll see what mechanisms Kubernetes provides to deliver con-
figuration data, secret information, and metadata about the pod and container to the
processes running inside a pod. This is done with the special types of volumes we’ve
mentioned in this chapter, but not yet explored.
Pod
Admin
Volume
1. Cluster admin sets up a PersistentVolume
provisioner (if one’s not already deployed)
2. Admin creates one or
more StorageClasses
and marks one as the
default (it may already
exist)
Actual
storage
Persistent
Volume
User
Persistent
Volume
provisioner
Persistent
VolumeClaim
Storage
Class
3. User creates a PVC referencing one of the
StorageClasses (or none to use the default)
6. User creates a pod with
a volume referencing the
PVC by name
4. Kubernetes looks up the
StorageClass and the provisioner
referenced in it and asks the provisioner
to provision a new PV based on the
PVC’s requested access mode and
storage size and the parameters
in the StorageClass
5. Provisioner provisions the
actual storage, creates
a PersistentVolume, and
binds it to the PVC
Figure 6.10
The complete picture of dynamic provisioning of PersistentVolumes
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="223">
  <data key="d0">Page_223</data>
  <data key="d5">Page_223</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_134">
  <data key="d0">191
ConfigMaps and Secrets:
configuring applications
Up to now you haven’t had to pass any kind of configuration data to the apps you’ve
run in the exercises in this book. Because almost all apps require configuration (set-
tings that differ between deployed instances, credentials for accessing external sys-
tems, and so on), which shouldn’t be baked into the built app itself, let’s see how to
pass configuration options to your app when running it in Kubernetes.
7.1
Configuring containerized applications
Before we go over how to pass configuration data to apps running in Kubernetes,
let’s look at how containerized applications are usually configured.
 If you skip the fact that you can bake the configuration into the application
itself, when starting development of a new app, you usually start off by having the
This chapter covers
Changing the main process of a container
Passing command-line options to the app
Setting environment variables exposed to the app
Configuring apps through ConfigMaps
Passing sensitive information through Secrets
 
</data>
  <data key="d5">191
ConfigMaps and Secrets:
configuring applications
Up to now you haven’t had to pass any kind of configuration data to the apps you’ve
run in the exercises in this book. Because almost all apps require configuration (set-
tings that differ between deployed instances, credentials for accessing external sys-
tems, and so on), which shouldn’t be baked into the built app itself, let’s see how to
pass configuration options to your app when running it in Kubernetes.
7.1
Configuring containerized applications
Before we go over how to pass configuration data to apps running in Kubernetes,
let’s look at how containerized applications are usually configured.
 If you skip the fact that you can bake the configuration into the application
itself, when starting development of a new app, you usually start off by having the
This chapter covers
Changing the main process of a container
Passing command-line options to the app
Setting environment variables exposed to the app
Configuring apps through ConfigMaps
Passing sensitive information through Secrets
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="224">
  <data key="d0">Page_224</data>
  <data key="d5">Page_224</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_135">
  <data key="d0">192
CHAPTER 7
ConfigMaps and Secrets: configuring applications
app configured through command-line arguments. Then, as the list of configuration
options grows, you can move the configuration into a config file. 
 Another way of passing configuration options to an application that’s widely popu-
lar in containerized applications is through environment variables. Instead of having
the app read a config file or command-line arguments, the app looks up the value of a
certain environment variable. The official MySQL container image, for example, uses
an environment variable called MYSQL_ROOT_PASSWORD for setting the password for the
root super-user account. 
 But why are environment variables so popular in containers? Using configuration
files inside Docker containers is a bit tricky, because you’d have to bake the config file
into the container image itself or mount a volume containing the file into the con-
tainer. Obviously, baking files into the image is similar to hardcoding configuration
into the source code of the application, because it requires you to rebuild the image
every time you want to change the config. Plus, everyone with access to the image can
see the config, including any information that should be kept secret, such as creden-
tials or encryption keys. Using a volume is better, but still requires you to make sure
the file is written to the volume before the container is started. 
 If you’ve read the previous chapter, you might think of using a gitRepo volume as
a configuration source. That’s not a bad idea, because it allows you to keep the config
nicely versioned and enables you to easily rollback a config change if necessary. But a
simpler way allows you to put the configuration data into a top-level Kubernetes
resource and store it and all the other resource definitions in the same Git repository
or in any other file-based storage. The Kubernetes resource for storing configuration
data is called a ConfigMap. We’ll learn how to use it in this chapter.
 Regardless if you’re using a ConfigMap to store configuration data or not, you can
configure your apps by
Passing command-line arguments to containers
Setting custom environment variables for each container
Mounting configuration files into containers through a special type of volume
We’ll go over all these options in the next few sections, but before we start, let’s look
at config options from a security perspective. Though most configuration options
don’t contain any sensitive information, several can. These include credentials, pri-
vate encryption keys, and similar data that needs to be kept secure. This type of infor-
mation needs to be handled with special care, which is why Kubernetes offers
another type of first-class object called a Secret. We’ll learn about it in the last part of
this chapter.
7.2
Passing command-line arguments to containers
In all the examples so far, you’ve created containers that ran the default command
defined in the container image, but Kubernetes allows overriding the command as
part of the pod’s container definition when you want to run a different executable
 
</data>
  <data key="d5">192
CHAPTER 7
ConfigMaps and Secrets: configuring applications
app configured through command-line arguments. Then, as the list of configuration
options grows, you can move the configuration into a config file. 
 Another way of passing configuration options to an application that’s widely popu-
lar in containerized applications is through environment variables. Instead of having
the app read a config file or command-line arguments, the app looks up the value of a
certain environment variable. The official MySQL container image, for example, uses
an environment variable called MYSQL_ROOT_PASSWORD for setting the password for the
root super-user account. 
 But why are environment variables so popular in containers? Using configuration
files inside Docker containers is a bit tricky, because you’d have to bake the config file
into the container image itself or mount a volume containing the file into the con-
tainer. Obviously, baking files into the image is similar to hardcoding configuration
into the source code of the application, because it requires you to rebuild the image
every time you want to change the config. Plus, everyone with access to the image can
see the config, including any information that should be kept secret, such as creden-
tials or encryption keys. Using a volume is better, but still requires you to make sure
the file is written to the volume before the container is started. 
 If you’ve read the previous chapter, you might think of using a gitRepo volume as
a configuration source. That’s not a bad idea, because it allows you to keep the config
nicely versioned and enables you to easily rollback a config change if necessary. But a
simpler way allows you to put the configuration data into a top-level Kubernetes
resource and store it and all the other resource definitions in the same Git repository
or in any other file-based storage. The Kubernetes resource for storing configuration
data is called a ConfigMap. We’ll learn how to use it in this chapter.
 Regardless if you’re using a ConfigMap to store configuration data or not, you can
configure your apps by
Passing command-line arguments to containers
Setting custom environment variables for each container
Mounting configuration files into containers through a special type of volume
We’ll go over all these options in the next few sections, but before we start, let’s look
at config options from a security perspective. Though most configuration options
don’t contain any sensitive information, several can. These include credentials, pri-
vate encryption keys, and similar data that needs to be kept secure. This type of infor-
mation needs to be handled with special care, which is why Kubernetes offers
another type of first-class object called a Secret. We’ll learn about it in the last part of
this chapter.
7.2
Passing command-line arguments to containers
In all the examples so far, you’ve created containers that ran the default command
defined in the container image, but Kubernetes allows overriding the command as
part of the pod’s container definition when you want to run a different executable
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="225">
  <data key="d0">Page_225</data>
  <data key="d5">Page_225</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_136">
  <data key="d0">193
Passing command-line arguments to containers
instead of the one specified in the image, or want to run it with a different set of com-
mand-line arguments. We’ll look at how to do that now.
7.2.1
Defining the command and arguments in Docker
The first thing I need to explain is that the whole command that gets executed in the
container is composed of two parts: the command and the arguments. 
UNDERSTANDING ENTRYPOINT AND CMD
In a Dockerfile, two instructions define the two parts:

ENTRYPOINT defines the executable invoked when the container is started.

CMD specifies the arguments that get passed to the ENTRYPOINT.
Although you can use the CMD instruction to specify the command you want to execute
when the image is run, the correct way is to do it through the ENTRYPOINT instruction
and to only specify the CMD if you want to define the default arguments. The image can
then be run without specifying any arguments
$ docker run &lt;image&gt;
or with additional arguments, which override whatever’s set under CMD in the Dockerfile:
$ docker run &lt;image&gt; &lt;arguments&gt;
UNDERSTANDING THE DIFFERENCE BETWEEN THE SHELL AND EXEC FORMS
But there’s more. Both instructions support two different forms:

shell form—For example, ENTRYPOINT node app.js.

exec form—For example, ENTRYPOINT ["node", "app.js"].
The difference is whether the specified command is invoked inside a shell or not. 
 In the kubia image you created in chapter 2, you used the exec form of the ENTRY-
POINT instruction: 
ENTRYPOINT ["node", "app.js"]
This runs the node process directly (not inside a shell), as you can see by listing the
processes running inside the container:
$ docker exec 4675d ps x
  PID TTY      STAT   TIME COMMAND
    1 ?        Ssl    0:00 node app.js
   12 ?        Rs     0:00 ps x
If you’d used the shell form (ENTRYPOINT node app.js), these would have been the
container’s processes:
$ docker exec -it e4bad ps x
  PID TTY      STAT   TIME COMMAND
    1 ?        Ss     0:00 /bin/sh -c node app.js
 
</data>
  <data key="d5">193
Passing command-line arguments to containers
instead of the one specified in the image, or want to run it with a different set of com-
mand-line arguments. We’ll look at how to do that now.
7.2.1
Defining the command and arguments in Docker
The first thing I need to explain is that the whole command that gets executed in the
container is composed of two parts: the command and the arguments. 
UNDERSTANDING ENTRYPOINT AND CMD
In a Dockerfile, two instructions define the two parts:

ENTRYPOINT defines the executable invoked when the container is started.

CMD specifies the arguments that get passed to the ENTRYPOINT.
Although you can use the CMD instruction to specify the command you want to execute
when the image is run, the correct way is to do it through the ENTRYPOINT instruction
and to only specify the CMD if you want to define the default arguments. The image can
then be run without specifying any arguments
$ docker run &lt;image&gt;
or with additional arguments, which override whatever’s set under CMD in the Dockerfile:
$ docker run &lt;image&gt; &lt;arguments&gt;
UNDERSTANDING THE DIFFERENCE BETWEEN THE SHELL AND EXEC FORMS
But there’s more. Both instructions support two different forms:

shell form—For example, ENTRYPOINT node app.js.

exec form—For example, ENTRYPOINT ["node", "app.js"].
The difference is whether the specified command is invoked inside a shell or not. 
 In the kubia image you created in chapter 2, you used the exec form of the ENTRY-
POINT instruction: 
ENTRYPOINT ["node", "app.js"]
This runs the node process directly (not inside a shell), as you can see by listing the
processes running inside the container:
$ docker exec 4675d ps x
  PID TTY      STAT   TIME COMMAND
    1 ?        Ssl    0:00 node app.js
   12 ?        Rs     0:00 ps x
If you’d used the shell form (ENTRYPOINT node app.js), these would have been the
container’s processes:
$ docker exec -it e4bad ps x
  PID TTY      STAT   TIME COMMAND
    1 ?        Ss     0:00 /bin/sh -c node app.js
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="226">
  <data key="d0">Page_226</data>
  <data key="d5">Page_226</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_137">
  <data key="d0">194
CHAPTER 7
ConfigMaps and Secrets: configuring applications
    7 ?        Sl     0:00 node app.js
   13 ?        Rs+    0:00 ps x
As you can see, in that case, the main process (PID 1) would be the shell process
instead of the node process. The node process (PID 7) would be started from that
shell. The shell process is unnecessary, which is why you should always use the exec
form of the ENTRYPOINT instruction.
MAKING THE INTERVAL CONFIGURABLE IN YOUR FORTUNE IMAGE
Let’s modify your fortune script and image so the delay interval in the loop is configu-
rable. You’ll add an INTERVAL variable and initialize it with the value of the first com-
mand-line argument, as shown in the following listing.
#!/bin/bash
trap "exit" SIGINT
INTERVAL=$1
echo Configured to generate new fortune every $INTERVAL seconds
mkdir -p /var/htdocs
while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune &gt; /var/htdocs/index.html
  sleep $INTERVAL
done
You’ve added or modified the lines in bold font. Now, you’ll modify the Dockerfile so
it uses the exec version of the ENTRYPOINT instruction and sets the default interval to
10 seconds using the CMD instruction, as shown in the following listing.
FROM ubuntu:latest
RUN apt-get update ; apt-get -y install fortune
ADD fortuneloop.sh /bin/fortuneloop.sh
ENTRYPOINT ["/bin/fortuneloop.sh"]        
CMD ["10"]                                
You can now build and push the image to Docker Hub. This time, you’ll tag the image
as args instead of latest:
$ docker build -t docker.io/luksa/fortune:args .
$ docker push docker.io/luksa/fortune:args
You can test the image by running it locally with Docker:
$ docker run -it docker.io/luksa/fortune:args
Configured to generate new fortune every 10 seconds
Fri May 19 10:39:44 UTC 2017 Writing fortune to /var/htdocs/index.html
Listing 7.1
Fortune script with interval configurable through argument: fortune-args/
fortuneloop.sh
Listing 7.2
Dockerfile for the updated fortune image: fortune-args/Dockerfile
The exec form of the 
ENTRYPOINT instruction
The default argument 
for the executable
 
</data>
  <data key="d5">194
CHAPTER 7
ConfigMaps and Secrets: configuring applications
    7 ?        Sl     0:00 node app.js
   13 ?        Rs+    0:00 ps x
As you can see, in that case, the main process (PID 1) would be the shell process
instead of the node process. The node process (PID 7) would be started from that
shell. The shell process is unnecessary, which is why you should always use the exec
form of the ENTRYPOINT instruction.
MAKING THE INTERVAL CONFIGURABLE IN YOUR FORTUNE IMAGE
Let’s modify your fortune script and image so the delay interval in the loop is configu-
rable. You’ll add an INTERVAL variable and initialize it with the value of the first com-
mand-line argument, as shown in the following listing.
#!/bin/bash
trap "exit" SIGINT
INTERVAL=$1
echo Configured to generate new fortune every $INTERVAL seconds
mkdir -p /var/htdocs
while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune &gt; /var/htdocs/index.html
  sleep $INTERVAL
done
You’ve added or modified the lines in bold font. Now, you’ll modify the Dockerfile so
it uses the exec version of the ENTRYPOINT instruction and sets the default interval to
10 seconds using the CMD instruction, as shown in the following listing.
FROM ubuntu:latest
RUN apt-get update ; apt-get -y install fortune
ADD fortuneloop.sh /bin/fortuneloop.sh
ENTRYPOINT ["/bin/fortuneloop.sh"]        
CMD ["10"]                                
You can now build and push the image to Docker Hub. This time, you’ll tag the image
as args instead of latest:
$ docker build -t docker.io/luksa/fortune:args .
$ docker push docker.io/luksa/fortune:args
You can test the image by running it locally with Docker:
$ docker run -it docker.io/luksa/fortune:args
Configured to generate new fortune every 10 seconds
Fri May 19 10:39:44 UTC 2017 Writing fortune to /var/htdocs/index.html
Listing 7.1
Fortune script with interval configurable through argument: fortune-args/
fortuneloop.sh
Listing 7.2
Dockerfile for the updated fortune image: fortune-args/Dockerfile
The exec form of the 
ENTRYPOINT instruction
The default argument 
for the executable
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="227">
  <data key="d0">Page_227</data>
  <data key="d5">Page_227</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_138">
  <data key="d0">195
Passing command-line arguments to containers
NOTE
You can stop the script with Control+C.
And you can override the default sleep interval by passing it as an argument:
$ docker run -it docker.io/luksa/fortune:args 15
Configured to generate new fortune every 15 seconds
Now that you’re sure your image honors the argument passed to it, let’s see how to use
it in a pod.
7.2.2
Overriding the command and arguments in Kubernetes
In Kubernetes, when specifying a container, you can choose to override both ENTRY-
POINT and CMD. To do that, you set the properties command and args in the container
specification, as shown in the following listing.
kind: Pod
spec:
  containers:
  - image: some/image
    command: ["/bin/command"]
    args: ["arg1", "arg2", "arg3"]
In most cases, you’ll only set custom arguments and rarely override the command
(except in general-purpose images such as busybox, which doesn’t define an ENTRY-
POINT at all). 
NOTE
The command and args fields can’t be updated after the pod is created.
The two Dockerfile instructions and the equivalent pod spec fields are shown in table 7.1.
RUNNING THE FORTUNE POD WITH A CUSTOM INTERVAL
To run the fortune pod with a custom delay interval, you’ll copy your fortune-
pod.yaml into fortune-pod-args.yaml and modify it as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: fortune2s        
Listing 7.3
A pod definition specifying a custom command and arguments
Table 7.1
Specifying the executable and its arguments in Docker vs Kubernetes
Docker
Kubernetes
Description
ENTRYPOINT
command
The executable that’s executed inside the container
CMD
args
The arguments passed to the executable
Listing 7.4
Passing an argument in the pod definition: fortune-pod-args.yaml
You changed the 
pod’s name.
 
</data>
  <data key="d5">195
Passing command-line arguments to containers
NOTE
You can stop the script with Control+C.
And you can override the default sleep interval by passing it as an argument:
$ docker run -it docker.io/luksa/fortune:args 15
Configured to generate new fortune every 15 seconds
Now that you’re sure your image honors the argument passed to it, let’s see how to use
it in a pod.
7.2.2
Overriding the command and arguments in Kubernetes
In Kubernetes, when specifying a container, you can choose to override both ENTRY-
POINT and CMD. To do that, you set the properties command and args in the container
specification, as shown in the following listing.
kind: Pod
spec:
  containers:
  - image: some/image
    command: ["/bin/command"]
    args: ["arg1", "arg2", "arg3"]
In most cases, you’ll only set custom arguments and rarely override the command
(except in general-purpose images such as busybox, which doesn’t define an ENTRY-
POINT at all). 
NOTE
The command and args fields can’t be updated after the pod is created.
The two Dockerfile instructions and the equivalent pod spec fields are shown in table 7.1.
RUNNING THE FORTUNE POD WITH A CUSTOM INTERVAL
To run the fortune pod with a custom delay interval, you’ll copy your fortune-
pod.yaml into fortune-pod-args.yaml and modify it as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: fortune2s        
Listing 7.3
A pod definition specifying a custom command and arguments
Table 7.1
Specifying the executable and its arguments in Docker vs Kubernetes
Docker
Kubernetes
Description
ENTRYPOINT
command
The executable that’s executed inside the container
CMD
args
The arguments passed to the executable
Listing 7.4
Passing an argument in the pod definition: fortune-pod-args.yaml
You changed the 
pod’s name.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="228">
  <data key="d0">Page_228</data>
  <data key="d5">Page_228</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_139">
  <data key="d0">196
CHAPTER 7
ConfigMaps and Secrets: configuring applications
spec:
  containers:
  - image: luksa/fortune:args      
    args: ["2"]                  
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
...
You added the args array to the container definition. Try creating this pod now. The
values of the array will be passed to the container as command-line arguments when it
is run. 
 The array notation used in this listing is great if you have one argument or a few. If
you have several, you can also use the following notation:
    args:
    - foo
    - bar
    - "15"
TIP
You don’t need to enclose string values in quotations marks (but you
must enclose numbers). 
Specifying arguments is one way of passing config
options to your containers through command-
line arguments. Next, you’ll see how to do it
through environment variables.
7.3
Setting environment variables for 
a container
As I’ve already mentioned, containerized appli-
cations often use environment variables as a
source of configuration options. Kubernetes
allows you to specify a custom list of environ-
ment variables for each container of a pod, as
shown in figure 7.1. Although it would be use-
ful to also define environment variables at the
pod level and have them be inherited by its
containers, no such option currently exists.
NOTE
Like the container’s command and
arguments, the list of environment variables
also cannot be updated after the pod is created.
Using fortune:args 
instead of fortune:latest
This argument makes the 
script generate a new fortune 
every two seconds.
Pod
Container A
Environment variables
FOO=BAR
ABC=123
Container B
Environment variables
FOO=FOOBAR
BAR=567
Figure 7.1
Environment variables can 
be set per container.
 
</data>
  <data key="d5">196
CHAPTER 7
ConfigMaps and Secrets: configuring applications
spec:
  containers:
  - image: luksa/fortune:args      
    args: ["2"]                  
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
...
You added the args array to the container definition. Try creating this pod now. The
values of the array will be passed to the container as command-line arguments when it
is run. 
 The array notation used in this listing is great if you have one argument or a few. If
you have several, you can also use the following notation:
    args:
    - foo
    - bar
    - "15"
TIP
You don’t need to enclose string values in quotations marks (but you
must enclose numbers). 
Specifying arguments is one way of passing config
options to your containers through command-
line arguments. Next, you’ll see how to do it
through environment variables.
7.3
Setting environment variables for 
a container
As I’ve already mentioned, containerized appli-
cations often use environment variables as a
source of configuration options. Kubernetes
allows you to specify a custom list of environ-
ment variables for each container of a pod, as
shown in figure 7.1. Although it would be use-
ful to also define environment variables at the
pod level and have them be inherited by its
containers, no such option currently exists.
NOTE
Like the container’s command and
arguments, the list of environment variables
also cannot be updated after the pod is created.
Using fortune:args 
instead of fortune:latest
This argument makes the 
script generate a new fortune 
every two seconds.
Pod
Container A
Environment variables
FOO=BAR
ABC=123
Container B
Environment variables
FOO=FOOBAR
BAR=567
Figure 7.1
Environment variables can 
be set per container.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="229">
  <data key="d0">Page_229</data>
  <data key="d5">Page_229</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_140">
  <data key="d0">197
Setting environment variables for a container
MAKING THE INTERVAL IN YOUR FORTUNE IMAGE CONFIGURABLE THROUGH AN ENVIRONMENT VARIABLE
Let’s see how to modify your fortuneloop.sh script once again to allow it to be config-
ured from an environment variable, as shown in the following listing.
#!/bin/bash
trap "exit" SIGINT
echo Configured to generate new fortune every $INTERVAL seconds
mkdir -p /var/htdocs
while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune &gt; /var/htdocs/index.html
  sleep $INTERVAL
done
All you had to do was remove the row where the INTERVAL variable is initialized. Because
your “app” is a simple bash script, you didn’t need to do anything else. If the app was
written in Java you’d use System.getenv("INTERVAL"), whereas in Node.JS you’d use
process.env.INTERVAL, and in Python you’d use os.environ['INTERVAL'].
7.3.1
Specifying environment variables in a container definition
After building the new image (I’ve tagged it as luksa/fortune:env this time) and
pushing it to Docker Hub, you can run it by creating a new pod, in which you pass the
environment variable to the script by including it in your container definition, as
shown in the following listing.
kind: Pod
spec:
 containers:
 - image: luksa/fortune:env
   env:                        
   - name: INTERVAL            
     value: "30"               
   name: html-generator
...
As mentioned previously, you set the environment variable inside the container defini-
tion, not at the pod level. 
NOTE
Don’t forget that in each container, Kubernetes also automatically
exposes environment variables for each service in the same namespace. These
environment variables are basically auto-injected configuration.
Listing 7.5
Fortune script with interval configurable through env var: fortune-env/
fortuneloop.sh
Listing 7.6
Defining an environment variable in a pod: fortune-pod-env.yaml
Adding a single variable to 
the environment variable list
 
</data>
  <data key="d5">197
Setting environment variables for a container
MAKING THE INTERVAL IN YOUR FORTUNE IMAGE CONFIGURABLE THROUGH AN ENVIRONMENT VARIABLE
Let’s see how to modify your fortuneloop.sh script once again to allow it to be config-
ured from an environment variable, as shown in the following listing.
#!/bin/bash
trap "exit" SIGINT
echo Configured to generate new fortune every $INTERVAL seconds
mkdir -p /var/htdocs
while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune &gt; /var/htdocs/index.html
  sleep $INTERVAL
done
All you had to do was remove the row where the INTERVAL variable is initialized. Because
your “app” is a simple bash script, you didn’t need to do anything else. If the app was
written in Java you’d use System.getenv("INTERVAL"), whereas in Node.JS you’d use
process.env.INTERVAL, and in Python you’d use os.environ['INTERVAL'].
7.3.1
Specifying environment variables in a container definition
After building the new image (I’ve tagged it as luksa/fortune:env this time) and
pushing it to Docker Hub, you can run it by creating a new pod, in which you pass the
environment variable to the script by including it in your container definition, as
shown in the following listing.
kind: Pod
spec:
 containers:
 - image: luksa/fortune:env
   env:                        
   - name: INTERVAL            
     value: "30"               
   name: html-generator
...
As mentioned previously, you set the environment variable inside the container defini-
tion, not at the pod level. 
NOTE
Don’t forget that in each container, Kubernetes also automatically
exposes environment variables for each service in the same namespace. These
environment variables are basically auto-injected configuration.
Listing 7.5
Fortune script with interval configurable through env var: fortune-env/
fortuneloop.sh
Listing 7.6
Defining an environment variable in a pod: fortune-pod-env.yaml
Adding a single variable to 
the environment variable list
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="230">
  <data key="d0">Page_230</data>
  <data key="d5">Page_230</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_141">
  <data key="d0">198
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.3.2
Referring to other environment variables in a variable’s value
In the previous example, you set a fixed value for the environment variable, but you
can also reference previously defined environment variables or any other existing vari-
ables by using the $(VAR) syntax. If you define two environment variables, the second
one can include the value of the first one as shown in the following listing.
env:
- name: FIRST_VAR
  value: "foo"
- name: SECOND_VAR
  value: "$(FIRST_VAR)bar"
In this case, the SECOND_VAR’s value will be "foobar". Similarly, both the command and
args attributes you learned about in section 7.2 can also refer to environment vari-
ables like this. You’ll use this method in section 7.4.5.
7.3.3
Understanding the drawback of hardcoding environment 
variables
Having values effectively hardcoded in the pod definition means you need to have
separate pod definitions for your production and your development pods. To reuse
the same pod definition in multiple environments, it makes sense to decouple the
configuration from the pod descriptor. Luckily, you can do that using a ConfigMap
resource and using it as a source for environment variable values using the valueFrom
instead of the value field. You’ll learn about this next. 
7.4
Decoupling configuration with a ConfigMap
The whole point of an app’s configuration is to keep the config options that vary
between environments, or change frequently, separate from the application’s source
code. If you think of a pod descriptor as source code for your app (and in microservices
architectures that’s what it really is, because it defines how to compose the individual
components into a functioning system), it’s clear you should move the configuration
out of the pod description.
7.4.1
Introducing ConfigMaps
Kubernetes allows separating configuration options into a separate object called a
ConfigMap, which is a map containing key/value pairs with the values ranging from
short literals to full config files. 
 An application doesn’t need to read the ConfigMap directly or even know that it
exists. The contents of the map are instead passed to containers as either environ-
ment variables or as files in a volume (see figure 7.2). And because environment
Listing 7.7
Referring to an environment variable inside another one
 
</data>
  <data key="d5">198
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.3.2
Referring to other environment variables in a variable’s value
In the previous example, you set a fixed value for the environment variable, but you
can also reference previously defined environment variables or any other existing vari-
ables by using the $(VAR) syntax. If you define two environment variables, the second
one can include the value of the first one as shown in the following listing.
env:
- name: FIRST_VAR
  value: "foo"
- name: SECOND_VAR
  value: "$(FIRST_VAR)bar"
In this case, the SECOND_VAR’s value will be "foobar". Similarly, both the command and
args attributes you learned about in section 7.2 can also refer to environment vari-
ables like this. You’ll use this method in section 7.4.5.
7.3.3
Understanding the drawback of hardcoding environment 
variables
Having values effectively hardcoded in the pod definition means you need to have
separate pod definitions for your production and your development pods. To reuse
the same pod definition in multiple environments, it makes sense to decouple the
configuration from the pod descriptor. Luckily, you can do that using a ConfigMap
resource and using it as a source for environment variable values using the valueFrom
instead of the value field. You’ll learn about this next. 
7.4
Decoupling configuration with a ConfigMap
The whole point of an app’s configuration is to keep the config options that vary
between environments, or change frequently, separate from the application’s source
code. If you think of a pod descriptor as source code for your app (and in microservices
architectures that’s what it really is, because it defines how to compose the individual
components into a functioning system), it’s clear you should move the configuration
out of the pod description.
7.4.1
Introducing ConfigMaps
Kubernetes allows separating configuration options into a separate object called a
ConfigMap, which is a map containing key/value pairs with the values ranging from
short literals to full config files. 
 An application doesn’t need to read the ConfigMap directly or even know that it
exists. The contents of the map are instead passed to containers as either environ-
ment variables or as files in a volume (see figure 7.2). And because environment
Listing 7.7
Referring to an environment variable inside another one
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="231">
  <data key="d0">Page_231</data>
  <data key="d5">Page_231</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_142">
  <data key="d0">199
Decoupling configuration with a ConfigMap
variables can be referenced in command-line arguments using the $(ENV_VAR) syn-
tax, you can also pass ConfigMap entries to processes as command-line arguments.
Sure, the application can also read the contents of a ConfigMap directly through the
Kubernetes REST API endpoint if needed, but unless you have a real need for this,
you should keep your app Kubernetes-agnostic as much as possible.
 Regardless of how an app consumes a ConfigMap, having the config in a separate
standalone object like this allows you to keep multiple manifests for ConfigMaps with
the same name, each for a different environment (development, testing, QA, produc-
tion, and so on). Because pods reference the ConfigMap by name, you can use a dif-
ferent config in each environment while using the same pod specification across all of
them (see figure 7.3).
Pod
Environment variables
ConﬁgMap
key1=value1
key2=value2
...
conﬁgMap
volume
Figure 7.2
Pods use ConfigMaps 
through environment variables and 
configMap volumes.
ConﬁgMap:
app-conﬁg
Namespace: development
(contains
development
values)
Pod(s)
ConﬁgMaps created
from different manifests
Pods created from the
same pod manifests
Namespace: production
ConﬁgMap:
app-conﬁg
(contains
production
values)
Pod(s)
Figure 7.3
Two different ConfigMaps with the same name used in different 
environments
 
</data>
  <data key="d5">199
Decoupling configuration with a ConfigMap
variables can be referenced in command-line arguments using the $(ENV_VAR) syn-
tax, you can also pass ConfigMap entries to processes as command-line arguments.
Sure, the application can also read the contents of a ConfigMap directly through the
Kubernetes REST API endpoint if needed, but unless you have a real need for this,
you should keep your app Kubernetes-agnostic as much as possible.
 Regardless of how an app consumes a ConfigMap, having the config in a separate
standalone object like this allows you to keep multiple manifests for ConfigMaps with
the same name, each for a different environment (development, testing, QA, produc-
tion, and so on). Because pods reference the ConfigMap by name, you can use a dif-
ferent config in each environment while using the same pod specification across all of
them (see figure 7.3).
Pod
Environment variables
ConﬁgMap
key1=value1
key2=value2
...
conﬁgMap
volume
Figure 7.2
Pods use ConfigMaps 
through environment variables and 
configMap volumes.
ConﬁgMap:
app-conﬁg
Namespace: development
(contains
development
values)
Pod(s)
ConﬁgMaps created
from different manifests
Pods created from the
same pod manifests
Namespace: production
ConﬁgMap:
app-conﬁg
(contains
production
values)
Pod(s)
Figure 7.3
Two different ConfigMaps with the same name used in different 
environments
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="232">
  <data key="d0">Page_232</data>
  <data key="d5">Page_232</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_143">
  <data key="d0">200
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.4.2
Creating a ConfigMap
Let’s see how to use a ConfigMap in one of your pods. To start with the simplest exam-
ple, you’ll first create a map with a single key and use it to fill the INTERVAL environment
variable from your previous example. You’ll create the ConfigMap with the special
kubectl create configmap command instead of posting a YAML with the generic
kubectl create -f command. 
USING THE KUBECTL CREATE CONFIGMAP COMMAND
You can define the map’s entries by passing literals to the kubectl command or you
can create the ConfigMap from files stored on your disk. Use a simple literal first:
$ kubectl create configmap fortune-config --from-literal=sleep-interval=25
configmap "fortune-config" created
NOTE
ConfigMap keys must be a valid DNS subdomain (they may only con-
tain alphanumeric characters, dashes, underscores, and dots). They may
optionally include a leading dot.
This creates a ConfigMap called fortune-config with the single-entry sleep-interval
=25 (figure 7.4).
ConfigMaps usually contain more than one entry. To create a ConfigMap with multi-
ple literal entries, you add multiple --from-literal arguments:
$ kubectl create configmap myconfigmap
➥  --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two
Let’s inspect the YAML descriptor of the ConfigMap you created by using the kubectl
get command, as shown in the following listing.
$ kubectl get configmap fortune-config -o yaml
apiVersion: v1
data:
  sleep-interval: "25"                      
kind: ConfigMap                              
metadata:
  creationTimestamp: 2016-08-11T20:31:08Z
  name: fortune-config                      
  namespace: default
  resourceVersion: "910025"
  selfLink: /api/v1/namespaces/default/configmaps/fortune-config
  uid: 88c4167e-6002-11e6-a50d-42010af00237
Listing 7.8
A ConfigMap definition
sleep-interval
25
ConﬁgMap: fortune-conﬁg
Figure 7.4
The fortune-config 
ConfigMap containing a single entry
The single entry 
in this map
This descriptor 
describes a ConfigMap.
The name of this map 
(you’re referencing it 
by this name)
 
</data>
  <data key="d5">200
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.4.2
Creating a ConfigMap
Let’s see how to use a ConfigMap in one of your pods. To start with the simplest exam-
ple, you’ll first create a map with a single key and use it to fill the INTERVAL environment
variable from your previous example. You’ll create the ConfigMap with the special
kubectl create configmap command instead of posting a YAML with the generic
kubectl create -f command. 
USING THE KUBECTL CREATE CONFIGMAP COMMAND
You can define the map’s entries by passing literals to the kubectl command or you
can create the ConfigMap from files stored on your disk. Use a simple literal first:
$ kubectl create configmap fortune-config --from-literal=sleep-interval=25
configmap "fortune-config" created
NOTE
ConfigMap keys must be a valid DNS subdomain (they may only con-
tain alphanumeric characters, dashes, underscores, and dots). They may
optionally include a leading dot.
This creates a ConfigMap called fortune-config with the single-entry sleep-interval
=25 (figure 7.4).
ConfigMaps usually contain more than one entry. To create a ConfigMap with multi-
ple literal entries, you add multiple --from-literal arguments:
$ kubectl create configmap myconfigmap
➥  --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two
Let’s inspect the YAML descriptor of the ConfigMap you created by using the kubectl
get command, as shown in the following listing.
$ kubectl get configmap fortune-config -o yaml
apiVersion: v1
data:
  sleep-interval: "25"                      
kind: ConfigMap                              
metadata:
  creationTimestamp: 2016-08-11T20:31:08Z
  name: fortune-config                      
  namespace: default
  resourceVersion: "910025"
  selfLink: /api/v1/namespaces/default/configmaps/fortune-config
  uid: 88c4167e-6002-11e6-a50d-42010af00237
Listing 7.8
A ConfigMap definition
sleep-interval
25
ConﬁgMap: fortune-conﬁg
Figure 7.4
The fortune-config 
ConfigMap containing a single entry
The single entry 
in this map
This descriptor 
describes a ConfigMap.
The name of this map 
(you’re referencing it 
by this name)
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="233">
  <data key="d0">Page_233</data>
  <data key="d5">Page_233</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_144">
  <data key="d0">201
Decoupling configuration with a ConfigMap
Nothing extraordinary. You could easily have written this YAML yourself (you wouldn’t
need to specify anything but the name in the metadata section, of course) and posted
it to the Kubernetes API with the well-known
$ kubectl create -f fortune-config.yaml
CREATING A CONFIGMAP ENTRY FROM THE CONTENTS OF A FILE
ConfigMaps can also store coarse-grained config data, such as complete config files.
To do this, the kubectl create configmap command also supports reading files from
disk and storing them as individual entries in the ConfigMap:
$ kubectl create configmap my-config --from-file=config-file.conf
When you run the previous command, kubectl looks for the file config-file.conf in
the directory you run kubectl in. It will then store the contents of the file under the
key config-file.conf in the ConfigMap (the filename is used as the map key), but
you can also specify a key manually like this:
$ kubectl create configmap my-config --from-file=customkey=config-file.conf
This command will store the file’s contents under the key customkey. As with literals,
you can add multiple files by using the --from-file argument multiple times. 
CREATING A CONFIGMAP FROM FILES IN A DIRECTORY
Instead of importing each file individually, you can even import all files from a file
directory:
$ kubectl create configmap my-config --from-file=/path/to/dir
In this case, kubectl will create an individual map entry for each file in the specified
directory, but only for files whose name is a valid ConfigMap key. 
COMBINING DIFFERENT OPTIONS
When creating ConfigMaps, you can use a combination of all the options mentioned
here (note that these files aren’t included in the book’s code archive—you can create
them yourself if you’d like to try out the command):
$ kubectl create configmap my-config  
➥  --from-file=foo.json                  
➥  --from-file=bar=foobar.conf              
➥  --from-file=config-opts/               
➥  --from-literal=some=thing    
Here, you’ve created the ConfigMap from multiple sources: a whole directory, a file,
another file (but stored under a custom key instead of using the filename as the key),
and a literal value. Figure 7.5 shows all these sources and the resulting ConfigMap.
A single file
A file stored under 
a custom key
A whole directory
A literal value
 
</data>
  <data key="d5">201
Decoupling configuration with a ConfigMap
Nothing extraordinary. You could easily have written this YAML yourself (you wouldn’t
need to specify anything but the name in the metadata section, of course) and posted
it to the Kubernetes API with the well-known
$ kubectl create -f fortune-config.yaml
CREATING A CONFIGMAP ENTRY FROM THE CONTENTS OF A FILE
ConfigMaps can also store coarse-grained config data, such as complete config files.
To do this, the kubectl create configmap command also supports reading files from
disk and storing them as individual entries in the ConfigMap:
$ kubectl create configmap my-config --from-file=config-file.conf
When you run the previous command, kubectl looks for the file config-file.conf in
the directory you run kubectl in. It will then store the contents of the file under the
key config-file.conf in the ConfigMap (the filename is used as the map key), but
you can also specify a key manually like this:
$ kubectl create configmap my-config --from-file=customkey=config-file.conf
This command will store the file’s contents under the key customkey. As with literals,
you can add multiple files by using the --from-file argument multiple times. 
CREATING A CONFIGMAP FROM FILES IN A DIRECTORY
Instead of importing each file individually, you can even import all files from a file
directory:
$ kubectl create configmap my-config --from-file=/path/to/dir
In this case, kubectl will create an individual map entry for each file in the specified
directory, but only for files whose name is a valid ConfigMap key. 
COMBINING DIFFERENT OPTIONS
When creating ConfigMaps, you can use a combination of all the options mentioned
here (note that these files aren’t included in the book’s code archive—you can create
them yourself if you’d like to try out the command):
$ kubectl create configmap my-config  
➥  --from-file=foo.json                  
➥  --from-file=bar=foobar.conf              
➥  --from-file=config-opts/               
➥  --from-literal=some=thing    
Here, you’ve created the ConfigMap from multiple sources: a whole directory, a file,
another file (but stored under a custom key instead of using the filename as the key),
and a literal value. Figure 7.5 shows all these sources and the resulting ConfigMap.
A single file
A file stored under 
a custom key
A whole directory
A literal value
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="234">
  <data key="d0">Page_234</data>
  <data key="d5">Page_234</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_145">
  <data key="d0">202
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.4.3
Passing a ConfigMap entry to a container as an environment 
variable
How do you now get the values from this map into a pod’s container? You have three
options. Let’s start with the simplest—setting an environment variable. You’ll use the
valueFrom field I mentioned in section 7.3.3. The pod descriptor should look like
the following listing.
apiVersion: v1
kind: Pod
Listing 7.9
Pod with env var from a config map: fortune-pod-env-configmap.yaml
ConﬁgMap: my-conﬁg
Key
foo.json
foo.json
Value
bar
abc
debug
true
repeat
100
some
thing
{
foo: bar
baz: 5
}
conﬁg-opts directory
Literal
some=thing
{
foo: bar
baz: 5
}
--from-ﬁle=foo.json
--from-ﬁle=conﬁg-opts/
--from-literal=some=thing
foobar.conf
abc
debug
true
repeat
100
--from-ﬁle=bar=foobar.conf
Figure 7.5
Creating a ConfigMap from individual files, a directory, and a literal value
 
</data>
  <data key="d5">202
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.4.3
Passing a ConfigMap entry to a container as an environment 
variable
How do you now get the values from this map into a pod’s container? You have three
options. Let’s start with the simplest—setting an environment variable. You’ll use the
valueFrom field I mentioned in section 7.3.3. The pod descriptor should look like
the following listing.
apiVersion: v1
kind: Pod
Listing 7.9
Pod with env var from a config map: fortune-pod-env-configmap.yaml
ConﬁgMap: my-conﬁg
Key
foo.json
foo.json
Value
bar
abc
debug
true
repeat
100
some
thing
{
foo: bar
baz: 5
}
conﬁg-opts directory
Literal
some=thing
{
foo: bar
baz: 5
}
--from-ﬁle=foo.json
--from-ﬁle=conﬁg-opts/
--from-literal=some=thing
foobar.conf
abc
debug
true
repeat
100
--from-ﬁle=bar=foobar.conf
Figure 7.5
Creating a ConfigMap from individual files, a directory, and a literal value
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="235">
  <data key="d0">Page_235</data>
  <data key="d5">Page_235</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_146">
  <data key="d0">203
Decoupling configuration with a ConfigMap
metadata:
  name: fortune-env-from-configmap
spec:
  containers:
  - image: luksa/fortune:env
    env:                             
    - name: INTERVAL                 
      valueFrom:                       
        configMapKeyRef:               
          name: fortune-config      
          key: sleep-interval    
...
You defined an environment variable called INTERVAL and set its value to whatever is
stored in the fortune-config ConfigMap under the key sleep-interval. When the
process running in the html-generator container reads the INTERVAL environment
variable, it will see the value 25 (shown in figure 7.6).
REFERENCING NON-EXISTING CONFIGMAPS IN A POD
You might wonder what happens if the referenced ConfigMap doesn’t exist when you
create the pod. Kubernetes schedules the pod normally and tries to run its containers.
The container referencing the non-existing ConfigMap will fail to start, but the other
container will start normally. If you then create the missing ConfigMap, the failed con-
tainer is started without requiring you to recreate the pod.
NOTE
You can also mark a reference to a ConfigMap as optional (by setting
configMapKeyRef.optional: true). In that case, the container starts even if
the ConfigMap doesn’t exist.
This example shows you how to decouple the configuration from the pod specifica-
tion. This allows you to keep all the configuration options closely together (even for
multiple pods) instead of having them splattered around the pod definition (or dupli-
cated across multiple pod manifests). 
You’re setting the environment 
variable called INTERVAL.
Instead of setting a fixed value, you're 
initializing it from a ConfigMap key.
The name of the ConfigMap 
you're referencing
You're setting the variable to whatever is
stored under this key in the ConfigMap.
ConﬁgMap: fortune-conﬁg
sleep-interval
25
Pod
Container: web-server
Container: html-generator
Environment variables
INTERVAL=25
fortuneloop.sh
process
Figure 7.6
Passing a ConfigMap entry as 
an environment variable to a container
 
</data>
  <data key="d5">203
Decoupling configuration with a ConfigMap
metadata:
  name: fortune-env-from-configmap
spec:
  containers:
  - image: luksa/fortune:env
    env:                             
    - name: INTERVAL                 
      valueFrom:                       
        configMapKeyRef:               
          name: fortune-config      
          key: sleep-interval    
...
You defined an environment variable called INTERVAL and set its value to whatever is
stored in the fortune-config ConfigMap under the key sleep-interval. When the
process running in the html-generator container reads the INTERVAL environment
variable, it will see the value 25 (shown in figure 7.6).
REFERENCING NON-EXISTING CONFIGMAPS IN A POD
You might wonder what happens if the referenced ConfigMap doesn’t exist when you
create the pod. Kubernetes schedules the pod normally and tries to run its containers.
The container referencing the non-existing ConfigMap will fail to start, but the other
container will start normally. If you then create the missing ConfigMap, the failed con-
tainer is started without requiring you to recreate the pod.
NOTE
You can also mark a reference to a ConfigMap as optional (by setting
configMapKeyRef.optional: true). In that case, the container starts even if
the ConfigMap doesn’t exist.
This example shows you how to decouple the configuration from the pod specifica-
tion. This allows you to keep all the configuration options closely together (even for
multiple pods) instead of having them splattered around the pod definition (or dupli-
cated across multiple pod manifests). 
You’re setting the environment 
variable called INTERVAL.
Instead of setting a fixed value, you're 
initializing it from a ConfigMap key.
The name of the ConfigMap 
you're referencing
You're setting the variable to whatever is
stored under this key in the ConfigMap.
ConﬁgMap: fortune-conﬁg
sleep-interval
25
Pod
Container: web-server
Container: html-generator
Environment variables
INTERVAL=25
fortuneloop.sh
process
Figure 7.6
Passing a ConfigMap entry as 
an environment variable to a container
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="236">
  <data key="d0">Page_236</data>
  <data key="d5">Page_236</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_147">
  <data key="d0">204
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.4.4
Passing all entries of a ConfigMap as environment variables 
at once
When your ConfigMap contains more than just a few entries, it becomes tedious and
error-prone to create environment variables from each entry individually. Luckily,
Kubernetes version 1.6 provides a way to expose all entries of a ConfigMap as environ-
ment variables. 
 Imagine having a ConfigMap with three keys called FOO, BAR, and FOO-BAR. You can
expose them all as environment variables by using the envFrom attribute, instead of
env the way you did in previous examples. The following listing shows an example.
spec:
  containers:
  - image: some-image
    envFrom:                
    - prefix: CONFIG_             
      configMapRef:              
        name: my-config-map      
...
As you can see, you can also specify a prefix for the environment variables (CONFIG_ in
this case). This results in the following two environment variables being present inside
the container: CONFIG_FOO and CONFIG_BAR. 
NOTE
The prefix is optional, so if you omit it the environment variables will
have the same name as the keys. 
Did you notice I said two variables, but earlier, I said the ConfigMap has three entries
(FOO, BAR, and FOO-BAR)? Why is there no environment variable for the FOO-BAR
ConfigMap entry?
 The reason is that CONFIG_FOO-BAR isn’t a valid environment variable name
because it contains a dash. Kubernetes doesn’t convert the keys in any way (it doesn’t
convert dashes to underscores, for example). If a ConfigMap key isn’t in the proper
format, it skips the entry (but it does record an event informing you it skipped it).
7.4.5
Passing a ConfigMap entry as a command-line argument
Now, let’s also look at how to pass values from a ConfigMap as arguments to the main
process running in the container. You can’t reference ConfigMap entries directly in
the pod.spec.containers.args field, but you can first initialize an environment vari-
able from the ConfigMap entry and then refer to the variable inside the arguments as
shown in figure 7.7.
 Listing 7.11 shows an example of how to do this in the YAML.
 
Listing 7.10
Pod with env vars from all entries of a ConfigMap
Using envFrom instead of env
All environment variables will 
be prefixed with CONFIG_.
Referencing the ConfigMap 
called my-config-map
 
</data>
  <data key="d5">204
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.4.4
Passing all entries of a ConfigMap as environment variables 
at once
When your ConfigMap contains more than just a few entries, it becomes tedious and
error-prone to create environment variables from each entry individually. Luckily,
Kubernetes version 1.6 provides a way to expose all entries of a ConfigMap as environ-
ment variables. 
 Imagine having a ConfigMap with three keys called FOO, BAR, and FOO-BAR. You can
expose them all as environment variables by using the envFrom attribute, instead of
env the way you did in previous examples. The following listing shows an example.
spec:
  containers:
  - image: some-image
    envFrom:                
    - prefix: CONFIG_             
      configMapRef:              
        name: my-config-map      
...
As you can see, you can also specify a prefix for the environment variables (CONFIG_ in
this case). This results in the following two environment variables being present inside
the container: CONFIG_FOO and CONFIG_BAR. 
NOTE
The prefix is optional, so if you omit it the environment variables will
have the same name as the keys. 
Did you notice I said two variables, but earlier, I said the ConfigMap has three entries
(FOO, BAR, and FOO-BAR)? Why is there no environment variable for the FOO-BAR
ConfigMap entry?
 The reason is that CONFIG_FOO-BAR isn’t a valid environment variable name
because it contains a dash. Kubernetes doesn’t convert the keys in any way (it doesn’t
convert dashes to underscores, for example). If a ConfigMap key isn’t in the proper
format, it skips the entry (but it does record an event informing you it skipped it).
7.4.5
Passing a ConfigMap entry as a command-line argument
Now, let’s also look at how to pass values from a ConfigMap as arguments to the main
process running in the container. You can’t reference ConfigMap entries directly in
the pod.spec.containers.args field, but you can first initialize an environment vari-
able from the ConfigMap entry and then refer to the variable inside the arguments as
shown in figure 7.7.
 Listing 7.11 shows an example of how to do this in the YAML.
 
Listing 7.10
Pod with env vars from all entries of a ConfigMap
Using envFrom instead of env
All environment variables will 
be prefixed with CONFIG_.
Referencing the ConfigMap 
called my-config-map
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="237">
  <data key="d0">Page_237</data>
  <data key="d5">Page_237</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_148">
  <data key="d0">205
Decoupling configuration with a ConfigMap
apiVersion: v1
kind: Pod
metadata:
  name: fortune-args-from-configmap
spec:
  containers:
  - image: luksa/fortune:args         
    env:                               
    - name: INTERVAL                   
      valueFrom:                       
        configMapKeyRef:               
          name: fortune-config         
          key: sleep-interval          
    args: ["$(INTERVAL)"]      
...
You defined the environment variable exactly as you did before, but then you used the
$(ENV_VARIABLE_NAME) syntax to have Kubernetes inject the value of the variable into
the argument. 
7.4.6
Using a configMap volume to expose ConfigMap entries as files
Passing configuration options as environment variables or command-line arguments
is usually used for short variable values. A ConfigMap, as you’ve seen, can also con-
tain whole config files. When you want to expose those to the container, you can use
one of the special volume types I mentioned in the previous chapter, namely a
configMap volume.
 A configMap volume will expose each entry of the ConfigMap as a file. The pro-
cess running in the container can obtain the entry’s value by reading the contents of
the file.
Listing 7.11
Using ConfigMap entries as arguments: fortune-pod-args-configmap.yaml
ConﬁgMap: fortune-conﬁg
sleep-interval
25
Pod
Container: web-server
Container: html-generator
Environment variables
INTERVAL=25
fortuneloop.sh $(INTERVAL)
Figure 7.7
Passing a ConfigMap entry as a command-line argument
Using the image that takes the 
interval from the first argument, 
not from an environment variable
Defining the 
environment variable 
exactly as before
Referencing the environment 
variable in the argument
 
</data>
  <data key="d5">205
Decoupling configuration with a ConfigMap
apiVersion: v1
kind: Pod
metadata:
  name: fortune-args-from-configmap
spec:
  containers:
  - image: luksa/fortune:args         
    env:                               
    - name: INTERVAL                   
      valueFrom:                       
        configMapKeyRef:               
          name: fortune-config         
          key: sleep-interval          
    args: ["$(INTERVAL)"]      
...
You defined the environment variable exactly as you did before, but then you used the
$(ENV_VARIABLE_NAME) syntax to have Kubernetes inject the value of the variable into
the argument. 
7.4.6
Using a configMap volume to expose ConfigMap entries as files
Passing configuration options as environment variables or command-line arguments
is usually used for short variable values. A ConfigMap, as you’ve seen, can also con-
tain whole config files. When you want to expose those to the container, you can use
one of the special volume types I mentioned in the previous chapter, namely a
configMap volume.
 A configMap volume will expose each entry of the ConfigMap as a file. The pro-
cess running in the container can obtain the entry’s value by reading the contents of
the file.
Listing 7.11
Using ConfigMap entries as arguments: fortune-pod-args-configmap.yaml
ConﬁgMap: fortune-conﬁg
sleep-interval
25
Pod
Container: web-server
Container: html-generator
Environment variables
INTERVAL=25
fortuneloop.sh $(INTERVAL)
Figure 7.7
Passing a ConfigMap entry as a command-line argument
Using the image that takes the 
interval from the first argument, 
not from an environment variable
Defining the 
environment variable 
exactly as before
Referencing the environment 
variable in the argument
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="238">
  <data key="d0">Page_238</data>
  <data key="d5">Page_238</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_149">
  <data key="d0">206
CHAPTER 7
ConfigMaps and Secrets: configuring applications
 Although this method is mostly meant for passing large config files to the con-
tainer, nothing prevents you from passing short single values this way. 
CREATING THE CONFIGMAP
Instead of modifying your fortuneloop.sh script once again, you’ll now try a different
example. You’ll use a config file to configure the Nginx web server running inside the
fortune pod’s web-server container. Let’s say you want your Nginx server to compress
responses it sends to the client. To enable compression, the config file for Nginx
needs to look like the following listing.
server {
  listen              80;
  server_name         www.kubia-example.com;
  gzip on;                                       
  gzip_types text/plain application/xml;         
  location / {
    root   /usr/share/nginx/html;
    index  index.html index.htm;
  }
}
Now delete your existing fortune-config ConfigMap with kubectl delete config-
map fortune-config, so that you can replace it with a new one, which will include the
Nginx config file. You’ll create the ConfigMap from files stored on your local disk. 
 Create a new directory called configmap-files and store the Nginx config from the
previous listing into configmap-files/my-nginx-config.conf. To make the ConfigMap
also contain the sleep-interval entry, add a plain text file called sleep-interval to the
same directory and store the number 25 in it (see figure 7.8).
Now create a ConfigMap from all the files in the directory like this:
$ kubectl create configmap fortune-config --from-file=configmap-files
configmap "fortune-config" created
Listing 7.12
An Nginx config with enabled gzip compression: my-nginx-config.conf
This enables gzip compression 
for plain text and XML files.
conﬁgmap-ﬁles/
my-nginx-conﬁg.conf
server {
listen 80;
server_name www.kubia...
...
}
sleep-interval
25
Figure 7.8
The contents of the 
configmap-files directory and its files
 
</data>
  <data key="d5">206
CHAPTER 7
ConfigMaps and Secrets: configuring applications
 Although this method is mostly meant for passing large config files to the con-
tainer, nothing prevents you from passing short single values this way. 
CREATING THE CONFIGMAP
Instead of modifying your fortuneloop.sh script once again, you’ll now try a different
example. You’ll use a config file to configure the Nginx web server running inside the
fortune pod’s web-server container. Let’s say you want your Nginx server to compress
responses it sends to the client. To enable compression, the config file for Nginx
needs to look like the following listing.
server {
  listen              80;
  server_name         www.kubia-example.com;
  gzip on;                                       
  gzip_types text/plain application/xml;         
  location / {
    root   /usr/share/nginx/html;
    index  index.html index.htm;
  }
}
Now delete your existing fortune-config ConfigMap with kubectl delete config-
map fortune-config, so that you can replace it with a new one, which will include the
Nginx config file. You’ll create the ConfigMap from files stored on your local disk. 
 Create a new directory called configmap-files and store the Nginx config from the
previous listing into configmap-files/my-nginx-config.conf. To make the ConfigMap
also contain the sleep-interval entry, add a plain text file called sleep-interval to the
same directory and store the number 25 in it (see figure 7.8).
Now create a ConfigMap from all the files in the directory like this:
$ kubectl create configmap fortune-config --from-file=configmap-files
configmap "fortune-config" created
Listing 7.12
An Nginx config with enabled gzip compression: my-nginx-config.conf
This enables gzip compression 
for plain text and XML files.
conﬁgmap-ﬁles/
my-nginx-conﬁg.conf
server {
listen 80;
server_name www.kubia...
...
}
sleep-interval
25
Figure 7.8
The contents of the 
configmap-files directory and its files
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="239">
  <data key="d0">Page_239</data>
  <data key="d5">Page_239</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_150">
  <data key="d0">207
Decoupling configuration with a ConfigMap
The following listing shows what the YAML of this ConfigMap looks like.
$ kubectl get configmap fortune-config -o yaml
apiVersion: v1
data:
  my-nginx-config.conf: |                            
    server {                                         
      listen              80;                        
      server_name         www.kubia-example.com;     
      gzip on;                                       
      gzip_types text/plain application/xml;         
      location / {                                   
        root   /usr/share/nginx/html;                
        index  index.html index.htm;                 
      }                                              
    }                                                
  sleep-interval: |         
    25                      
kind: ConfigMap
...
NOTE
The pipeline character after the colon in the first line of both entries
signals that a literal multi-line value follows.
The ConfigMap contains two entries, with keys corresponding to the actual names
of the files they were created from. You’ll now use the ConfigMap in both of your
pod’s containers.
USING THE CONFIGMAP'S ENTRIES IN A VOLUME
Creating a volume populated with the contents of a ConfigMap is as easy as creating
a volume that references the ConfigMap by name and mounting the volume in a
container. You already learned how to create volumes and mount them, so the only
thing left to learn is how to initialize the volume with files created from a Config-
Map’s entries.
 Nginx reads its config file from /etc/nginx/nginx.conf. The Nginx image
already contains this file with default configuration options, which you don’t want
to override, so you don’t want to replace this file as a whole. Luckily, the default
config file automatically includes all .conf files in the /etc/nginx/conf.d/ subdirec-
tory as well, so you should add your config file in there. Figure 7.9 shows what you
want to achieve.
 The pod descriptor is shown in listing 7.14 (the irrelevant parts are omitted, but
you’ll find the complete file in the code archive).
 
 
Listing 7.13
YAML definition of a config map created from a file
The entry holding the 
Nginx config file’s 
contents
The sleep-interval entry
 
</data>
  <data key="d5">207
Decoupling configuration with a ConfigMap
The following listing shows what the YAML of this ConfigMap looks like.
$ kubectl get configmap fortune-config -o yaml
apiVersion: v1
data:
  my-nginx-config.conf: |                            
    server {                                         
      listen              80;                        
      server_name         www.kubia-example.com;     
      gzip on;                                       
      gzip_types text/plain application/xml;         
      location / {                                   
        root   /usr/share/nginx/html;                
        index  index.html index.htm;                 
      }                                              
    }                                                
  sleep-interval: |         
    25                      
kind: ConfigMap
...
NOTE
The pipeline character after the colon in the first line of both entries
signals that a literal multi-line value follows.
The ConfigMap contains two entries, with keys corresponding to the actual names
of the files they were created from. You’ll now use the ConfigMap in both of your
pod’s containers.
USING THE CONFIGMAP'S ENTRIES IN A VOLUME
Creating a volume populated with the contents of a ConfigMap is as easy as creating
a volume that references the ConfigMap by name and mounting the volume in a
container. You already learned how to create volumes and mount them, so the only
thing left to learn is how to initialize the volume with files created from a Config-
Map’s entries.
 Nginx reads its config file from /etc/nginx/nginx.conf. The Nginx image
already contains this file with default configuration options, which you don’t want
to override, so you don’t want to replace this file as a whole. Luckily, the default
config file automatically includes all .conf files in the /etc/nginx/conf.d/ subdirec-
tory as well, so you should add your config file in there. Figure 7.9 shows what you
want to achieve.
 The pod descriptor is shown in listing 7.14 (the irrelevant parts are omitted, but
you’ll find the complete file in the code archive).
 
 
Listing 7.13
YAML definition of a config map created from a file
The entry holding the 
Nginx config file’s 
contents
The sleep-interval entry
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="240">
  <data key="d0">Page_240</data>
  <data key="d5">Page_240</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_151">
  <data key="d0">208
CHAPTER 7
ConfigMaps and Secrets: configuring applications
apiVersion: v1
kind: Pod
metadata:
  name: fortune-configmap-volume
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    ...
    - name: config
      mountPath: /etc/nginx/conf.d      
      readOnly: true
    ...
  volumes:
  ...
  - name: config              
    configMap:                 
      name: fortune-config     
  ...
This pod definition includes a volume, which references your fortune-config
ConfigMap. You mount the volume into the /etc/nginx/conf.d directory to make
Nginx use it. 
VERIFYING NGINX IS USING THE MOUNTED CONFIG FILE
The web server should now be configured to compress the responses it sends. You can
verify this by enabling port-forwarding from localhost:8080 to the pod’s port 80 and
checking the server’s response with curl, as shown in the following listing.
 
Listing 7.14
A pod with ConfigMap entries mounted as files: fortune-pod-configmap-
volume.yaml
Pod
Container: html-generator
Container: web-server
Filesystem
/
etc/
nginx/
conf.d/
ConﬁgMap: fortune-conﬁg
my-nginx-conﬁg.conf
server {
…
}
Volume:
conﬁg
Figure 7.9
Passing ConfigMap entries to a pod as files in a volume
You’re mounting the 
configMap volume at 
this location.
The volume refers to your 
fortune-config ConfigMap.
 
</data>
  <data key="d5">208
CHAPTER 7
ConfigMaps and Secrets: configuring applications
apiVersion: v1
kind: Pod
metadata:
  name: fortune-configmap-volume
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    ...
    - name: config
      mountPath: /etc/nginx/conf.d      
      readOnly: true
    ...
  volumes:
  ...
  - name: config              
    configMap:                 
      name: fortune-config     
  ...
This pod definition includes a volume, which references your fortune-config
ConfigMap. You mount the volume into the /etc/nginx/conf.d directory to make
Nginx use it. 
VERIFYING NGINX IS USING THE MOUNTED CONFIG FILE
The web server should now be configured to compress the responses it sends. You can
verify this by enabling port-forwarding from localhost:8080 to the pod’s port 80 and
checking the server’s response with curl, as shown in the following listing.
 
Listing 7.14
A pod with ConfigMap entries mounted as files: fortune-pod-configmap-
volume.yaml
Pod
Container: html-generator
Container: web-server
Filesystem
/
etc/
nginx/
conf.d/
ConﬁgMap: fortune-conﬁg
my-nginx-conﬁg.conf
server {
…
}
Volume:
conﬁg
Figure 7.9
Passing ConfigMap entries to a pod as files in a volume
You’re mounting the 
configMap volume at 
this location.
The volume refers to your 
fortune-config ConfigMap.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="241">
  <data key="d0">Page_241</data>
  <data key="d5">Page_241</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_152">
  <data key="d0">209
Decoupling configuration with a ConfigMap
$ kubectl port-forward fortune-configmap-volume 8080:80 &amp;
Forwarding from 127.0.0.1:8080 -&gt; 80
Forwarding from [::1]:8080 -&gt; 80
$ curl -H "Accept-Encoding: gzip" -I localhost:8080
HTTP/1.1 200 OK
Server: nginx/1.11.1
Date: Thu, 18 Aug 2016 11:52:57 GMT
Content-Type: text/html
Last-Modified: Thu, 18 Aug 2016 11:52:55 GMT
Connection: keep-alive
ETag: W/"57b5a197-37"
Content-Encoding: gzip           
EXAMINING THE MOUNTED CONFIGMAP VOLUME’S CONTENTS
The response shows you achieved what you wanted, but let’s look at what’s in the
/etc/nginx/conf.d directory now:
$ kubectl exec fortune-configmap-volume -c web-server ls /etc/nginx/conf.d
my-nginx-config.conf
sleep-interval
Both entries from the ConfigMap have been added as files to the directory. The
sleep-interval entry is also included, although it has no business being there,
because it’s only meant to be used by the fortuneloop container. You could create
two different ConfigMaps and use one to configure the fortuneloop container and
the other one to configure the web-server container. But somehow it feels wrong to
use multiple ConfigMaps to configure containers of the same pod. After all, having
containers in the same pod implies that the containers are closely related and should
probably also be configured as a unit. 
EXPOSING CERTAIN CONFIGMAP ENTRIES IN THE VOLUME
Luckily, you can populate a configMap volume with only part of the ConfigMap’s
entries—in your case, only the my-nginx-config.conf entry. This won’t affect the
fortuneloop container, because you’re passing the sleep-interval entry to it through
an environment variable and not through the volume. 
 To define which entries should be exposed as files in a configMap volume, use the
volume’s items attribute as shown in the following listing.
  volumes:
  - name: config              
    configMap:                                  
      name: fortune-config                      
      items:                       
      - key: my-nginx-config.conf        
        path: gzip.conf                  
Listing 7.15
Seeing if nginx responses have compression enabled
Listing 7.16
A pod with a specific ConfigMap entry mounted into a file directory: 
fortune-pod-configmap-volume-with-items.yaml
This shows the response 
is compressed.
Selecting which entries to include 
in the volume by listing them
You want the entry 
under this key included.
The entry’s value should 
be stored in this file.
 
</data>
  <data key="d5">209
Decoupling configuration with a ConfigMap
$ kubectl port-forward fortune-configmap-volume 8080:80 &amp;
Forwarding from 127.0.0.1:8080 -&gt; 80
Forwarding from [::1]:8080 -&gt; 80
$ curl -H "Accept-Encoding: gzip" -I localhost:8080
HTTP/1.1 200 OK
Server: nginx/1.11.1
Date: Thu, 18 Aug 2016 11:52:57 GMT
Content-Type: text/html
Last-Modified: Thu, 18 Aug 2016 11:52:55 GMT
Connection: keep-alive
ETag: W/"57b5a197-37"
Content-Encoding: gzip           
EXAMINING THE MOUNTED CONFIGMAP VOLUME’S CONTENTS
The response shows you achieved what you wanted, but let’s look at what’s in the
/etc/nginx/conf.d directory now:
$ kubectl exec fortune-configmap-volume -c web-server ls /etc/nginx/conf.d
my-nginx-config.conf
sleep-interval
Both entries from the ConfigMap have been added as files to the directory. The
sleep-interval entry is also included, although it has no business being there,
because it’s only meant to be used by the fortuneloop container. You could create
two different ConfigMaps and use one to configure the fortuneloop container and
the other one to configure the web-server container. But somehow it feels wrong to
use multiple ConfigMaps to configure containers of the same pod. After all, having
containers in the same pod implies that the containers are closely related and should
probably also be configured as a unit. 
EXPOSING CERTAIN CONFIGMAP ENTRIES IN THE VOLUME
Luckily, you can populate a configMap volume with only part of the ConfigMap’s
entries—in your case, only the my-nginx-config.conf entry. This won’t affect the
fortuneloop container, because you’re passing the sleep-interval entry to it through
an environment variable and not through the volume. 
 To define which entries should be exposed as files in a configMap volume, use the
volume’s items attribute as shown in the following listing.
  volumes:
  - name: config              
    configMap:                                  
      name: fortune-config                      
      items:                       
      - key: my-nginx-config.conf        
        path: gzip.conf                  
Listing 7.15
Seeing if nginx responses have compression enabled
Listing 7.16
A pod with a specific ConfigMap entry mounted into a file directory: 
fortune-pod-configmap-volume-with-items.yaml
This shows the response 
is compressed.
Selecting which entries to include 
in the volume by listing them
You want the entry 
under this key included.
The entry’s value should 
be stored in this file.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="242">
  <data key="d0">Page_242</data>
  <data key="d5">Page_242</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_153">
  <data key="d0">210
CHAPTER 7
ConfigMaps and Secrets: configuring applications
When specifying individual entries, you need to set the filename for each individual
entry, along with the entry’s key. If you run the pod from the previous listing, the
/etc/nginx/conf.d directory is kept nice and clean, because it only contains the
gzip.conf file and nothing else. 
UNDERSTANDING THAT MOUNTING A DIRECTORY HIDES EXISTING FILES IN THAT DIRECTORY
There’s one important thing to discuss at this point. In both this and in your previous
example, you mounted the volume as a directory, which means you’ve hidden any files
that are stored in the /etc/nginx/conf.d directory in the container image itself. 
 This is generally what happens in Linux when you mount a filesystem into a non-
empty directory. The directory then only contains the files from the mounted filesys-
tem, whereas the original files in that directory are inaccessible for as long as the
filesystem is mounted. 
 In your case, this has no terrible side effects, but imagine mounting a volume to
the /etc directory, which usually contains many important files. This would most likely
break the whole container, because all of the original files that should be in the /etc
directory would no longer be there. If you need to add a file to a directory like /etc,
you can’t use this method at all.
MOUNTING INDIVIDUAL CONFIGMAP ENTRIES AS FILES WITHOUT HIDING OTHER FILES IN THE DIRECTORY
Naturally, you’re now wondering how to add individual files from a ConfigMap into
an existing directory without hiding existing files stored in it. An additional subPath
property on the volumeMount allows you to mount either a single file or a single direc-
tory from the volume instead of mounting the whole volume. Perhaps this is easier to
explain visually (see figure 7.10).
 Say you have a configMap volume containing a myconfig.conf file, which you want
to add to the /etc directory as someconfig.conf. You can use the subPath property to
mount it there without affecting any other files in that directory. The relevant part of
the pod definition is shown in the following listing.
Pod
Container
Filesystem
/
etc/
someconﬁg.conf
existingﬁle1
existingﬁle2
ConﬁgMap: app-conﬁg
myconﬁg.conf
Contents
of the ﬁle
another-ﬁle
Contents
of the ﬁle
conﬁgMap
volume
myconﬁg.conf
another-ﬁle
existingﬁle1
and existingﬁle2
aren’t hidden.
Only myconﬁg.conf is mounted
into the container (yet under a
different ﬁlename).
another-ﬁle isn’t
mounted into the
container.
Figure 7.10
Mounting a single file from a volume
 
</data>
  <data key="d5">210
CHAPTER 7
ConfigMaps and Secrets: configuring applications
When specifying individual entries, you need to set the filename for each individual
entry, along with the entry’s key. If you run the pod from the previous listing, the
/etc/nginx/conf.d directory is kept nice and clean, because it only contains the
gzip.conf file and nothing else. 
UNDERSTANDING THAT MOUNTING A DIRECTORY HIDES EXISTING FILES IN THAT DIRECTORY
There’s one important thing to discuss at this point. In both this and in your previous
example, you mounted the volume as a directory, which means you’ve hidden any files
that are stored in the /etc/nginx/conf.d directory in the container image itself. 
 This is generally what happens in Linux when you mount a filesystem into a non-
empty directory. The directory then only contains the files from the mounted filesys-
tem, whereas the original files in that directory are inaccessible for as long as the
filesystem is mounted. 
 In your case, this has no terrible side effects, but imagine mounting a volume to
the /etc directory, which usually contains many important files. This would most likely
break the whole container, because all of the original files that should be in the /etc
directory would no longer be there. If you need to add a file to a directory like /etc,
you can’t use this method at all.
MOUNTING INDIVIDUAL CONFIGMAP ENTRIES AS FILES WITHOUT HIDING OTHER FILES IN THE DIRECTORY
Naturally, you’re now wondering how to add individual files from a ConfigMap into
an existing directory without hiding existing files stored in it. An additional subPath
property on the volumeMount allows you to mount either a single file or a single direc-
tory from the volume instead of mounting the whole volume. Perhaps this is easier to
explain visually (see figure 7.10).
 Say you have a configMap volume containing a myconfig.conf file, which you want
to add to the /etc directory as someconfig.conf. You can use the subPath property to
mount it there without affecting any other files in that directory. The relevant part of
the pod definition is shown in the following listing.
Pod
Container
Filesystem
/
etc/
someconﬁg.conf
existingﬁle1
existingﬁle2
ConﬁgMap: app-conﬁg
myconﬁg.conf
Contents
of the ﬁle
another-ﬁle
Contents
of the ﬁle
conﬁgMap
volume
myconﬁg.conf
another-ﬁle
existingﬁle1
and existingﬁle2
aren’t hidden.
Only myconﬁg.conf is mounted
into the container (yet under a
different ﬁlename).
another-ﬁle isn’t
mounted into the
container.
Figure 7.10
Mounting a single file from a volume
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="243">
  <data key="d0">Page_243</data>
  <data key="d5">Page_243</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_154">
  <data key="d0">211
Decoupling configuration with a ConfigMap
spec:
  containers:
  - image: some/image
    volumeMounts:
    - name: myvolume
      mountPath: /etc/someconfig.conf     
      subPath: myconfig.conf            
The subPath property can be used when mounting any kind of volume. Instead of
mounting the whole volume, you can mount part of it. But this method of mounting
individual files has a relatively big deficiency related to updating files. You’ll learn
more about this in the following section, but first, let’s finish talking about the initial
state of a configMap volume by saying a few words about file permissions.
SETTING THE FILE PERMISSIONS FOR FILES IN A CONFIGMAP VOLUME
By default, the permissions on all files in a configMap volume are set to 644 (-rw-r—r--).
You can change this by setting the defaultMode property in the volume spec, as shown
in the following listing.
  volumes:
  - name: config
    configMap:
      name: fortune-config
      defaultMode: "6600"       
Although ConfigMaps should be used for non-sensitive configuration data, you may
want to make the file readable and writable only to the user and group the file is
owned by, as the example in the previous listing shows. 
7.4.7
Updating an app’s config without having to restart the app
We’ve said that one of the drawbacks of using environment variables or command-line
arguments as a configuration source is the inability to update them while the pro-
cess is running. Using a ConfigMap and exposing it through a volume brings the
ability to update the configuration without having to recreate the pod or even restart
the container. 
 When you update a ConfigMap, the files in all the volumes referencing it are
updated. It’s then up to the process to detect that they’ve been changed and reload
them. But Kubernetes will most likely eventually also support sending a signal to the
container after updating the files.
WARNING
Be aware that as I’m writing this, it takes a surprisingly long time
for the files to be updated after you update the ConfigMap (it can take up to
one whole minute).
Listing 7.17
A pod with a specific config map entry mounted into a specific file
Listing 7.18
Setting file permissions: fortune-pod-configmap-volume-defaultMode.yaml 
You’re mounting into 
a file, not a directory.
Instead of mounting the whole 
volume, you’re only mounting 
the myconfig.conf entry.
This sets the permissions 
for all files to -rw-rw------.
 
</data>
  <data key="d5">211
Decoupling configuration with a ConfigMap
spec:
  containers:
  - image: some/image
    volumeMounts:
    - name: myvolume
      mountPath: /etc/someconfig.conf     
      subPath: myconfig.conf            
The subPath property can be used when mounting any kind of volume. Instead of
mounting the whole volume, you can mount part of it. But this method of mounting
individual files has a relatively big deficiency related to updating files. You’ll learn
more about this in the following section, but first, let’s finish talking about the initial
state of a configMap volume by saying a few words about file permissions.
SETTING THE FILE PERMISSIONS FOR FILES IN A CONFIGMAP VOLUME
By default, the permissions on all files in a configMap volume are set to 644 (-rw-r—r--).
You can change this by setting the defaultMode property in the volume spec, as shown
in the following listing.
  volumes:
  - name: config
    configMap:
      name: fortune-config
      defaultMode: "6600"       
Although ConfigMaps should be used for non-sensitive configuration data, you may
want to make the file readable and writable only to the user and group the file is
owned by, as the example in the previous listing shows. 
7.4.7
Updating an app’s config without having to restart the app
We’ve said that one of the drawbacks of using environment variables or command-line
arguments as a configuration source is the inability to update them while the pro-
cess is running. Using a ConfigMap and exposing it through a volume brings the
ability to update the configuration without having to recreate the pod or even restart
the container. 
 When you update a ConfigMap, the files in all the volumes referencing it are
updated. It’s then up to the process to detect that they’ve been changed and reload
them. But Kubernetes will most likely eventually also support sending a signal to the
container after updating the files.
WARNING
Be aware that as I’m writing this, it takes a surprisingly long time
for the files to be updated after you update the ConfigMap (it can take up to
one whole minute).
Listing 7.17
A pod with a specific config map entry mounted into a specific file
Listing 7.18
Setting file permissions: fortune-pod-configmap-volume-defaultMode.yaml 
You’re mounting into 
a file, not a directory.
Instead of mounting the whole 
volume, you’re only mounting 
the myconfig.conf entry.
This sets the permissions 
for all files to -rw-rw------.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="244">
  <data key="d0">Page_244</data>
  <data key="d5">Page_244</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_155">
  <data key="d0">212
CHAPTER 7
ConfigMaps and Secrets: configuring applications
EDITING A CONFIGMAP
Let’s see how you can change a ConfigMap and have the process running in the pod
reload the files exposed in the configMap volume. You’ll modify the Nginx config file
from your previous example and make Nginx use the new config without restarting
the pod. Try switching gzip compression off by editing the fortune-config Config-
Map with kubectl edit:
$ kubectl edit configmap fortune-config
Once your editor opens, change the gzip on line to gzip off, save the file, and then
close the editor. The ConfigMap is then updated, and soon afterward, the actual file
in the volume is updated as well. You can confirm this by printing the contents of the
file with kubectl exec:
$ kubectl exec fortune-configmap-volume -c web-server
➥  cat /etc/nginx/conf.d/my-nginx-config.conf
If you don’t see the update yet, wait a while and try again. It takes a while for the
files to get updated. Eventually, you’ll see the change in the config file, but you’ll
find this has no effect on Nginx, because it doesn’t watch the files and reload them
automatically. 
SIGNALING NGINX TO RELOAD THE CONFIG
Nginx will continue to compress its responses until you tell it to reload its config files,
which you can do with the following command:
$ kubectl exec fortune-configmap-volume -c web-server -- nginx -s reload
Now, if you try hitting the server again with curl, you should see the response is no
longer compressed (it no longer contains the Content-Encoding: gzip header).
You’ve effectively changed the app’s config without having to restart the container or
recreate the pod. 
UNDERSTANDING HOW THE FILES ARE UPDATED ATOMICALLY
You may wonder what happens if an app can detect config file changes on its own and
reloads them before Kubernetes has finished updating all the files in the configMap
volume. Luckily, this can’t happen, because all the files are updated atomically, which
means all updates occur at once. Kubernetes achieves this by using symbolic links. If
you list all the files in the mounted configMap volume, you’ll see something like the
following listing.
$ kubectl exec -it fortune-configmap-volume -c web-server -- ls -lA 
➥  /etc/nginx/conf.d
total 4
drwxr-xr-x  ... 12:15 ..4984_09_04_12_15_06.865837643
Listing 7.19
Files in a mounted configMap volume
 
</data>
  <data key="d5">212
CHAPTER 7
ConfigMaps and Secrets: configuring applications
EDITING A CONFIGMAP
Let’s see how you can change a ConfigMap and have the process running in the pod
reload the files exposed in the configMap volume. You’ll modify the Nginx config file
from your previous example and make Nginx use the new config without restarting
the pod. Try switching gzip compression off by editing the fortune-config Config-
Map with kubectl edit:
$ kubectl edit configmap fortune-config
Once your editor opens, change the gzip on line to gzip off, save the file, and then
close the editor. The ConfigMap is then updated, and soon afterward, the actual file
in the volume is updated as well. You can confirm this by printing the contents of the
file with kubectl exec:
$ kubectl exec fortune-configmap-volume -c web-server
➥  cat /etc/nginx/conf.d/my-nginx-config.conf
If you don’t see the update yet, wait a while and try again. It takes a while for the
files to get updated. Eventually, you’ll see the change in the config file, but you’ll
find this has no effect on Nginx, because it doesn’t watch the files and reload them
automatically. 
SIGNALING NGINX TO RELOAD THE CONFIG
Nginx will continue to compress its responses until you tell it to reload its config files,
which you can do with the following command:
$ kubectl exec fortune-configmap-volume -c web-server -- nginx -s reload
Now, if you try hitting the server again with curl, you should see the response is no
longer compressed (it no longer contains the Content-Encoding: gzip header).
You’ve effectively changed the app’s config without having to restart the container or
recreate the pod. 
UNDERSTANDING HOW THE FILES ARE UPDATED ATOMICALLY
You may wonder what happens if an app can detect config file changes on its own and
reloads them before Kubernetes has finished updating all the files in the configMap
volume. Luckily, this can’t happen, because all the files are updated atomically, which
means all updates occur at once. Kubernetes achieves this by using symbolic links. If
you list all the files in the mounted configMap volume, you’ll see something like the
following listing.
$ kubectl exec -it fortune-configmap-volume -c web-server -- ls -lA 
➥  /etc/nginx/conf.d
total 4
drwxr-xr-x  ... 12:15 ..4984_09_04_12_15_06.865837643
Listing 7.19
Files in a mounted configMap volume
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="245">
  <data key="d0">Page_245</data>
  <data key="d5">Page_245</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_156">
  <data key="d0">213
Using Secrets to pass sensitive data to containers
lrwxrwxrwx  ... 12:15 ..data -&gt; ..4984_09_04_12_15_06.865837643
lrwxrwxrwx  ... 12:15 my-nginx-config.conf -&gt; ..data/my-nginx-config.conf
lrwxrwxrwx  ... 12:15 sleep-interval -&gt; ..data/sleep-interval
As you can see, the files in the mounted configMap volume are symbolic links point-
ing to files in the ..data dir. The ..data dir is also a symbolic link pointing to a direc-
tory called ..4984_09_04_something. When the ConfigMap is updated, Kubernetes
creates a new directory like this, writes all the files to it, and then re-links the ..data
symbolic link to the new directory, effectively changing all files at once.
UNDERSTANDING THAT FILES MOUNTED INTO EXISTING DIRECTORIES DON’T GET UPDATED
One big caveat relates to updating ConfigMap-backed volumes. If you’ve mounted a
single file in the container instead of the whole volume, the file will not be updated!
At least, this is true at the time of writing this chapter. 
 For now, if you need to add an individual file and have it updated when you update
its source ConfigMap, one workaround is to mount the whole volume into a different
directory and then create a symbolic link pointing to the file in question. The sym-
link can either be created in the container image itself, or you could create the
symlink when the container starts.
UNDERSTANDING THE CONSEQUENCES OF UPDATING A CONFIGMAP
One of the most important features of containers is their immutability, which allows
us to be certain that no differences exist between multiple running containers created
from the same image, so is it wrong to bypass this immutability by modifying a Config-
Map used by running containers? 
 The main problem occurs when the app doesn’t support reloading its configura-
tion. This results in different running instances being configured differently—those
pods that are created after the ConfigMap is changed will use the new config, whereas
the old pods will still use the old one. And this isn’t limited to new pods. If a pod’s con-
tainer is restarted (for whatever reason), the new process will also see the new config.
Therefore, if the app doesn’t reload its config automatically, modifying an existing
ConfigMap (while pods are using it) may not be a good idea. 
 If the app does support reloading, modifying the ConfigMap usually isn’t such a
big deal, but you do need to be aware that because files in the ConfigMap volumes
aren’t updated synchronously across all running instances, the files in individual pods
may be out of sync for up to a whole minute.
7.5
Using Secrets to pass sensitive data to containers
All the information you’ve passed to your containers so far is regular, non-sensitive
configuration data that doesn’t need to be kept secure. But as we mentioned at the
start of the chapter, the config usually also includes sensitive information, such as cre-
dentials and private encryption keys, which need to be kept secure.
 
</data>
  <data key="d5">213
Using Secrets to pass sensitive data to containers
lrwxrwxrwx  ... 12:15 ..data -&gt; ..4984_09_04_12_15_06.865837643
lrwxrwxrwx  ... 12:15 my-nginx-config.conf -&gt; ..data/my-nginx-config.conf
lrwxrwxrwx  ... 12:15 sleep-interval -&gt; ..data/sleep-interval
As you can see, the files in the mounted configMap volume are symbolic links point-
ing to files in the ..data dir. The ..data dir is also a symbolic link pointing to a direc-
tory called ..4984_09_04_something. When the ConfigMap is updated, Kubernetes
creates a new directory like this, writes all the files to it, and then re-links the ..data
symbolic link to the new directory, effectively changing all files at once.
UNDERSTANDING THAT FILES MOUNTED INTO EXISTING DIRECTORIES DON’T GET UPDATED
One big caveat relates to updating ConfigMap-backed volumes. If you’ve mounted a
single file in the container instead of the whole volume, the file will not be updated!
At least, this is true at the time of writing this chapter. 
 For now, if you need to add an individual file and have it updated when you update
its source ConfigMap, one workaround is to mount the whole volume into a different
directory and then create a symbolic link pointing to the file in question. The sym-
link can either be created in the container image itself, or you could create the
symlink when the container starts.
UNDERSTANDING THE CONSEQUENCES OF UPDATING A CONFIGMAP
One of the most important features of containers is their immutability, which allows
us to be certain that no differences exist between multiple running containers created
from the same image, so is it wrong to bypass this immutability by modifying a Config-
Map used by running containers? 
 The main problem occurs when the app doesn’t support reloading its configura-
tion. This results in different running instances being configured differently—those
pods that are created after the ConfigMap is changed will use the new config, whereas
the old pods will still use the old one. And this isn’t limited to new pods. If a pod’s con-
tainer is restarted (for whatever reason), the new process will also see the new config.
Therefore, if the app doesn’t reload its config automatically, modifying an existing
ConfigMap (while pods are using it) may not be a good idea. 
 If the app does support reloading, modifying the ConfigMap usually isn’t such a
big deal, but you do need to be aware that because files in the ConfigMap volumes
aren’t updated synchronously across all running instances, the files in individual pods
may be out of sync for up to a whole minute.
7.5
Using Secrets to pass sensitive data to containers
All the information you’ve passed to your containers so far is regular, non-sensitive
configuration data that doesn’t need to be kept secure. But as we mentioned at the
start of the chapter, the config usually also includes sensitive information, such as cre-
dentials and private encryption keys, which need to be kept secure.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="246">
  <data key="d0">Page_246</data>
  <data key="d5">Page_246</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_157">
  <data key="d0">214
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.5.1
Introducing Secrets
To store and distribute such information, Kubernetes provides a separate object called
a Secret. Secrets are much like ConfigMaps—they’re also maps that hold key-value
pairs. They can be used the same way as a ConfigMap. You can
Pass Secret entries to the container as environment variables
Expose Secret entries as files in a volume
Kubernetes helps keep your Secrets safe by making sure each Secret is only distributed
to the nodes that run the pods that need access to the Secret. Also, on the nodes
themselves, Secrets are always stored in memory and never written to physical storage,
which would require wiping the disks after deleting the Secrets from them. 
 On the master node itself (more specifically in etcd), Secrets used to be stored in
unencrypted form, which meant the master node needs to be secured to keep the sensi-
tive data stored in Secrets secure. This didn’t only include keeping the etcd storage
secure, but also preventing unauthorized users from using the API server, because any-
one who can create pods can mount the Secret into the pod and gain access to the sen-
sitive data through it. From Kubernetes version 1.7, etcd stores Secrets in encrypted
form, making the system much more secure. Because of this, it’s imperative you prop-
erly choose when to use a Secret or a ConfigMap. Choosing between them is simple:
Use a ConfigMap to store non-sensitive, plain configuration data.
Use a Secret to store any data that is sensitive in nature and needs to be kept
under key. If a config file includes both sensitive and not-sensitive data, you
should store the file in a Secret.
You already used Secrets in chapter 5, when you created a Secret to hold the TLS certifi-
cate needed for the Ingress resource. Now you’ll explore Secrets in more detail.
7.5.2
Introducing the default token Secret
You’ll start learning about Secrets by examining a Secret that’s mounted into every
container you run. You may have noticed it when using kubectl describe on a pod.
The command’s output has always contained something like this:
Volumes:
  default-token-cfee9:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-cfee9
Every pod has a secret volume attached to it automatically. The volume in the previ-
ous kubectl describe output refers to a Secret called default-token-cfee9. Because
Secrets are resources, you can list them with kubectl get secrets and find the
default-token Secret in that list. Let’s see:
$ kubectl get secrets
NAME                  TYPE                                  DATA      AGE
default-token-cfee9   kubernetes.io/service-account-token   3         39d
 
</data>
  <data key="d5">214
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.5.1
Introducing Secrets
To store and distribute such information, Kubernetes provides a separate object called
a Secret. Secrets are much like ConfigMaps—they’re also maps that hold key-value
pairs. They can be used the same way as a ConfigMap. You can
Pass Secret entries to the container as environment variables
Expose Secret entries as files in a volume
Kubernetes helps keep your Secrets safe by making sure each Secret is only distributed
to the nodes that run the pods that need access to the Secret. Also, on the nodes
themselves, Secrets are always stored in memory and never written to physical storage,
which would require wiping the disks after deleting the Secrets from them. 
 On the master node itself (more specifically in etcd), Secrets used to be stored in
unencrypted form, which meant the master node needs to be secured to keep the sensi-
tive data stored in Secrets secure. This didn’t only include keeping the etcd storage
secure, but also preventing unauthorized users from using the API server, because any-
one who can create pods can mount the Secret into the pod and gain access to the sen-
sitive data through it. From Kubernetes version 1.7, etcd stores Secrets in encrypted
form, making the system much more secure. Because of this, it’s imperative you prop-
erly choose when to use a Secret or a ConfigMap. Choosing between them is simple:
Use a ConfigMap to store non-sensitive, plain configuration data.
Use a Secret to store any data that is sensitive in nature and needs to be kept
under key. If a config file includes both sensitive and not-sensitive data, you
should store the file in a Secret.
You already used Secrets in chapter 5, when you created a Secret to hold the TLS certifi-
cate needed for the Ingress resource. Now you’ll explore Secrets in more detail.
7.5.2
Introducing the default token Secret
You’ll start learning about Secrets by examining a Secret that’s mounted into every
container you run. You may have noticed it when using kubectl describe on a pod.
The command’s output has always contained something like this:
Volumes:
  default-token-cfee9:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-cfee9
Every pod has a secret volume attached to it automatically. The volume in the previ-
ous kubectl describe output refers to a Secret called default-token-cfee9. Because
Secrets are resources, you can list them with kubectl get secrets and find the
default-token Secret in that list. Let’s see:
$ kubectl get secrets
NAME                  TYPE                                  DATA      AGE
default-token-cfee9   kubernetes.io/service-account-token   3         39d
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="247">
  <data key="d0">Page_247</data>
  <data key="d5">Page_247</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_158">
  <data key="d0">215
Using Secrets to pass sensitive data to containers
You can also use kubectl describe to learn a bit more about it, as shown in the follow-
ing listing.
$ kubectl describe secrets
Name:        default-token-cfee9
Namespace:   default
Labels:      &lt;none&gt;
Annotations: kubernetes.io/service-account.name=default
             kubernetes.io/service-account.uid=cc04bb39-b53f-42010af00237
Type:        kubernetes.io/service-account-token
Data
====
ca.crt:      1139 bytes                                   
namespace:   7 bytes                                      
token:       eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...      
You can see that the Secret contains three entries—ca.crt, namespace, and token—
which represent everything you need to securely talk to the Kubernetes API server
from within your pods, should you need to do that. Although ideally you want your
application to be completely Kubernetes-agnostic, when there’s no alternative other
than to talk to Kubernetes directly, you’ll use the files provided through this secret
volume. 
 The kubectl describe pod command shows where the secret volume is mounted:
Mounts:
  /var/run/secrets/kubernetes.io/serviceaccount from default-token-cfee9
NOTE
By default, the default-token Secret is mounted into every container,
but you can disable that in each pod by setting the automountService-
AccountToken field in the pod spec to false or by setting it to false on the
service account the pod is using. (You’ll learn about service accounts later in
the book.)
To help you visualize where and how the default token Secret is mounted, see fig-
ure 7.11.
 We’ve said Secrets are like ConfigMaps, so because this Secret contains three
entries, you can expect to see three files in the directory the secret volume is mounted
into. You can check this easily with kubectl exec:
$ kubectl exec mypod ls /var/run/secrets/kubernetes.io/serviceaccount/
ca.crt
namespace
token
You’ll see how your app can use these files to access the API server in the next chapter.
Listing 7.20
Describing a Secret
This secret 
contains three 
entries.
 
</data>
  <data key="d5">215
Using Secrets to pass sensitive data to containers
You can also use kubectl describe to learn a bit more about it, as shown in the follow-
ing listing.
$ kubectl describe secrets
Name:        default-token-cfee9
Namespace:   default
Labels:      &lt;none&gt;
Annotations: kubernetes.io/service-account.name=default
             kubernetes.io/service-account.uid=cc04bb39-b53f-42010af00237
Type:        kubernetes.io/service-account-token
Data
====
ca.crt:      1139 bytes                                   
namespace:   7 bytes                                      
token:       eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...      
You can see that the Secret contains three entries—ca.crt, namespace, and token—
which represent everything you need to securely talk to the Kubernetes API server
from within your pods, should you need to do that. Although ideally you want your
application to be completely Kubernetes-agnostic, when there’s no alternative other
than to talk to Kubernetes directly, you’ll use the files provided through this secret
volume. 
 The kubectl describe pod command shows where the secret volume is mounted:
Mounts:
  /var/run/secrets/kubernetes.io/serviceaccount from default-token-cfee9
NOTE
By default, the default-token Secret is mounted into every container,
but you can disable that in each pod by setting the automountService-
AccountToken field in the pod spec to false or by setting it to false on the
service account the pod is using. (You’ll learn about service accounts later in
the book.)
To help you visualize where and how the default token Secret is mounted, see fig-
ure 7.11.
 We’ve said Secrets are like ConfigMaps, so because this Secret contains three
entries, you can expect to see three files in the directory the secret volume is mounted
into. You can check this easily with kubectl exec:
$ kubectl exec mypod ls /var/run/secrets/kubernetes.io/serviceaccount/
ca.crt
namespace
token
You’ll see how your app can use these files to access the API server in the next chapter.
Listing 7.20
Describing a Secret
This secret 
contains three 
entries.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="248">
  <data key="d0">Page_248</data>
  <data key="d5">Page_248</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_159">
  <data key="d0">216
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.5.3
Creating a Secret
Now, you’ll create your own little Secret. You’ll improve your fortune-serving Nginx
container by configuring it to also serve HTTPS traffic. For this, you need to create a
certificate and a private key. The private key needs to be kept secure, so you’ll put it
and the certificate into a Secret.
 First, generate the certificate and private key files (do this on your local machine).
You can also use the files in the book’s code archive (the cert and key files are in the
fortune-https directory):
$ openssl genrsa -out https.key 2048
$ openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj 
/CN=www.kubia-example.com
Now, to help better demonstrate a few things about Secrets, create an additional
dummy file called foo and make it contain the string bar. You’ll understand why you
need to do this in a moment or two:
$ echo bar &gt; foo
Now you can use kubectl create secret to create a Secret from the three files:
$ kubectl create secret generic fortune-https --from-file=https.key
➥  --from-file=https.cert --from-file=foo
secret "fortune-https" created
This isn’t very different from creating ConfigMaps. In this case, you’re creating a
generic Secret called fortune-https and including two entries in it (https.key with
the contents of the https.key file and likewise for the https.cert key/file). As you
learned earlier, you could also include the whole directory with --from-file=fortune-
https instead of specifying each file individually.
Pod
Container
Filesystem
/
var/
run/
secrets/
kubernetes.io/
serviceaccount/
Default token Secret
Default token
secret
volume
ca.crt
...
...
...
namespace
token
Figure 7.11
The default-token Secret is created automatically and a corresponding 
volume is mounted in each pod automatically.
 
</data>
  <data key="d5">216
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.5.3
Creating a Secret
Now, you’ll create your own little Secret. You’ll improve your fortune-serving Nginx
container by configuring it to also serve HTTPS traffic. For this, you need to create a
certificate and a private key. The private key needs to be kept secure, so you’ll put it
and the certificate into a Secret.
 First, generate the certificate and private key files (do this on your local machine).
You can also use the files in the book’s code archive (the cert and key files are in the
fortune-https directory):
$ openssl genrsa -out https.key 2048
$ openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj 
/CN=www.kubia-example.com
Now, to help better demonstrate a few things about Secrets, create an additional
dummy file called foo and make it contain the string bar. You’ll understand why you
need to do this in a moment or two:
$ echo bar &gt; foo
Now you can use kubectl create secret to create a Secret from the three files:
$ kubectl create secret generic fortune-https --from-file=https.key
➥  --from-file=https.cert --from-file=foo
secret "fortune-https" created
This isn’t very different from creating ConfigMaps. In this case, you’re creating a
generic Secret called fortune-https and including two entries in it (https.key with
the contents of the https.key file and likewise for the https.cert key/file). As you
learned earlier, you could also include the whole directory with --from-file=fortune-
https instead of specifying each file individually.
Pod
Container
Filesystem
/
var/
run/
secrets/
kubernetes.io/
serviceaccount/
Default token Secret
Default token
secret
volume
ca.crt
...
...
...
namespace
token
Figure 7.11
The default-token Secret is created automatically and a corresponding 
volume is mounted in each pod automatically.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="249">
  <data key="d0">Page_249</data>
  <data key="d5">Page_249</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_160">
  <data key="d0">217
Using Secrets to pass sensitive data to containers
NOTE
You’re creating a generic Secret, but you could also have created a tls
Secret with the kubectl create secret tls command, as you did in chapter 5.
This would create the Secret with different entry names, though.
7.5.4
Comparing ConfigMaps and Secrets
Secrets and ConfigMaps have a pretty big difference. This is what drove Kubernetes
developers to create ConfigMaps after Kubernetes had already supported Secrets for a
while. The following listing shows the YAML of the Secret you created.
$ kubectl get secret fortune-https -o yaml
apiVersion: v1
data:
  foo: YmFyCg==
  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...
  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...
kind: Secret
...
Now compare this to the YAML of the ConfigMap you created earlier, which is shown
in the following listing.
$ kubectl get configmap fortune-config -o yaml
apiVersion: v1
data:
  my-nginx-config.conf: |
    server {
      ...
    }
  sleep-interval: |
    25
kind: ConfigMap
...
Notice the difference? The contents of a Secret’s entries are shown as Base64-encoded
strings, whereas those of a ConfigMap are shown in clear text. This initially made
working with Secrets in YAML and JSON manifests a bit more painful, because you
had to encode and decode them when setting and reading their entries. 
USING SECRETS FOR BINARY DATA
The reason for using Base64 encoding is simple. A Secret’s entries can contain binary
values, not only plain-text. Base64 encoding allows you to include the binary data in
YAML or JSON, which are both plain-text formats. 
TIP
You can use Secrets even for non-sensitive binary data, but be aware that
the maximum size of a Secret is limited to 1MB.
Listing 7.21
A Secret’s YAML definition
Listing 7.22
A ConfigMap’s YAML definition
 
</data>
  <data key="d5">217
Using Secrets to pass sensitive data to containers
NOTE
You’re creating a generic Secret, but you could also have created a tls
Secret with the kubectl create secret tls command, as you did in chapter 5.
This would create the Secret with different entry names, though.
7.5.4
Comparing ConfigMaps and Secrets
Secrets and ConfigMaps have a pretty big difference. This is what drove Kubernetes
developers to create ConfigMaps after Kubernetes had already supported Secrets for a
while. The following listing shows the YAML of the Secret you created.
$ kubectl get secret fortune-https -o yaml
apiVersion: v1
data:
  foo: YmFyCg==
  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...
  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...
kind: Secret
...
Now compare this to the YAML of the ConfigMap you created earlier, which is shown
in the following listing.
$ kubectl get configmap fortune-config -o yaml
apiVersion: v1
data:
  my-nginx-config.conf: |
    server {
      ...
    }
  sleep-interval: |
    25
kind: ConfigMap
...
Notice the difference? The contents of a Secret’s entries are shown as Base64-encoded
strings, whereas those of a ConfigMap are shown in clear text. This initially made
working with Secrets in YAML and JSON manifests a bit more painful, because you
had to encode and decode them when setting and reading their entries. 
USING SECRETS FOR BINARY DATA
The reason for using Base64 encoding is simple. A Secret’s entries can contain binary
values, not only plain-text. Base64 encoding allows you to include the binary data in
YAML or JSON, which are both plain-text formats. 
TIP
You can use Secrets even for non-sensitive binary data, but be aware that
the maximum size of a Secret is limited to 1MB.
Listing 7.21
A Secret’s YAML definition
Listing 7.22
A ConfigMap’s YAML definition
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="250">
  <data key="d0">Page_250</data>
  <data key="d5">Page_250</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_161">
  <data key="d0">218
CHAPTER 7
ConfigMaps and Secrets: configuring applications
INTRODUCING THE STRINGDATA FIELD
Because not all sensitive data is in binary form, Kubernetes also allows setting a Secret’s
values through the stringData field. The following listing shows how it’s used.
kind: Secret
apiVersion: v1
stringData:           
  foo: plain text      
data:
  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...
  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...
The stringData field is write-only (note: write-only, not read-only). It can only be
used to set values. When you retrieve the Secret’s YAML with kubectl get -o yaml, the
stringData field will not be shown. Instead, all entries you specified in the string-
Data field (such as the foo entry in the previous example) will be shown under data
and will be Base64-encoded like all the other entries. 
READING A SECRET’S ENTRY IN A POD
When you expose the Secret to a container through a secret volume, the value of the
Secret entry is decoded and written to the file in its actual form (regardless if it’s plain
text or binary). The same is also true when exposing the Secret entry through an envi-
ronment variable. In both cases, the app doesn’t need to decode it, but can read the
file’s contents or look up the environment variable value and use it directly.
7.5.5
Using the Secret in a pod
With your fortune-https Secret containing both the cert and key files, all you need to
do now is configure Nginx to use them. 
MODIFYING THE FORTUNE-CONFIG CONFIGMAP TO ENABLE HTTPS
For this, you need to modify the config file again by editing the ConfigMap:
$ kubectl edit configmap fortune-config
After the text editor opens, modify the part that defines the contents of the my-nginx-
config.conf entry so it looks like the following listing.
...
data:
  my-nginx-config.conf: |
    server {
      listen              80;
      listen              443 ssl;
      server_name         www.kubia-example.com;
Listing 7.23
Adding plain text entries to a Secret using the stringData field
Listing 7.24
Modifying the fortune-config ConfigMap’s data
The stringData can be used 
for non-binary Secret data.
See, “plain text” is not Base64-encoded.
 
</data>
  <data key="d5">218
CHAPTER 7
ConfigMaps and Secrets: configuring applications
INTRODUCING THE STRINGDATA FIELD
Because not all sensitive data is in binary form, Kubernetes also allows setting a Secret’s
values through the stringData field. The following listing shows how it’s used.
kind: Secret
apiVersion: v1
stringData:           
  foo: plain text      
data:
  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...
  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...
The stringData field is write-only (note: write-only, not read-only). It can only be
used to set values. When you retrieve the Secret’s YAML with kubectl get -o yaml, the
stringData field will not be shown. Instead, all entries you specified in the string-
Data field (such as the foo entry in the previous example) will be shown under data
and will be Base64-encoded like all the other entries. 
READING A SECRET’S ENTRY IN A POD
When you expose the Secret to a container through a secret volume, the value of the
Secret entry is decoded and written to the file in its actual form (regardless if it’s plain
text or binary). The same is also true when exposing the Secret entry through an envi-
ronment variable. In both cases, the app doesn’t need to decode it, but can read the
file’s contents or look up the environment variable value and use it directly.
7.5.5
Using the Secret in a pod
With your fortune-https Secret containing both the cert and key files, all you need to
do now is configure Nginx to use them. 
MODIFYING THE FORTUNE-CONFIG CONFIGMAP TO ENABLE HTTPS
For this, you need to modify the config file again by editing the ConfigMap:
$ kubectl edit configmap fortune-config
After the text editor opens, modify the part that defines the contents of the my-nginx-
config.conf entry so it looks like the following listing.
...
data:
  my-nginx-config.conf: |
    server {
      listen              80;
      listen              443 ssl;
      server_name         www.kubia-example.com;
Listing 7.23
Adding plain text entries to a Secret using the stringData field
Listing 7.24
Modifying the fortune-config ConfigMap’s data
The stringData can be used 
for non-binary Secret data.
See, “plain text” is not Base64-encoded.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="251">
  <data key="d0">Page_251</data>
  <data key="d5">Page_251</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_162">
  <data key="d0">219
Using Secrets to pass sensitive data to containers
      ssl_certificate     certs/https.cert;           
      ssl_certificate_key certs/https.key;            
      ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;
      ssl_ciphers         HIGH:!aNULL:!MD5;
      location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
      }
    }
  sleep-interval: |
...
This configures the server to read the certificate and key files from /etc/nginx/certs,
so you’ll need to mount the secret volume there. 
MOUNTING THE FORTUNE-HTTPS SECRET IN A POD
Next, you’ll create a new fortune-https pod and mount the secret volume holding
the certificate and key into the proper location in the web-server container, as shown
in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: fortune-https
spec:
  containers:
  - image: luksa/fortune:env
    name: html-generator
    env:
    - name: INTERVAL
      valueFrom: 
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true
    - name: certs                         
      mountPath: /etc/nginx/certs/        
      readOnly: true                      
    ports:
    - containerPort: 80
Listing 7.25
YAML definition of the fortune-https pod: fortune-pod-https.yaml
The paths are 
relative to /etc/nginx.
You configured Nginx to read the cert and 
key file from /etc/nginx/certs, so you need 
to mount the Secret volume there.
 
</data>
  <data key="d5">219
Using Secrets to pass sensitive data to containers
      ssl_certificate     certs/https.cert;           
      ssl_certificate_key certs/https.key;            
      ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;
      ssl_ciphers         HIGH:!aNULL:!MD5;
      location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
      }
    }
  sleep-interval: |
...
This configures the server to read the certificate and key files from /etc/nginx/certs,
so you’ll need to mount the secret volume there. 
MOUNTING THE FORTUNE-HTTPS SECRET IN A POD
Next, you’ll create a new fortune-https pod and mount the secret volume holding
the certificate and key into the proper location in the web-server container, as shown
in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: fortune-https
spec:
  containers:
  - image: luksa/fortune:env
    name: html-generator
    env:
    - name: INTERVAL
      valueFrom: 
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true
    - name: certs                         
      mountPath: /etc/nginx/certs/        
      readOnly: true                      
    ports:
    - containerPort: 80
Listing 7.25
YAML definition of the fortune-https pod: fortune-pod-https.yaml
The paths are 
relative to /etc/nginx.
You configured Nginx to read the cert and 
key file from /etc/nginx/certs, so you need 
to mount the Secret volume there.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="252">
  <data key="d0">Page_252</data>
  <data key="d5">Page_252</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_163">
  <data key="d0">220
CHAPTER 7
ConfigMaps and Secrets: configuring applications
    - containerPort: 443
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    configMap:
      name: fortune-config
      items:
      - key: my-nginx-config.conf
        path: https.conf
  - name: certs                            
    secret:                                
      secretName: fortune-https            
Much is going on in this pod descriptor, so let me help you visualize it. Figure 7.12
shows the components defined in the YAML. The default-token Secret, volume, and
volume mount, which aren’t part of the YAML, but are added to your pod automati-
cally, aren’t shown in the figure.
NOTE
Like configMap volumes, secret volumes also support specifying file
permissions for the files exposed in the volume through the defaultMode
property.
You define the secret 
volume here, referring to 
the fortune-https Secret.
Container: web-server
Container: html-generator
Secret: fortune-https
Default token Secret and volume not shown
secret
volume:
certs
emptyDir
volume:
html
conﬁgMap
volume:
conﬁg
https.cert
...
...
...
https.key
foo
/etc/nginx/conf.d/
/etc/nginx/certs/
/usr/share/nginx/html/
/var/htdocs
ConﬁgMap: fortune-conﬁg
my-nginx-conﬁg.conf
server {
…
}
Pod
Environment variables:
INTERVAL=25
sleep-interval
25
Figure 7.12
Combining a ConfigMap and a Secret to run your fortune-https pod
 
</data>
  <data key="d5">220
CHAPTER 7
ConfigMaps and Secrets: configuring applications
    - containerPort: 443
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    configMap:
      name: fortune-config
      items:
      - key: my-nginx-config.conf
        path: https.conf
  - name: certs                            
    secret:                                
      secretName: fortune-https            
Much is going on in this pod descriptor, so let me help you visualize it. Figure 7.12
shows the components defined in the YAML. The default-token Secret, volume, and
volume mount, which aren’t part of the YAML, but are added to your pod automati-
cally, aren’t shown in the figure.
NOTE
Like configMap volumes, secret volumes also support specifying file
permissions for the files exposed in the volume through the defaultMode
property.
You define the secret 
volume here, referring to 
the fortune-https Secret.
Container: web-server
Container: html-generator
Secret: fortune-https
Default token Secret and volume not shown
secret
volume:
certs
emptyDir
volume:
html
conﬁgMap
volume:
conﬁg
https.cert
...
...
...
https.key
foo
/etc/nginx/conf.d/
/etc/nginx/certs/
/usr/share/nginx/html/
/var/htdocs
ConﬁgMap: fortune-conﬁg
my-nginx-conﬁg.conf
server {
…
}
Pod
Environment variables:
INTERVAL=25
sleep-interval
25
Figure 7.12
Combining a ConfigMap and a Secret to run your fortune-https pod
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="253">
  <data key="d0">Page_253</data>
  <data key="d5">Page_253</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_164">
  <data key="d0">221
Using Secrets to pass sensitive data to containers
TESTING WHETHER NGINX IS USING THE CERT AND KEY FROM THE SECRET
Once the pod is running, you can see if it’s serving HTTPS traffic by opening a port-
forward tunnel to the pod’s port 443 and using it to send a request to the server
with curl: 
$ kubectl port-forward fortune-https 8443:443 &amp;
Forwarding from 127.0.0.1:8443 -&gt; 443
Forwarding from [::1]:8443 -&gt; 443
$ curl https://localhost:8443 -k
If you configured the server properly, you should get a response. You can check the
server’s certificate to see if it matches the one you generated earlier. This can also be
done with curl by turning on verbose logging using the -v option, as shown in the fol-
lowing listing.
$ curl https://localhost:8443 -k -v
* About to connect() to localhost port 8443 (#0)
*   Trying ::1...
* Connected to localhost (::1) port 8443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* Server certificate:
*   subject: CN=www.kubia-example.com          
*   start date: aug 16 18:43:13 2016 GMT       
*   expire date: aug 14 18:43:13 2026 GMT      
*   common name: www.kubia-example.com         
*   issuer: CN=www.kubia-example.com           
UNDERSTANDING SECRET VOLUMES ARE STORED IN MEMORY
You successfully delivered your certificate and private key to your container by mount-
ing a secret volume in its directory tree at /etc/nginx/certs. The secret volume uses
an in-memory filesystem (tmpfs) for the Secret files. You can see this if you list mounts
in the container:
$ kubectl exec fortune-https -c web-server -- mount | grep certs
tmpfs on /etc/nginx/certs type tmpfs (ro,relatime) 
Because tmpfs is used, the sensitive data stored in the Secret is never written to disk,
where it could be compromised. 
EXPOSING A SECRET’S ENTRIES THROUGH ENVIRONMENT VARIABLES
Instead of using a volume, you could also have exposed individual entries from the
secret as environment variables, the way you did with the sleep-interval entry from
the ConfigMap. For example, if you wanted to expose the foo key from your Secret as
environment variable FOO_SECRET, you’d add the snippet from the following listing to
the container definition.
Listing 7.26
Displaying the server certificate sent by Nginx
The certificate 
matches the one you 
created and stored 
in the Secret.
 
</data>
  <data key="d5">221
Using Secrets to pass sensitive data to containers
TESTING WHETHER NGINX IS USING THE CERT AND KEY FROM THE SECRET
Once the pod is running, you can see if it’s serving HTTPS traffic by opening a port-
forward tunnel to the pod’s port 443 and using it to send a request to the server
with curl: 
$ kubectl port-forward fortune-https 8443:443 &amp;
Forwarding from 127.0.0.1:8443 -&gt; 443
Forwarding from [::1]:8443 -&gt; 443
$ curl https://localhost:8443 -k
If you configured the server properly, you should get a response. You can check the
server’s certificate to see if it matches the one you generated earlier. This can also be
done with curl by turning on verbose logging using the -v option, as shown in the fol-
lowing listing.
$ curl https://localhost:8443 -k -v
* About to connect() to localhost port 8443 (#0)
*   Trying ::1...
* Connected to localhost (::1) port 8443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* Server certificate:
*   subject: CN=www.kubia-example.com          
*   start date: aug 16 18:43:13 2016 GMT       
*   expire date: aug 14 18:43:13 2026 GMT      
*   common name: www.kubia-example.com         
*   issuer: CN=www.kubia-example.com           
UNDERSTANDING SECRET VOLUMES ARE STORED IN MEMORY
You successfully delivered your certificate and private key to your container by mount-
ing a secret volume in its directory tree at /etc/nginx/certs. The secret volume uses
an in-memory filesystem (tmpfs) for the Secret files. You can see this if you list mounts
in the container:
$ kubectl exec fortune-https -c web-server -- mount | grep certs
tmpfs on /etc/nginx/certs type tmpfs (ro,relatime) 
Because tmpfs is used, the sensitive data stored in the Secret is never written to disk,
where it could be compromised. 
EXPOSING A SECRET’S ENTRIES THROUGH ENVIRONMENT VARIABLES
Instead of using a volume, you could also have exposed individual entries from the
secret as environment variables, the way you did with the sleep-interval entry from
the ConfigMap. For example, if you wanted to expose the foo key from your Secret as
environment variable FOO_SECRET, you’d add the snippet from the following listing to
the container definition.
Listing 7.26
Displaying the server certificate sent by Nginx
The certificate 
matches the one you 
created and stored 
in the Secret.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="254">
  <data key="d0">Page_254</data>
  <data key="d5">Page_254</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_165">
  <data key="d0">222
CHAPTER 7
ConfigMaps and Secrets: configuring applications
    env:
    - name: FOO_SECRET
      valueFrom:                  
        secretKeyRef:             
          name: fortune-https    
          key: foo           
This is almost exactly like when you set the INTERVAL environment variable, except
that this time you’re referring to a Secret by using secretKeyRef instead of config-
MapKeyRef, which is used to refer to a ConfigMap.
 Even though Kubernetes enables you to expose Secrets through environment vari-
ables, it may not be the best idea to use this feature. Applications usually dump envi-
ronment variables in error reports or even write them to the application log at startup,
which may unintentionally expose them. Additionally, child processes inherit all the
environment variables of the parent process, so if your app runs a third-party binary,
you have no way of knowing what happens with your secret data. 
TIP
Think twice before using environment variables to pass your Secrets to
your container, because they may get exposed inadvertently. To be safe, always
use secret volumes for exposing Secrets.
7.5.6
Understanding image pull Secrets
You’ve learned how to pass Secrets to your applications and use the data they contain.
But sometimes Kubernetes itself requires you to pass credentials to it—for example,
when you’d like to use images from a private container image registry. This is also
done through Secrets.
 Up to now all your container images have been stored on public image registries,
which don’t require any special credentials to pull images from them. But most orga-
nizations don’t want their images to be available to everyone and thus use a private
image registry. When deploying a pod, whose container images reside in a private reg-
istry, Kubernetes needs to know the credentials required to pull the image. Let’s see
how to do that.
USING A PRIVATE IMAGE REPOSITORY ON DOCKER HUB
Docker Hub, in addition to public image repositories, also allows you to create private
repositories. You can mark a repository as private by logging in at http:/
/hub.docker
.com with your web browser, finding the repository and checking a checkbox. 
 To run a pod, which uses an image from the private repository, you need to do
two things:
Create a Secret holding the credentials for the Docker registry.
Reference that Secret in the imagePullSecrets field of the pod manifest.
Listing 7.27
Exposing a Secret’s entry as an environment variable
The variable should be set 
from the entry of a Secret.
The name of the Secret 
holding the key
The key of the Secret 
to expose
 
</data>
  <data key="d5">222
CHAPTER 7
ConfigMaps and Secrets: configuring applications
    env:
    - name: FOO_SECRET
      valueFrom:                  
        secretKeyRef:             
          name: fortune-https    
          key: foo           
This is almost exactly like when you set the INTERVAL environment variable, except
that this time you’re referring to a Secret by using secretKeyRef instead of config-
MapKeyRef, which is used to refer to a ConfigMap.
 Even though Kubernetes enables you to expose Secrets through environment vari-
ables, it may not be the best idea to use this feature. Applications usually dump envi-
ronment variables in error reports or even write them to the application log at startup,
which may unintentionally expose them. Additionally, child processes inherit all the
environment variables of the parent process, so if your app runs a third-party binary,
you have no way of knowing what happens with your secret data. 
TIP
Think twice before using environment variables to pass your Secrets to
your container, because they may get exposed inadvertently. To be safe, always
use secret volumes for exposing Secrets.
7.5.6
Understanding image pull Secrets
You’ve learned how to pass Secrets to your applications and use the data they contain.
But sometimes Kubernetes itself requires you to pass credentials to it—for example,
when you’d like to use images from a private container image registry. This is also
done through Secrets.
 Up to now all your container images have been stored on public image registries,
which don’t require any special credentials to pull images from them. But most orga-
nizations don’t want their images to be available to everyone and thus use a private
image registry. When deploying a pod, whose container images reside in a private reg-
istry, Kubernetes needs to know the credentials required to pull the image. Let’s see
how to do that.
USING A PRIVATE IMAGE REPOSITORY ON DOCKER HUB
Docker Hub, in addition to public image repositories, also allows you to create private
repositories. You can mark a repository as private by logging in at http:/
/hub.docker
.com with your web browser, finding the repository and checking a checkbox. 
 To run a pod, which uses an image from the private repository, you need to do
two things:
Create a Secret holding the credentials for the Docker registry.
Reference that Secret in the imagePullSecrets field of the pod manifest.
Listing 7.27
Exposing a Secret’s entry as an environment variable
The variable should be set 
from the entry of a Secret.
The name of the Secret 
holding the key
The key of the Secret 
to expose
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="255">
  <data key="d0">Page_255</data>
  <data key="d5">Page_255</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_166">
  <data key="d0">223
Using Secrets to pass sensitive data to containers
CREATING A SECRET FOR AUTHENTICATING WITH A DOCKER REGISTRY
Creating a Secret holding the credentials for authenticating with a Docker registry
isn’t that different from creating the generic Secret you created in section 7.5.3. You
use the same kubectl create secret command, but with a different type and
options:
$ kubectl create secret docker-registry mydockerhubsecret \
  --docker-username=myusername --docker-password=mypassword \ 
  --docker-email=my.email@provider.com
Rather than create a generic secret, you’re creating a docker-registry Secret called
mydockerhubsecret. You’re specifying your Docker Hub username, password, and
email. If you inspect the contents of the newly created Secret with kubectl describe,
you’ll see that it includes a single entry called .dockercfg. This is equivalent to the
.dockercfg file in your home directory, which is created by Docker when you run the
docker login command.
USING THE DOCKER-REGISTRY SECRET IN A POD DEFINITION
To have Kubernetes use the Secret when pulling images from your private Docker
Hub repository, all you need to do is specify the Secret’s name in the pod spec, as
shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: private-pod
spec:
  imagePullSecrets:                 
  - name: mydockerhubsecret         
  containers:
  - image: username/private:tag
    name: main
In the pod definition in the previous listing, you’re specifying the mydockerhubsecret
Secret as one of the imagePullSecrets. I suggest you try this out yourself, because it’s
likely you’ll deal with private container images soon.
NOT HAVING TO SPECIFY IMAGE PULL SECRETS ON EVERY POD
Given that people usually run many different pods in their systems, it makes you won-
der if you need to add the same image pull Secrets to every pod. Luckily, that’s not the
case. In chapter 12 you’ll learn how image pull Secrets can be added to all your pods
automatically if you add the Secrets to a ServiceAccount.
Listing 7.28
A pod definition using an image pull Secret: pod-with-private-image.yaml
This enables pulling images 
from a private image registry.
 
</data>
  <data key="d5">223
Using Secrets to pass sensitive data to containers
CREATING A SECRET FOR AUTHENTICATING WITH A DOCKER REGISTRY
Creating a Secret holding the credentials for authenticating with a Docker registry
isn’t that different from creating the generic Secret you created in section 7.5.3. You
use the same kubectl create secret command, but with a different type and
options:
$ kubectl create secret docker-registry mydockerhubsecret \
  --docker-username=myusername --docker-password=mypassword \ 
  --docker-email=my.email@provider.com
Rather than create a generic secret, you’re creating a docker-registry Secret called
mydockerhubsecret. You’re specifying your Docker Hub username, password, and
email. If you inspect the contents of the newly created Secret with kubectl describe,
you’ll see that it includes a single entry called .dockercfg. This is equivalent to the
.dockercfg file in your home directory, which is created by Docker when you run the
docker login command.
USING THE DOCKER-REGISTRY SECRET IN A POD DEFINITION
To have Kubernetes use the Secret when pulling images from your private Docker
Hub repository, all you need to do is specify the Secret’s name in the pod spec, as
shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: private-pod
spec:
  imagePullSecrets:                 
  - name: mydockerhubsecret         
  containers:
  - image: username/private:tag
    name: main
In the pod definition in the previous listing, you’re specifying the mydockerhubsecret
Secret as one of the imagePullSecrets. I suggest you try this out yourself, because it’s
likely you’ll deal with private container images soon.
NOT HAVING TO SPECIFY IMAGE PULL SECRETS ON EVERY POD
Given that people usually run many different pods in their systems, it makes you won-
der if you need to add the same image pull Secrets to every pod. Luckily, that’s not the
case. In chapter 12 you’ll learn how image pull Secrets can be added to all your pods
automatically if you add the Secrets to a ServiceAccount.
Listing 7.28
A pod definition using an image pull Secret: pod-with-private-image.yaml
This enables pulling images 
from a private image registry.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="256">
  <data key="d0">Page_256</data>
  <data key="d5">Page_256</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_167">
  <data key="d0">224
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.6
Summary
This wraps up this chapter on how to pass configuration data to containers. You’ve
learned how to
Override the default command defined in a container image in the pod definition
Pass command-line arguments to the main container process
Set environment variables for a container
Decouple configuration from a pod specification and put it into a ConfigMap
Store sensitive data in a Secret and deliver it securely to containers
Create a docker-registry Secret and use it to pull images from a private image
registry
In the next chapter, you’ll learn how to pass pod and container metadata to applica-
tions running inside them. You’ll also see how the default token Secret, which we
learned about in this chapter, is used to talk to the API server from within a pod. 
 
</data>
  <data key="d5">224
CHAPTER 7
ConfigMaps and Secrets: configuring applications
7.6
Summary
This wraps up this chapter on how to pass configuration data to containers. You’ve
learned how to
Override the default command defined in a container image in the pod definition
Pass command-line arguments to the main container process
Set environment variables for a container
Decouple configuration from a pod specification and put it into a ConfigMap
Store sensitive data in a Secret and deliver it securely to containers
Create a docker-registry Secret and use it to pull images from a private image
registry
In the next chapter, you’ll learn how to pass pod and container metadata to applica-
tions running inside them. You’ll also see how the default token Secret, which we
learned about in this chapter, is used to talk to the API server from within a pod. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="257">
  <data key="d0">Page_257</data>
  <data key="d5">Page_257</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_168">
  <data key="d0">225
Accessing pod metadata
and other resources
from applications
Applications often need information about the environment they’re running in,
including details about themselves and that of other components in the cluster.
You’ve already seen how Kubernetes enables service discovery through environ-
ment variables or DNS, but what about other information? In this chapter, you’ll
see how certain pod and container metadata can be passed to the container and
how easy it is for an app running inside a container to talk to the Kubernetes API
server to get information about the resources deployed in the cluster and even how
to create or modify those resources.
This chapter covers
Using the Downward API to pass information into 
containers
Exploring the Kubernetes REST API
Leaving authentication and server verification to 
kubectl proxy
Accessing the API server from within a container
Understanding the ambassador container pattern
Using Kubernetes client libraries
 
</data>
  <data key="d5">225
Accessing pod metadata
and other resources
from applications
Applications often need information about the environment they’re running in,
including details about themselves and that of other components in the cluster.
You’ve already seen how Kubernetes enables service discovery through environ-
ment variables or DNS, but what about other information? In this chapter, you’ll
see how certain pod and container metadata can be passed to the container and
how easy it is for an app running inside a container to talk to the Kubernetes API
server to get information about the resources deployed in the cluster and even how
to create or modify those resources.
This chapter covers
Using the Downward API to pass information into 
containers
Exploring the Kubernetes REST API
Leaving authentication and server verification to 
kubectl proxy
Accessing the API server from within a container
Understanding the ambassador container pattern
Using Kubernetes client libraries
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="258">
  <data key="d0">Page_258</data>
  <data key="d5">Page_258</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_169">
  <data key="d0">226
CHAPTER 8
Accessing pod metadata and other resources from applications
8.1
Passing metadata through the Downward API
In the previous chapter you saw how you can pass configuration data to your appli-
cations through environment variables or through configMap and secret volumes.
This works well for data that you set yourself and that is known before the pod is
scheduled to a node and run there. But what about data that isn’t known up until
that point—such as the pod’s IP, the host node’s name, or even the pod’s own name
(when the name is generated; for example, when the pod is created by a ReplicaSet
or similar controller)? And what about data that’s already specified elsewhere, such
as a pod’s labels and annotations? You don’t want to repeat the same information in
multiple places.
 Both these problems are solved by the Kubernetes Downward API. It allows you to
pass metadata about the pod and its environment through environment variables or
files (in a downwardAPI volume). Don’t be confused by the name. The Downward API
isn’t like a REST endpoint that your app needs to hit so it can get the data. It’s a way of
having environment variables or files populated with values from the pod’s specifica-
tion or status, as shown in figure 8.1.
8.1.1
Understanding the available metadata
The Downward API enables you to expose the pod’s own metadata to the processes
running inside that pod. Currently, it allows you to pass the following information to
your containers:
The pod’s name
The pod’s IP address
Container: main
Environment
variables
API server
Used to initialize environment
variables and ﬁles in the
downwardAPI volume
Pod manifest
- Metadata
- Status
Pod
downwardAPI
volume
App process
Figure 8.1
The Downward API exposes pod metadata through environment variables or files.
 
</data>
  <data key="d5">226
CHAPTER 8
Accessing pod metadata and other resources from applications
8.1
Passing metadata through the Downward API
In the previous chapter you saw how you can pass configuration data to your appli-
cations through environment variables or through configMap and secret volumes.
This works well for data that you set yourself and that is known before the pod is
scheduled to a node and run there. But what about data that isn’t known up until
that point—such as the pod’s IP, the host node’s name, or even the pod’s own name
(when the name is generated; for example, when the pod is created by a ReplicaSet
or similar controller)? And what about data that’s already specified elsewhere, such
as a pod’s labels and annotations? You don’t want to repeat the same information in
multiple places.
 Both these problems are solved by the Kubernetes Downward API. It allows you to
pass metadata about the pod and its environment through environment variables or
files (in a downwardAPI volume). Don’t be confused by the name. The Downward API
isn’t like a REST endpoint that your app needs to hit so it can get the data. It’s a way of
having environment variables or files populated with values from the pod’s specifica-
tion or status, as shown in figure 8.1.
8.1.1
Understanding the available metadata
The Downward API enables you to expose the pod’s own metadata to the processes
running inside that pod. Currently, it allows you to pass the following information to
your containers:
The pod’s name
The pod’s IP address
Container: main
Environment
variables
API server
Used to initialize environment
variables and ﬁles in the
downwardAPI volume
Pod manifest
- Metadata
- Status
Pod
downwardAPI
volume
App process
Figure 8.1
The Downward API exposes pod metadata through environment variables or files.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="259">
  <data key="d0">Page_259</data>
  <data key="d5">Page_259</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_170">
  <data key="d0">227
Passing metadata through the Downward API
The namespace the pod belongs to
The name of the node the pod is running on
The name of the service account the pod is running under
The CPU and memory requests for each container
The CPU and memory limits for each container
The pod’s labels
The pod’s annotations
Most of the items in the list shouldn’t require further explanation, except perhaps the
service account and CPU/memory requests and limits, which we haven’t introduced
yet. We’ll cover service accounts in detail in chapter 12. For now, all you need to know
is that a service account is the account that the pod authenticates as when talking to
the API server. CPU and memory requests and limits are explained in chapter 14.
They’re the amount of CPU and memory guaranteed to a container and the maxi-
mum amount it can get.
 Most items in the list can be passed to containers either through environment vari-
ables or through a downwardAPI volume, but labels and annotations can only be
exposed through the volume. Part of the data can be acquired by other means (for
example, from the operating system directly), but the Downward API provides a sim-
pler alternative.
 Let’s look at an example to pass metadata to your containerized process.
8.1.2
Exposing metadata through environment variables
First, let’s look at how you can pass the pod’s and container’s metadata to the con-
tainer through environment variables. You’ll create a simple single-container pod
from the following listing’s manifest.
apiVersion: v1
kind: Pod
metadata:
  name: downward
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    env:
    - name: POD_NAME
Listing 8.1
Downward API used in environment variables: downward-api-env.yaml
 
</data>
  <data key="d5">227
Passing metadata through the Downward API
The namespace the pod belongs to
The name of the node the pod is running on
The name of the service account the pod is running under
The CPU and memory requests for each container
The CPU and memory limits for each container
The pod’s labels
The pod’s annotations
Most of the items in the list shouldn’t require further explanation, except perhaps the
service account and CPU/memory requests and limits, which we haven’t introduced
yet. We’ll cover service accounts in detail in chapter 12. For now, all you need to know
is that a service account is the account that the pod authenticates as when talking to
the API server. CPU and memory requests and limits are explained in chapter 14.
They’re the amount of CPU and memory guaranteed to a container and the maxi-
mum amount it can get.
 Most items in the list can be passed to containers either through environment vari-
ables or through a downwardAPI volume, but labels and annotations can only be
exposed through the volume. Part of the data can be acquired by other means (for
example, from the operating system directly), but the Downward API provides a sim-
pler alternative.
 Let’s look at an example to pass metadata to your containerized process.
8.1.2
Exposing metadata through environment variables
First, let’s look at how you can pass the pod’s and container’s metadata to the con-
tainer through environment variables. You’ll create a simple single-container pod
from the following listing’s manifest.
apiVersion: v1
kind: Pod
metadata:
  name: downward
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    env:
    - name: POD_NAME
Listing 8.1
Downward API used in environment variables: downward-api-env.yaml
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="260">
  <data key="d0">Page_260</data>
  <data key="d5">Page_260</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_171">
  <data key="d0">228
CHAPTER 8
Accessing pod metadata and other resources from applications
      valueFrom:                            
        fieldRef:                           
          fieldPath: metadata.name          
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: SERVICE_ACCOUNT
      valueFrom:
        fieldRef:
          fieldPath: spec.serviceAccountName
    - name: CONTAINER_CPU_REQUEST_MILLICORES
      valueFrom:                                   
        resourceFieldRef:                          
          resource: requests.cpu                   
          divisor: 1m                            
    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES
      valueFrom:
        resourceFieldRef:
          resource: limits.memory
          divisor: 1Ki
When your process runs, it can look up all the environment variables you defined in
the pod spec. Figure 8.2 shows the environment variables and the sources of their val-
ues. The pod’s name, IP, and namespace will be exposed through the POD_NAME,
POD_IP, and POD_NAMESPACE environment variables, respectively. The name of the
node the container is running on will be exposed through the NODE_NAME variable.
The name of the service account is made available through the SERVICE_ACCOUNT
environment variable. You’re also creating two environment variables that will hold
the amount of CPU requested for this container and the maximum amount of mem-
ory the container is allowed to consume.
 For environment variables exposing resource limits or requests, you specify a divi-
sor. The actual value of the limit or the request will be divided by the divisor and the
result exposed through the environment variable. In the previous example, you’re set-
ting the divisor for CPU requests to 1m (one milli-core, or one one-thousandth of a
CPU core). Because you’ve set the CPU request to 15m, the environment variable
CONTAINER_CPU_REQUEST_MILLICORES will be set to 15. Likewise, you set the memory
limit to 4Mi (4 mebibytes) and the divisor to 1Ki (1 Kibibyte), so the CONTAINER_MEMORY
_LIMIT_KIBIBYTES environment variable will be set to 4096. 
Instead of specifying an absolute value, 
you’re referencing the metadata.name 
field from the pod manifest.
A container’s CPU and memory 
requests and limits are referenced 
by using resourceFieldRef instead 
of fieldRef.
For resource fields, you 
define a divisor to get the 
value in the unit you need.
 
</data>
  <data key="d5">228
CHAPTER 8
Accessing pod metadata and other resources from applications
      valueFrom:                            
        fieldRef:                           
          fieldPath: metadata.name          
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: SERVICE_ACCOUNT
      valueFrom:
        fieldRef:
          fieldPath: spec.serviceAccountName
    - name: CONTAINER_CPU_REQUEST_MILLICORES
      valueFrom:                                   
        resourceFieldRef:                          
          resource: requests.cpu                   
          divisor: 1m                            
    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES
      valueFrom:
        resourceFieldRef:
          resource: limits.memory
          divisor: 1Ki
When your process runs, it can look up all the environment variables you defined in
the pod spec. Figure 8.2 shows the environment variables and the sources of their val-
ues. The pod’s name, IP, and namespace will be exposed through the POD_NAME,
POD_IP, and POD_NAMESPACE environment variables, respectively. The name of the
node the container is running on will be exposed through the NODE_NAME variable.
The name of the service account is made available through the SERVICE_ACCOUNT
environment variable. You’re also creating two environment variables that will hold
the amount of CPU requested for this container and the maximum amount of mem-
ory the container is allowed to consume.
 For environment variables exposing resource limits or requests, you specify a divi-
sor. The actual value of the limit or the request will be divided by the divisor and the
result exposed through the environment variable. In the previous example, you’re set-
ting the divisor for CPU requests to 1m (one milli-core, or one one-thousandth of a
CPU core). Because you’ve set the CPU request to 15m, the environment variable
CONTAINER_CPU_REQUEST_MILLICORES will be set to 15. Likewise, you set the memory
limit to 4Mi (4 mebibytes) and the divisor to 1Ki (1 Kibibyte), so the CONTAINER_MEMORY
_LIMIT_KIBIBYTES environment variable will be set to 4096. 
Instead of specifying an absolute value, 
you’re referencing the metadata.name 
field from the pod manifest.
A container’s CPU and memory 
requests and limits are referenced 
by using resourceFieldRef instead 
of fieldRef.
For resource fields, you 
define a divisor to get the 
value in the unit you need.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="261">
  <data key="d0">Page_261</data>
  <data key="d5">Page_261</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_172">
  <data key="d0">229
Passing metadata through the Downward API
The divisor for CPU limits and requests can be either 1, which means one whole core,
or 1m, which is one millicore. The divisor for memory limits/requests can be 1 (byte),
1k (kilobyte) or 1Ki (kibibyte), 1M (megabyte) or 1Mi (mebibyte), and so on.
 After creating the pod, you can use kubectl exec to see all these environment vari-
ables in your container, as shown in the following listing.
$ kubectl exec downward env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=downward
CONTAINER_MEMORY_LIMIT_KIBIBYTES=4096
POD_NAME=downward
POD_NAMESPACE=default
POD_IP=10.0.0.10
NODE_NAME=gke-kubia-default-pool-32a2cac8-sgl7
SERVICE_ACCOUNT=default
CONTAINER_CPU_REQUEST_MILLICORES=15
KUBERNETES_SERVICE_HOST=10.3.240.1
KUBERNETES_SERVICE_PORT=443
...
Listing 8.2
Environment variables in the downward pod
Pod manifest
metadata:
name: downward
namespace: default
spec:
nodeName: minikube
serviceAccountName: default
containers:
- name: main
image: busybox
command: ["sleep", "9999999"]
resources:
requests:
cpu: 15m
memory: 100Ki
limits:
cpu: 100m
memory: 4Mi
...
status:
podIP: 172.17.0.4
...
Pod: downward
Container: main
Environment variables
POD_NAME=downward
POD_NAMESPACE=default
POD_IP=172.17.0.4
NODE_NAME=minikube
SERVICE_ACCOUNT=default
CONTAINER_CPU_REQUEST_MILLICORES=15
CONTAINER_MEMORY_LIMIT_KIBIBYTES=4096
divisor: 1m
divisor: 1Ki
Figure 8.2
Pod metadata and attributes can be exposed to the pod through environment variables.
 
</data>
  <data key="d5">229
Passing metadata through the Downward API
The divisor for CPU limits and requests can be either 1, which means one whole core,
or 1m, which is one millicore. The divisor for memory limits/requests can be 1 (byte),
1k (kilobyte) or 1Ki (kibibyte), 1M (megabyte) or 1Mi (mebibyte), and so on.
 After creating the pod, you can use kubectl exec to see all these environment vari-
ables in your container, as shown in the following listing.
$ kubectl exec downward env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=downward
CONTAINER_MEMORY_LIMIT_KIBIBYTES=4096
POD_NAME=downward
POD_NAMESPACE=default
POD_IP=10.0.0.10
NODE_NAME=gke-kubia-default-pool-32a2cac8-sgl7
SERVICE_ACCOUNT=default
CONTAINER_CPU_REQUEST_MILLICORES=15
KUBERNETES_SERVICE_HOST=10.3.240.1
KUBERNETES_SERVICE_PORT=443
...
Listing 8.2
Environment variables in the downward pod
Pod manifest
metadata:
name: downward
namespace: default
spec:
nodeName: minikube
serviceAccountName: default
containers:
- name: main
image: busybox
command: ["sleep", "9999999"]
resources:
requests:
cpu: 15m
memory: 100Ki
limits:
cpu: 100m
memory: 4Mi
...
status:
podIP: 172.17.0.4
...
Pod: downward
Container: main
Environment variables
POD_NAME=downward
POD_NAMESPACE=default
POD_IP=172.17.0.4
NODE_NAME=minikube
SERVICE_ACCOUNT=default
CONTAINER_CPU_REQUEST_MILLICORES=15
CONTAINER_MEMORY_LIMIT_KIBIBYTES=4096
divisor: 1m
divisor: 1Ki
Figure 8.2
Pod metadata and attributes can be exposed to the pod through environment variables.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="262">
  <data key="d0">Page_262</data>
  <data key="d5">Page_262</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_173">
  <data key="d0">230
CHAPTER 8
Accessing pod metadata and other resources from applications
All processes running inside the container can read those variables and use them how-
ever they need. 
8.1.3
Passing metadata through files in a downwardAPI volume
If you prefer to expose the metadata through files instead of environment variables,
you can define a downwardAPI volume and mount it into your container. You must use
a downwardAPI volume for exposing the pod’s labels or its annotations, because nei-
ther can be exposed through environment variables. We’ll discuss why later.
 As with environment variables, you need to specify each metadata field explicitly if
you want to have it exposed to the process. Let’s see how to modify the previous exam-
ple to use a volume instead of environment variables, as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: downward
  labels:                  
    foo: bar               
  annotations:             
    key1: value1           
    key2: |                
      multi                
      line                 
      value                
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    volumeMounts:                        
    - name: downward                     
      mountPath: /etc/downward           
  volumes:
  - name: downward                 
    downwardAPI:                   
      items:
      - path: "podName"                     
        fieldRef:                           
          fieldPath: metadata.name          
      - path: "podNamespace"
        fieldRef:
          fieldPath: metadata.namespace
Listing 8.3
Pod with a downwardAPI volume: downward-api-volume.yaml
These labels and 
annotations will be 
exposed through the 
downwardAPI volume.
You’re mounting the 
downward volume 
under /etc/downward.
You’re defining a downwardAPI 
volume with the name downward.
The pod’s name (from the metadata.name 
field in the manifest) will be written to 
the podName file.
 
</data>
  <data key="d5">230
CHAPTER 8
Accessing pod metadata and other resources from applications
All processes running inside the container can read those variables and use them how-
ever they need. 
8.1.3
Passing metadata through files in a downwardAPI volume
If you prefer to expose the metadata through files instead of environment variables,
you can define a downwardAPI volume and mount it into your container. You must use
a downwardAPI volume for exposing the pod’s labels or its annotations, because nei-
ther can be exposed through environment variables. We’ll discuss why later.
 As with environment variables, you need to specify each metadata field explicitly if
you want to have it exposed to the process. Let’s see how to modify the previous exam-
ple to use a volume instead of environment variables, as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: downward
  labels:                  
    foo: bar               
  annotations:             
    key1: value1           
    key2: |                
      multi                
      line                 
      value                
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    volumeMounts:                        
    - name: downward                     
      mountPath: /etc/downward           
  volumes:
  - name: downward                 
    downwardAPI:                   
      items:
      - path: "podName"                     
        fieldRef:                           
          fieldPath: metadata.name          
      - path: "podNamespace"
        fieldRef:
          fieldPath: metadata.namespace
Listing 8.3
Pod with a downwardAPI volume: downward-api-volume.yaml
These labels and 
annotations will be 
exposed through the 
downwardAPI volume.
You’re mounting the 
downward volume 
under /etc/downward.
You’re defining a downwardAPI 
volume with the name downward.
The pod’s name (from the metadata.name 
field in the manifest) will be written to 
the podName file.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="263">
  <data key="d0">Page_263</data>
  <data key="d5">Page_263</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_174">
  <data key="d0">231
Passing metadata through the Downward API
      - path: "labels"                       
        fieldRef:                            
          fieldPath: metadata.labels         
      - path: "annotations"                       
        fieldRef:                                 
          fieldPath: metadata.annotations         
      - path: "containerCpuRequestMilliCores"
        resourceFieldRef:
          containerName: main
          resource: requests.cpu
          divisor: 1m
      - path: "containerMemoryLimitBytes"
        resourceFieldRef:
          containerName: main
          resource: limits.memory
          divisor: 1
Instead of passing the metadata through environment variables, you’re defining a vol-
ume called downward and mounting it in your container under /etc/downward. The
files this volume will contain are configured under the downwardAPI.items attribute
in the volume specification.
 Each item specifies the path (the filename) where the metadata should be written
to and references either a pod-level field or a container resource field whose value you
want stored in the file (see figure 8.3).
The pod’s labels will be written 
to the /etc/downward/labels file.
The pod’s annotations will be 
written to the /etc/downward/
annotations file.
downwardAPI volume
Pod manifest
metadata:
name: downward
namespace: default
labels:
foo: bar
annotations:
key1: value1
...
spec:
containers:
- name: main
image: busybox
command: ["sleep", "9999999"]
resources:
requests:
cpu: 15m
memory: 100Ki
limits:
cpu: 100m
memory: 4Mi
...
/podName
/podNamespace
/labels
/annotations
/containerCpuRequestMilliCores
/containerMemoryLimitBytes
divisor: 1
divisor: 1m
Container: main
Pod: downward
Filesystem
/
etc/
downward/
Figure 8.3
Using a downwardAPI volume to pass metadata to the container
 
</data>
  <data key="d5">231
Passing metadata through the Downward API
      - path: "labels"                       
        fieldRef:                            
          fieldPath: metadata.labels         
      - path: "annotations"                       
        fieldRef:                                 
          fieldPath: metadata.annotations         
      - path: "containerCpuRequestMilliCores"
        resourceFieldRef:
          containerName: main
          resource: requests.cpu
          divisor: 1m
      - path: "containerMemoryLimitBytes"
        resourceFieldRef:
          containerName: main
          resource: limits.memory
          divisor: 1
Instead of passing the metadata through environment variables, you’re defining a vol-
ume called downward and mounting it in your container under /etc/downward. The
files this volume will contain are configured under the downwardAPI.items attribute
in the volume specification.
 Each item specifies the path (the filename) where the metadata should be written
to and references either a pod-level field or a container resource field whose value you
want stored in the file (see figure 8.3).
The pod’s labels will be written 
to the /etc/downward/labels file.
The pod’s annotations will be 
written to the /etc/downward/
annotations file.
downwardAPI volume
Pod manifest
metadata:
name: downward
namespace: default
labels:
foo: bar
annotations:
key1: value1
...
spec:
containers:
- name: main
image: busybox
command: ["sleep", "9999999"]
resources:
requests:
cpu: 15m
memory: 100Ki
limits:
cpu: 100m
memory: 4Mi
...
/podName
/podNamespace
/labels
/annotations
/containerCpuRequestMilliCores
/containerMemoryLimitBytes
divisor: 1
divisor: 1m
Container: main
Pod: downward
Filesystem
/
etc/
downward/
Figure 8.3
Using a downwardAPI volume to pass metadata to the container
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="264">
  <data key="d0">Page_264</data>
  <data key="d5">Page_264</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_175">
  <data key="d0">232
CHAPTER 8
Accessing pod metadata and other resources from applications
Delete the previous pod and create a new one from the manifest in the previous list-
ing. Then look at the contents of the mounted downwardAPI volume directory. You
mounted the volume under /etc/downward/, so list the files in there, as shown in the
following listing.
$ kubectl exec downward ls -lL /etc/downward
-rw-r--r--   1 root   root   134 May 25 10:23 annotations
-rw-r--r--   1 root   root     2 May 25 10:23 containerCpuRequestMilliCores
-rw-r--r--   1 root   root     7 May 25 10:23 containerMemoryLimitBytes
-rw-r--r--   1 root   root     9 May 25 10:23 labels
-rw-r--r--   1 root   root     8 May 25 10:23 podName
-rw-r--r--   1 root   root     7 May 25 10:23 podNamespace
NOTE
As with the configMap and secret volumes, you can change the file
permissions through the downwardAPI volume’s defaultMode property in the
pod spec.
Each file corresponds to an item in the volume’s definition. The contents of files,
which correspond to the same metadata fields as in the previous example, are the
same as the values of environment variables you used before, so we won’t show them
here. But because you couldn’t expose labels and annotations through environment
variables before, examine the following listing for the contents of the two files you
exposed them in.
$ kubectl exec downward cat /etc/downward/labels
foo="bar"
$ kubectl exec downward cat /etc/downward/annotations
key1="value1"
key2="multi\nline\nvalue\n"
kubernetes.io/config.seen="2016-11-28T14:27:45.664924282Z"
kubernetes.io/config.source="api"
As you can see, each label/annotation is written in the key=value format on a sepa-
rate line. Multi-line values are written to a single line with newline characters denoted
with \n.
UPDATING LABELS AND ANNOTATIONS
You may remember that labels and annotations can be modified while a pod is run-
ning. As you might expect, when they change, Kubernetes updates the files holding
them, allowing the pod to always see up-to-date data. This also explains why labels and
annotations can’t be exposed through environment variables. Because environment
variable values can’t be updated afterward, if the labels or annotations of a pod were
exposed through environment variables, there’s no way to expose the new values after
they’re modified.
Listing 8.4
Files in the downwardAPI volume
Listing 8.5
Displaying labels and annotations in the downwardAPI volume
 
</data>
  <data key="d5">232
CHAPTER 8
Accessing pod metadata and other resources from applications
Delete the previous pod and create a new one from the manifest in the previous list-
ing. Then look at the contents of the mounted downwardAPI volume directory. You
mounted the volume under /etc/downward/, so list the files in there, as shown in the
following listing.
$ kubectl exec downward ls -lL /etc/downward
-rw-r--r--   1 root   root   134 May 25 10:23 annotations
-rw-r--r--   1 root   root     2 May 25 10:23 containerCpuRequestMilliCores
-rw-r--r--   1 root   root     7 May 25 10:23 containerMemoryLimitBytes
-rw-r--r--   1 root   root     9 May 25 10:23 labels
-rw-r--r--   1 root   root     8 May 25 10:23 podName
-rw-r--r--   1 root   root     7 May 25 10:23 podNamespace
NOTE
As with the configMap and secret volumes, you can change the file
permissions through the downwardAPI volume’s defaultMode property in the
pod spec.
Each file corresponds to an item in the volume’s definition. The contents of files,
which correspond to the same metadata fields as in the previous example, are the
same as the values of environment variables you used before, so we won’t show them
here. But because you couldn’t expose labels and annotations through environment
variables before, examine the following listing for the contents of the two files you
exposed them in.
$ kubectl exec downward cat /etc/downward/labels
foo="bar"
$ kubectl exec downward cat /etc/downward/annotations
key1="value1"
key2="multi\nline\nvalue\n"
kubernetes.io/config.seen="2016-11-28T14:27:45.664924282Z"
kubernetes.io/config.source="api"
As you can see, each label/annotation is written in the key=value format on a sepa-
rate line. Multi-line values are written to a single line with newline characters denoted
with \n.
UPDATING LABELS AND ANNOTATIONS
You may remember that labels and annotations can be modified while a pod is run-
ning. As you might expect, when they change, Kubernetes updates the files holding
them, allowing the pod to always see up-to-date data. This also explains why labels and
annotations can’t be exposed through environment variables. Because environment
variable values can’t be updated afterward, if the labels or annotations of a pod were
exposed through environment variables, there’s no way to expose the new values after
they’re modified.
Listing 8.4
Files in the downwardAPI volume
Listing 8.5
Displaying labels and annotations in the downwardAPI volume
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="265">
  <data key="d0">Page_265</data>
  <data key="d5">Page_265</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_176">
  <data key="d0">233
Talking to the Kubernetes API server
REFERRING TO CONTAINER-LEVEL METADATA IN THE VOLUME SPECIFICATION
Before we wrap up this section, we need to point out one thing. When exposing con-
tainer-level metadata, such as a container’s resource limit or requests (done using
resourceFieldRef), you need to specify the name of the container whose resource
field you’re referencing, as shown in the following listing.
spec:
  volumes:
  - name: downward                       
    downwardAPI:                         
      items:
      - path: "containerCpuRequestMilliCores"
        resourceFieldRef:
          containerName: main       
          resource: requests.cpu
          divisor: 1m
The reason for this becomes obvious if you consider that volumes are defined at the
pod level, not at the container level. When referring to a container’s resource field
inside a volume specification, you need to explicitly specify the name of the container
you’re referring to. This is true even for single-container pods. 
 Using volumes to expose a container’s resource requests and/or limits is slightly
more complicated than using environment variables, but the benefit is that it allows
you to pass one container’s resource fields to a different container if needed (but
both containers need to be in the same pod). With environment variables, a container
can only be passed its own resource limits and requests. 
UNDERSTANDING WHEN TO USE THE DOWNWARD API
As you’ve seen, using the Downward API isn’t complicated. It allows you to keep the
application Kubernetes-agnostic. This is especially useful when you’re dealing with an
existing application that expects certain data in environment variables. The Down-
ward API allows you to expose the data to the application without having to rewrite
the application or wrap it in a shell script, which collects the data and then exposes it
through environment variables.
 But the metadata available through the Downward API is fairly limited. If you need
more, you’ll need to obtain it from the Kubernetes API server directly. You’ll learn
how to do that next.
8.2
Talking to the Kubernetes API server
We’ve seen how the Downward API provides a simple way to pass certain pod and con-
tainer metadata to the process running inside them. It only exposes the pod’s own
metadata and a subset of all of the pod’s data. But sometimes your app will need to
know more about other pods and even other resources defined in your cluster. The
Downward API doesn’t help in those cases.
Listing 8.6
Referring to container-level metadata in a downwardAPI volume
Container name 
must be specified
 
</data>
  <data key="d5">233
Talking to the Kubernetes API server
REFERRING TO CONTAINER-LEVEL METADATA IN THE VOLUME SPECIFICATION
Before we wrap up this section, we need to point out one thing. When exposing con-
tainer-level metadata, such as a container’s resource limit or requests (done using
resourceFieldRef), you need to specify the name of the container whose resource
field you’re referencing, as shown in the following listing.
spec:
  volumes:
  - name: downward                       
    downwardAPI:                         
      items:
      - path: "containerCpuRequestMilliCores"
        resourceFieldRef:
          containerName: main       
          resource: requests.cpu
          divisor: 1m
The reason for this becomes obvious if you consider that volumes are defined at the
pod level, not at the container level. When referring to a container’s resource field
inside a volume specification, you need to explicitly specify the name of the container
you’re referring to. This is true even for single-container pods. 
 Using volumes to expose a container’s resource requests and/or limits is slightly
more complicated than using environment variables, but the benefit is that it allows
you to pass one container’s resource fields to a different container if needed (but
both containers need to be in the same pod). With environment variables, a container
can only be passed its own resource limits and requests. 
UNDERSTANDING WHEN TO USE THE DOWNWARD API
As you’ve seen, using the Downward API isn’t complicated. It allows you to keep the
application Kubernetes-agnostic. This is especially useful when you’re dealing with an
existing application that expects certain data in environment variables. The Down-
ward API allows you to expose the data to the application without having to rewrite
the application or wrap it in a shell script, which collects the data and then exposes it
through environment variables.
 But the metadata available through the Downward API is fairly limited. If you need
more, you’ll need to obtain it from the Kubernetes API server directly. You’ll learn
how to do that next.
8.2
Talking to the Kubernetes API server
We’ve seen how the Downward API provides a simple way to pass certain pod and con-
tainer metadata to the process running inside them. It only exposes the pod’s own
metadata and a subset of all of the pod’s data. But sometimes your app will need to
know more about other pods and even other resources defined in your cluster. The
Downward API doesn’t help in those cases.
Listing 8.6
Referring to container-level metadata in a downwardAPI volume
Container name 
must be specified
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="266">
  <data key="d0">Page_266</data>
  <data key="d5">Page_266</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_177">
  <data key="d0">234
CHAPTER 8
Accessing pod metadata and other resources from applications
 As you’ve seen throughout the book, information about services and pods can be
obtained by looking at the service-related environment variables or through DNS. But
when the app needs data about other resources or when it requires access to the most
up-to-date information as possible, it needs to talk to the API server directly (as shown
in figure 8.4).
Before you see how apps within pods can talk to the Kubernetes API server, let’s first
explore the server’s REST endpoints from your local machine, so you can see what
talking to the API server looks like.
8.2.1
Exploring the Kubernetes REST API
You’ve learned about different Kubernetes resource types. But if you’re planning on
developing apps that talk to the Kubernetes API, you’ll want to know the API first. 
 To do that, you can try hitting the API server directly. You can get its URL by run-
ning kubectl cluster-info:
$ kubectl cluster-info
Kubernetes master is running at https://192.168.99.100:8443
Because the server uses HTTPS and requires authentication, it’s not simple to talk to
it directly. You can try accessing it with curl and using curl’s --insecure (or -k)
option to skip the server certificate check, but that doesn’t get you far:
$ curl https://192.168.99.100:8443 -k
Unauthorized
Luckily, rather than dealing with authentication yourself, you can talk to the server
through a proxy by running the kubectl proxy command. 
ACCESSING THE API SERVER THROUGH KUBECTL PROXY 
The kubectl proxy command runs a proxy server that accepts HTTP connections on
your local machine and proxies them to the API server while taking care of authenti-
cation, so you don’t need to pass the authentication token in every request. It also
makes sure you’re talking to the actual API server and not a man in the middle (by
verifying the server’s certificate on each request).
Container
API server
Pod
App process
API objects
Figure 8.4
Talking to the API server 
from inside a pod to get information 
about other API objects
 
</data>
  <data key="d5">234
CHAPTER 8
Accessing pod metadata and other resources from applications
 As you’ve seen throughout the book, information about services and pods can be
obtained by looking at the service-related environment variables or through DNS. But
when the app needs data about other resources or when it requires access to the most
up-to-date information as possible, it needs to talk to the API server directly (as shown
in figure 8.4).
Before you see how apps within pods can talk to the Kubernetes API server, let’s first
explore the server’s REST endpoints from your local machine, so you can see what
talking to the API server looks like.
8.2.1
Exploring the Kubernetes REST API
You’ve learned about different Kubernetes resource types. But if you’re planning on
developing apps that talk to the Kubernetes API, you’ll want to know the API first. 
 To do that, you can try hitting the API server directly. You can get its URL by run-
ning kubectl cluster-info:
$ kubectl cluster-info
Kubernetes master is running at https://192.168.99.100:8443
Because the server uses HTTPS and requires authentication, it’s not simple to talk to
it directly. You can try accessing it with curl and using curl’s --insecure (or -k)
option to skip the server certificate check, but that doesn’t get you far:
$ curl https://192.168.99.100:8443 -k
Unauthorized
Luckily, rather than dealing with authentication yourself, you can talk to the server
through a proxy by running the kubectl proxy command. 
ACCESSING THE API SERVER THROUGH KUBECTL PROXY 
The kubectl proxy command runs a proxy server that accepts HTTP connections on
your local machine and proxies them to the API server while taking care of authenti-
cation, so you don’t need to pass the authentication token in every request. It also
makes sure you’re talking to the actual API server and not a man in the middle (by
verifying the server’s certificate on each request).
Container
API server
Pod
App process
API objects
Figure 8.4
Talking to the API server 
from inside a pod to get information 
about other API objects
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="267">
  <data key="d0">Page_267</data>
  <data key="d5">Page_267</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_178">
  <data key="d0">235
Talking to the Kubernetes API server
 Running the proxy is trivial. All you need to do is run the following command:
$ kubectl proxy
Starting to serve on 127.0.0.1:8001
You don’t need to pass in any other arguments, because kubectl already knows every-
thing it needs (the API server URL, authorization token, and so on). As soon as it starts
up, the proxy starts accepting connections on local port 8001. Let’s see if it works:
$ curl localhost:8001
{
  "paths": [
    "/api",
    "/api/v1",
    ...
Voila! You sent the request to the proxy, it sent a request to the API server, and then
the proxy returned whatever the server returned. Now, let’s start exploring.
EXPLORING THE KUBERNETES API THROUGH THE KUBECTL PROXY
You can continue to use curl, or you can open your web browser and point it to
http:/
/localhost:8001. Let’s examine what the API server returns when you hit its base
URL more closely. The server responds with a list of paths, as shown in the follow-
ing listing.
$ curl http://localhost:8001
{
  "paths": [
    "/api",
    "/api/v1",                  
    "/apis",
    "/apis/apps",
    "/apis/apps/v1beta1",
    ...
    "/apis/batch",              
    "/apis/batch/v1",           
    "/apis/batch/v2alpha1",     
    ...
These paths correspond to the API groups and versions you specify in your resource
definitions when creating resources such as Pods, Services, and so on. 
 You may recognize the batch/v1 in the /apis/batch/v1 path as the API group and
version of the Job resources you learned about in chapter 4. Likewise, the /api/v1
corresponds to the apiVersion: v1 you refer to in the common resources you created
(Pods, Services, ReplicationControllers, and so on). The most common resource
types, which were introduced in the earliest versions of Kubernetes, don’t belong to
Listing 8.7
Listing the API server’s REST endpoints: http:/
/localhost:8001
Most resource types 
can be found here.
The batch API 
group and its 
two versions
 
</data>
  <data key="d5">235
Talking to the Kubernetes API server
 Running the proxy is trivial. All you need to do is run the following command:
$ kubectl proxy
Starting to serve on 127.0.0.1:8001
You don’t need to pass in any other arguments, because kubectl already knows every-
thing it needs (the API server URL, authorization token, and so on). As soon as it starts
up, the proxy starts accepting connections on local port 8001. Let’s see if it works:
$ curl localhost:8001
{
  "paths": [
    "/api",
    "/api/v1",
    ...
Voila! You sent the request to the proxy, it sent a request to the API server, and then
the proxy returned whatever the server returned. Now, let’s start exploring.
EXPLORING THE KUBERNETES API THROUGH THE KUBECTL PROXY
You can continue to use curl, or you can open your web browser and point it to
http:/
/localhost:8001. Let’s examine what the API server returns when you hit its base
URL more closely. The server responds with a list of paths, as shown in the follow-
ing listing.
$ curl http://localhost:8001
{
  "paths": [
    "/api",
    "/api/v1",                  
    "/apis",
    "/apis/apps",
    "/apis/apps/v1beta1",
    ...
    "/apis/batch",              
    "/apis/batch/v1",           
    "/apis/batch/v2alpha1",     
    ...
These paths correspond to the API groups and versions you specify in your resource
definitions when creating resources such as Pods, Services, and so on. 
 You may recognize the batch/v1 in the /apis/batch/v1 path as the API group and
version of the Job resources you learned about in chapter 4. Likewise, the /api/v1
corresponds to the apiVersion: v1 you refer to in the common resources you created
(Pods, Services, ReplicationControllers, and so on). The most common resource
types, which were introduced in the earliest versions of Kubernetes, don’t belong to
Listing 8.7
Listing the API server’s REST endpoints: http:/
/localhost:8001
Most resource types 
can be found here.
The batch API 
group and its 
two versions
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="268">
  <data key="d0">Page_268</data>
  <data key="d5">Page_268</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_179">
  <data key="d0">236
CHAPTER 8
Accessing pod metadata and other resources from applications
any specific group, because Kubernetes initially didn’t even use the concept of API
groups; they were introduced later. 
NOTE
These initial resource types without an API group are now considered
to belong to the core API group.
EXPLORING THE BATCH API GROUP’S REST ENDPOINT
Let’s explore the Job resource API. You’ll start by looking at what’s behind the
/apis/batch path (you’ll omit the version for now), as shown in the following listing.
$ curl http://localhost:8001/apis/batch
{
  "kind": "APIGroup",
  "apiVersion": "v1",
  "name": "batch",
  "versions": [
    {
      "groupVersion": "batch/v1",             
      "version": "v1"                         
    },
    {
      "groupVersion": "batch/v2alpha1",       
      "version": "v2alpha1"                   
    }
  ],
  "preferredVersion": {                    
    "groupVersion": "batch/v1",            
    "version": "v1"                        
  },
  "serverAddressByClientCIDRs": null
}
The response shows a description of the batch API group, including the available ver-
sions and the preferred version clients should use. Let’s continue and see what’s
behind the /apis/batch/v1 path. It’s shown in the following listing.
$ curl http://localhost:8001/apis/batch/v1
{
  "kind": "APIResourceList",              
  "apiVersion": "v1",
  "groupVersion": "batch/v1",             
  "resources": [                          
    {
      "name": "jobs",             
      "namespaced": true,         
      "kind": "Job",              
Listing 8.8
Listing endpoints under /apis/batch: http:/
/localhost:8001/apis/batch
Listing 8.9
Resource types in batch/v1: http:/
/localhost:8001/apis/batch/v1
The batch API 
group contains 
two versions.
Clients should use the 
v1 version instead of 
v2alpha1.
This is a list of API resources 
in the batch/v1 API group.
Here’s an array holding 
all the resource types 
in this group.
This describes the 
Job resource, which 
is namespaced.
 
</data>
  <data key="d5">236
CHAPTER 8
Accessing pod metadata and other resources from applications
any specific group, because Kubernetes initially didn’t even use the concept of API
groups; they were introduced later. 
NOTE
These initial resource types without an API group are now considered
to belong to the core API group.
EXPLORING THE BATCH API GROUP’S REST ENDPOINT
Let’s explore the Job resource API. You’ll start by looking at what’s behind the
/apis/batch path (you’ll omit the version for now), as shown in the following listing.
$ curl http://localhost:8001/apis/batch
{
  "kind": "APIGroup",
  "apiVersion": "v1",
  "name": "batch",
  "versions": [
    {
      "groupVersion": "batch/v1",             
      "version": "v1"                         
    },
    {
      "groupVersion": "batch/v2alpha1",       
      "version": "v2alpha1"                   
    }
  ],
  "preferredVersion": {                    
    "groupVersion": "batch/v1",            
    "version": "v1"                        
  },
  "serverAddressByClientCIDRs": null
}
The response shows a description of the batch API group, including the available ver-
sions and the preferred version clients should use. Let’s continue and see what’s
behind the /apis/batch/v1 path. It’s shown in the following listing.
$ curl http://localhost:8001/apis/batch/v1
{
  "kind": "APIResourceList",              
  "apiVersion": "v1",
  "groupVersion": "batch/v1",             
  "resources": [                          
    {
      "name": "jobs",             
      "namespaced": true,         
      "kind": "Job",              
Listing 8.8
Listing endpoints under /apis/batch: http:/
/localhost:8001/apis/batch
Listing 8.9
Resource types in batch/v1: http:/
/localhost:8001/apis/batch/v1
The batch API 
group contains 
two versions.
Clients should use the 
v1 version instead of 
v2alpha1.
This is a list of API resources 
in the batch/v1 API group.
Here’s an array holding 
all the resource types 
in this group.
This describes the 
Job resource, which 
is namespaced.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="269">
  <data key="d0">Page_269</data>
  <data key="d5">Page_269</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_180">
  <data key="d0">237
Talking to the Kubernetes API server
      "verbs": [                 
        "create",                
        "delete",                
        "deletecollection",      
        "get",                   
        "list",                  
        "patch",                 
        "update",                
        "watch"                  
      ]
    },
    {
      "name": "jobs/status",            
      "namespaced": true,                  
      "kind": "Job",
      "verbs": [             
        "get",               
        "patch",             
        "update"             
      ]
    }
  ]
}
As you can see, the API server returns a list of resource types and REST endpoints in
the batch/v1 API group. One of those is the Job resource. In addition to the name of
the resource and the associated kind, the API server also includes information on
whether the resource is namespaced or not, its short name (if it has one; Jobs don’t),
and a list of verbs you can use with the resource. 
 The returned list describes the REST resources exposed in the API server. The
"name": "jobs" line tells you that the API contains the /apis/batch/v1/jobs end-
point. The "verbs" array says you can retrieve, update, and delete Job resources
through that endpoint. For certain resources, additional API endpoints are also
exposed (such as the jobs/status path, which allows modifying only the status of
a Job).
LISTING ALL JOB INSTANCES IN THE CLUSTER
To get a list of Jobs in your cluster, perform a GET request on path /apis/batch/
v1/jobs, as shown in the following listing.
$ curl http://localhost:8001/apis/batch/v1/jobs
{
  "kind": "JobList",
  "apiVersion": "batch/v1",
  "metadata": {
    "selfLink": "/apis/batch/v1/jobs",
    "resourceVersion": "225162"
  },
Listing 8.10
List of Jobs: http:/
/localhost:8001/apis/batch/v1/jobs
Here are the verbs that can be used 
with this resource (you can create 
Jobs; delete individual ones or a 
collection of them; and retrieve, 
watch, and update them).
Resources also have a 
special REST endpoint for 
modifying their status.
The status can be 
retrieved, patched, 
or updated.
 
</data>
  <data key="d5">237
Talking to the Kubernetes API server
      "verbs": [                 
        "create",                
        "delete",                
        "deletecollection",      
        "get",                   
        "list",                  
        "patch",                 
        "update",                
        "watch"                  
      ]
    },
    {
      "name": "jobs/status",            
      "namespaced": true,                  
      "kind": "Job",
      "verbs": [             
        "get",               
        "patch",             
        "update"             
      ]
    }
  ]
}
As you can see, the API server returns a list of resource types and REST endpoints in
the batch/v1 API group. One of those is the Job resource. In addition to the name of
the resource and the associated kind, the API server also includes information on
whether the resource is namespaced or not, its short name (if it has one; Jobs don’t),
and a list of verbs you can use with the resource. 
 The returned list describes the REST resources exposed in the API server. The
"name": "jobs" line tells you that the API contains the /apis/batch/v1/jobs end-
point. The "verbs" array says you can retrieve, update, and delete Job resources
through that endpoint. For certain resources, additional API endpoints are also
exposed (such as the jobs/status path, which allows modifying only the status of
a Job).
LISTING ALL JOB INSTANCES IN THE CLUSTER
To get a list of Jobs in your cluster, perform a GET request on path /apis/batch/
v1/jobs, as shown in the following listing.
$ curl http://localhost:8001/apis/batch/v1/jobs
{
  "kind": "JobList",
  "apiVersion": "batch/v1",
  "metadata": {
    "selfLink": "/apis/batch/v1/jobs",
    "resourceVersion": "225162"
  },
Listing 8.10
List of Jobs: http:/
/localhost:8001/apis/batch/v1/jobs
Here are the verbs that can be used 
with this resource (you can create 
Jobs; delete individual ones or a 
collection of them; and retrieve, 
watch, and update them).
Resources also have a 
special REST endpoint for 
modifying their status.
The status can be 
retrieved, patched, 
or updated.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="270">
  <data key="d0">Page_270</data>
  <data key="d5">Page_270</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_181">
  <data key="d0">238
CHAPTER 8
Accessing pod metadata and other resources from applications
  "items": [
    {
      "metadata": {
        "name": "my-job",
        "namespace": "default",
        ...
You probably have no Job resources deployed in your cluster, so the items array will be
empty. You can try deploying the Job in Chapter08/my-job.yaml and hitting the REST
endpoint again to get the same output as in listing 8.10.
RETRIEVING A SPECIFIC JOB INSTANCE BY NAME
The previous endpoint returned a list of all Jobs across all namespaces. To get back
only one specific Job, you need to specify its name and namespace in the URL. To
retrieve the Job shown in the previous listing (name: my-job; namespace: default),
you need to request the following path: /apis/batch/v1/namespaces/default/jobs/
my-job, as shown in the following listing.
$ curl http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job
{
  "kind": "Job",
  "apiVersion": "batch/v1",
  "metadata": {
    "name": "my-job",
    "namespace": "default",
    ...
As you can see, you get back the complete JSON definition of the my-job Job resource,
exactly like you do if you run:
$ kubectl get job my-job -o json
You’ve seen that you can browse the Kubernetes REST API server without using any
special tools, but to fully explore the REST API and interact with it, a better option is
described at the end of this chapter. For now, exploring it with curl like this is enough
to make you understand how an application running in a pod talks to Kubernetes. 
8.2.2
Talking to the API server from within a pod
You’ve learned how to talk to the API server from your local machine, using the
kubectl proxy. Now, let’s see how to talk to it from within a pod, where you (usually)
don’t have kubectl. Therefore, to talk to the API server from inside a pod, you need
to take care of three things:
Find the location of the API server.
Make sure you’re talking to the API server and not something impersonating it.
Authenticate with the server; otherwise it won’t let you see or do anything.
Listing 8.11
Retrieving a resource in a specific namespace by name
 
</data>
  <data key="d5">238
CHAPTER 8
Accessing pod metadata and other resources from applications
  "items": [
    {
      "metadata": {
        "name": "my-job",
        "namespace": "default",
        ...
You probably have no Job resources deployed in your cluster, so the items array will be
empty. You can try deploying the Job in Chapter08/my-job.yaml and hitting the REST
endpoint again to get the same output as in listing 8.10.
RETRIEVING A SPECIFIC JOB INSTANCE BY NAME
The previous endpoint returned a list of all Jobs across all namespaces. To get back
only one specific Job, you need to specify its name and namespace in the URL. To
retrieve the Job shown in the previous listing (name: my-job; namespace: default),
you need to request the following path: /apis/batch/v1/namespaces/default/jobs/
my-job, as shown in the following listing.
$ curl http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job
{
  "kind": "Job",
  "apiVersion": "batch/v1",
  "metadata": {
    "name": "my-job",
    "namespace": "default",
    ...
As you can see, you get back the complete JSON definition of the my-job Job resource,
exactly like you do if you run:
$ kubectl get job my-job -o json
You’ve seen that you can browse the Kubernetes REST API server without using any
special tools, but to fully explore the REST API and interact with it, a better option is
described at the end of this chapter. For now, exploring it with curl like this is enough
to make you understand how an application running in a pod talks to Kubernetes. 
8.2.2
Talking to the API server from within a pod
You’ve learned how to talk to the API server from your local machine, using the
kubectl proxy. Now, let’s see how to talk to it from within a pod, where you (usually)
don’t have kubectl. Therefore, to talk to the API server from inside a pod, you need
to take care of three things:
Find the location of the API server.
Make sure you’re talking to the API server and not something impersonating it.
Authenticate with the server; otherwise it won’t let you see or do anything.
Listing 8.11
Retrieving a resource in a specific namespace by name
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="271">
  <data key="d0">Page_271</data>
  <data key="d5">Page_271</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_182">
  <data key="d0">239
Talking to the Kubernetes API server
You’ll see how this is done in the next three sections. 
RUNNING A POD TO TRY OUT COMMUNICATION WITH THE API SERVER
The first thing you need is a pod from which to talk to the API server. You’ll run a pod
that does nothing (it runs the sleep command in its only container), and then run a
shell in the container with kubectl exec. Then you’ll try to access the API server from
within that shell using curl.
 Therefore, you need to use a container image that contains the curl binary. If you
search for such an image on, say, Docker Hub, you’ll find the tutum/curl image, so
use it (you can also use any other existing image containing the curl binary or you
can build your own). The pod definition is shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: curl
spec:
  containers:
  - name: main
    image: tutum/curl                
    command: ["sleep", "9999999"]    
After creating the pod, run kubectl exec to run a bash shell inside its container:
$ kubectl exec -it curl bash
root@curl:/#
You’re now ready to talk to the API server.
FINDING THE API SERVER’S ADDRESS
First, you need to find the IP and port of the Kubernetes API server. This is easy,
because a Service called kubernetes is automatically exposed in the default name-
space and configured to point to the API server. You may remember seeing it every
time you listed services with kubectl get svc:
$ kubectl get svc
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.0.0.1     &lt;none&gt;        443/TCP   46d
And you’ll remember from chapter 5 that environment variables are configured for
each service. You can get both the IP address and the port of the API server by looking
up the KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT variables (inside
the container):
root@curl:/# env | grep KUBERNETES_SERVICE
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT_HTTPS=443
Listing 8.12
A pod for trying out communication with the API server: curl.yaml
Using the tutum/curl image, 
because you need curl 
available in the container
You’re running the sleep 
command with a long delay to 
keep your container running.
 
</data>
  <data key="d5">239
Talking to the Kubernetes API server
You’ll see how this is done in the next three sections. 
RUNNING A POD TO TRY OUT COMMUNICATION WITH THE API SERVER
The first thing you need is a pod from which to talk to the API server. You’ll run a pod
that does nothing (it runs the sleep command in its only container), and then run a
shell in the container with kubectl exec. Then you’ll try to access the API server from
within that shell using curl.
 Therefore, you need to use a container image that contains the curl binary. If you
search for such an image on, say, Docker Hub, you’ll find the tutum/curl image, so
use it (you can also use any other existing image containing the curl binary or you
can build your own). The pod definition is shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: curl
spec:
  containers:
  - name: main
    image: tutum/curl                
    command: ["sleep", "9999999"]    
After creating the pod, run kubectl exec to run a bash shell inside its container:
$ kubectl exec -it curl bash
root@curl:/#
You’re now ready to talk to the API server.
FINDING THE API SERVER’S ADDRESS
First, you need to find the IP and port of the Kubernetes API server. This is easy,
because a Service called kubernetes is automatically exposed in the default name-
space and configured to point to the API server. You may remember seeing it every
time you listed services with kubectl get svc:
$ kubectl get svc
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.0.0.1     &lt;none&gt;        443/TCP   46d
And you’ll remember from chapter 5 that environment variables are configured for
each service. You can get both the IP address and the port of the API server by looking
up the KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT variables (inside
the container):
root@curl:/# env | grep KUBERNETES_SERVICE
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT_HTTPS=443
Listing 8.12
A pod for trying out communication with the API server: curl.yaml
Using the tutum/curl image, 
because you need curl 
available in the container
You’re running the sleep 
command with a long delay to 
keep your container running.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="272">
  <data key="d0">Page_272</data>
  <data key="d5">Page_272</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_183">
  <data key="d0">240
CHAPTER 8
Accessing pod metadata and other resources from applications
You may also remember that each service also gets a DNS entry, so you don’t even
need to look up the environment variables, but instead simply point curl to
https:/
/kubernetes. To be fair, if you don’t know which port the service is available at,
you also either need to look up the environment variables or perform a DNS SRV
record lookup to get the service’s actual port number. 
 The environment variables shown previously say that the API server is listening on
port 443, which is the default port for HTTPS, so try hitting the server through
HTTPS:
root@curl:/# curl https://kubernetes
curl: (60) SSL certificate problem: unable to get local issuer certificate
...
If you'd like to turn off curl's verification of the certificate, use
  the -k (or --insecure) option.
Although the simplest way to get around this is to use the proposed -k option (and
this is what you’d normally use when playing with the API server manually), let’s look
at the longer (and correct) route. Instead of blindly trusting that the server you’re
connecting to is the authentic API server, you’ll verify its identity by having curl check
its certificate. 
TIP
Never skip checking the server’s certificate in an actual application.
Doing so could make your app expose its authentication token to an attacker
using a man-in-the-middle attack.
VERIFYING THE SERVER’S IDENTITY
In the previous chapter, while discussing Secrets, we looked at an automatically cre-
ated Secret called default-token-xyz, which is mounted into each container at
/var/run/secrets/kubernetes.io/serviceaccount/. Let’s see the contents of that Secret
again, by listing files in that directory:
root@curl:/# 
ls 
/var/run/secrets/kubernetes.io/serviceaccount/ 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ca.crt    namespace    token
The Secret has three entries (and therefore three files in the Secret volume). Right
now, we’ll focus on the ca.crt file, which holds the certificate of the certificate author-
ity (CA) used to sign the Kubernetes API server’s certificate. To verify you’re talking to
the API server, you need to check if the server’s certificate is signed by the CA. curl
allows you to specify the CA certificate with the --cacert option, so try hitting the API
server again:
root@curl:/# curl --cacert /var/run/secrets/kubernetes.io/serviceaccount
             ➥ /ca.crt https://kubernetes
Unauthorized
NOTE
You may see a longer error description than “Unauthorized.”
 
</data>
  <data key="d5">240
CHAPTER 8
Accessing pod metadata and other resources from applications
You may also remember that each service also gets a DNS entry, so you don’t even
need to look up the environment variables, but instead simply point curl to
https:/
/kubernetes. To be fair, if you don’t know which port the service is available at,
you also either need to look up the environment variables or perform a DNS SRV
record lookup to get the service’s actual port number. 
 The environment variables shown previously say that the API server is listening on
port 443, which is the default port for HTTPS, so try hitting the server through
HTTPS:
root@curl:/# curl https://kubernetes
curl: (60) SSL certificate problem: unable to get local issuer certificate
...
If you'd like to turn off curl's verification of the certificate, use
  the -k (or --insecure) option.
Although the simplest way to get around this is to use the proposed -k option (and
this is what you’d normally use when playing with the API server manually), let’s look
at the longer (and correct) route. Instead of blindly trusting that the server you’re
connecting to is the authentic API server, you’ll verify its identity by having curl check
its certificate. 
TIP
Never skip checking the server’s certificate in an actual application.
Doing so could make your app expose its authentication token to an attacker
using a man-in-the-middle attack.
VERIFYING THE SERVER’S IDENTITY
In the previous chapter, while discussing Secrets, we looked at an automatically cre-
ated Secret called default-token-xyz, which is mounted into each container at
/var/run/secrets/kubernetes.io/serviceaccount/. Let’s see the contents of that Secret
again, by listing files in that directory:
root@curl:/# 
ls 
/var/run/secrets/kubernetes.io/serviceaccount/ 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ca.crt    namespace    token
The Secret has three entries (and therefore three files in the Secret volume). Right
now, we’ll focus on the ca.crt file, which holds the certificate of the certificate author-
ity (CA) used to sign the Kubernetes API server’s certificate. To verify you’re talking to
the API server, you need to check if the server’s certificate is signed by the CA. curl
allows you to specify the CA certificate with the --cacert option, so try hitting the API
server again:
root@curl:/# curl --cacert /var/run/secrets/kubernetes.io/serviceaccount
             ➥ /ca.crt https://kubernetes
Unauthorized
NOTE
You may see a longer error description than “Unauthorized.”
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="273">
  <data key="d0">Page_273</data>
  <data key="d5">Page_273</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_184">
  <data key="d0">241
Talking to the Kubernetes API server
Okay, you’ve made progress. curl verified the server’s identity because its certificate
was signed by the CA you trust. As the Unauthorized response suggests, you still need
to take care of authentication. You’ll do that in a moment, but first let’s see how to
make life easier by setting the CURL_CA_BUNDLE environment variable, so you don’t
need to specify --cacert every time you run curl:
root@curl:/# export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/
             ➥ serviceaccount/ca.crt
You can now hit the API server without using --cacert:
root@curl:/# curl https://kubernetes
Unauthorized
This is much nicer now. Your client (curl) trusts the API server now, but the API
server itself says you’re not authorized to access it, because it doesn’t know who
you are.
AUTHENTICATING WITH THE API SERVER
You need to authenticate with the server, so it allows you to read and even update
and/or delete the API objects deployed in the cluster. To authenticate, you need an
authentication token. Luckily, the token is provided through the default-token Secret
mentioned previously, and is stored in the token file in the secret volume. As the
Secret’s name suggests, that’s the primary purpose of the Secret. 
 You’re going to use the token to access the API server. First, load the token into an
environment variable:
root@curl:/# TOKEN=$(cat /var/run/secrets/kubernetes.io/
             ➥ serviceaccount/token)
The token is now stored in the TOKEN environment variable. You can use it when send-
ing requests to the API server, as shown in the following listing.
root@curl:/# curl -H "Authorization: Bearer $TOKEN" https://kubernetes
{
  "paths": [
    "/api",
    "/api/v1",
    "/apis",
    "/apis/apps",
    "/apis/apps/v1beta1",
    "/apis/authorization.k8s.io",    
    ...
    "/ui/",
    "/version"
  ]
}
Listing 8.13
Getting a proper response from the API server
 
</data>
  <data key="d5">241
Talking to the Kubernetes API server
Okay, you’ve made progress. curl verified the server’s identity because its certificate
was signed by the CA you trust. As the Unauthorized response suggests, you still need
to take care of authentication. You’ll do that in a moment, but first let’s see how to
make life easier by setting the CURL_CA_BUNDLE environment variable, so you don’t
need to specify --cacert every time you run curl:
root@curl:/# export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/
             ➥ serviceaccount/ca.crt
You can now hit the API server without using --cacert:
root@curl:/# curl https://kubernetes
Unauthorized
This is much nicer now. Your client (curl) trusts the API server now, but the API
server itself says you’re not authorized to access it, because it doesn’t know who
you are.
AUTHENTICATING WITH THE API SERVER
You need to authenticate with the server, so it allows you to read and even update
and/or delete the API objects deployed in the cluster. To authenticate, you need an
authentication token. Luckily, the token is provided through the default-token Secret
mentioned previously, and is stored in the token file in the secret volume. As the
Secret’s name suggests, that’s the primary purpose of the Secret. 
 You’re going to use the token to access the API server. First, load the token into an
environment variable:
root@curl:/# TOKEN=$(cat /var/run/secrets/kubernetes.io/
             ➥ serviceaccount/token)
The token is now stored in the TOKEN environment variable. You can use it when send-
ing requests to the API server, as shown in the following listing.
root@curl:/# curl -H "Authorization: Bearer $TOKEN" https://kubernetes
{
  "paths": [
    "/api",
    "/api/v1",
    "/apis",
    "/apis/apps",
    "/apis/apps/v1beta1",
    "/apis/authorization.k8s.io",    
    ...
    "/ui/",
    "/version"
  ]
}
Listing 8.13
Getting a proper response from the API server
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="274">
  <data key="d0">Page_274</data>
  <data key="d5">Page_274</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_185">
  <data key="d0">242
CHAPTER 8
Accessing pod metadata and other resources from applications
As you can see, you passed the token inside the Authorization HTTP header in the
request. The API server recognized the token as authentic and returned a proper
response. You can now explore all the resources in your cluster, the way you did a few
sections ago. 
 For example, you could list all the pods in the same namespace. But first you need
to know what namespace the curl pod is running in.
GETTING THE NAMESPACE THE POD IS RUNNING IN
In the first part of this chapter, you saw how to pass the namespace to the pod
through the Downward API. But if you’re paying attention, you probably noticed
your secret volume also contains a file called namespace. It contains the name-
space the pod is running in, so you can read the file instead of having to explicitly
pass the namespace to your pod through an environment variable. Load the con-
tents of the file into the NS environment variable and then list all the pods, as shown
in the following listing.
root@curl:/# NS=$(cat /var/run/secrets/kubernetes.io/
             ➥ serviceaccount/namespace)           
root@curl:/# curl -H "Authorization: Bearer $TOKEN"
             ➥ https://kubernetes/api/v1/namespaces/$NS/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
And there you go. By using the three files in the mounted secret volume directory,
you listed all the pods running in the same namespace as your pod. In the same man-
ner, you could also retrieve other API objects and even update them by sending PUT or
PATCH instead of simple GET requests. 
Disabling role-based access control (RBAC)
If you’re using a Kubernetes cluster with RBAC enabled, the service account may not
be authorized to access (parts of) the API server. You’ll learn about service accounts
and RBAC in chapter 12. For now, the simplest way to allow you to query the API
server is to work around RBAC by running the following command:
$ kubectl create clusterrolebinding permissive-binding \
  --clusterrole=cluster-admin \
  --group=system:serviceaccounts
This gives all service accounts (we could also say all pods) cluster-admin privileges,
allowing them to do whatever they want. Obviously, doing this is dangerous and
should never be done on production clusters. For test purposes, it’s fine.
Listing 8.14
Listing pods in the pod’s own namespace
 
</data>
  <data key="d5">242
CHAPTER 8
Accessing pod metadata and other resources from applications
As you can see, you passed the token inside the Authorization HTTP header in the
request. The API server recognized the token as authentic and returned a proper
response. You can now explore all the resources in your cluster, the way you did a few
sections ago. 
 For example, you could list all the pods in the same namespace. But first you need
to know what namespace the curl pod is running in.
GETTING THE NAMESPACE THE POD IS RUNNING IN
In the first part of this chapter, you saw how to pass the namespace to the pod
through the Downward API. But if you’re paying attention, you probably noticed
your secret volume also contains a file called namespace. It contains the name-
space the pod is running in, so you can read the file instead of having to explicitly
pass the namespace to your pod through an environment variable. Load the con-
tents of the file into the NS environment variable and then list all the pods, as shown
in the following listing.
root@curl:/# NS=$(cat /var/run/secrets/kubernetes.io/
             ➥ serviceaccount/namespace)           
root@curl:/# curl -H "Authorization: Bearer $TOKEN"
             ➥ https://kubernetes/api/v1/namespaces/$NS/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
And there you go. By using the three files in the mounted secret volume directory,
you listed all the pods running in the same namespace as your pod. In the same man-
ner, you could also retrieve other API objects and even update them by sending PUT or
PATCH instead of simple GET requests. 
Disabling role-based access control (RBAC)
If you’re using a Kubernetes cluster with RBAC enabled, the service account may not
be authorized to access (parts of) the API server. You’ll learn about service accounts
and RBAC in chapter 12. For now, the simplest way to allow you to query the API
server is to work around RBAC by running the following command:
$ kubectl create clusterrolebinding permissive-binding \
  --clusterrole=cluster-admin \
  --group=system:serviceaccounts
This gives all service accounts (we could also say all pods) cluster-admin privileges,
allowing them to do whatever they want. Obviously, doing this is dangerous and
should never be done on production clusters. For test purposes, it’s fine.
Listing 8.14
Listing pods in the pod’s own namespace
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="275">
  <data key="d0">Page_275</data>
  <data key="d5">Page_275</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_186">
  <data key="d0">243
Talking to the Kubernetes API server
RECAPPING HOW PODS TALK TO KUBERNETES
Let’s recap how an app running inside a pod can access the Kubernetes API properly:
The app should verify whether the API server’s certificate is signed by the certif-
icate authority, whose certificate is in the ca.crt file. 
The app should authenticate itself by sending the Authorization header with
the bearer token from the token file. 
The namespace file should be used to pass the namespace to the API server when
performing CRUD operations on API objects inside the pod’s namespace.
DEFINITION
CRUD stands for Create, Read, Update, and Delete. The corre-
sponding HTTP methods are POST, GET, PATCH/PUT, and DELETE, respectively.
All three aspects of pod to API server communication are displayed in figure 8.5.
8.2.3
Simplifying API server communication with ambassador 
containers
Dealing with HTTPS, certificates, and authentication tokens sometimes seems too
complicated to developers. I’ve seen developers disable validation of server certifi-
cates on way too many occasions (and I’ll admit to doing it myself a few times). Luck-
ily, you can make the communication much simpler while keeping it secure. 
API server
GET /api/v1/namespaces/&lt;namespace&gt;/pods
Authorization: Bearer &lt;token&gt;
Pod
Container
Filesystem
App
/
var/
run/
secrets/
kubernetes.io/
serviceaccount/
Default token secret volume
ca.crt
token
namespace
Server
certiﬁcate
Validate
certiﬁcate
Figure 8.5
Using the files from the default-token Secret to talk to the API server
 
</data>
  <data key="d5">243
Talking to the Kubernetes API server
RECAPPING HOW PODS TALK TO KUBERNETES
Let’s recap how an app running inside a pod can access the Kubernetes API properly:
The app should verify whether the API server’s certificate is signed by the certif-
icate authority, whose certificate is in the ca.crt file. 
The app should authenticate itself by sending the Authorization header with
the bearer token from the token file. 
The namespace file should be used to pass the namespace to the API server when
performing CRUD operations on API objects inside the pod’s namespace.
DEFINITION
CRUD stands for Create, Read, Update, and Delete. The corre-
sponding HTTP methods are POST, GET, PATCH/PUT, and DELETE, respectively.
All three aspects of pod to API server communication are displayed in figure 8.5.
8.2.3
Simplifying API server communication with ambassador 
containers
Dealing with HTTPS, certificates, and authentication tokens sometimes seems too
complicated to developers. I’ve seen developers disable validation of server certifi-
cates on way too many occasions (and I’ll admit to doing it myself a few times). Luck-
ily, you can make the communication much simpler while keeping it secure. 
API server
GET /api/v1/namespaces/&lt;namespace&gt;/pods
Authorization: Bearer &lt;token&gt;
Pod
Container
Filesystem
App
/
var/
run/
secrets/
kubernetes.io/
serviceaccount/
Default token secret volume
ca.crt
token
namespace
Server
certiﬁcate
Validate
certiﬁcate
Figure 8.5
Using the files from the default-token Secret to talk to the API server
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="276">
  <data key="d0">Page_276</data>
  <data key="d5">Page_276</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_187">
  <data key="d0">244
CHAPTER 8
Accessing pod metadata and other resources from applications
 Remember the kubectl proxy command we mentioned in section 8.2.1? You ran
the command on your local machine to make it easier to access the API server. Instead
of sending requests to the API server directly, you sent them to the proxy and let it
take care of authentication, encryption, and server verification. The same method can
be used inside your pods, as well.
INTRODUCING THE AMBASSADOR CONTAINER PATTERN
Imagine having an application that (among other things) needs to query the API
server. Instead of it talking to the API server directly, as you did in the previous sec-
tion, you can run kubectl proxy in an ambassador container alongside the main con-
tainer and communicate with the API server through it. 
 Instead of talking to the API server directly, the app in the main container can con-
nect to the ambassador through HTTP (instead of HTTPS) and let the ambassador
proxy handle the HTTPS connection to the API server, taking care of security trans-
parently (see figure 8.6). It does this by using the files from the default token’s secret
volume.
Because all containers in a pod share the same loopback network interface, your app
can access the proxy through a port on localhost.
RUNNING THE CURL POD WITH AN ADDITIONAL AMBASSADOR CONTAINER
To see the ambassador container pattern in action, you’ll create a new pod like the
curl pod you created earlier, but this time, instead of running a single container in
the pod, you’ll run an additional ambassador container based on a general-purpose
kubectl-proxy container image I’ve created and pushed to Docker Hub. You’ll find
the Dockerfile for the image in the code archive (in /Chapter08/kubectl-proxy/) if
you want to build it yourself.
 The pod’s manifest is shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: curl-with-ambassador
spec:
  containers:
  - name: main
Listing 8.15
A pod with an ambassador container: curl-with-ambassador.yaml
Container:
main
Container:
ambassador
HTTP
HTTPS
API server
Pod
Figure 8.6
Using an ambassador to connect to the API server
 
</data>
  <data key="d5">244
CHAPTER 8
Accessing pod metadata and other resources from applications
 Remember the kubectl proxy command we mentioned in section 8.2.1? You ran
the command on your local machine to make it easier to access the API server. Instead
of sending requests to the API server directly, you sent them to the proxy and let it
take care of authentication, encryption, and server verification. The same method can
be used inside your pods, as well.
INTRODUCING THE AMBASSADOR CONTAINER PATTERN
Imagine having an application that (among other things) needs to query the API
server. Instead of it talking to the API server directly, as you did in the previous sec-
tion, you can run kubectl proxy in an ambassador container alongside the main con-
tainer and communicate with the API server through it. 
 Instead of talking to the API server directly, the app in the main container can con-
nect to the ambassador through HTTP (instead of HTTPS) and let the ambassador
proxy handle the HTTPS connection to the API server, taking care of security trans-
parently (see figure 8.6). It does this by using the files from the default token’s secret
volume.
Because all containers in a pod share the same loopback network interface, your app
can access the proxy through a port on localhost.
RUNNING THE CURL POD WITH AN ADDITIONAL AMBASSADOR CONTAINER
To see the ambassador container pattern in action, you’ll create a new pod like the
curl pod you created earlier, but this time, instead of running a single container in
the pod, you’ll run an additional ambassador container based on a general-purpose
kubectl-proxy container image I’ve created and pushed to Docker Hub. You’ll find
the Dockerfile for the image in the code archive (in /Chapter08/kubectl-proxy/) if
you want to build it yourself.
 The pod’s manifest is shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: curl-with-ambassador
spec:
  containers:
  - name: main
Listing 8.15
A pod with an ambassador container: curl-with-ambassador.yaml
Container:
main
Container:
ambassador
HTTP
HTTPS
API server
Pod
Figure 8.6
Using an ambassador to connect to the API server
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="277">
  <data key="d0">Page_277</data>
  <data key="d5">Page_277</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_188">
  <data key="d0">245
Talking to the Kubernetes API server
    image: tutum/curl
    command: ["sleep", "9999999"]
  - name: ambassador                         
    image: luksa/kubectl-proxy:1.6.2         
The pod spec is almost the same as before, but with a different pod name and an addi-
tional container. Run the pod and then enter the main container with
$ kubectl exec -it curl-with-ambassador -c main bash
root@curl-with-ambassador:/#
Your pod now has two containers, and you want to run bash in the main container,
hence the -c main option. You don’t need to specify the container explicitly if you
want to run the command in the pod’s first container. But if you want to run a com-
mand inside any other container, you do need to specify the container’s name using
the -c option.
TALKING TO THE API SERVER THROUGH THE AMBASSADOR
Next you’ll try connecting to the API server through the ambassador container. By
default, kubectl proxy binds to port 8001, and because both containers in the pod
share the same network interfaces, including loopback, you can point curl to local-
host:8001, as shown in the following listing.
root@curl-with-ambassador:/# curl localhost:8001
{
  "paths": [
    "/api",
    ...
  ]
}
Success! The output printed by curl is the same response you saw earlier, but this time
you didn’t need to deal with authentication tokens and server certificates. 
 To get a clear picture of what exactly happened, refer to figure 8.7. curl sent the
plain HTTP request (without any authentication headers) to the proxy running inside
the ambassador container, and then the proxy sent an HTTPS request to the API
server, handling the client authentication by sending the token and checking the
server’s identity by validating its certificate.
 This is a great example of how an ambassador container can be used to hide the
complexities of connecting to an external service and simplify the app running in
the main container. The ambassador container is reusable across many different apps,
regardless of what language the main app is written in. The downside is that an addi-
tional process is running and consuming additional resources.
Listing 8.16
Accessing the API server through the ambassador container
The ambassador container, 
running the kubectl-proxy image
 
</data>
  <data key="d5">245
Talking to the Kubernetes API server
    image: tutum/curl
    command: ["sleep", "9999999"]
  - name: ambassador                         
    image: luksa/kubectl-proxy:1.6.2         
The pod spec is almost the same as before, but with a different pod name and an addi-
tional container. Run the pod and then enter the main container with
$ kubectl exec -it curl-with-ambassador -c main bash
root@curl-with-ambassador:/#
Your pod now has two containers, and you want to run bash in the main container,
hence the -c main option. You don’t need to specify the container explicitly if you
want to run the command in the pod’s first container. But if you want to run a com-
mand inside any other container, you do need to specify the container’s name using
the -c option.
TALKING TO THE API SERVER THROUGH THE AMBASSADOR
Next you’ll try connecting to the API server through the ambassador container. By
default, kubectl proxy binds to port 8001, and because both containers in the pod
share the same network interfaces, including loopback, you can point curl to local-
host:8001, as shown in the following listing.
root@curl-with-ambassador:/# curl localhost:8001
{
  "paths": [
    "/api",
    ...
  ]
}
Success! The output printed by curl is the same response you saw earlier, but this time
you didn’t need to deal with authentication tokens and server certificates. 
 To get a clear picture of what exactly happened, refer to figure 8.7. curl sent the
plain HTTP request (without any authentication headers) to the proxy running inside
the ambassador container, and then the proxy sent an HTTPS request to the API
server, handling the client authentication by sending the token and checking the
server’s identity by validating its certificate.
 This is a great example of how an ambassador container can be used to hide the
complexities of connecting to an external service and simplify the app running in
the main container. The ambassador container is reusable across many different apps,
regardless of what language the main app is written in. The downside is that an addi-
tional process is running and consuming additional resources.
Listing 8.16
Accessing the API server through the ambassador container
The ambassador container, 
running the kubectl-proxy image
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="278">
  <data key="d0">Page_278</data>
  <data key="d5">Page_278</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_189">
  <data key="d0">246
CHAPTER 8
Accessing pod metadata and other resources from applications
8.2.4
Using client libraries to talk to the API server
If your app only needs to perform a few simple operations on the API server, you can
often use a regular HTTP client library and perform simple HTTP requests, especially
if you take advantage of the kubectl-proxy ambassador container the way you did in
the previous example. But if you plan on doing more than simple API requests, it’s
better to use one of the existing Kubernetes API client libraries.
USING EXISTING CLIENT LIBRARIES
Currently, two Kubernetes API client libraries exist that are supported by the API
Machinery special interest group (SIG):
Golang client—https:/
/github.com/kubernetes/client-go
Python—https:/
/github.com/kubernetes-incubator/client-python
NOTE
The Kubernetes community has a number of Special Interest Groups
(SIGs) and Working Groups that focus on specific parts of the Kubernetes
ecosystem. You’ll find a list of them at https:/
/github.com/kubernetes/com-
munity/blob/master/sig-list.md.
In addition to the two officially supported libraries, here’s a list of user-contributed cli-
ent libraries for many other languages:
Java client by Fabric8—https:/
/github.com/fabric8io/kubernetes-client
Java client by Amdatu—https:/
/bitbucket.org/amdatulabs/amdatu-kubernetes
Node.js client by tenxcloud—https:/
/github.com/tenxcloud/node-kubernetes-client
Node.js client by GoDaddy—https:/
/github.com/godaddy/kubernetes-client
PHP—https:/
/github.com/devstub/kubernetes-api-php-client
Another PHP client—https:/
/github.com/maclof/kubernetes-client
Container: main
API server
sleep
curl
Container: ambassador
kubectl proxy
Port 8001
GET http://localhost:8001
GET https://kubernetes:443
Authorization: Bearer &lt;token&gt;
Pod
Figure 8.7
Offloading encryption, authentication, and server verification to kubectl proxy in an 
ambassador container 
 
</data>
  <data key="d5">246
CHAPTER 8
Accessing pod metadata and other resources from applications
8.2.4
Using client libraries to talk to the API server
If your app only needs to perform a few simple operations on the API server, you can
often use a regular HTTP client library and perform simple HTTP requests, especially
if you take advantage of the kubectl-proxy ambassador container the way you did in
the previous example. But if you plan on doing more than simple API requests, it’s
better to use one of the existing Kubernetes API client libraries.
USING EXISTING CLIENT LIBRARIES
Currently, two Kubernetes API client libraries exist that are supported by the API
Machinery special interest group (SIG):
Golang client—https:/
/github.com/kubernetes/client-go
Python—https:/
/github.com/kubernetes-incubator/client-python
NOTE
The Kubernetes community has a number of Special Interest Groups
(SIGs) and Working Groups that focus on specific parts of the Kubernetes
ecosystem. You’ll find a list of them at https:/
/github.com/kubernetes/com-
munity/blob/master/sig-list.md.
In addition to the two officially supported libraries, here’s a list of user-contributed cli-
ent libraries for many other languages:
Java client by Fabric8—https:/
/github.com/fabric8io/kubernetes-client
Java client by Amdatu—https:/
/bitbucket.org/amdatulabs/amdatu-kubernetes
Node.js client by tenxcloud—https:/
/github.com/tenxcloud/node-kubernetes-client
Node.js client by GoDaddy—https:/
/github.com/godaddy/kubernetes-client
PHP—https:/
/github.com/devstub/kubernetes-api-php-client
Another PHP client—https:/
/github.com/maclof/kubernetes-client
Container: main
API server
sleep
curl
Container: ambassador
kubectl proxy
Port 8001
GET http://localhost:8001
GET https://kubernetes:443
Authorization: Bearer &lt;token&gt;
Pod
Figure 8.7
Offloading encryption, authentication, and server verification to kubectl proxy in an 
ambassador container 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="279">
  <data key="d0">Page_279</data>
  <data key="d5">Page_279</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_190">
  <data key="d0">247
Talking to the Kubernetes API server
Ruby—https:/
/github.com/Ch00k/kubr
Another Ruby client—https:/
/github.com/abonas/kubeclient
Clojure—https:/
/github.com/yanatan16/clj-kubernetes-api
Scala—https:/
/github.com/doriordan/skuber
Perl—https:/
/metacpan.org/pod/Net::Kubernetes
These libraries usually support HTTPS and take care of authentication, so you won’t
need to use the ambassador container. 
AN EXAMPLE OF INTERACTING WITH KUBERNETES WITH THE FABRIC8 JAVA CLIENT
To give you a sense of how client libraries enable you to talk to the API server, the fol-
lowing listing shows an example of how to list services in a Java app using the Fabric8
Kubernetes client.
import java.util.Arrays;
import io.fabric8.kubernetes.api.model.Pod;
import io.fabric8.kubernetes.api.model.PodList;
import io.fabric8.kubernetes.client.DefaultKubernetesClient;
import io.fabric8.kubernetes.client.KubernetesClient;
public class Test {
  public static void main(String[] args) throws Exception {
    KubernetesClient client = new DefaultKubernetesClient();
    // list pods in the default namespace
    PodList pods = client.pods().inNamespace("default").list();
    pods.getItems().stream()
      .forEach(s -&gt; System.out.println("Found pod: " +
               s.getMetadata().getName()));
    // create a pod
    System.out.println("Creating a pod");
    Pod pod = client.pods().inNamespace("default")
      .createNew()
      .withNewMetadata()
        .withName("programmatically-created-pod")
      .endMetadata()
      .withNewSpec()
        .addNewContainer()
          .withName("main")
          .withImage("busybox")
          .withCommand(Arrays.asList("sleep", "99999"))
        .endContainer()
      .endSpec()
      .done();
    System.out.println("Created pod: " + pod);
    // edit the pod (add a label to it)
    client.pods().inNamespace("default")
      .withName("programmatically-created-pod")
      .edit()
      .editMetadata()
Listing 8.17
Listing, creating, updating, and deleting pods with the Fabric8 Java client
 
</data>
  <data key="d5">247
Talking to the Kubernetes API server
Ruby—https:/
/github.com/Ch00k/kubr
Another Ruby client—https:/
/github.com/abonas/kubeclient
Clojure—https:/
/github.com/yanatan16/clj-kubernetes-api
Scala—https:/
/github.com/doriordan/skuber
Perl—https:/
/metacpan.org/pod/Net::Kubernetes
These libraries usually support HTTPS and take care of authentication, so you won’t
need to use the ambassador container. 
AN EXAMPLE OF INTERACTING WITH KUBERNETES WITH THE FABRIC8 JAVA CLIENT
To give you a sense of how client libraries enable you to talk to the API server, the fol-
lowing listing shows an example of how to list services in a Java app using the Fabric8
Kubernetes client.
import java.util.Arrays;
import io.fabric8.kubernetes.api.model.Pod;
import io.fabric8.kubernetes.api.model.PodList;
import io.fabric8.kubernetes.client.DefaultKubernetesClient;
import io.fabric8.kubernetes.client.KubernetesClient;
public class Test {
  public static void main(String[] args) throws Exception {
    KubernetesClient client = new DefaultKubernetesClient();
    // list pods in the default namespace
    PodList pods = client.pods().inNamespace("default").list();
    pods.getItems().stream()
      .forEach(s -&gt; System.out.println("Found pod: " +
               s.getMetadata().getName()));
    // create a pod
    System.out.println("Creating a pod");
    Pod pod = client.pods().inNamespace("default")
      .createNew()
      .withNewMetadata()
        .withName("programmatically-created-pod")
      .endMetadata()
      .withNewSpec()
        .addNewContainer()
          .withName("main")
          .withImage("busybox")
          .withCommand(Arrays.asList("sleep", "99999"))
        .endContainer()
      .endSpec()
      .done();
    System.out.println("Created pod: " + pod);
    // edit the pod (add a label to it)
    client.pods().inNamespace("default")
      .withName("programmatically-created-pod")
      .edit()
      .editMetadata()
Listing 8.17
Listing, creating, updating, and deleting pods with the Fabric8 Java client
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="280">
  <data key="d0">Page_280</data>
  <data key="d5">Page_280</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_191">
  <data key="d0">248
CHAPTER 8
Accessing pod metadata and other resources from applications
        .addToLabels("foo", "bar")
      .endMetadata()
      .done();
    System.out.println("Added label foo=bar to pod");
    System.out.println("Waiting 1 minute before deleting pod...");
    Thread.sleep(60000);
    // delete the pod
    client.pods().inNamespace("default")
      .withName("programmatically-created-pod")
      .delete();
    System.out.println("Deleted the pod");
  }
}
The code should be self-explanatory, especially because the Fabric8 client exposes
a nice, fluent Domain-Specific-Language (DSL) API, which is easy to read and
understand.
BUILDING YOUR OWN LIBRARY WITH SWAGGER AND OPENAPI
If no client is available for your programming language of choice, you can use the
Swagger API framework to generate the client library and documentation. The Kuber-
netes API server exposes Swagger API definitions at /swaggerapi and OpenAPI spec at
/swagger.json. 
 To find out more about the Swagger framework, visit the website at http:/
/swagger.io.
EXPLORING THE API WITH SWAGGER UI
Earlier in the chapter I said I’d point you to a better way of exploring the REST API
instead of hitting the REST endpoints with curl. Swagger, which I mentioned in the
previous section, is not just a tool for specifying an API, but also provides a web UI for
exploring REST APIs if they expose the Swagger API definitions. The better way of
exploring REST APIs is through this UI.
 Kubernetes not only exposes the Swagger API, but it also has Swagger UI inte-
grated into the API server, though it’s not enabled by default. You can enable it by
running the API server with the --enable-swagger-ui=true option.
TIP
If you’re using Minikube, you can enable Swagger UI when starting the
cluster: minikube start --extra-config=apiserver.Features.Enable-
SwaggerUI=true
After you enable the UI, you can open it in your browser by pointing it to:
http(s)://&lt;api server&gt;:&lt;port&gt;/swagger-ui
I urge you to give Swagger UI a try. It not only allows you to browse the Kubernetes
API, but also interact with it (you can POST JSON resource manifests, PATCH resources,
or DELETE them, for example). 
 
</data>
  <data key="d5">248
CHAPTER 8
Accessing pod metadata and other resources from applications
        .addToLabels("foo", "bar")
      .endMetadata()
      .done();
    System.out.println("Added label foo=bar to pod");
    System.out.println("Waiting 1 minute before deleting pod...");
    Thread.sleep(60000);
    // delete the pod
    client.pods().inNamespace("default")
      .withName("programmatically-created-pod")
      .delete();
    System.out.println("Deleted the pod");
  }
}
The code should be self-explanatory, especially because the Fabric8 client exposes
a nice, fluent Domain-Specific-Language (DSL) API, which is easy to read and
understand.
BUILDING YOUR OWN LIBRARY WITH SWAGGER AND OPENAPI
If no client is available for your programming language of choice, you can use the
Swagger API framework to generate the client library and documentation. The Kuber-
netes API server exposes Swagger API definitions at /swaggerapi and OpenAPI spec at
/swagger.json. 
 To find out more about the Swagger framework, visit the website at http:/
/swagger.io.
EXPLORING THE API WITH SWAGGER UI
Earlier in the chapter I said I’d point you to a better way of exploring the REST API
instead of hitting the REST endpoints with curl. Swagger, which I mentioned in the
previous section, is not just a tool for specifying an API, but also provides a web UI for
exploring REST APIs if they expose the Swagger API definitions. The better way of
exploring REST APIs is through this UI.
 Kubernetes not only exposes the Swagger API, but it also has Swagger UI inte-
grated into the API server, though it’s not enabled by default. You can enable it by
running the API server with the --enable-swagger-ui=true option.
TIP
If you’re using Minikube, you can enable Swagger UI when starting the
cluster: minikube start --extra-config=apiserver.Features.Enable-
SwaggerUI=true
After you enable the UI, you can open it in your browser by pointing it to:
http(s)://&lt;api server&gt;:&lt;port&gt;/swagger-ui
I urge you to give Swagger UI a try. It not only allows you to browse the Kubernetes
API, but also interact with it (you can POST JSON resource manifests, PATCH resources,
or DELETE them, for example). 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="281">
  <data key="d0">Page_281</data>
  <data key="d5">Page_281</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_192">
  <data key="d0">249
Summary
8.3
Summary
After reading this chapter, you now know how your app, running inside a pod, can get
data about itself, other pods, and other components deployed in the cluster. You’ve
learned
How a pod’s name, namespace, and other metadata can be exposed to the pro-
cess either through environment variables or files in a downwardAPI volume
How CPU and memory requests and limits are passed to your app in any unit
the app requires
How a pod can use downwardAPI volumes to get up-to-date metadata, which
may change during the lifetime of the pod (such as labels and annotations) 
How you can browse the Kubernetes REST API through kubectl proxy
How pods can find the API server’s location through environment variables or
DNS, similar to any other Service defined in Kubernetes
How an application running in a pod can verify that it’s talking to the API
server and how it can authenticate itself
How using an ambassador container can make talking to the API server from
within an app much simpler
How client libraries can get you interacting with Kubernetes in minutes
In this chapter, you learned how to talk to the API server, so the next step is learning
more about how it works. You’ll do that in chapter 11, but before we dive into such
details, you still need to learn about two other Kubernetes resources—Deployments
and StatefulSets. They’re explained in the next two chapters.
 
</data>
  <data key="d5">249
Summary
8.3
Summary
After reading this chapter, you now know how your app, running inside a pod, can get
data about itself, other pods, and other components deployed in the cluster. You’ve
learned
How a pod’s name, namespace, and other metadata can be exposed to the pro-
cess either through environment variables or files in a downwardAPI volume
How CPU and memory requests and limits are passed to your app in any unit
the app requires
How a pod can use downwardAPI volumes to get up-to-date metadata, which
may change during the lifetime of the pod (such as labels and annotations) 
How you can browse the Kubernetes REST API through kubectl proxy
How pods can find the API server’s location through environment variables or
DNS, similar to any other Service defined in Kubernetes
How an application running in a pod can verify that it’s talking to the API
server and how it can authenticate itself
How using an ambassador container can make talking to the API server from
within an app much simpler
How client libraries can get you interacting with Kubernetes in minutes
In this chapter, you learned how to talk to the API server, so the next step is learning
more about how it works. You’ll do that in chapter 11, but before we dive into such
details, you still need to learn about two other Kubernetes resources—Deployments
and StatefulSets. They’re explained in the next two chapters.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="282">
  <data key="d0">Page_282</data>
  <data key="d5">Page_282</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_193">
  <data key="d0">250
Deployments: updating
applications declaratively
You now know how to package your app components into containers, group them
into pods, provide them with temporary or permanent storage, pass both secret
and non-secret config data to them, and allow pods to find and talk to each other.
You know how to run a full-fledged system composed of independently running
smaller components—microservices, if you will. Is there anything else? 
 Eventually, you’re going to want to update your app. This chapter covers how to
update apps running in a Kubernetes cluster and how Kubernetes helps you move
toward a true zero-downtime update process. Although this can be achieved using
only ReplicationControllers or ReplicaSets, Kubernetes also provides a Deployment
This chapter covers
Replacing pods with newer versions
Updating managed pods
Updating pods declaratively using Deployment 
resources
Performing rolling updates
Automatically blocking rollouts of bad versions
Controlling the rate of the rollout
Reverting pods to a previous version
 
</data>
  <data key="d5">250
Deployments: updating
applications declaratively
You now know how to package your app components into containers, group them
into pods, provide them with temporary or permanent storage, pass both secret
and non-secret config data to them, and allow pods to find and talk to each other.
You know how to run a full-fledged system composed of independently running
smaller components—microservices, if you will. Is there anything else? 
 Eventually, you’re going to want to update your app. This chapter covers how to
update apps running in a Kubernetes cluster and how Kubernetes helps you move
toward a true zero-downtime update process. Although this can be achieved using
only ReplicationControllers or ReplicaSets, Kubernetes also provides a Deployment
This chapter covers
Replacing pods with newer versions
Updating managed pods
Updating pods declaratively using Deployment 
resources
Performing rolling updates
Automatically blocking rollouts of bad versions
Controlling the rate of the rollout
Reverting pods to a previous version
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="283">
  <data key="d0">Page_283</data>
  <data key="d5">Page_283</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_194">
  <data key="d0">251
Updating applications running in pods
resource that sits on top of ReplicaSets and enables declarative application updates. If
you’re not completely sure what that means, keep reading—it’s not as complicated as
it sounds.
9.1
Updating applications running in pods
Let’s start off with a simple example. Imagine having a set of pod instances providing a
service to other pods and/or external clients. After reading this book up to this point,
you likely recognize that these pods are backed by a ReplicationController or a
ReplicaSet. A Service also exists through which clients (apps running in other pods or
external clients) access the pods. This is how a basic application looks in Kubernetes
(shown in figure 9.1).
Initially, the pods run the first version of your application—let’s suppose its image is
tagged as v1. You then develop a newer version of the app and push it to an image
repository as a new image, tagged as v2. You’d next like to replace all the pods with
this new version. Because you can’t change an existing pod’s image after the pod is
created, you need to remove the old pods and replace them with new ones running
the new image. 
 You have two ways of updating all those pods. You can do one of the following:
Delete all existing pods first and then start the new ones.
Start new ones and, once they’re up, delete the old ones. You can do this either
by adding all the new pods and then deleting all the old ones at once, or
sequentially, by adding new pods and removing old ones gradually.
Both these strategies have their benefits and drawbacks. The first option would lead to
a short period of time when your application is unavailable. The second option
requires your app to handle running two versions of the app at the same time. If your
app stores data in a data store, the new version shouldn’t modify the data schema or
the data in such a way that breaks the previous version.
ReplicationController
or ReplicaSet
Clients
Service
Pod
Pod
Pod
Figure 9.1
The basic outline of an 
application running in Kubernetes
 
</data>
  <data key="d5">251
Updating applications running in pods
resource that sits on top of ReplicaSets and enables declarative application updates. If
you’re not completely sure what that means, keep reading—it’s not as complicated as
it sounds.
9.1
Updating applications running in pods
Let’s start off with a simple example. Imagine having a set of pod instances providing a
service to other pods and/or external clients. After reading this book up to this point,
you likely recognize that these pods are backed by a ReplicationController or a
ReplicaSet. A Service also exists through which clients (apps running in other pods or
external clients) access the pods. This is how a basic application looks in Kubernetes
(shown in figure 9.1).
Initially, the pods run the first version of your application—let’s suppose its image is
tagged as v1. You then develop a newer version of the app and push it to an image
repository as a new image, tagged as v2. You’d next like to replace all the pods with
this new version. Because you can’t change an existing pod’s image after the pod is
created, you need to remove the old pods and replace them with new ones running
the new image. 
 You have two ways of updating all those pods. You can do one of the following:
Delete all existing pods first and then start the new ones.
Start new ones and, once they’re up, delete the old ones. You can do this either
by adding all the new pods and then deleting all the old ones at once, or
sequentially, by adding new pods and removing old ones gradually.
Both these strategies have their benefits and drawbacks. The first option would lead to
a short period of time when your application is unavailable. The second option
requires your app to handle running two versions of the app at the same time. If your
app stores data in a data store, the new version shouldn’t modify the data schema or
the data in such a way that breaks the previous version.
ReplicationController
or ReplicaSet
Clients
Service
Pod
Pod
Pod
Figure 9.1
The basic outline of an 
application running in Kubernetes
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="284">
  <data key="d0">Page_284</data>
  <data key="d5">Page_284</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_195">
  <data key="d0">252
CHAPTER 9
Deployments: updating applications declaratively
 How do you perform these two update methods in Kubernetes? First, let’s look at
how to do this manually; then, once you know what’s involved in the process, you’ll
learn how to have Kubernetes perform the update automatically.
9.1.1
Deleting old pods and replacing them with new ones
You already know how to get a ReplicationController to replace all its pod instances
with pods running a new version. You probably remember the pod template of a
ReplicationController can be updated at any time. When the ReplicationController
creates new instances, it uses the updated pod template to create them.
 If you have a ReplicationController managing a set of v1 pods, you can easily
replace them by modifying the pod template so it refers to version v2 of the image and
then deleting the old pod instances. The ReplicationController will notice that no
pods match its label selector and it will spin up new instances. The whole process is
shown in figure 9.2.
This is the simplest way to update a set of pods, if you can accept the short downtime
between the time the old pods are deleted and new ones are started.
9.1.2
Spinning up new pods and then deleting the old ones
If you don’t want to see any downtime and your app supports running multiple ver-
sions at once, you can turn the process around and first spin up all the new pods and
Pod template
changed
v pods deleted
1
manually
ReplicationController
Service
Pod: v1
Pod: v1
Pod
template: v2
ReplicationController
Pod
template: v2
Pod: v1
Service
Pod: v2
Pod: v2
Pod: v2
ReplicationController
Service
Pod: v1
Pod: v1
Pod
template: v1
Pod: v1
ReplicationController
Service
Pod: v1
Pod: v1
Pod: v1
Pod
template: v2
Short period of
downtime here
v2 pods created by
ReplicationController
Figure 9.2
Updating pods by changing a ReplicationController’s pod template and deleting old Pods
 
</data>
  <data key="d5">252
CHAPTER 9
Deployments: updating applications declaratively
 How do you perform these two update methods in Kubernetes? First, let’s look at
how to do this manually; then, once you know what’s involved in the process, you’ll
learn how to have Kubernetes perform the update automatically.
9.1.1
Deleting old pods and replacing them with new ones
You already know how to get a ReplicationController to replace all its pod instances
with pods running a new version. You probably remember the pod template of a
ReplicationController can be updated at any time. When the ReplicationController
creates new instances, it uses the updated pod template to create them.
 If you have a ReplicationController managing a set of v1 pods, you can easily
replace them by modifying the pod template so it refers to version v2 of the image and
then deleting the old pod instances. The ReplicationController will notice that no
pods match its label selector and it will spin up new instances. The whole process is
shown in figure 9.2.
This is the simplest way to update a set of pods, if you can accept the short downtime
between the time the old pods are deleted and new ones are started.
9.1.2
Spinning up new pods and then deleting the old ones
If you don’t want to see any downtime and your app supports running multiple ver-
sions at once, you can turn the process around and first spin up all the new pods and
Pod template
changed
v pods deleted
1
manually
ReplicationController
Service
Pod: v1
Pod: v1
Pod
template: v2
ReplicationController
Pod
template: v2
Pod: v1
Service
Pod: v2
Pod: v2
Pod: v2
ReplicationController
Service
Pod: v1
Pod: v1
Pod
template: v1
Pod: v1
ReplicationController
Service
Pod: v1
Pod: v1
Pod: v1
Pod
template: v2
Short period of
downtime here
v2 pods created by
ReplicationController
Figure 9.2
Updating pods by changing a ReplicationController’s pod template and deleting old Pods
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="285">
  <data key="d0">Page_285</data>
  <data key="d5">Page_285</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_196">
  <data key="d0">253
Updating applications running in pods
only then delete the old ones. This will require more hardware resources, because
you’ll have double the number of pods running at the same time for a short while. 
 This is a slightly more complex method compared to the previous one, but you
should be able to do it by combining what you’ve learned about ReplicationControl-
lers and Services so far.
SWITCHING FROM THE OLD TO THE NEW VERSION AT ONCE
Pods are usually fronted by a Service. It’s possible to have the Service front only the
initial version of the pods while you bring up the pods running the new version. Then,
once all the new pods are up, you can change the Service’s label selector and have the
Service switch over to the new pods, as shown in figure 9.3. This is called a blue-green
deployment. After switching over, and once you’re sure the new version functions cor-
rectly, you’re free to delete the old pods by deleting the old ReplicationController.
NOTE
You can change a Service’s pod selector with the kubectl set selec-
tor command.
PERFORMING A ROLLING UPDATE
Instead of bringing up all the new pods and deleting the old pods at once, you can
also perform a rolling update, which replaces pods step by step. You do this by slowly
scaling down the previous ReplicationController and scaling up the new one. In this
case, you’ll want the Service’s pod selector to include both the old and the new pods,
so it directs requests toward both sets of pods. See figure 9.4.
 Doing a rolling update manually is laborious and error-prone. Depending on the
number of replicas, you’d need to run a dozen or more commands in the proper
order to perform the update process. Luckily, Kubernetes allows you to perform the
rolling update with a single command. You’ll learn how in the next section.
Service
Service
ReplicationController:
v1
Pod: v1
Pod: v1
Pod
template: v1
Pod: v1
ReplicationController:
v2
Pod
template: v2
Pod: v2
Pod: v2
Pod: v2
ReplicationController:
v1
Pod: v1
Pod: v1
Pod
template: v1
Pod: v1
ReplicationController:
v2
Pod
template: v2
Pod: v2
Pod: v2
Pod: v2
Figure 9.3
Switching a Service from the old pods to the new ones
 
</data>
  <data key="d5">253
Updating applications running in pods
only then delete the old ones. This will require more hardware resources, because
you’ll have double the number of pods running at the same time for a short while. 
 This is a slightly more complex method compared to the previous one, but you
should be able to do it by combining what you’ve learned about ReplicationControl-
lers and Services so far.
SWITCHING FROM THE OLD TO THE NEW VERSION AT ONCE
Pods are usually fronted by a Service. It’s possible to have the Service front only the
initial version of the pods while you bring up the pods running the new version. Then,
once all the new pods are up, you can change the Service’s label selector and have the
Service switch over to the new pods, as shown in figure 9.3. This is called a blue-green
deployment. After switching over, and once you’re sure the new version functions cor-
rectly, you’re free to delete the old pods by deleting the old ReplicationController.
NOTE
You can change a Service’s pod selector with the kubectl set selec-
tor command.
PERFORMING A ROLLING UPDATE
Instead of bringing up all the new pods and deleting the old pods at once, you can
also perform a rolling update, which replaces pods step by step. You do this by slowly
scaling down the previous ReplicationController and scaling up the new one. In this
case, you’ll want the Service’s pod selector to include both the old and the new pods,
so it directs requests toward both sets of pods. See figure 9.4.
 Doing a rolling update manually is laborious and error-prone. Depending on the
number of replicas, you’d need to run a dozen or more commands in the proper
order to perform the update process. Luckily, Kubernetes allows you to perform the
rolling update with a single command. You’ll learn how in the next section.
Service
Service
ReplicationController:
v1
Pod: v1
Pod: v1
Pod
template: v1
Pod: v1
ReplicationController:
v2
Pod
template: v2
Pod: v2
Pod: v2
Pod: v2
ReplicationController:
v1
Pod: v1
Pod: v1
Pod
template: v1
Pod: v1
ReplicationController:
v2
Pod
template: v2
Pod: v2
Pod: v2
Pod: v2
Figure 9.3
Switching a Service from the old pods to the new ones
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="286">
  <data key="d0">Page_286</data>
  <data key="d5">Page_286</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_197">
  <data key="d0">254
CHAPTER 9
Deployments: updating applications declaratively
9.2
Performing an automatic rolling update with a 
ReplicationController
Instead of performing rolling updates using ReplicationControllers manually, you can
have kubectl perform them. Using kubectl to perform the update makes the process
much easier, but, as you’ll see later, this is now an outdated way of updating apps. Nev-
ertheless, we’ll walk through this option first, because it was historically the first way of
doing an automatic rolling update, and also allows us to discuss the process without
introducing too many additional concepts. 
9.2.1
Running the initial version of the app
Obviously, before you can update an app, you need to have an app deployed. You’re
going to use a slightly modified version of the kubia NodeJS app you created in chap-
ter 2 as your initial version. In case you don’t remember what it does, it’s a simple web-
app that returns the pod’s hostname in the HTTP response. 
CREATING THE V1 APP
You’ll change the app so it also returns its version number in the response, which will
allow you to distinguish between the different versions you’re about to build. I’ve
already built and pushed the app image to Docker Hub under luksa/kubia:v1. The
following listing shows the app’s code.
const http = require('http');
const os = require('os');
console.log("Kubia server starting...");
Listing 9.1
The v1 version of our app: v1/app.js
Service
Pod: v1
Pod: v1
Replication
Controller:
v1
v1
Replication
Controller:
v2
Pod: v2
Service
Pod: v2
Pod: v2
Pod: v2
Service
Pod: v1
Pod: v1
Pod: v1
Service
Pod: v1
Pod: v2
Pod: v2
v2
Replication
Controller:
v1
v1
Replication
Controller:
v2
v2
Replication
Controller:
v1
Replication
Controller:
v2
v2
Replication
Controller:
v1
v1
v1
Replication
Controller:
v2
v2
Figure 9.4
A rolling update of pods using two ReplicationControllers
 
</data>
  <data key="d5">254
CHAPTER 9
Deployments: updating applications declaratively
9.2
Performing an automatic rolling update with a 
ReplicationController
Instead of performing rolling updates using ReplicationControllers manually, you can
have kubectl perform them. Using kubectl to perform the update makes the process
much easier, but, as you’ll see later, this is now an outdated way of updating apps. Nev-
ertheless, we’ll walk through this option first, because it was historically the first way of
doing an automatic rolling update, and also allows us to discuss the process without
introducing too many additional concepts. 
9.2.1
Running the initial version of the app
Obviously, before you can update an app, you need to have an app deployed. You’re
going to use a slightly modified version of the kubia NodeJS app you created in chap-
ter 2 as your initial version. In case you don’t remember what it does, it’s a simple web-
app that returns the pod’s hostname in the HTTP response. 
CREATING THE V1 APP
You’ll change the app so it also returns its version number in the response, which will
allow you to distinguish between the different versions you’re about to build. I’ve
already built and pushed the app image to Docker Hub under luksa/kubia:v1. The
following listing shows the app’s code.
const http = require('http');
const os = require('os');
console.log("Kubia server starting...");
Listing 9.1
The v1 version of our app: v1/app.js
Service
Pod: v1
Pod: v1
Replication
Controller:
v1
v1
Replication
Controller:
v2
Pod: v2
Service
Pod: v2
Pod: v2
Pod: v2
Service
Pod: v1
Pod: v1
Pod: v1
Service
Pod: v1
Pod: v2
Pod: v2
v2
Replication
Controller:
v1
v1
Replication
Controller:
v2
v2
Replication
Controller:
v1
Replication
Controller:
v2
v2
Replication
Controller:
v1
v1
v1
Replication
Controller:
v2
v2
Figure 9.4
A rolling update of pods using two ReplicationControllers
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="287">
  <data key="d0">Page_287</data>
  <data key="d5">Page_287</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_198">
  <data key="d0">255
Performing an automatic rolling update with a ReplicationController
var handler = function(request, response) {
  console.log("Received request from " + request.connection.remoteAddress);
  response.writeHead(200);
  response.end("This is v1 running in pod " + os.hostname() + "\n");
};
var www = http.createServer(handler);
www.listen(8080);
RUNNING THE APP AND EXPOSING IT THROUGH A SERVICE USING A SINGLE YAML FILE
To run your app, you’ll create a ReplicationController and a LoadBalancer Service to
enable you to access the app externally. This time, rather than create these two
resources separately, you’ll create a single YAML for both of them and post it to the
Kubernetes API with a single kubectl create command. A YAML manifest can con-
tain multiple objects delimited with a line containing three dashes, as shown in the
following listing.
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia-v1
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:                      
        app: kubia                 
    spec:
      containers:
      - image: luksa/kubia:v1     
        name: nodejs
---                         
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  type: LoadBalancer
  selector:                                        
    app: kubia                                     
  ports:
  - port: 80
    targetPort: 8080
The YAML defines a ReplicationController called kubia-v1 and a Service called
kubia. Go ahead and post the YAML to Kubernetes. After a while, your three v1 pods
and the load balancer should all be running, so you can look up the Service’s external
IP and start hitting the service with curl, as shown in the following listing.
Listing 9.2
A YAML containing an RC and a Service: kubia-rc-and-service-v1.yaml
The Service fronts all 
pods created by the 
ReplicationController.
You’re creating a 
ReplicationController for 
pods running this image.
YAML files can contain 
multiple resource 
definitions separated by 
a line with three dashes.
 
</data>
  <data key="d5">255
Performing an automatic rolling update with a ReplicationController
var handler = function(request, response) {
  console.log("Received request from " + request.connection.remoteAddress);
  response.writeHead(200);
  response.end("This is v1 running in pod " + os.hostname() + "\n");
};
var www = http.createServer(handler);
www.listen(8080);
RUNNING THE APP AND EXPOSING IT THROUGH A SERVICE USING A SINGLE YAML FILE
To run your app, you’ll create a ReplicationController and a LoadBalancer Service to
enable you to access the app externally. This time, rather than create these two
resources separately, you’ll create a single YAML for both of them and post it to the
Kubernetes API with a single kubectl create command. A YAML manifest can con-
tain multiple objects delimited with a line containing three dashes, as shown in the
following listing.
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia-v1
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:                      
        app: kubia                 
    spec:
      containers:
      - image: luksa/kubia:v1     
        name: nodejs
---                         
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  type: LoadBalancer
  selector:                                        
    app: kubia                                     
  ports:
  - port: 80
    targetPort: 8080
The YAML defines a ReplicationController called kubia-v1 and a Service called
kubia. Go ahead and post the YAML to Kubernetes. After a while, your three v1 pods
and the load balancer should all be running, so you can look up the Service’s external
IP and start hitting the service with curl, as shown in the following listing.
Listing 9.2
A YAML containing an RC and a Service: kubia-rc-and-service-v1.yaml
The Service fronts all 
pods created by the 
ReplicationController.
You’re creating a 
ReplicationController for 
pods running this image.
YAML files can contain 
multiple resource 
definitions separated by 
a line with three dashes.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="288">
  <data key="d0">Page_288</data>
  <data key="d5">Page_288</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_199">
  <data key="d0">256
CHAPTER 9
Deployments: updating applications declaratively
$ kubectl get svc kubia
NAME      CLUSTER-IP     EXTERNAL-IP       PORT(S)         AGE
kubia     10.3.246.195   130.211.109.222   80:32143/TCP    5m
$ while true; do curl http://130.211.109.222; done
This is v1 running in pod kubia-v1-qr192
This is v1 running in pod kubia-v1-kbtsk
This is v1 running in pod kubia-v1-qr192
This is v1 running in pod kubia-v1-2321o
...
NOTE
If you’re using Minikube or any other Kubernetes cluster where load
balancer services aren’t supported, you can use the Service’s node port to
access the app. This was explained in chapter 5.
9.2.2
Performing a rolling update with kubectl
Next you’ll create version 2 of the app. To keep things simple, all you’ll do is change
the response to say, “This is v2”:
  response.end("This is v2 running in pod " + os.hostname() + "\n");
This new version is available in the image luksa/kubia:v2 on Docker Hub, so you
don’t need to build it yourself.
Listing 9.3
Getting the Service’s external IP and hitting the service in a loop with curl
Pushing updates to the same image tag
Modifying an app and pushing the changes to the same image tag isn’t a good idea,
but we all tend to do that during development. If you’re modifying the latest tag,
that’s not a problem, but when you’re tagging an image with a different tag (for exam-
ple, tag v1 instead of latest), once the image is pulled by a worker node, the image
will be stored on the node and not pulled again when a new pod using the same
image is run (at least that’s the default policy for pulling images).
That means any changes you make to the image won’t be picked up if you push them
to the same tag. If a new pod is scheduled to the same node, the Kubelet will run the
old version of the image. On the other hand, nodes that haven’t run the old version
will pull and run the new image, so you might end up with two different versions of
the pod running. To make sure this doesn’t happen, you need to set the container’s
imagePullPolicy property to Always. 
You need to be aware that the default imagePullPolicy depends on the image tag.
If a container refers to the latest tag (either explicitly or by not specifying the tag at
all), imagePullPolicy defaults to Always, but if the container refers to any other
tag, the policy defaults to IfNotPresent. 
When using a tag other than latest, you need to set the imagePullPolicy properly
if you make changes to an image without changing the tag. Or better yet, make sure
you always push changes to an image under a new tag.
 
</data>
  <data key="d5">256
CHAPTER 9
Deployments: updating applications declaratively
$ kubectl get svc kubia
NAME      CLUSTER-IP     EXTERNAL-IP       PORT(S)         AGE
kubia     10.3.246.195   130.211.109.222   80:32143/TCP    5m
$ while true; do curl http://130.211.109.222; done
This is v1 running in pod kubia-v1-qr192
This is v1 running in pod kubia-v1-kbtsk
This is v1 running in pod kubia-v1-qr192
This is v1 running in pod kubia-v1-2321o
...
NOTE
If you’re using Minikube or any other Kubernetes cluster where load
balancer services aren’t supported, you can use the Service’s node port to
access the app. This was explained in chapter 5.
9.2.2
Performing a rolling update with kubectl
Next you’ll create version 2 of the app. To keep things simple, all you’ll do is change
the response to say, “This is v2”:
  response.end("This is v2 running in pod " + os.hostname() + "\n");
This new version is available in the image luksa/kubia:v2 on Docker Hub, so you
don’t need to build it yourself.
Listing 9.3
Getting the Service’s external IP and hitting the service in a loop with curl
Pushing updates to the same image tag
Modifying an app and pushing the changes to the same image tag isn’t a good idea,
but we all tend to do that during development. If you’re modifying the latest tag,
that’s not a problem, but when you’re tagging an image with a different tag (for exam-
ple, tag v1 instead of latest), once the image is pulled by a worker node, the image
will be stored on the node and not pulled again when a new pod using the same
image is run (at least that’s the default policy for pulling images).
That means any changes you make to the image won’t be picked up if you push them
to the same tag. If a new pod is scheduled to the same node, the Kubelet will run the
old version of the image. On the other hand, nodes that haven’t run the old version
will pull and run the new image, so you might end up with two different versions of
the pod running. To make sure this doesn’t happen, you need to set the container’s
imagePullPolicy property to Always. 
You need to be aware that the default imagePullPolicy depends on the image tag.
If a container refers to the latest tag (either explicitly or by not specifying the tag at
all), imagePullPolicy defaults to Always, but if the container refers to any other
tag, the policy defaults to IfNotPresent. 
When using a tag other than latest, you need to set the imagePullPolicy properly
if you make changes to an image without changing the tag. Or better yet, make sure
you always push changes to an image under a new tag.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="289">
  <data key="d0">Page_289</data>
  <data key="d5">Page_289</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_200">
  <data key="d0">257
Performing an automatic rolling update with a ReplicationController
Keep the curl loop running and open another terminal, where you’ll get the rolling
update started. To perform the update, you’ll run the kubectl rolling-update com-
mand. All you need to do is tell it which ReplicationController you’re replacing, give a
name for the new ReplicationController, and specify the new image you’d like to
replace the original one with. The following listing shows the full command for per-
forming the rolling update.
$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2
Created kubia-v2
Scaling up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0 (keep 3 
pods available, don't exceed 4 pods)
...
Because you’re replacing ReplicationController kubia-v1 with one running version 2
of your kubia app, you’d like the new ReplicationController to be called kubia-v2
and use the luksa/kubia:v2 container image. 
 When you run the command, a new ReplicationController called kubia-v2 is cre-
ated immediately. The state of the system at this point is shown in figure 9.5.
The new ReplicationController’s pod template references the luksa/kubia:v2 image
and its initial desired replica count is set to 0, as you can see in the following listing.
$ kubectl describe rc kubia-v2
Name:       kubia-v2
Namespace:  default
Image(s):   luksa/kubia:v2          
Selector:   app=kubia,deployment=757d16a0f02f6a5c387f2b5edb62b155
Labels:     app=kubia            
Replicas:   0 current / 0 desired    
...
Listing 9.4
Initiating a rolling-update of a ReplicationController using kubectl
Listing 9.5
Describing the new ReplicationController created by the rolling update
Pod: v1
Pod: v1
No v2 pods yet
Pod: v1
ReplicationController: kubia-v1
Image: kubia/v1
Replicas: 3
ReplicationController: kubia-v2
Image: kubia/v2
Replicas: 0
Figure 9.5
The state of the system immediately after starting the rolling update
The new 
ReplicationController 
refers to the v2 image.
Initially, the desired 
number of replicas is zero.
 
</data>
  <data key="d5">257
Performing an automatic rolling update with a ReplicationController
Keep the curl loop running and open another terminal, where you’ll get the rolling
update started. To perform the update, you’ll run the kubectl rolling-update com-
mand. All you need to do is tell it which ReplicationController you’re replacing, give a
name for the new ReplicationController, and specify the new image you’d like to
replace the original one with. The following listing shows the full command for per-
forming the rolling update.
$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2
Created kubia-v2
Scaling up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0 (keep 3 
pods available, don't exceed 4 pods)
...
Because you’re replacing ReplicationController kubia-v1 with one running version 2
of your kubia app, you’d like the new ReplicationController to be called kubia-v2
and use the luksa/kubia:v2 container image. 
 When you run the command, a new ReplicationController called kubia-v2 is cre-
ated immediately. The state of the system at this point is shown in figure 9.5.
The new ReplicationController’s pod template references the luksa/kubia:v2 image
and its initial desired replica count is set to 0, as you can see in the following listing.
$ kubectl describe rc kubia-v2
Name:       kubia-v2
Namespace:  default
Image(s):   luksa/kubia:v2          
Selector:   app=kubia,deployment=757d16a0f02f6a5c387f2b5edb62b155
Labels:     app=kubia            
Replicas:   0 current / 0 desired    
...
Listing 9.4
Initiating a rolling-update of a ReplicationController using kubectl
Listing 9.5
Describing the new ReplicationController created by the rolling update
Pod: v1
Pod: v1
No v2 pods yet
Pod: v1
ReplicationController: kubia-v1
Image: kubia/v1
Replicas: 3
ReplicationController: kubia-v2
Image: kubia/v2
Replicas: 0
Figure 9.5
The state of the system immediately after starting the rolling update
The new 
ReplicationController 
refers to the v2 image.
Initially, the desired 
number of replicas is zero.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="290">
  <data key="d0">Page_290</data>
  <data key="d5">Page_290</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_201">
  <data key="d0">258
CHAPTER 9
Deployments: updating applications declaratively
UNDERSTANDING THE STEPS PERFORMED BY KUBECTL BEFORE THE ROLLING UPDATE COMMENCES
kubectl created this ReplicationController by copying the kubia-v1 controller and
changing the image in its pod template. If you look closely at the controller’s label
selector, you’ll notice it has been modified, too. It includes not only a simple
app=kubia label, but also an additional deployment label which the pods must have in
order to be managed by this ReplicationController.
 You probably know this already, but this is necessary to avoid having both the new
and the old ReplicationControllers operating on the same set of pods. But even if pods
created by the new controller have the additional deployment label in addition to the
app=kubia label, doesn’t this mean they’ll be selected by the first ReplicationControl-
ler’s selector, because it’s set to app=kubia? 
 Yes, that’s exactly what would happen, but there’s a catch. The rolling-update pro-
cess has modified the selector of the first ReplicationController, as well:
$ kubectl describe rc kubia-v1
Name:       kubia-v1
Namespace:  default
Image(s):   luksa/kubia:v1
Selector:   app=kubia,deployment=3ddd307978b502a5b975ed4045ae4964-orig 
Okay, but doesn’t this mean the first controller now sees zero pods matching its selec-
tor, because the three pods previously created by it contain only the app=kubia label?
No, because kubectl had also modified the labels of the live pods just before modify-
ing the ReplicationController’s selector:
$ kubectl get po --show-labels
NAME            READY  STATUS   RESTARTS  AGE  LABELS
kubia-v1-m33mv  1/1    Running  0         2m   app=kubia,deployment=3ddd...
kubia-v1-nmzw9  1/1    Running  0         2m   app=kubia,deployment=3ddd...
kubia-v1-cdtey  1/1    Running  0         2m   app=kubia,deployment=3ddd...
If this is getting too complicated, examine figure 9.6, which shows the pods, their
labels, and the two ReplicationControllers, along with their pod selectors.
ReplicationController: kubia-v1
Replicas: 3
Selector: app=kubia,
deployment=3ddd…
ReplicationController: kubia-v2
Replicas: 0
Selector: app=kubia,
deployment=757d...
deployment: 3ddd...
app: kubia
Pod: v1
deployment: 3ddd...
app: kubia
Pod: v1
deployment: 3ddd...
app: kubia
Pod: v1
Figure 9.6
Detailed state of the old and new ReplicationControllers and pods at the start of a rolling 
update
 
</data>
  <data key="d5">258
CHAPTER 9
Deployments: updating applications declaratively
UNDERSTANDING THE STEPS PERFORMED BY KUBECTL BEFORE THE ROLLING UPDATE COMMENCES
kubectl created this ReplicationController by copying the kubia-v1 controller and
changing the image in its pod template. If you look closely at the controller’s label
selector, you’ll notice it has been modified, too. It includes not only a simple
app=kubia label, but also an additional deployment label which the pods must have in
order to be managed by this ReplicationController.
 You probably know this already, but this is necessary to avoid having both the new
and the old ReplicationControllers operating on the same set of pods. But even if pods
created by the new controller have the additional deployment label in addition to the
app=kubia label, doesn’t this mean they’ll be selected by the first ReplicationControl-
ler’s selector, because it’s set to app=kubia? 
 Yes, that’s exactly what would happen, but there’s a catch. The rolling-update pro-
cess has modified the selector of the first ReplicationController, as well:
$ kubectl describe rc kubia-v1
Name:       kubia-v1
Namespace:  default
Image(s):   luksa/kubia:v1
Selector:   app=kubia,deployment=3ddd307978b502a5b975ed4045ae4964-orig 
Okay, but doesn’t this mean the first controller now sees zero pods matching its selec-
tor, because the three pods previously created by it contain only the app=kubia label?
No, because kubectl had also modified the labels of the live pods just before modify-
ing the ReplicationController’s selector:
$ kubectl get po --show-labels
NAME            READY  STATUS   RESTARTS  AGE  LABELS
kubia-v1-m33mv  1/1    Running  0         2m   app=kubia,deployment=3ddd...
kubia-v1-nmzw9  1/1    Running  0         2m   app=kubia,deployment=3ddd...
kubia-v1-cdtey  1/1    Running  0         2m   app=kubia,deployment=3ddd...
If this is getting too complicated, examine figure 9.6, which shows the pods, their
labels, and the two ReplicationControllers, along with their pod selectors.
ReplicationController: kubia-v1
Replicas: 3
Selector: app=kubia,
deployment=3ddd…
ReplicationController: kubia-v2
Replicas: 0
Selector: app=kubia,
deployment=757d...
deployment: 3ddd...
app: kubia
Pod: v1
deployment: 3ddd...
app: kubia
Pod: v1
deployment: 3ddd...
app: kubia
Pod: v1
Figure 9.6
Detailed state of the old and new ReplicationControllers and pods at the start of a rolling 
update
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="291">
  <data key="d0">Page_291</data>
  <data key="d5">Page_291</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_202">
  <data key="d0">259
Performing an automatic rolling update with a ReplicationController
kubectl had to do all this before even starting to scale anything up or down. Now
imagine doing the rolling update manually. It’s easy to see yourself making a mistake
here and possibly having the ReplicationController kill off all your pods—pods that
are actively serving your production clients!
REPLACING OLD PODS WITH NEW ONES BY SCALING THE TWO REPLICATIONCONTROLLERS
After setting up all this, kubectl starts replacing pods by first scaling up the new
controller to 1. The controller thus creates the first v2 pod. kubectl then scales
down the old ReplicationController by 1. This is shown in the next two lines printed
by kubectl:
Scaling kubia-v2 up to 1
Scaling kubia-v1 down to 2
Because the Service is targeting all pods with the app=kubia label, you should start see-
ing your curl requests redirected to the new v2 pod every few loop iterations:
This is v2 running in pod kubia-v2-nmzw9      
This is v1 running in pod kubia-v1-kbtsk
This is v1 running in pod kubia-v1-2321o
This is v2 running in pod kubia-v2-nmzw9      
...
Figure 9.7 shows the current state of the system.
As kubectl continues with the rolling update, you start seeing a progressively bigger
percentage of requests hitting v2 pods, as the update process deletes more of the v1
pods and replaces them with those running your new image. Eventually, the original
Requests hitting the pod 
running the new version
ReplicationController: kubia-v1
Replicas: 2
Selector: app=kubia,
deployment=3ddd…
ReplicationController: kubia-v2
Replicas: 1
Selector: app=kubia,
deployment=757d…
deployment: 3ddd...
app: kubia
Pod: v1
deployment: 3ddd...
app: kubia
Pod: v1
deployment: 757d...
app: kubia
Pod: v2
curl
Service
Selector: app=kubia
Figure 9.7
The Service is redirecting requests to both the old and new pods during the 
rolling update.
 
</data>
  <data key="d5">259
Performing an automatic rolling update with a ReplicationController
kubectl had to do all this before even starting to scale anything up or down. Now
imagine doing the rolling update manually. It’s easy to see yourself making a mistake
here and possibly having the ReplicationController kill off all your pods—pods that
are actively serving your production clients!
REPLACING OLD PODS WITH NEW ONES BY SCALING THE TWO REPLICATIONCONTROLLERS
After setting up all this, kubectl starts replacing pods by first scaling up the new
controller to 1. The controller thus creates the first v2 pod. kubectl then scales
down the old ReplicationController by 1. This is shown in the next two lines printed
by kubectl:
Scaling kubia-v2 up to 1
Scaling kubia-v1 down to 2
Because the Service is targeting all pods with the app=kubia label, you should start see-
ing your curl requests redirected to the new v2 pod every few loop iterations:
This is v2 running in pod kubia-v2-nmzw9      
This is v1 running in pod kubia-v1-kbtsk
This is v1 running in pod kubia-v1-2321o
This is v2 running in pod kubia-v2-nmzw9      
...
Figure 9.7 shows the current state of the system.
As kubectl continues with the rolling update, you start seeing a progressively bigger
percentage of requests hitting v2 pods, as the update process deletes more of the v1
pods and replaces them with those running your new image. Eventually, the original
Requests hitting the pod 
running the new version
ReplicationController: kubia-v1
Replicas: 2
Selector: app=kubia,
deployment=3ddd…
ReplicationController: kubia-v2
Replicas: 1
Selector: app=kubia,
deployment=757d…
deployment: 3ddd...
app: kubia
Pod: v1
deployment: 3ddd...
app: kubia
Pod: v1
deployment: 757d...
app: kubia
Pod: v2
curl
Service
Selector: app=kubia
Figure 9.7
The Service is redirecting requests to both the old and new pods during the 
rolling update.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="292">
  <data key="d0">Page_292</data>
  <data key="d5">Page_292</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_203">
  <data key="d0">260
CHAPTER 9
Deployments: updating applications declaratively
ReplicationController is scaled to zero, causing the last v1 pod to be deleted, which
means the Service will now be backed by v2 pods only. At that point, kubectl will
delete the original ReplicationController and the update process will be finished, as
shown in the following listing.
...
Scaling kubia-v2 up to 2
Scaling kubia-v1 down to 1
Scaling kubia-v2 up to 3
Scaling kubia-v1 down to 0
Update succeeded. Deleting kubia-v1
replicationcontroller "kubia-v1" rolling updated to "kubia-v2"
You’re now left with only the kubia-v2 ReplicationController and three v2 pods. All
throughout this update process, you’ve hit your service and gotten a response every
time. You have, in fact, performed a rolling update with zero downtime. 
9.2.3
Understanding why kubectl rolling-update is now obsolete
At the beginning of this section, I mentioned an even better way of doing updates
than through kubectl rolling-update. What’s so wrong with this process that a bet-
ter one had to be introduced? 
 Well, for starters, I, for one, don’t like Kubernetes modifying objects I’ve created.
Okay, it’s perfectly fine for the scheduler to assign a node to my pods after I create
them, but Kubernetes modifying the labels of my pods and the label selectors of my
ReplicationControllers is something that I don’t expect and could cause me to go
around the office yelling at my colleagues, “Who’s been messing with my controllers!?!?” 
 But even more importantly, if you’ve paid close attention to the words I’ve used,
you probably noticed that all this time I said explicitly that the kubectl client was the
one performing all these steps of the rolling update. 
 You can see this by turning on verbose logging with the --v option when triggering
the rolling update:
$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 --v 6
TIP
Using the --v 6 option increases the logging level enough to let you see
the requests kubectl is sending to the API server.
Using this option, kubectl will print out each HTTP request it sends to the Kuberne-
tes API server. You’ll see PUT requests to
/api/v1/namespaces/default/replicationcontrollers/kubia-v1
which is the RESTful URL representing your kubia-v1 ReplicationController resource.
These requests are the ones scaling down your ReplicationController, which shows
Listing 9.6
The final steps performed by kubectl rolling-update
 
</data>
  <data key="d5">260
CHAPTER 9
Deployments: updating applications declaratively
ReplicationController is scaled to zero, causing the last v1 pod to be deleted, which
means the Service will now be backed by v2 pods only. At that point, kubectl will
delete the original ReplicationController and the update process will be finished, as
shown in the following listing.
...
Scaling kubia-v2 up to 2
Scaling kubia-v1 down to 1
Scaling kubia-v2 up to 3
Scaling kubia-v1 down to 0
Update succeeded. Deleting kubia-v1
replicationcontroller "kubia-v1" rolling updated to "kubia-v2"
You’re now left with only the kubia-v2 ReplicationController and three v2 pods. All
throughout this update process, you’ve hit your service and gotten a response every
time. You have, in fact, performed a rolling update with zero downtime. 
9.2.3
Understanding why kubectl rolling-update is now obsolete
At the beginning of this section, I mentioned an even better way of doing updates
than through kubectl rolling-update. What’s so wrong with this process that a bet-
ter one had to be introduced? 
 Well, for starters, I, for one, don’t like Kubernetes modifying objects I’ve created.
Okay, it’s perfectly fine for the scheduler to assign a node to my pods after I create
them, but Kubernetes modifying the labels of my pods and the label selectors of my
ReplicationControllers is something that I don’t expect and could cause me to go
around the office yelling at my colleagues, “Who’s been messing with my controllers!?!?” 
 But even more importantly, if you’ve paid close attention to the words I’ve used,
you probably noticed that all this time I said explicitly that the kubectl client was the
one performing all these steps of the rolling update. 
 You can see this by turning on verbose logging with the --v option when triggering
the rolling update:
$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 --v 6
TIP
Using the --v 6 option increases the logging level enough to let you see
the requests kubectl is sending to the API server.
Using this option, kubectl will print out each HTTP request it sends to the Kuberne-
tes API server. You’ll see PUT requests to
/api/v1/namespaces/default/replicationcontrollers/kubia-v1
which is the RESTful URL representing your kubia-v1 ReplicationController resource.
These requests are the ones scaling down your ReplicationController, which shows
Listing 9.6
The final steps performed by kubectl rolling-update
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="293">
  <data key="d0">Page_293</data>
  <data key="d5">Page_293</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_204">
  <data key="d0">261
Using Deployments for updating apps declaratively
that the kubectl client is the one doing the scaling, instead of it being performed by
the Kubernetes master. 
TIP
Use the verbose logging option when running other kubectl commands,
to learn more about the communication between kubectl and the API server. 
But why is it such a bad thing that the update process is being performed by the client
instead of on the server? Well, in your case, the update went smoothly, but what if you
lost network connectivity while kubectl was performing the update? The update pro-
cess would be interrupted mid-way. Pods and ReplicationControllers would end up in
an intermediate state.
 Another reason why performing an update like this isn’t as good as it could be is
because it’s imperative. Throughout this book, I’ve stressed how Kubernetes is about
you telling it the desired state of the system and having Kubernetes achieve that
state on its own, by figuring out the best way to do it. This is how pods are deployed
and how pods are scaled up and down. You never tell Kubernetes to add an addi-
tional pod or remove an excess one—you change the number of desired replicas
and that’s it.
 Similarly, you will also want to change the desired image tag in your pod defini-
tions and have Kubernetes replace the pods with new ones running the new image.
This is exactly what drove the introduction of a new resource called a Deployment,
which is now the preferred way of deploying applications in Kubernetes. 
9.3
Using Deployments for updating apps declaratively
A Deployment is a higher-level resource meant for deploying applications and
updating them declaratively, instead of doing it through a ReplicationController or
a ReplicaSet, which are both considered lower-level concepts.
 When you create a Deployment, a ReplicaSet resource is created underneath
(eventually more of them). As you may remember from chapter 4, ReplicaSets are a
new generation of ReplicationControllers, and should be used instead of them. Replica-
Sets replicate and manage pods, as well. When using a Deployment, the actual pods
are created and managed by the Deployment’s ReplicaSets, not by the Deployment
directly (the relationship is shown in figure 9.8).
You might wonder why you’d want to complicate things by introducing another object
on top of a ReplicationController or ReplicaSet, when they’re what suffices to keep a set
of pod instances running. As the rolling update example in section 9.2 demonstrates,
when updating the app, you need to introduce an additional ReplicationController and
Pods
ReplicaSet
Deployment
Figure 9.8
A Deployment is backed 
by a ReplicaSet, which supervises the 
deployment’s pods.
 
</data>
  <data key="d5">261
Using Deployments for updating apps declaratively
that the kubectl client is the one doing the scaling, instead of it being performed by
the Kubernetes master. 
TIP
Use the verbose logging option when running other kubectl commands,
to learn more about the communication between kubectl and the API server. 
But why is it such a bad thing that the update process is being performed by the client
instead of on the server? Well, in your case, the update went smoothly, but what if you
lost network connectivity while kubectl was performing the update? The update pro-
cess would be interrupted mid-way. Pods and ReplicationControllers would end up in
an intermediate state.
 Another reason why performing an update like this isn’t as good as it could be is
because it’s imperative. Throughout this book, I’ve stressed how Kubernetes is about
you telling it the desired state of the system and having Kubernetes achieve that
state on its own, by figuring out the best way to do it. This is how pods are deployed
and how pods are scaled up and down. You never tell Kubernetes to add an addi-
tional pod or remove an excess one—you change the number of desired replicas
and that’s it.
 Similarly, you will also want to change the desired image tag in your pod defini-
tions and have Kubernetes replace the pods with new ones running the new image.
This is exactly what drove the introduction of a new resource called a Deployment,
which is now the preferred way of deploying applications in Kubernetes. 
9.3
Using Deployments for updating apps declaratively
A Deployment is a higher-level resource meant for deploying applications and
updating them declaratively, instead of doing it through a ReplicationController or
a ReplicaSet, which are both considered lower-level concepts.
 When you create a Deployment, a ReplicaSet resource is created underneath
(eventually more of them). As you may remember from chapter 4, ReplicaSets are a
new generation of ReplicationControllers, and should be used instead of them. Replica-
Sets replicate and manage pods, as well. When using a Deployment, the actual pods
are created and managed by the Deployment’s ReplicaSets, not by the Deployment
directly (the relationship is shown in figure 9.8).
You might wonder why you’d want to complicate things by introducing another object
on top of a ReplicationController or ReplicaSet, when they’re what suffices to keep a set
of pod instances running. As the rolling update example in section 9.2 demonstrates,
when updating the app, you need to introduce an additional ReplicationController and
Pods
ReplicaSet
Deployment
Figure 9.8
A Deployment is backed 
by a ReplicaSet, which supervises the 
deployment’s pods.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="294">
  <data key="d0">Page_294</data>
  <data key="d5">Page_294</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_205">
  <data key="d0">262
CHAPTER 9
Deployments: updating applications declaratively
coordinate the two controllers to dance around each other without stepping on each
other’s toes. You need something coordinating this dance. A Deployment resource
takes care of that (it’s not the Deployment resource itself, but the controller process
running in the Kubernetes control plane that does that; but we’ll get to that in chap-
ter 11).
 Using a Deployment instead of the lower-level constructs makes updating an app
much easier, because you’re defining the desired state through the single Deployment
resource and letting Kubernetes take care of the rest, as you’ll see in the next few pages.
9.3.1
Creating a Deployment
Creating a Deployment isn’t that different from creating a ReplicationController. A
Deployment is also composed of a label selector, a desired replica count, and a pod
template. In addition to that, it also contains a field, which specifies a deployment
strategy that defines how an update should be performed when the Deployment
resource is modified.  
CREATING A DEPLOYMENT MANIFEST
Let’s see how to use the kubia-v1 ReplicationController example from earlier in this
chapter and modify it so it describes a Deployment instead of a ReplicationController.
As you’ll see, this requires only three trivial changes. The following listing shows the
modified YAML.
apiVersion: apps/v1beta1          
kind: Deployment                  
metadata:
  name: kubia          
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
NOTE
You’ll find an older version of the Deployment resource in extensions/
v1beta1, and a newer one in apps/v1beta2 with different required fields and
different defaults. Be aware that kubectl explain shows the older version.
Because the ReplicationController from before was managing a specific version of the
pods, you called it kubia-v1. A Deployment, on the other hand, is above that version
stuff. At a given point in time, the Deployment can have multiple pod versions run-
ning under its wing, so its name shouldn’t reference the app version.
Listing 9.7
A Deployment definition: kubia-deployment-v1.yaml
Deployments are in the apps 
API group, version v1beta1.
You’ve changed the kind 
from ReplicationController 
to Deployment.
There’s no need to include 
the version in the name of 
the Deployment.
 
</data>
  <data key="d5">262
CHAPTER 9
Deployments: updating applications declaratively
coordinate the two controllers to dance around each other without stepping on each
other’s toes. You need something coordinating this dance. A Deployment resource
takes care of that (it’s not the Deployment resource itself, but the controller process
running in the Kubernetes control plane that does that; but we’ll get to that in chap-
ter 11).
 Using a Deployment instead of the lower-level constructs makes updating an app
much easier, because you’re defining the desired state through the single Deployment
resource and letting Kubernetes take care of the rest, as you’ll see in the next few pages.
9.3.1
Creating a Deployment
Creating a Deployment isn’t that different from creating a ReplicationController. A
Deployment is also composed of a label selector, a desired replica count, and a pod
template. In addition to that, it also contains a field, which specifies a deployment
strategy that defines how an update should be performed when the Deployment
resource is modified.  
CREATING A DEPLOYMENT MANIFEST
Let’s see how to use the kubia-v1 ReplicationController example from earlier in this
chapter and modify it so it describes a Deployment instead of a ReplicationController.
As you’ll see, this requires only three trivial changes. The following listing shows the
modified YAML.
apiVersion: apps/v1beta1          
kind: Deployment                  
metadata:
  name: kubia          
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
NOTE
You’ll find an older version of the Deployment resource in extensions/
v1beta1, and a newer one in apps/v1beta2 with different required fields and
different defaults. Be aware that kubectl explain shows the older version.
Because the ReplicationController from before was managing a specific version of the
pods, you called it kubia-v1. A Deployment, on the other hand, is above that version
stuff. At a given point in time, the Deployment can have multiple pod versions run-
ning under its wing, so its name shouldn’t reference the app version.
Listing 9.7
A Deployment definition: kubia-deployment-v1.yaml
Deployments are in the apps 
API group, version v1beta1.
You’ve changed the kind 
from ReplicationController 
to Deployment.
There’s no need to include 
the version in the name of 
the Deployment.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="295">
  <data key="d0">Page_295</data>
  <data key="d5">Page_295</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_206">
  <data key="d0">263
Using Deployments for updating apps declaratively
CREATING THE DEPLOYMENT RESOURCE
Before you create this Deployment, make sure you delete any ReplicationControllers
and pods that are still running, but keep the kubia Service for now. You can use the
--all switch to delete all those ReplicationControllers like this:
$ kubectl delete rc --all
You’re now ready to create the Deployment: 
$ kubectl create -f kubia-deployment-v1.yaml --record
deployment "kubia" created
TIP
Be sure to include the --record command-line option when creating it.
This records the command in the revision history, which will be useful later.
DISPLAYING THE STATUS OF THE DEPLOYMENT ROLLOUT
You can use the usual kubectl get deployment and the kubectl describe deployment
commands to see details of the Deployment, but let me point you to an additional
command, which is made specifically for checking a Deployment’s status:
$ kubectl rollout status deployment kubia
deployment kubia successfully rolled out
According to this, the Deployment has been successfully rolled out, so you should see
the three pod replicas up and running. Let’s see:
$ kubectl get po
NAME                     READY     STATUS    RESTARTS   AGE
kubia-1506449474-otnnh   1/1       Running   0          14s
kubia-1506449474-vmn7s   1/1       Running   0          14s
kubia-1506449474-xis6m   1/1       Running   0          14s
UNDERSTANDING HOW DEPLOYMENTS CREATE REPLICASETS, WHICH THEN CREATE THE PODS
Take note of the names of these pods. Earlier, when you used a ReplicationController
to create pods, their names were composed of the name of the controller plus a ran-
domly generated string (for example, kubia-v1-m33mv). The three pods created by
the Deployment include an additional numeric value in the middle of their names.
What is that exactly?
 The number corresponds to the hashed value of the pod template in the Deploy-
ment and the ReplicaSet managing these pods. As we said earlier, a Deployment
doesn’t manage pods directly. Instead, it creates ReplicaSets and leaves the managing
to them, so let’s look at the ReplicaSet created by your Deployment:
$ kubectl get replicasets
NAME               DESIRED   CURRENT   AGE
kubia-1506449474   3         3         10s
The ReplicaSet’s name also contains the hash value of its pod template. As you’ll see
later, a Deployment creates multiple ReplicaSets—one for each version of the pod
 
</data>
  <data key="d5">263
Using Deployments for updating apps declaratively
CREATING THE DEPLOYMENT RESOURCE
Before you create this Deployment, make sure you delete any ReplicationControllers
and pods that are still running, but keep the kubia Service for now. You can use the
--all switch to delete all those ReplicationControllers like this:
$ kubectl delete rc --all
You’re now ready to create the Deployment: 
$ kubectl create -f kubia-deployment-v1.yaml --record
deployment "kubia" created
TIP
Be sure to include the --record command-line option when creating it.
This records the command in the revision history, which will be useful later.
DISPLAYING THE STATUS OF THE DEPLOYMENT ROLLOUT
You can use the usual kubectl get deployment and the kubectl describe deployment
commands to see details of the Deployment, but let me point you to an additional
command, which is made specifically for checking a Deployment’s status:
$ kubectl rollout status deployment kubia
deployment kubia successfully rolled out
According to this, the Deployment has been successfully rolled out, so you should see
the three pod replicas up and running. Let’s see:
$ kubectl get po
NAME                     READY     STATUS    RESTARTS   AGE
kubia-1506449474-otnnh   1/1       Running   0          14s
kubia-1506449474-vmn7s   1/1       Running   0          14s
kubia-1506449474-xis6m   1/1       Running   0          14s
UNDERSTANDING HOW DEPLOYMENTS CREATE REPLICASETS, WHICH THEN CREATE THE PODS
Take note of the names of these pods. Earlier, when you used a ReplicationController
to create pods, their names were composed of the name of the controller plus a ran-
domly generated string (for example, kubia-v1-m33mv). The three pods created by
the Deployment include an additional numeric value in the middle of their names.
What is that exactly?
 The number corresponds to the hashed value of the pod template in the Deploy-
ment and the ReplicaSet managing these pods. As we said earlier, a Deployment
doesn’t manage pods directly. Instead, it creates ReplicaSets and leaves the managing
to them, so let’s look at the ReplicaSet created by your Deployment:
$ kubectl get replicasets
NAME               DESIRED   CURRENT   AGE
kubia-1506449474   3         3         10s
The ReplicaSet’s name also contains the hash value of its pod template. As you’ll see
later, a Deployment creates multiple ReplicaSets—one for each version of the pod
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="296">
  <data key="d0">Page_296</data>
  <data key="d5">Page_296</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_207">
  <data key="d0">264
CHAPTER 9
Deployments: updating applications declaratively
template. Using the hash value of the pod template like this allows the Deployment
to always use the same (possibly existing) ReplicaSet for a given version of the pod
template.
ACCESSING THE PODS THROUGH THE SERVICE
With the three replicas created by this ReplicaSet now running, you can use the Ser-
vice you created a while ago to access them, because you made the new pods’ labels
match the Service’s label selector. 
 Up until this point, you probably haven’t seen a good-enough reason why you should
use Deployments over ReplicationControllers. Luckily, creating a Deployment also hasn’t
been any harder than creating a ReplicationController. Now, you’ll start doing things
with this Deployment, which will make it clear why Deployments are superior. This will
become clear in the next few moments, when you see how updating the app through
a Deployment resource compares to updating it through a ReplicationController.
9.3.2
Updating a Deployment
Previously, when you ran your app using a ReplicationController, you had to explicitly
tell Kubernetes to perform the update by running kubectl rolling-update. You even
had to specify the name for the new ReplicationController that should replace the old
one. Kubernetes replaced all the original pods with new ones and deleted the original
ReplicationController at the end of the process. During the process, you basically had
to stay around, keeping your terminal open and waiting for kubectl to finish the roll-
ing update. 
 Now compare this to how you’re about to update a Deployment. The only thing
you need to do is modify the pod template defined in the Deployment resource and
Kubernetes will take all the steps necessary to get the actual system state to what’s
defined in the resource. Similar to scaling a ReplicationController or ReplicaSet up or
down, all you need to do is reference a new image tag in the Deployment’s pod tem-
plate and leave it to Kubernetes to transform your system so it matches the new
desired state.
UNDERSTANDING THE AVAILABLE DEPLOYMENT STRATEGIES
How this new state should be achieved is governed by the deployment strategy config-
ured on the Deployment itself. The default strategy is to perform a rolling update (the
strategy is called RollingUpdate). The alternative is the Recreate strategy, which
deletes all the old pods at once and then creates new ones, similar to modifying a
ReplicationController’s pod template and then deleting all the pods (we talked about
this in section 9.1.1).
 The Recreate strategy causes all old pods to be deleted before the new ones are
created. Use this strategy when your application doesn’t support running multiple ver-
sions in parallel and requires the old version to be stopped completely before the
new one is started. This strategy does involve a short period of time when your app
becomes completely unavailable.
 
</data>
  <data key="d5">264
CHAPTER 9
Deployments: updating applications declaratively
template. Using the hash value of the pod template like this allows the Deployment
to always use the same (possibly existing) ReplicaSet for a given version of the pod
template.
ACCESSING THE PODS THROUGH THE SERVICE
With the three replicas created by this ReplicaSet now running, you can use the Ser-
vice you created a while ago to access them, because you made the new pods’ labels
match the Service’s label selector. 
 Up until this point, you probably haven’t seen a good-enough reason why you should
use Deployments over ReplicationControllers. Luckily, creating a Deployment also hasn’t
been any harder than creating a ReplicationController. Now, you’ll start doing things
with this Deployment, which will make it clear why Deployments are superior. This will
become clear in the next few moments, when you see how updating the app through
a Deployment resource compares to updating it through a ReplicationController.
9.3.2
Updating a Deployment
Previously, when you ran your app using a ReplicationController, you had to explicitly
tell Kubernetes to perform the update by running kubectl rolling-update. You even
had to specify the name for the new ReplicationController that should replace the old
one. Kubernetes replaced all the original pods with new ones and deleted the original
ReplicationController at the end of the process. During the process, you basically had
to stay around, keeping your terminal open and waiting for kubectl to finish the roll-
ing update. 
 Now compare this to how you’re about to update a Deployment. The only thing
you need to do is modify the pod template defined in the Deployment resource and
Kubernetes will take all the steps necessary to get the actual system state to what’s
defined in the resource. Similar to scaling a ReplicationController or ReplicaSet up or
down, all you need to do is reference a new image tag in the Deployment’s pod tem-
plate and leave it to Kubernetes to transform your system so it matches the new
desired state.
UNDERSTANDING THE AVAILABLE DEPLOYMENT STRATEGIES
How this new state should be achieved is governed by the deployment strategy config-
ured on the Deployment itself. The default strategy is to perform a rolling update (the
strategy is called RollingUpdate). The alternative is the Recreate strategy, which
deletes all the old pods at once and then creates new ones, similar to modifying a
ReplicationController’s pod template and then deleting all the pods (we talked about
this in section 9.1.1).
 The Recreate strategy causes all old pods to be deleted before the new ones are
created. Use this strategy when your application doesn’t support running multiple ver-
sions in parallel and requires the old version to be stopped completely before the
new one is started. This strategy does involve a short period of time when your app
becomes completely unavailable.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="297">
  <data key="d0">Page_297</data>
  <data key="d5">Page_297</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_208">
  <data key="d0">265
Using Deployments for updating apps declaratively
 The RollingUpdate strategy, on the other hand, removes old pods one by one,
while adding new ones at the same time, keeping the application available throughout
the whole process, and ensuring there’s no drop in its capacity to handle requests.
This is the default strategy. The upper and lower limits for the number of pods above
or below the desired replica count are configurable. You should use this strategy only
when your app can handle running both the old and new version at the same time.
SLOWING DOWN THE ROLLING UPDATE FOR DEMO PURPOSES
In the next exercise, you’ll use the RollingUpdate strategy, but you need to slow down
the update process a little, so you can see that the update is indeed performed in a
rolling fashion. You can do that by setting the minReadySeconds attribute on the
Deployment. We’ll explain what this attribute does by the end of this chapter. For
now, set it to 10 seconds with the kubectl patch command.
$ kubectl patch deployment kubia -p '{"spec": {"minReadySeconds": 10}}'
"kubia" patched
TIP
The kubectl patch command is useful for modifying a single property
or a limited number of properties of a resource without having to edit its defi-
nition in a text editor.
You used the patch command to change the spec of the Deployment. This doesn’t
cause any kind of update to the pods, because you didn’t change the pod template.
Changing other Deployment properties, like the desired replica count or the deploy-
ment strategy, also doesn’t trigger a rollout, because it doesn’t affect the existing indi-
vidual pods in any way.
TRIGGERING THE ROLLING UPDATE
If you’d like to track the update process as it progresses, first run the curl loop again
in another terminal to see what’s happening with the requests (don’t forget to replace
the IP with the actual external IP of your service):
$ while true; do curl http://130.211.109.222; done
To trigger the actual rollout, you’ll change the image used in the single pod container
to luksa/kubia:v2. Instead of editing the whole YAML of the Deployment object or
using the patch command to change the image, you’ll use the kubectl set image
command, which allows changing the image of any resource that contains a container
(ReplicationControllers, ReplicaSets, Deployments, and so on). You’ll use it to modify
your Deployment like this:
$ kubectl set image deployment kubia nodejs=luksa/kubia:v2
deployment "kubia" image updated
When you execute this command, you’re updating the kubia Deployment’s pod tem-
plate so the image used in its nodejs container is changed to luksa/kubia:v2 (from
:v1). This is shown in figure 9.9.
 
</data>
  <data key="d5">265
Using Deployments for updating apps declaratively
 The RollingUpdate strategy, on the other hand, removes old pods one by one,
while adding new ones at the same time, keeping the application available throughout
the whole process, and ensuring there’s no drop in its capacity to handle requests.
This is the default strategy. The upper and lower limits for the number of pods above
or below the desired replica count are configurable. You should use this strategy only
when your app can handle running both the old and new version at the same time.
SLOWING DOWN THE ROLLING UPDATE FOR DEMO PURPOSES
In the next exercise, you’ll use the RollingUpdate strategy, but you need to slow down
the update process a little, so you can see that the update is indeed performed in a
rolling fashion. You can do that by setting the minReadySeconds attribute on the
Deployment. We’ll explain what this attribute does by the end of this chapter. For
now, set it to 10 seconds with the kubectl patch command.
$ kubectl patch deployment kubia -p '{"spec": {"minReadySeconds": 10}}'
"kubia" patched
TIP
The kubectl patch command is useful for modifying a single property
or a limited number of properties of a resource without having to edit its defi-
nition in a text editor.
You used the patch command to change the spec of the Deployment. This doesn’t
cause any kind of update to the pods, because you didn’t change the pod template.
Changing other Deployment properties, like the desired replica count or the deploy-
ment strategy, also doesn’t trigger a rollout, because it doesn’t affect the existing indi-
vidual pods in any way.
TRIGGERING THE ROLLING UPDATE
If you’d like to track the update process as it progresses, first run the curl loop again
in another terminal to see what’s happening with the requests (don’t forget to replace
the IP with the actual external IP of your service):
$ while true; do curl http://130.211.109.222; done
To trigger the actual rollout, you’ll change the image used in the single pod container
to luksa/kubia:v2. Instead of editing the whole YAML of the Deployment object or
using the patch command to change the image, you’ll use the kubectl set image
command, which allows changing the image of any resource that contains a container
(ReplicationControllers, ReplicaSets, Deployments, and so on). You’ll use it to modify
your Deployment like this:
$ kubectl set image deployment kubia nodejs=luksa/kubia:v2
deployment "kubia" image updated
When you execute this command, you’re updating the kubia Deployment’s pod tem-
plate so the image used in its nodejs container is changed to luksa/kubia:v2 (from
:v1). This is shown in figure 9.9.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="298">
  <data key="d0">Page_298</data>
  <data key="d5">Page_298</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_209">
  <data key="d0">266
CHAPTER 9
Deployments: updating applications declaratively
Ways of modifying Deployments and other resources
Over the course of this book, you’ve learned several ways how to modify an existing
object. Let’s list all of them together to refresh your memory.
All these methods are equivalent as far as Deployments go. What they do is change
the Deployment’s specification. This change then triggers the rollout process.
Image registry
Pod template
Deployment
kubectl set image…
luksa/kubia:v2
Container:
nodejs
:v1
:v2
Image registry
Pod template
Deployment
Container:
nodejs
:v1
:v2
Figure 9.9
Updating a Deployment’s pod template to point to a new image
Table 9.1
Modifying an existing resource in Kubernetes
Method
What it does
kubectl edit
Opens the object’s manifest in your default editor. After making 
changes, saving the file, and exiting the editor, the object is updated.
Example: kubectl edit deployment kubia
kubectl patch
Modifies individual properties of an object.
Example: kubectl patch deployment kubia -p '{"spec": 
{"template": {"spec": {"containers": [{"name": 
"nodejs", "image": "luksa/kubia:v2"}]}}}}'
kubectl apply
Modifies the object by applying property values from a full YAML or 
JSON file. If the object specified in the YAML/JSON doesn’t exist yet, 
it’s created. The file needs to contain the full definition of the 
resource (it can’t include only the fields you want to update, as is the 
case with kubectl patch).
Example: kubectl apply -f kubia-deployment-v2.yaml
kubectl replace
Replaces the object with a new one from a YAML/JSON file. In con-
trast to the apply command, this command requires the object to 
exist; otherwise it prints an error.
Example: kubectl replace -f kubia-deployment-v2.yaml
kubectl set image
Changes the container image defined in a Pod, ReplicationControl-
ler’s template, Deployment, DaemonSet, Job, or ReplicaSet.
Example: kubectl set image deployment kubia 
nodejs=luksa/kubia:v2
 
</data>
  <data key="d5">266
CHAPTER 9
Deployments: updating applications declaratively
Ways of modifying Deployments and other resources
Over the course of this book, you’ve learned several ways how to modify an existing
object. Let’s list all of them together to refresh your memory.
All these methods are equivalent as far as Deployments go. What they do is change
the Deployment’s specification. This change then triggers the rollout process.
Image registry
Pod template
Deployment
kubectl set image…
luksa/kubia:v2
Container:
nodejs
:v1
:v2
Image registry
Pod template
Deployment
Container:
nodejs
:v1
:v2
Figure 9.9
Updating a Deployment’s pod template to point to a new image
Table 9.1
Modifying an existing resource in Kubernetes
Method
What it does
kubectl edit
Opens the object’s manifest in your default editor. After making 
changes, saving the file, and exiting the editor, the object is updated.
Example: kubectl edit deployment kubia
kubectl patch
Modifies individual properties of an object.
Example: kubectl patch deployment kubia -p '{"spec": 
{"template": {"spec": {"containers": [{"name": 
"nodejs", "image": "luksa/kubia:v2"}]}}}}'
kubectl apply
Modifies the object by applying property values from a full YAML or 
JSON file. If the object specified in the YAML/JSON doesn’t exist yet, 
it’s created. The file needs to contain the full definition of the 
resource (it can’t include only the fields you want to update, as is the 
case with kubectl patch).
Example: kubectl apply -f kubia-deployment-v2.yaml
kubectl replace
Replaces the object with a new one from a YAML/JSON file. In con-
trast to the apply command, this command requires the object to 
exist; otherwise it prints an error.
Example: kubectl replace -f kubia-deployment-v2.yaml
kubectl set image
Changes the container image defined in a Pod, ReplicationControl-
ler’s template, Deployment, DaemonSet, Job, or ReplicaSet.
Example: kubectl set image deployment kubia 
nodejs=luksa/kubia:v2
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="299">
  <data key="d0">Page_299</data>
  <data key="d5">Page_299</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_210">
  <data key="d0">267
Using Deployments for updating apps declaratively
If you’ve run the curl loop, you’ll see requests initially hitting only the v1 pods; then
more and more of them hit the v2 pods until, finally, all of them hit only the remain-
ing v2 pods, after all v1 pods are deleted. This works much like the rolling update per-
formed by kubectl.
UNDERSTANDING THE AWESOMENESS OF DEPLOYMENTS
Let’s think about what has happened. By changing the pod template in your Deploy-
ment resource, you’ve updated your app to a newer version—by changing a single
field! 
 The controllers running as part of the Kubernetes control plane then performed
the update. The process wasn’t performed by the kubectl client, like it was when you
used kubectl rolling-update. I don’t know about you, but I think that’s simpler than
having to run a special command telling Kubernetes what to do and then waiting
around for the process to be completed.
NOTE
Be aware that if the pod template in the Deployment references a
ConfigMap (or a Secret), modifying the ConfigMap will not trigger an
update. One way to trigger an update when you need to modify an app’s con-
fig is to create a new ConfigMap and modify the pod template so it references
the new ConfigMap.
The events that occurred below the Deployment’s surface during the update are simi-
lar to what happened during the kubectl rolling-update. An additional ReplicaSet
was created and it was then scaled up slowly, while the previous ReplicaSet was scaled
down to zero (the initial and final states are shown in figure 9.10).
You can still see the old ReplicaSet next to the new one if you list them:
$ kubectl get rs
NAME               DESIRED   CURRENT   AGE
kubia-1506449474   0         0         24m
kubia-1581357123   3         3         23m
Pods: v1
ReplicaSet: v1
Replicas: --
Before
After
ReplicaSet: v2
Replicas: ++
Deployment
Pods: v2
ReplicaSet: v1
ReplicaSet: v2
Deployment
Figure 9.10
A Deployment at the start and end of a rolling update
 
</data>
  <data key="d5">267
Using Deployments for updating apps declaratively
If you’ve run the curl loop, you’ll see requests initially hitting only the v1 pods; then
more and more of them hit the v2 pods until, finally, all of them hit only the remain-
ing v2 pods, after all v1 pods are deleted. This works much like the rolling update per-
formed by kubectl.
UNDERSTANDING THE AWESOMENESS OF DEPLOYMENTS
Let’s think about what has happened. By changing the pod template in your Deploy-
ment resource, you’ve updated your app to a newer version—by changing a single
field! 
 The controllers running as part of the Kubernetes control plane then performed
the update. The process wasn’t performed by the kubectl client, like it was when you
used kubectl rolling-update. I don’t know about you, but I think that’s simpler than
having to run a special command telling Kubernetes what to do and then waiting
around for the process to be completed.
NOTE
Be aware that if the pod template in the Deployment references a
ConfigMap (or a Secret), modifying the ConfigMap will not trigger an
update. One way to trigger an update when you need to modify an app’s con-
fig is to create a new ConfigMap and modify the pod template so it references
the new ConfigMap.
The events that occurred below the Deployment’s surface during the update are simi-
lar to what happened during the kubectl rolling-update. An additional ReplicaSet
was created and it was then scaled up slowly, while the previous ReplicaSet was scaled
down to zero (the initial and final states are shown in figure 9.10).
You can still see the old ReplicaSet next to the new one if you list them:
$ kubectl get rs
NAME               DESIRED   CURRENT   AGE
kubia-1506449474   0         0         24m
kubia-1581357123   3         3         23m
Pods: v1
ReplicaSet: v1
Replicas: --
Before
After
ReplicaSet: v2
Replicas: ++
Deployment
Pods: v2
ReplicaSet: v1
ReplicaSet: v2
Deployment
Figure 9.10
A Deployment at the start and end of a rolling update
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="300">
  <data key="d0">Page_300</data>
  <data key="d5">Page_300</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_211">
  <data key="d0">268
CHAPTER 9
Deployments: updating applications declaratively
Similar to ReplicationControllers, all your new pods are now managed by the new
ReplicaSet. Unlike before, the old ReplicaSet is still there, whereas the old Replication-
Controller was deleted at the end of the rolling-update process. You’ll soon see what
the purpose of this inactive ReplicaSet is. 
 But you shouldn’t care about ReplicaSets here, because you didn’t create them
directly. You created and operated only on the Deployment resource; the underlying
ReplicaSets are an implementation detail. You’ll agree that managing a single Deploy-
ment object is much easier compared to dealing with and keeping track of multiple
ReplicationControllers. 
 Although this difference may not be so apparent when everything goes well with a
rollout, it becomes much more obvious when you hit a problem during the rollout
process. Let’s simulate one problem right now.
9.3.3
Rolling back a deployment
You’re currently running version v2 of your image, so you’ll need to prepare version 3
first. 
CREATING VERSION 3 OF YOUR APP
In version 3, you’ll introduce a bug that makes your app handle only the first four
requests properly. All requests from the fifth request onward will return an internal
server error (HTTP status code 500). You’ll simulate this by adding an if statement at
the beginning of the handler function. The following listing shows the new code, with
all required changes shown in bold.
const http = require('http');
const os = require('os');
var requestCount = 0;
console.log("Kubia server starting...");
var handler = function(request, response) {
  console.log("Received request from " + request.connection.remoteAddress);
  if (++requestCount &gt;= 5) {
    response.writeHead(500);
    response.end("Some internal error has occurred! This is pod " + 
os.hostname() + "\n");
    return;
  }
  response.writeHead(200);
  response.end("This is v3 running in pod " + os.hostname() + "\n");
};
var www = http.createServer(handler);
www.listen(8080); 
As you can see, on the fifth and all subsequent requests, the code returns a 500 error
with the message “Some internal error has occurred...”
Listing 9.8
Version 3 of our app (a broken version): v3/app.js
 
</data>
  <data key="d5">268
CHAPTER 9
Deployments: updating applications declaratively
Similar to ReplicationControllers, all your new pods are now managed by the new
ReplicaSet. Unlike before, the old ReplicaSet is still there, whereas the old Replication-
Controller was deleted at the end of the rolling-update process. You’ll soon see what
the purpose of this inactive ReplicaSet is. 
 But you shouldn’t care about ReplicaSets here, because you didn’t create them
directly. You created and operated only on the Deployment resource; the underlying
ReplicaSets are an implementation detail. You’ll agree that managing a single Deploy-
ment object is much easier compared to dealing with and keeping track of multiple
ReplicationControllers. 
 Although this difference may not be so apparent when everything goes well with a
rollout, it becomes much more obvious when you hit a problem during the rollout
process. Let’s simulate one problem right now.
9.3.3
Rolling back a deployment
You’re currently running version v2 of your image, so you’ll need to prepare version 3
first. 
CREATING VERSION 3 OF YOUR APP
In version 3, you’ll introduce a bug that makes your app handle only the first four
requests properly. All requests from the fifth request onward will return an internal
server error (HTTP status code 500). You’ll simulate this by adding an if statement at
the beginning of the handler function. The following listing shows the new code, with
all required changes shown in bold.
const http = require('http');
const os = require('os');
var requestCount = 0;
console.log("Kubia server starting...");
var handler = function(request, response) {
  console.log("Received request from " + request.connection.remoteAddress);
  if (++requestCount &gt;= 5) {
    response.writeHead(500);
    response.end("Some internal error has occurred! This is pod " + 
os.hostname() + "\n");
    return;
  }
  response.writeHead(200);
  response.end("This is v3 running in pod " + os.hostname() + "\n");
};
var www = http.createServer(handler);
www.listen(8080); 
As you can see, on the fifth and all subsequent requests, the code returns a 500 error
with the message “Some internal error has occurred...”
Listing 9.8
Version 3 of our app (a broken version): v3/app.js
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="301">
  <data key="d0">Page_301</data>
  <data key="d5">Page_301</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_212">
  <data key="d0">269
Using Deployments for updating apps declaratively
DEPLOYING VERSION 3
I’ve made the v3 version of the image available as luksa/kubia:v3. You’ll deploy this
new version by changing the image in the Deployment specification again: 
$ kubectl set image deployment kubia nodejs=luksa/kubia:v3
deployment "kubia" image updated
You can follow the progress of the rollout with kubectl rollout status:
$ kubectl rollout status deployment kubia
Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for rollout to finish: 1 old replicas are pending termination...
deployment "kubia" successfully rolled out
The new version is now live. As the following listing shows, after a few requests, your
web clients start receiving errors.
$ while true; do curl http://130.211.109.222; done
This is v3 running in pod kubia-1914148340-lalmx
This is v3 running in pod kubia-1914148340-bz35w
This is v3 running in pod kubia-1914148340-w0voh
...
This is v3 running in pod kubia-1914148340-w0voh
Some internal error has occurred! This is pod kubia-1914148340-bz35w
This is v3 running in pod kubia-1914148340-w0voh
Some internal error has occurred! This is pod kubia-1914148340-lalmx
This is v3 running in pod kubia-1914148340-w0voh
Some internal error has occurred! This is pod kubia-1914148340-lalmx
Some internal error has occurred! This is pod kubia-1914148340-bz35w
Some internal error has occurred! This is pod kubia-1914148340-w0voh
UNDOING A ROLLOUT
You can’t have your users experiencing internal server errors, so you need to do some-
thing about it fast. In section 9.3.6 you’ll see how to block bad rollouts automatically,
but for now, let’s see what you can do about your bad rollout manually. Luckily,
Deployments make it easy to roll back to the previously deployed version by telling
Kubernetes to undo the last rollout of a Deployment:
$ kubectl rollout undo deployment kubia
deployment "kubia" rolled back
This rolls the Deployment back to the previous revision. 
TIP
The undo command can also be used while the rollout process is still in
progress to essentially abort the rollout. Pods already created during the roll-
out process are removed and replaced with the old ones again.
Listing 9.9
Hitting your broken version 3
 
</data>
  <data key="d5">269
Using Deployments for updating apps declaratively
DEPLOYING VERSION 3
I’ve made the v3 version of the image available as luksa/kubia:v3. You’ll deploy this
new version by changing the image in the Deployment specification again: 
$ kubectl set image deployment kubia nodejs=luksa/kubia:v3
deployment "kubia" image updated
You can follow the progress of the rollout with kubectl rollout status:
$ kubectl rollout status deployment kubia
Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for rollout to finish: 1 old replicas are pending termination...
deployment "kubia" successfully rolled out
The new version is now live. As the following listing shows, after a few requests, your
web clients start receiving errors.
$ while true; do curl http://130.211.109.222; done
This is v3 running in pod kubia-1914148340-lalmx
This is v3 running in pod kubia-1914148340-bz35w
This is v3 running in pod kubia-1914148340-w0voh
...
This is v3 running in pod kubia-1914148340-w0voh
Some internal error has occurred! This is pod kubia-1914148340-bz35w
This is v3 running in pod kubia-1914148340-w0voh
Some internal error has occurred! This is pod kubia-1914148340-lalmx
This is v3 running in pod kubia-1914148340-w0voh
Some internal error has occurred! This is pod kubia-1914148340-lalmx
Some internal error has occurred! This is pod kubia-1914148340-bz35w
Some internal error has occurred! This is pod kubia-1914148340-w0voh
UNDOING A ROLLOUT
You can’t have your users experiencing internal server errors, so you need to do some-
thing about it fast. In section 9.3.6 you’ll see how to block bad rollouts automatically,
but for now, let’s see what you can do about your bad rollout manually. Luckily,
Deployments make it easy to roll back to the previously deployed version by telling
Kubernetes to undo the last rollout of a Deployment:
$ kubectl rollout undo deployment kubia
deployment "kubia" rolled back
This rolls the Deployment back to the previous revision. 
TIP
The undo command can also be used while the rollout process is still in
progress to essentially abort the rollout. Pods already created during the roll-
out process are removed and replaced with the old ones again.
Listing 9.9
Hitting your broken version 3
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="302">
  <data key="d0">Page_302</data>
  <data key="d5">Page_302</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_213">
  <data key="d0">270
CHAPTER 9
Deployments: updating applications declaratively
DISPLAYING A DEPLOYMENT’S ROLLOUT HISTORY
Rolling back a rollout is possible because Deployments keep a revision history. As
you’ll see later, the history is stored in the underlying ReplicaSets. When a rollout
completes, the old ReplicaSet isn’t deleted, and this enables rolling back to any revi-
sion, not only the previous one. The revision history can be displayed with the
kubectl rollout history command:
$ kubectl rollout history deployment kubia
deployments "kubia":
REVISION    CHANGE-CAUSE
2           kubectl set image deployment kubia nodejs=luksa/kubia:v2
3           kubectl set image deployment kubia nodejs=luksa/kubia:v3
Remember the --record command-line option you used when creating the Deploy-
ment? Without it, the CHANGE-CAUSE column in the revision history would be empty,
making it much harder to figure out what’s behind each revision.
ROLLING BACK TO A SPECIFIC DEPLOYMENT REVISION
You can roll back to a specific revision by specifying the revision in the undo com-
mand. For example, if you want to roll back to the first version, you’d execute the fol-
lowing command:
$ kubectl rollout undo deployment kubia --to-revision=1
Remember the inactive ReplicaSet left over when you modified the Deployment the
first time? The ReplicaSet represents the first revision of your Deployment. All Replica-
Sets created by a Deployment represent the complete revision history, as shown in fig-
ure 9.11. Each ReplicaSet stores the complete information of the Deployment at that
specific revision, so you shouldn’t delete it manually. If you do, you’ll lose that specific
revision from the Deployment’s history, preventing you from rolling back to it.
But having old ReplicaSets cluttering your ReplicaSet list is not ideal, so the length of
the revision history is limited by the revisionHistoryLimit property on the Deploy-
ment resource. It defaults to two, so normally only the current and the previous revision
are shown in the history (and only the current and the previous ReplicaSet are pre-
served). Older ReplicaSets are deleted automatically. 
Deployment
v1 ReplicaSet
ReplicaSet
Pods: v1
ReplicaSet
ReplicaSet
ReplicaSet
Revision 2
Revision 4
Revision 3
Revision 1
Revision history
Current revision
Figure 9.11
A Deployment’s ReplicaSets also act as its revision history.
 
</data>
  <data key="d5">270
CHAPTER 9
Deployments: updating applications declaratively
DISPLAYING A DEPLOYMENT’S ROLLOUT HISTORY
Rolling back a rollout is possible because Deployments keep a revision history. As
you’ll see later, the history is stored in the underlying ReplicaSets. When a rollout
completes, the old ReplicaSet isn’t deleted, and this enables rolling back to any revi-
sion, not only the previous one. The revision history can be displayed with the
kubectl rollout history command:
$ kubectl rollout history deployment kubia
deployments "kubia":
REVISION    CHANGE-CAUSE
2           kubectl set image deployment kubia nodejs=luksa/kubia:v2
3           kubectl set image deployment kubia nodejs=luksa/kubia:v3
Remember the --record command-line option you used when creating the Deploy-
ment? Without it, the CHANGE-CAUSE column in the revision history would be empty,
making it much harder to figure out what’s behind each revision.
ROLLING BACK TO A SPECIFIC DEPLOYMENT REVISION
You can roll back to a specific revision by specifying the revision in the undo com-
mand. For example, if you want to roll back to the first version, you’d execute the fol-
lowing command:
$ kubectl rollout undo deployment kubia --to-revision=1
Remember the inactive ReplicaSet left over when you modified the Deployment the
first time? The ReplicaSet represents the first revision of your Deployment. All Replica-
Sets created by a Deployment represent the complete revision history, as shown in fig-
ure 9.11. Each ReplicaSet stores the complete information of the Deployment at that
specific revision, so you shouldn’t delete it manually. If you do, you’ll lose that specific
revision from the Deployment’s history, preventing you from rolling back to it.
But having old ReplicaSets cluttering your ReplicaSet list is not ideal, so the length of
the revision history is limited by the revisionHistoryLimit property on the Deploy-
ment resource. It defaults to two, so normally only the current and the previous revision
are shown in the history (and only the current and the previous ReplicaSet are pre-
served). Older ReplicaSets are deleted automatically. 
Deployment
v1 ReplicaSet
ReplicaSet
Pods: v1
ReplicaSet
ReplicaSet
ReplicaSet
Revision 2
Revision 4
Revision 3
Revision 1
Revision history
Current revision
Figure 9.11
A Deployment’s ReplicaSets also act as its revision history.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="303">
  <data key="d0">Page_303</data>
  <data key="d5">Page_303</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_214">
  <data key="d0">271
Using Deployments for updating apps declaratively
NOTE
The extensions/v1beta1 version of Deployments doesn’t have a default
revisionHistoryLimit, whereas the default in version apps/v1beta2 is 10.
9.3.4
Controlling the rate of the rollout
When you performed the rollout to v3 and tracked its progress with the kubectl
rollout status command, you saw that first a new pod was created, and when it
became available, one of the old pods was deleted and another new pod was created.
This continued until there were no old pods left. The way new pods are created and
old ones are deleted is configurable through two additional properties of the rolling
update strategy. 
INTRODUCING THE MAXSURGE AND MAXUNAVAILABLE PROPERTIES OF THE ROLLING UPDATE STRATEGY
Two properties affect how many pods are replaced at once during a Deployment’s roll-
ing update. They are maxSurge and maxUnavailable and can be set as part of the
rollingUpdate sub-property of the Deployment’s strategy attribute, as shown in
the following listing.
spec:
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
What these properties do is explained in table 9.2.
Because the desired replica count in your case was three, and both these properties
default to 25%, maxSurge allowed the number of all pods to reach four, and
Listing 9.10
Specifying parameters for the rollingUpdate strategy
Table 9.2
Properties for configuring the rate of the rolling update
Property
What it does
maxSurge
Determines how many pod instances you allow to exist above the desired replica 
count configured on the Deployment. It defaults to 25%, so there can be at most 
25% more pod instances than the desired count. If the desired replica count is 
set to four, there will never be more than five pod instances running at the same 
time during an update. When converting a percentage to an absolute number, 
the number is rounded up. Instead of a percentage, the value can also be an 
absolute value (for example, one or two additional pods can be allowed).
maxUnavailable
Determines how many pod instances can be unavailable relative to the desired 
replica count during the update. It also defaults to 25%, so the number of avail-
able pod instances must never fall below 75% of the desired replica count. Here, 
when converting a percentage to an absolute number, the number is rounded 
down. If the desired replica count is set to four and the percentage is 25%, only 
one pod can be unavailable. There will always be at least three pod instances 
available to serve requests during the whole rollout. As with maxSurge, you can 
also specify an absolute value instead of a percentage.
 
</data>
  <data key="d5">271
Using Deployments for updating apps declaratively
NOTE
The extensions/v1beta1 version of Deployments doesn’t have a default
revisionHistoryLimit, whereas the default in version apps/v1beta2 is 10.
9.3.4
Controlling the rate of the rollout
When you performed the rollout to v3 and tracked its progress with the kubectl
rollout status command, you saw that first a new pod was created, and when it
became available, one of the old pods was deleted and another new pod was created.
This continued until there were no old pods left. The way new pods are created and
old ones are deleted is configurable through two additional properties of the rolling
update strategy. 
INTRODUCING THE MAXSURGE AND MAXUNAVAILABLE PROPERTIES OF THE ROLLING UPDATE STRATEGY
Two properties affect how many pods are replaced at once during a Deployment’s roll-
ing update. They are maxSurge and maxUnavailable and can be set as part of the
rollingUpdate sub-property of the Deployment’s strategy attribute, as shown in
the following listing.
spec:
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
What these properties do is explained in table 9.2.
Because the desired replica count in your case was three, and both these properties
default to 25%, maxSurge allowed the number of all pods to reach four, and
Listing 9.10
Specifying parameters for the rollingUpdate strategy
Table 9.2
Properties for configuring the rate of the rolling update
Property
What it does
maxSurge
Determines how many pod instances you allow to exist above the desired replica 
count configured on the Deployment. It defaults to 25%, so there can be at most 
25% more pod instances than the desired count. If the desired replica count is 
set to four, there will never be more than five pod instances running at the same 
time during an update. When converting a percentage to an absolute number, 
the number is rounded up. Instead of a percentage, the value can also be an 
absolute value (for example, one or two additional pods can be allowed).
maxUnavailable
Determines how many pod instances can be unavailable relative to the desired 
replica count during the update. It also defaults to 25%, so the number of avail-
able pod instances must never fall below 75% of the desired replica count. Here, 
when converting a percentage to an absolute number, the number is rounded 
down. If the desired replica count is set to four and the percentage is 25%, only 
one pod can be unavailable. There will always be at least three pod instances 
available to serve requests during the whole rollout. As with maxSurge, you can 
also specify an absolute value instead of a percentage.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="304">
  <data key="d0">Page_304</data>
  <data key="d5">Page_304</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_215">
  <data key="d0">272
CHAPTER 9
Deployments: updating applications declaratively
maxUnavailable disallowed having any unavailable pods (in other words, three pods
had to be available at all times). This is shown in figure 9.12.
UNDERSTANDING THE MAXUNAVAILABLE PROPERTY
The extensions/v1beta1 version of Deployments uses different defaults—it sets both
maxSurge and maxUnavailable to 1 instead of 25%. In the case of three replicas, max-
Surge is the same as before, but maxUnavailable is different (1 instead of 0). This
makes the rollout process unwind a bit differently, as shown in figure 9.13.
v1
Number
of pods
3
4
2
1
Time
v1
3 available
1 unavailable
Create
one
v2 pod
4 available
3 available
1 unavailable
4 available
3 available
1 unavailable
maxSurge = 1
maxUnavailable = 0
Desired replica count = 3
3 available
v2
v1
v1
v2
v2
v1
v1
v1
v1
v1
v1
v1
v1
v1
v1
v2
v2
v2
v2
v2
v2
v2
v2
v1
v2
v2
v2
v2
4 available
Wait
until
it’s
available
Delete
one v1
pod and
create one
v2 pod
Wait
until
it’s
available
Delete
one v1
pod and
create one
v2 pod
Wait
until
it’s
available
Delete
last
v1 pod
Figure 9.12
Rolling update of a Deployment with three replicas and default maxSurge and maxUnavailable 
v1
Number
of pods
3
4
2
1
Time
v1
2 available
2 unavailable
4 available
2 available
1 unavailable
3 available
maxSurge = 1
maxUnavailable = 1
Desired replica count = 3
v1
v1
v1
v1
v1
v2
v2
v2
v2
v2
v2
v2
v2
v2
v2
Wait until
both are
available
Delete
two v1
pods and
create one
v2 pod
Delete v1
pod and
create two
v2 pods
Wait
until it’s
available
Figure 9.13
Rolling update of a Deployment with the maxSurge=1 and maxUnavailable=1
 
</data>
  <data key="d5">272
CHAPTER 9
Deployments: updating applications declaratively
maxUnavailable disallowed having any unavailable pods (in other words, three pods
had to be available at all times). This is shown in figure 9.12.
UNDERSTANDING THE MAXUNAVAILABLE PROPERTY
The extensions/v1beta1 version of Deployments uses different defaults—it sets both
maxSurge and maxUnavailable to 1 instead of 25%. In the case of three replicas, max-
Surge is the same as before, but maxUnavailable is different (1 instead of 0). This
makes the rollout process unwind a bit differently, as shown in figure 9.13.
v1
Number
of pods
3
4
2
1
Time
v1
3 available
1 unavailable
Create
one
v2 pod
4 available
3 available
1 unavailable
4 available
3 available
1 unavailable
maxSurge = 1
maxUnavailable = 0
Desired replica count = 3
3 available
v2
v1
v1
v2
v2
v1
v1
v1
v1
v1
v1
v1
v1
v1
v1
v2
v2
v2
v2
v2
v2
v2
v2
v1
v2
v2
v2
v2
4 available
Wait
until
it’s
available
Delete
one v1
pod and
create one
v2 pod
Wait
until
it’s
available
Delete
one v1
pod and
create one
v2 pod
Wait
until
it’s
available
Delete
last
v1 pod
Figure 9.12
Rolling update of a Deployment with three replicas and default maxSurge and maxUnavailable 
v1
Number
of pods
3
4
2
1
Time
v1
2 available
2 unavailable
4 available
2 available
1 unavailable
3 available
maxSurge = 1
maxUnavailable = 1
Desired replica count = 3
v1
v1
v1
v1
v1
v2
v2
v2
v2
v2
v2
v2
v2
v2
v2
Wait until
both are
available
Delete
two v1
pods and
create one
v2 pod
Delete v1
pod and
create two
v2 pods
Wait
until it’s
available
Figure 9.13
Rolling update of a Deployment with the maxSurge=1 and maxUnavailable=1
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="305">
  <data key="d0">Page_305</data>
  <data key="d5">Page_305</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_216">
  <data key="d0">273
Using Deployments for updating apps declaratively
In this case, one replica can be unavailable, so if the desired replica count is three,
only two of them need to be available. That’s why the rollout process immediately
deletes one pod and creates two new ones. This ensures two pods are available and
that the maximum number of pods isn’t exceeded (the maximum is four in this
case—three plus one from maxSurge). As soon as the two new pods are available, the
two remaining old pods are deleted.
 This is a bit hard to grasp, especially since the maxUnavailable property leads you
to believe that that’s the maximum number of unavailable pods that are allowed. If
you look at the previous figure closely, you’ll see two unavailable pods in the second
column even though maxUnavailable is set to 1. 
 It’s important to keep in mind that maxUnavailable is relative to the desired
replica count. If the replica count is set to three and maxUnavailable is set to one,
that means that the update process must always keep at least two (3 minus 1) pods
available, while the number of pods that aren’t available can exceed one.
9.3.5
Pausing the rollout process
After the bad experience with version 3 of your app, imagine you’ve now fixed the bug
and pushed version 4 of your image. You’re a little apprehensive about rolling it out
across all your pods the way you did before. What you want is to run a single v4 pod
next to your existing v2 pods and see how it behaves with only a fraction of all your
users. Then, once you’re sure everything’s okay, you can replace all the old pods with
new ones. 
 You could achieve this by running an additional pod either directly or through an
additional Deployment, ReplicationController, or ReplicaSet, but you do have another
option available on the Deployment itself. A Deployment can also be paused during
the rollout process. This allows you to verify that everything is fine with the new ver-
sion before proceeding with the rest of the rollout.
PAUSING THE ROLLOUT
I’ve prepared the v4 image, so go ahead and trigger the rollout by changing the image
to luksa/kubia:v4, but then immediately (within a few seconds) pause the rollout:
$ kubectl set image deployment kubia nodejs=luksa/kubia:v4
deployment "kubia" image updated
$ kubectl rollout pause deployment kubia
deployment "kubia" paused
A single new pod should have been created, but all original pods should also still be
running. Once the new pod is up, a part of all requests to the service will be redirected
to the new pod. This way, you’ve effectively run a canary release. A canary release is a
technique for minimizing the risk of rolling out a bad version of an application and it
affecting all your users. Instead of rolling out the new version to everyone, you replace
only one or a small number of old pods with new ones. This way only a small number
of users will initially hit the new version. You can then verify whether the new version
 
</data>
  <data key="d5">273
Using Deployments for updating apps declaratively
In this case, one replica can be unavailable, so if the desired replica count is three,
only two of them need to be available. That’s why the rollout process immediately
deletes one pod and creates two new ones. This ensures two pods are available and
that the maximum number of pods isn’t exceeded (the maximum is four in this
case—three plus one from maxSurge). As soon as the two new pods are available, the
two remaining old pods are deleted.
 This is a bit hard to grasp, especially since the maxUnavailable property leads you
to believe that that’s the maximum number of unavailable pods that are allowed. If
you look at the previous figure closely, you’ll see two unavailable pods in the second
column even though maxUnavailable is set to 1. 
 It’s important to keep in mind that maxUnavailable is relative to the desired
replica count. If the replica count is set to three and maxUnavailable is set to one,
that means that the update process must always keep at least two (3 minus 1) pods
available, while the number of pods that aren’t available can exceed one.
9.3.5
Pausing the rollout process
After the bad experience with version 3 of your app, imagine you’ve now fixed the bug
and pushed version 4 of your image. You’re a little apprehensive about rolling it out
across all your pods the way you did before. What you want is to run a single v4 pod
next to your existing v2 pods and see how it behaves with only a fraction of all your
users. Then, once you’re sure everything’s okay, you can replace all the old pods with
new ones. 
 You could achieve this by running an additional pod either directly or through an
additional Deployment, ReplicationController, or ReplicaSet, but you do have another
option available on the Deployment itself. A Deployment can also be paused during
the rollout process. This allows you to verify that everything is fine with the new ver-
sion before proceeding with the rest of the rollout.
PAUSING THE ROLLOUT
I’ve prepared the v4 image, so go ahead and trigger the rollout by changing the image
to luksa/kubia:v4, but then immediately (within a few seconds) pause the rollout:
$ kubectl set image deployment kubia nodejs=luksa/kubia:v4
deployment "kubia" image updated
$ kubectl rollout pause deployment kubia
deployment "kubia" paused
A single new pod should have been created, but all original pods should also still be
running. Once the new pod is up, a part of all requests to the service will be redirected
to the new pod. This way, you’ve effectively run a canary release. A canary release is a
technique for minimizing the risk of rolling out a bad version of an application and it
affecting all your users. Instead of rolling out the new version to everyone, you replace
only one or a small number of old pods with new ones. This way only a small number
of users will initially hit the new version. You can then verify whether the new version
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="306">
  <data key="d0">Page_306</data>
  <data key="d5">Page_306</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_217">
  <data key="d0">274
CHAPTER 9
Deployments: updating applications declaratively
is working fine or not and then either continue the rollout across all remaining pods
or roll back to the previous version. 
RESUMING THE ROLLOUT
In your case, by pausing the rollout process, only a small portion of client requests will
hit your v4 pod, while most will still hit the v3 pods. Once you’re confident the new
version works as it should, you can resume the deployment to replace all the old pods
with new ones:
$ kubectl rollout resume deployment kubia
deployment "kubia" resumed
Obviously, having to pause the deployment at an exact point in the rollout process
isn’t what you want to do. In the future, a new upgrade strategy may do that automati-
cally, but currently, the proper way of performing a canary release is by using two dif-
ferent Deployments and scaling them appropriately. 
USING THE PAUSE FEATURE TO PREVENT ROLLOUTS
Pausing a Deployment can also be used to prevent updates to the Deployment from
kicking off the rollout process, allowing you to make multiple changes to the Deploy-
ment and starting the rollout only when you’re done making all the necessary changes.
Once you’re ready for changes to take effect, you resume the Deployment and the
rollout process will start.
NOTE
If a Deployment is paused, the undo command won’t undo it until you
resume the Deployment.
9.3.6
Blocking rollouts of bad versions
Before you conclude this chapter, we need to discuss one more property of the Deploy-
ment resource. Remember the minReadySeconds property you set on the Deployment
at the beginning of section 9.3.2? You used it to slow down the rollout, so you could see
it was indeed performing a rolling update and not replacing all the pods at once. The
main function of minReadySeconds is to prevent deploying malfunctioning versions, not
slowing down a deployment for fun. 
UNDERSTANDING THE APPLICABILITY OF MINREADYSECONDS
The minReadySeconds property specifies how long a newly created pod should be
ready before the pod is treated as available. Until the pod is available, the rollout pro-
cess will not continue (remember the maxUnavailable property?). A pod is ready
when readiness probes of all its containers return a success. If a new pod isn’t func-
tioning properly and its readiness probe starts failing before minReadySeconds have
passed, the rollout of the new version will effectively be blocked.
 You used this property to slow down your rollout process by having Kubernetes
wait 10 seconds after a pod was ready before continuing with the rollout. Usually,
you’d set minReadySeconds to something much higher to make sure pods keep report-
ing they’re ready after they’ve already started receiving actual traffic. 
 
</data>
  <data key="d5">274
CHAPTER 9
Deployments: updating applications declaratively
is working fine or not and then either continue the rollout across all remaining pods
or roll back to the previous version. 
RESUMING THE ROLLOUT
In your case, by pausing the rollout process, only a small portion of client requests will
hit your v4 pod, while most will still hit the v3 pods. Once you’re confident the new
version works as it should, you can resume the deployment to replace all the old pods
with new ones:
$ kubectl rollout resume deployment kubia
deployment "kubia" resumed
Obviously, having to pause the deployment at an exact point in the rollout process
isn’t what you want to do. In the future, a new upgrade strategy may do that automati-
cally, but currently, the proper way of performing a canary release is by using two dif-
ferent Deployments and scaling them appropriately. 
USING THE PAUSE FEATURE TO PREVENT ROLLOUTS
Pausing a Deployment can also be used to prevent updates to the Deployment from
kicking off the rollout process, allowing you to make multiple changes to the Deploy-
ment and starting the rollout only when you’re done making all the necessary changes.
Once you’re ready for changes to take effect, you resume the Deployment and the
rollout process will start.
NOTE
If a Deployment is paused, the undo command won’t undo it until you
resume the Deployment.
9.3.6
Blocking rollouts of bad versions
Before you conclude this chapter, we need to discuss one more property of the Deploy-
ment resource. Remember the minReadySeconds property you set on the Deployment
at the beginning of section 9.3.2? You used it to slow down the rollout, so you could see
it was indeed performing a rolling update and not replacing all the pods at once. The
main function of minReadySeconds is to prevent deploying malfunctioning versions, not
slowing down a deployment for fun. 
UNDERSTANDING THE APPLICABILITY OF MINREADYSECONDS
The minReadySeconds property specifies how long a newly created pod should be
ready before the pod is treated as available. Until the pod is available, the rollout pro-
cess will not continue (remember the maxUnavailable property?). A pod is ready
when readiness probes of all its containers return a success. If a new pod isn’t func-
tioning properly and its readiness probe starts failing before minReadySeconds have
passed, the rollout of the new version will effectively be blocked.
 You used this property to slow down your rollout process by having Kubernetes
wait 10 seconds after a pod was ready before continuing with the rollout. Usually,
you’d set minReadySeconds to something much higher to make sure pods keep report-
ing they’re ready after they’ve already started receiving actual traffic. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="307">
  <data key="d0">Page_307</data>
  <data key="d5">Page_307</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_218">
  <data key="d0">275
Using Deployments for updating apps declaratively
 Although you should obviously test your pods both in a test and in a staging envi-
ronment before deploying them into production, using minReadySeconds is like an
airbag that saves your app from making a big mess after you’ve already let a buggy ver-
sion slip into production. 
 With a properly configured readiness probe and a proper minReadySeconds set-
ting, Kubernetes would have prevented us from deploying the buggy v3 version ear-
lier. Let me show you how.
DEFINING A READINESS PROBE TO PREVENT OUR V3 VERSION FROM BEING ROLLED OUT FULLY
You’re going to deploy version v3 again, but this time, you’ll have the proper readi-
ness probe defined on the pod. Your Deployment is currently at version v4, so before
you start, roll back to version v2 again so you can pretend this is the first time you’re
upgrading to v3. If you wish, you can go straight from v4 to v3, but the text that fol-
lows assumes you returned to v2 first.
 Unlike before, where you only updated the image in the pod template, you’re now
also going to introduce a readiness probe for the container at the same time. Up until
now, because there was no explicit readiness probe defined, the container and the
pod were always considered ready, even if the app wasn’t truly ready or was returning
errors. There was no way for Kubernetes to know that the app was malfunctioning and
shouldn’t be exposed to clients. 
 To change the image and introduce the readiness probe at once, you’ll use the
kubectl apply command. You’ll use the following YAML to update the deployment
(you’ll store it as kubia-deployment-v3-with-readinesscheck.yaml), as shown in
the following listing.
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  minReadySeconds: 10           
  strategy:
    rollingUpdate:
      maxSurge: 1                  
      maxUnavailable: 0         
    type: RollingUpdate
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v3
Listing 9.11
Deployment with a readiness probe: kubia-deployment-v3-with-
readinesscheck.yaml
You’re keeping 
minReadySeconds 
set to 10.
You’re keeping maxUnavailable 
set to 0 to make the deployment 
replace pods one by one
 
</data>
  <data key="d5">275
Using Deployments for updating apps declaratively
 Although you should obviously test your pods both in a test and in a staging envi-
ronment before deploying them into production, using minReadySeconds is like an
airbag that saves your app from making a big mess after you’ve already let a buggy ver-
sion slip into production. 
 With a properly configured readiness probe and a proper minReadySeconds set-
ting, Kubernetes would have prevented us from deploying the buggy v3 version ear-
lier. Let me show you how.
DEFINING A READINESS PROBE TO PREVENT OUR V3 VERSION FROM BEING ROLLED OUT FULLY
You’re going to deploy version v3 again, but this time, you’ll have the proper readi-
ness probe defined on the pod. Your Deployment is currently at version v4, so before
you start, roll back to version v2 again so you can pretend this is the first time you’re
upgrading to v3. If you wish, you can go straight from v4 to v3, but the text that fol-
lows assumes you returned to v2 first.
 Unlike before, where you only updated the image in the pod template, you’re now
also going to introduce a readiness probe for the container at the same time. Up until
now, because there was no explicit readiness probe defined, the container and the
pod were always considered ready, even if the app wasn’t truly ready or was returning
errors. There was no way for Kubernetes to know that the app was malfunctioning and
shouldn’t be exposed to clients. 
 To change the image and introduce the readiness probe at once, you’ll use the
kubectl apply command. You’ll use the following YAML to update the deployment
(you’ll store it as kubia-deployment-v3-with-readinesscheck.yaml), as shown in
the following listing.
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  minReadySeconds: 10           
  strategy:
    rollingUpdate:
      maxSurge: 1                  
      maxUnavailable: 0         
    type: RollingUpdate
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v3
Listing 9.11
Deployment with a readiness probe: kubia-deployment-v3-with-
readinesscheck.yaml
You’re keeping 
minReadySeconds 
set to 10.
You’re keeping maxUnavailable 
set to 0 to make the deployment 
replace pods one by one
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="308">
  <data key="d0">Page_308</data>
  <data key="d5">Page_308</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_219">
  <data key="d0">276
CHAPTER 9
Deployments: updating applications declaratively
        name: nodejs
        readinessProbe:
          periodSeconds: 1       
          httpGet:                  
            path: /                 
            port: 8080              
UPDATING A DEPLOYMENT WITH KUBECTL APPLY
To update the Deployment this time, you’ll use kubectl apply like this:
$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml 
deployment "kubia" configured
The apply command updates the Deployment with everything that’s defined in the
YAML file. It not only updates the image but also adds the readiness probe definition
and anything else you’ve added or modified in the YAML. If the new YAML also con-
tains the replicas field, which doesn’t match the number of replicas on the existing
Deployment, the apply operation will also scale the Deployment, which isn’t usually
what you want. 
TIP
To keep the desired replica count unchanged when updating a Deploy-
ment with kubectl apply, don’t include the replicas field in the YAML. 
Running the apply command will kick off the update process, which you can again
follow with the rollout status command:
$ kubectl rollout status deployment kubia
Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
Because the status says one new pod has been created, your service should be hitting it
occasionally, right? Let’s see:
$ while true; do curl http://130.211.109.222; done
This is v2 running in pod kubia-1765119474-jvslk
This is v2 running in pod kubia-1765119474-jvslk
This is v2 running in pod kubia-1765119474-xk5g3
This is v2 running in pod kubia-1765119474-pmb26
This is v2 running in pod kubia-1765119474-pmb26
This is v2 running in pod kubia-1765119474-xk5g3
...
Nope, you never hit the v3 pod. Why not? Is it even there? List the pods:
$ kubectl get po
NAME                     READY     STATUS    RESTARTS   AGE
kubia-1163142519-7ws0i   0/1       Running   0          30s
kubia-1765119474-jvslk   1/1       Running   0          9m
kubia-1765119474-pmb26   1/1       Running   0          9m
kubia-1765119474-xk5g3   1/1       Running   0          8m
You’re defining a readiness probe 
that will be executed every second.
The readiness probe will 
perform an HTTP GET request 
against our container.
 
</data>
  <data key="d5">276
CHAPTER 9
Deployments: updating applications declaratively
        name: nodejs
        readinessProbe:
          periodSeconds: 1       
          httpGet:                  
            path: /                 
            port: 8080              
UPDATING A DEPLOYMENT WITH KUBECTL APPLY
To update the Deployment this time, you’ll use kubectl apply like this:
$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml 
deployment "kubia" configured
The apply command updates the Deployment with everything that’s defined in the
YAML file. It not only updates the image but also adds the readiness probe definition
and anything else you’ve added or modified in the YAML. If the new YAML also con-
tains the replicas field, which doesn’t match the number of replicas on the existing
Deployment, the apply operation will also scale the Deployment, which isn’t usually
what you want. 
TIP
To keep the desired replica count unchanged when updating a Deploy-
ment with kubectl apply, don’t include the replicas field in the YAML. 
Running the apply command will kick off the update process, which you can again
follow with the rollout status command:
$ kubectl rollout status deployment kubia
Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
Because the status says one new pod has been created, your service should be hitting it
occasionally, right? Let’s see:
$ while true; do curl http://130.211.109.222; done
This is v2 running in pod kubia-1765119474-jvslk
This is v2 running in pod kubia-1765119474-jvslk
This is v2 running in pod kubia-1765119474-xk5g3
This is v2 running in pod kubia-1765119474-pmb26
This is v2 running in pod kubia-1765119474-pmb26
This is v2 running in pod kubia-1765119474-xk5g3
...
Nope, you never hit the v3 pod. Why not? Is it even there? List the pods:
$ kubectl get po
NAME                     READY     STATUS    RESTARTS   AGE
kubia-1163142519-7ws0i   0/1       Running   0          30s
kubia-1765119474-jvslk   1/1       Running   0          9m
kubia-1765119474-pmb26   1/1       Running   0          9m
kubia-1765119474-xk5g3   1/1       Running   0          8m
You’re defining a readiness probe 
that will be executed every second.
The readiness probe will 
perform an HTTP GET request 
against our container.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="309">
  <data key="d0">Page_309</data>
  <data key="d5">Page_309</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_220">
  <data key="d0">277
Using Deployments for updating apps declaratively
Aha! There’s your problem (or as you’ll learn soon, your blessing)! The pod is shown
as not ready, but I guess you’ve been expecting that, right? What has happened?
UNDERSTANDING HOW A READINESS PROBE PREVENTS BAD VERSIONS FROM BEING ROLLED OUT
As soon as your new pod starts, the readiness probe starts being hit every second (you
set the probe’s interval to one second in the pod spec). On the fifth request the readi-
ness probe began failing, because your app starts returning HTTP status code 500
from the fifth request onward. 
 As a result, the pod is removed as an endpoint from the service (see figure 9.14).
By the time you start hitting the service in the curl loop, the pod has already been
marked as not ready. This explains why you never hit the new pod with curl. And
that’s exactly what you want, because you don’t want clients to hit a pod that’s not
functioning properly.
But what about the rollout process? The rollout status command shows only one
new replica has started. Thankfully, the rollout process will not continue, because the
new pod will never become available. To be considered available, it needs to be ready
for at least 10 seconds. Until it’s available, the rollout process will not create any new
pods, and it also won’t remove any original pods because you’ve set the maxUnavailable
property to 0. 
Service
curl
Pod: v2
Pod: v2
Pod: v3
(unhealthy)
Pod: v2
ReplicaSet: v2
Replicas: 3
Deployment
Replicas: 3
rollingUpdate:
maxSurge: 1
maxUnavailable: 0
ReplicaSet: v3
Replicas: 1
Requests are not forwarded
to v3 pod because of failed
readiness probe
Figure 9.14
Deployment blocked by a failing readiness probe in the new pod
 
</data>
  <data key="d5">277
Using Deployments for updating apps declaratively
Aha! There’s your problem (or as you’ll learn soon, your blessing)! The pod is shown
as not ready, but I guess you’ve been expecting that, right? What has happened?
UNDERSTANDING HOW A READINESS PROBE PREVENTS BAD VERSIONS FROM BEING ROLLED OUT
As soon as your new pod starts, the readiness probe starts being hit every second (you
set the probe’s interval to one second in the pod spec). On the fifth request the readi-
ness probe began failing, because your app starts returning HTTP status code 500
from the fifth request onward. 
 As a result, the pod is removed as an endpoint from the service (see figure 9.14).
By the time you start hitting the service in the curl loop, the pod has already been
marked as not ready. This explains why you never hit the new pod with curl. And
that’s exactly what you want, because you don’t want clients to hit a pod that’s not
functioning properly.
But what about the rollout process? The rollout status command shows only one
new replica has started. Thankfully, the rollout process will not continue, because the
new pod will never become available. To be considered available, it needs to be ready
for at least 10 seconds. Until it’s available, the rollout process will not create any new
pods, and it also won’t remove any original pods because you’ve set the maxUnavailable
property to 0. 
Service
curl
Pod: v2
Pod: v2
Pod: v3
(unhealthy)
Pod: v2
ReplicaSet: v2
Replicas: 3
Deployment
Replicas: 3
rollingUpdate:
maxSurge: 1
maxUnavailable: 0
ReplicaSet: v3
Replicas: 1
Requests are not forwarded
to v3 pod because of failed
readiness probe
Figure 9.14
Deployment blocked by a failing readiness probe in the new pod
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="310">
  <data key="d0">Page_310</data>
  <data key="d5">Page_310</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_221">
  <data key="d0">278
CHAPTER 9
Deployments: updating applications declaratively
 The fact that the deployment is stuck is a good thing, because if it had continued
replacing the old pods with the new ones, you’d end up with a completely non-working
service, like you did when you first rolled out version 3, when you weren’t using the
readiness probe. But now, with the readiness probe in place, there was virtually no
negative impact on your users. A few users may have experienced the internal server
error, but that’s not as big of a problem as if the rollout had replaced all pods with the
faulty version 3.
TIP
If you only define the readiness probe without setting minReadySeconds
properly, new pods are considered available immediately when the first invo-
cation of the readiness probe succeeds. If the readiness probe starts failing
shortly after, the bad version is rolled out across all pods. Therefore, you
should set minReadySeconds appropriately.
CONFIGURING A DEADLINE FOR THE ROLLOUT
By default, after the rollout can’t make any progress in 10 minutes, it’s considered as
failed. If you use the kubectl describe deployment command, you’ll see it display a
ProgressDeadlineExceeded condition, as shown in the following listing.
$ kubectl describe deploy kubia
Name:                   kubia
...
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   False   ProgressDeadlineExceeded   
The time after which the Deployment is considered failed is configurable through the
progressDeadlineSeconds property in the Deployment spec.
NOTE
The extensions/v1beta1 version of Deployments doesn’t set a deadline.
ABORTING A BAD ROLLOUT
Because the rollout will never continue, the only thing to do now is abort the rollout
by undoing it:
$ kubectl rollout undo deployment kubia
deployment "kubia" rolled back
NOTE
In future versions, the rollout will be aborted automatically when the
time specified in progressDeadlineSeconds is exceeded.
Listing 9.12
Seeing the conditions of a Deployment with kubectl describe
The Deployment 
took too long to 
make progress.
 
</data>
  <data key="d5">278
CHAPTER 9
Deployments: updating applications declaratively
 The fact that the deployment is stuck is a good thing, because if it had continued
replacing the old pods with the new ones, you’d end up with a completely non-working
service, like you did when you first rolled out version 3, when you weren’t using the
readiness probe. But now, with the readiness probe in place, there was virtually no
negative impact on your users. A few users may have experienced the internal server
error, but that’s not as big of a problem as if the rollout had replaced all pods with the
faulty version 3.
TIP
If you only define the readiness probe without setting minReadySeconds
properly, new pods are considered available immediately when the first invo-
cation of the readiness probe succeeds. If the readiness probe starts failing
shortly after, the bad version is rolled out across all pods. Therefore, you
should set minReadySeconds appropriately.
CONFIGURING A DEADLINE FOR THE ROLLOUT
By default, after the rollout can’t make any progress in 10 minutes, it’s considered as
failed. If you use the kubectl describe deployment command, you’ll see it display a
ProgressDeadlineExceeded condition, as shown in the following listing.
$ kubectl describe deploy kubia
Name:                   kubia
...
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   False   ProgressDeadlineExceeded   
The time after which the Deployment is considered failed is configurable through the
progressDeadlineSeconds property in the Deployment spec.
NOTE
The extensions/v1beta1 version of Deployments doesn’t set a deadline.
ABORTING A BAD ROLLOUT
Because the rollout will never continue, the only thing to do now is abort the rollout
by undoing it:
$ kubectl rollout undo deployment kubia
deployment "kubia" rolled back
NOTE
In future versions, the rollout will be aborted automatically when the
time specified in progressDeadlineSeconds is exceeded.
Listing 9.12
Seeing the conditions of a Deployment with kubectl describe
The Deployment 
took too long to 
make progress.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="311">
  <data key="d0">Page_311</data>
  <data key="d5">Page_311</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_222">
  <data key="d0">279
Summary
9.4
Summary
This chapter has shown you how to make your life easier by using a declarative
approach to deploying and updating applications in Kubernetes. Now that you’ve
read this chapter, you should know how to
Perform a rolling update of pods managed by a ReplicationController
Create Deployments instead of lower-level ReplicationControllers or ReplicaSets
Update your pods by editing the pod template in the Deployment specification
Roll back a Deployment either to the previous revision or to any earlier revision
still listed in the revision history
Abort a Deployment mid-way
Pause a Deployment to inspect how a single instance of the new version behaves
in production before allowing additional pod instances to replace the old ones
Control the rate of the rolling update through maxSurge and maxUnavailable
properties
Use minReadySeconds and readiness probes to have the rollout of a faulty ver-
sion blocked automatically
In addition to these Deployment-specific tasks, you also learned how to
Use three dashes as a separator to define multiple resources in a single YAML file
Turn on kubectl’s verbose logging to see exactly what it’s doing behind the
curtains
You now know how to deploy and manage sets of pods created from the same pod
template and thus share the same persistent storage. You even know how to update
them declaratively. But what about running sets of pods, where each instance needs to
use its own persistent storage? We haven’t looked at that yet. That’s the subject of our
next chapter.
 
</data>
  <data key="d5">279
Summary
9.4
Summary
This chapter has shown you how to make your life easier by using a declarative
approach to deploying and updating applications in Kubernetes. Now that you’ve
read this chapter, you should know how to
Perform a rolling update of pods managed by a ReplicationController
Create Deployments instead of lower-level ReplicationControllers or ReplicaSets
Update your pods by editing the pod template in the Deployment specification
Roll back a Deployment either to the previous revision or to any earlier revision
still listed in the revision history
Abort a Deployment mid-way
Pause a Deployment to inspect how a single instance of the new version behaves
in production before allowing additional pod instances to replace the old ones
Control the rate of the rolling update through maxSurge and maxUnavailable
properties
Use minReadySeconds and readiness probes to have the rollout of a faulty ver-
sion blocked automatically
In addition to these Deployment-specific tasks, you also learned how to
Use three dashes as a separator to define multiple resources in a single YAML file
Turn on kubectl’s verbose logging to see exactly what it’s doing behind the
curtains
You now know how to deploy and manage sets of pods created from the same pod
template and thus share the same persistent storage. You even know how to update
them declaratively. But what about running sets of pods, where each instance needs to
use its own persistent storage? We haven’t looked at that yet. That’s the subject of our
next chapter.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="312">
  <data key="d0">Page_312</data>
  <data key="d5">Page_312</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_223">
  <data key="d0">280
StatefulSets:
deploying replicated
stateful applications
You now know how to run both single-instance and replicated stateless pods,
and even stateful pods utilizing persistent storage. You can run several repli-
cated web-server pod instances and you can run a single database pod instance
that uses persistent storage, provided either through plain pod volumes or through
PersistentVolumes bound by a PersistentVolumeClaim. But can you employ a
ReplicaSet to replicate the database pod?
This chapter covers
Deploying stateful clustered applications
Providing separate storage for each instance of 
a replicated pod
Guaranteeing a stable name and hostname for 
pod replicas
Starting and stopping pod replicas in a 
predictable order
Discovering peers through DNS SRV records
 
</data>
  <data key="d5">280
StatefulSets:
deploying replicated
stateful applications
You now know how to run both single-instance and replicated stateless pods,
and even stateful pods utilizing persistent storage. You can run several repli-
cated web-server pod instances and you can run a single database pod instance
that uses persistent storage, provided either through plain pod volumes or through
PersistentVolumes bound by a PersistentVolumeClaim. But can you employ a
ReplicaSet to replicate the database pod?
This chapter covers
Deploying stateful clustered applications
Providing separate storage for each instance of 
a replicated pod
Guaranteeing a stable name and hostname for 
pod replicas
Starting and stopping pod replicas in a 
predictable order
Discovering peers through DNS SRV records
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="313">
  <data key="d0">Page_313</data>
  <data key="d5">Page_313</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_224">
  <data key="d0">281
Replicating stateful pods
10.1
Replicating stateful pods
ReplicaSets create multiple pod replicas from a single pod template. These replicas
don’t differ from each other, apart from their name and IP address. If the pod tem-
plate includes a volume, which refers to a specific PersistentVolumeClaim, all replicas
of the ReplicaSet will use the exact same PersistentVolumeClaim and therefore the
same PersistentVolume bound by the claim (shown in figure 10.1).
Because the reference to the claim is in the pod template, which is used to stamp out
multiple pod replicas, you can’t make each replica use its own separate Persistent-
VolumeClaim. You can’t use a ReplicaSet to run a distributed data store, where each
instance needs its own separate storage—at least not by using a single ReplicaSet. To
be honest, none of the API objects you’ve seen so far make running such a data store
possible. You need something else. 
10.1.1 Running multiple replicas with separate storage for each
How does one run multiple replicas of a pod and have each pod use its own storage
volume? ReplicaSets create exact copies (replicas) of a pod; therefore you can’t use
them for these types of pods. What can you use?
CREATING PODS MANUALLY
You could create pods manually and have each of them use its own PersistentVolume-
Claim, but because no ReplicaSet looks after them, you’d need to manage them man-
ually and recreate them when they disappear (as in the event of a node failure).
Therefore, this isn’t a viable option.
USING ONE REPLICASET PER POD INSTANCE
Instead of creating pods directly, you could create multiple ReplicaSets—one for each
pod with each ReplicaSet’s desired replica count set to one, and each ReplicaSet’s pod
template referencing a dedicated PersistentVolumeClaim (as shown in figure 10.2).
 Although this takes care of the automatic rescheduling in case of node failures or
accidental pod deletions, it’s much more cumbersome compared to having a single
ReplicaSet. For example, think about how you’d scale the pods in that case. You
Persistent
Volume
Claim
Persistent
Volume
ReplicaSet
Pod
Pod
Pod
Figure 10.1
All pods from the same ReplicaSet always use the same 
PersistentVolumeClaim and PersistentVolume.
 
</data>
  <data key="d5">281
Replicating stateful pods
10.1
Replicating stateful pods
ReplicaSets create multiple pod replicas from a single pod template. These replicas
don’t differ from each other, apart from their name and IP address. If the pod tem-
plate includes a volume, which refers to a specific PersistentVolumeClaim, all replicas
of the ReplicaSet will use the exact same PersistentVolumeClaim and therefore the
same PersistentVolume bound by the claim (shown in figure 10.1).
Because the reference to the claim is in the pod template, which is used to stamp out
multiple pod replicas, you can’t make each replica use its own separate Persistent-
VolumeClaim. You can’t use a ReplicaSet to run a distributed data store, where each
instance needs its own separate storage—at least not by using a single ReplicaSet. To
be honest, none of the API objects you’ve seen so far make running such a data store
possible. You need something else. 
10.1.1 Running multiple replicas with separate storage for each
How does one run multiple replicas of a pod and have each pod use its own storage
volume? ReplicaSets create exact copies (replicas) of a pod; therefore you can’t use
them for these types of pods. What can you use?
CREATING PODS MANUALLY
You could create pods manually and have each of them use its own PersistentVolume-
Claim, but because no ReplicaSet looks after them, you’d need to manage them man-
ually and recreate them when they disappear (as in the event of a node failure).
Therefore, this isn’t a viable option.
USING ONE REPLICASET PER POD INSTANCE
Instead of creating pods directly, you could create multiple ReplicaSets—one for each
pod with each ReplicaSet’s desired replica count set to one, and each ReplicaSet’s pod
template referencing a dedicated PersistentVolumeClaim (as shown in figure 10.2).
 Although this takes care of the automatic rescheduling in case of node failures or
accidental pod deletions, it’s much more cumbersome compared to having a single
ReplicaSet. For example, think about how you’d scale the pods in that case. You
Persistent
Volume
Claim
Persistent
Volume
ReplicaSet
Pod
Pod
Pod
Figure 10.1
All pods from the same ReplicaSet always use the same 
PersistentVolumeClaim and PersistentVolume.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="314">
  <data key="d0">Page_314</data>
  <data key="d5">Page_314</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_225">
  <data key="d0">282
CHAPTER 10
StatefulSets: deploying replicated stateful applications
couldn’t change the desired replica count—you’d have to create additional Replica-
Sets instead. 
 Using multiple ReplicaSets is therefore not the best solution. But could you maybe
use a single ReplicaSet and have each pod instance keep its own persistent state, even
though they’re all using the same storage volume? 
USING MULTIPLE DIRECTORIES IN THE SAME VOLUME
A trick you can use is to have all pods use the same PersistentVolume, but then have a
separate file directory inside that volume for each pod (this is shown in figure 10.3).
Because you can’t configure pod replicas differently from a single pod template, you
can’t tell each instance what directory it should use, but you can make each instance
automatically select (and possibly also create) a data directory that isn’t being used
by any other instance at that time. This solution does require coordination between
the instances, and isn’t easy to do correctly. It also makes the shared storage volume
the bottleneck.
10.1.2 Providing a stable identity for each pod
In addition to storage, certain clustered applications also require that each instance
has a long-lived stable identity. Pods can be killed from time to time and replaced with
PVC A1
PV A1
ReplicaSet A1
Pod A1-xyz
PVC A2
PV A2
ReplicaSet A2
Pod A2-xzy
PVC A3
PV A3
ReplicaSet A3
Pod A3-zyx
Figure 10.2
Using one ReplicaSet for each pod instance
Persistent
Volume
Claim
PersistentVolume
ReplicaSet
Pod
Pod
Pod
App
App
App
/data/1/
/data/3/
/data/2/
Figure 10.3
Working around the shared storage problem by having the app 
in each pod use a different file directory 
 
</data>
  <data key="d5">282
CHAPTER 10
StatefulSets: deploying replicated stateful applications
couldn’t change the desired replica count—you’d have to create additional Replica-
Sets instead. 
 Using multiple ReplicaSets is therefore not the best solution. But could you maybe
use a single ReplicaSet and have each pod instance keep its own persistent state, even
though they’re all using the same storage volume? 
USING MULTIPLE DIRECTORIES IN THE SAME VOLUME
A trick you can use is to have all pods use the same PersistentVolume, but then have a
separate file directory inside that volume for each pod (this is shown in figure 10.3).
Because you can’t configure pod replicas differently from a single pod template, you
can’t tell each instance what directory it should use, but you can make each instance
automatically select (and possibly also create) a data directory that isn’t being used
by any other instance at that time. This solution does require coordination between
the instances, and isn’t easy to do correctly. It also makes the shared storage volume
the bottleneck.
10.1.2 Providing a stable identity for each pod
In addition to storage, certain clustered applications also require that each instance
has a long-lived stable identity. Pods can be killed from time to time and replaced with
PVC A1
PV A1
ReplicaSet A1
Pod A1-xyz
PVC A2
PV A2
ReplicaSet A2
Pod A2-xzy
PVC A3
PV A3
ReplicaSet A3
Pod A3-zyx
Figure 10.2
Using one ReplicaSet for each pod instance
Persistent
Volume
Claim
PersistentVolume
ReplicaSet
Pod
Pod
Pod
App
App
App
/data/1/
/data/3/
/data/2/
Figure 10.3
Working around the shared storage problem by having the app 
in each pod use a different file directory 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="315">
  <data key="d0">Page_315</data>
  <data key="d5">Page_315</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_226">
  <data key="d0">283
Replicating stateful pods
new ones. When a ReplicaSet replaces a pod, the new pod is a completely new pod
with a new hostname and IP, although the data in its storage volume may be that of
the killed pod. For certain apps, starting up with the old instance’s data but with a
completely new network identity may cause problems.
 Why do certain apps mandate a stable network identity? This requirement is
fairly common in distributed stateful applications. Certain apps require the adminis-
trator to list all the other cluster members and their IP addresses (or hostnames) in
each member’s configuration file. But in Kubernetes, every time a pod is resched-
uled, the new pod gets both a new hostname and a new IP address, so the whole
application cluster would have to be reconfigured every time one of its members is
rescheduled. 
USING A DEDICATED SERVICE FOR EACH POD INSTANCE
A trick you can use to work around this problem is to provide a stable network address
for cluster members by creating a dedicated Kubernetes Service for each individual
member. Because service IPs are stable, you can then point to each member through
its service IP (rather than the pod IP) in the configuration. 
 This is similar to creating a ReplicaSet for each member to provide them with indi-
vidual storage, as described previously. Combining these two techniques results in the
setup shown in figure 10.4 (an additional service covering all the cluster members is
also shown, because you usually need one for clients of the cluster).
The solution is not only ugly, but it still doesn’t solve everything. The individual pods
can’t know which Service they are exposed through (and thus can’t know their stable
IP), so they can’t self-register in other pods using that IP. 
PVC A1
PV A1
ReplicaSet A1
Pod A1-xzy
Service A1
Service A
PVC A2
PV A2
ReplicaSet A2
Pod A2-xzy
Service A2
PVC A3
PV A3
ReplicaSet A3
Pod A3-zyx
Service A3
Figure 10.4
Using one 
Service and ReplicaSet per 
pod to provide a stable 
network address and an 
individual volume for each 
pod, respectively
 
</data>
  <data key="d5">283
Replicating stateful pods
new ones. When a ReplicaSet replaces a pod, the new pod is a completely new pod
with a new hostname and IP, although the data in its storage volume may be that of
the killed pod. For certain apps, starting up with the old instance’s data but with a
completely new network identity may cause problems.
 Why do certain apps mandate a stable network identity? This requirement is
fairly common in distributed stateful applications. Certain apps require the adminis-
trator to list all the other cluster members and their IP addresses (or hostnames) in
each member’s configuration file. But in Kubernetes, every time a pod is resched-
uled, the new pod gets both a new hostname and a new IP address, so the whole
application cluster would have to be reconfigured every time one of its members is
rescheduled. 
USING A DEDICATED SERVICE FOR EACH POD INSTANCE
A trick you can use to work around this problem is to provide a stable network address
for cluster members by creating a dedicated Kubernetes Service for each individual
member. Because service IPs are stable, you can then point to each member through
its service IP (rather than the pod IP) in the configuration. 
 This is similar to creating a ReplicaSet for each member to provide them with indi-
vidual storage, as described previously. Combining these two techniques results in the
setup shown in figure 10.4 (an additional service covering all the cluster members is
also shown, because you usually need one for clients of the cluster).
The solution is not only ugly, but it still doesn’t solve everything. The individual pods
can’t know which Service they are exposed through (and thus can’t know their stable
IP), so they can’t self-register in other pods using that IP. 
PVC A1
PV A1
ReplicaSet A1
Pod A1-xzy
Service A1
Service A
PVC A2
PV A2
ReplicaSet A2
Pod A2-xzy
Service A2
PVC A3
PV A3
ReplicaSet A3
Pod A3-zyx
Service A3
Figure 10.4
Using one 
Service and ReplicaSet per 
pod to provide a stable 
network address and an 
individual volume for each 
pod, respectively
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="316">
  <data key="d0">Page_316</data>
  <data key="d5">Page_316</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_227">
  <data key="d0">284
CHAPTER 10
StatefulSets: deploying replicated stateful applications
 Luckily, Kubernetes saves us from resorting to such complex solutions. The proper
clean and simple way of running these special types of applications in Kubernetes is
through a StatefulSet. 
10.2
Understanding StatefulSets
Instead of using a ReplicaSet to run these types of pods, you create a StatefulSet
resource, which is specifically tailored to applications where instances of the applica-
tion must be treated as non-fungible individuals, with each one having a stable name
and state. 
10.2.1 Comparing StatefulSets with ReplicaSets
To understand the purpose of StatefulSets, it’s best to compare them to ReplicaSets or
ReplicationControllers. But first let me explain them with a little analogy that’s widely
used in the field.
UNDERSTANDING STATEFUL PODS WITH THE PETS VS. CATTLE ANALOGY
You may have already heard of the pets vs. cattle analogy. If not, let me explain it. We
can treat our apps either as pets or as cattle. 
NOTE
StatefulSets were initially called PetSets. That name comes from the
pets vs. cattle analogy explained here.
We tend to treat our app instances as pets, where we give each instance a name and
take care of each instance individually. But it’s usually better to treat instances as cattle
and not pay special attention to each individual instance. This makes it easy to replace
unhealthy instances without giving it a second thought, similar to the way a farmer
replaces unhealthy cattle. 
 Instances of a stateless app, for example, behave much like heads of cattle. It
doesn’t matter if an instance dies—you can create a new instance and people won’t
notice the difference. 
 On the other hand, with stateful apps, an app instance is more like a pet. When a
pet dies, you can’t go buy a new one and expect people not to notice. To replace a lost
pet, you need to find a new one that looks and behaves exactly like the old one. In the
case of apps, this means the new instance needs to have the same state and identity as
the old one.
COMPARING STATEFULSETS WITH REPLICASETS OR REPLICATIONCONTROLLERS
Pod replicas managed by a ReplicaSet or ReplicationController are much like cattle.
Because they’re mostly stateless, they can be replaced with a completely new pod
replica at any time. Stateful pods require a different approach. When a stateful pod
instance dies (or the node it’s running on fails), the pod instance needs to be resur-
rected on another node, but the new instance needs to get the same name, network
identity, and state as the one it’s replacing. This is what happens when the pods are
managed through a StatefulSet. 
 
</data>
  <data key="d5">284
CHAPTER 10
StatefulSets: deploying replicated stateful applications
 Luckily, Kubernetes saves us from resorting to such complex solutions. The proper
clean and simple way of running these special types of applications in Kubernetes is
through a StatefulSet. 
10.2
Understanding StatefulSets
Instead of using a ReplicaSet to run these types of pods, you create a StatefulSet
resource, which is specifically tailored to applications where instances of the applica-
tion must be treated as non-fungible individuals, with each one having a stable name
and state. 
10.2.1 Comparing StatefulSets with ReplicaSets
To understand the purpose of StatefulSets, it’s best to compare them to ReplicaSets or
ReplicationControllers. But first let me explain them with a little analogy that’s widely
used in the field.
UNDERSTANDING STATEFUL PODS WITH THE PETS VS. CATTLE ANALOGY
You may have already heard of the pets vs. cattle analogy. If not, let me explain it. We
can treat our apps either as pets or as cattle. 
NOTE
StatefulSets were initially called PetSets. That name comes from the
pets vs. cattle analogy explained here.
We tend to treat our app instances as pets, where we give each instance a name and
take care of each instance individually. But it’s usually better to treat instances as cattle
and not pay special attention to each individual instance. This makes it easy to replace
unhealthy instances without giving it a second thought, similar to the way a farmer
replaces unhealthy cattle. 
 Instances of a stateless app, for example, behave much like heads of cattle. It
doesn’t matter if an instance dies—you can create a new instance and people won’t
notice the difference. 
 On the other hand, with stateful apps, an app instance is more like a pet. When a
pet dies, you can’t go buy a new one and expect people not to notice. To replace a lost
pet, you need to find a new one that looks and behaves exactly like the old one. In the
case of apps, this means the new instance needs to have the same state and identity as
the old one.
COMPARING STATEFULSETS WITH REPLICASETS OR REPLICATIONCONTROLLERS
Pod replicas managed by a ReplicaSet or ReplicationController are much like cattle.
Because they’re mostly stateless, they can be replaced with a completely new pod
replica at any time. Stateful pods require a different approach. When a stateful pod
instance dies (or the node it’s running on fails), the pod instance needs to be resur-
rected on another node, but the new instance needs to get the same name, network
identity, and state as the one it’s replacing. This is what happens when the pods are
managed through a StatefulSet. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="317">
  <data key="d0">Page_317</data>
  <data key="d5">Page_317</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_228">
  <data key="d0">285
Understanding StatefulSets
 A StatefulSet makes sure pods are rescheduled in such a way that they retain their
identity and state. It also allows you to easily scale the number of pets up and down. A
StatefulSet, like a ReplicaSet, has a desired replica count field that determines how
many pets you want running at that time. Similar to ReplicaSets, pods are created from
a pod template specified as part of the StatefulSet (remember the cookie-cutter anal-
ogy?). But unlike pods created by ReplicaSets, pods created by the StatefulSet aren’t
exact replicas of each other. Each can have its own set of volumes—in other words,
storage (and thus persistent state)—which differentiates it from its peers. Pet pods
also have a predictable (and stable) identity instead of each new pod instance getting
a completely random one. 
10.2.2 Providing a stable network identity
Each pod created by a StatefulSet is assigned an ordinal index (zero-based), which
is then used to derive the pod’s name and hostname, and to attach stable storage to
the pod. The names of the pods are thus predictable, because each pod’s name is
derived from the StatefulSet’s name and the ordinal index of the instance. Rather
than the pods having random names, they’re nicely organized, as shown in the next
figure.
INTRODUCING THE GOVERNING SERVICE
But it’s not all about the pods having a predictable name and hostname. Unlike regu-
lar pods, stateful pods sometimes need to be addressable by their hostname, whereas
stateless pods usually don’t. After all, each stateless pod is like any other. When you
need one, you pick any one of them. But with stateful pods, you usually want to oper-
ate on a specific pod from the group, because they differ from each other (they hold
different state, for example). 
 For this reason, a StatefulSet requires you to create a corresponding governing
headless Service that’s used to provide the actual network identity to each pod.
Through this Service, each pod gets its own DNS entry, so its peers and possibly other
clients in the cluster can address the pod by its hostname. For example, if the govern-
ing Service belongs to the default namespace and is called foo, and one of the pods
ReplicaSet A
Pod A-fewrb
Pod A-jwqec
Pod A-dsfwx
StatefulSet A
Pod A-1
Pod A-2
Pod A-0
Figure 10.5
Pods created by a StatefulSet have predictable names (and hostnames), 
unlike those created by a ReplicaSet
 
</data>
  <data key="d5">285
Understanding StatefulSets
 A StatefulSet makes sure pods are rescheduled in such a way that they retain their
identity and state. It also allows you to easily scale the number of pets up and down. A
StatefulSet, like a ReplicaSet, has a desired replica count field that determines how
many pets you want running at that time. Similar to ReplicaSets, pods are created from
a pod template specified as part of the StatefulSet (remember the cookie-cutter anal-
ogy?). But unlike pods created by ReplicaSets, pods created by the StatefulSet aren’t
exact replicas of each other. Each can have its own set of volumes—in other words,
storage (and thus persistent state)—which differentiates it from its peers. Pet pods
also have a predictable (and stable) identity instead of each new pod instance getting
a completely random one. 
10.2.2 Providing a stable network identity
Each pod created by a StatefulSet is assigned an ordinal index (zero-based), which
is then used to derive the pod’s name and hostname, and to attach stable storage to
the pod. The names of the pods are thus predictable, because each pod’s name is
derived from the StatefulSet’s name and the ordinal index of the instance. Rather
than the pods having random names, they’re nicely organized, as shown in the next
figure.
INTRODUCING THE GOVERNING SERVICE
But it’s not all about the pods having a predictable name and hostname. Unlike regu-
lar pods, stateful pods sometimes need to be addressable by their hostname, whereas
stateless pods usually don’t. After all, each stateless pod is like any other. When you
need one, you pick any one of them. But with stateful pods, you usually want to oper-
ate on a specific pod from the group, because they differ from each other (they hold
different state, for example). 
 For this reason, a StatefulSet requires you to create a corresponding governing
headless Service that’s used to provide the actual network identity to each pod.
Through this Service, each pod gets its own DNS entry, so its peers and possibly other
clients in the cluster can address the pod by its hostname. For example, if the govern-
ing Service belongs to the default namespace and is called foo, and one of the pods
ReplicaSet A
Pod A-fewrb
Pod A-jwqec
Pod A-dsfwx
StatefulSet A
Pod A-1
Pod A-2
Pod A-0
Figure 10.5
Pods created by a StatefulSet have predictable names (and hostnames), 
unlike those created by a ReplicaSet
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="318">
  <data key="d0">Page_318</data>
  <data key="d5">Page_318</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_229">
  <data key="d0">286
CHAPTER 10
StatefulSets: deploying replicated stateful applications
is called A-0, you can reach the pod through its fully qualified domain name, which
is a-0.foo.default.svc.cluster.local. You can’t do that with pods managed by a
ReplicaSet.
 Additionally, you can also use DNS to look up all the StatefulSet’s pods’ names by
looking up SRV records for the foo.default.svc.cluster.local domain. We’ll
explain SRV records in section 10.4 and learn how they’re used to discover members
of a StatefulSet.
REPLACING LOST PETS
When a pod instance managed by a StatefulSet disappears (because the node the pod
was running on has failed, it was evicted from the node, or someone deleted the pod
object manually), the StatefulSet makes sure it’s replaced with a new instance—similar
to how ReplicaSets do it. But in contrast to ReplicaSets, the replacement pod gets the
same name and hostname as the pod that has disappeared (this distinction between
ReplicaSets and StatefulSets is illustrated in figure 10.6).
Node 1
Node 2
Node 1
Node 2
ReplicaSet B
ReplicaSet B
StatefulSet
StatefulSet A
Pod A-0
Pod A-1
Pod A-0
Pod A-0
Pod A-1
Node 1 fails
StatefulSet A
Node 1
Node 2
Node 1
Node 2
ReplicaSet
Node 1 fails
Pod B-fdawr
Pod B-jkbde
Pod B-fdawr
Pod B-rsqkw
Pod B-jkbde
Figure 10.6
A StatefulSet replaces a lost pod with a new one with the same identity, whereas a 
ReplicaSet replaces it with a completely new unrelated pod.
 
</data>
  <data key="d5">286
CHAPTER 10
StatefulSets: deploying replicated stateful applications
is called A-0, you can reach the pod through its fully qualified domain name, which
is a-0.foo.default.svc.cluster.local. You can’t do that with pods managed by a
ReplicaSet.
 Additionally, you can also use DNS to look up all the StatefulSet’s pods’ names by
looking up SRV records for the foo.default.svc.cluster.local domain. We’ll
explain SRV records in section 10.4 and learn how they’re used to discover members
of a StatefulSet.
REPLACING LOST PETS
When a pod instance managed by a StatefulSet disappears (because the node the pod
was running on has failed, it was evicted from the node, or someone deleted the pod
object manually), the StatefulSet makes sure it’s replaced with a new instance—similar
to how ReplicaSets do it. But in contrast to ReplicaSets, the replacement pod gets the
same name and hostname as the pod that has disappeared (this distinction between
ReplicaSets and StatefulSets is illustrated in figure 10.6).
Node 1
Node 2
Node 1
Node 2
ReplicaSet B
ReplicaSet B
StatefulSet
StatefulSet A
Pod A-0
Pod A-1
Pod A-0
Pod A-0
Pod A-1
Node 1 fails
StatefulSet A
Node 1
Node 2
Node 1
Node 2
ReplicaSet
Node 1 fails
Pod B-fdawr
Pod B-jkbde
Pod B-fdawr
Pod B-rsqkw
Pod B-jkbde
Figure 10.6
A StatefulSet replaces a lost pod with a new one with the same identity, whereas a 
ReplicaSet replaces it with a completely new unrelated pod.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="319">
  <data key="d0">Page_319</data>
  <data key="d5">Page_319</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_230">
  <data key="d0">287
Understanding StatefulSets
The new pod isn’t necessarily scheduled to the same node, but as you learned early
on, what node a pod runs on shouldn’t matter. This holds true even for stateful pods.
Even if the pod is scheduled to a different node, it will still be available and reachable
under the same hostname as before. 
SCALING A STATEFULSET
Scaling the StatefulSet creates a new pod instance with the next unused ordinal index.
If you scale up from two to three instances, the new instance will get index 2 (the exist-
ing instances obviously have indexes 0 and 1). 
 The nice thing about scaling down a StatefulSet is the fact that you always know
what pod will be removed. Again, this is also in contrast to scaling down a ReplicaSet,
where you have no idea what instance will be deleted, and you can’t even specify
which one you want removed first (but this feature may be introduced in the future).
Scaling down a StatefulSet always removes the instances with the highest ordinal index
first (shown in figure 10.7). This makes the effects of a scale-down predictable.
Because certain stateful applications don’t handle rapid scale-downs nicely, Stateful-
Sets scale down only one pod instance at a time. A distributed data store, for example,
may lose data if multiple nodes go down at the same time. For example, if a replicated
data store is configured to store two copies of each data entry, in cases where two
nodes go down at the same time, a data entry would be lost if it was stored on exactly
those two nodes. If the scale-down was sequential, the distributed data store has time
to create an additional replica of the data entry somewhere else to replace the (single)
lost copy.
 For this exact reason, StatefulSets also never permit scale-down operations if any of
the instances are unhealthy. If an instance is unhealthy, and you scale down by one at
the same time, you’ve effectively lost two cluster members at once.
10.2.3 Providing stable dedicated storage to each stateful instance
You’ve seen how StatefulSets ensure stateful pods have a stable identity, but what
about storage? Each stateful pod instance needs to use its own storage, plus if a state-
ful pod is rescheduled (replaced with a new instance but with the same identity as
before), the new instance must have the same storage attached to it. How do Stateful-
Sets achieve this?
Pod
A-0
Pod
A-1
Pod
A-2
StatefulSet A
Replicas: 3
Pod
A-0
Pod
A-1
Pod
A-2
StatefulSet A
Replicas: 2
Pod
A-0
Pod
A-1
StatefulSet A
Replicas: 1
Scale down
Scale down
Figure 10.7
Scaling down a StatefulSet always removes the pod with the highest ordinal index first.
 
</data>
  <data key="d5">287
Understanding StatefulSets
The new pod isn’t necessarily scheduled to the same node, but as you learned early
on, what node a pod runs on shouldn’t matter. This holds true even for stateful pods.
Even if the pod is scheduled to a different node, it will still be available and reachable
under the same hostname as before. 
SCALING A STATEFULSET
Scaling the StatefulSet creates a new pod instance with the next unused ordinal index.
If you scale up from two to three instances, the new instance will get index 2 (the exist-
ing instances obviously have indexes 0 and 1). 
 The nice thing about scaling down a StatefulSet is the fact that you always know
what pod will be removed. Again, this is also in contrast to scaling down a ReplicaSet,
where you have no idea what instance will be deleted, and you can’t even specify
which one you want removed first (but this feature may be introduced in the future).
Scaling down a StatefulSet always removes the instances with the highest ordinal index
first (shown in figure 10.7). This makes the effects of a scale-down predictable.
Because certain stateful applications don’t handle rapid scale-downs nicely, Stateful-
Sets scale down only one pod instance at a time. A distributed data store, for example,
may lose data if multiple nodes go down at the same time. For example, if a replicated
data store is configured to store two copies of each data entry, in cases where two
nodes go down at the same time, a data entry would be lost if it was stored on exactly
those two nodes. If the scale-down was sequential, the distributed data store has time
to create an additional replica of the data entry somewhere else to replace the (single)
lost copy.
 For this exact reason, StatefulSets also never permit scale-down operations if any of
the instances are unhealthy. If an instance is unhealthy, and you scale down by one at
the same time, you’ve effectively lost two cluster members at once.
10.2.3 Providing stable dedicated storage to each stateful instance
You’ve seen how StatefulSets ensure stateful pods have a stable identity, but what
about storage? Each stateful pod instance needs to use its own storage, plus if a state-
ful pod is rescheduled (replaced with a new instance but with the same identity as
before), the new instance must have the same storage attached to it. How do Stateful-
Sets achieve this?
Pod
A-0
Pod
A-1
Pod
A-2
StatefulSet A
Replicas: 3
Pod
A-0
Pod
A-1
Pod
A-2
StatefulSet A
Replicas: 2
Pod
A-0
Pod
A-1
StatefulSet A
Replicas: 1
Scale down
Scale down
Figure 10.7
Scaling down a StatefulSet always removes the pod with the highest ordinal index first.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="320">
  <data key="d0">Page_320</data>
  <data key="d5">Page_320</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_231">
  <data key="d0">288
CHAPTER 10
StatefulSets: deploying replicated stateful applications
 Obviously, storage for stateful pods needs to be persistent and decoupled from
the pods. In chapter 6 you learned about PersistentVolumes and PersistentVolume-
Claims, which allow persistent storage to be attached to a pod by referencing the
PersistentVolumeClaim in the pod by name. Because PersistentVolumeClaims map
to PersistentVolumes one-to-one, each pod of a StatefulSet needs to reference a dif-
ferent PersistentVolumeClaim to have its own separate PersistentVolume. Because
all pod instances are stamped from the same pod template, how can they each refer
to a different PersistentVolumeClaim? And who creates these claims? Surely you’re
not expected to create as many PersistentVolumeClaims as the number of pods you
plan to have in the StatefulSet upfront? Of course not.
TEAMING UP POD TEMPLATES WITH VOLUME CLAIM TEMPLATES
The StatefulSet has to create the PersistentVolumeClaims as well, the same way it’s cre-
ating the pods. For this reason, a StatefulSet can also have one or more volume claim
templates, which enable it to stamp out PersistentVolumeClaims along with each pod
instance (see figure 10.8).
The PersistentVolumes for the claims can either be provisioned up-front by an admin-
istrator or just in time through dynamic provisioning of PersistentVolumes, as explained
at the end of chapter 6. 
UNDERSTANDING THE CREATION AND DELETION OF PERSISTENTVOLUMECLAIMS
Scaling up a StatefulSet by one creates two or more API objects (the pod and one or
more PersistentVolumeClaims referenced by the pod). Scaling down, however, deletes
only the pod, leaving the claims alone. The reason for this is obvious, if you consider
what happens when a claim is deleted. After a claim is deleted, the PersistentVolume it
was bound to gets recycled or deleted and its contents are lost. 
 Because stateful pods are meant to run stateful applications, which implies that the
data they store in the volume is important, deleting the claim on scale-down of a Stateful-
Set could be catastrophic—especially since triggering a scale-down is as simple as
decreasing the replicas field of the StatefulSet. For this reason, you’re required to
delete PersistentVolumeClaims manually to release the underlying PersistentVolume.
PVC A-0
PV
Pod A-0
PVC A-1
PV
Pod A-1
PVC A-2
PV
Pod A-2
StatefulSet A
Pod
template
Volume claim
template
Figure 10.8
A StatefulSet creates both pods and PersistentVolumeClaims.
 
</data>
  <data key="d5">288
CHAPTER 10
StatefulSets: deploying replicated stateful applications
 Obviously, storage for stateful pods needs to be persistent and decoupled from
the pods. In chapter 6 you learned about PersistentVolumes and PersistentVolume-
Claims, which allow persistent storage to be attached to a pod by referencing the
PersistentVolumeClaim in the pod by name. Because PersistentVolumeClaims map
to PersistentVolumes one-to-one, each pod of a StatefulSet needs to reference a dif-
ferent PersistentVolumeClaim to have its own separate PersistentVolume. Because
all pod instances are stamped from the same pod template, how can they each refer
to a different PersistentVolumeClaim? And who creates these claims? Surely you’re
not expected to create as many PersistentVolumeClaims as the number of pods you
plan to have in the StatefulSet upfront? Of course not.
TEAMING UP POD TEMPLATES WITH VOLUME CLAIM TEMPLATES
The StatefulSet has to create the PersistentVolumeClaims as well, the same way it’s cre-
ating the pods. For this reason, a StatefulSet can also have one or more volume claim
templates, which enable it to stamp out PersistentVolumeClaims along with each pod
instance (see figure 10.8).
The PersistentVolumes for the claims can either be provisioned up-front by an admin-
istrator or just in time through dynamic provisioning of PersistentVolumes, as explained
at the end of chapter 6. 
UNDERSTANDING THE CREATION AND DELETION OF PERSISTENTVOLUMECLAIMS
Scaling up a StatefulSet by one creates two or more API objects (the pod and one or
more PersistentVolumeClaims referenced by the pod). Scaling down, however, deletes
only the pod, leaving the claims alone. The reason for this is obvious, if you consider
what happens when a claim is deleted. After a claim is deleted, the PersistentVolume it
was bound to gets recycled or deleted and its contents are lost. 
 Because stateful pods are meant to run stateful applications, which implies that the
data they store in the volume is important, deleting the claim on scale-down of a Stateful-
Set could be catastrophic—especially since triggering a scale-down is as simple as
decreasing the replicas field of the StatefulSet. For this reason, you’re required to
delete PersistentVolumeClaims manually to release the underlying PersistentVolume.
PVC A-0
PV
Pod A-0
PVC A-1
PV
Pod A-1
PVC A-2
PV
Pod A-2
StatefulSet A
Pod
template
Volume claim
template
Figure 10.8
A StatefulSet creates both pods and PersistentVolumeClaims.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="321">
  <data key="d0">Page_321</data>
  <data key="d5">Page_321</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_232">
  <data key="d0">289
Understanding StatefulSets
REATTACHING THE PERSISTENTVOLUMECLAIM TO THE NEW INSTANCE OF THE SAME POD
The fact that the PersistentVolumeClaim remains after a scale-down means a subse-
quent scale-up can reattach the same claim along with the bound PersistentVolume
and its contents to the new pod instance (shown in figure 10.9). If you accidentally
scale down a StatefulSet, you can undo the mistake by scaling up again and the new
pod will get the same persisted state again (as well as the same name).
10.2.4 Understanding StatefulSet guarantees
As you’ve seen so far, StatefulSets behave differently from ReplicaSets or Replication-
Controllers. But this doesn’t end with the pods having a stable identity and storage.
StatefulSets also have different guarantees regarding their pods. 
UNDERSTANDING THE IMPLICATIONS OF STABLE IDENTITY AND STORAGE
While regular, stateless pods are fungible, stateful pods aren’t. We’ve already seen how
a stateful pod is always replaced with an identical pod (one having the same name and
hostname, using the same persistent storage, and so on). This happens when Kuber-
netes sees that the old pod is no longer there (for example, when you delete the pod
manually). 
 But what if Kubernetes can’t be sure about the state of the pod? If it creates a
replacement pod with the same identity, two instances of the app with the same iden-
tity might be running in the system. The two would also be bound to the same storage,
Pod
A-0
Pod
A-1
StatefulSet A
Replicas: 2
Scale
down
Scale
up
New pod instance created
with same identity as before
PVC is
re-attached
PVC
A-0
PV
PVC
A-1
PV
Pod
A-0
StatefulSet A
Replicas: 1
PVC
A-0
PV
PVC
A-1
PV
Pod
A-0
Pod has been deleted
Pod
A-1
StatefulSet A
Replicas: 2
PVC
A-0
PV
PVC
A-1
PVC has not
been deleted
PV
Figure 10.9
StatefulSets don’t delete PersistentVolumeClaims when scaling down; then they 
reattach them when scaling back up.
 
</data>
  <data key="d5">289
Understanding StatefulSets
REATTACHING THE PERSISTENTVOLUMECLAIM TO THE NEW INSTANCE OF THE SAME POD
The fact that the PersistentVolumeClaim remains after a scale-down means a subse-
quent scale-up can reattach the same claim along with the bound PersistentVolume
and its contents to the new pod instance (shown in figure 10.9). If you accidentally
scale down a StatefulSet, you can undo the mistake by scaling up again and the new
pod will get the same persisted state again (as well as the same name).
10.2.4 Understanding StatefulSet guarantees
As you’ve seen so far, StatefulSets behave differently from ReplicaSets or Replication-
Controllers. But this doesn’t end with the pods having a stable identity and storage.
StatefulSets also have different guarantees regarding their pods. 
UNDERSTANDING THE IMPLICATIONS OF STABLE IDENTITY AND STORAGE
While regular, stateless pods are fungible, stateful pods aren’t. We’ve already seen how
a stateful pod is always replaced with an identical pod (one having the same name and
hostname, using the same persistent storage, and so on). This happens when Kuber-
netes sees that the old pod is no longer there (for example, when you delete the pod
manually). 
 But what if Kubernetes can’t be sure about the state of the pod? If it creates a
replacement pod with the same identity, two instances of the app with the same iden-
tity might be running in the system. The two would also be bound to the same storage,
Pod
A-0
Pod
A-1
StatefulSet A
Replicas: 2
Scale
down
Scale
up
New pod instance created
with same identity as before
PVC is
re-attached
PVC
A-0
PV
PVC
A-1
PV
Pod
A-0
StatefulSet A
Replicas: 1
PVC
A-0
PV
PVC
A-1
PV
Pod
A-0
Pod has been deleted
Pod
A-1
StatefulSet A
Replicas: 2
PVC
A-0
PV
PVC
A-1
PVC has not
been deleted
PV
Figure 10.9
StatefulSets don’t delete PersistentVolumeClaims when scaling down; then they 
reattach them when scaling back up.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="322">
  <data key="d0">Page_322</data>
  <data key="d5">Page_322</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_233">
  <data key="d0">290
CHAPTER 10
StatefulSets: deploying replicated stateful applications
so two processes with the same identity would be writing over the same files. With pods
managed by a ReplicaSet, this isn’t a problem, because the apps are obviously made to
work on the same files. Also, ReplicaSets create pods with a randomly generated iden-
tity, so there’s no way for two processes to run with the same identity. 
INTRODUCING STATEFULSET’S AT-MOST-ONE SEMANTICS
Kubernetes must thus take great care to ensure two stateful pod instances are never
running with the same identity and are bound to the same PersistentVolumeClaim. A
StatefulSet must guarantee at-most-one semantics for stateful pod instances. 
 This means a StatefulSet must be absolutely certain that a pod is no longer run-
ning before it can create a replacement pod. This has a big effect on how node fail-
ures are handled. We’ll demonstrate this later in the chapter. Before we can do that,
however, you need to create a StatefulSet and see how it behaves. You’ll also learn a
few more things about them along the way.
10.3
Using a StatefulSet
To properly show StatefulSets in action, you’ll build your own little clustered data
store. Nothing fancy—more like a data store from the Stone Age. 
10.3.1 Creating the app and container image
You’ll use the kubia app you’ve used throughout the book as your starting point. You’ll
expand it so it allows you to store and retrieve a single data entry on each pod instance. 
 The important parts of the source code of your data store are shown in the follow-
ing listing.
...
const dataFile = "/var/data/kubia.txt";
...
var handler = function(request, response) {
  if (request.method == 'POST') {                
    var file = fs.createWriteStream(dataFile);                     
    file.on('open', function (fd) {                                
      request.pipe(file);                                          
      console.log("New data has been received and stored.");       
      response.writeHead(200);                                     
      response.end("Data stored on pod " + os.hostname() + "\n");  
    });
  } else {                                       
    var data = fileExists(dataFile)                                
      ? fs.readFileSync(dataFile, 'utf8')                          
      : "No data posted yet";                                      
    response.writeHead(200);                                       
    response.write("You've hit " + os.hostname() + "\n");          
    response.end("Data stored on this pod: " + data + "\n");       
  }
};
Listing 10.1
A simple stateful app: kubia-pet-image/app.js
On POST 
requests, store 
the request’s 
body into a 
data file.
On GET (and all 
other types of) 
requests, return 
your hostname 
and the contents 
of the data file.
 
</data>
  <data key="d5">290
CHAPTER 10
StatefulSets: deploying replicated stateful applications
so two processes with the same identity would be writing over the same files. With pods
managed by a ReplicaSet, this isn’t a problem, because the apps are obviously made to
work on the same files. Also, ReplicaSets create pods with a randomly generated iden-
tity, so there’s no way for two processes to run with the same identity. 
INTRODUCING STATEFULSET’S AT-MOST-ONE SEMANTICS
Kubernetes must thus take great care to ensure two stateful pod instances are never
running with the same identity and are bound to the same PersistentVolumeClaim. A
StatefulSet must guarantee at-most-one semantics for stateful pod instances. 
 This means a StatefulSet must be absolutely certain that a pod is no longer run-
ning before it can create a replacement pod. This has a big effect on how node fail-
ures are handled. We’ll demonstrate this later in the chapter. Before we can do that,
however, you need to create a StatefulSet and see how it behaves. You’ll also learn a
few more things about them along the way.
10.3
Using a StatefulSet
To properly show StatefulSets in action, you’ll build your own little clustered data
store. Nothing fancy—more like a data store from the Stone Age. 
10.3.1 Creating the app and container image
You’ll use the kubia app you’ve used throughout the book as your starting point. You’ll
expand it so it allows you to store and retrieve a single data entry on each pod instance. 
 The important parts of the source code of your data store are shown in the follow-
ing listing.
...
const dataFile = "/var/data/kubia.txt";
...
var handler = function(request, response) {
  if (request.method == 'POST') {                
    var file = fs.createWriteStream(dataFile);                     
    file.on('open', function (fd) {                                
      request.pipe(file);                                          
      console.log("New data has been received and stored.");       
      response.writeHead(200);                                     
      response.end("Data stored on pod " + os.hostname() + "\n");  
    });
  } else {                                       
    var data = fileExists(dataFile)                                
      ? fs.readFileSync(dataFile, 'utf8')                          
      : "No data posted yet";                                      
    response.writeHead(200);                                       
    response.write("You've hit " + os.hostname() + "\n");          
    response.end("Data stored on this pod: " + data + "\n");       
  }
};
Listing 10.1
A simple stateful app: kubia-pet-image/app.js
On POST 
requests, store 
the request’s 
body into a 
data file.
On GET (and all 
other types of) 
requests, return 
your hostname 
and the contents 
of the data file.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="323">
  <data key="d0">Page_323</data>
  <data key="d5">Page_323</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_234">
  <data key="d0">291
Using a StatefulSet
var www = http.createServer(handler);
www.listen(8080);
Whenever the app receives a POST request, it writes the data it receives in the body of
the request to the file /var/data/kubia.txt. Upon a GET request, it returns the host-
name and the stored data (contents of the file). Simple enough, right? This is the first
version of your app. It’s not clustered yet, but it’s enough to get you started. You’ll
expand the app later in the chapter.
 The Dockerfile for building the container image is shown in the following listing
and hasn’t changed from before.
FROM node:7
ADD app.js /app.js
ENTRYPOINT ["node", "app.js"]
Go ahead and build the image now, or use the one I pushed to docker.io/luksa/kubia-pet.
10.3.2 Deploying the app through a StatefulSet
To deploy your app, you’ll need to create two (or three) different types of objects:
PersistentVolumes for storing your data files (you’ll need to create these only if
the cluster doesn’t support dynamic provisioning of PersistentVolumes).
A governing Service required by the StatefulSet.
The StatefulSet itself.
For each pod instance, the StatefulSet will create a PersistentVolumeClaim that will
bind to a PersistentVolume. If your cluster supports dynamic provisioning, you don’t
need to create any PersistentVolumes manually (you can skip the next section). If it
doesn’t, you’ll need to create them as explained in the next section. 
CREATING THE PERSISTENT VOLUMES
You’ll need three PersistentVolumes, because you’ll be scaling the StatefulSet up to
three replicas. You must create more if you plan on scaling the StatefulSet up more
than that.
 If you’re using Minikube, deploy the PersistentVolumes defined in the Chapter06/
persistent-volumes-hostpath.yaml file in the book’s code archive. 
 If you’re using Google Kubernetes Engine, you’ll first need to create the actual
GCE Persistent Disks like this:
$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-a
$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-b
$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-c
NOTE
Make sure to create the disks in the same zone that your nodes are
running in.
Listing 10.2
Dockerfile for the stateful app: kubia-pet-image/Dockerfile
 
</data>
  <data key="d5">291
Using a StatefulSet
var www = http.createServer(handler);
www.listen(8080);
Whenever the app receives a POST request, it writes the data it receives in the body of
the request to the file /var/data/kubia.txt. Upon a GET request, it returns the host-
name and the stored data (contents of the file). Simple enough, right? This is the first
version of your app. It’s not clustered yet, but it’s enough to get you started. You’ll
expand the app later in the chapter.
 The Dockerfile for building the container image is shown in the following listing
and hasn’t changed from before.
FROM node:7
ADD app.js /app.js
ENTRYPOINT ["node", "app.js"]
Go ahead and build the image now, or use the one I pushed to docker.io/luksa/kubia-pet.
10.3.2 Deploying the app through a StatefulSet
To deploy your app, you’ll need to create two (or three) different types of objects:
PersistentVolumes for storing your data files (you’ll need to create these only if
the cluster doesn’t support dynamic provisioning of PersistentVolumes).
A governing Service required by the StatefulSet.
The StatefulSet itself.
For each pod instance, the StatefulSet will create a PersistentVolumeClaim that will
bind to a PersistentVolume. If your cluster supports dynamic provisioning, you don’t
need to create any PersistentVolumes manually (you can skip the next section). If it
doesn’t, you’ll need to create them as explained in the next section. 
CREATING THE PERSISTENT VOLUMES
You’ll need three PersistentVolumes, because you’ll be scaling the StatefulSet up to
three replicas. You must create more if you plan on scaling the StatefulSet up more
than that.
 If you’re using Minikube, deploy the PersistentVolumes defined in the Chapter06/
persistent-volumes-hostpath.yaml file in the book’s code archive. 
 If you’re using Google Kubernetes Engine, you’ll first need to create the actual
GCE Persistent Disks like this:
$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-a
$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-b
$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-c
NOTE
Make sure to create the disks in the same zone that your nodes are
running in.
Listing 10.2
Dockerfile for the stateful app: kubia-pet-image/Dockerfile
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="324">
  <data key="d0">Page_324</data>
  <data key="d5">Page_324</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_235">
  <data key="d0">292
CHAPTER 10
StatefulSets: deploying replicated stateful applications
Then create the PersistentVolumes from the persistent-volumes-gcepd.yaml file,
which is shown in the following listing.
kind: List                     
apiVersion: v1
items:
- apiVersion: v1
  kind: PersistentVolume       
  metadata:
    name: pv-a                
  spec:
    capacity:
      storage: 1Mi            
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle     
    gcePersistentDisk:         
      pdName: pv-a             
      fsType: nfs4                         
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-b
 ...
NOTE
In the previous chapter you specified multiple resources in the same
YAML by delimiting them with a three-dash line. Here you’re using a differ-
ent approach by defining a List object and listing the resources as items of
the object. Both methods are equivalent.
This manifest creates PersistentVolumes called pv-a, pv-b, and pv-c. They use GCE Per-
sistent Disks as the underlying storage mechanism, so they’re not appropriate for clus-
ters that aren’t running on Google Kubernetes Engine or Google Compute Engine. If
you’re running the cluster elsewhere, you must modify the PersistentVolume definition
and use an appropriate volume type, such as NFS (Network File System), or similar.
CREATING THE GOVERNING SERVICE
As explained earlier, before deploying a StatefulSet, you first need to create a headless
Service, which will be used to provide the network identity for your stateful pods. The
following listing shows the Service manifest.
apiVersion: v1
kind: Service
metadata:
  name: kubia       
spec:
  clusterIP: None    
Listing 10.3
Three PersistentVolumes: persistent-volumes-gcepd.yaml
Listing 10.4
Headless service to be used in the StatefulSet: kubia-service-headless.yaml
File describes a list 
of three persistent 
volumes
Persistent volumes’ names 
are pv-a, pv-b, and pv-c
Capacity of each persistent 
volume is 1 Mebibyte
When the volume 
is released by the 
claim, it’s recycled 
to be used again.
The volume uses a GCE 
Persistent Disk as the underlying 
storage mechanism.
Name of the 
Service
The StatefulSet’s governing 
Service must be headless.
 
</data>
  <data key="d5">292
CHAPTER 10
StatefulSets: deploying replicated stateful applications
Then create the PersistentVolumes from the persistent-volumes-gcepd.yaml file,
which is shown in the following listing.
kind: List                     
apiVersion: v1
items:
- apiVersion: v1
  kind: PersistentVolume       
  metadata:
    name: pv-a                
  spec:
    capacity:
      storage: 1Mi            
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle     
    gcePersistentDisk:         
      pdName: pv-a             
      fsType: nfs4                         
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-b
 ...
NOTE
In the previous chapter you specified multiple resources in the same
YAML by delimiting them with a three-dash line. Here you’re using a differ-
ent approach by defining a List object and listing the resources as items of
the object. Both methods are equivalent.
This manifest creates PersistentVolumes called pv-a, pv-b, and pv-c. They use GCE Per-
sistent Disks as the underlying storage mechanism, so they’re not appropriate for clus-
ters that aren’t running on Google Kubernetes Engine or Google Compute Engine. If
you’re running the cluster elsewhere, you must modify the PersistentVolume definition
and use an appropriate volume type, such as NFS (Network File System), or similar.
CREATING THE GOVERNING SERVICE
As explained earlier, before deploying a StatefulSet, you first need to create a headless
Service, which will be used to provide the network identity for your stateful pods. The
following listing shows the Service manifest.
apiVersion: v1
kind: Service
metadata:
  name: kubia       
spec:
  clusterIP: None    
Listing 10.3
Three PersistentVolumes: persistent-volumes-gcepd.yaml
Listing 10.4
Headless service to be used in the StatefulSet: kubia-service-headless.yaml
File describes a list 
of three persistent 
volumes
Persistent volumes’ names 
are pv-a, pv-b, and pv-c
Capacity of each persistent 
volume is 1 Mebibyte
When the volume 
is released by the 
claim, it’s recycled 
to be used again.
The volume uses a GCE 
Persistent Disk as the underlying 
storage mechanism.
Name of the 
Service
The StatefulSet’s governing 
Service must be headless.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="325">
  <data key="d0">Page_325</data>
  <data key="d5">Page_325</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_236">
  <data key="d0">293
Using a StatefulSet
  selector:           
    app: kubia        
  ports:
  - name: http
    port: 80
You’re setting the clusterIP field to None, which makes this a headless Service. It will
enable peer discovery between your pods (you’ll need this later). Once you create the
Service, you can move on to creating the actual StatefulSet.
CREATING THE STATEFULSET MANIFEST
Now you can finally create the StatefulSet. The following listing shows the manifest.
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: kubia
spec:
  serviceName: kubia
  replicas: 2
  template:
    metadata:
      labels:                  
        app: kubia             
    spec:
      containers:
      - name: kubia
        image: luksa/kubia-pet
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - name: data                  
          mountPath: /var/data        
  volumeClaimTemplates:
  - metadata:                  
      name: data               
    spec:                      
      resources:               
        requests:              
          storage: 1Mi         
      accessModes:             
      - ReadWriteOnce          
The StatefulSet manifest isn’t that different from ReplicaSet or Deployment manifests
you’ve created so far. What’s new is the volumeClaimTemplates list. In it, you’re defin-
ing one volume claim template called data, which will be used to create a Persistent-
VolumeClaim for each pod. As you may remember from chapter 6, a pod references a
claim by including a persistentVolumeClaim volume in the manifest. In the previous
Listing 10.5
StatefulSet manifest: kubia-statefulset.yaml
All pods with the app=kubia 
label belong to this service.
Pods created by the StatefulSet 
will have the app=kubia label.
The container inside the pod will 
mount the pvc volume at this path.
The PersistentVolumeClaims 
will be created from this 
template.
 
</data>
  <data key="d5">293
Using a StatefulSet
  selector:           
    app: kubia        
  ports:
  - name: http
    port: 80
You’re setting the clusterIP field to None, which makes this a headless Service. It will
enable peer discovery between your pods (you’ll need this later). Once you create the
Service, you can move on to creating the actual StatefulSet.
CREATING THE STATEFULSET MANIFEST
Now you can finally create the StatefulSet. The following listing shows the manifest.
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: kubia
spec:
  serviceName: kubia
  replicas: 2
  template:
    metadata:
      labels:                  
        app: kubia             
    spec:
      containers:
      - name: kubia
        image: luksa/kubia-pet
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - name: data                  
          mountPath: /var/data        
  volumeClaimTemplates:
  - metadata:                  
      name: data               
    spec:                      
      resources:               
        requests:              
          storage: 1Mi         
      accessModes:             
      - ReadWriteOnce          
The StatefulSet manifest isn’t that different from ReplicaSet or Deployment manifests
you’ve created so far. What’s new is the volumeClaimTemplates list. In it, you’re defin-
ing one volume claim template called data, which will be used to create a Persistent-
VolumeClaim for each pod. As you may remember from chapter 6, a pod references a
claim by including a persistentVolumeClaim volume in the manifest. In the previous
Listing 10.5
StatefulSet manifest: kubia-statefulset.yaml
All pods with the app=kubia 
label belong to this service.
Pods created by the StatefulSet 
will have the app=kubia label.
The container inside the pod will 
mount the pvc volume at this path.
The PersistentVolumeClaims 
will be created from this 
template.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="326">
  <data key="d0">Page_326</data>
  <data key="d5">Page_326</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_237">
  <data key="d0">294
CHAPTER 10
StatefulSets: deploying replicated stateful applications
pod template, you’ll find no such volume. The StatefulSet adds it to the pod specifica-
tion automatically and configures the volume to be bound to the claim the StatefulSet
created for the specific pod.
CREATING THE STATEFULSET
You’ll create the StatefulSet now:
$ kubectl create -f kubia-statefulset.yaml 
statefulset "kubia" created
Now, list your pods:
$ kubectl get po
NAME      READY     STATUS              RESTARTS   AGE
kubia-0   0/1       ContainerCreating   0          1s
Notice anything strange? Remember how a ReplicationController or a ReplicaSet cre-
ates all the pod instances at the same time? Your StatefulSet is configured to create
two replicas, but it created a single pod. 
 Don’t worry, nothing is wrong. The second pod will be created only after the first
one is up and ready. StatefulSets behave this way because certain clustered stateful
apps are sensitive to race conditions if two or more cluster members come up at the
same time, so it’s safer to bring each member up fully before continuing to bring up
the rest.
 List the pods again to see how the pod creation is progressing:
$ kubectl get po
NAME      READY     STATUS              RESTARTS   AGE
kubia-0   1/1       Running             0          8s
kubia-1   0/1       ContainerCreating   0          2s
See, the first pod is now running, and the second one has been created and is being
started. 
EXAMINING THE GENERATED STATEFUL POD
Let’s take a closer look at the first pod’s spec in the following listing to see how the
StatefulSet has constructed the pod from the pod template and the PersistentVolume-
Claim template.
$ kubectl get po kubia-0 -o yaml
apiVersion: v1
kind: Pod
metadata:
  ...
spec:
  containers:
  - image: luksa/kubia-pet
    ...
Listing 10.6
A stateful pod created by the StatefulSet
 
</data>
  <data key="d5">294
CHAPTER 10
StatefulSets: deploying replicated stateful applications
pod template, you’ll find no such volume. The StatefulSet adds it to the pod specifica-
tion automatically and configures the volume to be bound to the claim the StatefulSet
created for the specific pod.
CREATING THE STATEFULSET
You’ll create the StatefulSet now:
$ kubectl create -f kubia-statefulset.yaml 
statefulset "kubia" created
Now, list your pods:
$ kubectl get po
NAME      READY     STATUS              RESTARTS   AGE
kubia-0   0/1       ContainerCreating   0          1s
Notice anything strange? Remember how a ReplicationController or a ReplicaSet cre-
ates all the pod instances at the same time? Your StatefulSet is configured to create
two replicas, but it created a single pod. 
 Don’t worry, nothing is wrong. The second pod will be created only after the first
one is up and ready. StatefulSets behave this way because certain clustered stateful
apps are sensitive to race conditions if two or more cluster members come up at the
same time, so it’s safer to bring each member up fully before continuing to bring up
the rest.
 List the pods again to see how the pod creation is progressing:
$ kubectl get po
NAME      READY     STATUS              RESTARTS   AGE
kubia-0   1/1       Running             0          8s
kubia-1   0/1       ContainerCreating   0          2s
See, the first pod is now running, and the second one has been created and is being
started. 
EXAMINING THE GENERATED STATEFUL POD
Let’s take a closer look at the first pod’s spec in the following listing to see how the
StatefulSet has constructed the pod from the pod template and the PersistentVolume-
Claim template.
$ kubectl get po kubia-0 -o yaml
apiVersion: v1
kind: Pod
metadata:
  ...
spec:
  containers:
  - image: luksa/kubia-pet
    ...
Listing 10.6
A stateful pod created by the StatefulSet
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="327">
  <data key="d0">Page_327</data>
  <data key="d5">Page_327</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_238">
  <data key="d0">295
Using a StatefulSet
    volumeMounts:
    - mountPath: /var/data           
      name: data                     
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-r2m41
      readOnly: true
  ...
  volumes:
  - name: data                       
    persistentVolumeClaim:           
      claimName: data-kubia-0            
  - name: default-token-r2m41
    secret:
      secretName: default-token-r2m41
The PersistentVolumeClaim template was used to create the PersistentVolumeClaim
and the volume inside the pod, which refers to the created PersistentVolumeClaim. 
EXAMINING THE GENERATED PERSISTENTVOLUMECLAIMS
Now list the generated PersistentVolumeClaims to confirm they were created:
$ kubectl get pvc
NAME           STATUS    VOLUME    CAPACITY   ACCESSMODES   AGE
data-kubia-0   Bound     pv-c      0                        37s
data-kubia-1   Bound     pv-a      0                        37s
The names of the generated PersistentVolumeClaims are composed of the name
defined in the volumeClaimTemplate and the name of each pod. You can examine the
claims’ YAML to see that they match the template.
10.3.3 Playing with your pods
With the nodes of your data store cluster now running, you can start exploring it. You
can’t communicate with your pods through the Service you created because it’s head-
less. You’ll need to connect to individual pods directly (or create a regular Service, but
that wouldn’t allow you to talk to a specific pod).
 You’ve already seen ways to connect to a pod directly: by piggybacking on another
pod and running curl inside it, by using port-forwarding, and so on. This time, you’ll
try another option. You’ll use the API server as a proxy to the pods. 
COMMUNICATING WITH PODS THROUGH THE API SERVER
One useful feature of the API server is the ability to proxy connections directly to indi-
vidual pods. If you want to perform requests against your kubia-0 pod, you hit the fol-
lowing URL:
&lt;apiServerHost&gt;:&lt;port&gt;/api/v1/namespaces/default/pods/kubia-0/proxy/&lt;path&gt;
Because the API server is secured, sending requests to pods through the API server is
cumbersome (among other things, you need to pass the authorization token in each
request). Luckily, in chapter 8 you learned how to use kubectl proxy to talk to the
The volume mount, as 
specified in the manifest
The volume created 
by the StatefulSet
The claim referenced 
by this volume
 
</data>
  <data key="d5">295
Using a StatefulSet
    volumeMounts:
    - mountPath: /var/data           
      name: data                     
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-r2m41
      readOnly: true
  ...
  volumes:
  - name: data                       
    persistentVolumeClaim:           
      claimName: data-kubia-0            
  - name: default-token-r2m41
    secret:
      secretName: default-token-r2m41
The PersistentVolumeClaim template was used to create the PersistentVolumeClaim
and the volume inside the pod, which refers to the created PersistentVolumeClaim. 
EXAMINING THE GENERATED PERSISTENTVOLUMECLAIMS
Now list the generated PersistentVolumeClaims to confirm they were created:
$ kubectl get pvc
NAME           STATUS    VOLUME    CAPACITY   ACCESSMODES   AGE
data-kubia-0   Bound     pv-c      0                        37s
data-kubia-1   Bound     pv-a      0                        37s
The names of the generated PersistentVolumeClaims are composed of the name
defined in the volumeClaimTemplate and the name of each pod. You can examine the
claims’ YAML to see that they match the template.
10.3.3 Playing with your pods
With the nodes of your data store cluster now running, you can start exploring it. You
can’t communicate with your pods through the Service you created because it’s head-
less. You’ll need to connect to individual pods directly (or create a regular Service, but
that wouldn’t allow you to talk to a specific pod).
 You’ve already seen ways to connect to a pod directly: by piggybacking on another
pod and running curl inside it, by using port-forwarding, and so on. This time, you’ll
try another option. You’ll use the API server as a proxy to the pods. 
COMMUNICATING WITH PODS THROUGH THE API SERVER
One useful feature of the API server is the ability to proxy connections directly to indi-
vidual pods. If you want to perform requests against your kubia-0 pod, you hit the fol-
lowing URL:
&lt;apiServerHost&gt;:&lt;port&gt;/api/v1/namespaces/default/pods/kubia-0/proxy/&lt;path&gt;
Because the API server is secured, sending requests to pods through the API server is
cumbersome (among other things, you need to pass the authorization token in each
request). Luckily, in chapter 8 you learned how to use kubectl proxy to talk to the
The volume mount, as 
specified in the manifest
The volume created 
by the StatefulSet
The claim referenced 
by this volume
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="328">
  <data key="d0">Page_328</data>
  <data key="d5">Page_328</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_239">
  <data key="d0">296
CHAPTER 10
StatefulSets: deploying replicated stateful applications
API server without having to deal with authentication and SSL certificates. Run the
proxy again:
$ kubectl proxy
Starting to serve on 127.0.0.1:8001
Now, because you’ll be talking to the API server through the kubectl proxy, you’ll use
localhost:8001 rather than the actual API server host and port. You’ll send a request to
the kubia-0 pod like this:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: No data posted yet
The response shows that the request was indeed received and handled by the app run-
ning in your pod kubia-0. 
NOTE
If you receive an empty response, make sure you haven’t left out that
last slash character at the end of the URL (or make sure curl follows redirects
by using its -L option). 
Because you’re communicating with the pod through the API server, which you’re
connecting to through the kubectl proxy, the request went through two different
proxies (the first was the kubectl proxy and the other was the API server, which prox-
ied the request to the pod). For a clearer picture, examine figure 10.10.
The request you sent to the pod was a GET request, but you can also send POST
requests through the API server. This is done by sending a POST request to the same
proxy URL as the one you sent the GET request to. 
 When your app receives a POST request, it stores whatever’s in the request body
into a local file. Send a POST request to the kubia-0 pod:
$ curl -X POST -d "Hey there! This greeting was submitted to kubia-0."
➥ localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
Data stored on pod kubia-0
kubectl proxy
curl
GET localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
GET 192.168.99.100:8443/api/v1/namespaces/default/pods/kubia-0/proxy/
Authorization: Bearer &lt;token&gt;
GET 172.17.0.3:8080/
API server
Pod: kubia-0
192.168.99.100
172.17.0.3
localhost
Figure 10.10
Connecting to a pod through both the kubectl proxy and API server proxy
 
</data>
  <data key="d5">296
CHAPTER 10
StatefulSets: deploying replicated stateful applications
API server without having to deal with authentication and SSL certificates. Run the
proxy again:
$ kubectl proxy
Starting to serve on 127.0.0.1:8001
Now, because you’ll be talking to the API server through the kubectl proxy, you’ll use
localhost:8001 rather than the actual API server host and port. You’ll send a request to
the kubia-0 pod like this:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: No data posted yet
The response shows that the request was indeed received and handled by the app run-
ning in your pod kubia-0. 
NOTE
If you receive an empty response, make sure you haven’t left out that
last slash character at the end of the URL (or make sure curl follows redirects
by using its -L option). 
Because you’re communicating with the pod through the API server, which you’re
connecting to through the kubectl proxy, the request went through two different
proxies (the first was the kubectl proxy and the other was the API server, which prox-
ied the request to the pod). For a clearer picture, examine figure 10.10.
The request you sent to the pod was a GET request, but you can also send POST
requests through the API server. This is done by sending a POST request to the same
proxy URL as the one you sent the GET request to. 
 When your app receives a POST request, it stores whatever’s in the request body
into a local file. Send a POST request to the kubia-0 pod:
$ curl -X POST -d "Hey there! This greeting was submitted to kubia-0."
➥ localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
Data stored on pod kubia-0
kubectl proxy
curl
GET localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
GET 192.168.99.100:8443/api/v1/namespaces/default/pods/kubia-0/proxy/
Authorization: Bearer &lt;token&gt;
GET 172.17.0.3:8080/
API server
Pod: kubia-0
192.168.99.100
172.17.0.3
localhost
Figure 10.10
Connecting to a pod through both the kubectl proxy and API server proxy
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="329">
  <data key="d0">Page_329</data>
  <data key="d5">Page_329</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_240">
  <data key="d0">297
Using a StatefulSet
The data you sent should now be stored in that pod. Let’s see if it returns the stored
data when you perform a GET request again:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: Hey there! This greeting was submitted to kubia-0.
Okay, so far so good. Now let’s see what the other cluster node (the kubia-1 pod)
says:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/
You've hit kubia-1
Data stored on this pod: No data posted yet
As expected, each node has its own state. But is that state persisted? Let’s find out.
DELETING A STATEFUL POD TO SEE IF THE RESCHEDULED POD IS REATTACHED TO THE SAME STORAGE
You’re going to delete the kubia-0 pod and wait for it to be rescheduled. Then you’ll
see if it’s still serving the same data as before:
$ kubectl delete po kubia-0
pod "kubia-0" deleted
If you list the pods, you’ll see that the pod is terminating: 
$ kubectl get po
NAME      READY     STATUS        RESTARTS   AGE
kubia-0   1/1       Terminating   0          3m
kubia-1   1/1       Running       0          3m
As soon as it terminates successfully, a new pod with the same name is created by the
StatefulSet:
$ kubectl get po
NAME      READY     STATUS              RESTARTS   AGE
kubia-0   0/1       ContainerCreating   0          6s
kubia-1   1/1       Running             0          4m
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
kubia-0   1/1       Running   0          9s
kubia-1   1/1       Running   0          4m
Let me remind you again that this new pod may be scheduled to any node in the clus-
ter, not necessarily the same node that the old pod was scheduled to. The old pod’s
whole identity (the name, hostname, and the storage) is effectively moved to the new
node (as shown in figure 10.11). If you’re using Minikube, you can’t see this because it
only runs a single node, but in a multi-node cluster, you may see the pod scheduled to
a different node than before.
 
</data>
  <data key="d5">297
Using a StatefulSet
The data you sent should now be stored in that pod. Let’s see if it returns the stored
data when you perform a GET request again:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: Hey there! This greeting was submitted to kubia-0.
Okay, so far so good. Now let’s see what the other cluster node (the kubia-1 pod)
says:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/
You've hit kubia-1
Data stored on this pod: No data posted yet
As expected, each node has its own state. But is that state persisted? Let’s find out.
DELETING A STATEFUL POD TO SEE IF THE RESCHEDULED POD IS REATTACHED TO THE SAME STORAGE
You’re going to delete the kubia-0 pod and wait for it to be rescheduled. Then you’ll
see if it’s still serving the same data as before:
$ kubectl delete po kubia-0
pod "kubia-0" deleted
If you list the pods, you’ll see that the pod is terminating: 
$ kubectl get po
NAME      READY     STATUS        RESTARTS   AGE
kubia-0   1/1       Terminating   0          3m
kubia-1   1/1       Running       0          3m
As soon as it terminates successfully, a new pod with the same name is created by the
StatefulSet:
$ kubectl get po
NAME      READY     STATUS              RESTARTS   AGE
kubia-0   0/1       ContainerCreating   0          6s
kubia-1   1/1       Running             0          4m
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
kubia-0   1/1       Running   0          9s
kubia-1   1/1       Running   0          4m
Let me remind you again that this new pod may be scheduled to any node in the clus-
ter, not necessarily the same node that the old pod was scheduled to. The old pod’s
whole identity (the name, hostname, and the storage) is effectively moved to the new
node (as shown in figure 10.11). If you’re using Minikube, you can’t see this because it
only runs a single node, but in a multi-node cluster, you may see the pod scheduled to
a different node than before.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="330">
  <data key="d0">Page_330</data>
  <data key="d5">Page_330</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_241">
  <data key="d0">298
CHAPTER 10
StatefulSets: deploying replicated stateful applications
With the new pod now running, let’s check to see if it has the exact same identity as in
its previous incarnation. The pod’s name is the same, but what about the hostname
and persistent data? You can ask the pod itself to confirm:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: Hey there! This greeting was submitted to kubia-0.
The pod’s response shows that both the hostname and the data are the same as before,
confirming that a StatefulSet always replaces a deleted pod with what’s effectively the
exact same pod. 
SCALING A STATEFULSET
Scaling down a StatefulSet and scaling it back up after an extended time period
should be no different than deleting a pod and having the StatefulSet recreate it
immediately. Remember that scaling down a StatefulSet only deletes the pods, but
leaves the PersistentVolumeClaims untouched. I’ll let you try scaling down the State-
fulSet yourself and confirm this behavior. 
 The key thing to remember is that scaling down (and up) is performed gradu-
ally—similar to how individual pods are created when the StatefulSet is created ini-
tially. When scaling down by more than one instance, the pod with the highest ordinal
number is deleted first. Only after the pod terminates completely is the pod with the
second highest ordinal number deleted. 
EXPOSING STATEFUL PODS THROUGH A REGULAR, NON-HEADLESS SERVICE
Before you move on to the last part of this chapter, you’re going to add a proper, non-
headless Service in front of your pods, because clients usually connect to the pods
through a Service rather than connecting directly.
Node 1
Pod: kubia-0
Pod: kubia-1
Delete kubia-0
Storage
Storage
Storage
Pod: kubia-1
Storage
Node 1
kubia-0 rescheduled
Node 1
Node 2
Node 2
Node 2
Storage
Pod: kubia-1
Storage
Pod: kubia-0
Figure 10.11
A stateful pod may be rescheduled to a different node, but it retains the name, hostname, and storage.
 
</data>
  <data key="d5">298
CHAPTER 10
StatefulSets: deploying replicated stateful applications
With the new pod now running, let’s check to see if it has the exact same identity as in
its previous incarnation. The pod’s name is the same, but what about the hostname
and persistent data? You can ask the pod itself to confirm:
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: Hey there! This greeting was submitted to kubia-0.
The pod’s response shows that both the hostname and the data are the same as before,
confirming that a StatefulSet always replaces a deleted pod with what’s effectively the
exact same pod. 
SCALING A STATEFULSET
Scaling down a StatefulSet and scaling it back up after an extended time period
should be no different than deleting a pod and having the StatefulSet recreate it
immediately. Remember that scaling down a StatefulSet only deletes the pods, but
leaves the PersistentVolumeClaims untouched. I’ll let you try scaling down the State-
fulSet yourself and confirm this behavior. 
 The key thing to remember is that scaling down (and up) is performed gradu-
ally—similar to how individual pods are created when the StatefulSet is created ini-
tially. When scaling down by more than one instance, the pod with the highest ordinal
number is deleted first. Only after the pod terminates completely is the pod with the
second highest ordinal number deleted. 
EXPOSING STATEFUL PODS THROUGH A REGULAR, NON-HEADLESS SERVICE
Before you move on to the last part of this chapter, you’re going to add a proper, non-
headless Service in front of your pods, because clients usually connect to the pods
through a Service rather than connecting directly.
Node 1
Pod: kubia-0
Pod: kubia-1
Delete kubia-0
Storage
Storage
Storage
Pod: kubia-1
Storage
Node 1
kubia-0 rescheduled
Node 1
Node 2
Node 2
Node 2
Storage
Pod: kubia-1
Storage
Pod: kubia-0
Figure 10.11
A stateful pod may be rescheduled to a different node, but it retains the name, hostname, and storage.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="331">
  <data key="d0">Page_331</data>
  <data key="d5">Page_331</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_242">
  <data key="d0">299
Discovering peers in a StatefulSet
 You know how to create the Service by now, but in case you don’t, the following list-
ing shows the manifest.
apiVersion: v1
kind: Service
metadata:
  name: kubia-public
spec:
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
Because this isn’t an externally exposed Service (it’s a regular ClusterIP Service, not
a NodePort or a LoadBalancer-type Service), you can only access it from inside the
cluster. You’ll need a pod to access it from, right? Not necessarily.
CONNECTING TO CLUSTER-INTERNAL SERVICES THROUGH THE API SERVER
Instead of using a piggyback pod to access the service from inside the cluster, you can
use the same proxy feature provided by the API server to access the service the way
you’ve accessed individual pods.
 The URI path for proxy-ing requests to Services is formed like this:
/api/v1/namespaces/&lt;namespace&gt;/services/&lt;service name&gt;/proxy/&lt;path&gt;
Therefore, you can run curl on your local machine and access the service through the
kubectl proxy like this (you ran kubectl proxy earlier and it should still be running):
$ curl localhost:8001/api/v1/namespaces/default/services/kubia-
➥ public/proxy/
You've hit kubia-1
Data stored on this pod: No data posted yet
Likewise, clients (inside the cluster) can use the kubia-public service for storing to
and reading data from your clustered data store. Of course, each request lands on a
random cluster node, so you’ll get the data from a random node each time. You’ll
improve this next.
10.4
Discovering peers in a StatefulSet
We still need to cover one more important thing. An important requirement of clus-
tered apps is peer discovery—the ability to find other members of the cluster. Each
member of a StatefulSet needs to easily find all the other members. Sure, it could do
that by talking to the API server, but one of Kubernetes’ aims is to expose features that
help keep applications completely Kubernetes-agnostic. Having apps talk to the Kuber-
netes API is therefore undesirable.
Listing 10.7
A regular Service for accessing the stateful pods: kubia-service-public.yaml
 
</data>
  <data key="d5">299
Discovering peers in a StatefulSet
 You know how to create the Service by now, but in case you don’t, the following list-
ing shows the manifest.
apiVersion: v1
kind: Service
metadata:
  name: kubia-public
spec:
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
Because this isn’t an externally exposed Service (it’s a regular ClusterIP Service, not
a NodePort or a LoadBalancer-type Service), you can only access it from inside the
cluster. You’ll need a pod to access it from, right? Not necessarily.
CONNECTING TO CLUSTER-INTERNAL SERVICES THROUGH THE API SERVER
Instead of using a piggyback pod to access the service from inside the cluster, you can
use the same proxy feature provided by the API server to access the service the way
you’ve accessed individual pods.
 The URI path for proxy-ing requests to Services is formed like this:
/api/v1/namespaces/&lt;namespace&gt;/services/&lt;service name&gt;/proxy/&lt;path&gt;
Therefore, you can run curl on your local machine and access the service through the
kubectl proxy like this (you ran kubectl proxy earlier and it should still be running):
$ curl localhost:8001/api/v1/namespaces/default/services/kubia-
➥ public/proxy/
You've hit kubia-1
Data stored on this pod: No data posted yet
Likewise, clients (inside the cluster) can use the kubia-public service for storing to
and reading data from your clustered data store. Of course, each request lands on a
random cluster node, so you’ll get the data from a random node each time. You’ll
improve this next.
10.4
Discovering peers in a StatefulSet
We still need to cover one more important thing. An important requirement of clus-
tered apps is peer discovery—the ability to find other members of the cluster. Each
member of a StatefulSet needs to easily find all the other members. Sure, it could do
that by talking to the API server, but one of Kubernetes’ aims is to expose features that
help keep applications completely Kubernetes-agnostic. Having apps talk to the Kuber-
netes API is therefore undesirable.
Listing 10.7
A regular Service for accessing the stateful pods: kubia-service-public.yaml
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="332">
  <data key="d0">Page_332</data>
  <data key="d5">Page_332</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_243">
  <data key="d0">300
CHAPTER 10
StatefulSets: deploying replicated stateful applications
 How can a pod discover its peers without talking to the API? Is there an existing,
well-known technology you can use that makes this possible? How about the Domain
Name System (DNS)? Depending on how much you know about DNS, you probably
understand what an A, CNAME, or MX record is used for. Other lesser-known types of
DNS records also exist. One of them is the SRV record.
INTRODUCING SRV RECORDS
SRV records are used to point to hostnames and ports of servers providing a specific
service. Kubernetes creates SRV records to point to the hostnames of the pods back-
ing a headless service. 
 You’re going to list the SRV records for your stateful pods by running the dig DNS
lookup tool inside a new temporary pod. This is the command you’ll use:
$ kubectl run -it srvlookup --image=tutum/dnsutils --rm 
➥ --restart=Never -- dig SRV kubia.default.svc.cluster.local
The command runs a one-off pod (--restart=Never) called srvlookup, which is
attached to the console (-it) and is deleted as soon as it terminates (--rm). The
pod runs a single container from the tutum/dnsutils image and runs the following
command:
dig SRV kubia.default.svc.cluster.local
The following listing shows what the command prints out.
...
;; ANSWER SECTION:
k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.
k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.
;; ADDITIONAL SECTION:
kubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4
kubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6
...
NOTE
I’ve had to shorten the actual name to get records to fit into a single
line, so kubia.d.s.c.l is actually kubia.default.svc.cluster.local.
The ANSWER SECTION shows two SRV records pointing to the two pods backing your head-
less service. Each pod also gets its own A record, as shown in ADDITIONAL SECTION.
 For a pod to get a list of all the other pods of a StatefulSet, all you need to do is
perform an SRV DNS lookup. In Node.js, for example, the lookup is performed
like this:
dns.resolveSrv("kubia.default.svc.cluster.local", callBackFunction);
You’ll use this command in your app to enable each pod to discover its peers.
Listing 10.8
Listing DNS SRV records of your headless Service
 
</data>
  <data key="d5">300
CHAPTER 10
StatefulSets: deploying replicated stateful applications
 How can a pod discover its peers without talking to the API? Is there an existing,
well-known technology you can use that makes this possible? How about the Domain
Name System (DNS)? Depending on how much you know about DNS, you probably
understand what an A, CNAME, or MX record is used for. Other lesser-known types of
DNS records also exist. One of them is the SRV record.
INTRODUCING SRV RECORDS
SRV records are used to point to hostnames and ports of servers providing a specific
service. Kubernetes creates SRV records to point to the hostnames of the pods back-
ing a headless service. 
 You’re going to list the SRV records for your stateful pods by running the dig DNS
lookup tool inside a new temporary pod. This is the command you’ll use:
$ kubectl run -it srvlookup --image=tutum/dnsutils --rm 
➥ --restart=Never -- dig SRV kubia.default.svc.cluster.local
The command runs a one-off pod (--restart=Never) called srvlookup, which is
attached to the console (-it) and is deleted as soon as it terminates (--rm). The
pod runs a single container from the tutum/dnsutils image and runs the following
command:
dig SRV kubia.default.svc.cluster.local
The following listing shows what the command prints out.
...
;; ANSWER SECTION:
k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.
k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.
;; ADDITIONAL SECTION:
kubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4
kubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6
...
NOTE
I’ve had to shorten the actual name to get records to fit into a single
line, so kubia.d.s.c.l is actually kubia.default.svc.cluster.local.
The ANSWER SECTION shows two SRV records pointing to the two pods backing your head-
less service. Each pod also gets its own A record, as shown in ADDITIONAL SECTION.
 For a pod to get a list of all the other pods of a StatefulSet, all you need to do is
perform an SRV DNS lookup. In Node.js, for example, the lookup is performed
like this:
dns.resolveSrv("kubia.default.svc.cluster.local", callBackFunction);
You’ll use this command in your app to enable each pod to discover its peers.
Listing 10.8
Listing DNS SRV records of your headless Service
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="333">
  <data key="d0">Page_333</data>
  <data key="d5">Page_333</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_244">
  <data key="d0">301
Discovering peers in a StatefulSet
NOTE
The order of the returned SRV records is random, because they all have
the same priority. Don’t expect to always see kubia-0 listed before kubia-1.
10.4.1 Implementing peer discovery through DNS
Your Stone Age data store isn’t clustered yet. Each data store node runs completely
independently of all the others—no communication exists between them. You’ll get
them talking to each other next.
 Data posted by clients connecting to your data store cluster through the kubia-
public Service lands on a random cluster node. The cluster can store multiple data
entries, but clients currently have no good way to see all those entries. Because ser-
vices forward requests to pods randomly, a client would need to perform many
requests until it hit all the pods if it wanted to get the data from all the pods. 
 You can improve this by having the node respond with data from all the cluster
nodes. To do this, the node needs to find all its peers. You’re going to use what you
learned about StatefulSets and SRV records to do this.
 You’ll modify your app’s source code as shown in the following listing (the full
source is available in the book’s code archive; the listing shows only the important
parts).
...
const dns = require('dns');
const dataFile = "/var/data/kubia.txt";
const serviceName = "kubia.default.svc.cluster.local";
const port = 8080;
...
var handler = function(request, response) {
  if (request.method == 'POST') {
    ...
  } else {
    response.writeHead(200);
    if (request.url == '/data') {
      var data = fileExists(dataFile) 
        ? fs.readFileSync(dataFile, 'utf8') 
        : "No data posted yet";
      response.end(data);
    } else {
      response.write("You've hit " + os.hostname() + "\n");
      response.write("Data stored in the cluster:\n");
      dns.resolveSrv(serviceName, function (err, addresses) {    
        if (err) {
          response.end("Could not look up DNS SRV records: " + err);
          return;
        }
        var numResponses = 0;
        if (addresses.length == 0) {
          response.end("No peers discovered.");
        } else {
Listing 10.9
Discovering peers in a sample app: kubia-pet-peers-image/app.js
The app 
performs a DNS 
lookup to obtain 
SRV records.
 
</data>
  <data key="d5">301
Discovering peers in a StatefulSet
NOTE
The order of the returned SRV records is random, because they all have
the same priority. Don’t expect to always see kubia-0 listed before kubia-1.
10.4.1 Implementing peer discovery through DNS
Your Stone Age data store isn’t clustered yet. Each data store node runs completely
independently of all the others—no communication exists between them. You’ll get
them talking to each other next.
 Data posted by clients connecting to your data store cluster through the kubia-
public Service lands on a random cluster node. The cluster can store multiple data
entries, but clients currently have no good way to see all those entries. Because ser-
vices forward requests to pods randomly, a client would need to perform many
requests until it hit all the pods if it wanted to get the data from all the pods. 
 You can improve this by having the node respond with data from all the cluster
nodes. To do this, the node needs to find all its peers. You’re going to use what you
learned about StatefulSets and SRV records to do this.
 You’ll modify your app’s source code as shown in the following listing (the full
source is available in the book’s code archive; the listing shows only the important
parts).
...
const dns = require('dns');
const dataFile = "/var/data/kubia.txt";
const serviceName = "kubia.default.svc.cluster.local";
const port = 8080;
...
var handler = function(request, response) {
  if (request.method == 'POST') {
    ...
  } else {
    response.writeHead(200);
    if (request.url == '/data') {
      var data = fileExists(dataFile) 
        ? fs.readFileSync(dataFile, 'utf8') 
        : "No data posted yet";
      response.end(data);
    } else {
      response.write("You've hit " + os.hostname() + "\n");
      response.write("Data stored in the cluster:\n");
      dns.resolveSrv(serviceName, function (err, addresses) {    
        if (err) {
          response.end("Could not look up DNS SRV records: " + err);
          return;
        }
        var numResponses = 0;
        if (addresses.length == 0) {
          response.end("No peers discovered.");
        } else {
Listing 10.9
Discovering peers in a sample app: kubia-pet-peers-image/app.js
The app 
performs a DNS 
lookup to obtain 
SRV records.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="334">
  <data key="d0">Page_334</data>
  <data key="d5">Page_334</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_245">
  <data key="d0">302
CHAPTER 10
StatefulSets: deploying replicated stateful applications
          addresses.forEach(function (item) {                   
            var requestOptions = {
              host: item.name, 
              port: port, 
              path: '/data'
            };
            httpGet(requestOptions, function (returnedData) {   
              numResponses++;
              response.write("- " + item.name + ": " + returnedData);
              response.write("\n");
              if (numResponses == addresses.length) {
                response.end();
              }
            });
          });
        }
      });
    }
  }
};
...
Figure 10.12 shows what happens when a GET request is received by your app. The
server that receives the request first performs a lookup of SRV records for the head-
less kubia service and then sends a GET request to each of the pods backing the ser-
vice (even to itself, which obviously isn’t necessary, but I wanted to keep the code as
simple as possible). It then returns a list of all the nodes along with the data stored on
each of them.
The container image containing this new version of the app is available at docker.io/
luksa/kubia-pet-peers.
10.4.2 Updating a StatefulSet
Your StatefulSet is already running, so let’s see how to update its pod template so the
pods use the new image. You’ll also set the replica count to 3 at the same time. To
Each pod 
pointed to by 
an SRV record is 
then contacted 
to get its data.
curl
DNS
1. GET /
4. GET /data
5. GET /data
2. SRV lookup
6. Return collated data
kubia-0
kubia-1
kubia-2
3. GET /data
Figure 10.12
The operation of your simplistic distributed data store
 
</data>
  <data key="d5">302
CHAPTER 10
StatefulSets: deploying replicated stateful applications
          addresses.forEach(function (item) {                   
            var requestOptions = {
              host: item.name, 
              port: port, 
              path: '/data'
            };
            httpGet(requestOptions, function (returnedData) {   
              numResponses++;
              response.write("- " + item.name + ": " + returnedData);
              response.write("\n");
              if (numResponses == addresses.length) {
                response.end();
              }
            });
          });
        }
      });
    }
  }
};
...
Figure 10.12 shows what happens when a GET request is received by your app. The
server that receives the request first performs a lookup of SRV records for the head-
less kubia service and then sends a GET request to each of the pods backing the ser-
vice (even to itself, which obviously isn’t necessary, but I wanted to keep the code as
simple as possible). It then returns a list of all the nodes along with the data stored on
each of them.
The container image containing this new version of the app is available at docker.io/
luksa/kubia-pet-peers.
10.4.2 Updating a StatefulSet
Your StatefulSet is already running, so let’s see how to update its pod template so the
pods use the new image. You’ll also set the replica count to 3 at the same time. To
Each pod 
pointed to by 
an SRV record is 
then contacted 
to get its data.
curl
DNS
1. GET /
4. GET /data
5. GET /data
2. SRV lookup
6. Return collated data
kubia-0
kubia-1
kubia-2
3. GET /data
Figure 10.12
The operation of your simplistic distributed data store
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="335">
  <data key="d0">Page_335</data>
  <data key="d5">Page_335</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_246">
  <data key="d0">303
Discovering peers in a StatefulSet
update the StatefulSet, use the kubectl edit command (the patch command would
be another option):
$ kubectl edit statefulset kubia
This opens the StatefulSet definition in your default editor. In the definition, change
spec.replicas to 3 and modify the spec.template.spec.containers.image attri-
bute so it points to the new image (luksa/kubia-pet-peers instead of luksa/kubia-
pet). Save the file and exit the editor to update the StatefulSet. Two replicas were
running previously, so you should now see an additional replica called kubia-2 start-
ing. List the pods to confirm:
$ kubectl get po
NAME      READY     STATUS              RESTARTS   AGE
kubia-0   1/1       Running             0          25m
kubia-1   1/1       Running             0          26m
kubia-2   0/1       ContainerCreating   0          4s
The new pod instance is running the new image. But what about the existing two rep-
licas? Judging from their age, they don’t seem to have been updated. This is expected,
because initially, StatefulSets were more like ReplicaSets and not like Deployments,
so they don’t perform a rollout when the template is modified. You need to delete
the replicas manually and the StatefulSet will bring them up again based on the new
template:
$ kubectl delete po kubia-0 kubia-1
pod "kubia-0" deleted
pod "kubia-1" deleted
NOTE
Starting from Kubernetes version 1.7, StatefulSets support rolling
updates the same way Deployments and DaemonSets do. See the StatefulSet’s
spec.updateStrategy field documentation using kubectl explain for more
information.
10.4.3 Trying out your clustered data store
Once the two pods are up, you can see if your shiny new Stone Age data store works as
expected. Post a few requests to the cluster, as shown in the following listing.
$ curl -X POST -d "The sun is shining" \
➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
Data stored on pod kubia-1
$ curl -X POST -d "The weather is sweet" \
➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
Data stored on pod kubia-0
Now, read the stored data, as shown in the following listing.
Listing 10.10
Writing to the clustered data store through the service
 
</data>
  <data key="d5">303
Discovering peers in a StatefulSet
update the StatefulSet, use the kubectl edit command (the patch command would
be another option):
$ kubectl edit statefulset kubia
This opens the StatefulSet definition in your default editor. In the definition, change
spec.replicas to 3 and modify the spec.template.spec.containers.image attri-
bute so it points to the new image (luksa/kubia-pet-peers instead of luksa/kubia-
pet). Save the file and exit the editor to update the StatefulSet. Two replicas were
running previously, so you should now see an additional replica called kubia-2 start-
ing. List the pods to confirm:
$ kubectl get po
NAME      READY     STATUS              RESTARTS   AGE
kubia-0   1/1       Running             0          25m
kubia-1   1/1       Running             0          26m
kubia-2   0/1       ContainerCreating   0          4s
The new pod instance is running the new image. But what about the existing two rep-
licas? Judging from their age, they don’t seem to have been updated. This is expected,
because initially, StatefulSets were more like ReplicaSets and not like Deployments,
so they don’t perform a rollout when the template is modified. You need to delete
the replicas manually and the StatefulSet will bring them up again based on the new
template:
$ kubectl delete po kubia-0 kubia-1
pod "kubia-0" deleted
pod "kubia-1" deleted
NOTE
Starting from Kubernetes version 1.7, StatefulSets support rolling
updates the same way Deployments and DaemonSets do. See the StatefulSet’s
spec.updateStrategy field documentation using kubectl explain for more
information.
10.4.3 Trying out your clustered data store
Once the two pods are up, you can see if your shiny new Stone Age data store works as
expected. Post a few requests to the cluster, as shown in the following listing.
$ curl -X POST -d "The sun is shining" \
➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
Data stored on pod kubia-1
$ curl -X POST -d "The weather is sweet" \
➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
Data stored on pod kubia-0
Now, read the stored data, as shown in the following listing.
Listing 10.10
Writing to the clustered data store through the service
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="336">
  <data key="d0">Page_336</data>
  <data key="d5">Page_336</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_247">
  <data key="d0">304
CHAPTER 10
StatefulSets: deploying replicated stateful applications
$ curl localhost:8001/api/v1/namespaces/default/services
➥ /kubia-public/proxy/
You've hit kubia-2
Data stored on each cluster node:
- kubia-0.kubia.default.svc.cluster.local: The weather is sweet
- kubia-1.kubia.default.svc.cluster.local: The sun is shining
- kubia-2.kubia.default.svc.cluster.local: No data posted yet
Nice! When a client request reaches one of your cluster nodes, it discovers all its
peers, gathers data from them, and sends all the data back to the client. Even if you
scale the StatefulSet up or down, the pod servicing the client’s request can always find
all the peers running at that time. 
 The app itself isn’t that useful, but I hope you found it a fun way to show how
instances of a replicated stateful app can discover their peers and handle horizontal
scaling with ease.
10.5
Understanding how StatefulSets deal with node 
failures
In section 10.2.4 we stated that Kubernetes must be absolutely sure that a stateful
pod is no longer running before creating its replacement. When a node fails
abruptly, Kubernetes can’t know the state of the node or its pods. It can’t know
whether the pods are no longer running, or if they still are and are possibly even still
reachable, and it’s only the Kubelet that has stopped reporting the node’s state to
the master.
 Because a StatefulSet guarantees that there will never be two pods running with
the same identity and storage, when a node appears to have failed, the StatefulSet can-
not and should not create a replacement pod until it knows for certain that the pod is
no longer running. 
 It can only know that when the cluster administrator tells it so. To do that, the
admin needs to either delete the pod or delete the whole node (doing so then deletes
all the pods scheduled to the node).
 As your final exercise in this chapter, you’ll look at what happens to StatefulSets
and their pods when one of the cluster nodes gets disconnected from the network.
10.5.1 Simulating a node’s disconnection from the network 
As in chapter 4, you’ll simulate the node disconnecting from the network by shutting
down the node’s eth0 network interface. Because this example requires multiple
nodes, you can’t run it on Minikube. You’ll use Google Kubernetes Engine instead.
SHUTTING DOWN THE NODE’S NETWORK ADAPTER
To shut down a node’s eth0 interface, you need to ssh into one of the nodes like this:
$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1
Listing 10.11
Reading from the data store
 
</data>
  <data key="d5">304
CHAPTER 10
StatefulSets: deploying replicated stateful applications
$ curl localhost:8001/api/v1/namespaces/default/services
➥ /kubia-public/proxy/
You've hit kubia-2
Data stored on each cluster node:
- kubia-0.kubia.default.svc.cluster.local: The weather is sweet
- kubia-1.kubia.default.svc.cluster.local: The sun is shining
- kubia-2.kubia.default.svc.cluster.local: No data posted yet
Nice! When a client request reaches one of your cluster nodes, it discovers all its
peers, gathers data from them, and sends all the data back to the client. Even if you
scale the StatefulSet up or down, the pod servicing the client’s request can always find
all the peers running at that time. 
 The app itself isn’t that useful, but I hope you found it a fun way to show how
instances of a replicated stateful app can discover their peers and handle horizontal
scaling with ease.
10.5
Understanding how StatefulSets deal with node 
failures
In section 10.2.4 we stated that Kubernetes must be absolutely sure that a stateful
pod is no longer running before creating its replacement. When a node fails
abruptly, Kubernetes can’t know the state of the node or its pods. It can’t know
whether the pods are no longer running, or if they still are and are possibly even still
reachable, and it’s only the Kubelet that has stopped reporting the node’s state to
the master.
 Because a StatefulSet guarantees that there will never be two pods running with
the same identity and storage, when a node appears to have failed, the StatefulSet can-
not and should not create a replacement pod until it knows for certain that the pod is
no longer running. 
 It can only know that when the cluster administrator tells it so. To do that, the
admin needs to either delete the pod or delete the whole node (doing so then deletes
all the pods scheduled to the node).
 As your final exercise in this chapter, you’ll look at what happens to StatefulSets
and their pods when one of the cluster nodes gets disconnected from the network.
10.5.1 Simulating a node’s disconnection from the network 
As in chapter 4, you’ll simulate the node disconnecting from the network by shutting
down the node’s eth0 network interface. Because this example requires multiple
nodes, you can’t run it on Minikube. You’ll use Google Kubernetes Engine instead.
SHUTTING DOWN THE NODE’S NETWORK ADAPTER
To shut down a node’s eth0 interface, you need to ssh into one of the nodes like this:
$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1
Listing 10.11
Reading from the data store
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="337">
  <data key="d0">Page_337</data>
  <data key="d5">Page_337</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_248">
  <data key="d0">305
Understanding how StatefulSets deal with node failures
Then, inside the node, run the following command:
$ sudo ifconfig eth0 down
Your ssh session will stop working, so you’ll need to open another terminal to continue.
CHECKING THE NODE’S STATUS AS SEEN BY THE KUBERNETES MASTER
With the node’s network interface down, the Kubelet running on the node can no
longer contact the Kubernetes API server and let it know that the node and all its pods
are still running.
 After a while, the control plane will mark the node as NotReady. You can see this
when listing nodes, as the following listing shows.
$ kubectl get node
NAME                                   STATUS     AGE       VERSION
gke-kubia-default-pool-32a2cac8-596v   Ready      16m       v1.6.2
gke-kubia-default-pool-32a2cac8-m0g1   NotReady   16m       v1.6.2
gke-kubia-default-pool-32a2cac8-sgl7   Ready      16m       v1.6.2
Because the control plane is no longer getting status updates from the node, the
status of all pods on that node is Unknown. This is shown in the pod list in the follow-
ing listing.
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
kubia-0   1/1       Unknown   0          15m
kubia-1   1/1       Running   0          14m
kubia-2   1/1       Running   0          13m
As you can see, the kubia-0 pod’s status is no longer known because the pod was (and
still is) running on the node whose network interface you shut down.
UNDERSTANDING WHAT HAPPENS TO PODS WHOSE STATUS IS UNKNOWN
If the node were to come back online and report its and its pod statuses again, the pod
would again be marked as Running. But if the pod’s status remains unknown for more
than a few minutes (this time is configurable), the pod is automatically evicted from
the node. This is done by the master (the Kubernetes control plane). It evicts the pod
by deleting the pod resource. 
 When the Kubelet sees that the pod has been marked for deletion, it starts ter-
minating the pod. In your case, the Kubelet can no longer reach the master (because
you disconnected the node from the network), which means the pod will keep
running.
Listing 10.12
Observing a failed node’s status change to NotReady
Listing 10.13
Observing the pod’s status change after its node becomes NotReady
 
</data>
  <data key="d5">305
Understanding how StatefulSets deal with node failures
Then, inside the node, run the following command:
$ sudo ifconfig eth0 down
Your ssh session will stop working, so you’ll need to open another terminal to continue.
CHECKING THE NODE’S STATUS AS SEEN BY THE KUBERNETES MASTER
With the node’s network interface down, the Kubelet running on the node can no
longer contact the Kubernetes API server and let it know that the node and all its pods
are still running.
 After a while, the control plane will mark the node as NotReady. You can see this
when listing nodes, as the following listing shows.
$ kubectl get node
NAME                                   STATUS     AGE       VERSION
gke-kubia-default-pool-32a2cac8-596v   Ready      16m       v1.6.2
gke-kubia-default-pool-32a2cac8-m0g1   NotReady   16m       v1.6.2
gke-kubia-default-pool-32a2cac8-sgl7   Ready      16m       v1.6.2
Because the control plane is no longer getting status updates from the node, the
status of all pods on that node is Unknown. This is shown in the pod list in the follow-
ing listing.
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
kubia-0   1/1       Unknown   0          15m
kubia-1   1/1       Running   0          14m
kubia-2   1/1       Running   0          13m
As you can see, the kubia-0 pod’s status is no longer known because the pod was (and
still is) running on the node whose network interface you shut down.
UNDERSTANDING WHAT HAPPENS TO PODS WHOSE STATUS IS UNKNOWN
If the node were to come back online and report its and its pod statuses again, the pod
would again be marked as Running. But if the pod’s status remains unknown for more
than a few minutes (this time is configurable), the pod is automatically evicted from
the node. This is done by the master (the Kubernetes control plane). It evicts the pod
by deleting the pod resource. 
 When the Kubelet sees that the pod has been marked for deletion, it starts ter-
minating the pod. In your case, the Kubelet can no longer reach the master (because
you disconnected the node from the network), which means the pod will keep
running.
Listing 10.12
Observing a failed node’s status change to NotReady
Listing 10.13
Observing the pod’s status change after its node becomes NotReady
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="338">
  <data key="d0">Page_338</data>
  <data key="d5">Page_338</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_249">
  <data key="d0">306
CHAPTER 10
StatefulSets: deploying replicated stateful applications
 Let’s examine the current situation. Use kubectl describe to display details about
the kubia-0 pod, as shown in the following listing.
$ kubectl describe po kubia-0
Name:        kubia-0
Namespace:   default
Node:        gke-kubia-default-pool-32a2cac8-m0g1/10.132.0.2
...
Status:      Terminating (expires Tue, 23 May 2017 15:06:09 +0200)
Reason:      NodeLost
Message:     Node gke-kubia-default-pool-32a2cac8-m0g1 which was 
             running pod kubia-0 is unresponsive
The pod is shown as Terminating, with NodeLost listed as the reason for the termina-
tion. The message says the node is considered lost because it’s unresponsive.
NOTE
What’s shown here is the control plane’s view of the world. In reality,
the pod’s container is still running perfectly fine. It isn’t terminating at all.
10.5.2 Deleting the pod manually
You know the node isn’t coming back, but you need all three pods running to handle
clients properly. You need to get the kubia-0 pod rescheduled to a healthy node. As
mentioned earlier, you need to delete the node or the pod manually. 
DELETING THE POD IN THE USUAL WAY
Delete the pod the way you’ve always deleted pods:
$ kubectl delete po kubia-0
pod "kubia-0" deleted
All done, right? By deleting the pod, the StatefulSet should immediately create a
replacement pod, which will get scheduled to one of the remaining nodes. List the
pods again to confirm: 
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
kubia-0   1/1       Unknown   0          15m
kubia-1   1/1       Running   0          14m
kubia-2   1/1       Running   0          13m
That’s strange. You deleted the pod a moment ago and kubectl said it had deleted it.
Why is the same pod still there? 
NOTE
The kubia-0 pod in the listing isn’t a new pod with the same name—
this is clear by looking at the AGE column. If it were new, its age would be
merely a few seconds.
Listing 10.14
Displaying details of the pod with the unknown status
 
</data>
  <data key="d5">306
CHAPTER 10
StatefulSets: deploying replicated stateful applications
 Let’s examine the current situation. Use kubectl describe to display details about
the kubia-0 pod, as shown in the following listing.
$ kubectl describe po kubia-0
Name:        kubia-0
Namespace:   default
Node:        gke-kubia-default-pool-32a2cac8-m0g1/10.132.0.2
...
Status:      Terminating (expires Tue, 23 May 2017 15:06:09 +0200)
Reason:      NodeLost
Message:     Node gke-kubia-default-pool-32a2cac8-m0g1 which was 
             running pod kubia-0 is unresponsive
The pod is shown as Terminating, with NodeLost listed as the reason for the termina-
tion. The message says the node is considered lost because it’s unresponsive.
NOTE
What’s shown here is the control plane’s view of the world. In reality,
the pod’s container is still running perfectly fine. It isn’t terminating at all.
10.5.2 Deleting the pod manually
You know the node isn’t coming back, but you need all three pods running to handle
clients properly. You need to get the kubia-0 pod rescheduled to a healthy node. As
mentioned earlier, you need to delete the node or the pod manually. 
DELETING THE POD IN THE USUAL WAY
Delete the pod the way you’ve always deleted pods:
$ kubectl delete po kubia-0
pod "kubia-0" deleted
All done, right? By deleting the pod, the StatefulSet should immediately create a
replacement pod, which will get scheduled to one of the remaining nodes. List the
pods again to confirm: 
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
kubia-0   1/1       Unknown   0          15m
kubia-1   1/1       Running   0          14m
kubia-2   1/1       Running   0          13m
That’s strange. You deleted the pod a moment ago and kubectl said it had deleted it.
Why is the same pod still there? 
NOTE
The kubia-0 pod in the listing isn’t a new pod with the same name—
this is clear by looking at the AGE column. If it were new, its age would be
merely a few seconds.
Listing 10.14
Displaying details of the pod with the unknown status
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="339">
  <data key="d0">Page_339</data>
  <data key="d5">Page_339</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_250">
  <data key="d0">307
Summary
UNDERSTANDING WHY THE POD ISN’T DELETED
The pod was marked for deletion even before you deleted it. That’s because the con-
trol plane itself already deleted it (in order to evict it from the node). 
 If you look at listing 10.14 again, you’ll see that the pod’s status is Terminating.
The pod was already marked for deletion earlier and will be removed as soon as the
Kubelet on its node notifies the API server that the pod’s containers have terminated.
Because the node’s network is down, this will never happen. 
FORCIBLY DELETING THE POD
The only thing you can do is tell the API server to delete the pod without waiting for
the Kubelet to confirm that the pod is no longer running. You do that like this:
$ kubectl delete po kubia-0 --force --grace-period 0
warning: Immediate deletion does not wait for confirmation that the running 
resource has been terminated. The resource may continue to run on the 
cluster indefinitely.
pod "kubia-0" deleted
You need to use both the --force and --grace-period 0 options. The warning dis-
played by kubectl notifies you of what you did. If you list the pods again, you’ll finally
see a new kubia-0 pod created:
$ kubectl get po
NAME          READY     STATUS              RESTARTS   AGE
kubia-0       0/1       ContainerCreating   0          8s
kubia-1       1/1       Running             0          20m
kubia-2       1/1       Running             0          19m
WARNING
Don’t delete stateful pods forcibly unless you know the node is no
longer running or is unreachable (and will remain so forever). 
Before continuing, you may want to bring the node you disconnected back online.
You can do that by restarting the node through the GCE web console or in a terminal
by issuing the following command:
$ gcloud compute instances reset &lt;node name&gt;
10.6
Summary
This concludes the chapter on using StatefulSets to deploy stateful apps. This chapter
has shown you how to
Give replicated pods individual storage
Provide a stable identity to a pod
Create a StatefulSet and a corresponding headless governing Service
Scale and update a StatefulSet
Discover other members of the StatefulSet through DNS
 
</data>
  <data key="d5">307
Summary
UNDERSTANDING WHY THE POD ISN’T DELETED
The pod was marked for deletion even before you deleted it. That’s because the con-
trol plane itself already deleted it (in order to evict it from the node). 
 If you look at listing 10.14 again, you’ll see that the pod’s status is Terminating.
The pod was already marked for deletion earlier and will be removed as soon as the
Kubelet on its node notifies the API server that the pod’s containers have terminated.
Because the node’s network is down, this will never happen. 
FORCIBLY DELETING THE POD
The only thing you can do is tell the API server to delete the pod without waiting for
the Kubelet to confirm that the pod is no longer running. You do that like this:
$ kubectl delete po kubia-0 --force --grace-period 0
warning: Immediate deletion does not wait for confirmation that the running 
resource has been terminated. The resource may continue to run on the 
cluster indefinitely.
pod "kubia-0" deleted
You need to use both the --force and --grace-period 0 options. The warning dis-
played by kubectl notifies you of what you did. If you list the pods again, you’ll finally
see a new kubia-0 pod created:
$ kubectl get po
NAME          READY     STATUS              RESTARTS   AGE
kubia-0       0/1       ContainerCreating   0          8s
kubia-1       1/1       Running             0          20m
kubia-2       1/1       Running             0          19m
WARNING
Don’t delete stateful pods forcibly unless you know the node is no
longer running or is unreachable (and will remain so forever). 
Before continuing, you may want to bring the node you disconnected back online.
You can do that by restarting the node through the GCE web console or in a terminal
by issuing the following command:
$ gcloud compute instances reset &lt;node name&gt;
10.6
Summary
This concludes the chapter on using StatefulSets to deploy stateful apps. This chapter
has shown you how to
Give replicated pods individual storage
Provide a stable identity to a pod
Create a StatefulSet and a corresponding headless governing Service
Scale and update a StatefulSet
Discover other members of the StatefulSet through DNS
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="340">
  <data key="d0">Page_340</data>
  <data key="d5">Page_340</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_251">
  <data key="d0">308
CHAPTER 10
StatefulSets: deploying replicated stateful applications
Connect to other members through their host names
Forcibly delete stateful pods
Now that you know the major building blocks you can use to have Kubernetes run and
manage your apps, we can look more closely at how it does that. In the next chapter,
you’ll learn about the individual components that control the Kubernetes cluster and
keep your apps running.
 
</data>
  <data key="d5">308
CHAPTER 10
StatefulSets: deploying replicated stateful applications
Connect to other members through their host names
Forcibly delete stateful pods
Now that you know the major building blocks you can use to have Kubernetes run and
manage your apps, we can look more closely at how it does that. In the next chapter,
you’ll learn about the individual components that control the Kubernetes cluster and
keep your apps running.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="341">
  <data key="d0">Page_341</data>
  <data key="d5">Page_341</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_252">
  <data key="d0">309
Understanding
Kubernetes internals
By reading this book up to this point, you’ve become familiar with what Kubernetes
has to offer and what it does. But so far, I’ve intentionally not spent much time
explaining exactly how it does all this because, in my opinion, it makes no sense to
go into details of how a system works until you have a good understanding of what
the system does. That’s why we haven’t talked about exactly how a pod is scheduled
or how the various controllers running inside the Controller Manager make deployed
resources come to life. Because you now know most resources that can be deployed in
Kubernetes, it’s time to dive into how they’re implemented.
This chapter covers
What components make up a Kubernetes cluster
What each component does and how it does it
How creating a Deployment object results in a 
running pod
What a running pod is
How the network between pods works
How Kubernetes Services work
How high-availability is achieved
 
</data>
  <data key="d5">309
Understanding
Kubernetes internals
By reading this book up to this point, you’ve become familiar with what Kubernetes
has to offer and what it does. But so far, I’ve intentionally not spent much time
explaining exactly how it does all this because, in my opinion, it makes no sense to
go into details of how a system works until you have a good understanding of what
the system does. That’s why we haven’t talked about exactly how a pod is scheduled
or how the various controllers running inside the Controller Manager make deployed
resources come to life. Because you now know most resources that can be deployed in
Kubernetes, it’s time to dive into how they’re implemented.
This chapter covers
What components make up a Kubernetes cluster
What each component does and how it does it
How creating a Deployment object results in a 
running pod
What a running pod is
How the network between pods works
How Kubernetes Services work
How high-availability is achieved
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="342">
  <data key="d0">Page_342</data>
  <data key="d5">Page_342</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_253">
  <data key="d0">310
CHAPTER 11
Understanding Kubernetes internals
11.1
Understanding the architecture
Before you look at how Kubernetes does what it does, let’s take a closer look at the
components that make up a Kubernetes cluster. In chapter 1, you saw that a Kuberne-
tes cluster is split into two parts:
The Kubernetes Control Plane
The (worker) nodes
Let’s look more closely at what these two parts do and what’s running inside them.
COMPONENTS OF THE CONTROL PLANE
The Control Plane is what controls and makes the whole cluster function. To refresh
your memory, the components that make up the Control Plane are
The etcd distributed persistent storage
The API server
The Scheduler
The Controller Manager
These components store and manage the state of the cluster, but they aren’t what runs
the application containers. 
COMPONENTS RUNNING ON THE WORKER NODES
The task of running your containers is up to the components running on each
worker node:
The Kubelet
The Kubernetes Service Proxy (kube-proxy)
The Container Runtime (Docker, rkt, or others)
ADD-ON COMPONENTS
Beside the Control Plane components and the components running on the nodes, a
few add-on components are required for the cluster to provide everything discussed
so far. This includes
The Kubernetes DNS server
The Dashboard
An Ingress controller
Heapster, which we’ll talk about in chapter 14
The Container Network Interface network plugin (we’ll explain it later in this
chapter)
11.1.1 The distributed nature of Kubernetes components
The previously mentioned components all run as individual processes. The compo-
nents and their inter-dependencies are shown in figure 11.1.
 
</data>
  <data key="d5">310
CHAPTER 11
Understanding Kubernetes internals
11.1
Understanding the architecture
Before you look at how Kubernetes does what it does, let’s take a closer look at the
components that make up a Kubernetes cluster. In chapter 1, you saw that a Kuberne-
tes cluster is split into two parts:
The Kubernetes Control Plane
The (worker) nodes
Let’s look more closely at what these two parts do and what’s running inside them.
COMPONENTS OF THE CONTROL PLANE
The Control Plane is what controls and makes the whole cluster function. To refresh
your memory, the components that make up the Control Plane are
The etcd distributed persistent storage
The API server
The Scheduler
The Controller Manager
These components store and manage the state of the cluster, but they aren’t what runs
the application containers. 
COMPONENTS RUNNING ON THE WORKER NODES
The task of running your containers is up to the components running on each
worker node:
The Kubelet
The Kubernetes Service Proxy (kube-proxy)
The Container Runtime (Docker, rkt, or others)
ADD-ON COMPONENTS
Beside the Control Plane components and the components running on the nodes, a
few add-on components are required for the cluster to provide everything discussed
so far. This includes
The Kubernetes DNS server
The Dashboard
An Ingress controller
Heapster, which we’ll talk about in chapter 14
The Container Network Interface network plugin (we’ll explain it later in this
chapter)
11.1.1 The distributed nature of Kubernetes components
The previously mentioned components all run as individual processes. The compo-
nents and their inter-dependencies are shown in figure 11.1.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="343">
  <data key="d0">Page_343</data>
  <data key="d5">Page_343</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_254">
  <data key="d0">311
Understanding the architecture
To get all the features Kubernetes provides, all these components need to be running.
But several can also perform useful work individually without the other components.
You’ll see how as we examine each of them.
HOW THESE COMPONENTS COMMUNICATE
Kubernetes system components communicate only with the API server. They don’t
talk to each other directly. The API server is the only component that communicates
with etcd. None of the other components communicate with etcd directly, but instead
modify the cluster state by talking to the API server.
 Connections between the API server and the other components are almost always
initiated by the components, as shown in figure 11.1. But the API server does connect
to the Kubelet when you use kubectl to fetch logs, use kubectl attach to connect to
a running container, or use the kubectl port-forward command.
NOTE
The kubectl attach command is similar to kubectl exec, but it attaches
to the main process running in the container instead of running an addi-
tional one.
RUNNING MULTIPLE INSTANCES OF INDIVIDUAL COMPONENTS
Although the components on the worker nodes all need to run on the same node,
the components of the Control Plane can easily be split across multiple servers. There
Checking the status of the Control Plane components
The API server exposes an API resource called ComponentStatus, which shows the
health status of each Control Plane component. You can list the components and
their statuses with kubectl:
$ kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health": "true"}
Control Plane (master node)
Worker node(s)
etcd
API server
kube-proxy
Kubelet
Scheduler
Controller
Manager
Controller
Runtime
Figure 11.1
Kubernetes 
components of the Control 
Plane and the worker nodes
 
</data>
  <data key="d5">311
Understanding the architecture
To get all the features Kubernetes provides, all these components need to be running.
But several can also perform useful work individually without the other components.
You’ll see how as we examine each of them.
HOW THESE COMPONENTS COMMUNICATE
Kubernetes system components communicate only with the API server. They don’t
talk to each other directly. The API server is the only component that communicates
with etcd. None of the other components communicate with etcd directly, but instead
modify the cluster state by talking to the API server.
 Connections between the API server and the other components are almost always
initiated by the components, as shown in figure 11.1. But the API server does connect
to the Kubelet when you use kubectl to fetch logs, use kubectl attach to connect to
a running container, or use the kubectl port-forward command.
NOTE
The kubectl attach command is similar to kubectl exec, but it attaches
to the main process running in the container instead of running an addi-
tional one.
RUNNING MULTIPLE INSTANCES OF INDIVIDUAL COMPONENTS
Although the components on the worker nodes all need to run on the same node,
the components of the Control Plane can easily be split across multiple servers. There
Checking the status of the Control Plane components
The API server exposes an API resource called ComponentStatus, which shows the
health status of each Control Plane component. You can list the components and
their statuses with kubectl:
$ kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health": "true"}
Control Plane (master node)
Worker node(s)
etcd
API server
kube-proxy
Kubelet
Scheduler
Controller
Manager
Controller
Runtime
Figure 11.1
Kubernetes 
components of the Control 
Plane and the worker nodes
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="344">
  <data key="d0">Page_344</data>
  <data key="d5">Page_344</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_255">
  <data key="d0">312
CHAPTER 11
Understanding Kubernetes internals
can be more than one instance of each Control Plane component running to ensure
high availability. While multiple instances of etcd and API server can be active at the
same time and do perform their jobs in parallel, only a single instance of the Sched-
uler and the Controller Manager may be active at a given time—with the others in
standby mode.
HOW COMPONENTS ARE RUN
The Control Plane components, as well as kube-proxy, can either be deployed on the
system directly or they can run as pods (as shown in listing 11.1). You may be surprised
to hear this, but it will all make sense later when we talk about the Kubelet. 
 The Kubelet is the only component that always runs as a regular system compo-
nent, and it’s the Kubelet that then runs all the other components as pods. To run the
Control Plane components as pods, the Kubelet is also deployed on the master. The
next listing shows pods in the kube-system namespace in a cluster created with
kubeadm, which is explained in appendix B.
$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName 
➥ --sort-by spec.nodeName -n kube-system
POD                              NODE
kube-controller-manager-master   master      
kube-dns-2334855451-37d9k        master      
etcd-master                      master      
kube-apiserver-master            master      
kube-scheduler-master            master      
kube-flannel-ds-tgj9k            node1      
kube-proxy-ny3xm                 node1      
kube-flannel-ds-0eek8            node2      
kube-proxy-sp362                 node2      
kube-flannel-ds-r5yf4            node3      
kube-proxy-og9ac                 node3      
As you can see in the listing, all the Control Plane components are running as pods on
the master node. There are three worker nodes, and each one runs the kube-proxy
and a Flannel pod, which provides the overlay network for the pods (we’ll talk about
Flannel later). 
TIP
As shown in the listing, you can tell kubectl to display custom columns
with the -o custom-columns option and sort the resource list with --sort-by.
Now, let’s look at each of the components up close, starting with the lowest level com-
ponent of the Control Plane—the persistent storage.
11.1.2 How Kubernetes uses etcd
All the objects you’ve created throughout this book—Pods, ReplicationControllers,
Services, Secrets, and so on—need to be stored somewhere in a persistent manner so
their manifests survive API server restarts and failures. For this, Kubernetes uses etcd,
Listing 11.1
Kubernetes components running as pods
etcd, API server, Scheduler, 
Controller Manager, and 
the DNS server are running 
on the master.
The three nodes each run 
a Kube Proxy pod and a 
Flannel networking pod.
 
</data>
  <data key="d5">312
CHAPTER 11
Understanding Kubernetes internals
can be more than one instance of each Control Plane component running to ensure
high availability. While multiple instances of etcd and API server can be active at the
same time and do perform their jobs in parallel, only a single instance of the Sched-
uler and the Controller Manager may be active at a given time—with the others in
standby mode.
HOW COMPONENTS ARE RUN
The Control Plane components, as well as kube-proxy, can either be deployed on the
system directly or they can run as pods (as shown in listing 11.1). You may be surprised
to hear this, but it will all make sense later when we talk about the Kubelet. 
 The Kubelet is the only component that always runs as a regular system compo-
nent, and it’s the Kubelet that then runs all the other components as pods. To run the
Control Plane components as pods, the Kubelet is also deployed on the master. The
next listing shows pods in the kube-system namespace in a cluster created with
kubeadm, which is explained in appendix B.
$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName 
➥ --sort-by spec.nodeName -n kube-system
POD                              NODE
kube-controller-manager-master   master      
kube-dns-2334855451-37d9k        master      
etcd-master                      master      
kube-apiserver-master            master      
kube-scheduler-master            master      
kube-flannel-ds-tgj9k            node1      
kube-proxy-ny3xm                 node1      
kube-flannel-ds-0eek8            node2      
kube-proxy-sp362                 node2      
kube-flannel-ds-r5yf4            node3      
kube-proxy-og9ac                 node3      
As you can see in the listing, all the Control Plane components are running as pods on
the master node. There are three worker nodes, and each one runs the kube-proxy
and a Flannel pod, which provides the overlay network for the pods (we’ll talk about
Flannel later). 
TIP
As shown in the listing, you can tell kubectl to display custom columns
with the -o custom-columns option and sort the resource list with --sort-by.
Now, let’s look at each of the components up close, starting with the lowest level com-
ponent of the Control Plane—the persistent storage.
11.1.2 How Kubernetes uses etcd
All the objects you’ve created throughout this book—Pods, ReplicationControllers,
Services, Secrets, and so on—need to be stored somewhere in a persistent manner so
their manifests survive API server restarts and failures. For this, Kubernetes uses etcd,
Listing 11.1
Kubernetes components running as pods
etcd, API server, Scheduler, 
Controller Manager, and 
the DNS server are running 
on the master.
The three nodes each run 
a Kube Proxy pod and a 
Flannel networking pod.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="345">
  <data key="d0">Page_345</data>
  <data key="d5">Page_345</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_256">
  <data key="d0">313
Understanding the architecture
which is a fast, distributed, and consistent key-value store. Because it’s distributed,
you can run more than one etcd instance to provide both high availability and bet-
ter performance.
 The only component that talks to etcd directly is the Kubernetes API server. All
other components read and write data to etcd indirectly through the API server. This
brings a few benefits, among them a more robust optimistic locking system as well as
validation; and, by abstracting away the actual storage mechanism from all the other
components, it’s much simpler to replace it in the future. It’s worth emphasizing that
etcd is the only place Kubernetes stores cluster state and metadata.
HOW RESOURCES ARE STORED IN ETCD
As I’m writing this, Kubernetes can use either etcd version 2 or version 3, but version 3
is now recommended because of improved performance. etcd v2 stores keys in a hier-
archical key space, which makes key-value pairs similar to files in a file system. Each
key in etcd is either a directory, which contains other keys, or is a regular key with a
corresponding value. etcd v3 doesn’t support directories, but because the key format
remains the same (keys can include slashes), you can still think of them as being
grouped into directories. Kubernetes stores all its data in etcd under /registry. The
following listing shows a list of keys stored under /registry.
$ etcdctl ls /registry
/registry/configmaps
/registry/daemonsets
/registry/deployments
/registry/events
/registry/namespaces
/registry/pods
...
About optimistic concurrency control
Optimistic concurrency control (sometimes referred to as optimistic locking) is a
method where instead of locking a piece of data and preventing it from being read or
updated while the lock is in place, the piece of data includes a version number. Every
time the data is updated, the version number increases. When updating the data, the
version number is checked to see if it has increased between the time the client read
the data and the time it submits the update. If this happens, the update is rejected
and the client must re-read the new data and try to update it again. 
The result is that when two clients try to update the same data entry, only the first
one succeeds.
All Kubernetes resources include a metadata.resourceVersion field, which clients
need to pass back to the API server when updating an object. If the version doesn’t
match the one stored in etcd, the API server rejects the update.
Listing 11.2
Top-level entries stored in etcd by Kubernetes
 
</data>
  <data key="d5">313
Understanding the architecture
which is a fast, distributed, and consistent key-value store. Because it’s distributed,
you can run more than one etcd instance to provide both high availability and bet-
ter performance.
 The only component that talks to etcd directly is the Kubernetes API server. All
other components read and write data to etcd indirectly through the API server. This
brings a few benefits, among them a more robust optimistic locking system as well as
validation; and, by abstracting away the actual storage mechanism from all the other
components, it’s much simpler to replace it in the future. It’s worth emphasizing that
etcd is the only place Kubernetes stores cluster state and metadata.
HOW RESOURCES ARE STORED IN ETCD
As I’m writing this, Kubernetes can use either etcd version 2 or version 3, but version 3
is now recommended because of improved performance. etcd v2 stores keys in a hier-
archical key space, which makes key-value pairs similar to files in a file system. Each
key in etcd is either a directory, which contains other keys, or is a regular key with a
corresponding value. etcd v3 doesn’t support directories, but because the key format
remains the same (keys can include slashes), you can still think of them as being
grouped into directories. Kubernetes stores all its data in etcd under /registry. The
following listing shows a list of keys stored under /registry.
$ etcdctl ls /registry
/registry/configmaps
/registry/daemonsets
/registry/deployments
/registry/events
/registry/namespaces
/registry/pods
...
About optimistic concurrency control
Optimistic concurrency control (sometimes referred to as optimistic locking) is a
method where instead of locking a piece of data and preventing it from being read or
updated while the lock is in place, the piece of data includes a version number. Every
time the data is updated, the version number increases. When updating the data, the
version number is checked to see if it has increased between the time the client read
the data and the time it submits the update. If this happens, the update is rejected
and the client must re-read the new data and try to update it again. 
The result is that when two clients try to update the same data entry, only the first
one succeeds.
All Kubernetes resources include a metadata.resourceVersion field, which clients
need to pass back to the API server when updating an object. If the version doesn’t
match the one stored in etcd, the API server rejects the update.
Listing 11.2
Top-level entries stored in etcd by Kubernetes
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="346">
  <data key="d0">Page_346</data>
  <data key="d5">Page_346</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_257">
  <data key="d0">314
CHAPTER 11
Understanding Kubernetes internals
You’ll recognize that these keys correspond to the resource types you learned about in
the previous chapters. 
NOTE
If you’re using v3 of the etcd API, you can’t use the ls command to see
the contents of a directory. Instead, you can list all keys that start with a given
prefix with etcdctl get /registry --prefix=true.
The following listing shows the contents of the /registry/pods directory.
$ etcdctl ls /registry/pods
/registry/pods/default
/registry/pods/kube-system
As you can infer from the names, these two entries correspond to the default and the
kube-system namespaces, which means pods are stored per namespace. The follow-
ing listing shows the entries in the /registry/pods/default directory.
$ etcdctl ls /registry/pods/default
/registry/pods/default/kubia-159041347-xk0vc
/registry/pods/default/kubia-159041347-wt6ga
/registry/pods/default/kubia-159041347-hp2o5
Each entry corresponds to an individual pod. These aren’t directories, but key-value
entries. The following listing shows what’s stored in one of them.
$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga
{"kind":"Pod","apiVersion":"v1","metadata":{"name":"kubia-159041347-wt6ga",
"generateName":"kubia-159041347-","namespace":"default","selfLink":...
You’ll recognize that this is nothing other than a pod definition in JSON format. The
API server stores the complete JSON representation of a resource in etcd. Because of
etcd’s hierarchical key space, you can think of all the stored resources as JSON files in
a filesystem. Simple, right?
WARNING
Prior to Kubernetes version 1.7, the JSON manifest of a Secret
resource was also stored like this (it wasn’t encrypted). If someone got direct
access to etcd, they knew all your Secrets. From version 1.7, Secrets are
encrypted and thus stored much more securely.
ENSURING THE CONSISTENCY AND VALIDITY OF STORED OBJECTS
Remember Google’s Borg and Omega systems mentioned in chapter 1, which are
what Kubernetes is based on? Like Kubernetes, Omega also uses a centralized store to
hold the state of the cluster, but in contrast, multiple Control Plane components
access the store directly. All these components need to make sure they all adhere to
Listing 11.3
Keys in the /registry/pods directory
Listing 11.4
etcd entries for pods in the default namespace
Listing 11.5
An etcd entry representing a pod
 
</data>
  <data key="d5">314
CHAPTER 11
Understanding Kubernetes internals
You’ll recognize that these keys correspond to the resource types you learned about in
the previous chapters. 
NOTE
If you’re using v3 of the etcd API, you can’t use the ls command to see
the contents of a directory. Instead, you can list all keys that start with a given
prefix with etcdctl get /registry --prefix=true.
The following listing shows the contents of the /registry/pods directory.
$ etcdctl ls /registry/pods
/registry/pods/default
/registry/pods/kube-system
As you can infer from the names, these two entries correspond to the default and the
kube-system namespaces, which means pods are stored per namespace. The follow-
ing listing shows the entries in the /registry/pods/default directory.
$ etcdctl ls /registry/pods/default
/registry/pods/default/kubia-159041347-xk0vc
/registry/pods/default/kubia-159041347-wt6ga
/registry/pods/default/kubia-159041347-hp2o5
Each entry corresponds to an individual pod. These aren’t directories, but key-value
entries. The following listing shows what’s stored in one of them.
$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga
{"kind":"Pod","apiVersion":"v1","metadata":{"name":"kubia-159041347-wt6ga",
"generateName":"kubia-159041347-","namespace":"default","selfLink":...
You’ll recognize that this is nothing other than a pod definition in JSON format. The
API server stores the complete JSON representation of a resource in etcd. Because of
etcd’s hierarchical key space, you can think of all the stored resources as JSON files in
a filesystem. Simple, right?
WARNING
Prior to Kubernetes version 1.7, the JSON manifest of a Secret
resource was also stored like this (it wasn’t encrypted). If someone got direct
access to etcd, they knew all your Secrets. From version 1.7, Secrets are
encrypted and thus stored much more securely.
ENSURING THE CONSISTENCY AND VALIDITY OF STORED OBJECTS
Remember Google’s Borg and Omega systems mentioned in chapter 1, which are
what Kubernetes is based on? Like Kubernetes, Omega also uses a centralized store to
hold the state of the cluster, but in contrast, multiple Control Plane components
access the store directly. All these components need to make sure they all adhere to
Listing 11.3
Keys in the /registry/pods directory
Listing 11.4
etcd entries for pods in the default namespace
Listing 11.5
An etcd entry representing a pod
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="347">
  <data key="d0">Page_347</data>
  <data key="d5">Page_347</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_258">
  <data key="d0">315
Understanding the architecture
the same optimistic locking mechanism to handle conflicts properly. A single compo-
nent not adhering fully to the mechanism may lead to inconsistent data. 
 Kubernetes improves this by requiring all other Control Plane components to go
through the API server. This way updates to the cluster state are always consistent, because
the optimistic locking mechanism is implemented in a single place, so less chance exists,
if any, of error. The API server also makes sure that the data written to the store is always
valid and that changes to the data are only performed by authorized clients. 
ENSURING CONSISTENCY WHEN ETCD IS CLUSTERED
For ensuring high availability, you’ll usually run more than a single instance of etcd.
Multiple etcd instances will need to remain consistent. Such a distributed system
needs to reach a consensus on what the actual state is. etcd uses the RAFT consensus
algorithm to achieve this, which ensures that at any given moment, each node’s state is
either what the majority of the nodes agrees is the current state or is one of the previ-
ously agreed upon states. 
 Clients connecting to different nodes of an etcd cluster will either see the actual
current state or one of the states from the past (in Kubernetes, the only etcd client is
the API server, but there may be multiple instances). 
 The consensus algorithm requires a majority (or quorum) for the cluster to progress
to the next state. As a result, if the cluster splits into two disconnected groups of nodes,
the state in the two groups can never diverge, because to transition from the previous
state to the new one, there needs to be more than half of the nodes taking part in
the state change. If one group contains the majority of all nodes, the other one obvi-
ously doesn’t. The first group can modify the cluster state, whereas the other one can’t.
When the two groups reconnect, the second group can catch up with the state in the
first group (see figure 11.2).
Clients(s)
Clients(s)
Clients(s)
etcd-0
etcd-1
etcd-2
The nodes know
there are three nodes
in the etcd cluster.
etcd-0
etcd-1
These two nodes know
they still have quorum
and can accept state
changes from clients.
etcd-2
This node knows it does
not have quorum and
should therefore not
allow state changes.
Network
split
Figure 11.2
In a split-brain scenario, only the side which still has the majority (quorum) accepts 
state changes.
 
</data>
  <data key="d5">315
Understanding the architecture
the same optimistic locking mechanism to handle conflicts properly. A single compo-
nent not adhering fully to the mechanism may lead to inconsistent data. 
 Kubernetes improves this by requiring all other Control Plane components to go
through the API server. This way updates to the cluster state are always consistent, because
the optimistic locking mechanism is implemented in a single place, so less chance exists,
if any, of error. The API server also makes sure that the data written to the store is always
valid and that changes to the data are only performed by authorized clients. 
ENSURING CONSISTENCY WHEN ETCD IS CLUSTERED
For ensuring high availability, you’ll usually run more than a single instance of etcd.
Multiple etcd instances will need to remain consistent. Such a distributed system
needs to reach a consensus on what the actual state is. etcd uses the RAFT consensus
algorithm to achieve this, which ensures that at any given moment, each node’s state is
either what the majority of the nodes agrees is the current state or is one of the previ-
ously agreed upon states. 
 Clients connecting to different nodes of an etcd cluster will either see the actual
current state or one of the states from the past (in Kubernetes, the only etcd client is
the API server, but there may be multiple instances). 
 The consensus algorithm requires a majority (or quorum) for the cluster to progress
to the next state. As a result, if the cluster splits into two disconnected groups of nodes,
the state in the two groups can never diverge, because to transition from the previous
state to the new one, there needs to be more than half of the nodes taking part in
the state change. If one group contains the majority of all nodes, the other one obvi-
ously doesn’t. The first group can modify the cluster state, whereas the other one can’t.
When the two groups reconnect, the second group can catch up with the state in the
first group (see figure 11.2).
Clients(s)
Clients(s)
Clients(s)
etcd-0
etcd-1
etcd-2
The nodes know
there are three nodes
in the etcd cluster.
etcd-0
etcd-1
These two nodes know
they still have quorum
and can accept state
changes from clients.
etcd-2
This node knows it does
not have quorum and
should therefore not
allow state changes.
Network
split
Figure 11.2
In a split-brain scenario, only the side which still has the majority (quorum) accepts 
state changes.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="348">
  <data key="d0">Page_348</data>
  <data key="d5">Page_348</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_259">
  <data key="d0">316
CHAPTER 11
Understanding Kubernetes internals
WHY THE NUMBER OF ETCD INSTANCES SHOULD BE AN ODD NUMBER
etcd is usually deployed with an odd number of instances. I’m sure you’d like to know
why. Let’s compare having two vs. having one instance. Having two instances requires
both instances to be present to have a majority. If either of them fails, the etcd cluster
can’t transition to a new state because no majority exists. Having two instances is worse
than having only a single instance. By having two, the chance of the whole cluster fail-
ing has increased by 100%, compared to that of a single-node cluster failing. 
 The same applies when comparing three vs. four etcd instances. With three instances,
one instance can fail and a majority (of two) still exists. With four instances, you need
three nodes for a majority (two aren’t enough). In both three- and four-instance clus-
ters, only a single instance may fail. But when running four instances, if one fails, a
higher possibility exists of an additional instance of the three remaining instances fail-
ing (compared to a three-node cluster with one failed node and two remaining nodes).
 Usually, for large clusters, an etcd cluster of five or seven nodes is sufficient. It can
handle a two- or a three-node failure, respectively, which suffices in almost all situations. 
11.1.3 What the API server does
The Kubernetes API server is the central component used by all other components
and by clients, such as kubectl. It provides a CRUD (Create, Read, Update, Delete)
interface for querying and modifying the cluster state over a RESTful API. It stores
that state in etcd.
 In addition to providing a consistent way of storing objects in etcd, it also performs
validation of those objects, so clients can’t store improperly configured objects (which
they could if they were writing to the store directly). Along with validation, it also han-
dles optimistic locking, so changes to an object are never overridden by other clients
in the event of concurrent updates.
 One of the API server’s clients is the command-line tool kubectl you’ve been
using from the beginning of the book. When creating a resource from a JSON file, for
example, kubectl posts the file’s contents to the API server through an HTTP POST
request. Figure 11.3 shows what happens inside the API server when it receives the
request. This is explained in more detail in the next few paragraphs.
API server
etcd
Authentication
plugin 1
Authentication
plugin 2
Authentication
plugin 3
Client
(
)
kubectl
HTTP POST
request
Authorization
plugin 1
Authorization
plugin 2
Authorization
plugin 3
Admission
control plugin 1
Admission
control plugin 2
Admission
control plugin 3
Resource
validation
Figure 11.3
The operation of the API server
 
</data>
  <data key="d5">316
CHAPTER 11
Understanding Kubernetes internals
WHY THE NUMBER OF ETCD INSTANCES SHOULD BE AN ODD NUMBER
etcd is usually deployed with an odd number of instances. I’m sure you’d like to know
why. Let’s compare having two vs. having one instance. Having two instances requires
both instances to be present to have a majority. If either of them fails, the etcd cluster
can’t transition to a new state because no majority exists. Having two instances is worse
than having only a single instance. By having two, the chance of the whole cluster fail-
ing has increased by 100%, compared to that of a single-node cluster failing. 
 The same applies when comparing three vs. four etcd instances. With three instances,
one instance can fail and a majority (of two) still exists. With four instances, you need
three nodes for a majority (two aren’t enough). In both three- and four-instance clus-
ters, only a single instance may fail. But when running four instances, if one fails, a
higher possibility exists of an additional instance of the three remaining instances fail-
ing (compared to a three-node cluster with one failed node and two remaining nodes).
 Usually, for large clusters, an etcd cluster of five or seven nodes is sufficient. It can
handle a two- or a three-node failure, respectively, which suffices in almost all situations. 
11.1.3 What the API server does
The Kubernetes API server is the central component used by all other components
and by clients, such as kubectl. It provides a CRUD (Create, Read, Update, Delete)
interface for querying and modifying the cluster state over a RESTful API. It stores
that state in etcd.
 In addition to providing a consistent way of storing objects in etcd, it also performs
validation of those objects, so clients can’t store improperly configured objects (which
they could if they were writing to the store directly). Along with validation, it also han-
dles optimistic locking, so changes to an object are never overridden by other clients
in the event of concurrent updates.
 One of the API server’s clients is the command-line tool kubectl you’ve been
using from the beginning of the book. When creating a resource from a JSON file, for
example, kubectl posts the file’s contents to the API server through an HTTP POST
request. Figure 11.3 shows what happens inside the API server when it receives the
request. This is explained in more detail in the next few paragraphs.
API server
etcd
Authentication
plugin 1
Authentication
plugin 2
Authentication
plugin 3
Client
(
)
kubectl
HTTP POST
request
Authorization
plugin 1
Authorization
plugin 2
Authorization
plugin 3
Admission
control plugin 1
Admission
control plugin 2
Admission
control plugin 3
Resource
validation
Figure 11.3
The operation of the API server
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="349">
  <data key="d0">Page_349</data>
  <data key="d5">Page_349</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_260">
  <data key="d0">317
Understanding the architecture
AUTHENTICATING THE CLIENT WITH AUTHENTICATION PLUGINS
First, the API server needs to authenticate the client sending the request. This is per-
formed by one or more authentication plugins configured in the API server. The API
server calls these plugins in turn, until one of them determines who is sending the
request. It does this by inspecting the HTTP request. 
 Depending on the authentication method, the user can be extracted from the cli-
ent’s certificate or an HTTP header, such as Authorization, which you used in chap-
ter 8. The plugin extracts the client’s username, user ID, and groups the user belongs
to. This data is then used in the next stage, which is authorization.
AUTHORIZING THE CLIENT WITH AUTHORIZATION PLUGINS
Besides authentication plugins, the API server is also configured to use one or more
authorization plugins. Their job is to determine whether the authenticated user can
perform the requested action on the requested resource. For example, when creating
pods, the API server consults all authorization plugins in turn, to determine whether
the user can create pods in the requested namespace. As soon as a plugin says the user
can perform the action, the API server progresses to the next stage.
VALIDATING AND/OR MODIFYING THE RESOURCE IN THE REQUEST WITH ADMISSION CONTROL PLUGINS
If the request is trying to create, modify, or delete a resource, the request is sent
through Admission Control. Again, the server is configured with multiple Admission
Control plugins. These plugins can modify the resource for different reasons. They
may initialize fields missing from the resource specification to the configured default
values or even override them. They may even modify other related resources, which
aren’t in the request, and can also reject a request for whatever reason. The resource
passes through all Admission Control plugins.
NOTE
When the request is only trying to read data, the request doesn’t go
through the Admission Control.
Examples of Admission Control plugins include

AlwaysPullImages—Overrides the pod’s imagePullPolicy to Always, forcing
the image to be pulled every time the pod is deployed.

ServiceAccount—Applies the default service account to pods that don’t specify
it explicitly.

NamespaceLifecycle—Prevents creation of pods in namespaces that are in the
process of being deleted, as well as in non-existing namespaces.

ResourceQuota—Ensures pods in a certain namespace only use as much CPU
and memory as has been allotted to the namespace. We’ll learn more about this
in chapter 14.
You’ll find a list of additional Admission Control plugins in the Kubernetes documen-
tation at https:/
/kubernetes.io/docs/admin/admission-controllers/.
 
</data>
  <data key="d5">317
Understanding the architecture
AUTHENTICATING THE CLIENT WITH AUTHENTICATION PLUGINS
First, the API server needs to authenticate the client sending the request. This is per-
formed by one or more authentication plugins configured in the API server. The API
server calls these plugins in turn, until one of them determines who is sending the
request. It does this by inspecting the HTTP request. 
 Depending on the authentication method, the user can be extracted from the cli-
ent’s certificate or an HTTP header, such as Authorization, which you used in chap-
ter 8. The plugin extracts the client’s username, user ID, and groups the user belongs
to. This data is then used in the next stage, which is authorization.
AUTHORIZING THE CLIENT WITH AUTHORIZATION PLUGINS
Besides authentication plugins, the API server is also configured to use one or more
authorization plugins. Their job is to determine whether the authenticated user can
perform the requested action on the requested resource. For example, when creating
pods, the API server consults all authorization plugins in turn, to determine whether
the user can create pods in the requested namespace. As soon as a plugin says the user
can perform the action, the API server progresses to the next stage.
VALIDATING AND/OR MODIFYING THE RESOURCE IN THE REQUEST WITH ADMISSION CONTROL PLUGINS
If the request is trying to create, modify, or delete a resource, the request is sent
through Admission Control. Again, the server is configured with multiple Admission
Control plugins. These plugins can modify the resource for different reasons. They
may initialize fields missing from the resource specification to the configured default
values or even override them. They may even modify other related resources, which
aren’t in the request, and can also reject a request for whatever reason. The resource
passes through all Admission Control plugins.
NOTE
When the request is only trying to read data, the request doesn’t go
through the Admission Control.
Examples of Admission Control plugins include

AlwaysPullImages—Overrides the pod’s imagePullPolicy to Always, forcing
the image to be pulled every time the pod is deployed.

ServiceAccount—Applies the default service account to pods that don’t specify
it explicitly.

NamespaceLifecycle—Prevents creation of pods in namespaces that are in the
process of being deleted, as well as in non-existing namespaces.

ResourceQuota—Ensures pods in a certain namespace only use as much CPU
and memory as has been allotted to the namespace. We’ll learn more about this
in chapter 14.
You’ll find a list of additional Admission Control plugins in the Kubernetes documen-
tation at https:/
/kubernetes.io/docs/admin/admission-controllers/.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="350">
  <data key="d0">Page_350</data>
  <data key="d5">Page_350</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_261">
  <data key="d0">318
CHAPTER 11
Understanding Kubernetes internals
VALIDATING THE RESOURCE AND STORING IT PERSISTENTLY
After letting the request pass through all the Admission Control plugins, the API server
then validates the object, stores it in etcd, and returns a response to the client.
11.1.4 Understanding how the API server notifies clients of resource 
changes
The API server doesn’t do anything else except what we’ve discussed. For example, it
doesn’t create pods when you create a ReplicaSet resource and it doesn’t manage the
endpoints of a service. That’s what controllers in the Controller Manager do. 
 But the API server doesn’t even tell these controllers what to do. All it does is
enable those controllers and other components to observe changes to deployed
resources. A Control Plane component can request to be notified when a resource is
created, modified, or deleted. This enables the component to perform whatever task
it needs in response to a change of the cluster metadata.
 Clients watch for changes by opening an HTTP connection to the API server.
Through this connection, the client will then receive a stream of modifications to the
watched objects. Every time an object is updated, the server sends the new version of
the object to all connected clients watching the object. Figure 11.4 shows how clients
can watch for changes to pods and how a change to one of the pods is stored into etcd
and then relayed to all clients watching pods at that moment.
One of the API server’s clients is the kubectl tool, which also supports watching
resources. For example, when deploying a pod, you don’t need to constantly poll the list
of pods by repeatedly executing kubectl get pods. Instead, you can use the --watch
flag and be notified of each creation, modification, or deletion of a pod, as shown in
the following listing.
$ kubectl get pods --watch
NAME                    READY     STATUS              RESTARTS   AGE
Listing 11.6
Watching a pod being created and then deleted
Various
clients
kubectl
API server
1. GET /.../pods?watch=true
2. POST /.../pods/pod-xyz
5. Send updated object
to all watchers
3. Update object
in etcd
4. Modiﬁcation
notiﬁcation
etcd
Figure 11.4
When an object is updated, the API server sends the updated object to all interested 
watchers.
 
</data>
  <data key="d5">318
CHAPTER 11
Understanding Kubernetes internals
VALIDATING THE RESOURCE AND STORING IT PERSISTENTLY
After letting the request pass through all the Admission Control plugins, the API server
then validates the object, stores it in etcd, and returns a response to the client.
11.1.4 Understanding how the API server notifies clients of resource 
changes
The API server doesn’t do anything else except what we’ve discussed. For example, it
doesn’t create pods when you create a ReplicaSet resource and it doesn’t manage the
endpoints of a service. That’s what controllers in the Controller Manager do. 
 But the API server doesn’t even tell these controllers what to do. All it does is
enable those controllers and other components to observe changes to deployed
resources. A Control Plane component can request to be notified when a resource is
created, modified, or deleted. This enables the component to perform whatever task
it needs in response to a change of the cluster metadata.
 Clients watch for changes by opening an HTTP connection to the API server.
Through this connection, the client will then receive a stream of modifications to the
watched objects. Every time an object is updated, the server sends the new version of
the object to all connected clients watching the object. Figure 11.4 shows how clients
can watch for changes to pods and how a change to one of the pods is stored into etcd
and then relayed to all clients watching pods at that moment.
One of the API server’s clients is the kubectl tool, which also supports watching
resources. For example, when deploying a pod, you don’t need to constantly poll the list
of pods by repeatedly executing kubectl get pods. Instead, you can use the --watch
flag and be notified of each creation, modification, or deletion of a pod, as shown in
the following listing.
$ kubectl get pods --watch
NAME                    READY     STATUS              RESTARTS   AGE
Listing 11.6
Watching a pod being created and then deleted
Various
clients
kubectl
API server
1. GET /.../pods?watch=true
2. POST /.../pods/pod-xyz
5. Send updated object
to all watchers
3. Update object
in etcd
4. Modiﬁcation
notiﬁcation
etcd
Figure 11.4
When an object is updated, the API server sends the updated object to all interested 
watchers.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="351">
  <data key="d0">Page_351</data>
  <data key="d5">Page_351</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_262">
  <data key="d0">319
Understanding the architecture
kubia-159041347-14j3i   0/1       Pending             0          0s
kubia-159041347-14j3i   0/1       Pending             0          0s
kubia-159041347-14j3i   0/1       ContainerCreating   0          1s
kubia-159041347-14j3i   0/1       Running             0          3s
kubia-159041347-14j3i   1/1       Running             0          5s
kubia-159041347-14j3i   1/1       Terminating         0          9s
kubia-159041347-14j3i   0/1       Terminating         0          17s
kubia-159041347-14j3i   0/1       Terminating         0          17s
kubia-159041347-14j3i   0/1       Terminating         0          17s
You can even have kubectl print out the whole YAML on each watch event like this:
$ kubectl get pods -o yaml --watch
The watch mechanism is also used by the Scheduler, which is the next Control Plane
component you’re going to learn more about.
11.1.5 Understanding the Scheduler
You’ve already learned that you don’t usually specify which cluster node a pod should
run on. This is left to the Scheduler. From afar, the operation of the Scheduler looks
simple. All it does is wait for newly created pods through the API server’s watch mech-
anism and assign a node to each new pod that doesn’t already have the node set. 
 The Scheduler doesn’t instruct the selected node (or the Kubelet running on that
node) to run the pod. All the Scheduler does is update the pod definition through the
API server. The API server then notifies the Kubelet (again, through the watch mech-
anism described previously) that the pod has been scheduled. As soon as the Kubelet
on the target node sees the pod has been scheduled to its node, it creates and runs the
pod’s containers.
 Although a coarse-grained view of the scheduling process seems trivial, the actual
task of selecting the best node for the pod isn’t that simple. Sure, the simplest
Scheduler could pick a random node and not care about the pods already running on
that node. On the other side of the spectrum, the Scheduler could use advanced tech-
niques such as machine learning to anticipate what kind of pods are about to be
scheduled in the next minutes or hours and schedule pods to maximize future hard-
ware utilization without requiring any rescheduling of existing pods. Kubernetes’
default Scheduler falls somewhere in between. 
UNDERSTANDING THE DEFAULT SCHEDULING ALGORITHM
The selection of a node can be broken down into two parts, as shown in figure 11.5:
Filtering the list of all nodes to obtain a list of acceptable nodes the pod can be
scheduled to.
Prioritizing the acceptable nodes and choosing the best one. If multiple nodes
have the highest score, round-robin is used to ensure pods are deployed across
all of them evenly.
 
</data>
  <data key="d5">319
Understanding the architecture
kubia-159041347-14j3i   0/1       Pending             0          0s
kubia-159041347-14j3i   0/1       Pending             0          0s
kubia-159041347-14j3i   0/1       ContainerCreating   0          1s
kubia-159041347-14j3i   0/1       Running             0          3s
kubia-159041347-14j3i   1/1       Running             0          5s
kubia-159041347-14j3i   1/1       Terminating         0          9s
kubia-159041347-14j3i   0/1       Terminating         0          17s
kubia-159041347-14j3i   0/1       Terminating         0          17s
kubia-159041347-14j3i   0/1       Terminating         0          17s
You can even have kubectl print out the whole YAML on each watch event like this:
$ kubectl get pods -o yaml --watch
The watch mechanism is also used by the Scheduler, which is the next Control Plane
component you’re going to learn more about.
11.1.5 Understanding the Scheduler
You’ve already learned that you don’t usually specify which cluster node a pod should
run on. This is left to the Scheduler. From afar, the operation of the Scheduler looks
simple. All it does is wait for newly created pods through the API server’s watch mech-
anism and assign a node to each new pod that doesn’t already have the node set. 
 The Scheduler doesn’t instruct the selected node (or the Kubelet running on that
node) to run the pod. All the Scheduler does is update the pod definition through the
API server. The API server then notifies the Kubelet (again, through the watch mech-
anism described previously) that the pod has been scheduled. As soon as the Kubelet
on the target node sees the pod has been scheduled to its node, it creates and runs the
pod’s containers.
 Although a coarse-grained view of the scheduling process seems trivial, the actual
task of selecting the best node for the pod isn’t that simple. Sure, the simplest
Scheduler could pick a random node and not care about the pods already running on
that node. On the other side of the spectrum, the Scheduler could use advanced tech-
niques such as machine learning to anticipate what kind of pods are about to be
scheduled in the next minutes or hours and schedule pods to maximize future hard-
ware utilization without requiring any rescheduling of existing pods. Kubernetes’
default Scheduler falls somewhere in between. 
UNDERSTANDING THE DEFAULT SCHEDULING ALGORITHM
The selection of a node can be broken down into two parts, as shown in figure 11.5:
Filtering the list of all nodes to obtain a list of acceptable nodes the pod can be
scheduled to.
Prioritizing the acceptable nodes and choosing the best one. If multiple nodes
have the highest score, round-robin is used to ensure pods are deployed across
all of them evenly.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="352">
  <data key="d0">Page_352</data>
  <data key="d5">Page_352</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_263">
  <data key="d0">320
CHAPTER 11
Understanding Kubernetes internals
FINDING ACCEPTABLE NODES
To determine which nodes are acceptable for the pod, the Scheduler passes each
node through a list of configured predicate functions. These check various things
such as
Can the node fulfill the pod’s requests for hardware resources? You’ll learn how
to specify them in chapter 14.
Is the node running out of resources (is it reporting a memory or a disk pres-
sure condition)? 
If the pod requests to be scheduled to a specific node (by name), is this the node?
Does the node have a label that matches the node selector in the pod specifica-
tion (if one is defined)?
If the pod requests to be bound to a specific host port (discussed in chapter 13),
is that port already taken on this node or not? 
If the pod requests a certain type of volume, can this volume be mounted for
this pod on this node, or is another pod on the node already using the same
volume?
Does the pod tolerate the taints of the node? Taints and tolerations are explained
in chapter 16.
Does the pod specify node and/or pod affinity or anti-affinity rules? If yes,
would scheduling the pod to this node break those rules? This is also explained
in chapter 16.
All these checks must pass for the node to be eligible to host the pod. After perform-
ing these checks on every node, the Scheduler ends up with a subset of the nodes. Any
of these nodes could run the pod, because they have enough available resources for
the pod and conform to all requirements you’ve specified in the pod definition.
SELECTING THE BEST NODE FOR THE POD
Even though all these nodes are acceptable and can run the pod, several may be a
better choice than others. Suppose you have a two-node cluster. Both nodes are eli-
gible, but one is already running 10 pods, while the other, for whatever reason, isn’t
running any pods right now. It’s obvious the Scheduler should favor the second
node in this case. 
Node 1
Node 2
Node 3
Node 4
Node 5
...
Find acceptable
nodes
Node 1
Node 2
Node 3
Node 4
Node 5
...
Prioritize nodes
and select the
top one
Node 3
Node 1
Node 4
Figure 11.5
The Scheduler finds acceptable nodes for a pod and then selects the best node 
for the pod.
 
</data>
  <data key="d5">320
CHAPTER 11
Understanding Kubernetes internals
FINDING ACCEPTABLE NODES
To determine which nodes are acceptable for the pod, the Scheduler passes each
node through a list of configured predicate functions. These check various things
such as
Can the node fulfill the pod’s requests for hardware resources? You’ll learn how
to specify them in chapter 14.
Is the node running out of resources (is it reporting a memory or a disk pres-
sure condition)? 
If the pod requests to be scheduled to a specific node (by name), is this the node?
Does the node have a label that matches the node selector in the pod specifica-
tion (if one is defined)?
If the pod requests to be bound to a specific host port (discussed in chapter 13),
is that port already taken on this node or not? 
If the pod requests a certain type of volume, can this volume be mounted for
this pod on this node, or is another pod on the node already using the same
volume?
Does the pod tolerate the taints of the node? Taints and tolerations are explained
in chapter 16.
Does the pod specify node and/or pod affinity or anti-affinity rules? If yes,
would scheduling the pod to this node break those rules? This is also explained
in chapter 16.
All these checks must pass for the node to be eligible to host the pod. After perform-
ing these checks on every node, the Scheduler ends up with a subset of the nodes. Any
of these nodes could run the pod, because they have enough available resources for
the pod and conform to all requirements you’ve specified in the pod definition.
SELECTING THE BEST NODE FOR THE POD
Even though all these nodes are acceptable and can run the pod, several may be a
better choice than others. Suppose you have a two-node cluster. Both nodes are eli-
gible, but one is already running 10 pods, while the other, for whatever reason, isn’t
running any pods right now. It’s obvious the Scheduler should favor the second
node in this case. 
Node 1
Node 2
Node 3
Node 4
Node 5
...
Find acceptable
nodes
Node 1
Node 2
Node 3
Node 4
Node 5
...
Prioritize nodes
and select the
top one
Node 3
Node 1
Node 4
Figure 11.5
The Scheduler finds acceptable nodes for a pod and then selects the best node 
for the pod.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="353">
  <data key="d0">Page_353</data>
  <data key="d5">Page_353</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_264">
  <data key="d0">321
Understanding the architecture
 Or is it? If these two nodes are provided by the cloud infrastructure, it may be bet-
ter to schedule the pod to the first node and relinquish the second node back to the
cloud provider to save money. 
ADVANCED SCHEDULING OF PODS
Consider another example. Imagine having multiple replicas of a pod. Ideally, you’d
want them spread across as many nodes as possible instead of having them all sched-
uled to a single one. Failure of that node would cause the service backed by those
pods to become unavailable. But if the pods were spread across different nodes, a sin-
gle node failure would barely leave a dent in the service’s capacity. 
 Pods belonging to the same Service or ReplicaSet are spread across multiple nodes
by default. It’s not guaranteed that this is always the case. But you can force pods to be
spread around the cluster or kept close together by defining pod affinity and anti-
affinity rules, which are explained in chapter 16. 
 Even these two simple cases show how complex scheduling can be, because it
depends on a multitude of factors. Because of this, the Scheduler can either be config-
ured to suit your specific needs or infrastructure specifics, or it can even be replaced
with a custom implementation altogether. You could also run a Kubernetes cluster
without a Scheduler, but then you’d have to perform the scheduling manually.
USING MULTIPLE SCHEDULERS
Instead of running a single Scheduler in the cluster, you can run multiple Schedulers.
Then, for each pod, you specify the Scheduler that should schedule this particular
pod by setting the schedulerName property in the pod spec.
 Pods without this property set are scheduled using the default Scheduler, and so
are pods with schedulerName set to default-scheduler. All other pods are ignored by
the default Scheduler, so they need to be scheduled either manually or by another
Scheduler watching for such pods. 
 You can implement your own Schedulers and deploy them in the cluster, or you
can deploy an additional instance of Kubernetes’ Scheduler with different configura-
tion options.
11.1.6 Introducing the controllers running in the Controller Manager
As previously mentioned, the API server doesn’t do anything except store resources in
etcd and notify clients about the change. The Scheduler only assigns a node to the
pod, so you need other active components to make sure the actual state of the system
converges toward the desired state, as specified in the resources deployed through the
API server. This work is done by controllers running inside the Controller Manager. 
 The single Controller Manager process currently combines a multitude of control-
lers performing various reconciliation tasks. Eventually those controllers will be split
up into separate processes, enabling you to replace each one with a custom imple-
mentation if necessary. The list of these controllers includes the
Replication Manager (a controller for ReplicationController resources)
ReplicaSet, DaemonSet, and Job controllers
 
</data>
  <data key="d5">321
Understanding the architecture
 Or is it? If these two nodes are provided by the cloud infrastructure, it may be bet-
ter to schedule the pod to the first node and relinquish the second node back to the
cloud provider to save money. 
ADVANCED SCHEDULING OF PODS
Consider another example. Imagine having multiple replicas of a pod. Ideally, you’d
want them spread across as many nodes as possible instead of having them all sched-
uled to a single one. Failure of that node would cause the service backed by those
pods to become unavailable. But if the pods were spread across different nodes, a sin-
gle node failure would barely leave a dent in the service’s capacity. 
 Pods belonging to the same Service or ReplicaSet are spread across multiple nodes
by default. It’s not guaranteed that this is always the case. But you can force pods to be
spread around the cluster or kept close together by defining pod affinity and anti-
affinity rules, which are explained in chapter 16. 
 Even these two simple cases show how complex scheduling can be, because it
depends on a multitude of factors. Because of this, the Scheduler can either be config-
ured to suit your specific needs or infrastructure specifics, or it can even be replaced
with a custom implementation altogether. You could also run a Kubernetes cluster
without a Scheduler, but then you’d have to perform the scheduling manually.
USING MULTIPLE SCHEDULERS
Instead of running a single Scheduler in the cluster, you can run multiple Schedulers.
Then, for each pod, you specify the Scheduler that should schedule this particular
pod by setting the schedulerName property in the pod spec.
 Pods without this property set are scheduled using the default Scheduler, and so
are pods with schedulerName set to default-scheduler. All other pods are ignored by
the default Scheduler, so they need to be scheduled either manually or by another
Scheduler watching for such pods. 
 You can implement your own Schedulers and deploy them in the cluster, or you
can deploy an additional instance of Kubernetes’ Scheduler with different configura-
tion options.
11.1.6 Introducing the controllers running in the Controller Manager
As previously mentioned, the API server doesn’t do anything except store resources in
etcd and notify clients about the change. The Scheduler only assigns a node to the
pod, so you need other active components to make sure the actual state of the system
converges toward the desired state, as specified in the resources deployed through the
API server. This work is done by controllers running inside the Controller Manager. 
 The single Controller Manager process currently combines a multitude of control-
lers performing various reconciliation tasks. Eventually those controllers will be split
up into separate processes, enabling you to replace each one with a custom imple-
mentation if necessary. The list of these controllers includes the
Replication Manager (a controller for ReplicationController resources)
ReplicaSet, DaemonSet, and Job controllers
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="354">
  <data key="d0">Page_354</data>
  <data key="d5">Page_354</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_265">
  <data key="d0">322
CHAPTER 11
Understanding Kubernetes internals
Deployment controller
StatefulSet controller
Node controller
Service controller
Endpoints controller
Namespace controller
PersistentVolume controller
Others
What each of these controllers does should be evident from its name. From the list,
you can tell there’s a controller for almost every resource you can create. Resources
are descriptions of what should be running in the cluster, whereas the controllers are
the active Kubernetes components that perform actual work as a result of the deployed
resources.
UNDERSTANDING WHAT CONTROLLERS DO AND HOW THEY DO IT
Controllers do many different things, but they all watch the API server for changes to
resources (Deployments, Services, and so on) and perform operations for each change,
whether it’s a creation of a new object or an update or deletion of an existing object.
Most of the time, these operations include creating other resources or updating the
watched resources themselves (to update the object’s status, for example).
 In general, controllers run a reconciliation loop, which reconciles the actual state
with the desired state (specified in the resource’s spec section) and writes the new
actual state to the resource’s status section. Controllers use the watch mechanism to
be notified of changes, but because using watches doesn’t guarantee the controller
won’t miss an event, they also perform a re-list operation periodically to make sure
they haven’t missed anything.
 Controllers never talk to each other directly. They don’t even know any other con-
trollers exist. Each controller connects to the API server and, through the watch
mechanism described in section 11.1.3, asks to be notified when a change occurs in
the list of resources of any type the controller is responsible for. 
 We’ll briefly look at what each of the controllers does, but if you’d like an in-depth
view of what they do, I suggest you look at their source code directly. The sidebar
explains how to get started.
A few pointers on exploring the controllers’ source code
If you’re interested in seeing exactly how these controllers operate, I strongly encour-
age you to browse through their source code. To make it easier, here are a few tips:
The source code for the controllers is available at https:/
/github.com/kubernetes/
kubernetes/blob/master/pkg/controller.
Each controller usually has a constructor in which it creates an Informer, which is
basically a listener that gets called every time an API object gets updated. Usually,
 
</data>
  <data key="d5">322
CHAPTER 11
Understanding Kubernetes internals
Deployment controller
StatefulSet controller
Node controller
Service controller
Endpoints controller
Namespace controller
PersistentVolume controller
Others
What each of these controllers does should be evident from its name. From the list,
you can tell there’s a controller for almost every resource you can create. Resources
are descriptions of what should be running in the cluster, whereas the controllers are
the active Kubernetes components that perform actual work as a result of the deployed
resources.
UNDERSTANDING WHAT CONTROLLERS DO AND HOW THEY DO IT
Controllers do many different things, but they all watch the API server for changes to
resources (Deployments, Services, and so on) and perform operations for each change,
whether it’s a creation of a new object or an update or deletion of an existing object.
Most of the time, these operations include creating other resources or updating the
watched resources themselves (to update the object’s status, for example).
 In general, controllers run a reconciliation loop, which reconciles the actual state
with the desired state (specified in the resource’s spec section) and writes the new
actual state to the resource’s status section. Controllers use the watch mechanism to
be notified of changes, but because using watches doesn’t guarantee the controller
won’t miss an event, they also perform a re-list operation periodically to make sure
they haven’t missed anything.
 Controllers never talk to each other directly. They don’t even know any other con-
trollers exist. Each controller connects to the API server and, through the watch
mechanism described in section 11.1.3, asks to be notified when a change occurs in
the list of resources of any type the controller is responsible for. 
 We’ll briefly look at what each of the controllers does, but if you’d like an in-depth
view of what they do, I suggest you look at their source code directly. The sidebar
explains how to get started.
A few pointers on exploring the controllers’ source code
If you’re interested in seeing exactly how these controllers operate, I strongly encour-
age you to browse through their source code. To make it easier, here are a few tips:
The source code for the controllers is available at https:/
/github.com/kubernetes/
kubernetes/blob/master/pkg/controller.
Each controller usually has a constructor in which it creates an Informer, which is
basically a listener that gets called every time an API object gets updated. Usually,
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="355">
  <data key="d0">Page_355</data>
  <data key="d5">Page_355</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_266">
  <data key="d0">323
Understanding the architecture
THE REPLICATION MANAGER
The controller that makes ReplicationController resources come to life is called the
Replication Manager. We talked about how ReplicationControllers work in chapter 4.
It’s not the ReplicationControllers that do the actual work, but the Replication Man-
ager. Let’s quickly review what the controller does, because this will help you under-
stand the rest of the controllers.
 In chapter 4, we said that the operation of a ReplicationController could be
thought of as an infinite loop, where in each iteration, the controller finds the num-
ber of pods matching its pod selector and compares the number to the desired replica
count. 
 Now that you know how the API server can notify clients through the watch
mechanism, it’s clear that the controller doesn’t poll the pods in every iteration, but
is instead notified by the watch mechanism of each change that may affect the
desired replica count or the number of matched pods (see figure 11.6). Any such
changes trigger the controller to recheck the desired vs. actual replica count and act
accordingly.
 You already know that when too few pod instances are running, the Replication-
Controller runs additional instances. But it doesn’t actually run them itself. It creates
an Informer listens for changes to a specific type of resource. Looking at the con-
structor will show you which resources the controller is watching.
Next, go look for the worker() method. In it, you’ll find the method that gets invoked
each time the controller needs to do something. The actual function is often stored
in a field called syncHandler or something similar. This field is also initialized in the
constructor, so that’s where you’ll find the name of the function that gets called. That
function is the place where all the magic happens.
Controller Manager
Watches
Creates and
deletes
Replication
Manager
API server
ReplicationController
resources
Pod resources
Other resources
Figure 11.6
The Replication Manager watches for changes to API 
objects.
 
</data>
  <data key="d5">323
Understanding the architecture
THE REPLICATION MANAGER
The controller that makes ReplicationController resources come to life is called the
Replication Manager. We talked about how ReplicationControllers work in chapter 4.
It’s not the ReplicationControllers that do the actual work, but the Replication Man-
ager. Let’s quickly review what the controller does, because this will help you under-
stand the rest of the controllers.
 In chapter 4, we said that the operation of a ReplicationController could be
thought of as an infinite loop, where in each iteration, the controller finds the num-
ber of pods matching its pod selector and compares the number to the desired replica
count. 
 Now that you know how the API server can notify clients through the watch
mechanism, it’s clear that the controller doesn’t poll the pods in every iteration, but
is instead notified by the watch mechanism of each change that may affect the
desired replica count or the number of matched pods (see figure 11.6). Any such
changes trigger the controller to recheck the desired vs. actual replica count and act
accordingly.
 You already know that when too few pod instances are running, the Replication-
Controller runs additional instances. But it doesn’t actually run them itself. It creates
an Informer listens for changes to a specific type of resource. Looking at the con-
structor will show you which resources the controller is watching.
Next, go look for the worker() method. In it, you’ll find the method that gets invoked
each time the controller needs to do something. The actual function is often stored
in a field called syncHandler or something similar. This field is also initialized in the
constructor, so that’s where you’ll find the name of the function that gets called. That
function is the place where all the magic happens.
Controller Manager
Watches
Creates and
deletes
Replication
Manager
API server
ReplicationController
resources
Pod resources
Other resources
Figure 11.6
The Replication Manager watches for changes to API 
objects.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="356">
  <data key="d0">Page_356</data>
  <data key="d5">Page_356</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_267">
  <data key="d0">324
CHAPTER 11
Understanding Kubernetes internals
new Pod manifests, posts them to the API server, and lets the Scheduler and the
Kubelet do their job of scheduling and running the pod.
 The Replication Manager performs its work by manipulating Pod API objects
through the API server. This is how all controllers operate.
THE REPLICASET, THE DAEMONSET, AND THE JOB CONTROLLERS
The ReplicaSet controller does almost the same thing as the Replication Manager
described previously, so we don’t have much to add here. The DaemonSet and Job
controllers are similar. They create Pod resources from the pod template defined in
their respective resources. Like the Replication Manager, these controllers don’t run
the pods, but post Pod definitions to the API server, letting the Kubelet create their
containers and run them.
THE DEPLOYMENT CONTROLLER
The Deployment controller takes care of keeping the actual state of a deployment in
sync with the desired state specified in the corresponding Deployment API object. 
 The Deployment controller performs a rollout of a new version each time a
Deployment object is modified (if the modification should affect the deployed pods).
It does this by creating a ReplicaSet and then appropriately scaling both the old and
the new ReplicaSet based on the strategy specified in the Deployment, until all the old
pods have been replaced with new ones. It doesn’t create any pods directly.
THE STATEFULSET CONTROLLER
The StatefulSet controller, similarly to the ReplicaSet controller and other related
controllers, creates, manages, and deletes Pods according to the spec of a StatefulSet
resource. But while those other controllers only manage Pods, the StatefulSet control-
ler also instantiates and manages PersistentVolumeClaims for each Pod instance.
THE NODE CONTROLLER
The Node controller manages the Node resources, which describe the cluster’s worker
nodes. Among other things, a Node controller keeps the list of Node objects in sync
with the actual list of machines running in the cluster. It also monitors each node’s
health and evicts pods from unreachable nodes.
 The Node controller isn’t the only component making changes to Node objects.
They’re also changed by the Kubelet, and can obviously also be modified by users
through REST API calls. 
THE SERVICE CONTROLLER
In chapter 5, when we talked about Services, you learned that a few different types
exist. One of them was the LoadBalancer service, which requests a load balancer from
the infrastructure to make the service available externally. The Service controller is
the one requesting and releasing a load balancer from the infrastructure, when a
LoadBalancer-type Service is created or deleted.
 
</data>
  <data key="d5">324
CHAPTER 11
Understanding Kubernetes internals
new Pod manifests, posts them to the API server, and lets the Scheduler and the
Kubelet do their job of scheduling and running the pod.
 The Replication Manager performs its work by manipulating Pod API objects
through the API server. This is how all controllers operate.
THE REPLICASET, THE DAEMONSET, AND THE JOB CONTROLLERS
The ReplicaSet controller does almost the same thing as the Replication Manager
described previously, so we don’t have much to add here. The DaemonSet and Job
controllers are similar. They create Pod resources from the pod template defined in
their respective resources. Like the Replication Manager, these controllers don’t run
the pods, but post Pod definitions to the API server, letting the Kubelet create their
containers and run them.
THE DEPLOYMENT CONTROLLER
The Deployment controller takes care of keeping the actual state of a deployment in
sync with the desired state specified in the corresponding Deployment API object. 
 The Deployment controller performs a rollout of a new version each time a
Deployment object is modified (if the modification should affect the deployed pods).
It does this by creating a ReplicaSet and then appropriately scaling both the old and
the new ReplicaSet based on the strategy specified in the Deployment, until all the old
pods have been replaced with new ones. It doesn’t create any pods directly.
THE STATEFULSET CONTROLLER
The StatefulSet controller, similarly to the ReplicaSet controller and other related
controllers, creates, manages, and deletes Pods according to the spec of a StatefulSet
resource. But while those other controllers only manage Pods, the StatefulSet control-
ler also instantiates and manages PersistentVolumeClaims for each Pod instance.
THE NODE CONTROLLER
The Node controller manages the Node resources, which describe the cluster’s worker
nodes. Among other things, a Node controller keeps the list of Node objects in sync
with the actual list of machines running in the cluster. It also monitors each node’s
health and evicts pods from unreachable nodes.
 The Node controller isn’t the only component making changes to Node objects.
They’re also changed by the Kubelet, and can obviously also be modified by users
through REST API calls. 
THE SERVICE CONTROLLER
In chapter 5, when we talked about Services, you learned that a few different types
exist. One of them was the LoadBalancer service, which requests a load balancer from
the infrastructure to make the service available externally. The Service controller is
the one requesting and releasing a load balancer from the infrastructure, when a
LoadBalancer-type Service is created or deleted.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="357">
  <data key="d0">Page_357</data>
  <data key="d5">Page_357</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_268">
  <data key="d0">325
Understanding the architecture
THE ENDPOINTS CONTROLLER
You’ll remember that Services aren’t linked directly to pods, but instead contain a list
of endpoints (IPs and ports), which is created and updated either manually or auto-
matically according to the pod selector defined on the Service. The Endpoints con-
troller is the active component that keeps the endpoint list constantly updated with
the IPs and ports of pods matching the label selector.
 As figure 11.7 shows, the controller watches both Services and Pods. When
Services are added or updated or Pods are added, updated, or deleted, it selects Pods
matching the Service’s pod selector and adds their IPs and ports to the Endpoints
resource. Remember, the Endpoints object is a standalone object, so the controller
creates it if necessary. Likewise, it also deletes the Endpoints object when the Service is
deleted.
THE NAMESPACE CONTROLLER
Remember namespaces (we talked about them in chapter 3)? Most resources belong
to a specific namespace. When a Namespace resource is deleted, all the resources in
that namespace must also be deleted. This is what the Namespace controller does.
When it’s notified of the deletion of a Namespace object, it deletes all the resources
belonging to the namespace through the API server. 
THE PERSISTENTVOLUME CONTROLLER
In chapter 6 you learned about PersistentVolumes and PersistentVolumeClaims.
Once a user creates a PersistentVolumeClaim, Kubernetes must find an appropriate
PersistentVolume and bind it to the claim. This is performed by the PersistentVolume
controller. 
 When a PersistentVolumeClaim pops up, the controller finds the best match for
the claim by selecting the smallest PersistentVolume with the access mode matching
the one requested in the claim and the declared capacity above the capacity requested
Controller Manager
Watches
Creates, modiﬁes,
and deletes
Endpoints
controller
API server
Service resources
Pod resources
Endpoints resources
Figure 11.7
The Endpoints controller watches Service and Pod resources, 
and manages Endpoints.
 
</data>
  <data key="d5">325
Understanding the architecture
THE ENDPOINTS CONTROLLER
You’ll remember that Services aren’t linked directly to pods, but instead contain a list
of endpoints (IPs and ports), which is created and updated either manually or auto-
matically according to the pod selector defined on the Service. The Endpoints con-
troller is the active component that keeps the endpoint list constantly updated with
the IPs and ports of pods matching the label selector.
 As figure 11.7 shows, the controller watches both Services and Pods. When
Services are added or updated or Pods are added, updated, or deleted, it selects Pods
matching the Service’s pod selector and adds their IPs and ports to the Endpoints
resource. Remember, the Endpoints object is a standalone object, so the controller
creates it if necessary. Likewise, it also deletes the Endpoints object when the Service is
deleted.
THE NAMESPACE CONTROLLER
Remember namespaces (we talked about them in chapter 3)? Most resources belong
to a specific namespace. When a Namespace resource is deleted, all the resources in
that namespace must also be deleted. This is what the Namespace controller does.
When it’s notified of the deletion of a Namespace object, it deletes all the resources
belonging to the namespace through the API server. 
THE PERSISTENTVOLUME CONTROLLER
In chapter 6 you learned about PersistentVolumes and PersistentVolumeClaims.
Once a user creates a PersistentVolumeClaim, Kubernetes must find an appropriate
PersistentVolume and bind it to the claim. This is performed by the PersistentVolume
controller. 
 When a PersistentVolumeClaim pops up, the controller finds the best match for
the claim by selecting the smallest PersistentVolume with the access mode matching
the one requested in the claim and the declared capacity above the capacity requested
Controller Manager
Watches
Creates, modiﬁes,
and deletes
Endpoints
controller
API server
Service resources
Pod resources
Endpoints resources
Figure 11.7
The Endpoints controller watches Service and Pod resources, 
and manages Endpoints.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="358">
  <data key="d0">Page_358</data>
  <data key="d5">Page_358</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_269">
  <data key="d0">326
CHAPTER 11
Understanding Kubernetes internals
in the claim. It does this by keeping an ordered list of PersistentVolumes for each
access mode by ascending capacity and returning the first volume from the list.
 Then, when the user deletes the PersistentVolumeClaim, the volume is unbound
and reclaimed according to the volume’s reclaim policy (left as is, deleted, or emptied).
CONTROLLER WRAP-UP
You should now have a good feel for what each controller does and how controllers
work in general. Again, all these controllers operate on the API objects through the
API server. They don’t communicate with the Kubelets directly or issue any kind of
instructions to them. In fact, they don’t even know Kubelets exist. After a controller
updates a resource in the API server, the Kubelets and Kubernetes Service Proxies,
also oblivious of the controllers’ existence, perform their work, such as spinning up a
pod’s containers and attaching network storage to them, or in the case of services, set-
ting up the actual load balancing across pods. 
 The Control Plane handles one part of the operation of the whole system, so to
fully understand how things unfold in a Kubernetes cluster, you also need to under-
stand what the Kubelet and the Kubernetes Service Proxy do. We’ll learn that next.
11.1.7 What the Kubelet does
In contrast to all the controllers, which are part of the Kubernetes Control Plane and
run on the master node(s), the Kubelet and the Service Proxy both run on the worker
nodes, where the actual pods containers run. What does the Kubelet do exactly?
UNDERSTANDING THE KUBELET’S JOB
In a nutshell, the Kubelet is the component responsible for everything running on a
worker node. Its initial job is to register the node it’s running on by creating a Node
resource in the API server. Then it needs to continuously monitor the API server for
Pods that have been scheduled to the node, and start the pod’s containers. It does this
by telling the configured container runtime (which is Docker, CoreOS’ rkt, or some-
thing else) to run a container from a specific container image. The Kubelet then con-
stantly monitors running containers and reports their status, events, and resource
consumption to the API server. 
 The Kubelet is also the component that runs the container liveness probes, restart-
ing containers when the probes fail. Lastly, it terminates containers when their Pod is
deleted from the API server and notifies the server that the pod has terminated.
RUNNING STATIC PODS WITHOUT THE API SERVER
Although the Kubelet talks to the Kubernetes API server and gets the pod manifests
from there, it can also run pods based on pod manifest files in a specific local direc-
tory as shown in figure 11.8. This feature is used to run the containerized versions of
the Control Plane components as pods, as you saw in the beginning of the chapter.
 Instead of running Kubernetes system components natively, you can put their pod
manifests into the Kubelet’s manifest directory and have the Kubelet run and manage
 
</data>
  <data key="d5">326
CHAPTER 11
Understanding Kubernetes internals
in the claim. It does this by keeping an ordered list of PersistentVolumes for each
access mode by ascending capacity and returning the first volume from the list.
 Then, when the user deletes the PersistentVolumeClaim, the volume is unbound
and reclaimed according to the volume’s reclaim policy (left as is, deleted, or emptied).
CONTROLLER WRAP-UP
You should now have a good feel for what each controller does and how controllers
work in general. Again, all these controllers operate on the API objects through the
API server. They don’t communicate with the Kubelets directly or issue any kind of
instructions to them. In fact, they don’t even know Kubelets exist. After a controller
updates a resource in the API server, the Kubelets and Kubernetes Service Proxies,
also oblivious of the controllers’ existence, perform their work, such as spinning up a
pod’s containers and attaching network storage to them, or in the case of services, set-
ting up the actual load balancing across pods. 
 The Control Plane handles one part of the operation of the whole system, so to
fully understand how things unfold in a Kubernetes cluster, you also need to under-
stand what the Kubelet and the Kubernetes Service Proxy do. We’ll learn that next.
11.1.7 What the Kubelet does
In contrast to all the controllers, which are part of the Kubernetes Control Plane and
run on the master node(s), the Kubelet and the Service Proxy both run on the worker
nodes, where the actual pods containers run. What does the Kubelet do exactly?
UNDERSTANDING THE KUBELET’S JOB
In a nutshell, the Kubelet is the component responsible for everything running on a
worker node. Its initial job is to register the node it’s running on by creating a Node
resource in the API server. Then it needs to continuously monitor the API server for
Pods that have been scheduled to the node, and start the pod’s containers. It does this
by telling the configured container runtime (which is Docker, CoreOS’ rkt, or some-
thing else) to run a container from a specific container image. The Kubelet then con-
stantly monitors running containers and reports their status, events, and resource
consumption to the API server. 
 The Kubelet is also the component that runs the container liveness probes, restart-
ing containers when the probes fail. Lastly, it terminates containers when their Pod is
deleted from the API server and notifies the server that the pod has terminated.
RUNNING STATIC PODS WITHOUT THE API SERVER
Although the Kubelet talks to the Kubernetes API server and gets the pod manifests
from there, it can also run pods based on pod manifest files in a specific local direc-
tory as shown in figure 11.8. This feature is used to run the containerized versions of
the Control Plane components as pods, as you saw in the beginning of the chapter.
 Instead of running Kubernetes system components natively, you can put their pod
manifests into the Kubelet’s manifest directory and have the Kubelet run and manage
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="359">
  <data key="d0">Page_359</data>
  <data key="d5">Page_359</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_270">
  <data key="d0">327
Understanding the architecture
them. You can also use the same method to run your custom system containers, but
doing it through a DaemonSet is the recommended method.
11.1.8 The role of the Kubernetes Service Proxy
Beside the Kubelet, every worker node also runs the kube-proxy, whose purpose is to
make sure clients can connect to the services you define through the Kubernetes API.
The kube-proxy makes sure connections to the service IP and port end up at one of
the pods backing that service (or other, non-pod service endpoints). When a service is
backed by more than one pod, the proxy performs load balancing across those pods. 
WHY IT’S CALLED A PROXY
The initial implementation of the kube-proxy was the userspace proxy. It used an
actual server process to accept connections and proxy them to the pods. To inter-
cept connections destined to the service IPs, the proxy configured iptables rules
(iptables is the tool for managing the Linux kernel’s packet filtering features) to
redirect the connections to the proxy server. A rough diagram of the userspace proxy
mode is shown in figure 11.9.
Container Runtime
(Docker, rkt, ...)
Kubelet
API server
Worker node
Runs, monitors,
and manages
containers
Pod resource
Container A
Container B
Container A
Container B
Container C
Pod manifest (ﬁle)
Local manifest directory
Container C
Figure 11.8
The Kubelet runs pods based on pod specs from the API server and a local file directory.
Client
kube-proxy
Conﬁgures
:
iptables
redirect through proxy server
iptables
Pod
Figure 11.9
The userspace proxy mode
 
</data>
  <data key="d5">327
Understanding the architecture
them. You can also use the same method to run your custom system containers, but
doing it through a DaemonSet is the recommended method.
11.1.8 The role of the Kubernetes Service Proxy
Beside the Kubelet, every worker node also runs the kube-proxy, whose purpose is to
make sure clients can connect to the services you define through the Kubernetes API.
The kube-proxy makes sure connections to the service IP and port end up at one of
the pods backing that service (or other, non-pod service endpoints). When a service is
backed by more than one pod, the proxy performs load balancing across those pods. 
WHY IT’S CALLED A PROXY
The initial implementation of the kube-proxy was the userspace proxy. It used an
actual server process to accept connections and proxy them to the pods. To inter-
cept connections destined to the service IPs, the proxy configured iptables rules
(iptables is the tool for managing the Linux kernel’s packet filtering features) to
redirect the connections to the proxy server. A rough diagram of the userspace proxy
mode is shown in figure 11.9.
Container Runtime
(Docker, rkt, ...)
Kubelet
API server
Worker node
Runs, monitors,
and manages
containers
Pod resource
Container A
Container B
Container A
Container B
Container C
Pod manifest (ﬁle)
Local manifest directory
Container C
Figure 11.8
The Kubelet runs pods based on pod specs from the API server and a local file directory.
Client
kube-proxy
Conﬁgures
:
iptables
redirect through proxy server
iptables
Pod
Figure 11.9
The userspace proxy mode
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="360">
  <data key="d0">Page_360</data>
  <data key="d5">Page_360</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_271">
  <data key="d0">328
CHAPTER 11
Understanding Kubernetes internals
The kube-proxy got its name because it was an actual proxy, but the current, much
better performing implementation only uses iptables rules to redirect packets to a
randomly selected backend pod without passing them through an actual proxy server.
This mode is called the iptables proxy mode and is shown in figure 11.10.
The major difference between these two modes is whether packets pass through the
kube-proxy and must be handled in user space, or whether they’re handled only by
the Kernel (in kernel space). This has a major impact on performance. 
 Another smaller difference is that the userspace proxy mode balanced connec-
tions across pods in a true round-robin fashion, while the iptables proxy mode
doesn’t—it selects pods randomly. When only a few clients use a service, they may not
be spread evenly across pods. For example, if a service has two backing pods but only
five or so clients, don’t be surprised if you see four clients connect to pod A and only
one client connect to pod B. With a higher number of clients or pods, this problem
isn’t so apparent.
 You’ll learn exactly how iptables proxy mode works in section 11.5. 
11.1.9 Introducing Kubernetes add-ons
We’ve now discussed the core components that make a Kubernetes cluster work. But
in the beginning of the chapter, we also listed a few add-ons, which although not
always required, enable features such as DNS lookup of Kubernetes services, exposing
multiple HTTP services through a single external IP address, the Kubernetes web
dashboard, and so on.
HOW ADD-ONS ARE DEPLOYED
These components are available as add-ons and are deployed as pods by submitting
YAML manifests to the API server, the way you’ve been doing throughout the book.
Some of these components are deployed through a Deployment resource or a Repli-
cationController resource, and some through a DaemonSet. 
 For example, as I’m writing this, in Minikube, the Ingress controller and the
dashboard add-ons are deployed as ReplicationControllers, as shown in the follow-
ing listing.
 
Client
Conﬁgures
:
iptables
redirect straight to pod
(no proxy server in-between)
iptables
Pod
kube-proxy
Figure 11.10
The iptables proxy mode
 
</data>
  <data key="d5">328
CHAPTER 11
Understanding Kubernetes internals
The kube-proxy got its name because it was an actual proxy, but the current, much
better performing implementation only uses iptables rules to redirect packets to a
randomly selected backend pod without passing them through an actual proxy server.
This mode is called the iptables proxy mode and is shown in figure 11.10.
The major difference between these two modes is whether packets pass through the
kube-proxy and must be handled in user space, or whether they’re handled only by
the Kernel (in kernel space). This has a major impact on performance. 
 Another smaller difference is that the userspace proxy mode balanced connec-
tions across pods in a true round-robin fashion, while the iptables proxy mode
doesn’t—it selects pods randomly. When only a few clients use a service, they may not
be spread evenly across pods. For example, if a service has two backing pods but only
five or so clients, don’t be surprised if you see four clients connect to pod A and only
one client connect to pod B. With a higher number of clients or pods, this problem
isn’t so apparent.
 You’ll learn exactly how iptables proxy mode works in section 11.5. 
11.1.9 Introducing Kubernetes add-ons
We’ve now discussed the core components that make a Kubernetes cluster work. But
in the beginning of the chapter, we also listed a few add-ons, which although not
always required, enable features such as DNS lookup of Kubernetes services, exposing
multiple HTTP services through a single external IP address, the Kubernetes web
dashboard, and so on.
HOW ADD-ONS ARE DEPLOYED
These components are available as add-ons and are deployed as pods by submitting
YAML manifests to the API server, the way you’ve been doing throughout the book.
Some of these components are deployed through a Deployment resource or a Repli-
cationController resource, and some through a DaemonSet. 
 For example, as I’m writing this, in Minikube, the Ingress controller and the
dashboard add-ons are deployed as ReplicationControllers, as shown in the follow-
ing listing.
 
Client
Conﬁgures
:
iptables
redirect straight to pod
(no proxy server in-between)
iptables
Pod
kube-proxy
Figure 11.10
The iptables proxy mode
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="361">
  <data key="d0">Page_361</data>
  <data key="d5">Page_361</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_272">
  <data key="d0">329
Understanding the architecture
$ kubectl get rc -n kube-system
NAME                       DESIRED   CURRENT   READY     AGE
default-http-backend       1         1         1         6d
kubernetes-dashboard       1         1         1         6d
nginx-ingress-controller   1         1         1         6d
The DNS add-on is deployed as a Deployment, as shown in the following listing.
$ kubectl get deploy -n kube-system
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-dns   1         1         1            1           6d
Let’s see how DNS and the Ingress controllers work.
HOW THE DNS SERVER WORKS
All the pods in the cluster are configured to use the cluster’s internal DNS server by
default. This allows pods to easily look up services by name or even the pod’s IP
addresses in the case of headless services.
 The DNS server pod is exposed through the kube-dns service, allowing the pod to
be moved around the cluster, like any other pod. The service’s IP address is specified
as the nameserver in the /etc/resolv.conf file inside every container deployed in the
cluster. The kube-dns pod uses the API server’s watch mechanism to observe changes
to Services and Endpoints and updates its DNS records with every change, allowing its
clients to always get (fairly) up-to-date DNS information. I say fairly because during
the time between the update of the Service or Endpoints resource and the time the
DNS pod receives the watch notification, the DNS records may be invalid.
HOW (MOST) INGRESS CONTROLLERS WORK
Unlike the DNS add-on, you’ll find a few different implementations of Ingress con-
trollers, but most of them work in the same way. An Ingress controller runs a reverse
proxy server (like Nginx, for example), and keeps it configured according to the
Ingress, Service, and Endpoints resources defined in the cluster. The controller thus
needs to observe those resources (again, through the watch mechanism) and change
the proxy server’s config every time one of them changes. 
 Although the Ingress resource’s definition points to a Service, Ingress controllers
forward traffic to the service’s pod directly instead of going through the service IP.
This affects the preservation of client IPs when external clients connect through the
Ingress controller, which makes them preferred over Services in certain use cases.
USING OTHER ADD-ONS
You’ve seen how both the DNS server and the Ingress controller add-ons are similar to
the controllers running in the Controller Manager, except that they also accept client
connections instead of only observing and modifying resources through the API server. 
Listing 11.7
Add-ons deployed with ReplicationControllers in Minikube
Listing 11.8
The kube-dns Deployment 
 
</data>
  <data key="d5">329
Understanding the architecture
$ kubectl get rc -n kube-system
NAME                       DESIRED   CURRENT   READY     AGE
default-http-backend       1         1         1         6d
kubernetes-dashboard       1         1         1         6d
nginx-ingress-controller   1         1         1         6d
The DNS add-on is deployed as a Deployment, as shown in the following listing.
$ kubectl get deploy -n kube-system
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-dns   1         1         1            1           6d
Let’s see how DNS and the Ingress controllers work.
HOW THE DNS SERVER WORKS
All the pods in the cluster are configured to use the cluster’s internal DNS server by
default. This allows pods to easily look up services by name or even the pod’s IP
addresses in the case of headless services.
 The DNS server pod is exposed through the kube-dns service, allowing the pod to
be moved around the cluster, like any other pod. The service’s IP address is specified
as the nameserver in the /etc/resolv.conf file inside every container deployed in the
cluster. The kube-dns pod uses the API server’s watch mechanism to observe changes
to Services and Endpoints and updates its DNS records with every change, allowing its
clients to always get (fairly) up-to-date DNS information. I say fairly because during
the time between the update of the Service or Endpoints resource and the time the
DNS pod receives the watch notification, the DNS records may be invalid.
HOW (MOST) INGRESS CONTROLLERS WORK
Unlike the DNS add-on, you’ll find a few different implementations of Ingress con-
trollers, but most of them work in the same way. An Ingress controller runs a reverse
proxy server (like Nginx, for example), and keeps it configured according to the
Ingress, Service, and Endpoints resources defined in the cluster. The controller thus
needs to observe those resources (again, through the watch mechanism) and change
the proxy server’s config every time one of them changes. 
 Although the Ingress resource’s definition points to a Service, Ingress controllers
forward traffic to the service’s pod directly instead of going through the service IP.
This affects the preservation of client IPs when external clients connect through the
Ingress controller, which makes them preferred over Services in certain use cases.
USING OTHER ADD-ONS
You’ve seen how both the DNS server and the Ingress controller add-ons are similar to
the controllers running in the Controller Manager, except that they also accept client
connections instead of only observing and modifying resources through the API server. 
Listing 11.7
Add-ons deployed with ReplicationControllers in Minikube
Listing 11.8
The kube-dns Deployment 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="362">
  <data key="d0">Page_362</data>
  <data key="d5">Page_362</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_273">
  <data key="d0">330
CHAPTER 11
Understanding Kubernetes internals
 Other add-ons are similar. They all need to observe the cluster state and perform
the necessary actions when that changes. We’ll introduce a few other add-ons in this
and the remaining chapters.
11.1.10Bringing it all together
You’ve now learned that the whole Kubernetes system is composed of relatively small,
loosely coupled components with good separation of concerns. The API server, the
Scheduler, the individual controllers running inside the Controller Manager, the
Kubelet, and the kube-proxy all work together to keep the actual state of the system
synchronized with what you specify as the desired state. 
 For example, submitting a pod manifest to the API server triggers a coordinated
dance of various Kubernetes components, which eventually results in the pod’s con-
tainers running. You’ll learn how this dance unfolds in the next section. 
11.2
How controllers cooperate
You now know about all the components that a Kubernetes cluster is comprised of.
Now, to solidify your understanding of how Kubernetes works, let’s go over what hap-
pens when a Pod resource is created. Because you normally don’t create Pods directly,
you’re going to create a Deployment resource instead and see everything that must
happen for the pod’s containers to be started.
11.2.1 Understanding which components are involved
Even before you start the whole process, the controllers, the Scheduler, and the
Kubelet are watching the API server for changes to their respective resource types.
This is shown in figure 11.11. The components depicted in the figure will each play a
part in the process you’re about to trigger. The diagram doesn’t include etcd, because
it’s hidden behind the API server, and you can think of the API server as the place
where objects are stored.
Master node
Controller Manager
Watches
Deployment
controller
Scheduler
ReplicaSet
controller
API server
Deployments
Pods
ReplicaSets
Watches
Watches
Node X
Watches
Docker
Kubelet
Figure 11.11
Kubernetes components watching API objects through the API server
 
</data>
  <data key="d5">330
CHAPTER 11
Understanding Kubernetes internals
 Other add-ons are similar. They all need to observe the cluster state and perform
the necessary actions when that changes. We’ll introduce a few other add-ons in this
and the remaining chapters.
11.1.10Bringing it all together
You’ve now learned that the whole Kubernetes system is composed of relatively small,
loosely coupled components with good separation of concerns. The API server, the
Scheduler, the individual controllers running inside the Controller Manager, the
Kubelet, and the kube-proxy all work together to keep the actual state of the system
synchronized with what you specify as the desired state. 
 For example, submitting a pod manifest to the API server triggers a coordinated
dance of various Kubernetes components, which eventually results in the pod’s con-
tainers running. You’ll learn how this dance unfolds in the next section. 
11.2
How controllers cooperate
You now know about all the components that a Kubernetes cluster is comprised of.
Now, to solidify your understanding of how Kubernetes works, let’s go over what hap-
pens when a Pod resource is created. Because you normally don’t create Pods directly,
you’re going to create a Deployment resource instead and see everything that must
happen for the pod’s containers to be started.
11.2.1 Understanding which components are involved
Even before you start the whole process, the controllers, the Scheduler, and the
Kubelet are watching the API server for changes to their respective resource types.
This is shown in figure 11.11. The components depicted in the figure will each play a
part in the process you’re about to trigger. The diagram doesn’t include etcd, because
it’s hidden behind the API server, and you can think of the API server as the place
where objects are stored.
Master node
Controller Manager
Watches
Deployment
controller
Scheduler
ReplicaSet
controller
API server
Deployments
Pods
ReplicaSets
Watches
Watches
Node X
Watches
Docker
Kubelet
Figure 11.11
Kubernetes components watching API objects through the API server
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="363">
  <data key="d0">Page_363</data>
  <data key="d5">Page_363</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_274">
  <data key="d0">331
How controllers cooperate
11.2.2 The chain of events
Imagine you prepared the YAML file containing the Deployment manifest and you’re
about to submit it to Kubernetes through kubectl. kubectl sends the manifest to the
Kubernetes API server in an HTTP POST request. The API server validates the Deploy-
ment specification, stores it in etcd, and returns a response to kubectl. Now a chain
of events starts to unfold, as shown in figure 11.12.
THE DEPLOYMENT CONTROLLER CREATES THE REPLICASET
All API server clients watching the list of Deployments through the API server’s watch
mechanism are notified of the newly created Deployment resource immediately after
it’s created. One of those clients is the Deployment controller, which, as we discussed
earlier, is the active component responsible for handling Deployments. 
 As you may remember from chapter 9, a Deployment is backed by one or more
ReplicaSets, which then create the actual pods. As a new Deployment object is
detected by the Deployment controller, it creates a ReplicaSet for the current speci-
fication of the Deployment. This involves creating a new ReplicaSet resource
through the Kubernetes API. The Deployment controller doesn’t deal with individ-
ual pods at all.
Master node
Controller
Manager
2. Notiﬁcation
through watch
3. Creates
ReplicaSet
4. Notiﬁcation
5. Creates pod
6. Notiﬁcation
through watch
7. Assigns pod to node
1. Creates Deployment
resource
Deployment
controller
Scheduler
kubectl
ReplicaSet
controller
API server
Deployment A
Deployments
ReplicaSets
Pod A
Pods
ReplicaSet A
Node X
8. Notiﬁcation
through watch
9. Tells Docker to
run containers
Docker
10. Runs
containers
Container(s)
Kubelet
Figure 11.12
The chain of events that unfolds when a Deployment resource is posted to the API server
 
</data>
  <data key="d5">331
How controllers cooperate
11.2.2 The chain of events
Imagine you prepared the YAML file containing the Deployment manifest and you’re
about to submit it to Kubernetes through kubectl. kubectl sends the manifest to the
Kubernetes API server in an HTTP POST request. The API server validates the Deploy-
ment specification, stores it in etcd, and returns a response to kubectl. Now a chain
of events starts to unfold, as shown in figure 11.12.
THE DEPLOYMENT CONTROLLER CREATES THE REPLICASET
All API server clients watching the list of Deployments through the API server’s watch
mechanism are notified of the newly created Deployment resource immediately after
it’s created. One of those clients is the Deployment controller, which, as we discussed
earlier, is the active component responsible for handling Deployments. 
 As you may remember from chapter 9, a Deployment is backed by one or more
ReplicaSets, which then create the actual pods. As a new Deployment object is
detected by the Deployment controller, it creates a ReplicaSet for the current speci-
fication of the Deployment. This involves creating a new ReplicaSet resource
through the Kubernetes API. The Deployment controller doesn’t deal with individ-
ual pods at all.
Master node
Controller
Manager
2. Notiﬁcation
through watch
3. Creates
ReplicaSet
4. Notiﬁcation
5. Creates pod
6. Notiﬁcation
through watch
7. Assigns pod to node
1. Creates Deployment
resource
Deployment
controller
Scheduler
kubectl
ReplicaSet
controller
API server
Deployment A
Deployments
ReplicaSets
Pod A
Pods
ReplicaSet A
Node X
8. Notiﬁcation
through watch
9. Tells Docker to
run containers
Docker
10. Runs
containers
Container(s)
Kubelet
Figure 11.12
The chain of events that unfolds when a Deployment resource is posted to the API server
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="364">
  <data key="d0">Page_364</data>
  <data key="d5">Page_364</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_275">
  <data key="d0">332
CHAPTER 11
Understanding Kubernetes internals
THE REPLICASET CONTROLLER CREATES THE POD RESOURCES
The newly created ReplicaSet is then picked up by the ReplicaSet controller, which
watches for creations, modifications, and deletions of ReplicaSet resources in the
API server. The controller takes into consideration the replica count and pod selec-
tor defined in the ReplicaSet and verifies whether enough existing Pods match
the selector.
 The controller then creates the Pod resources based on the pod template in the
ReplicaSet (the pod template was copied over from the Deployment when the Deploy-
ment controller created the ReplicaSet). 
THE SCHEDULER ASSIGNS A NODE TO THE NEWLY CREATED PODS
These newly created Pods are now stored in etcd, but they each still lack one import-
ant thing—they don’t have an associated node yet. Their nodeName attribute isn’t set.
The Scheduler watches for Pods like this, and when it encounters one, chooses the
best node for the Pod and assigns the Pod to the node. The Pod’s definition now
includes the name of the node it should be running on.
 Everything so far has been happening in the Kubernetes Control Plane. None of
the controllers that have taken part in this whole process have done anything tangible
except update the resources through the API server. 
THE KUBELET RUNS THE POD’S CONTAINERS
The worker nodes haven’t done anything up to this point. The pod’s containers
haven’t been started yet. The images for the pod’s containers haven’t even been down-
loaded yet. 
 But with the Pod now scheduled to a specific node, the Kubelet on that node can
finally get to work. The Kubelet, watching for changes to Pods on the API server, sees a
new Pod scheduled to its node, so it inspects the Pod definition and instructs Docker,
or whatever container runtime it’s using, to start the pod’s containers. The container
runtime then runs the containers.
11.2.3 Observing cluster events
Both the Control Plane components and the Kubelet emit events to the API server as
they perform these actions. They do this by creating Event resources, which are like
any other Kubernetes resource. You’ve already seen events pertaining to specific
resources every time you used kubectl describe to inspect those resources, but you
can also retrieve events directly with kubectl get events.
 Maybe it’s me, but using kubectl get to inspect events is painful, because they’re
not shown in proper temporal order. Instead, if an event occurs multiple times, the
event is displayed only once, showing when it was first seen, when it was last seen, and
the number of times it occurred. Luckily, watching events with the --watch option is
much easier on the eyes and useful for seeing what’s happening in the cluster. 
 The following listing shows the events emitted in the process described previously
(some columns have been removed and the output is edited heavily to make it legible
in the limited space on the page).
 
</data>
  <data key="d5">332
CHAPTER 11
Understanding Kubernetes internals
THE REPLICASET CONTROLLER CREATES THE POD RESOURCES
The newly created ReplicaSet is then picked up by the ReplicaSet controller, which
watches for creations, modifications, and deletions of ReplicaSet resources in the
API server. The controller takes into consideration the replica count and pod selec-
tor defined in the ReplicaSet and verifies whether enough existing Pods match
the selector.
 The controller then creates the Pod resources based on the pod template in the
ReplicaSet (the pod template was copied over from the Deployment when the Deploy-
ment controller created the ReplicaSet). 
THE SCHEDULER ASSIGNS A NODE TO THE NEWLY CREATED PODS
These newly created Pods are now stored in etcd, but they each still lack one import-
ant thing—they don’t have an associated node yet. Their nodeName attribute isn’t set.
The Scheduler watches for Pods like this, and when it encounters one, chooses the
best node for the Pod and assigns the Pod to the node. The Pod’s definition now
includes the name of the node it should be running on.
 Everything so far has been happening in the Kubernetes Control Plane. None of
the controllers that have taken part in this whole process have done anything tangible
except update the resources through the API server. 
THE KUBELET RUNS THE POD’S CONTAINERS
The worker nodes haven’t done anything up to this point. The pod’s containers
haven’t been started yet. The images for the pod’s containers haven’t even been down-
loaded yet. 
 But with the Pod now scheduled to a specific node, the Kubelet on that node can
finally get to work. The Kubelet, watching for changes to Pods on the API server, sees a
new Pod scheduled to its node, so it inspects the Pod definition and instructs Docker,
or whatever container runtime it’s using, to start the pod’s containers. The container
runtime then runs the containers.
11.2.3 Observing cluster events
Both the Control Plane components and the Kubelet emit events to the API server as
they perform these actions. They do this by creating Event resources, which are like
any other Kubernetes resource. You’ve already seen events pertaining to specific
resources every time you used kubectl describe to inspect those resources, but you
can also retrieve events directly with kubectl get events.
 Maybe it’s me, but using kubectl get to inspect events is painful, because they’re
not shown in proper temporal order. Instead, if an event occurs multiple times, the
event is displayed only once, showing when it was first seen, when it was last seen, and
the number of times it occurred. Luckily, watching events with the --watch option is
much easier on the eyes and useful for seeing what’s happening in the cluster. 
 The following listing shows the events emitted in the process described previously
(some columns have been removed and the output is edited heavily to make it legible
in the limited space on the page).
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="365">
  <data key="d0">Page_365</data>
  <data key="d5">Page_365</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_276">
  <data key="d0">333
Understanding what a running pod is
$ kubectl get events --watch
    NAME             KIND         REASON              SOURCE 
... kubia            Deployment   ScalingReplicaSet   deployment-controller  
                     ➥ Scaled up replica set kubia-193 to 3
... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  
                     ➥ Created pod: kubia-193-w7ll2
... kubia-193-tpg6j  Pod          Scheduled           default-scheduler   
                     ➥ Successfully assigned kubia-193-tpg6j to node1
... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  
                     ➥ Created pod: kubia-193-39590
... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  
                     ➥ Created pod: kubia-193-tpg6j
... kubia-193-39590  Pod          Scheduled           default-scheduler  
                     ➥ Successfully assigned kubia-193-39590 to node2
... kubia-193-w7ll2  Pod          Scheduled           default-scheduler  
                     ➥ Successfully assigned kubia-193-w7ll2 to node2
... kubia-193-tpg6j  Pod          Pulled              kubelet, node1  
                     ➥ Container image already present on machine
... kubia-193-tpg6j  Pod          Created             kubelet, node1  
                     ➥ Created container with id 13da752
... kubia-193-39590  Pod          Pulled              kubelet, node2  
                     ➥ Container image already present on machine
... kubia-193-tpg6j  Pod          Started             kubelet, node1  
                     ➥ Started container with id 13da752
... kubia-193-w7ll2  Pod          Pulled              kubelet, node2  
                     ➥ Container image already present on machine
... kubia-193-39590  Pod          Created             kubelet, node2  
                     ➥ Created container with id 8850184
...
As you can see, the SOURCE column shows the controller performing the action, and
the NAME and KIND columns show the resource the controller is acting on. The REASON
column and the MESSAGE column (shown in every second line) give more details
about what the controller has done.
11.3
Understanding what a running pod is
With the pod now running, let’s look more closely at what a running pod even is. If a
pod contains a single container, do you think that the Kubelet just runs this single
container, or is there more to it?
 You’ve run several pods throughout this book. If you’re the investigative type, you
may have already snuck a peek at what exactly Docker ran when you created a pod. If
not, let me explain what you’d see.
 Imagine you run a single container pod. Let’s say you create an Nginx pod:
$ kubectl run nginx --image=nginx
deployment "nginx" created
You can now ssh into the worker node running the pod and inspect the list of run-
ning Docker containers. I’m using Minikube to test this out, so to ssh into the single
Listing 11.9
Watching events emitted by the controllers
 
</data>
  <data key="d5">333
Understanding what a running pod is
$ kubectl get events --watch
    NAME             KIND         REASON              SOURCE 
... kubia            Deployment   ScalingReplicaSet   deployment-controller  
                     ➥ Scaled up replica set kubia-193 to 3
... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  
                     ➥ Created pod: kubia-193-w7ll2
... kubia-193-tpg6j  Pod          Scheduled           default-scheduler   
                     ➥ Successfully assigned kubia-193-tpg6j to node1
... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  
                     ➥ Created pod: kubia-193-39590
... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  
                     ➥ Created pod: kubia-193-tpg6j
... kubia-193-39590  Pod          Scheduled           default-scheduler  
                     ➥ Successfully assigned kubia-193-39590 to node2
... kubia-193-w7ll2  Pod          Scheduled           default-scheduler  
                     ➥ Successfully assigned kubia-193-w7ll2 to node2
... kubia-193-tpg6j  Pod          Pulled              kubelet, node1  
                     ➥ Container image already present on machine
... kubia-193-tpg6j  Pod          Created             kubelet, node1  
                     ➥ Created container with id 13da752
... kubia-193-39590  Pod          Pulled              kubelet, node2  
                     ➥ Container image already present on machine
... kubia-193-tpg6j  Pod          Started             kubelet, node1  
                     ➥ Started container with id 13da752
... kubia-193-w7ll2  Pod          Pulled              kubelet, node2  
                     ➥ Container image already present on machine
... kubia-193-39590  Pod          Created             kubelet, node2  
                     ➥ Created container with id 8850184
...
As you can see, the SOURCE column shows the controller performing the action, and
the NAME and KIND columns show the resource the controller is acting on. The REASON
column and the MESSAGE column (shown in every second line) give more details
about what the controller has done.
11.3
Understanding what a running pod is
With the pod now running, let’s look more closely at what a running pod even is. If a
pod contains a single container, do you think that the Kubelet just runs this single
container, or is there more to it?
 You’ve run several pods throughout this book. If you’re the investigative type, you
may have already snuck a peek at what exactly Docker ran when you created a pod. If
not, let me explain what you’d see.
 Imagine you run a single container pod. Let’s say you create an Nginx pod:
$ kubectl run nginx --image=nginx
deployment "nginx" created
You can now ssh into the worker node running the pod and inspect the list of run-
ning Docker containers. I’m using Minikube to test this out, so to ssh into the single
Listing 11.9
Watching events emitted by the controllers
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="366">
  <data key="d0">Page_366</data>
  <data key="d5">Page_366</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_277">
  <data key="d0">334
CHAPTER 11
Understanding Kubernetes internals
node, I use minikube ssh. If you’re using GKE, you can ssh into a node with gcloud
compute ssh &lt;node name&gt;.
 Once you’re inside the node, you can list all the running containers with docker
ps, as shown in the following listing.
docker@minikubeVM:~$ docker ps
CONTAINER ID   IMAGE                  COMMAND                 CREATED
c917a6f3c3f7   nginx                  "nginx -g 'daemon off"  4 seconds ago 
98b8bf797174   gcr.io/.../pause:3.0   "/pause"                7 seconds ago
...
NOTE
I’ve removed irrelevant information from the previous listing—this
includes both columns and rows. I’ve also removed all the other running con-
tainers. If you’re trying this out yourself, pay attention to the two containers
that were created a few seconds ago. 
As expected, you see the Nginx container, but also an additional container. Judging
from the COMMAND column, this additional container isn’t doing anything (the con-
tainer’s command is "pause"). If you look closely, you’ll see that this container was
created a few seconds before the Nginx container. What’s its role?
 This pause container is the container that holds all the containers of a pod
together. Remember how all containers of a pod share the same network and other
Linux namespaces? The pause container is an infrastructure container whose sole
purpose is to hold all these namespaces. All other user-defined containers of the pod
then use the namespaces of the pod infrastructure container (see figure 11.13).
Actual application containers may die and get restarted. When such a container starts
up again, it needs to become part of the same Linux namespaces as before. The infra-
structure container makes this possible since its lifecycle is tied to that of the pod—the
container runs from the time the pod is scheduled until the pod is deleted. If the
infrastructure pod is killed in the meantime, the Kubelet recreates it and all the pod’s
containers.
Listing 11.10
Listing running Docker containers
Pod
Container A
Container A
Pod infrastructure
container
Container B
Container B
Uses Linux
namespaces from
Uses Linux
namespaces from
Figure 11.13
A two-container pod results in three running containers 
sharing the same Linux namespaces.
 
</data>
  <data key="d5">334
CHAPTER 11
Understanding Kubernetes internals
node, I use minikube ssh. If you’re using GKE, you can ssh into a node with gcloud
compute ssh &lt;node name&gt;.
 Once you’re inside the node, you can list all the running containers with docker
ps, as shown in the following listing.
docker@minikubeVM:~$ docker ps
CONTAINER ID   IMAGE                  COMMAND                 CREATED
c917a6f3c3f7   nginx                  "nginx -g 'daemon off"  4 seconds ago 
98b8bf797174   gcr.io/.../pause:3.0   "/pause"                7 seconds ago
...
NOTE
I’ve removed irrelevant information from the previous listing—this
includes both columns and rows. I’ve also removed all the other running con-
tainers. If you’re trying this out yourself, pay attention to the two containers
that were created a few seconds ago. 
As expected, you see the Nginx container, but also an additional container. Judging
from the COMMAND column, this additional container isn’t doing anything (the con-
tainer’s command is "pause"). If you look closely, you’ll see that this container was
created a few seconds before the Nginx container. What’s its role?
 This pause container is the container that holds all the containers of a pod
together. Remember how all containers of a pod share the same network and other
Linux namespaces? The pause container is an infrastructure container whose sole
purpose is to hold all these namespaces. All other user-defined containers of the pod
then use the namespaces of the pod infrastructure container (see figure 11.13).
Actual application containers may die and get restarted. When such a container starts
up again, it needs to become part of the same Linux namespaces as before. The infra-
structure container makes this possible since its lifecycle is tied to that of the pod—the
container runs from the time the pod is scheduled until the pod is deleted. If the
infrastructure pod is killed in the meantime, the Kubelet recreates it and all the pod’s
containers.
Listing 11.10
Listing running Docker containers
Pod
Container A
Container A
Pod infrastructure
container
Container B
Container B
Uses Linux
namespaces from
Uses Linux
namespaces from
Figure 11.13
A two-container pod results in three running containers 
sharing the same Linux namespaces.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="367">
  <data key="d0">Page_367</data>
  <data key="d5">Page_367</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_278">
  <data key="d0">335
Inter-pod networking
11.4
Inter-pod networking
By now, you know that each pod gets its own unique IP address and can communicate
with all other pods through a flat, NAT-less network. How exactly does Kubernetes
achieve this? In short, it doesn’t. The network is set up by the system administrator or
by a Container Network Interface (CNI) plugin, not by Kubernetes itself. 
11.4.1 What the network must be like
Kubernetes doesn’t require you to use a specific networking technology, but it does
mandate that the pods (or to be more precise, their containers) can communicate
with each other, regardless if they’re running on the same worker node or not. The
network the pods use to communicate must be such that the IP address a pod sees as
its own is the exact same address that all other pods see as the IP address of the pod in
question. 
 Look at figure 11.14. When pod A connects to (sends a network packet to) pod B,
the source IP pod B sees must be the same IP that pod A sees as its own. There should
be no network address translation (NAT) performed in between—the packet sent by
pod A must reach pod B with both the source and destination address unchanged.
This is important, because it makes networking for applications running inside pods
simple and exactly as if they were running on machines connected to the same net-
work switch. The absence of NAT between pods enables applications running inside
them to self-register in other pods. 
Node 1
Pod A
IP: 10.1.1.1
srcIP: 10.1.1.1
dstIP: 10.1.2.1
srcIP: 10.1.1.1
dstIP: 10.1.2.1
Packet
Node 2
Pod B
IP: 10.1.2.1
srcIP: 10.1.1.1
dstIP: 10.1.2.1
Packet
Network
No NAT (IPs
are preserved)
Figure 11.14
Kubernetes mandates pods are connected through a NAT-less 
network.
 
</data>
  <data key="d5">335
Inter-pod networking
11.4
Inter-pod networking
By now, you know that each pod gets its own unique IP address and can communicate
with all other pods through a flat, NAT-less network. How exactly does Kubernetes
achieve this? In short, it doesn’t. The network is set up by the system administrator or
by a Container Network Interface (CNI) plugin, not by Kubernetes itself. 
11.4.1 What the network must be like
Kubernetes doesn’t require you to use a specific networking technology, but it does
mandate that the pods (or to be more precise, their containers) can communicate
with each other, regardless if they’re running on the same worker node or not. The
network the pods use to communicate must be such that the IP address a pod sees as
its own is the exact same address that all other pods see as the IP address of the pod in
question. 
 Look at figure 11.14. When pod A connects to (sends a network packet to) pod B,
the source IP pod B sees must be the same IP that pod A sees as its own. There should
be no network address translation (NAT) performed in between—the packet sent by
pod A must reach pod B with both the source and destination address unchanged.
This is important, because it makes networking for applications running inside pods
simple and exactly as if they were running on machines connected to the same net-
work switch. The absence of NAT between pods enables applications running inside
them to self-register in other pods. 
Node 1
Pod A
IP: 10.1.1.1
srcIP: 10.1.1.1
dstIP: 10.1.2.1
srcIP: 10.1.1.1
dstIP: 10.1.2.1
Packet
Node 2
Pod B
IP: 10.1.2.1
srcIP: 10.1.1.1
dstIP: 10.1.2.1
Packet
Network
No NAT (IPs
are preserved)
Figure 11.14
Kubernetes mandates pods are connected through a NAT-less 
network.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="368">
  <data key="d0">Page_368</data>
  <data key="d5">Page_368</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_279">
  <data key="d0">336
CHAPTER 11
Understanding Kubernetes internals
 For example, say you have a client pod X and pod Y, which provides a kind of noti-
fication service to all pods that register with it. Pod X connects to pod Y and tells it,
“Hey, I’m pod X, available at IP 1.2.3.4; please send updates to me at this IP address.”
The pod providing the service can connect to the first pod by using the received
IP address. 
 The requirement for NAT-less communication between pods also extends to pod-
to-node and node-to-pod communication. But when a pod communicates with ser-
vices out on the internet, the source IP of the packets the pod sends does need to be
changed, because the pod’s IP is private. The source IP of outbound packets is
changed to the host worker node’s IP address.
 Building a proper Kubernetes cluster involves setting up the networking according
to these requirements. There are various methods and technologies available to do
this, each with its own benefits or drawbacks in a given scenario. Because of this, we’re
not going to go into specific technologies. Instead, let’s explain how inter-pod net-
working works in general. 
11.4.2 Diving deeper into how networking works
In section 11.3, we saw that a pod’s IP address and network namespace are set up and
held by the infrastructure container (the pause container). The pod’s containers then
use its network namespace. A pod’s network interface is thus whatever is set up in the
infrastructure container. Let’s see how the interface is created and how it’s connected
to the interfaces in all the other pods. Look at figure 11.15. We’ll discuss it next.
ENABLING COMMUNICATION BETWEEN PODS ON THE SAME NODE
Before the infrastructure container is started, a virtual Ethernet interface pair (a veth
pair) is created for the container. One interface of the pair remains in the host’s
namespace (you’ll see it listed as vethXXX when you run ifconfig on the node),
whereas the other is moved into the container’s network namespace and renamed
eth0. The two virtual interfaces are like two ends of a pipe (or like two network
devices connected by an Ethernet cable)—what goes in on one side comes out on the
other, and vice-versa. 
Node
Pod A
eth0
10.1.1.1
veth123
Pod B
eth0
10.1.1.2
veth234
Bridge
10.1.1.0/24
This is pod A’s
veth pair.
This is pod B’s
veth pair.
Figure 11.15
Pods on a node are 
connected to the same bridge through 
virtual Ethernet interface pairs.
 
</data>
  <data key="d5">336
CHAPTER 11
Understanding Kubernetes internals
 For example, say you have a client pod X and pod Y, which provides a kind of noti-
fication service to all pods that register with it. Pod X connects to pod Y and tells it,
“Hey, I’m pod X, available at IP 1.2.3.4; please send updates to me at this IP address.”
The pod providing the service can connect to the first pod by using the received
IP address. 
 The requirement for NAT-less communication between pods also extends to pod-
to-node and node-to-pod communication. But when a pod communicates with ser-
vices out on the internet, the source IP of the packets the pod sends does need to be
changed, because the pod’s IP is private. The source IP of outbound packets is
changed to the host worker node’s IP address.
 Building a proper Kubernetes cluster involves setting up the networking according
to these requirements. There are various methods and technologies available to do
this, each with its own benefits or drawbacks in a given scenario. Because of this, we’re
not going to go into specific technologies. Instead, let’s explain how inter-pod net-
working works in general. 
11.4.2 Diving deeper into how networking works
In section 11.3, we saw that a pod’s IP address and network namespace are set up and
held by the infrastructure container (the pause container). The pod’s containers then
use its network namespace. A pod’s network interface is thus whatever is set up in the
infrastructure container. Let’s see how the interface is created and how it’s connected
to the interfaces in all the other pods. Look at figure 11.15. We’ll discuss it next.
ENABLING COMMUNICATION BETWEEN PODS ON THE SAME NODE
Before the infrastructure container is started, a virtual Ethernet interface pair (a veth
pair) is created for the container. One interface of the pair remains in the host’s
namespace (you’ll see it listed as vethXXX when you run ifconfig on the node),
whereas the other is moved into the container’s network namespace and renamed
eth0. The two virtual interfaces are like two ends of a pipe (or like two network
devices connected by an Ethernet cable)—what goes in on one side comes out on the
other, and vice-versa. 
Node
Pod A
eth0
10.1.1.1
veth123
Pod B
eth0
10.1.1.2
veth234
Bridge
10.1.1.0/24
This is pod A’s
veth pair.
This is pod B’s
veth pair.
Figure 11.15
Pods on a node are 
connected to the same bridge through 
virtual Ethernet interface pairs.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="369">
  <data key="d0">Page_369</data>
  <data key="d5">Page_369</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_280">
  <data key="d0">337
Inter-pod networking
 The interface in the host’s network namespace is attached to a network bridge that
the container runtime is configured to use. The eth0 interface in the container is
assigned an IP address from the bridge’s address range. Anything that an application
running inside the container sends to the eth0 network interface (the one in the con-
tainer’s namespace), comes out at the other veth interface in the host’s namespace
and is sent to the bridge. This means it can be received by any network interface that’s
connected to the bridge. 
 If pod A sends a network packet to pod B, the packet first goes through pod A’s
veth pair to the bridge and then through pod B’s veth pair. All containers on a node
are connected to the same bridge, which means they can all communicate with each
other. But to enable communication between containers running on different nodes,
the bridges on those nodes need to be connected somehow. 
ENABLING COMMUNICATION BETWEEN PODS ON DIFFERENT NODES
You have many ways to connect bridges on different nodes. This can be done with
overlay or underlay networks or by regular layer 3 routing, which we’ll look at next.
 You know pod IP addresses must be unique across the whole cluster, so the bridges
across the nodes must use non-overlapping address ranges to prevent pods on differ-
ent nodes from getting the same IP. In the example shown in figure 11.16, the bridge
on node A is using the 10.1.1.0/24 IP range and the bridge on node B is using
10.1.2.0/24, which ensures no IP address conflicts exist.
 Figure 11.16 shows that to enable communication between pods across two nodes
with plain layer 3 networking, the node’s physical network interface needs to be con-
nected to the bridge as well. Routing tables on node A need to be configured so all
packets destined for 10.1.2.0/24 are routed to node B, whereas node B’s routing
tables need to be configured so packets sent to 10.1.1.0/24 are routed to node A.
 With this type of setup, when a packet is sent by a container on one of the nodes
to a container on the other node, the packet first goes through the veth pair, then
Node A
Pod A
Network
eth0
10.1.1.1
veth123
Pod B
eth0
10.1.1.2
veth234
Bridge
10.1.1.0/24
eth0
10.100.0.1
Node B
Pod C
eth0
10.1.2.1
veth345
Pod D
eth0
10.1.2.2
veth456
Bridge
10.1.2.0/24
eth0
10.100.0.2
Figure 11.16
For pods on different nodes to communicate, the bridges need to be connected 
somehow.
 
</data>
  <data key="d5">337
Inter-pod networking
 The interface in the host’s network namespace is attached to a network bridge that
the container runtime is configured to use. The eth0 interface in the container is
assigned an IP address from the bridge’s address range. Anything that an application
running inside the container sends to the eth0 network interface (the one in the con-
tainer’s namespace), comes out at the other veth interface in the host’s namespace
and is sent to the bridge. This means it can be received by any network interface that’s
connected to the bridge. 
 If pod A sends a network packet to pod B, the packet first goes through pod A’s
veth pair to the bridge and then through pod B’s veth pair. All containers on a node
are connected to the same bridge, which means they can all communicate with each
other. But to enable communication between containers running on different nodes,
the bridges on those nodes need to be connected somehow. 
ENABLING COMMUNICATION BETWEEN PODS ON DIFFERENT NODES
You have many ways to connect bridges on different nodes. This can be done with
overlay or underlay networks or by regular layer 3 routing, which we’ll look at next.
 You know pod IP addresses must be unique across the whole cluster, so the bridges
across the nodes must use non-overlapping address ranges to prevent pods on differ-
ent nodes from getting the same IP. In the example shown in figure 11.16, the bridge
on node A is using the 10.1.1.0/24 IP range and the bridge on node B is using
10.1.2.0/24, which ensures no IP address conflicts exist.
 Figure 11.16 shows that to enable communication between pods across two nodes
with plain layer 3 networking, the node’s physical network interface needs to be con-
nected to the bridge as well. Routing tables on node A need to be configured so all
packets destined for 10.1.2.0/24 are routed to node B, whereas node B’s routing
tables need to be configured so packets sent to 10.1.1.0/24 are routed to node A.
 With this type of setup, when a packet is sent by a container on one of the nodes
to a container on the other node, the packet first goes through the veth pair, then
Node A
Pod A
Network
eth0
10.1.1.1
veth123
Pod B
eth0
10.1.1.2
veth234
Bridge
10.1.1.0/24
eth0
10.100.0.1
Node B
Pod C
eth0
10.1.2.1
veth345
Pod D
eth0
10.1.2.2
veth456
Bridge
10.1.2.0/24
eth0
10.100.0.2
Figure 11.16
For pods on different nodes to communicate, the bridges need to be connected 
somehow.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="370">
  <data key="d0">Page_370</data>
  <data key="d5">Page_370</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_281">
  <data key="d0">338
CHAPTER 11
Understanding Kubernetes internals
through the bridge to the node’s physical adapter, then over the wire to the other
node’s physical adapter, through the other node’s bridge, and finally through the veth
pair of the destination container.
 This works only when nodes are connected to the same network switch, without
any routers in between; otherwise those routers would drop the packets because
they refer to pod IPs, which are private. Sure, the routers in between could be con-
figured to route packets between the nodes, but this becomes increasingly difficult
and error-prone as the number of routers between the nodes increases. Because of
this, it’s easier to use a Software Defined Network (SDN), which makes the nodes
appear as though they’re connected to the same network switch, regardless of the
actual underlying network topology, no matter how complex it is. Packets sent
from the pod are encapsulated and sent over the network to the node running the
other pod, where they are de-encapsulated and delivered to the pod in their origi-
nal form.
11.4.3 Introducing the Container Network Interface
To make it easier to connect containers into a network, a project called Container
Network Interface (CNI) was started. The CNI allows Kubernetes to be configured to
use any CNI plugin that’s out there. These plugins include
Calico
Flannel
Romana
Weave Net 
And others
We’re not going to go into the details of these plugins; if you want to learn more about
them, refer to https:/
/kubernetes.io/docs/concepts/cluster-administration/addons/.
 Installing a network plugin isn’t difficult. You only need to deploy a YAML con-
taining a DaemonSet and a few other supporting resources. This YAML is provided
on each plugin’s project page. As you can imagine, the DaemonSet is used to deploy
a network agent on all cluster nodes. It then ties into the CNI interface on the node,
but be aware that the Kubelet needs to be started with --network-plugin=cni to
use CNI. 
11.5
How services are implemented
In chapter 5 you learned about Services, which allow exposing a set of pods at a long-
lived, stable IP address and port. In order to focus on what Services are meant for and
how they can be used, we intentionally didn’t go into how they work. But to truly
understand Services and have a better feel for where to look when things don’t behave
the way you expect, you need to understand how they are implemented. 
 
</data>
  <data key="d5">338
CHAPTER 11
Understanding Kubernetes internals
through the bridge to the node’s physical adapter, then over the wire to the other
node’s physical adapter, through the other node’s bridge, and finally through the veth
pair of the destination container.
 This works only when nodes are connected to the same network switch, without
any routers in between; otherwise those routers would drop the packets because
they refer to pod IPs, which are private. Sure, the routers in between could be con-
figured to route packets between the nodes, but this becomes increasingly difficult
and error-prone as the number of routers between the nodes increases. Because of
this, it’s easier to use a Software Defined Network (SDN), which makes the nodes
appear as though they’re connected to the same network switch, regardless of the
actual underlying network topology, no matter how complex it is. Packets sent
from the pod are encapsulated and sent over the network to the node running the
other pod, where they are de-encapsulated and delivered to the pod in their origi-
nal form.
11.4.3 Introducing the Container Network Interface
To make it easier to connect containers into a network, a project called Container
Network Interface (CNI) was started. The CNI allows Kubernetes to be configured to
use any CNI plugin that’s out there. These plugins include
Calico
Flannel
Romana
Weave Net 
And others
We’re not going to go into the details of these plugins; if you want to learn more about
them, refer to https:/
/kubernetes.io/docs/concepts/cluster-administration/addons/.
 Installing a network plugin isn’t difficult. You only need to deploy a YAML con-
taining a DaemonSet and a few other supporting resources. This YAML is provided
on each plugin’s project page. As you can imagine, the DaemonSet is used to deploy
a network agent on all cluster nodes. It then ties into the CNI interface on the node,
but be aware that the Kubelet needs to be started with --network-plugin=cni to
use CNI. 
11.5
How services are implemented
In chapter 5 you learned about Services, which allow exposing a set of pods at a long-
lived, stable IP address and port. In order to focus on what Services are meant for and
how they can be used, we intentionally didn’t go into how they work. But to truly
understand Services and have a better feel for where to look when things don’t behave
the way you expect, you need to understand how they are implemented. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="371">
  <data key="d0">Page_371</data>
  <data key="d5">Page_371</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_282">
  <data key="d0">339
How services are implemented
11.5.1 Introducing the kube-proxy
Everything related to Services is handled by the kube-proxy process running on each
node. Initially, the kube-proxy was an actual proxy waiting for connections and for
each incoming connection, opening a new connection to one of the pods. This was
called the userspace proxy mode. Later, a better-performing iptables proxy mode
replaced it. This is now the default, but you can still configure Kubernetes to use the
old mode if you want.
 Before we continue, let’s quickly review a few things about Services, which are rele-
vant for understanding the next few paragraphs.
 We’ve learned that each Service gets its own stable IP address and port. Clients
(usually pods) use the service by connecting to this IP address and port. The IP
address is virtual—it’s not assigned to any network interfaces and is never listed as
either the source or the destination IP address in a network packet when the packet
leaves the node. A key detail of Services is that they consist of an IP and port pair (or
multiple IP and port pairs in the case of multi-port Services), so the service IP by itself
doesn’t represent anything. That’s why you can’t ping them. 
11.5.2 How kube-proxy uses iptables
When a service is created in the API server, the virtual IP address is assigned to it
immediately. Soon afterward, the API server notifies all kube-proxy agents running on
the worker nodes that a new Service has been created. Then, each kube-proxy makes
that service addressable on the node it’s running on. It does this by setting up a few
iptables rules, which make sure each packet destined for the service IP/port pair is
intercepted and its destination address modified, so the packet is redirected to one of
the pods backing the service. 
 Besides watching the API server for changes to Services, kube-proxy also watches
for changes to Endpoints objects. We talked about them in chapter 5, but let me
refresh your memory, as it’s easy to forget they even exist, because you rarely create
them manually. An Endpoints object holds the IP/port pairs of all the pods that back
the service (an IP/port pair can also point to something other than a pod). That’s
why the kube-proxy must also watch all Endpoints objects. After all, an Endpoints
object changes every time a new backing pod is created or deleted, and when the
pod’s readiness status changes or the pod’s labels change and it falls in or out of scope
of the service. 
 Now let’s see how kube-proxy enables clients to connect to those pods through the
Service. This is shown in figure 11.17.
 The figure shows what the kube-proxy does and how a packet sent by a client pod
reaches one of the pods backing the Service. Let’s examine what happens to the
packet when it’s sent by the client pod (pod A in the figure). 
 The packet’s destination is initially set to the IP and port of the Service (in the
example, the Service is at 172.30.0.1:80). Before being sent to the network, the
 
</data>
  <data key="d5">339
How services are implemented
11.5.1 Introducing the kube-proxy
Everything related to Services is handled by the kube-proxy process running on each
node. Initially, the kube-proxy was an actual proxy waiting for connections and for
each incoming connection, opening a new connection to one of the pods. This was
called the userspace proxy mode. Later, a better-performing iptables proxy mode
replaced it. This is now the default, but you can still configure Kubernetes to use the
old mode if you want.
 Before we continue, let’s quickly review a few things about Services, which are rele-
vant for understanding the next few paragraphs.
 We’ve learned that each Service gets its own stable IP address and port. Clients
(usually pods) use the service by connecting to this IP address and port. The IP
address is virtual—it’s not assigned to any network interfaces and is never listed as
either the source or the destination IP address in a network packet when the packet
leaves the node. A key detail of Services is that they consist of an IP and port pair (or
multiple IP and port pairs in the case of multi-port Services), so the service IP by itself
doesn’t represent anything. That’s why you can’t ping them. 
11.5.2 How kube-proxy uses iptables
When a service is created in the API server, the virtual IP address is assigned to it
immediately. Soon afterward, the API server notifies all kube-proxy agents running on
the worker nodes that a new Service has been created. Then, each kube-proxy makes
that service addressable on the node it’s running on. It does this by setting up a few
iptables rules, which make sure each packet destined for the service IP/port pair is
intercepted and its destination address modified, so the packet is redirected to one of
the pods backing the service. 
 Besides watching the API server for changes to Services, kube-proxy also watches
for changes to Endpoints objects. We talked about them in chapter 5, but let me
refresh your memory, as it’s easy to forget they even exist, because you rarely create
them manually. An Endpoints object holds the IP/port pairs of all the pods that back
the service (an IP/port pair can also point to something other than a pod). That’s
why the kube-proxy must also watch all Endpoints objects. After all, an Endpoints
object changes every time a new backing pod is created or deleted, and when the
pod’s readiness status changes or the pod’s labels change and it falls in or out of scope
of the service. 
 Now let’s see how kube-proxy enables clients to connect to those pods through the
Service. This is shown in figure 11.17.
 The figure shows what the kube-proxy does and how a packet sent by a client pod
reaches one of the pods backing the Service. Let’s examine what happens to the
packet when it’s sent by the client pod (pod A in the figure). 
 The packet’s destination is initially set to the IP and port of the Service (in the
example, the Service is at 172.30.0.1:80). Before being sent to the network, the
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="372">
  <data key="d0">Page_372</data>
  <data key="d5">Page_372</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_283">
  <data key="d0">340
CHAPTER 11
Understanding Kubernetes internals
packet is first handled by node A’s kernel according to the iptables rules set up on
the node. 
 The kernel checks if the packet matches any of those iptables rules. One of them
says that if any packet has the destination IP equal to 172.30.0.1 and destination port
equal to 80, the packet’s destination IP and port should be replaced with the IP and
port of a randomly selected pod. 
 The packet in the example matches that rule and so its destination IP/port is
changed. In the example, pod B2 was randomly selected, so the packet’s destination
IP is changed to 10.1.2.1 (pod B2’s IP) and the port to 8080 (the target port specified
in the Service spec). From here on, it’s exactly as if the client pod had sent the packet
to pod B directly instead of through the service. 
 It’s slightly more complicated than that, but that’s the most important part you
need to understand.
 
Node A
Node B
API server
Pod A
Pod B1
Pod B2
Pod B3
Packet X
Source:
10.1.1.1
Destination:
172.30.0.1:80
10.1.2.1:8080
iptables
Service B
172.30.0.1:80
Conﬁgures
iptables
Packet X
Source:
10.1.1.1
Destination:
172.30.0.1:80
kube-proxy
Endpoints B
Pod A
IP: 10.1.1.1
Pod B1
IP: 10.1.1.2
Pod B2
IP: 10.1.2.1
Pod B3
IP: 10.1.2.2
Watches for changes to
services and endpoints
Figure 11.17
Network packets sent to a Service’s virtual IP/port pair are 
modified and redirected to a randomly selected backend pod.
 
</data>
  <data key="d5">340
CHAPTER 11
Understanding Kubernetes internals
packet is first handled by node A’s kernel according to the iptables rules set up on
the node. 
 The kernel checks if the packet matches any of those iptables rules. One of them
says that if any packet has the destination IP equal to 172.30.0.1 and destination port
equal to 80, the packet’s destination IP and port should be replaced with the IP and
port of a randomly selected pod. 
 The packet in the example matches that rule and so its destination IP/port is
changed. In the example, pod B2 was randomly selected, so the packet’s destination
IP is changed to 10.1.2.1 (pod B2’s IP) and the port to 8080 (the target port specified
in the Service spec). From here on, it’s exactly as if the client pod had sent the packet
to pod B directly instead of through the service. 
 It’s slightly more complicated than that, but that’s the most important part you
need to understand.
 
Node A
Node B
API server
Pod A
Pod B1
Pod B2
Pod B3
Packet X
Source:
10.1.1.1
Destination:
172.30.0.1:80
10.1.2.1:8080
iptables
Service B
172.30.0.1:80
Conﬁgures
iptables
Packet X
Source:
10.1.1.1
Destination:
172.30.0.1:80
kube-proxy
Endpoints B
Pod A
IP: 10.1.1.1
Pod B1
IP: 10.1.1.2
Pod B2
IP: 10.1.2.1
Pod B3
IP: 10.1.2.2
Watches for changes to
services and endpoints
Figure 11.17
Network packets sent to a Service’s virtual IP/port pair are 
modified and redirected to a randomly selected backend pod.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="373">
  <data key="d0">Page_373</data>
  <data key="d5">Page_373</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_284">
  <data key="d0">341
Running highly available clusters
11.6
Running highly available clusters
One of the reasons for running apps inside Kubernetes is to keep them running with-
out interruption with no or limited manual intervention in case of infrastructure
failures. For running services without interruption it’s not only the apps that need to
be up all the time, but also the Kubernetes Control Plane components. We’ll look at
what’s involved in achieving high availability next.
11.6.1 Making your apps highly available
When running apps in Kubernetes, the various controllers make sure your app keeps
running smoothly and at the specified scale even when nodes fail. To ensure your app
is highly available, you only need to run them through a Deployment resource and
configure an appropriate number of replicas; everything else is taken care of by
Kubernetes. 
RUNNING MULTIPLE INSTANCES TO REDUCE THE LIKELIHOOD OF DOWNTIME
This requires your apps to be horizontally scalable, but even if that’s not the case in
your app, you should still use a Deployment with its replica count set to one. If the
replica becomes unavailable, it will be replaced with a new one quickly, although that
doesn’t happen instantaneously. It takes time for all the involved controllers to notice
that a node has failed, create the new pod replica, and start the pod’s containers.
There will inevitably be a short period of downtime in between. 
USING LEADER-ELECTION FOR NON-HORIZONTALLY SCALABLE APPS
To avoid the downtime, you need to run additional inactive replicas along with the
active one and use a fast-acting lease or leader-election mechanism to make sure only
one is active. In case you’re unfamiliar with leader election, it’s a way for multiple app
instances running in a distributed environment to come to an agreement on which is
the leader. That leader is either the only one performing tasks, while all others are
waiting for the leader to fail and then becoming leaders themselves, or they can all be
active, with the leader being the only instance performing writes, while all the others
are providing read-only access to their data, for example. This ensures two instances
are never doing the same job, if that would lead to unpredictable system behavior due
to race conditions.
 The mechanism doesn’t need to be incorporated into the app itself. You can use a
sidecar container that performs all the leader-election operations and signals the
main container when it should become active. You’ll find an example of leader elec-
tion in Kubernetes at https:/
/github.com/kubernetes/contrib/tree/master/election.
 Ensuring your apps are highly available is relatively simple, because Kubernetes
takes care of almost everything. But what if Kubernetes itself fails? What if the servers
running the Kubernetes Control Plane components go down? How are those compo-
nents made highly available?
 
</data>
  <data key="d5">341
Running highly available clusters
11.6
Running highly available clusters
One of the reasons for running apps inside Kubernetes is to keep them running with-
out interruption with no or limited manual intervention in case of infrastructure
failures. For running services without interruption it’s not only the apps that need to
be up all the time, but also the Kubernetes Control Plane components. We’ll look at
what’s involved in achieving high availability next.
11.6.1 Making your apps highly available
When running apps in Kubernetes, the various controllers make sure your app keeps
running smoothly and at the specified scale even when nodes fail. To ensure your app
is highly available, you only need to run them through a Deployment resource and
configure an appropriate number of replicas; everything else is taken care of by
Kubernetes. 
RUNNING MULTIPLE INSTANCES TO REDUCE THE LIKELIHOOD OF DOWNTIME
This requires your apps to be horizontally scalable, but even if that’s not the case in
your app, you should still use a Deployment with its replica count set to one. If the
replica becomes unavailable, it will be replaced with a new one quickly, although that
doesn’t happen instantaneously. It takes time for all the involved controllers to notice
that a node has failed, create the new pod replica, and start the pod’s containers.
There will inevitably be a short period of downtime in between. 
USING LEADER-ELECTION FOR NON-HORIZONTALLY SCALABLE APPS
To avoid the downtime, you need to run additional inactive replicas along with the
active one and use a fast-acting lease or leader-election mechanism to make sure only
one is active. In case you’re unfamiliar with leader election, it’s a way for multiple app
instances running in a distributed environment to come to an agreement on which is
the leader. That leader is either the only one performing tasks, while all others are
waiting for the leader to fail and then becoming leaders themselves, or they can all be
active, with the leader being the only instance performing writes, while all the others
are providing read-only access to their data, for example. This ensures two instances
are never doing the same job, if that would lead to unpredictable system behavior due
to race conditions.
 The mechanism doesn’t need to be incorporated into the app itself. You can use a
sidecar container that performs all the leader-election operations and signals the
main container when it should become active. You’ll find an example of leader elec-
tion in Kubernetes at https:/
/github.com/kubernetes/contrib/tree/master/election.
 Ensuring your apps are highly available is relatively simple, because Kubernetes
takes care of almost everything. But what if Kubernetes itself fails? What if the servers
running the Kubernetes Control Plane components go down? How are those compo-
nents made highly available?
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="374">
  <data key="d0">Page_374</data>
  <data key="d5">Page_374</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_285">
  <data key="d0">342
CHAPTER 11
Understanding Kubernetes internals
11.6.2 Making Kubernetes Control Plane components highly available
In the beginning of this chapter, you learned about the few components that make up
a Kubernetes Control Plane. To make Kubernetes highly available, you need to run
multiple master nodes, which run multiple instances of the following components:
etcd, which is the distributed data store where all the API objects are kept
API server
Controller Manager, which is the process in which all the controllers run
Scheduler
Without going into the actual details of how to install and run these components, let’s
see what’s involved in making each of these components highly available. Figure 11.18
shows an overview of a highly available cluster.
RUNNING AN ETCD CLUSTER
Because etcd was designed as a distributed system, one of its key features is the ability
to run multiple etcd instances, so making it highly available is no big deal. All you
need to do is run it on an appropriate number of machines (three, five, or seven, as
explained earlier in the chapter) and make them aware of each other. You do this by
including the list of all the other instances in every instance’s configuration. For
example, when starting an instance, you specify the IPs and ports where the other etcd
instances can be reached. 
 etcd will replicate data across all its instances, so a failure of one of the nodes when
running a three-machine cluster will still allow the cluster to accept both read and
write operations. To increase the fault tolerance to more than a single node, you need
to run five or seven etcd nodes, which would allow the cluster to handle two or three
Node 1
Kubelet
Node 2
Kubelet
Node 3
Kubelet
Node 4
Kubelet
Node 5
Kubelet
...
Node N
Kubelet
Load
balancer
Master 3
etcd
API server
Scheduler
Controller
Manager
[standing-by]
[standing-by]
Master 2
etcd
API server
Scheduler
Controller
Manager
[standing-by]
[standing-by]
Master 1
etcd
API server
Scheduler
Controller
Manager
[active]
[active]
Figure 11.18
A highly-available cluster with three master nodes
 
</data>
  <data key="d5">342
CHAPTER 11
Understanding Kubernetes internals
11.6.2 Making Kubernetes Control Plane components highly available
In the beginning of this chapter, you learned about the few components that make up
a Kubernetes Control Plane. To make Kubernetes highly available, you need to run
multiple master nodes, which run multiple instances of the following components:
etcd, which is the distributed data store where all the API objects are kept
API server
Controller Manager, which is the process in which all the controllers run
Scheduler
Without going into the actual details of how to install and run these components, let’s
see what’s involved in making each of these components highly available. Figure 11.18
shows an overview of a highly available cluster.
RUNNING AN ETCD CLUSTER
Because etcd was designed as a distributed system, one of its key features is the ability
to run multiple etcd instances, so making it highly available is no big deal. All you
need to do is run it on an appropriate number of machines (three, five, or seven, as
explained earlier in the chapter) and make them aware of each other. You do this by
including the list of all the other instances in every instance’s configuration. For
example, when starting an instance, you specify the IPs and ports where the other etcd
instances can be reached. 
 etcd will replicate data across all its instances, so a failure of one of the nodes when
running a three-machine cluster will still allow the cluster to accept both read and
write operations. To increase the fault tolerance to more than a single node, you need
to run five or seven etcd nodes, which would allow the cluster to handle two or three
Node 1
Kubelet
Node 2
Kubelet
Node 3
Kubelet
Node 4
Kubelet
Node 5
Kubelet
...
Node N
Kubelet
Load
balancer
Master 3
etcd
API server
Scheduler
Controller
Manager
[standing-by]
[standing-by]
Master 2
etcd
API server
Scheduler
Controller
Manager
[standing-by]
[standing-by]
Master 1
etcd
API server
Scheduler
Controller
Manager
[active]
[active]
Figure 11.18
A highly-available cluster with three master nodes
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="375">
  <data key="d0">Page_375</data>
  <data key="d5">Page_375</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_286">
  <data key="d0">343
Running highly available clusters
node failures, respectively. Having more than seven etcd instances is almost never nec-
essary and begins impacting performance.
RUNNING MULTIPLE INSTANCES OF THE API SERVER
Making the API server highly available is even simpler. Because the API server is (almost
completely) stateless (all the data is stored in etcd, but the API server does cache it), you
can run as many API servers as you need, and they don’t need to be aware of each other
at all. Usually, one API server is collocated with every etcd instance. By doing this, the
etcd instances don’t need any kind of load balancer in front of them, because every API
server instance only talks to the local etcd instance. 
 The API servers, on the other hand, do need to be fronted by a load balancer, so
clients (kubectl, but also the Controller Manager, Scheduler, and all the Kubelets)
always connect only to the healthy API server instances. 
ENSURING HIGH AVAILABILITY OF THE CONTROLLERS AND THE SCHEDULER
Compared to the API server, where multiple replicas can run simultaneously, run-
ning multiple instances of the Controller Manager or the Scheduler isn’t as simple.
Because controllers and the Scheduler all actively watch the cluster state and act when
it changes, possibly modifying the cluster state further (for example, when the desired
replica count on a ReplicaSet is increased by one, the ReplicaSet controller creates an
additional pod), running multiple instances of each of those components would
result in all of them performing the same action. They’d be racing each other, which
could cause undesired effects (creating two new pods instead of one, as mentioned in
the previous example).
 For this reason, when running multiple instances of these components, only one
instance may be active at any given time. Luckily, this is all taken care of by the compo-
nents themselves (this is controlled with the --leader-elect option, which defaults to
true). Each individual component will only be active when it’s the elected leader. Only
the leader performs actual work, whereas all other instances are standing by and waiting
for the current leader to fail. When it does, the remaining instances elect a new leader,
which then takes over the work. This mechanism ensures that two components are never
operating at the same time and doing the same work (see figure 11.19).
Master 3
Scheduler
Controller
Manager
[standing-by]
[standing-by]
Master 1
Scheduler
Controller
Manager
[active]
[active]
Master 2
Scheduler
Controller
Manager
[standing-by]
[standing-by]
Only the controllers in
this Controller Manager
are reacting to API
resources being created,
updated, and deleted.
These Controller Managers
and Schedulers aren’t doing
anything except waiting to
become leaders.
Only this Scheduler
is scheduling pods.
Figure 11.19
Only a single Controller Manager and a single Scheduler are active; others are standing by.
 
</data>
  <data key="d5">343
Running highly available clusters
node failures, respectively. Having more than seven etcd instances is almost never nec-
essary and begins impacting performance.
RUNNING MULTIPLE INSTANCES OF THE API SERVER
Making the API server highly available is even simpler. Because the API server is (almost
completely) stateless (all the data is stored in etcd, but the API server does cache it), you
can run as many API servers as you need, and they don’t need to be aware of each other
at all. Usually, one API server is collocated with every etcd instance. By doing this, the
etcd instances don’t need any kind of load balancer in front of them, because every API
server instance only talks to the local etcd instance. 
 The API servers, on the other hand, do need to be fronted by a load balancer, so
clients (kubectl, but also the Controller Manager, Scheduler, and all the Kubelets)
always connect only to the healthy API server instances. 
ENSURING HIGH AVAILABILITY OF THE CONTROLLERS AND THE SCHEDULER
Compared to the API server, where multiple replicas can run simultaneously, run-
ning multiple instances of the Controller Manager or the Scheduler isn’t as simple.
Because controllers and the Scheduler all actively watch the cluster state and act when
it changes, possibly modifying the cluster state further (for example, when the desired
replica count on a ReplicaSet is increased by one, the ReplicaSet controller creates an
additional pod), running multiple instances of each of those components would
result in all of them performing the same action. They’d be racing each other, which
could cause undesired effects (creating two new pods instead of one, as mentioned in
the previous example).
 For this reason, when running multiple instances of these components, only one
instance may be active at any given time. Luckily, this is all taken care of by the compo-
nents themselves (this is controlled with the --leader-elect option, which defaults to
true). Each individual component will only be active when it’s the elected leader. Only
the leader performs actual work, whereas all other instances are standing by and waiting
for the current leader to fail. When it does, the remaining instances elect a new leader,
which then takes over the work. This mechanism ensures that two components are never
operating at the same time and doing the same work (see figure 11.19).
Master 3
Scheduler
Controller
Manager
[standing-by]
[standing-by]
Master 1
Scheduler
Controller
Manager
[active]
[active]
Master 2
Scheduler
Controller
Manager
[standing-by]
[standing-by]
Only the controllers in
this Controller Manager
are reacting to API
resources being created,
updated, and deleted.
These Controller Managers
and Schedulers aren’t doing
anything except waiting to
become leaders.
Only this Scheduler
is scheduling pods.
Figure 11.19
Only a single Controller Manager and a single Scheduler are active; others are standing by.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="376">
  <data key="d0">Page_376</data>
  <data key="d5">Page_376</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_287">
  <data key="d0">344
CHAPTER 11
Understanding Kubernetes internals
The Controller Manager and Scheduler can run collocated with the API server and
etcd, or they can run on separate machines. When collocated, they can talk to the
local API server directly; otherwise they connect to the API servers through the load
balancer.
UNDERSTANDING THE LEADER ELECTION MECHANISM USED IN CONTROL PLANE COMPONENTS
What I find most interesting here is that these components don’t need to talk to each
other directly to elect a leader. The leader election mechanism works purely by creat-
ing a resource in the API server. And it’s not even a special kind of resource—the End-
points resource is used to achieve this (abused is probably a more appropriate term).
 There’s nothing special about using an Endpoints object to do this. It’s used
because it has no side effects as long as no Service with the same name exists. Any
other resource could be used (in fact, the leader election mechanism will soon use
ConfigMaps instead of Endpoints). 
 I’m sure you’re interested in how a resource can be used for this purpose. Let’s
take the Scheduler, for example. All instances of the Scheduler try to create (and later
update) an Endpoints resource called kube-scheduler. You’ll find it in the kube-
system namespace, as the following listing shows.
$ kubectl get endpoints kube-scheduler -n kube-system -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":
      ➥ "minikube","leaseDurationSeconds":15,"acquireTime":
      ➥ "2017-05-27T18:54:53Z","renewTime":"2017-05-28T13:07:49Z",
      ➥ "leaderTransitions":0}'
  creationTimestamp: 2017-05-27T18:54:53Z
  name: kube-scheduler
  namespace: kube-system
  resourceVersion: "654059"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
  uid: f847bd14-430d-11e7-9720-080027f8fa4e
subsets: []
The control-plane.alpha.kubernetes.io/leader annotation is the important part.
As you can see, it contains a field called holderIdentity, which holds the name of the
current leader. The first instance that succeeds in putting its name there becomes
the leader. Instances race each other to do that, but there’s always only one winner.
 Remember the optimistic concurrency we explained earlier? That’s what ensures
that if multiple instances try to write their name into the resource only one of them
succeeds. Based on whether the write succeeded or not, each instance knows whether
it is or it isn’t the leader. 
 Once becoming the leader, it must periodically update the resource (every two sec-
onds by default), so all other instances know that it’s still alive. When the leader fails,
Listing 11.11
The kube-scheduler Endpoints resource used for leader-election
 
</data>
  <data key="d5">344
CHAPTER 11
Understanding Kubernetes internals
The Controller Manager and Scheduler can run collocated with the API server and
etcd, or they can run on separate machines. When collocated, they can talk to the
local API server directly; otherwise they connect to the API servers through the load
balancer.
UNDERSTANDING THE LEADER ELECTION MECHANISM USED IN CONTROL PLANE COMPONENTS
What I find most interesting here is that these components don’t need to talk to each
other directly to elect a leader. The leader election mechanism works purely by creat-
ing a resource in the API server. And it’s not even a special kind of resource—the End-
points resource is used to achieve this (abused is probably a more appropriate term).
 There’s nothing special about using an Endpoints object to do this. It’s used
because it has no side effects as long as no Service with the same name exists. Any
other resource could be used (in fact, the leader election mechanism will soon use
ConfigMaps instead of Endpoints). 
 I’m sure you’re interested in how a resource can be used for this purpose. Let’s
take the Scheduler, for example. All instances of the Scheduler try to create (and later
update) an Endpoints resource called kube-scheduler. You’ll find it in the kube-
system namespace, as the following listing shows.
$ kubectl get endpoints kube-scheduler -n kube-system -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":
      ➥ "minikube","leaseDurationSeconds":15,"acquireTime":
      ➥ "2017-05-27T18:54:53Z","renewTime":"2017-05-28T13:07:49Z",
      ➥ "leaderTransitions":0}'
  creationTimestamp: 2017-05-27T18:54:53Z
  name: kube-scheduler
  namespace: kube-system
  resourceVersion: "654059"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
  uid: f847bd14-430d-11e7-9720-080027f8fa4e
subsets: []
The control-plane.alpha.kubernetes.io/leader annotation is the important part.
As you can see, it contains a field called holderIdentity, which holds the name of the
current leader. The first instance that succeeds in putting its name there becomes
the leader. Instances race each other to do that, but there’s always only one winner.
 Remember the optimistic concurrency we explained earlier? That’s what ensures
that if multiple instances try to write their name into the resource only one of them
succeeds. Based on whether the write succeeded or not, each instance knows whether
it is or it isn’t the leader. 
 Once becoming the leader, it must periodically update the resource (every two sec-
onds by default), so all other instances know that it’s still alive. When the leader fails,
Listing 11.11
The kube-scheduler Endpoints resource used for leader-election
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="377">
  <data key="d0">Page_377</data>
  <data key="d5">Page_377</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_288">
  <data key="d0">345
Summary
other instances see that the resource hasn’t been updated for a while, and try to become
the leader by writing their own name to the resource. Simple, right?
11.7
Summary
Hopefully, this has been an interesting chapter that has improved your knowledge of
the inner workings of Kubernetes. This chapter has shown you
What components make up a Kubernetes cluster and what each component is
responsible for
How the API server, Scheduler, various controllers running in the Controller
Manager, and the Kubelet work together to bring a pod to life
How the infrastructure container binds together all the containers of a pod
How pods communicate with other pods running on the same node through
the network bridge, and how those bridges on different nodes are connected,
so pods running on different nodes can talk to each other
How the kube-proxy performs load balancing across pods in the same service by
configuring iptables rules on the node
How multiple instances of each component of the Control Plane can be run to
make the cluster highly available
Next, we’ll look at how to secure the API server and, by extension, the cluster as a whole.
 
</data>
  <data key="d5">345
Summary
other instances see that the resource hasn’t been updated for a while, and try to become
the leader by writing their own name to the resource. Simple, right?
11.7
Summary
Hopefully, this has been an interesting chapter that has improved your knowledge of
the inner workings of Kubernetes. This chapter has shown you
What components make up a Kubernetes cluster and what each component is
responsible for
How the API server, Scheduler, various controllers running in the Controller
Manager, and the Kubelet work together to bring a pod to life
How the infrastructure container binds together all the containers of a pod
How pods communicate with other pods running on the same node through
the network bridge, and how those bridges on different nodes are connected,
so pods running on different nodes can talk to each other
How the kube-proxy performs load balancing across pods in the same service by
configuring iptables rules on the node
How multiple instances of each component of the Control Plane can be run to
make the cluster highly available
Next, we’ll look at how to secure the API server and, by extension, the cluster as a whole.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="378">
  <data key="d0">Page_378</data>
  <data key="d5">Page_378</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_289">
  <data key="d0">346
Securing the
Kubernetes API server
In chapter 8 you learned how applications running in pods can talk to the API
server to retrieve or change the state of resources deployed in the cluster. To
authenticate with the API server, you used the ServiceAccount token mounted into
the pod. In this chapter, you’ll learn what ServiceAccounts are and how to config-
ure their permissions, as well as permissions for other subjects using the cluster. 
12.1
Understanding authentication
In the previous chapter, we said the API server can be configured with one or more
authentication plugins (and the same is true for authorization plugins). When a
request is received by the API server, it goes through the list of authentication
This chapter covers
Understanding authentication
What ServiceAccounts are and why they’re used
Understanding the role-based access control 
(RBAC) plugin
Using Roles and RoleBindings
Using ClusterRoles and ClusterRoleBindings
Understanding the default roles and bindings
 
</data>
  <data key="d5">346
Securing the
Kubernetes API server
In chapter 8 you learned how applications running in pods can talk to the API
server to retrieve or change the state of resources deployed in the cluster. To
authenticate with the API server, you used the ServiceAccount token mounted into
the pod. In this chapter, you’ll learn what ServiceAccounts are and how to config-
ure their permissions, as well as permissions for other subjects using the cluster. 
12.1
Understanding authentication
In the previous chapter, we said the API server can be configured with one or more
authentication plugins (and the same is true for authorization plugins). When a
request is received by the API server, it goes through the list of authentication
This chapter covers
Understanding authentication
What ServiceAccounts are and why they’re used
Understanding the role-based access control 
(RBAC) plugin
Using Roles and RoleBindings
Using ClusterRoles and ClusterRoleBindings
Understanding the default roles and bindings
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="379">
  <data key="d0">Page_379</data>
  <data key="d5">Page_379</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_290">
  <data key="d0">347
Understanding authentication
plugins, so they can each examine the request and try to determine who’s sending the
request. The first plugin that can extract that information from the request returns
the username, user ID, and the groups the client belongs to back to the API server
core. The API server stops invoking the remaining authentication plugins and contin-
ues onto the authorization phase. 
 Several authentication plugins are available. They obtain the identity of the client
using the following methods:
From the client certificate
From an authentication token passed in an HTTP header
Basic HTTP authentication
Others
The authentication plugins are enabled through command-line options when starting
the API server. 
12.1.1 Users and groups
An authentication plugin returns the username and group(s) of the authenticated
user. Kubernetes doesn’t store that information anywhere; it uses it to verify whether
the user is authorized to perform an action or not.
UNDERSTANDING USERS
Kubernetes distinguishes between two kinds of clients connecting to the API server:
Actual humans (users)
Pods (more specifically, applications running inside them)
Both these types of clients are authenticated using the aforementioned authentication
plugins. Users are meant to be managed by an external system, such as a Single Sign
On (SSO) system, but the pods use a mechanism called service accounts, which are cre-
ated and stored in the cluster as ServiceAccount resources. In contrast, no resource
represents user accounts, which means you can’t create, update, or delete users through
the API server. 
 We won’t go into any details of how to manage users, but we will explore Service-
Accounts in detail, because they’re essential for running pods. For more informa-
tion on how to configure the cluster for authentication of human users, cluster
administrators should refer to the Kubernetes Cluster Administrator guide at http:/
/
kubernetes.io/docs/admin.
UNDERSTANDING GROUPS
Both human users and ServiceAccounts can belong to one or more groups. We’ve said
that the authentication plugin returns groups along with the username and user ID.
Groups are used to grant permissions to several users at once, instead of having to
grant them to individual users. 
 
</data>
  <data key="d5">347
Understanding authentication
plugins, so they can each examine the request and try to determine who’s sending the
request. The first plugin that can extract that information from the request returns
the username, user ID, and the groups the client belongs to back to the API server
core. The API server stops invoking the remaining authentication plugins and contin-
ues onto the authorization phase. 
 Several authentication plugins are available. They obtain the identity of the client
using the following methods:
From the client certificate
From an authentication token passed in an HTTP header
Basic HTTP authentication
Others
The authentication plugins are enabled through command-line options when starting
the API server. 
12.1.1 Users and groups
An authentication plugin returns the username and group(s) of the authenticated
user. Kubernetes doesn’t store that information anywhere; it uses it to verify whether
the user is authorized to perform an action or not.
UNDERSTANDING USERS
Kubernetes distinguishes between two kinds of clients connecting to the API server:
Actual humans (users)
Pods (more specifically, applications running inside them)
Both these types of clients are authenticated using the aforementioned authentication
plugins. Users are meant to be managed by an external system, such as a Single Sign
On (SSO) system, but the pods use a mechanism called service accounts, which are cre-
ated and stored in the cluster as ServiceAccount resources. In contrast, no resource
represents user accounts, which means you can’t create, update, or delete users through
the API server. 
 We won’t go into any details of how to manage users, but we will explore Service-
Accounts in detail, because they’re essential for running pods. For more informa-
tion on how to configure the cluster for authentication of human users, cluster
administrators should refer to the Kubernetes Cluster Administrator guide at http:/
/
kubernetes.io/docs/admin.
UNDERSTANDING GROUPS
Both human users and ServiceAccounts can belong to one or more groups. We’ve said
that the authentication plugin returns groups along with the username and user ID.
Groups are used to grant permissions to several users at once, instead of having to
grant them to individual users. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="380">
  <data key="d0">Page_380</data>
  <data key="d5">Page_380</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_291">
  <data key="d0">348
CHAPTER 12
Securing the Kubernetes API server
 Groups returned by the plugin are nothing but strings, representing arbitrary
group names, but built-in groups have special meaning:
The system:unauthenticated group is used for requests where none of the
authentication plugins could authenticate the client.
The system:authenticated group is automatically assigned to a user who was
authenticated successfully.
The system:serviceaccounts group encompasses all ServiceAccounts in the
system.
The system:serviceaccounts:&lt;namespace&gt; includes all ServiceAccounts in a
specific namespace.
12.1.2 Introducing ServiceAccounts
Let’s explore ServiceAccounts up close. You’ve already learned that the API server
requires clients to authenticate themselves before they’re allowed to perform opera-
tions on the server. And you’ve already seen how pods can authenticate by sending the
contents of the file/var/run/secrets/kubernetes.io/serviceaccount/token, which
is mounted into each container’s filesystem through a secret volume.
 But what exactly does that file represent? Every pod is associated with a Service-
Account, which represents the identity of the app running in the pod. The token file
holds the ServiceAccount’s authentication token. When an app uses this token to con-
nect to the API server, the authentication plugin authenticates the ServiceAccount
and passes the ServiceAccount’s username back to the API server core. Service-
Account usernames are formatted like this:
system:serviceaccount:&lt;namespace&gt;:&lt;service account name&gt;
The API server passes this username to the configured authorization plugins, which
determine whether the action the app is trying to perform is allowed to be performed
by the ServiceAccount.
 ServiceAccounts are nothing more than a way for an application running inside a
pod to authenticate itself with the API server. As already mentioned, applications do
that by passing the ServiceAccount’s token in the request.
UNDERSTANDING THE SERVICEACCOUNT RESOURCE
ServiceAccounts are resources just like Pods, Secrets, ConfigMaps, and so on, and are
scoped to individual namespaces. A default ServiceAccount is automatically created
for each namespace (that’s the one your pods have used all along). 
 You can list ServiceAccounts like you do other resources:
$ kubectl get sa
NAME      SECRETS   AGE
default   1         1d
NOTE
The shorthand for serviceaccount is sa.
 
</data>
  <data key="d5">348
CHAPTER 12
Securing the Kubernetes API server
 Groups returned by the plugin are nothing but strings, representing arbitrary
group names, but built-in groups have special meaning:
The system:unauthenticated group is used for requests where none of the
authentication plugins could authenticate the client.
The system:authenticated group is automatically assigned to a user who was
authenticated successfully.
The system:serviceaccounts group encompasses all ServiceAccounts in the
system.
The system:serviceaccounts:&lt;namespace&gt; includes all ServiceAccounts in a
specific namespace.
12.1.2 Introducing ServiceAccounts
Let’s explore ServiceAccounts up close. You’ve already learned that the API server
requires clients to authenticate themselves before they’re allowed to perform opera-
tions on the server. And you’ve already seen how pods can authenticate by sending the
contents of the file/var/run/secrets/kubernetes.io/serviceaccount/token, which
is mounted into each container’s filesystem through a secret volume.
 But what exactly does that file represent? Every pod is associated with a Service-
Account, which represents the identity of the app running in the pod. The token file
holds the ServiceAccount’s authentication token. When an app uses this token to con-
nect to the API server, the authentication plugin authenticates the ServiceAccount
and passes the ServiceAccount’s username back to the API server core. Service-
Account usernames are formatted like this:
system:serviceaccount:&lt;namespace&gt;:&lt;service account name&gt;
The API server passes this username to the configured authorization plugins, which
determine whether the action the app is trying to perform is allowed to be performed
by the ServiceAccount.
 ServiceAccounts are nothing more than a way for an application running inside a
pod to authenticate itself with the API server. As already mentioned, applications do
that by passing the ServiceAccount’s token in the request.
UNDERSTANDING THE SERVICEACCOUNT RESOURCE
ServiceAccounts are resources just like Pods, Secrets, ConfigMaps, and so on, and are
scoped to individual namespaces. A default ServiceAccount is automatically created
for each namespace (that’s the one your pods have used all along). 
 You can list ServiceAccounts like you do other resources:
$ kubectl get sa
NAME      SECRETS   AGE
default   1         1d
NOTE
The shorthand for serviceaccount is sa.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="381">
  <data key="d0">Page_381</data>
  <data key="d5">Page_381</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_292">
  <data key="d0">349
Understanding authentication
As you can see, the current namespace only contains the default ServiceAccount. Addi-
tional ServiceAccounts can be added when required. Each pod is associated with exactly
one ServiceAccount, but multiple pods can use the same ServiceAccount. As you can
see in figure 12.1, a pod can only use a ServiceAccount from the same namespace.
UNDERSTANDING HOW SERVICEACCOUNTS TIE INTO AUTHORIZATION
You can assign a ServiceAccount to a pod by specifying the account’s name in the pod
manifest. If you don’t assign it explicitly, the pod will use the default ServiceAccount
in the namespace.
 By assigning different ServiceAccounts to pods, you can control which resources
each pod has access to. When a request bearing the authentication token is received
by the API server, the server uses the token to authenticate the client sending the
request and then determines whether or not the related ServiceAccount is allowed to
perform the requested operation. The API server obtains this information from the
system-wide authorization plugin configured by the cluster administrator. One of
the available authorization plugins is the role-based access control (RBAC) plugin,
which is discussed later in this chapter. From Kubernetes version 1.6 on, the RBAC
plugin is the plugin most clusters should use.
12.1.3 Creating ServiceAccounts
We’ve said every namespace contains its own default ServiceAccount, but additional
ones can be created if necessary. But why should you bother with creating Service-
Accounts instead of using the default one for all your pods? 
 The obvious reason is cluster security. Pods that don’t need to read any cluster
metadata should run under a constrained account that doesn’t allow them to retrieve
or modify any resources deployed in the cluster. Pods that need to retrieve resource
metadata should run under a ServiceAccount that only allows reading those objects’
metadata, whereas pods that need to modify those objects should run under their own
ServiceAccount allowing modifications of API objects. 
Pod
Namespace: foo
Service-
Account:
default
Pod
Pod
Namespace: baz
Pod
Namespace: bar
Pod
Pod
Not possible
Service-
Account:
default
Another
Service-
Account
Multiple pods using the
same ServiceAccount
Figure 12.1
Each pod is associated with a single ServiceAccount in the pod’s namespace.
 
</data>
  <data key="d5">349
Understanding authentication
As you can see, the current namespace only contains the default ServiceAccount. Addi-
tional ServiceAccounts can be added when required. Each pod is associated with exactly
one ServiceAccount, but multiple pods can use the same ServiceAccount. As you can
see in figure 12.1, a pod can only use a ServiceAccount from the same namespace.
UNDERSTANDING HOW SERVICEACCOUNTS TIE INTO AUTHORIZATION
You can assign a ServiceAccount to a pod by specifying the account’s name in the pod
manifest. If you don’t assign it explicitly, the pod will use the default ServiceAccount
in the namespace.
 By assigning different ServiceAccounts to pods, you can control which resources
each pod has access to. When a request bearing the authentication token is received
by the API server, the server uses the token to authenticate the client sending the
request and then determines whether or not the related ServiceAccount is allowed to
perform the requested operation. The API server obtains this information from the
system-wide authorization plugin configured by the cluster administrator. One of
the available authorization plugins is the role-based access control (RBAC) plugin,
which is discussed later in this chapter. From Kubernetes version 1.6 on, the RBAC
plugin is the plugin most clusters should use.
12.1.3 Creating ServiceAccounts
We’ve said every namespace contains its own default ServiceAccount, but additional
ones can be created if necessary. But why should you bother with creating Service-
Accounts instead of using the default one for all your pods? 
 The obvious reason is cluster security. Pods that don’t need to read any cluster
metadata should run under a constrained account that doesn’t allow them to retrieve
or modify any resources deployed in the cluster. Pods that need to retrieve resource
metadata should run under a ServiceAccount that only allows reading those objects’
metadata, whereas pods that need to modify those objects should run under their own
ServiceAccount allowing modifications of API objects. 
Pod
Namespace: foo
Service-
Account:
default
Pod
Pod
Namespace: baz
Pod
Namespace: bar
Pod
Pod
Not possible
Service-
Account:
default
Another
Service-
Account
Multiple pods using the
same ServiceAccount
Figure 12.1
Each pod is associated with a single ServiceAccount in the pod’s namespace.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="382">
  <data key="d0">Page_382</data>
  <data key="d5">Page_382</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_293">
  <data key="d0">350
CHAPTER 12
Securing the Kubernetes API server
 Let’s see how you can create additional ServiceAccounts, how they relate to Secrets,
and how you can assign them to your pods.
CREATING A SERVICEACCOUNT
Creating a ServiceAccount is incredibly easy, thanks to the dedicated kubectl create
serviceaccount command. Let’s create a new ServiceAccount called foo:
$ kubectl create serviceaccount foo
serviceaccount "foo" created
Now, you can inspect the ServiceAccount with the describe command, as shown in
the following listing.
$ kubectl describe sa foo
Name:               foo
Namespace:          default
Labels:             &lt;none&gt;
Image pull secrets: &lt;none&gt;             
Mountable secrets:  foo-token-qzq7j    
Tokens:             foo-token-qzq7j    
You can see that a custom token Secret has been created and associated with the
ServiceAccount. If you look at the Secret’s data with kubectl describe secret foo-
token-qzq7j, you’ll see it contains the same items (the CA certificate, namespace, and
token) as the default ServiceAccount’s token does (the token itself will obviously be
different), as shown in the following listing.
$ kubectl describe secret foo-token-qzq7j
...
ca.crt:         1066 bytes
namespace:      7 bytes
token:          eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...
NOTE
You’ve probably heard of JSON Web Tokens (JWT). The authentica-
tion tokens used in ServiceAccounts are JWT tokens.
UNDERSTANDING A SERVICEACCOUNT’S MOUNTABLE SECRETS
The token is shown in the Mountable secrets list when you inspect a ServiceAccount
with kubectl describe. Let me explain what that list represents. In chapter 7 you
learned how to create Secrets and mount them inside a pod. By default, a pod can
mount any Secret it wants. But the pod’s ServiceAccount can be configured to only
Listing 12.1
Inspecting a ServiceAccount with kubectl describe
Listing 12.2
Inspecting the custom ServiceAccount’s Secret
These will be added 
automatically to all pods 
using this ServiceAccount.
Pods using this ServiceAccount 
can only mount these Secrets if 
mountable Secrets are enforced.
Authentication token(s). 
The first one is mounted 
inside the container.
 
</data>
  <data key="d5">350
CHAPTER 12
Securing the Kubernetes API server
 Let’s see how you can create additional ServiceAccounts, how they relate to Secrets,
and how you can assign them to your pods.
CREATING A SERVICEACCOUNT
Creating a ServiceAccount is incredibly easy, thanks to the dedicated kubectl create
serviceaccount command. Let’s create a new ServiceAccount called foo:
$ kubectl create serviceaccount foo
serviceaccount "foo" created
Now, you can inspect the ServiceAccount with the describe command, as shown in
the following listing.
$ kubectl describe sa foo
Name:               foo
Namespace:          default
Labels:             &lt;none&gt;
Image pull secrets: &lt;none&gt;             
Mountable secrets:  foo-token-qzq7j    
Tokens:             foo-token-qzq7j    
You can see that a custom token Secret has been created and associated with the
ServiceAccount. If you look at the Secret’s data with kubectl describe secret foo-
token-qzq7j, you’ll see it contains the same items (the CA certificate, namespace, and
token) as the default ServiceAccount’s token does (the token itself will obviously be
different), as shown in the following listing.
$ kubectl describe secret foo-token-qzq7j
...
ca.crt:         1066 bytes
namespace:      7 bytes
token:          eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...
NOTE
You’ve probably heard of JSON Web Tokens (JWT). The authentica-
tion tokens used in ServiceAccounts are JWT tokens.
UNDERSTANDING A SERVICEACCOUNT’S MOUNTABLE SECRETS
The token is shown in the Mountable secrets list when you inspect a ServiceAccount
with kubectl describe. Let me explain what that list represents. In chapter 7 you
learned how to create Secrets and mount them inside a pod. By default, a pod can
mount any Secret it wants. But the pod’s ServiceAccount can be configured to only
Listing 12.1
Inspecting a ServiceAccount with kubectl describe
Listing 12.2
Inspecting the custom ServiceAccount’s Secret
These will be added 
automatically to all pods 
using this ServiceAccount.
Pods using this ServiceAccount 
can only mount these Secrets if 
mountable Secrets are enforced.
Authentication token(s). 
The first one is mounted 
inside the container.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="383">
  <data key="d0">Page_383</data>
  <data key="d5">Page_383</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_294">
  <data key="d0">351
Understanding authentication
allow the pod to mount Secrets that are listed as mountable Secrets on the Service-
Account. To enable this feature, the ServiceAccount must contain the following anno-
tation: kubernetes.io/enforce-mountable-secrets="true". 
 If the ServiceAccount is annotated with this annotation, any pods using it can mount
only the ServiceAccount’s mountable Secrets—they can’t use any other Secret.
UNDERSTANDING A SERVICEACCOUNT’S IMAGE PULL SECRETS
A ServiceAccount can also contain a list of image pull Secrets, which we examined in
chapter 7. In case you don’t remember, they are Secrets that hold the credentials for
pulling container images from a private image repository. 
 The following listing shows an example of a ServiceAccount definition, which
includes the image pull Secret you created in chapter 7.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-service-account
imagePullSecrets:
- name: my-dockerhub-secret
A ServiceAccount’s image pull Secrets behave slightly differently than its mountable
Secrets. Unlike mountable Secrets, they don’t determine which image pull Secrets a
pod can use, but which ones are added automatically to all pods using the Service-
Account. Adding image pull Secrets to a ServiceAccount saves you from having to add
them to each pod individually. 
12.1.4 Assigning a ServiceAccount to a pod
After you create additional ServiceAccounts, you need to assign them to pods. This is
done by setting the name of the ServiceAccount in the spec.serviceAccountName
field in the pod definition. 
NOTE
A pod’s ServiceAccount must be set when creating the pod. It can’t be
changed later. 
CREATING A POD WHICH USES A CUSTOM SERVICEACCOUNT
In chapter 8 you deployed a pod that ran a container based on the tutum/curl image
and an ambassador container alongside it. You used it to explore the API server’s
REST interface. The ambassador container ran the kubectl proxy process, which
used the pod’s ServiceAccount’s token to authenticate with the API server. 
 You can now modify the pod so it uses the foo ServiceAccount you created minutes
ago. The next listing shows the pod definition.
 
 
Listing 12.3
ServiceAccount with an image pull Secret: sa-image-pull-secrets.yaml
 
</data>
  <data key="d5">351
Understanding authentication
allow the pod to mount Secrets that are listed as mountable Secrets on the Service-
Account. To enable this feature, the ServiceAccount must contain the following anno-
tation: kubernetes.io/enforce-mountable-secrets="true". 
 If the ServiceAccount is annotated with this annotation, any pods using it can mount
only the ServiceAccount’s mountable Secrets—they can’t use any other Secret.
UNDERSTANDING A SERVICEACCOUNT’S IMAGE PULL SECRETS
A ServiceAccount can also contain a list of image pull Secrets, which we examined in
chapter 7. In case you don’t remember, they are Secrets that hold the credentials for
pulling container images from a private image repository. 
 The following listing shows an example of a ServiceAccount definition, which
includes the image pull Secret you created in chapter 7.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-service-account
imagePullSecrets:
- name: my-dockerhub-secret
A ServiceAccount’s image pull Secrets behave slightly differently than its mountable
Secrets. Unlike mountable Secrets, they don’t determine which image pull Secrets a
pod can use, but which ones are added automatically to all pods using the Service-
Account. Adding image pull Secrets to a ServiceAccount saves you from having to add
them to each pod individually. 
12.1.4 Assigning a ServiceAccount to a pod
After you create additional ServiceAccounts, you need to assign them to pods. This is
done by setting the name of the ServiceAccount in the spec.serviceAccountName
field in the pod definition. 
NOTE
A pod’s ServiceAccount must be set when creating the pod. It can’t be
changed later. 
CREATING A POD WHICH USES A CUSTOM SERVICEACCOUNT
In chapter 8 you deployed a pod that ran a container based on the tutum/curl image
and an ambassador container alongside it. You used it to explore the API server’s
REST interface. The ambassador container ran the kubectl proxy process, which
used the pod’s ServiceAccount’s token to authenticate with the API server. 
 You can now modify the pod so it uses the foo ServiceAccount you created minutes
ago. The next listing shows the pod definition.
 
 
Listing 12.3
ServiceAccount with an image pull Secret: sa-image-pull-secrets.yaml
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="384">
  <data key="d0">Page_384</data>
  <data key="d5">Page_384</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_295">
  <data key="d0">352
CHAPTER 12
Securing the Kubernetes API server
apiVersion: v1
kind: Pod
metadata:
  name: curl-custom-sa
spec:
  serviceAccountName: foo           
  containers:
  - name: main
    image: tutum/curl
    command: ["sleep", "9999999"]
  - name: ambassador                  
    image: luksa/kubectl-proxy:1.6.2
To confirm that the custom ServiceAccount’s token is mounted into the two contain-
ers, you can print the contents of the token as shown in the following listing.
$ kubectl exec -it curl-custom-sa -c main 
➥ cat /var/run/secrets/kubernetes.io/serviceaccount/token
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...
You can see the token is the one from the foo ServiceAccount by comparing the token
string in listing 12.5 with the one in listing 12.2. 
USING THE CUSTOM SERVICEACCOUNT’S TOKEN TO TALK TO THE API SERVER
Let’s see if you can talk to the API server using this token. As mentioned previously,
the ambassador container uses the token when talking to the server, so you can test
the token by going through the ambassador, which listens on localhost:8001, as
shown in the following listing.
$ kubectl exec -it curl-custom-sa -c main curl localhost:8001/api/v1/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  "metadata": {
    "selfLink": "/api/v1/pods",
    "resourceVersion": "433895"
  },
  "items": [
  ...
Okay, you got back a proper response from the server, which means the custom
ServiceAccount is allowed to list pods. This may be because your cluster doesn’t use
the RBAC authorization plugin, or you gave all ServiceAccounts full permissions, as
instructed in chapter 8. 
Listing 12.4
Pod using a non-default ServiceAccount: curl-custom-sa.yaml
Listing 12.5
Inspecting the token mounted into the pod’s container(s)
Listing 12.6
Talking to the API server with a custom ServiceAccount
This pod uses the 
foo ServiceAccount 
instead of the default.
 
</data>
  <data key="d5">352
CHAPTER 12
Securing the Kubernetes API server
apiVersion: v1
kind: Pod
metadata:
  name: curl-custom-sa
spec:
  serviceAccountName: foo           
  containers:
  - name: main
    image: tutum/curl
    command: ["sleep", "9999999"]
  - name: ambassador                  
    image: luksa/kubectl-proxy:1.6.2
To confirm that the custom ServiceAccount’s token is mounted into the two contain-
ers, you can print the contents of the token as shown in the following listing.
$ kubectl exec -it curl-custom-sa -c main 
➥ cat /var/run/secrets/kubernetes.io/serviceaccount/token
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...
You can see the token is the one from the foo ServiceAccount by comparing the token
string in listing 12.5 with the one in listing 12.2. 
USING THE CUSTOM SERVICEACCOUNT’S TOKEN TO TALK TO THE API SERVER
Let’s see if you can talk to the API server using this token. As mentioned previously,
the ambassador container uses the token when talking to the server, so you can test
the token by going through the ambassador, which listens on localhost:8001, as
shown in the following listing.
$ kubectl exec -it curl-custom-sa -c main curl localhost:8001/api/v1/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  "metadata": {
    "selfLink": "/api/v1/pods",
    "resourceVersion": "433895"
  },
  "items": [
  ...
Okay, you got back a proper response from the server, which means the custom
ServiceAccount is allowed to list pods. This may be because your cluster doesn’t use
the RBAC authorization plugin, or you gave all ServiceAccounts full permissions, as
instructed in chapter 8. 
Listing 12.4
Pod using a non-default ServiceAccount: curl-custom-sa.yaml
Listing 12.5
Inspecting the token mounted into the pod’s container(s)
Listing 12.6
Talking to the API server with a custom ServiceAccount
This pod uses the 
foo ServiceAccount 
instead of the default.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="385">
  <data key="d0">Page_385</data>
  <data key="d5">Page_385</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_296">
  <data key="d0">353
Securing the cluster with role-based access control
 When your cluster isn’t using proper authorization, creating and using additional
ServiceAccounts doesn’t make much sense, since even the default ServiceAccount is
allowed to do anything. The only reason to use ServiceAccounts in that case is to
enforce mountable Secrets or to provide image pull Secrets through the Service-
Account, as explained earlier. 
 But creating additional ServiceAccounts is practically a must when you use the
RBAC authorization plugin, which we’ll explore next.
12.2
Securing the cluster with role-based access control
Starting with Kubernetes version 1.6.0, cluster security was ramped up considerably. In
earlier versions, if you managed to acquire the authentication token from one of the
pods, you could use it to do anything you want in the cluster. If you google around,
you’ll find demos showing how a path traversal (or directory traversal) attack (where clients
can retrieve files located outside of the web server’s web root directory) can be used to
get the token and use it to run your malicious pods in an insecure Kubernetes cluster.
 But in version 1.8.0, the RBAC authorization plugin graduated to GA (General
Availability) and is now enabled by default on many clusters (for example, when
deploying a cluster with kubadm, as described in appendix B). RBAC prevents unau-
thorized users from viewing or modifying the cluster state. The default Service-
Account isn’t allowed to view cluster state, let alone modify it in any way, unless you
grant it additional privileges. To write apps that communicate with the Kubernetes
API server (as described in chapter 8), you need to understand how to manage
authorization through RBAC-specific resources.
NOTE
In addition to RBAC, Kubernetes also includes other authorization
plugins, such as the Attribute-based access control (ABAC) plugin, a Web-
Hook plugin and custom plugin implementations. RBAC is the standard,
though.
12.2.1 Introducing the RBAC authorization plugin
The Kubernetes API server can be configured to use an authorization plugin to check
whether an action is allowed to be performed by the user requesting the action. Because
the API server exposes a REST interface, users perform actions by sending HTTP
requests to the server. Users authenticate themselves by including credentials in the
request (an authentication token, username and password, or a client certificate).
UNDERSTANDING ACTIONS
But what actions are there? As you know, REST clients send GET, POST, PUT, DELETE,
and other types of HTTP requests to specific URL paths, which represent specific
REST resources. In Kubernetes, those resources are Pods, Services, Secrets, and so on.
Here are a few examples of actions in Kubernetes:
Get Pods
Create Services
 
</data>
  <data key="d5">353
Securing the cluster with role-based access control
 When your cluster isn’t using proper authorization, creating and using additional
ServiceAccounts doesn’t make much sense, since even the default ServiceAccount is
allowed to do anything. The only reason to use ServiceAccounts in that case is to
enforce mountable Secrets or to provide image pull Secrets through the Service-
Account, as explained earlier. 
 But creating additional ServiceAccounts is practically a must when you use the
RBAC authorization plugin, which we’ll explore next.
12.2
Securing the cluster with role-based access control
Starting with Kubernetes version 1.6.0, cluster security was ramped up considerably. In
earlier versions, if you managed to acquire the authentication token from one of the
pods, you could use it to do anything you want in the cluster. If you google around,
you’ll find demos showing how a path traversal (or directory traversal) attack (where clients
can retrieve files located outside of the web server’s web root directory) can be used to
get the token and use it to run your malicious pods in an insecure Kubernetes cluster.
 But in version 1.8.0, the RBAC authorization plugin graduated to GA (General
Availability) and is now enabled by default on many clusters (for example, when
deploying a cluster with kubadm, as described in appendix B). RBAC prevents unau-
thorized users from viewing or modifying the cluster state. The default Service-
Account isn’t allowed to view cluster state, let alone modify it in any way, unless you
grant it additional privileges. To write apps that communicate with the Kubernetes
API server (as described in chapter 8), you need to understand how to manage
authorization through RBAC-specific resources.
NOTE
In addition to RBAC, Kubernetes also includes other authorization
plugins, such as the Attribute-based access control (ABAC) plugin, a Web-
Hook plugin and custom plugin implementations. RBAC is the standard,
though.
12.2.1 Introducing the RBAC authorization plugin
The Kubernetes API server can be configured to use an authorization plugin to check
whether an action is allowed to be performed by the user requesting the action. Because
the API server exposes a REST interface, users perform actions by sending HTTP
requests to the server. Users authenticate themselves by including credentials in the
request (an authentication token, username and password, or a client certificate).
UNDERSTANDING ACTIONS
But what actions are there? As you know, REST clients send GET, POST, PUT, DELETE,
and other types of HTTP requests to specific URL paths, which represent specific
REST resources. In Kubernetes, those resources are Pods, Services, Secrets, and so on.
Here are a few examples of actions in Kubernetes:
Get Pods
Create Services
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="386">
  <data key="d0">Page_386</data>
  <data key="d5">Page_386</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_297">
  <data key="d0">354
CHAPTER 12
Securing the Kubernetes API server
Update Secrets
And so on
The verbs in those examples (get, create, update) map to HTTP methods (GET, POST,
PUT) performed by the client (the complete mapping is shown in table 12.1). The
nouns (Pods, Service, Secrets) obviously map to Kubernetes resources. 
 An authorization plugin such as RBAC, which runs inside the API server, deter-
mines whether a client is allowed to perform the requested verb on the requested
resource or not.
NOTE
The additional verb use is used for PodSecurityPolicy resources, which
are explained in the next chapter.
Besides applying security permissions to whole resource types, RBAC rules can also
apply to specific instances of a resource (for example, a Service called myservice).
And later you’ll see that permissions can also apply to non-resource URL paths,
because not every path the API server exposes maps to a resource (such as the /api
path itself or the server health information at /healthz). 
UNDERSTANDING THE RBAC PLUGIN
The RBAC authorization plugin, as the name suggests, uses user roles as the key factor
in determining whether the user may perform the action or not. A subject (which may
be a human, a ServiceAccount, or a group of users or ServiceAccounts) is associated
with one or more roles and each role is allowed to perform certain verbs on certain
resources. 
 If a user has multiple roles, they may do anything that any of their roles allows
them to do. If none of the user’s roles contains a permission to, for example, update
Secrets, the API server will prevent the user from performing PUT or PATCH requests
on Secrets.
 Managing authorization through the RBAC plugin is simple. It’s all done by creat-
ing four RBAC-specific Kubernetes resources, which we’ll look at next.
Table 12.1
Mapping HTTP methods to authorization verbs
HTTP method
Verb for single resource
Verb for collection
GET, HEAD
get (and watch for watching)
list (and watch)
POST
create
n/a
PUT
update
n/a
PATCH
patch
n/a
DELETE
delete
deletecollection
 
</data>
  <data key="d5">354
CHAPTER 12
Securing the Kubernetes API server
Update Secrets
And so on
The verbs in those examples (get, create, update) map to HTTP methods (GET, POST,
PUT) performed by the client (the complete mapping is shown in table 12.1). The
nouns (Pods, Service, Secrets) obviously map to Kubernetes resources. 
 An authorization plugin such as RBAC, which runs inside the API server, deter-
mines whether a client is allowed to perform the requested verb on the requested
resource or not.
NOTE
The additional verb use is used for PodSecurityPolicy resources, which
are explained in the next chapter.
Besides applying security permissions to whole resource types, RBAC rules can also
apply to specific instances of a resource (for example, a Service called myservice).
And later you’ll see that permissions can also apply to non-resource URL paths,
because not every path the API server exposes maps to a resource (such as the /api
path itself or the server health information at /healthz). 
UNDERSTANDING THE RBAC PLUGIN
The RBAC authorization plugin, as the name suggests, uses user roles as the key factor
in determining whether the user may perform the action or not. A subject (which may
be a human, a ServiceAccount, or a group of users or ServiceAccounts) is associated
with one or more roles and each role is allowed to perform certain verbs on certain
resources. 
 If a user has multiple roles, they may do anything that any of their roles allows
them to do. If none of the user’s roles contains a permission to, for example, update
Secrets, the API server will prevent the user from performing PUT or PATCH requests
on Secrets.
 Managing authorization through the RBAC plugin is simple. It’s all done by creat-
ing four RBAC-specific Kubernetes resources, which we’ll look at next.
Table 12.1
Mapping HTTP methods to authorization verbs
HTTP method
Verb for single resource
Verb for collection
GET, HEAD
get (and watch for watching)
list (and watch)
POST
create
n/a
PUT
update
n/a
PATCH
patch
n/a
DELETE
delete
deletecollection
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="387">
  <data key="d0">Page_387</data>
  <data key="d5">Page_387</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_298">
  <data key="d0">355
Securing the cluster with role-based access control
12.2.2 Introducing RBAC resources
The RBAC authorization rules are configured through four resources, which can be
grouped into two groups:
Roles and ClusterRoles, which specify which verbs can be performed on which
resources.
RoleBindings and ClusterRoleBindings, which bind the above roles to specific
users, groups, or ServiceAccounts.
Roles define what can be done, while bindings define who can do it (this is shown in
figure 12.2).
The distinction between a Role and a ClusterRole, or between a RoleBinding and a
ClusterRoleBinding, is that the Role and RoleBinding are namespaced resources,
whereas the ClusterRole and ClusterRoleBinding are cluster-level resources (not
namespaced). This is depicted in figure 12.3.
 As you can see from the figure, multiple RoleBindings can exist in a single name-
space (this is also true for Roles). Likewise, multiple ClusterRoleBindings and Cluster-
Roles can be created. Another thing shown in the figure is that although RoleBindings
are namespaced, they can also reference ClusterRoles, which aren’t. 
 The best way to learn about these four resources and what their effects are is by try-
ing them out in a hands-on exercise. You’ll do that now.
 
 
 
 
What?
Role
Binding
Some
resources
Other
resources
Role
Doesn’t allow
doing anything
with other resources
User A
Who?
Admins group
Allows users
to access
Service-
Account:
x
Figure 12.2
Roles grant permissions, whereas RoleBindings bind Roles to subjects.
 
</data>
  <data key="d5">355
Securing the cluster with role-based access control
12.2.2 Introducing RBAC resources
The RBAC authorization rules are configured through four resources, which can be
grouped into two groups:
Roles and ClusterRoles, which specify which verbs can be performed on which
resources.
RoleBindings and ClusterRoleBindings, which bind the above roles to specific
users, groups, or ServiceAccounts.
Roles define what can be done, while bindings define who can do it (this is shown in
figure 12.2).
The distinction between a Role and a ClusterRole, or between a RoleBinding and a
ClusterRoleBinding, is that the Role and RoleBinding are namespaced resources,
whereas the ClusterRole and ClusterRoleBinding are cluster-level resources (not
namespaced). This is depicted in figure 12.3.
 As you can see from the figure, multiple RoleBindings can exist in a single name-
space (this is also true for Roles). Likewise, multiple ClusterRoleBindings and Cluster-
Roles can be created. Another thing shown in the figure is that although RoleBindings
are namespaced, they can also reference ClusterRoles, which aren’t. 
 The best way to learn about these four resources and what their effects are is by try-
ing them out in a hands-on exercise. You’ll do that now.
 
 
 
 
What?
Role
Binding
Some
resources
Other
resources
Role
Doesn’t allow
doing anything
with other resources
User A
Who?
Admins group
Allows users
to access
Service-
Account:
x
Figure 12.2
Roles grant permissions, whereas RoleBindings bind Roles to subjects.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="388">
  <data key="d0">Page_388</data>
  <data key="d5">Page_388</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_299">
  <data key="d0">356
CHAPTER 12
Securing the Kubernetes API server
SETTING UP YOUR EXERCISE
Before you can explore how RBAC resources affect what you can do through the API
server, you need to make sure RBAC is enabled in your cluster. First, ensure you’re
using at least version 1.6 of Kubernetes and that the RBAC plugin is the only config-
ured authorization plugin. There can be multiple plugins enabled in parallel and if
one of them allows an action to be performed, the action is allowed.
NOTE
If you’re using GKE 1.6 or 1.7, you need to explicitly disable legacy autho-
rization by creating the cluster with the --no-enable-legacy-authorization
option. If you’re using Minikube, you also may need to enable RBAC by start-
ing Minikube with --extra-config=apiserver.Authorization.Mode=RBAC
If you followed the instructions on how to disable RBAC in chapter 8, now’s the time
to re-enable it by running the following command:
$ kubectl delete clusterrolebinding permissive-binding
To try out RBAC, you’ll run a pod through which you’ll try to talk to the API server,
the way you did in chapter 8. But this time you’ll run two pods in different namespaces
to see how per-namespace security behaves.
 In the examples in chapter 8, you ran two containers to demonstrate how an appli-
cation in one container uses the other container to talk to the API server. This time,
you’ll run a single container (based on the kubectl-proxy image) and use kubectl
exec to run curl inside that container directly. The proxy will take care of authentica-
tion and HTTPS, so you can focus on the authorization aspect of API server security.
Namespace C
Namespaced
resources
Cluster-level
resources
RoleBinding
RoleBinding
Role
Namespace B
Namespaced
resources
RoleBinding
Role
Namespace A
Namespaced
resources
RoleBinding
Role
Cluster scope (resources that aren’t namespaced)
ClusterRoleBinding
ClusterRole
Figure 12.3
Roles and RoleBindings are namespaced; ClusterRoles and ClusterRoleBindings aren’t.
 
</data>
  <data key="d5">356
CHAPTER 12
Securing the Kubernetes API server
SETTING UP YOUR EXERCISE
Before you can explore how RBAC resources affect what you can do through the API
server, you need to make sure RBAC is enabled in your cluster. First, ensure you’re
using at least version 1.6 of Kubernetes and that the RBAC plugin is the only config-
ured authorization plugin. There can be multiple plugins enabled in parallel and if
one of them allows an action to be performed, the action is allowed.
NOTE
If you’re using GKE 1.6 or 1.7, you need to explicitly disable legacy autho-
rization by creating the cluster with the --no-enable-legacy-authorization
option. If you’re using Minikube, you also may need to enable RBAC by start-
ing Minikube with --extra-config=apiserver.Authorization.Mode=RBAC
If you followed the instructions on how to disable RBAC in chapter 8, now’s the time
to re-enable it by running the following command:
$ kubectl delete clusterrolebinding permissive-binding
To try out RBAC, you’ll run a pod through which you’ll try to talk to the API server,
the way you did in chapter 8. But this time you’ll run two pods in different namespaces
to see how per-namespace security behaves.
 In the examples in chapter 8, you ran two containers to demonstrate how an appli-
cation in one container uses the other container to talk to the API server. This time,
you’ll run a single container (based on the kubectl-proxy image) and use kubectl
exec to run curl inside that container directly. The proxy will take care of authentica-
tion and HTTPS, so you can focus on the authorization aspect of API server security.
Namespace C
Namespaced
resources
Cluster-level
resources
RoleBinding
RoleBinding
Role
Namespace B
Namespaced
resources
RoleBinding
Role
Namespace A
Namespaced
resources
RoleBinding
Role
Cluster scope (resources that aren’t namespaced)
ClusterRoleBinding
ClusterRole
Figure 12.3
Roles and RoleBindings are namespaced; ClusterRoles and ClusterRoleBindings aren’t.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="389">
  <data key="d0">Page_389</data>
  <data key="d5">Page_389</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_300">
  <data key="d0">357
Securing the cluster with role-based access control
CREATING THE NAMESPACES AND RUNNING THE PODS
You’re going to create one pod in namespace foo and the other one in namespace
bar, as shown in the following listing.
$ kubectl create ns foo
namespace "foo" created
$ kubectl run test --image=luksa/kubectl-proxy -n foo
deployment "test" created
$ kubectl create ns bar
namespace "bar" created
$ kubectl run test --image=luksa/kubectl-proxy -n bar
deployment "test" created
Now open two terminals and use kubectl exec to run a shell inside each of the two
pods (one in each terminal). For example, to run the shell in the pod in namespace
foo, first get the name of the pod:
$ kubectl get po -n foo
NAME                   READY     STATUS    RESTARTS   AGE
test-145485760-ttq36   1/1       Running   0          1m
Then use the name in the kubectl exec command:
$ kubectl exec -it test-145485760-ttq36 -n foo sh
/ #
Do the same in the other terminal, but for the pod in the bar namespace.
LISTING SERVICES FROM YOUR PODS
To verify that RBAC is enabled and preventing the pod from reading cluster state, use
curl to list Services in the foo namespace:
/ # curl localhost:8001/api/v1/namespaces/foo/services
User "system:serviceaccount:foo:default" cannot list services in the 
namespace "foo".
You’re connecting to localhost:8001, which is where the kubectl proxy process is
listening (as explained in chapter 8). The process received your request and sent it to
the API server while authenticating as the default ServiceAccount in the foo name-
space (as evident from the API server’s response). 
 The API server responded that the ServiceAccount isn’t allowed to list Services in
the foo namespace, even though the pod is running in that same namespace. You’re
seeing RBAC in action. The default permissions for a ServiceAccount don’t allow it to
list or modify any resources. Now, let’s learn how to allow the ServiceAccount to do
that. First, you’ll need to create a Role resource.
Listing 12.7
Running test pods in different namespaces
 
</data>
  <data key="d5">357
Securing the cluster with role-based access control
CREATING THE NAMESPACES AND RUNNING THE PODS
You’re going to create one pod in namespace foo and the other one in namespace
bar, as shown in the following listing.
$ kubectl create ns foo
namespace "foo" created
$ kubectl run test --image=luksa/kubectl-proxy -n foo
deployment "test" created
$ kubectl create ns bar
namespace "bar" created
$ kubectl run test --image=luksa/kubectl-proxy -n bar
deployment "test" created
Now open two terminals and use kubectl exec to run a shell inside each of the two
pods (one in each terminal). For example, to run the shell in the pod in namespace
foo, first get the name of the pod:
$ kubectl get po -n foo
NAME                   READY     STATUS    RESTARTS   AGE
test-145485760-ttq36   1/1       Running   0          1m
Then use the name in the kubectl exec command:
$ kubectl exec -it test-145485760-ttq36 -n foo sh
/ #
Do the same in the other terminal, but for the pod in the bar namespace.
LISTING SERVICES FROM YOUR PODS
To verify that RBAC is enabled and preventing the pod from reading cluster state, use
curl to list Services in the foo namespace:
/ # curl localhost:8001/api/v1/namespaces/foo/services
User "system:serviceaccount:foo:default" cannot list services in the 
namespace "foo".
You’re connecting to localhost:8001, which is where the kubectl proxy process is
listening (as explained in chapter 8). The process received your request and sent it to
the API server while authenticating as the default ServiceAccount in the foo name-
space (as evident from the API server’s response). 
 The API server responded that the ServiceAccount isn’t allowed to list Services in
the foo namespace, even though the pod is running in that same namespace. You’re
seeing RBAC in action. The default permissions for a ServiceAccount don’t allow it to
list or modify any resources. Now, let’s learn how to allow the ServiceAccount to do
that. First, you’ll need to create a Role resource.
Listing 12.7
Running test pods in different namespaces
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="390">
  <data key="d0">Page_390</data>
  <data key="d5">Page_390</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_301">
  <data key="d0">358
CHAPTER 12
Securing the Kubernetes API server
12.2.3 Using Roles and RoleBindings
A Role resource defines what actions can be taken on which resources (or, as
explained earlier, which types of HTTP requests can be performed on which RESTful
resources). The following listing defines a Role, which allows users to get and list
Services in the foo namespace.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: foo            
  name: service-reader
rules:
- apiGroups: [""]            
  verbs: ["get", "list"]     
  resources: ["services"]   
WARNING
The plural form must be used when specifying resources.
This Role resource will be created in the foo namespace. In chapter 8, you learned that
each resource type belongs to an API group, which you specify in the apiVersion field
(along with the version) in the resource’s manifest. In a Role definition, you need to spec-
ify the apiGroup for the resources listed in each rule included in the definition. If you’re
allowing access to resources belonging to different API groups, you use multiple rules.
NOTE
In the example, you’re allowing access to all Service resources, but you
could also limit access only to specific Service instances by specifying their
names through an additional resourceNames field.
Figure 12.4 shows the Role, its verbs and resources, and the namespace it will be cre-
ated in.
Listing 12.8
A definition of a Role: service-reader.yaml
Roles are namespaced (if namespace is 
omitted, the current namespace is used).
Services are resources in the core apiGroup, 
which has no name – hence the "".
Getting individual Services (by name) 
and listing all of them is allowed.
This rule pertains to services 
(plural name must be used!).
Allows getting
Allows listing
Services
Role:
service-reader
Services
Namespace: foo
Namespace: bar
Does not allow users to
get or list Services in
other namespaces
Figure 12.4
The service-reader Role allows getting and listing Services in the foo namespace.
 
</data>
  <data key="d5">358
CHAPTER 12
Securing the Kubernetes API server
12.2.3 Using Roles and RoleBindings
A Role resource defines what actions can be taken on which resources (or, as
explained earlier, which types of HTTP requests can be performed on which RESTful
resources). The following listing defines a Role, which allows users to get and list
Services in the foo namespace.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: foo            
  name: service-reader
rules:
- apiGroups: [""]            
  verbs: ["get", "list"]     
  resources: ["services"]   
WARNING
The plural form must be used when specifying resources.
This Role resource will be created in the foo namespace. In chapter 8, you learned that
each resource type belongs to an API group, which you specify in the apiVersion field
(along with the version) in the resource’s manifest. In a Role definition, you need to spec-
ify the apiGroup for the resources listed in each rule included in the definition. If you’re
allowing access to resources belonging to different API groups, you use multiple rules.
NOTE
In the example, you’re allowing access to all Service resources, but you
could also limit access only to specific Service instances by specifying their
names through an additional resourceNames field.
Figure 12.4 shows the Role, its verbs and resources, and the namespace it will be cre-
ated in.
Listing 12.8
A definition of a Role: service-reader.yaml
Roles are namespaced (if namespace is 
omitted, the current namespace is used).
Services are resources in the core apiGroup, 
which has no name – hence the "".
Getting individual Services (by name) 
and listing all of them is allowed.
This rule pertains to services 
(plural name must be used!).
Allows getting
Allows listing
Services
Role:
service-reader
Services
Namespace: foo
Namespace: bar
Does not allow users to
get or list Services in
other namespaces
Figure 12.4
The service-reader Role allows getting and listing Services in the foo namespace.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="391">
  <data key="d0">Page_391</data>
  <data key="d5">Page_391</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_302">
  <data key="d0">359
Securing the cluster with role-based access control
CREATING A ROLE
Create the previous Role in the foo namespace now:
$ kubectl create -f service-reader.yaml -n foo
role "service-reader" created
NOTE
The -n option is shorthand for --namespace.
Note that if you’re using GKE, the previous command may fail because you don’t have
cluster-admin rights. To grant the rights, run the following command:
$ kubectl create clusterrolebinding cluster-admin-binding 
➥ --clusterrole=cluster-admin --user=your.email@address.com
Instead of creating the service-reader Role from a YAML file, you could also create
it with the special kubectl create role command. Let’s use this method to create the
Role in the bar namespace:
$ kubectl create role service-reader --verb=get --verb=list 
➥ --resource=services -n bar
role "service-reader" created
These two Roles will allow you to list Services in the foo and bar namespaces from
within your two pods (running in the foo and bar namespace, respectively). But cre-
ating the two Roles isn’t enough (you can check by executing the curl command
again). You need to bind each of the Roles to the ServiceAccounts in their respec-
tive namespaces. 
BINDING A ROLE TO A SERVICEACCOUNT
A Role defines what actions can be performed, but it doesn’t specify who can perform
them. To do that, you must bind the Role to a subject, which can be a user, a Service-
Account, or a group (of users or ServiceAccounts).
 Binding Roles to subjects is achieved by creating a RoleBinding resource. To bind
the Role to the default ServiceAccount, run the following command:
$ kubectl create rolebinding test --role=service-reader 
➥ --serviceaccount=foo:default -n foo
rolebinding "test" created
The command should be self-explanatory. You’re creating a RoleBinding, which binds
the service-reader Role to the default ServiceAccount in namespace foo. You’re cre-
ating the RoleBinding in namespace foo. The RoleBinding and the referenced Service-
Account and Role are shown in figure 12.5.
NOTE
To bind a Role to a user instead of a ServiceAccount, use the --user
argument to specify the username. To bind it to a group, use --group.
 
</data>
  <data key="d5">359
Securing the cluster with role-based access control
CREATING A ROLE
Create the previous Role in the foo namespace now:
$ kubectl create -f service-reader.yaml -n foo
role "service-reader" created
NOTE
The -n option is shorthand for --namespace.
Note that if you’re using GKE, the previous command may fail because you don’t have
cluster-admin rights. To grant the rights, run the following command:
$ kubectl create clusterrolebinding cluster-admin-binding 
➥ --clusterrole=cluster-admin --user=your.email@address.com
Instead of creating the service-reader Role from a YAML file, you could also create
it with the special kubectl create role command. Let’s use this method to create the
Role in the bar namespace:
$ kubectl create role service-reader --verb=get --verb=list 
➥ --resource=services -n bar
role "service-reader" created
These two Roles will allow you to list Services in the foo and bar namespaces from
within your two pods (running in the foo and bar namespace, respectively). But cre-
ating the two Roles isn’t enough (you can check by executing the curl command
again). You need to bind each of the Roles to the ServiceAccounts in their respec-
tive namespaces. 
BINDING A ROLE TO A SERVICEACCOUNT
A Role defines what actions can be performed, but it doesn’t specify who can perform
them. To do that, you must bind the Role to a subject, which can be a user, a Service-
Account, or a group (of users or ServiceAccounts).
 Binding Roles to subjects is achieved by creating a RoleBinding resource. To bind
the Role to the default ServiceAccount, run the following command:
$ kubectl create rolebinding test --role=service-reader 
➥ --serviceaccount=foo:default -n foo
rolebinding "test" created
The command should be self-explanatory. You’re creating a RoleBinding, which binds
the service-reader Role to the default ServiceAccount in namespace foo. You’re cre-
ating the RoleBinding in namespace foo. The RoleBinding and the referenced Service-
Account and Role are shown in figure 12.5.
NOTE
To bind a Role to a user instead of a ServiceAccount, use the --user
argument to specify the username. To bind it to a group, use --group.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="392">
  <data key="d0">Page_392</data>
  <data key="d5">Page_392</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_303">
  <data key="d0">360
CHAPTER 12
Securing the Kubernetes API server
The following listing shows the YAML of the RoleBinding you created.
$ kubectl get rolebinding test -n foo -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: test
  namespace: foo
  ...
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role                         
  name: service-reader               
subjects:
- kind: ServiceAccount       
  name: default              
  namespace: foo             
As you can see, a RoleBinding always references a single Role (as evident from the
roleRef property), but can bind the Role to multiple subjects (for example, one or
more ServiceAccounts and any number of users or groups). Because this RoleBinding
binds the Role to the ServiceAccount the pod in namespace foo is running under, you
can now list Services from within that pod.
/ # curl localhost:8001/api/v1/namespaces/foo/services
{
  "kind": "ServiceList",
  "apiVersion": "v1",
  "metadata": {
    "selfLink": "/api/v1/namespaces/foo/services",
Listing 12.9
A RoleBinding referencing a Role
Listing 12.10
Getting Services from the API server
Namespace: foo
Role:
service-reader
Get, list
Default ServiceAccount
is allowed to get and list
services in this namespace
Services
RoleBinding:
test
Service-
Account:
default
Figure 12.5
The test RoleBinding binds the default ServiceAccount with the 
service-reader Role.
This RoleBinding references 
the service-reader Role.
And binds it to the 
default ServiceAccount 
in the foo namespace.
 
</data>
  <data key="d5">360
CHAPTER 12
Securing the Kubernetes API server
The following listing shows the YAML of the RoleBinding you created.
$ kubectl get rolebinding test -n foo -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: test
  namespace: foo
  ...
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role                         
  name: service-reader               
subjects:
- kind: ServiceAccount       
  name: default              
  namespace: foo             
As you can see, a RoleBinding always references a single Role (as evident from the
roleRef property), but can bind the Role to multiple subjects (for example, one or
more ServiceAccounts and any number of users or groups). Because this RoleBinding
binds the Role to the ServiceAccount the pod in namespace foo is running under, you
can now list Services from within that pod.
/ # curl localhost:8001/api/v1/namespaces/foo/services
{
  "kind": "ServiceList",
  "apiVersion": "v1",
  "metadata": {
    "selfLink": "/api/v1/namespaces/foo/services",
Listing 12.9
A RoleBinding referencing a Role
Listing 12.10
Getting Services from the API server
Namespace: foo
Role:
service-reader
Get, list
Default ServiceAccount
is allowed to get and list
services in this namespace
Services
RoleBinding:
test
Service-
Account:
default
Figure 12.5
The test RoleBinding binds the default ServiceAccount with the 
service-reader Role.
This RoleBinding references 
the service-reader Role.
And binds it to the 
default ServiceAccount 
in the foo namespace.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="393">
  <data key="d0">Page_393</data>
  <data key="d5">Page_393</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_304">
  <data key="d0">361
Securing the cluster with role-based access control
    "resourceVersion": "24906"
  },
  "items": []     
}
INCLUDING SERVICEACCOUNTS FROM OTHER NAMESPACES IN A ROLEBINDING
The pod in namespace bar can’t list the Services in its own namespace, and obviously
also not those in the foo namespace. But you can edit your RoleBinding in the foo
namespace and add the other pod’s ServiceAccount, even though it’s in a different
namespace. Run the following command:
$ kubectl edit rolebinding test -n foo
Then add the following lines to the list of subjects, as shown in the following listing.
subjects:
- kind: ServiceAccount
  name: default          
  namespace: bar         
Now you can also list Services in the foo namespace from inside the pod running in
the bar namespace. Run the same command as in listing 12.10, but do it in the other
terminal, where you’re running the shell in the other pod.
 Before moving on to ClusterRoles and ClusterRoleBindings, let’s summarize
what RBAC resources you currently have. You have a RoleBinding in namespace
foo, which references the service-reader Role (also in the foo namespace) and
binds the default ServiceAccounts in both the foo and the bar namespaces, as
depicted in figure 12.6.
Listing 12.11
Referencing a ServiceAccount from another namespace
The list of items is empty, 
because no Services exist.
You’re referencing the default 
ServiceAccount in the bar namespace.
Namespace: foo
Role:
service-reader
Get, list
Both ServiceAccounts are
allowed to get and list Services
in the foo namespace
Services
Namespace: bar
RoleBinding:
test
Service-
Account:
default
Service-
Account:
default
Figure 12.6
A RoleBinding binding ServiceAccounts from different namespaces to the same Role.
 
</data>
  <data key="d5">361
Securing the cluster with role-based access control
    "resourceVersion": "24906"
  },
  "items": []     
}
INCLUDING SERVICEACCOUNTS FROM OTHER NAMESPACES IN A ROLEBINDING
The pod in namespace bar can’t list the Services in its own namespace, and obviously
also not those in the foo namespace. But you can edit your RoleBinding in the foo
namespace and add the other pod’s ServiceAccount, even though it’s in a different
namespace. Run the following command:
$ kubectl edit rolebinding test -n foo
Then add the following lines to the list of subjects, as shown in the following listing.
subjects:
- kind: ServiceAccount
  name: default          
  namespace: bar         
Now you can also list Services in the foo namespace from inside the pod running in
the bar namespace. Run the same command as in listing 12.10, but do it in the other
terminal, where you’re running the shell in the other pod.
 Before moving on to ClusterRoles and ClusterRoleBindings, let’s summarize
what RBAC resources you currently have. You have a RoleBinding in namespace
foo, which references the service-reader Role (also in the foo namespace) and
binds the default ServiceAccounts in both the foo and the bar namespaces, as
depicted in figure 12.6.
Listing 12.11
Referencing a ServiceAccount from another namespace
The list of items is empty, 
because no Services exist.
You’re referencing the default 
ServiceAccount in the bar namespace.
Namespace: foo
Role:
service-reader
Get, list
Both ServiceAccounts are
allowed to get and list Services
in the foo namespace
Services
Namespace: bar
RoleBinding:
test
Service-
Account:
default
Service-
Account:
default
Figure 12.6
A RoleBinding binding ServiceAccounts from different namespaces to the same Role.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="394">
  <data key="d0">Page_394</data>
  <data key="d5">Page_394</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_305">
  <data key="d0">362
CHAPTER 12
Securing the Kubernetes API server
12.2.4 Using ClusterRoles and ClusterRoleBindings
Roles and RoleBindings are namespaced resources, meaning they reside in and apply
to resources in a single namespace, but, as we saw, RoleBindings can refer to Service-
Accounts from other namespaces, too. 
 In addition to these namespaced resources, two cluster-level RBAC resources also
exist: ClusterRole and ClusterRoleBinding. They’re not namespaced. Let’s see why
you need them.
 A regular Role only allows access to resources in the same namespace the Role is
in. If you want to allow someone access to resources across different namespaces, you
have to create a Role and RoleBinding in every one of those namespaces. If you want
to extend this to all namespaces (this is something a cluster administrator would prob-
ably need), you need to create the same Role and RoleBinding in each namespace.
When creating an additional namespace, you have to remember to create the two
resources there as well. 
 As you’ve learned throughout the book, certain resources aren’t namespaced at
all (this includes Nodes, PersistentVolumes, Namespaces, and so on). We’ve also
mentioned the API server exposes some URL paths that don’t represent resources
(/healthz for example). Regular Roles can’t grant access to those resources or non-
resource URLs, but ClusterRoles can.
 A ClusterRole is a cluster-level resource for allowing access to non-namespaced
resources or non-resource URLs or used as a common role to be bound inside individ-
ual namespaces, saving you from having to redefine the same role in each of them.
ALLOWING ACCESS TO CLUSTER-LEVEL RESOURCES
As mentioned, a ClusterRole can be used to allow access to cluster-level resources.
Let’s look at how to allow your pod to list PersistentVolumes in your cluster. First,
you’ll create a ClusterRole called pv-reader:
$ kubectl create clusterrole pv-reader --verb=get,list 
➥ --resource=persistentvolumes
clusterrole "pv-reader" created
The ClusterRole’s YAML is shown in the following listing.
$ kubectl get clusterrole pv-reader -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:                                       
  name: pv-reader                               
  resourceVersion: "39932"                      
  selfLink: ...                                 
  uid: e9ac1099-30e2-11e7-955c-080027e6b159     
Listing 12.12
A ClusterRole definition
ClusterRoles aren’t 
namespaced, hence 
no namespace field.
 
</data>
  <data key="d5">362
CHAPTER 12
Securing the Kubernetes API server
12.2.4 Using ClusterRoles and ClusterRoleBindings
Roles and RoleBindings are namespaced resources, meaning they reside in and apply
to resources in a single namespace, but, as we saw, RoleBindings can refer to Service-
Accounts from other namespaces, too. 
 In addition to these namespaced resources, two cluster-level RBAC resources also
exist: ClusterRole and ClusterRoleBinding. They’re not namespaced. Let’s see why
you need them.
 A regular Role only allows access to resources in the same namespace the Role is
in. If you want to allow someone access to resources across different namespaces, you
have to create a Role and RoleBinding in every one of those namespaces. If you want
to extend this to all namespaces (this is something a cluster administrator would prob-
ably need), you need to create the same Role and RoleBinding in each namespace.
When creating an additional namespace, you have to remember to create the two
resources there as well. 
 As you’ve learned throughout the book, certain resources aren’t namespaced at
all (this includes Nodes, PersistentVolumes, Namespaces, and so on). We’ve also
mentioned the API server exposes some URL paths that don’t represent resources
(/healthz for example). Regular Roles can’t grant access to those resources or non-
resource URLs, but ClusterRoles can.
 A ClusterRole is a cluster-level resource for allowing access to non-namespaced
resources or non-resource URLs or used as a common role to be bound inside individ-
ual namespaces, saving you from having to redefine the same role in each of them.
ALLOWING ACCESS TO CLUSTER-LEVEL RESOURCES
As mentioned, a ClusterRole can be used to allow access to cluster-level resources.
Let’s look at how to allow your pod to list PersistentVolumes in your cluster. First,
you’ll create a ClusterRole called pv-reader:
$ kubectl create clusterrole pv-reader --verb=get,list 
➥ --resource=persistentvolumes
clusterrole "pv-reader" created
The ClusterRole’s YAML is shown in the following listing.
$ kubectl get clusterrole pv-reader -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:                                       
  name: pv-reader                               
  resourceVersion: "39932"                      
  selfLink: ...                                 
  uid: e9ac1099-30e2-11e7-955c-080027e6b159     
Listing 12.12
A ClusterRole definition
ClusterRoles aren’t 
namespaced, hence 
no namespace field.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="395">
  <data key="d0">Page_395</data>
  <data key="d5">Page_395</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_306">
  <data key="d0">363
Securing the cluster with role-based access control
rules:
- apiGroups:                      
  - ""                            
  resources:                      
  - persistentvolumes             
  verbs:                          
  - get                           
  - list                          
Before you bind this ClusterRole to your pod’s ServiceAccount, verify whether the pod
can list PersistentVolumes. Run the following command in the first terminal, where
you’re running the shell inside the pod in the foo namespace:
/ # curl localhost:8001/api/v1/persistentvolumes
User "system:serviceaccount:foo:default" cannot list persistentvolumes at the 
cluster scope.
NOTE
The URL contains no namespace, because PersistentVolumes aren’t
namespaced. 
As expected, the default ServiceAccount can’t list PersistentVolumes. You need to
bind the ClusterRole to your ServiceAccount to allow it to do that. ClusterRoles can
be bound to subjects with regular RoleBindings, so you’ll create a RoleBinding now:
$ kubectl create rolebinding pv-test --clusterrole=pv-reader 
➥ --serviceaccount=foo:default -n foo
rolebinding "pv-test" created
Can you list PersistentVolumes now?
/ # curl localhost:8001/api/v1/persistentvolumes
User "system:serviceaccount:foo:default" cannot list persistentvolumes at the 
cluster scope.
Hmm, that’s strange. Let’s examine the RoleBinding’s YAML in the following listing.
Can you tell what (if anything) is wrong with it?
$ kubectl get rolebindings pv-test -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pv-test
  namespace: foo
  ...
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole              
  name: pv-reader                
Listing 12.13
A RoleBinding referencing a ClusterRole
In this case, the 
rules are exactly 
like those in a 
regular Role.
The binding references the 
pv-reader ClusterRole.
 
</data>
  <data key="d5">363
Securing the cluster with role-based access control
rules:
- apiGroups:                      
  - ""                            
  resources:                      
  - persistentvolumes             
  verbs:                          
  - get                           
  - list                          
Before you bind this ClusterRole to your pod’s ServiceAccount, verify whether the pod
can list PersistentVolumes. Run the following command in the first terminal, where
you’re running the shell inside the pod in the foo namespace:
/ # curl localhost:8001/api/v1/persistentvolumes
User "system:serviceaccount:foo:default" cannot list persistentvolumes at the 
cluster scope.
NOTE
The URL contains no namespace, because PersistentVolumes aren’t
namespaced. 
As expected, the default ServiceAccount can’t list PersistentVolumes. You need to
bind the ClusterRole to your ServiceAccount to allow it to do that. ClusterRoles can
be bound to subjects with regular RoleBindings, so you’ll create a RoleBinding now:
$ kubectl create rolebinding pv-test --clusterrole=pv-reader 
➥ --serviceaccount=foo:default -n foo
rolebinding "pv-test" created
Can you list PersistentVolumes now?
/ # curl localhost:8001/api/v1/persistentvolumes
User "system:serviceaccount:foo:default" cannot list persistentvolumes at the 
cluster scope.
Hmm, that’s strange. Let’s examine the RoleBinding’s YAML in the following listing.
Can you tell what (if anything) is wrong with it?
$ kubectl get rolebindings pv-test -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pv-test
  namespace: foo
  ...
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole              
  name: pv-reader                
Listing 12.13
A RoleBinding referencing a ClusterRole
In this case, the 
rules are exactly 
like those in a 
regular Role.
The binding references the 
pv-reader ClusterRole.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="396">
  <data key="d0">Page_396</data>
  <data key="d5">Page_396</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_307">
  <data key="d0">364
CHAPTER 12
Securing the Kubernetes API server
subjects:
- kind: ServiceAccount          
  name: default                 
  namespace: foo                
The YAML looks perfectly fine. You’re referencing the correct ClusterRole and the
correct ServiceAccount, as shown in figure 12.7, so what’s wrong?
Although you can create a RoleBinding and have it reference a ClusterRole when you
want to enable access to namespaced resources, you can’t use the same approach for
cluster-level (non-namespaced) resources. To grant access to cluster-level resources,
you must always use a ClusterRoleBinding.
 Luckily, creating a ClusterRoleBinding isn’t that different from creating a Role-
Binding, but you’ll clean up and delete the RoleBinding first:
$ kubectl delete rolebinding pv-test
rolebinding "pv-test" deleted
Now create the ClusterRoleBinding:
$ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader 
➥ --serviceaccount=foo:default
clusterrolebinding "pv-test" created
As you can see, you replaced rolebinding with clusterrolebinding in the command
and didn’t (need to) specify the namespace. Figure 12.8 shows what you have now.
 Let’s see if you can list PersistentVolumes now:
/ # curl localhost:8001/api/v1/persistentvolumes
{
  "kind": "PersistentVolumeList",
  "apiVersion": "v1",
...
The bound subject is the 
default ServiceAccount in 
the foo namespace.
Namespace: foo
Cluster-level resources
ClusterRole:
pv-reader
Get, list
Persistent
Volumes
RoleBinding:
pv-test
Default ServiceAccount
is unable to get and list
PersistentVolumes
Service-
Account:
default
Figure 12.7
A RoleBinding referencing a ClusterRole doesn’t grant access to cluster-
level resources.
 
</data>
  <data key="d5">364
CHAPTER 12
Securing the Kubernetes API server
subjects:
- kind: ServiceAccount          
  name: default                 
  namespace: foo                
The YAML looks perfectly fine. You’re referencing the correct ClusterRole and the
correct ServiceAccount, as shown in figure 12.7, so what’s wrong?
Although you can create a RoleBinding and have it reference a ClusterRole when you
want to enable access to namespaced resources, you can’t use the same approach for
cluster-level (non-namespaced) resources. To grant access to cluster-level resources,
you must always use a ClusterRoleBinding.
 Luckily, creating a ClusterRoleBinding isn’t that different from creating a Role-
Binding, but you’ll clean up and delete the RoleBinding first:
$ kubectl delete rolebinding pv-test
rolebinding "pv-test" deleted
Now create the ClusterRoleBinding:
$ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader 
➥ --serviceaccount=foo:default
clusterrolebinding "pv-test" created
As you can see, you replaced rolebinding with clusterrolebinding in the command
and didn’t (need to) specify the namespace. Figure 12.8 shows what you have now.
 Let’s see if you can list PersistentVolumes now:
/ # curl localhost:8001/api/v1/persistentvolumes
{
  "kind": "PersistentVolumeList",
  "apiVersion": "v1",
...
The bound subject is the 
default ServiceAccount in 
the foo namespace.
Namespace: foo
Cluster-level resources
ClusterRole:
pv-reader
Get, list
Persistent
Volumes
RoleBinding:
pv-test
Default ServiceAccount
is unable to get and list
PersistentVolumes
Service-
Account:
default
Figure 12.7
A RoleBinding referencing a ClusterRole doesn’t grant access to cluster-
level resources.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="397">
  <data key="d0">Page_397</data>
  <data key="d5">Page_397</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_308">
  <data key="d0">365
Securing the cluster with role-based access control
You can! It turns out you must use a ClusterRole and a ClusterRoleBinding when
granting access to cluster-level resources.
TIP
Remember that a RoleBinding can’t grant access to cluster-level resources,
even if it references a ClusterRoleBinding.
ALLOWING ACCESS TO NON-RESOURCE URLS
We’ve mentioned that the API server also exposes non-resource URLs. Access to these
URLs must also be granted explicitly; otherwise the API server will reject the client’s
request. Usually, this is done for you automatically through the system:discovery
ClusterRole and the identically named ClusterRoleBinding, which appear among
other predefined ClusterRoles and ClusterRoleBindings (we’ll explore them in sec-
tion 12.2.5). 
 Let’s inspect the system:discovery ClusterRole shown in the following listing.
$ kubectl get clusterrole system:discovery -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:discovery
  ...
rules:
- nonResourceURLs:      
  - /api                
  - /api/*              
  - /apis               
  - /apis/*             
  - /healthz            
  - /swaggerapi         
  - /swaggerapi/*       
  - /version            
Listing 12.14
The default system:discovery ClusterRole
Namespace: foo
Cluster-level resources
ClusterRole:
pv-reader
Get, list
Persistent
Volumes
ClusterRoleBinding:
pv-test
Default ServiceAccount in
foo namespace is now allowed
to get and list PersistentVolumes
Service-
Account:
default
Figure 12.8
A ClusterRoleBinding and ClusterRole must be used to grant access to cluster-
level resources.
Instead of referring 
to resources, this rule 
refers to non-resource 
URLs.
 
</data>
  <data key="d5">365
Securing the cluster with role-based access control
You can! It turns out you must use a ClusterRole and a ClusterRoleBinding when
granting access to cluster-level resources.
TIP
Remember that a RoleBinding can’t grant access to cluster-level resources,
even if it references a ClusterRoleBinding.
ALLOWING ACCESS TO NON-RESOURCE URLS
We’ve mentioned that the API server also exposes non-resource URLs. Access to these
URLs must also be granted explicitly; otherwise the API server will reject the client’s
request. Usually, this is done for you automatically through the system:discovery
ClusterRole and the identically named ClusterRoleBinding, which appear among
other predefined ClusterRoles and ClusterRoleBindings (we’ll explore them in sec-
tion 12.2.5). 
 Let’s inspect the system:discovery ClusterRole shown in the following listing.
$ kubectl get clusterrole system:discovery -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:discovery
  ...
rules:
- nonResourceURLs:      
  - /api                
  - /api/*              
  - /apis               
  - /apis/*             
  - /healthz            
  - /swaggerapi         
  - /swaggerapi/*       
  - /version            
Listing 12.14
The default system:discovery ClusterRole
Namespace: foo
Cluster-level resources
ClusterRole:
pv-reader
Get, list
Persistent
Volumes
ClusterRoleBinding:
pv-test
Default ServiceAccount in
foo namespace is now allowed
to get and list PersistentVolumes
Service-
Account:
default
Figure 12.8
A ClusterRoleBinding and ClusterRole must be used to grant access to cluster-
level resources.
Instead of referring 
to resources, this rule 
refers to non-resource 
URLs.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="398">
  <data key="d0">Page_398</data>
  <data key="d5">Page_398</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_309">
  <data key="d0">366
CHAPTER 12
Securing the Kubernetes API server
  verbs:             
  - get              
You can see this ClusterRole refers to URLs instead of resources (field nonResource-
URLs is used instead of the resources field). The verbs field only allows the GET HTTP
method to be used on these URLs.
NOTE
For non-resource URLs, plain HTTP verbs such as post, put, and
patch are used instead of create or update. The verbs need to be specified in
lowercase.
As with cluster-level resources, ClusterRoles for non-resource URLs must be bound
with a ClusterRoleBinding. Binding them with a RoleBinding won’t have any effect.
The system:discovery ClusterRole has a corresponding system:discovery Cluster-
RoleBinding, so let’s see what’s in it by examining the following listing.
$ kubectl get clusterrolebinding system:discovery -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:discovery
  ...
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole                           
  name: system:discovery                      
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group                                 
  name: system:authenticated                  
- apiGroup: rbac.authorization.k8s.io
  kind: Group                                 
  name: system:unauthenticated                
The YAML shows the ClusterRoleBinding refers to the system:discovery ClusterRole,
as expected. It’s bound to two groups, system:authenticated and system:unauthenti-
cated, which makes it bound to all users. This means absolutely everyone can access
the URLs listed in the ClusterRole. 
NOTE
Groups are in the domain of the authentication plugin. When a
request is received by the API server, it calls the authentication plugin to
obtain the list of groups the user belongs to. This information is then used
in authorization.
You can confirm this by accessing the /api URL path from inside the pod (through
the kubectl proxy, which means you’ll be authenticated as the pod’s ServiceAccount)
Listing 12.15
The default system:discovery ClusterRoleBinding
Only the HTTP GET method 
is allowed for these URLs.
This ClusterRoleBinding references 
the system:discovery ClusterRole.
It binds the ClusterRole 
to all authenticated and 
unauthenticated users 
(that is, everyone).
 
</data>
  <data key="d5">366
CHAPTER 12
Securing the Kubernetes API server
  verbs:             
  - get              
You can see this ClusterRole refers to URLs instead of resources (field nonResource-
URLs is used instead of the resources field). The verbs field only allows the GET HTTP
method to be used on these URLs.
NOTE
For non-resource URLs, plain HTTP verbs such as post, put, and
patch are used instead of create or update. The verbs need to be specified in
lowercase.
As with cluster-level resources, ClusterRoles for non-resource URLs must be bound
with a ClusterRoleBinding. Binding them with a RoleBinding won’t have any effect.
The system:discovery ClusterRole has a corresponding system:discovery Cluster-
RoleBinding, so let’s see what’s in it by examining the following listing.
$ kubectl get clusterrolebinding system:discovery -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:discovery
  ...
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole                           
  name: system:discovery                      
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group                                 
  name: system:authenticated                  
- apiGroup: rbac.authorization.k8s.io
  kind: Group                                 
  name: system:unauthenticated                
The YAML shows the ClusterRoleBinding refers to the system:discovery ClusterRole,
as expected. It’s bound to two groups, system:authenticated and system:unauthenti-
cated, which makes it bound to all users. This means absolutely everyone can access
the URLs listed in the ClusterRole. 
NOTE
Groups are in the domain of the authentication plugin. When a
request is received by the API server, it calls the authentication plugin to
obtain the list of groups the user belongs to. This information is then used
in authorization.
You can confirm this by accessing the /api URL path from inside the pod (through
the kubectl proxy, which means you’ll be authenticated as the pod’s ServiceAccount)
Listing 12.15
The default system:discovery ClusterRoleBinding
Only the HTTP GET method 
is allowed for these URLs.
This ClusterRoleBinding references 
the system:discovery ClusterRole.
It binds the ClusterRole 
to all authenticated and 
unauthenticated users 
(that is, everyone).
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="399">
  <data key="d0">Page_399</data>
  <data key="d5">Page_399</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_310">
  <data key="d0">367
Securing the cluster with role-based access control
and from your local machine, without specifying any authentication tokens (making
you an unauthenticated user):
$ curl https://$(minikube ip):8443/api -k
{
  "kind": "APIVersions",
  "versions": [
  ...
You’ve now used ClusterRoles and ClusterRoleBindings to grant access to cluster-level
resources and non-resource URLs. Now let’s look at how ClusterRoles can be used
with namespaced RoleBindings to grant access to namespaced resources in the Role-
Binding’s namespace.
USING CLUSTERROLES TO GRANT ACCESS TO RESOURCES IN SPECIFIC NAMESPACES
ClusterRoles don’t always need to be bound with cluster-level ClusterRoleBindings.
They can also be bound with regular, namespaced RoleBindings. You’ve already
started looking at predefined ClusterRoles, so let’s look at another one called view,
which is shown in the following listing.
$ kubectl get clusterrole view -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: view
  ...
rules:
- apiGroups:
  - ""
  resources:                           
  - configmaps                         
  - endpoints                          
  - persistentvolumeclaims             
  - pods                               
  - replicationcontrollers             
  - replicationcontrollers/scale       
  - serviceaccounts                    
  - services                           
  verbs:                
  - get                 
  - list                
  - watch               
...
This ClusterRole has many rules. Only the first one is shown in the listing. The rule
allows getting, listing, and watching resources like ConfigMaps, Endpoints, Persistent-
VolumeClaims, and so on. These are namespaced resources, even though you’re
looking at a ClusterRole (not a regular, namespaced Role). What exactly does this
ClusterRole do?
Listing 12.16
The default view ClusterRole
This rule applies to 
these resources (note: 
they’re all namespaced 
resources).
As the ClusterRole’s name 
suggests, it only allows 
reading, not writing the 
resources listed. 
 
</data>
  <data key="d5">367
Securing the cluster with role-based access control
and from your local machine, without specifying any authentication tokens (making
you an unauthenticated user):
$ curl https://$(minikube ip):8443/api -k
{
  "kind": "APIVersions",
  "versions": [
  ...
You’ve now used ClusterRoles and ClusterRoleBindings to grant access to cluster-level
resources and non-resource URLs. Now let’s look at how ClusterRoles can be used
with namespaced RoleBindings to grant access to namespaced resources in the Role-
Binding’s namespace.
USING CLUSTERROLES TO GRANT ACCESS TO RESOURCES IN SPECIFIC NAMESPACES
ClusterRoles don’t always need to be bound with cluster-level ClusterRoleBindings.
They can also be bound with regular, namespaced RoleBindings. You’ve already
started looking at predefined ClusterRoles, so let’s look at another one called view,
which is shown in the following listing.
$ kubectl get clusterrole view -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: view
  ...
rules:
- apiGroups:
  - ""
  resources:                           
  - configmaps                         
  - endpoints                          
  - persistentvolumeclaims             
  - pods                               
  - replicationcontrollers             
  - replicationcontrollers/scale       
  - serviceaccounts                    
  - services                           
  verbs:                
  - get                 
  - list                
  - watch               
...
This ClusterRole has many rules. Only the first one is shown in the listing. The rule
allows getting, listing, and watching resources like ConfigMaps, Endpoints, Persistent-
VolumeClaims, and so on. These are namespaced resources, even though you’re
looking at a ClusterRole (not a regular, namespaced Role). What exactly does this
ClusterRole do?
Listing 12.16
The default view ClusterRole
This rule applies to 
these resources (note: 
they’re all namespaced 
resources).
As the ClusterRole’s name 
suggests, it only allows 
reading, not writing the 
resources listed. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="400">
  <data key="d0">Page_400</data>
  <data key="d5">Page_400</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_311">
  <data key="d0">368
CHAPTER 12
Securing the Kubernetes API server
 It depends whether it’s bound with a ClusterRoleBinding or a RoleBinding (it can
be bound with either). If you create a ClusterRoleBinding and reference the Cluster-
Role in it, the subjects listed in the binding can view the specified resources across all
namespaces. If, on the other hand, you create a RoleBinding, the subjects listed in the
binding can only view resources in the namespace of the RoleBinding. You’ll try both
options now.
 You’ll see how the two options affect your test pod’s ability to list pods. First, let’s
see what happens before any bindings are in place:
/ # curl localhost:8001/api/v1/pods
User "system:serviceaccount:foo:default" cannot list pods at the cluster 
scope./ #
/ # curl localhost:8001/api/v1/namespaces/foo/pods
User "system:serviceaccount:foo:default" cannot list pods in the namespace 
"foo".
With the first command, you’re trying to list pods across all namespaces. With the sec-
ond, you’re trying to list pods in the foo namespace. The server doesn’t allow you to
do either.
 Now, let’s see what happens when you create a ClusterRoleBinding and bind it to
the pod’s ServiceAccount:
$ kubectl create clusterrolebinding view-test --clusterrole=view 
➥ --serviceaccount=foo:default
clusterrolebinding "view-test" created
Can the pod now list pods in the foo namespace?
/ # curl localhost:8001/api/v1/namespaces/foo/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
It can! Because you created a ClusterRoleBinding, it applies across all namespaces.
The pod in namespace foo can list pods in the bar namespace as well:
/ # curl localhost:8001/api/v1/namespaces/bar/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
Okay, the pod is allowed to list pods in a different namespace. It can also retrieve pods
across all namespaces by hitting the /api/v1/pods URL path:
/ # curl localhost:8001/api/v1/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
 
</data>
  <data key="d5">368
CHAPTER 12
Securing the Kubernetes API server
 It depends whether it’s bound with a ClusterRoleBinding or a RoleBinding (it can
be bound with either). If you create a ClusterRoleBinding and reference the Cluster-
Role in it, the subjects listed in the binding can view the specified resources across all
namespaces. If, on the other hand, you create a RoleBinding, the subjects listed in the
binding can only view resources in the namespace of the RoleBinding. You’ll try both
options now.
 You’ll see how the two options affect your test pod’s ability to list pods. First, let’s
see what happens before any bindings are in place:
/ # curl localhost:8001/api/v1/pods
User "system:serviceaccount:foo:default" cannot list pods at the cluster 
scope./ #
/ # curl localhost:8001/api/v1/namespaces/foo/pods
User "system:serviceaccount:foo:default" cannot list pods in the namespace 
"foo".
With the first command, you’re trying to list pods across all namespaces. With the sec-
ond, you’re trying to list pods in the foo namespace. The server doesn’t allow you to
do either.
 Now, let’s see what happens when you create a ClusterRoleBinding and bind it to
the pod’s ServiceAccount:
$ kubectl create clusterrolebinding view-test --clusterrole=view 
➥ --serviceaccount=foo:default
clusterrolebinding "view-test" created
Can the pod now list pods in the foo namespace?
/ # curl localhost:8001/api/v1/namespaces/foo/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
It can! Because you created a ClusterRoleBinding, it applies across all namespaces.
The pod in namespace foo can list pods in the bar namespace as well:
/ # curl localhost:8001/api/v1/namespaces/bar/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
Okay, the pod is allowed to list pods in a different namespace. It can also retrieve pods
across all namespaces by hitting the /api/v1/pods URL path:
/ # curl localhost:8001/api/v1/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="401">
  <data key="d0">Page_401</data>
  <data key="d5">Page_401</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_312">
  <data key="d0">369
Securing the cluster with role-based access control
As expected, the pod can get a list of all the pods in the cluster. To summarize, com-
bining a ClusterRoleBinding with a ClusterRole referring to namespaced resources
allows the pod to access namespaced resources in any namespace, as shown in fig-
ure 12.9.
Now, let’s see what happens if you replace the ClusterRoleBinding with a RoleBinding.
First, delete the ClusterRoleBinding:
$ kubectl delete clusterrolebinding view-test
clusterrolebinding "view-test" deleted
Next create a RoleBinding instead. Because a RoleBinding is namespaced, you need
to specify the namespace you want to create it in. Create it in the foo namespace:
$ kubectl create rolebinding view-test --clusterrole=view 
➥ --serviceaccount=foo:default -n foo
rolebinding "view-test" created
You now have a RoleBinding in the foo namespace, binding the default Service-
Account in that same namespace with the view ClusterRole. What can your pod
access now?
/ # curl localhost:8001/api/v1/namespaces/foo/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
Namespace: foo
Cluster-level
resources
Namespace: bar
Pods
Pods
Default
ServiceAccount
in foo namespace
is allowed to
view pods in
any namespace
ClusterRole:
view
Allows getting,
listing, watching
ClusterRoleBinding:
view-test
Pods,
Services,
Endpoints,
ConﬁgMaps,
…
Service-
Account:
default
Figure 12.9
A ClusterRoleBinding and ClusterRole grants permission to resources across all 
namespaces.
 
</data>
  <data key="d5">369
Securing the cluster with role-based access control
As expected, the pod can get a list of all the pods in the cluster. To summarize, com-
bining a ClusterRoleBinding with a ClusterRole referring to namespaced resources
allows the pod to access namespaced resources in any namespace, as shown in fig-
ure 12.9.
Now, let’s see what happens if you replace the ClusterRoleBinding with a RoleBinding.
First, delete the ClusterRoleBinding:
$ kubectl delete clusterrolebinding view-test
clusterrolebinding "view-test" deleted
Next create a RoleBinding instead. Because a RoleBinding is namespaced, you need
to specify the namespace you want to create it in. Create it in the foo namespace:
$ kubectl create rolebinding view-test --clusterrole=view 
➥ --serviceaccount=foo:default -n foo
rolebinding "view-test" created
You now have a RoleBinding in the foo namespace, binding the default Service-
Account in that same namespace with the view ClusterRole. What can your pod
access now?
/ # curl localhost:8001/api/v1/namespaces/foo/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
Namespace: foo
Cluster-level
resources
Namespace: bar
Pods
Pods
Default
ServiceAccount
in foo namespace
is allowed to
view pods in
any namespace
ClusterRole:
view
Allows getting,
listing, watching
ClusterRoleBinding:
view-test
Pods,
Services,
Endpoints,
ConﬁgMaps,
…
Service-
Account:
default
Figure 12.9
A ClusterRoleBinding and ClusterRole grants permission to resources across all 
namespaces.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="402">
  <data key="d0">Page_402</data>
  <data key="d5">Page_402</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_313">
  <data key="d0">370
CHAPTER 12
Securing the Kubernetes API server
/ # curl localhost:8001/api/v1/namespaces/bar/pods
User "system:serviceaccount:foo:default" cannot list pods in the namespace 
"bar".
/ # curl localhost:8001/api/v1/pods
User "system:serviceaccount:foo:default" cannot list pods at the cluster 
scope.
As you can see, your pod can list pods in the foo namespace, but not in any other spe-
cific namespace or across all namespaces. This is visualized in figure 12.10.
SUMMARIZING ROLE, CLUSTERROLE, ROLEBINDING, AND CLUSTERROLEBINDING COMBINATIONS
We’ve covered many different combinations and it may be hard for you to remember
when to use each one. Let’s see if we can make sense of all these combinations by cat-
egorizing them per specific use case. Refer to table 12.2.
Table 12.2
When to use specific combinations of role and binding types
For accessing
Role type to use
Binding type to use
Cluster-level resources (Nodes, PersistentVolumes, ...)
ClusterRole
ClusterRoleBinding
Non-resource URLs (/api, /healthz, ...)
ClusterRole
ClusterRoleBinding
Namespaced resources in any namespace (and 
across all namespaces)
ClusterRole
ClusterRoleBinding
Namespaced resources in a specific namespace (reus-
ing the same ClusterRole in multiple namespaces)
ClusterRole
RoleBinding
Namespaced resources in a specific namespace 
(Role must be defined in each namespace)
Role
RoleBinding
Namespace: foo
Cluster-level resources
Namespace: bar
Pods
Pods
ClusterRole:
view
Allows getting,
listing, watching
RoleBinding:
view-test
Pods,
Services,
Endpoints,
ConﬁgMaps,
…
Default ServiceAccount in
foo namespace is only allowed
to view pods in namespace foo,
despite using a ClusterRole
Service-
Account:
default
Figure 12.10
A RoleBinding referring to a ClusterRole only grants access to resources inside the 
RoleBinding’s namespace.
 
</data>
  <data key="d5">370
CHAPTER 12
Securing the Kubernetes API server
/ # curl localhost:8001/api/v1/namespaces/bar/pods
User "system:serviceaccount:foo:default" cannot list pods in the namespace 
"bar".
/ # curl localhost:8001/api/v1/pods
User "system:serviceaccount:foo:default" cannot list pods at the cluster 
scope.
As you can see, your pod can list pods in the foo namespace, but not in any other spe-
cific namespace or across all namespaces. This is visualized in figure 12.10.
SUMMARIZING ROLE, CLUSTERROLE, ROLEBINDING, AND CLUSTERROLEBINDING COMBINATIONS
We’ve covered many different combinations and it may be hard for you to remember
when to use each one. Let’s see if we can make sense of all these combinations by cat-
egorizing them per specific use case. Refer to table 12.2.
Table 12.2
When to use specific combinations of role and binding types
For accessing
Role type to use
Binding type to use
Cluster-level resources (Nodes, PersistentVolumes, ...)
ClusterRole
ClusterRoleBinding
Non-resource URLs (/api, /healthz, ...)
ClusterRole
ClusterRoleBinding
Namespaced resources in any namespace (and 
across all namespaces)
ClusterRole
ClusterRoleBinding
Namespaced resources in a specific namespace (reus-
ing the same ClusterRole in multiple namespaces)
ClusterRole
RoleBinding
Namespaced resources in a specific namespace 
(Role must be defined in each namespace)
Role
RoleBinding
Namespace: foo
Cluster-level resources
Namespace: bar
Pods
Pods
ClusterRole:
view
Allows getting,
listing, watching
RoleBinding:
view-test
Pods,
Services,
Endpoints,
ConﬁgMaps,
…
Default ServiceAccount in
foo namespace is only allowed
to view pods in namespace foo,
despite using a ClusterRole
Service-
Account:
default
Figure 12.10
A RoleBinding referring to a ClusterRole only grants access to resources inside the 
RoleBinding’s namespace.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="403">
  <data key="d0">Page_403</data>
  <data key="d5">Page_403</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_314">
  <data key="d0">371
Securing the cluster with role-based access control
Hopefully, the relationships between the four RBAC resources are much clearer
now. Don’t worry if you still feel like you don’t yet grasp everything. Things may
clear up as we explore the pre-configured ClusterRoles and ClusterRoleBindings in
the next section.
12.2.5 Understanding default ClusterRoles and ClusterRoleBindings
Kubernetes comes with a default set of ClusterRoles and ClusterRoleBindings, which
are updated every time the API server starts. This ensures all the default roles and
bindings are recreated if you mistakenly delete them or if a newer version of Kuberne-
tes uses a different configuration of cluster roles and bindings.
 You can see the default cluster roles and bindings in the following listing.
$ kubectl get clusterrolebindings
NAME                                           AGE
cluster-admin                                  1d
system:basic-user                              1d
system:controller:attachdetach-controller      1d
...
system:controller:ttl-controller               1d
system:discovery                               1d
system:kube-controller-manager                 1d
system:kube-dns                                1d
system:kube-scheduler                          1d
system:node                                    1d
system:node-proxier                            1d
$ kubectl get clusterroles
NAME                                           AGE
admin                                          1d
cluster-admin                                  1d
edit                                           1d
system:auth-delegator                          1d
system:basic-user                              1d
system:controller:attachdetach-controller      1d
...
system:controller:ttl-controller               1d
system:discovery                               1d
system:heapster                                1d
system:kube-aggregator                         1d
system:kube-controller-manager                 1d
system:kube-dns                                1d
system:kube-scheduler                          1d
system:node                                    1d
system:node-bootstrapper                       1d
system:node-problem-detector                   1d
system:node-proxier                            1d
system:persistent-volume-provisioner           1d
view                                           1d
Listing 12.17
Listing all ClusterRoleBindings and ClusterRoles
 
</data>
  <data key="d5">371
Securing the cluster with role-based access control
Hopefully, the relationships between the four RBAC resources are much clearer
now. Don’t worry if you still feel like you don’t yet grasp everything. Things may
clear up as we explore the pre-configured ClusterRoles and ClusterRoleBindings in
the next section.
12.2.5 Understanding default ClusterRoles and ClusterRoleBindings
Kubernetes comes with a default set of ClusterRoles and ClusterRoleBindings, which
are updated every time the API server starts. This ensures all the default roles and
bindings are recreated if you mistakenly delete them or if a newer version of Kuberne-
tes uses a different configuration of cluster roles and bindings.
 You can see the default cluster roles and bindings in the following listing.
$ kubectl get clusterrolebindings
NAME                                           AGE
cluster-admin                                  1d
system:basic-user                              1d
system:controller:attachdetach-controller      1d
...
system:controller:ttl-controller               1d
system:discovery                               1d
system:kube-controller-manager                 1d
system:kube-dns                                1d
system:kube-scheduler                          1d
system:node                                    1d
system:node-proxier                            1d
$ kubectl get clusterroles
NAME                                           AGE
admin                                          1d
cluster-admin                                  1d
edit                                           1d
system:auth-delegator                          1d
system:basic-user                              1d
system:controller:attachdetach-controller      1d
...
system:controller:ttl-controller               1d
system:discovery                               1d
system:heapster                                1d
system:kube-aggregator                         1d
system:kube-controller-manager                 1d
system:kube-dns                                1d
system:kube-scheduler                          1d
system:node                                    1d
system:node-bootstrapper                       1d
system:node-problem-detector                   1d
system:node-proxier                            1d
system:persistent-volume-provisioner           1d
view                                           1d
Listing 12.17
Listing all ClusterRoleBindings and ClusterRoles
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="404">
  <data key="d0">Page_404</data>
  <data key="d5">Page_404</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_315">
  <data key="d0">372
CHAPTER 12
Securing the Kubernetes API server
The most important roles are the view, edit, admin, and cluster-admin ClusterRoles.
They’re meant to be bound to ServiceAccounts used by user-defined pods.
ALLOWING READ-ONLY ACCESS TO RESOURCES WITH THE VIEW CLUSTERROLE
You already used the default view ClusterRole in the previous example. It allows read-
ing most resources in a namespace, except for Roles, RoleBindings, and Secrets. You’re
probably wondering, why not Secrets? Because one of those Secrets might include an
authentication token with greater privileges than those defined in the view Cluster-
Role and could allow the user to masquerade as a different user to gain additional
privileges (privilege escalation). 
ALLOWING MODIFYING RESOURCES WITH THE EDIT CLUSTERROLE
Next is the edit ClusterRole, which allows you to modify resources in a namespace,
but also allows both reading and modifying Secrets. It doesn’t, however, allow viewing
or modifying Roles or RoleBindings—again, this is to prevent privilege escalation.
GRANTING FULL CONTROL OF A NAMESPACE WITH THE ADMIN CLUSTERROLE
Complete control of the resources in a namespace is granted in the admin Cluster-
Role. Subjects with this ClusterRole can read and modify any resource in the name-
space, except ResourceQuotas (we’ll learn what those are in chapter 14) and the
Namespace resource itself. The main difference between the edit and the admin Cluster-
Roles is in the ability to view and modify Roles and RoleBindings in the namespace.
NOTE
To prevent privilege escalation, the API server only allows users to cre-
ate and update Roles if they already have all the permissions listed in that
Role (and for the same scope). 
ALLOWING COMPLETE CONTROL WITH THE CLUSTER-ADMIN CLUSTERROLE 
Complete control of the Kubernetes cluster can be given by assigning the cluster-
admin ClusterRole to a subject. As you’ve seen before, the admin ClusterRole doesn’t
allow users to modify the namespace’s ResourceQuota objects or the Namespace
resource itself. If you want to allow a user to do that, you need to create a RoleBinding
that references the cluster-admin ClusterRole. This gives the user included in the
RoleBinding complete control over all aspects of the namespace in which the Role-
Binding is created.
 If you’ve paid attention, you probably already know how to give users complete
control of all the namespaces in the cluster. Yes, by referencing the cluster-admin
ClusterRole in a ClusterRoleBinding instead of a RoleBinding.
UNDERSTANDING THE OTHER DEFAULT CLUSTERROLES
The list of default ClusterRoles includes a large number of other ClusterRoles, which
start with the system: prefix. These are meant to be used by the various Kubernetes
components. Among them, you’ll find roles such as system:kube-scheduler, which
is obviously used by the Scheduler, system:node, which is used by the Kubelets, and
so on. 
 
</data>
  <data key="d5">372
CHAPTER 12
Securing the Kubernetes API server
The most important roles are the view, edit, admin, and cluster-admin ClusterRoles.
They’re meant to be bound to ServiceAccounts used by user-defined pods.
ALLOWING READ-ONLY ACCESS TO RESOURCES WITH THE VIEW CLUSTERROLE
You already used the default view ClusterRole in the previous example. It allows read-
ing most resources in a namespace, except for Roles, RoleBindings, and Secrets. You’re
probably wondering, why not Secrets? Because one of those Secrets might include an
authentication token with greater privileges than those defined in the view Cluster-
Role and could allow the user to masquerade as a different user to gain additional
privileges (privilege escalation). 
ALLOWING MODIFYING RESOURCES WITH THE EDIT CLUSTERROLE
Next is the edit ClusterRole, which allows you to modify resources in a namespace,
but also allows both reading and modifying Secrets. It doesn’t, however, allow viewing
or modifying Roles or RoleBindings—again, this is to prevent privilege escalation.
GRANTING FULL CONTROL OF A NAMESPACE WITH THE ADMIN CLUSTERROLE
Complete control of the resources in a namespace is granted in the admin Cluster-
Role. Subjects with this ClusterRole can read and modify any resource in the name-
space, except ResourceQuotas (we’ll learn what those are in chapter 14) and the
Namespace resource itself. The main difference between the edit and the admin Cluster-
Roles is in the ability to view and modify Roles and RoleBindings in the namespace.
NOTE
To prevent privilege escalation, the API server only allows users to cre-
ate and update Roles if they already have all the permissions listed in that
Role (and for the same scope). 
ALLOWING COMPLETE CONTROL WITH THE CLUSTER-ADMIN CLUSTERROLE 
Complete control of the Kubernetes cluster can be given by assigning the cluster-
admin ClusterRole to a subject. As you’ve seen before, the admin ClusterRole doesn’t
allow users to modify the namespace’s ResourceQuota objects or the Namespace
resource itself. If you want to allow a user to do that, you need to create a RoleBinding
that references the cluster-admin ClusterRole. This gives the user included in the
RoleBinding complete control over all aspects of the namespace in which the Role-
Binding is created.
 If you’ve paid attention, you probably already know how to give users complete
control of all the namespaces in the cluster. Yes, by referencing the cluster-admin
ClusterRole in a ClusterRoleBinding instead of a RoleBinding.
UNDERSTANDING THE OTHER DEFAULT CLUSTERROLES
The list of default ClusterRoles includes a large number of other ClusterRoles, which
start with the system: prefix. These are meant to be used by the various Kubernetes
components. Among them, you’ll find roles such as system:kube-scheduler, which
is obviously used by the Scheduler, system:node, which is used by the Kubelets, and
so on. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="405">
  <data key="d0">Page_405</data>
  <data key="d5">Page_405</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_316">
  <data key="d0">373
Summary
 Although the Controller Manager runs as a single pod, each controller running
inside it can use a separate ClusterRole and ClusterRoleBinding (they’re prefixed
with system: controller:). 
 Each of these system ClusterRoles has a matching ClusterRoleBinding, which binds
it to the user the system component authenticates as. The system:kube-scheduler
ClusterRoleBinding, for example, assigns the identically named ClusterRole to the
system:kube-scheduler user, which is the username the scheduler Authenticates as. 
12.2.6 Granting authorization permissions wisely
By default, the default ServiceAccount in a namespace has no permissions other than
those of an unauthenticated user (as you may remember from one of the previous
examples, the system:discovery ClusterRole and associated binding allow anyone to
make GET requests on a few non-resource URLs). Therefore, pods, by default, can’t
even view cluster state. It’s up to you to grant them appropriate permissions to do that. 
 Obviously, giving all your ServiceAccounts the cluster-admin ClusterRole is a
bad idea. As is always the case with security, it’s best to give everyone only the permis-
sions they need to do their job and not a single permission more (principle of least
privilege).
CREATING SPECIFIC SERVICEACCOUNTS FOR EACH POD
It’s a good idea to create a specific ServiceAccount for each pod (or a set of pod rep-
licas) and then associate it with a tailor-made Role (or a ClusterRole) through a
RoleBinding (not a ClusterRoleBinding, because that would give the pod access to
resources in other namespaces, which is probably not what you want). 
 If one of your pods (the application running within it) only needs to read pods,
while the other also needs to modify them, then create two different ServiceAccounts
and make those pods use them by specifying the serviceAccountName property in the
pod spec, as you learned in the first part of this chapter. Don’t add all the necessary
permissions required by both pods to the default ServiceAccount in the namespace. 
EXPECTING YOUR APPS TO BE COMPROMISED
Your aim is to reduce the possibility of an intruder getting hold of your cluster. Today’s
complex apps contain many vulnerabilities. You should expect unwanted persons to
eventually get their hands on the ServiceAccount’s authentication token, so you should
always constrain the ServiceAccount to prevent them from doing any real damage.
12.3
Summary
This chapter has given you a foundation on how to secure the Kubernetes API server.
You learned the following:
Clients of the API server include both human users and applications running
in pods.
Applications in pods are associated with a ServiceAccount. 
Both users and ServiceAccounts are associated with groups.
 
</data>
  <data key="d5">373
Summary
 Although the Controller Manager runs as a single pod, each controller running
inside it can use a separate ClusterRole and ClusterRoleBinding (they’re prefixed
with system: controller:). 
 Each of these system ClusterRoles has a matching ClusterRoleBinding, which binds
it to the user the system component authenticates as. The system:kube-scheduler
ClusterRoleBinding, for example, assigns the identically named ClusterRole to the
system:kube-scheduler user, which is the username the scheduler Authenticates as. 
12.2.6 Granting authorization permissions wisely
By default, the default ServiceAccount in a namespace has no permissions other than
those of an unauthenticated user (as you may remember from one of the previous
examples, the system:discovery ClusterRole and associated binding allow anyone to
make GET requests on a few non-resource URLs). Therefore, pods, by default, can’t
even view cluster state. It’s up to you to grant them appropriate permissions to do that. 
 Obviously, giving all your ServiceAccounts the cluster-admin ClusterRole is a
bad idea. As is always the case with security, it’s best to give everyone only the permis-
sions they need to do their job and not a single permission more (principle of least
privilege).
CREATING SPECIFIC SERVICEACCOUNTS FOR EACH POD
It’s a good idea to create a specific ServiceAccount for each pod (or a set of pod rep-
licas) and then associate it with a tailor-made Role (or a ClusterRole) through a
RoleBinding (not a ClusterRoleBinding, because that would give the pod access to
resources in other namespaces, which is probably not what you want). 
 If one of your pods (the application running within it) only needs to read pods,
while the other also needs to modify them, then create two different ServiceAccounts
and make those pods use them by specifying the serviceAccountName property in the
pod spec, as you learned in the first part of this chapter. Don’t add all the necessary
permissions required by both pods to the default ServiceAccount in the namespace. 
EXPECTING YOUR APPS TO BE COMPROMISED
Your aim is to reduce the possibility of an intruder getting hold of your cluster. Today’s
complex apps contain many vulnerabilities. You should expect unwanted persons to
eventually get their hands on the ServiceAccount’s authentication token, so you should
always constrain the ServiceAccount to prevent them from doing any real damage.
12.3
Summary
This chapter has given you a foundation on how to secure the Kubernetes API server.
You learned the following:
Clients of the API server include both human users and applications running
in pods.
Applications in pods are associated with a ServiceAccount. 
Both users and ServiceAccounts are associated with groups.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="406">
  <data key="d0">Page_406</data>
  <data key="d5">Page_406</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_317">
  <data key="d0">374
CHAPTER 12
Securing the Kubernetes API server
By default, pods run under the default ServiceAccount, which is created for
each namespace automatically.
Additional ServiceAccounts can be created manually and associated with a pod.
ServiceAccounts can be configured to allow mounting only a constrained list of
Secrets in a given pod.
A ServiceAccount can also be used to attach image pull Secrets to pods, so you
don’t need to specify the Secrets in every pod.
Roles and ClusterRoles define what actions can be performed on which resources.
RoleBindings and ClusterRoleBindings bind Roles and ClusterRoles to users,
groups, and ServiceAccounts.
Each cluster comes with default ClusterRoles and ClusterRoleBindings.
In the next chapter, you’ll learn how to protect the cluster nodes from pods and how
to isolate pods from each other by securing the network.
 
</data>
  <data key="d5">374
CHAPTER 12
Securing the Kubernetes API server
By default, pods run under the default ServiceAccount, which is created for
each namespace automatically.
Additional ServiceAccounts can be created manually and associated with a pod.
ServiceAccounts can be configured to allow mounting only a constrained list of
Secrets in a given pod.
A ServiceAccount can also be used to attach image pull Secrets to pods, so you
don’t need to specify the Secrets in every pod.
Roles and ClusterRoles define what actions can be performed on which resources.
RoleBindings and ClusterRoleBindings bind Roles and ClusterRoles to users,
groups, and ServiceAccounts.
Each cluster comes with default ClusterRoles and ClusterRoleBindings.
In the next chapter, you’ll learn how to protect the cluster nodes from pods and how
to isolate pods from each other by securing the network.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="407">
  <data key="d0">Page_407</data>
  <data key="d5">Page_407</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_318">
  <data key="d0">375
Securing cluster nodes
and the network
In the previous chapter, we talked about securing the API server. If an attacker
gets access to the API server, they can run whatever they like by packaging their
code into a container image and running it in a pod. But can they do any real
damage? Aren’t containers isolated from other containers and from the node
they’re running on? 
 Not necessarily. In this chapter, you’ll learn how to allow pods to access the
resources of the node they’re running on. You’ll also learn how to configure the
cluster so users aren’t able to do whatever they want with their pods. Then, in
This chapter covers
Using the node’s default Linux namespaces 
in pods
Running containers as different users
Running privileged containers
Adding or dropping a container’s kernel 
capabilities
Defining security policies to limit what pods can do
Securing the pod network
 
</data>
  <data key="d5">375
Securing cluster nodes
and the network
In the previous chapter, we talked about securing the API server. If an attacker
gets access to the API server, they can run whatever they like by packaging their
code into a container image and running it in a pod. But can they do any real
damage? Aren’t containers isolated from other containers and from the node
they’re running on? 
 Not necessarily. In this chapter, you’ll learn how to allow pods to access the
resources of the node they’re running on. You’ll also learn how to configure the
cluster so users aren’t able to do whatever they want with their pods. Then, in
This chapter covers
Using the node’s default Linux namespaces 
in pods
Running containers as different users
Running privileged containers
Adding or dropping a container’s kernel 
capabilities
Defining security policies to limit what pods can do
Securing the pod network
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="408">
  <data key="d0">Page_408</data>
  <data key="d5">Page_408</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_319">
  <data key="d0">376
CHAPTER 13
Securing cluster nodes and the network
the last part of the chapter, you’ll also learn how to secure the network the pods use
to communicate.
13.1
Using the host node’s namespaces in a pod
Containers in a pod usually run under separate Linux namespaces, which isolate
their processes from processes running in other containers or in the node’s default
namespaces. 
 For example, we learned that each pod gets its own IP and port space, because it
uses its own network namespace. Likewise, each pod has its own process tree, because
it has its own PID namespace, and it also uses its own IPC namespace, allowing only
processes in the same pod to communicate with each other through the Inter-Process
Communication mechanism (IPC).
13.1.1 Using the node’s network namespace in a pod
Certain pods (usually system pods) need to operate in the host’s default namespaces,
allowing them to see and manipulate node-level resources and devices. For example, a
pod may need to use the node’s network adapters instead of its own virtual network
adapters. This can be achieved by setting the hostNetwork property in the pod spec
to true.
 In that case, the pod gets to use the node’s network interfaces instead of having its
own set, as shown in figure 13.1. This means the pod doesn’t get its own IP address and
if it runs a process that binds to a port, the process will be bound to the node’s port.
You can try running such a pod. The next listing shows an example pod manifest.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-host-network
Listing 13.1
A pod using the node’s network namespace: pod-with-host-network.yaml
Node
Pod A
Pod’s own network
namespace
eth0
lo
eth0
docker0
lo
eth1
Node’s default network
namespace
Pod B
hostNetwork: true
Figure 13.1
A pod 
with hostNetwork: 
true uses the node’s 
network interfaces 
instead of its own.
 
</data>
  <data key="d5">376
CHAPTER 13
Securing cluster nodes and the network
the last part of the chapter, you’ll also learn how to secure the network the pods use
to communicate.
13.1
Using the host node’s namespaces in a pod
Containers in a pod usually run under separate Linux namespaces, which isolate
their processes from processes running in other containers or in the node’s default
namespaces. 
 For example, we learned that each pod gets its own IP and port space, because it
uses its own network namespace. Likewise, each pod has its own process tree, because
it has its own PID namespace, and it also uses its own IPC namespace, allowing only
processes in the same pod to communicate with each other through the Inter-Process
Communication mechanism (IPC).
13.1.1 Using the node’s network namespace in a pod
Certain pods (usually system pods) need to operate in the host’s default namespaces,
allowing them to see and manipulate node-level resources and devices. For example, a
pod may need to use the node’s network adapters instead of its own virtual network
adapters. This can be achieved by setting the hostNetwork property in the pod spec
to true.
 In that case, the pod gets to use the node’s network interfaces instead of having its
own set, as shown in figure 13.1. This means the pod doesn’t get its own IP address and
if it runs a process that binds to a port, the process will be bound to the node’s port.
You can try running such a pod. The next listing shows an example pod manifest.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-host-network
Listing 13.1
A pod using the node’s network namespace: pod-with-host-network.yaml
Node
Pod A
Pod’s own network
namespace
eth0
lo
eth0
docker0
lo
eth1
Node’s default network
namespace
Pod B
hostNetwork: true
Figure 13.1
A pod 
with hostNetwork: 
true uses the node’s 
network interfaces 
instead of its own.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="409">
  <data key="d0">Page_409</data>
  <data key="d5">Page_409</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_320">
  <data key="d0">377
Using the host node’s namespaces in a pod
spec:
  hostNetwork: true              
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
After you run the pod, you can use the following command to see that it’s indeed using
the host’s network namespace (it sees all the host’s network adapters, for example).
$ kubectl exec pod-with-host-network ifconfig
docker0   Link encap:Ethernet  HWaddr 02:42:14:08:23:47
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          ...
eth0      Link encap:Ethernet  HWaddr 08:00:27:F8:FA:4E
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          ...
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          ...
veth1178d4f Link encap:Ethernet  HWaddr 1E:03:8D:D6:E1:2C
          inet6 addr: fe80::1c03:8dff:fed6:e12c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
...
When the Kubernetes Control Plane components are deployed as pods (such as when
you deploy your cluster with kubeadm, as explained in appendix B), you’ll find that
those pods use the hostNetwork option, effectively making them behave as if they
weren’t running inside a pod.
13.1.2 Binding to a host port without using the host’s network 
namespace
A related feature allows pods to bind to a port in the node’s default namespace, but
still have their own network namespace. This is done by using the hostPort property
in one of the container’s ports defined in the spec.containers.ports field.
 Don’t confuse pods using hostPort with pods exposed through a NodePort service.
They’re two different things, as explained in figure 13.2.
 The first thing you’ll notice in the figure is that when a pod is using a hostPort, a
connection to the node’s port is forwarded directly to the pod running on that node,
whereas with a NodePort service, a connection to the node’s port is forwarded to a
randomly selected pod (possibly on another node). The other difference is that with
pods using a hostPort, the node’s port is only bound on nodes that run such pods,
whereas NodePort services bind the port on all nodes, even on those that don’t run
such a pod (as on node 3 in the figure).
Listing 13.2
Network interfaces in a pod using the host’s network namespace
Using the host node’s 
network namespace
 
</data>
  <data key="d5">377
Using the host node’s namespaces in a pod
spec:
  hostNetwork: true              
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
After you run the pod, you can use the following command to see that it’s indeed using
the host’s network namespace (it sees all the host’s network adapters, for example).
$ kubectl exec pod-with-host-network ifconfig
docker0   Link encap:Ethernet  HWaddr 02:42:14:08:23:47
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          ...
eth0      Link encap:Ethernet  HWaddr 08:00:27:F8:FA:4E
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          ...
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          ...
veth1178d4f Link encap:Ethernet  HWaddr 1E:03:8D:D6:E1:2C
          inet6 addr: fe80::1c03:8dff:fed6:e12c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
...
When the Kubernetes Control Plane components are deployed as pods (such as when
you deploy your cluster with kubeadm, as explained in appendix B), you’ll find that
those pods use the hostNetwork option, effectively making them behave as if they
weren’t running inside a pod.
13.1.2 Binding to a host port without using the host’s network 
namespace
A related feature allows pods to bind to a port in the node’s default namespace, but
still have their own network namespace. This is done by using the hostPort property
in one of the container’s ports defined in the spec.containers.ports field.
 Don’t confuse pods using hostPort with pods exposed through a NodePort service.
They’re two different things, as explained in figure 13.2.
 The first thing you’ll notice in the figure is that when a pod is using a hostPort, a
connection to the node’s port is forwarded directly to the pod running on that node,
whereas with a NodePort service, a connection to the node’s port is forwarded to a
randomly selected pod (possibly on another node). The other difference is that with
pods using a hostPort, the node’s port is only bound on nodes that run such pods,
whereas NodePort services bind the port on all nodes, even on those that don’t run
such a pod (as on node 3 in the figure).
Listing 13.2
Network interfaces in a pod using the host’s network namespace
Using the host node’s 
network namespace
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="410">
  <data key="d0">Page_410</data>
  <data key="d5">Page_410</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_321">
  <data key="d0">378
CHAPTER 13
Securing cluster nodes and the network
It’s important to understand that if a pod is using a specific host port, only one
instance of the pod can be scheduled to each node, because two processes can’t bind
to the same host port. The Scheduler takes this into account when scheduling pods, so
it doesn’t schedule multiple pods to the same node, as shown in figure 13.3. If you
have three nodes and want to deploy four pod replicas, only three will be scheduled
(one pod will remain Pending).
Node 1
Pod 1
Two pods using
hostPort
Port
8080
Port
9000
Node 2
Pod 2
Port
8080
Port
9000
Node 3
Node 1
Pod 1
Two pods under
the same
NodePort
service
Port
8080
Node 2
Pod 2
Port
8080
Node 3
Port
88
Port
88
Port
88
Service
(
)
iptables
Service
(
)
iptables
Service
(
)
iptables
Figure 13.2
Difference between pods using a hostPort and pods behind a NodePort service.
Node 1
Pod 1
Port
8080
Host
port
9000
Host
port
9000
Pod 2
Port
8080
Node 2
Pod 3
Port
8080
Host
port
9000
Node 3
Pod 4
Port
8080
Cannot be scheduled to the same
node, because the port is already bound
Only a single
replica per node
Figure 13.3
If a host port is used, only a single pod instance can be scheduled to a node.
 
</data>
  <data key="d5">378
CHAPTER 13
Securing cluster nodes and the network
It’s important to understand that if a pod is using a specific host port, only one
instance of the pod can be scheduled to each node, because two processes can’t bind
to the same host port. The Scheduler takes this into account when scheduling pods, so
it doesn’t schedule multiple pods to the same node, as shown in figure 13.3. If you
have three nodes and want to deploy four pod replicas, only three will be scheduled
(one pod will remain Pending).
Node 1
Pod 1
Two pods using
hostPort
Port
8080
Port
9000
Node 2
Pod 2
Port
8080
Port
9000
Node 3
Node 1
Pod 1
Two pods under
the same
NodePort
service
Port
8080
Node 2
Pod 2
Port
8080
Node 3
Port
88
Port
88
Port
88
Service
(
)
iptables
Service
(
)
iptables
Service
(
)
iptables
Figure 13.2
Difference between pods using a hostPort and pods behind a NodePort service.
Node 1
Pod 1
Port
8080
Host
port
9000
Host
port
9000
Pod 2
Port
8080
Node 2
Pod 3
Port
8080
Host
port
9000
Node 3
Pod 4
Port
8080
Cannot be scheduled to the same
node, because the port is already bound
Only a single
replica per node
Figure 13.3
If a host port is used, only a single pod instance can be scheduled to a node.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="411">
  <data key="d0">Page_411</data>
  <data key="d5">Page_411</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_322">
  <data key="d0">379
Using the host node’s namespaces in a pod
Let’s see how to define the hostPort in a pod’s YAML definition. The following listing
shows the YAML to run your kubia pod and bind it to the node’s port 9000.
apiVersion: v1
kind: Pod
metadata:
  name: kubia-hostport
spec:
  containers:
  - image: luksa/kubia
    name: kubia
    ports:
    - containerPort: 8080    
      hostPort: 9000        
      protocol: TCP
After you create this pod, you can access it through port 9000 of the node it’s sched-
uled to. If you have multiple nodes, you’ll see you can’t access the pod through that
port on the other nodes. 
NOTE
If you’re trying this on GKE, you need to configure the firewall prop-
erly using gcloud compute firewall-rules, the way you did in chapter 5.
The hostPort feature is primarily used for exposing system services, which are
deployed to every node using DaemonSets. Initially, people also used it to ensure two
replicas of the same pod were never scheduled to the same node, but now you have a
better way of achieving this—it’s explained in chapter 16.
13.1.3 Using the node’s PID and IPC namespaces
Similar to the hostNetwork option are the hostPID and hostIPC pod spec properties.
When you set them to true, the pod’s containers will use the node’s PID and IPC
namespaces, allowing processes running in the containers to see all the other pro-
cesses on the node or communicate with them through IPC, respectively. See the fol-
lowing listing for an example.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-host-pid-and-ipc
spec:
  hostPID: true                    
  hostIPC: true                     
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
Listing 13.3
Binding a pod to a port in the node’s port space: kubia-hostport.yaml
Listing 13.4
Using the host’s PID and IPC namespaces: pod-with-host-pid-and-ipc.yaml
The container can be 
reached on port 8080 
of the pod’s IP.
It can also be reached 
on port 9000 of the 
node it’s deployed on.
You want the pod to 
use the host’s PID 
namespace.
You also want the 
pod to use the host’s 
IPC namespace.
 
</data>
  <data key="d5">379
Using the host node’s namespaces in a pod
Let’s see how to define the hostPort in a pod’s YAML definition. The following listing
shows the YAML to run your kubia pod and bind it to the node’s port 9000.
apiVersion: v1
kind: Pod
metadata:
  name: kubia-hostport
spec:
  containers:
  - image: luksa/kubia
    name: kubia
    ports:
    - containerPort: 8080    
      hostPort: 9000        
      protocol: TCP
After you create this pod, you can access it through port 9000 of the node it’s sched-
uled to. If you have multiple nodes, you’ll see you can’t access the pod through that
port on the other nodes. 
NOTE
If you’re trying this on GKE, you need to configure the firewall prop-
erly using gcloud compute firewall-rules, the way you did in chapter 5.
The hostPort feature is primarily used for exposing system services, which are
deployed to every node using DaemonSets. Initially, people also used it to ensure two
replicas of the same pod were never scheduled to the same node, but now you have a
better way of achieving this—it’s explained in chapter 16.
13.1.3 Using the node’s PID and IPC namespaces
Similar to the hostNetwork option are the hostPID and hostIPC pod spec properties.
When you set them to true, the pod’s containers will use the node’s PID and IPC
namespaces, allowing processes running in the containers to see all the other pro-
cesses on the node or communicate with them through IPC, respectively. See the fol-
lowing listing for an example.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-host-pid-and-ipc
spec:
  hostPID: true                    
  hostIPC: true                     
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
Listing 13.3
Binding a pod to a port in the node’s port space: kubia-hostport.yaml
Listing 13.4
Using the host’s PID and IPC namespaces: pod-with-host-pid-and-ipc.yaml
The container can be 
reached on port 8080 
of the pod’s IP.
It can also be reached 
on port 9000 of the 
node it’s deployed on.
You want the pod to 
use the host’s PID 
namespace.
You also want the 
pod to use the host’s 
IPC namespace.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="412">
  <data key="d0">Page_412</data>
  <data key="d5">Page_412</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_323">
  <data key="d0">380
CHAPTER 13
Securing cluster nodes and the network
You’ll remember that pods usually see only their own processes, but if you run this pod
and then list the processes from within its container, you’ll see all the processes run-
ning on the host node, not only the ones running in the container, as shown in the
following listing.
$ kubectl exec pod-with-host-pid-and-ipc ps aux
PID   USER     TIME   COMMAND
    1 root       0:01 /usr/lib/systemd/systemd --switched-root --system ...
    2 root       0:00 [kthreadd]
    3 root       0:00 [ksoftirqd/0]
    5 root       0:00 [kworker/0:0H]
    6 root       0:00 [kworker/u2:0]
    7 root       0:00 [migration/0]
    8 root       0:00 [rcu_bh]
    9 root       0:00 [rcu_sched]
   10 root       0:00 [watchdog/0]
...
By setting the hostIPC property to true, processes in the pod’s containers can also
communicate with all the other processes running on the node, through Inter-Process
Communication.
13.2
Configuring the container’s security context
Besides allowing the pod to use the host’s Linux namespaces, other security-related
features can also be configured on the pod and its container through the security-
Context properties, which can be specified under the pod spec directly and inside the
spec of individual containers.
UNDERSTANDING WHAT’S CONFIGURABLE IN THE SECURITY CONTEXT
Configuring the security context allows you to do various things:
Specify the user (the user’s ID) under which the process in the container will run.
Prevent the container from running as root (the default user a container runs
as is usually defined in the container image itself, so you may want to prevent
containers from running as root).
Run the container in privileged mode, giving it full access to the node’s kernel.
Configure fine-grained privileges, by adding or dropping capabilities—in con-
trast to giving the container all possible permissions by running it in privi-
leged mode.
Set SELinux (Security Enhanced Linux) options to strongly lock down a
container.
Prevent the process from writing to the container’s filesystem.
We’ll explore these options next. 
Listing 13.5
Processes visible in a pod with hostPID: true
 
</data>
  <data key="d5">380
CHAPTER 13
Securing cluster nodes and the network
You’ll remember that pods usually see only their own processes, but if you run this pod
and then list the processes from within its container, you’ll see all the processes run-
ning on the host node, not only the ones running in the container, as shown in the
following listing.
$ kubectl exec pod-with-host-pid-and-ipc ps aux
PID   USER     TIME   COMMAND
    1 root       0:01 /usr/lib/systemd/systemd --switched-root --system ...
    2 root       0:00 [kthreadd]
    3 root       0:00 [ksoftirqd/0]
    5 root       0:00 [kworker/0:0H]
    6 root       0:00 [kworker/u2:0]
    7 root       0:00 [migration/0]
    8 root       0:00 [rcu_bh]
    9 root       0:00 [rcu_sched]
   10 root       0:00 [watchdog/0]
...
By setting the hostIPC property to true, processes in the pod’s containers can also
communicate with all the other processes running on the node, through Inter-Process
Communication.
13.2
Configuring the container’s security context
Besides allowing the pod to use the host’s Linux namespaces, other security-related
features can also be configured on the pod and its container through the security-
Context properties, which can be specified under the pod spec directly and inside the
spec of individual containers.
UNDERSTANDING WHAT’S CONFIGURABLE IN THE SECURITY CONTEXT
Configuring the security context allows you to do various things:
Specify the user (the user’s ID) under which the process in the container will run.
Prevent the container from running as root (the default user a container runs
as is usually defined in the container image itself, so you may want to prevent
containers from running as root).
Run the container in privileged mode, giving it full access to the node’s kernel.
Configure fine-grained privileges, by adding or dropping capabilities—in con-
trast to giving the container all possible permissions by running it in privi-
leged mode.
Set SELinux (Security Enhanced Linux) options to strongly lock down a
container.
Prevent the process from writing to the container’s filesystem.
We’ll explore these options next. 
Listing 13.5
Processes visible in a pod with hostPID: true
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="413">
  <data key="d0">Page_413</data>
  <data key="d5">Page_413</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_324">
  <data key="d0">381
Configuring the container’s security context
RUNNING A POD WITHOUT SPECIFYING A SECURITY CONTEXT
First, run a pod with the default security context options (by not specifying them at
all), so you can see how it behaves compared to pods with a custom security context:
$ kubectl run pod-with-defaults --image alpine --restart Never 
➥  -- /bin/sleep 999999
pod "pod-with-defaults" created
Let’s see what user and group ID the container is running as, and which groups it
belongs to. You can see this by running the id command inside the container:
$ kubectl exec pod-with-defaults id
uid=0(root) gid=0(root) groups=0(root), 1(bin), 2(daemon), 3(sys), 4(adm), 
6(disk), 10(wheel), 11(floppy), 20(dialout), 26(tape), 27(video)
The container is running as user ID (uid) 0, which is root, and group ID (gid) 0 (also
root). It’s also a member of multiple other groups. 
NOTE
What user the container runs as is specified in the container image. In
a Dockerfile, this is done using the USER directive. If omitted, the container
runs as root.
Now, you’ll run a pod where the container runs as a different user.
13.2.1 Running a container as a specific user
To run a pod under a different user ID than the one that’s baked into the container
image, you’ll need to set the pod’s securityContext.runAsUser property. You’ll
make the container run as user guest, whose user ID in the alpine container image is
405, as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-as-user-guest
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 405      
Now, to see the effect of the runAsUser property, run the id command in this new
pod, the way you did before:
$ kubectl exec pod-as-user-guest id
uid=405(guest) gid=100(users)
Listing 13.6
Running containers as a specific user: pod-as-user-guest.yaml
You need to specify a user ID, not 
a username (id 405 corresponds 
to the guest user).
 
</data>
  <data key="d5">381
Configuring the container’s security context
RUNNING A POD WITHOUT SPECIFYING A SECURITY CONTEXT
First, run a pod with the default security context options (by not specifying them at
all), so you can see how it behaves compared to pods with a custom security context:
$ kubectl run pod-with-defaults --image alpine --restart Never 
➥  -- /bin/sleep 999999
pod "pod-with-defaults" created
Let’s see what user and group ID the container is running as, and which groups it
belongs to. You can see this by running the id command inside the container:
$ kubectl exec pod-with-defaults id
uid=0(root) gid=0(root) groups=0(root), 1(bin), 2(daemon), 3(sys), 4(adm), 
6(disk), 10(wheel), 11(floppy), 20(dialout), 26(tape), 27(video)
The container is running as user ID (uid) 0, which is root, and group ID (gid) 0 (also
root). It’s also a member of multiple other groups. 
NOTE
What user the container runs as is specified in the container image. In
a Dockerfile, this is done using the USER directive. If omitted, the container
runs as root.
Now, you’ll run a pod where the container runs as a different user.
13.2.1 Running a container as a specific user
To run a pod under a different user ID than the one that’s baked into the container
image, you’ll need to set the pod’s securityContext.runAsUser property. You’ll
make the container run as user guest, whose user ID in the alpine container image is
405, as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-as-user-guest
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 405      
Now, to see the effect of the runAsUser property, run the id command in this new
pod, the way you did before:
$ kubectl exec pod-as-user-guest id
uid=405(guest) gid=100(users)
Listing 13.6
Running containers as a specific user: pod-as-user-guest.yaml
You need to specify a user ID, not 
a username (id 405 corresponds 
to the guest user).
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="414">
  <data key="d0">Page_414</data>
  <data key="d5">Page_414</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_325">
  <data key="d0">382
CHAPTER 13
Securing cluster nodes and the network
As requested, the container is running as the guest user. 
13.2.2 Preventing a container from running as root
What if you don’t care what user the container runs as, but you still want to prevent it
from running as root? 
 Imagine having a pod deployed with a container image that was built with a USER
daemon directive in the Dockerfile, which makes the container run under the daemon
user. What if an attacker gets access to your image registry and pushes a different
image under the same tag? The attacker’s image is configured to run as the root user.
When Kubernetes schedules a new instance of your pod, the Kubelet will download
the attacker’s image and run whatever code they put into it. 
 Although containers are mostly isolated from the host system, running their pro-
cesses as root is still considered a bad practice. For example, when a host directory is
mounted into the container, if the process running in the container is running as
root, it has full access to the mounted directory, whereas if it’s running as non-root,
it won’t. 
 To prevent the attack scenario described previously, you can specify that the pod’s
container needs to run as a non-root user, as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-run-as-non-root
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:                   
      runAsNonRoot: true               
If you deploy this pod, it gets scheduled, but is not allowed to run:
$ kubectl get po pod-run-as-non-root
NAME                 READY  STATUS                                                  
pod-run-as-non-root  0/1    container has runAsNonRoot and image will run 
                            ➥  as root
Now, if anyone tampers with your container images, they won’t get far.
13.2.3 Running pods in privileged mode
Sometimes pods need to do everything that the node they’re running on can do, such
as use protected system devices or other kernel features, which aren’t accessible to
regular containers. 
Listing 13.7
Preventing containers from running as root: pod-run-as-non-root.yaml
This container will only 
be allowed to run as a 
non-root user.
 
</data>
  <data key="d5">382
CHAPTER 13
Securing cluster nodes and the network
As requested, the container is running as the guest user. 
13.2.2 Preventing a container from running as root
What if you don’t care what user the container runs as, but you still want to prevent it
from running as root? 
 Imagine having a pod deployed with a container image that was built with a USER
daemon directive in the Dockerfile, which makes the container run under the daemon
user. What if an attacker gets access to your image registry and pushes a different
image under the same tag? The attacker’s image is configured to run as the root user.
When Kubernetes schedules a new instance of your pod, the Kubelet will download
the attacker’s image and run whatever code they put into it. 
 Although containers are mostly isolated from the host system, running their pro-
cesses as root is still considered a bad practice. For example, when a host directory is
mounted into the container, if the process running in the container is running as
root, it has full access to the mounted directory, whereas if it’s running as non-root,
it won’t. 
 To prevent the attack scenario described previously, you can specify that the pod’s
container needs to run as a non-root user, as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-run-as-non-root
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:                   
      runAsNonRoot: true               
If you deploy this pod, it gets scheduled, but is not allowed to run:
$ kubectl get po pod-run-as-non-root
NAME                 READY  STATUS                                                  
pod-run-as-non-root  0/1    container has runAsNonRoot and image will run 
                            ➥  as root
Now, if anyone tampers with your container images, they won’t get far.
13.2.3 Running pods in privileged mode
Sometimes pods need to do everything that the node they’re running on can do, such
as use protected system devices or other kernel features, which aren’t accessible to
regular containers. 
Listing 13.7
Preventing containers from running as root: pod-run-as-non-root.yaml
This container will only 
be allowed to run as a 
non-root user.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="415">
  <data key="d0">Page_415</data>
  <data key="d5">Page_415</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_326">
  <data key="d0">383
Configuring the container’s security context
 An example of such a pod is the kube-proxy pod, which needs to modify the node’s
iptables rules to make services work, as was explained in chapter 11. If you follow the
instructions in appendix B and deploy a cluster with kubeadm, you’ll see every cluster
node runs a kube-proxy pod and you can examine its YAML specification to see all the
special features it’s using. 
 To get full access to the node’s kernel, the pod’s container runs in privileged
mode. This is achieved by setting the privileged property in the container’s security-
Context property to true. You’ll create a privileged pod from the YAML in the follow-
ing listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-privileged
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      privileged: true     
Go ahead and deploy this pod, so you can compare it with the non-privileged pod you
ran earlier. 
 If you’re familiar with Linux, you may know it has a special file directory called /dev,
which contains device files for all the devices on the system. These aren’t regular files on
disk, but are special files used to communicate with devices. Let’s see what devices are
visible in the non-privileged container you deployed earlier (the pod-with-defaults
pod), by listing files in its /dev directory, as shown in the following listing.
$ kubectl exec -it pod-with-defaults ls /dev
core             null             stderr           urandom
fd               ptmx             stdin            zero
full             pts              stdout
fuse             random           termination-log
mqueue           shm              tty
The listing shows all the devices. The list is fairly short. Now, compare this with the fol-
lowing listing, which shows the device files your privileged pod can see.
$ kubectl exec -it pod-privileged ls /dev
autofs              snd                 tty46
bsg                 sr0                 tty47
Listing 13.8
A pod with a privileged container: pod-privileged.yaml
Listing 13.9
List of available devices in a non-privileged pod
Listing 13.10
List of available devices in a privileged pod
This container will 
run in privileged 
mode
 
</data>
  <data key="d5">383
Configuring the container’s security context
 An example of such a pod is the kube-proxy pod, which needs to modify the node’s
iptables rules to make services work, as was explained in chapter 11. If you follow the
instructions in appendix B and deploy a cluster with kubeadm, you’ll see every cluster
node runs a kube-proxy pod and you can examine its YAML specification to see all the
special features it’s using. 
 To get full access to the node’s kernel, the pod’s container runs in privileged
mode. This is achieved by setting the privileged property in the container’s security-
Context property to true. You’ll create a privileged pod from the YAML in the follow-
ing listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-privileged
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      privileged: true     
Go ahead and deploy this pod, so you can compare it with the non-privileged pod you
ran earlier. 
 If you’re familiar with Linux, you may know it has a special file directory called /dev,
which contains device files for all the devices on the system. These aren’t regular files on
disk, but are special files used to communicate with devices. Let’s see what devices are
visible in the non-privileged container you deployed earlier (the pod-with-defaults
pod), by listing files in its /dev directory, as shown in the following listing.
$ kubectl exec -it pod-with-defaults ls /dev
core             null             stderr           urandom
fd               ptmx             stdin            zero
full             pts              stdout
fuse             random           termination-log
mqueue           shm              tty
The listing shows all the devices. The list is fairly short. Now, compare this with the fol-
lowing listing, which shows the device files your privileged pod can see.
$ kubectl exec -it pod-privileged ls /dev
autofs              snd                 tty46
bsg                 sr0                 tty47
Listing 13.8
A pod with a privileged container: pod-privileged.yaml
Listing 13.9
List of available devices in a non-privileged pod
Listing 13.10
List of available devices in a privileged pod
This container will 
run in privileged 
mode
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="416">
  <data key="d0">Page_416</data>
  <data key="d5">Page_416</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_327">
  <data key="d0">384
CHAPTER 13
Securing cluster nodes and the network
btrfs-control       stderr              tty48
core                stdin               tty49
cpu                 stdout              tty5
cpu_dma_latency     termination-log     tty50
fd                  tty                 tty51
full                tty0                tty52
fuse                tty1                tty53
hpet                tty10               tty54
hwrng               tty11               tty55
...                 ...                 ...
I haven’t included the whole list, because it’s too long for the book, but it’s evident
that the device list is much longer than before. In fact, the privileged container sees
all the host node’s devices. This means it can use any device freely. 
 For example, I had to use privileged mode like this when I wanted a pod running
on a Raspberry Pi to control LEDs connected it.
13.2.4 Adding individual kernel capabilities to a container
In the previous section, you saw one way of giving a container unlimited power. In the
old days, traditional UNIX implementations only distinguished between privileged
and unprivileged processes, but for many years, Linux has supported a much more
fine-grained permission system through kernel capabilities.
 Instead of making a container privileged and giving it unlimited permissions, a
much safer method (from a security perspective) is to give it access only to the kernel
features it really requires. Kubernetes allows you to add capabilities to each container
or drop part of them, which allows you to fine-tune the container’s permissions and
limit the impact of a potential intrusion by an attacker.
 For example, a container usually isn’t allowed to change the system time (the hard-
ware clock’s time). You can confirm this by trying to set the time in your pod-with-
defaults pod:
$ kubectl exec -it pod-with-defaults -- date +%T -s "12:00:00"
date: can't set date: Operation not permitted
If you want to allow the container to change the system time, you can add a capabil-
ity called CAP_SYS_TIME to the container’s capabilities list, as shown in the follow-
ing listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-add-settime-capability
spec:
  containers:
  - name: main
    image: alpine
Listing 13.11
Adding the CAP_SYS_TIME capability: pod-add-settime-capability.yaml
 
</data>
  <data key="d5">384
CHAPTER 13
Securing cluster nodes and the network
btrfs-control       stderr              tty48
core                stdin               tty49
cpu                 stdout              tty5
cpu_dma_latency     termination-log     tty50
fd                  tty                 tty51
full                tty0                tty52
fuse                tty1                tty53
hpet                tty10               tty54
hwrng               tty11               tty55
...                 ...                 ...
I haven’t included the whole list, because it’s too long for the book, but it’s evident
that the device list is much longer than before. In fact, the privileged container sees
all the host node’s devices. This means it can use any device freely. 
 For example, I had to use privileged mode like this when I wanted a pod running
on a Raspberry Pi to control LEDs connected it.
13.2.4 Adding individual kernel capabilities to a container
In the previous section, you saw one way of giving a container unlimited power. In the
old days, traditional UNIX implementations only distinguished between privileged
and unprivileged processes, but for many years, Linux has supported a much more
fine-grained permission system through kernel capabilities.
 Instead of making a container privileged and giving it unlimited permissions, a
much safer method (from a security perspective) is to give it access only to the kernel
features it really requires. Kubernetes allows you to add capabilities to each container
or drop part of them, which allows you to fine-tune the container’s permissions and
limit the impact of a potential intrusion by an attacker.
 For example, a container usually isn’t allowed to change the system time (the hard-
ware clock’s time). You can confirm this by trying to set the time in your pod-with-
defaults pod:
$ kubectl exec -it pod-with-defaults -- date +%T -s "12:00:00"
date: can't set date: Operation not permitted
If you want to allow the container to change the system time, you can add a capabil-
ity called CAP_SYS_TIME to the container’s capabilities list, as shown in the follow-
ing listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-add-settime-capability
spec:
  containers:
  - name: main
    image: alpine
Listing 13.11
Adding the CAP_SYS_TIME capability: pod-add-settime-capability.yaml
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="417">
  <data key="d0">Page_417</data>
  <data key="d5">Page_417</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_328">
  <data key="d0">385
Configuring the container’s security context
    command: ["/bin/sleep", "999999"]
    securityContext:                     
      capabilities:                      
        add:                  
        - SYS_TIME            
NOTE
Linux kernel capabilities are usually prefixed with CAP_. But when
specifying them in a pod spec, you must leave out the prefix.
If you run the same command in this new pod’s container, the system time is changed
successfully:
$ kubectl exec -it pod-add-settime-capability -- date +%T -s "12:00:00"
12:00:00
$ kubectl exec -it pod-add-settime-capability -- date
Sun May  7 12:00:03 UTC 2017
WARNING
If you try this yourself, be aware that it may cause your worker
node to become unusable. In Minikube, although the system time was auto-
matically reset back by the Network Time Protocol (NTP) daemon, I had to
reboot the VM to schedule new pods. 
You can confirm the node’s time has been changed by checking the time on the node
running the pod. In my case, I’m using Minikube, so I have only one node and I can
get its time like this:
$ minikube ssh date
Sun May  7 12:00:07 UTC 2017
Adding capabilities like this is a much better way than giving a container full privileges
with privileged: true. Admittedly, it does require you to know and understand what
each capability does.
TIP
You’ll find the list of Linux kernel capabilities in the Linux man pages.
13.2.5 Dropping capabilities from a container
You’ve seen how to add capabilities, but you can also drop capabilities that may oth-
erwise be available to the container. For example, the default capabilities given to a
container include the CAP_CHOWN capability, which allows processes to change the
ownership of files in the filesystem. 
 You can see that’s the case by changing the ownership of the /tmp directory in
your pod-with-defaults pod to the guest user, for example:
$ kubectl exec pod-with-defaults chown guest /tmp
$ kubectl exec pod-with-defaults -- ls -la / | grep tmp
drwxrwxrwt    2 guest    root             6 May 25 15:18 tmp
Capabilities are added or dropped 
under the securityContext property.
You’re adding the 
SYS_TIME capability.
 
</data>
  <data key="d5">385
Configuring the container’s security context
    command: ["/bin/sleep", "999999"]
    securityContext:                     
      capabilities:                      
        add:                  
        - SYS_TIME            
NOTE
Linux kernel capabilities are usually prefixed with CAP_. But when
specifying them in a pod spec, you must leave out the prefix.
If you run the same command in this new pod’s container, the system time is changed
successfully:
$ kubectl exec -it pod-add-settime-capability -- date +%T -s "12:00:00"
12:00:00
$ kubectl exec -it pod-add-settime-capability -- date
Sun May  7 12:00:03 UTC 2017
WARNING
If you try this yourself, be aware that it may cause your worker
node to become unusable. In Minikube, although the system time was auto-
matically reset back by the Network Time Protocol (NTP) daemon, I had to
reboot the VM to schedule new pods. 
You can confirm the node’s time has been changed by checking the time on the node
running the pod. In my case, I’m using Minikube, so I have only one node and I can
get its time like this:
$ minikube ssh date
Sun May  7 12:00:07 UTC 2017
Adding capabilities like this is a much better way than giving a container full privileges
with privileged: true. Admittedly, it does require you to know and understand what
each capability does.
TIP
You’ll find the list of Linux kernel capabilities in the Linux man pages.
13.2.5 Dropping capabilities from a container
You’ve seen how to add capabilities, but you can also drop capabilities that may oth-
erwise be available to the container. For example, the default capabilities given to a
container include the CAP_CHOWN capability, which allows processes to change the
ownership of files in the filesystem. 
 You can see that’s the case by changing the ownership of the /tmp directory in
your pod-with-defaults pod to the guest user, for example:
$ kubectl exec pod-with-defaults chown guest /tmp
$ kubectl exec pod-with-defaults -- ls -la / | grep tmp
drwxrwxrwt    2 guest    root             6 May 25 15:18 tmp
Capabilities are added or dropped 
under the securityContext property.
You’re adding the 
SYS_TIME capability.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="418">
  <data key="d0">Page_418</data>
  <data key="d5">Page_418</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_329">
  <data key="d0">386
CHAPTER 13
Securing cluster nodes and the network
To prevent the container from doing that, you need to drop the capability by listing it
under the container’s securityContext.capabilities.drop property, as shown in
the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-drop-chown-capability
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      capabilities:
        drop:                   
        - CHOWN                 
By dropping the CHOWN capability, you’re not allowed to change the owner of the /tmp
directory in this pod:
$ kubectl exec pod-drop-chown-capability chown guest /tmp
chown: /tmp: Operation not permitted
You’re almost done exploring the container’s security context options. Let’s look at
one more.
13.2.6 Preventing processes from writing to the container’s filesystem
You may want to prevent the processes running in the container from writing to the
container’s filesystem, and only allow them to write to mounted volumes. You’d want
to do that mostly for security reasons. 
 Let’s imagine you’re running a PHP application with a hidden vulnerability, allow-
ing an attacker to write to the filesystem. The PHP files are added to the container
image at build time and are served from the container’s filesystem. Because of the vul-
nerability, the attacker can modify those files and inject them with malicious code. 
 These types of attacks can be thwarted by preventing the container from writing to
its filesystem, where the app’s executable code is normally stored. This is done by set-
ting the container’s securityContext.readOnlyRootFilesystem property to true, as
shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-readonly-filesystem
Listing 13.12
Dropping a capability from a container: pod-drop-chown-capability.yaml
Listing 13.13
A container with a read-only filesystem: pod-with-readonly-filesystem.yaml
You’re not allowing this container 
to change file ownership.
 
</data>
  <data key="d5">386
CHAPTER 13
Securing cluster nodes and the network
To prevent the container from doing that, you need to drop the capability by listing it
under the container’s securityContext.capabilities.drop property, as shown in
the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-drop-chown-capability
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      capabilities:
        drop:                   
        - CHOWN                 
By dropping the CHOWN capability, you’re not allowed to change the owner of the /tmp
directory in this pod:
$ kubectl exec pod-drop-chown-capability chown guest /tmp
chown: /tmp: Operation not permitted
You’re almost done exploring the container’s security context options. Let’s look at
one more.
13.2.6 Preventing processes from writing to the container’s filesystem
You may want to prevent the processes running in the container from writing to the
container’s filesystem, and only allow them to write to mounted volumes. You’d want
to do that mostly for security reasons. 
 Let’s imagine you’re running a PHP application with a hidden vulnerability, allow-
ing an attacker to write to the filesystem. The PHP files are added to the container
image at build time and are served from the container’s filesystem. Because of the vul-
nerability, the attacker can modify those files and inject them with malicious code. 
 These types of attacks can be thwarted by preventing the container from writing to
its filesystem, where the app’s executable code is normally stored. This is done by set-
ting the container’s securityContext.readOnlyRootFilesystem property to true, as
shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-readonly-filesystem
Listing 13.12
Dropping a capability from a container: pod-drop-chown-capability.yaml
Listing 13.13
A container with a read-only filesystem: pod-with-readonly-filesystem.yaml
You’re not allowing this container 
to change file ownership.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="419">
  <data key="d0">Page_419</data>
  <data key="d5">Page_419</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_330">
  <data key="d0">387
Configuring the container’s security context
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:                      
      readOnlyRootFilesystem: true        
    volumeMounts:                      
    - name: my-volume                  
      mountPath: /volume               
      readOnly: false                  
  volumes:
  - name: my-volume
    emptyDir:
When you deploy this pod, the container is running as root, which has write permis-
sions to the / directory, but trying to write a file there fails:
$ kubectl exec -it pod-with-readonly-filesystem touch /new-file
touch: /new-file: Read-only file system
On the other hand, writing to the mounted volume is allowed:
$ kubectl exec -it pod-with-readonly-filesystem touch /volume/newfile
$ kubectl exec -it pod-with-readonly-filesystem -- ls -la /volume/newfile
-rw-r--r--    1 root     root       0 May  7 19:11 /mountedVolume/newfile
As shown in the example, when you make the container’s filesystem read-only, you’ll
probably want to mount a volume in every directory the application writes to (for
example, logs, on-disk caches, and so on).
TIP
To increase security, when running pods in production, set their con-
tainer’s readOnlyRootFilesystem property to true.
SETTING SECURITY CONTEXT OPTIONS AT THE POD LEVEL
In all these examples, you’ve set the security context of an individual container. Sev-
eral of these options can also be set at the pod level (through the pod.spec.security-
Context property). They serve as a default for all the pod’s containers but can be
overridden at the container level. The pod-level security context also allows you to set
additional properties, which we’ll explain next.
13.2.7 Sharing volumes when containers run as different users
In chapter 6, we explained how volumes are used to share data between the pod’s
containers. You had no trouble writing files in one container and reading them in
the other. 
 But this was only because both containers were running as root, giving them full
access to all the files in the volume. Now imagine using the runAsUser option we
explained earlier. You may need to run the two containers as two different users (per-
haps you’re using two third-party container images, where each one runs its process
This container’s filesystem 
can’t be written to...
...but writing to /volume is 
allowed, becase a volume 
is mounted there.
 
</data>
  <data key="d5">387
Configuring the container’s security context
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:                      
      readOnlyRootFilesystem: true        
    volumeMounts:                      
    - name: my-volume                  
      mountPath: /volume               
      readOnly: false                  
  volumes:
  - name: my-volume
    emptyDir:
When you deploy this pod, the container is running as root, which has write permis-
sions to the / directory, but trying to write a file there fails:
$ kubectl exec -it pod-with-readonly-filesystem touch /new-file
touch: /new-file: Read-only file system
On the other hand, writing to the mounted volume is allowed:
$ kubectl exec -it pod-with-readonly-filesystem touch /volume/newfile
$ kubectl exec -it pod-with-readonly-filesystem -- ls -la /volume/newfile
-rw-r--r--    1 root     root       0 May  7 19:11 /mountedVolume/newfile
As shown in the example, when you make the container’s filesystem read-only, you’ll
probably want to mount a volume in every directory the application writes to (for
example, logs, on-disk caches, and so on).
TIP
To increase security, when running pods in production, set their con-
tainer’s readOnlyRootFilesystem property to true.
SETTING SECURITY CONTEXT OPTIONS AT THE POD LEVEL
In all these examples, you’ve set the security context of an individual container. Sev-
eral of these options can also be set at the pod level (through the pod.spec.security-
Context property). They serve as a default for all the pod’s containers but can be
overridden at the container level. The pod-level security context also allows you to set
additional properties, which we’ll explain next.
13.2.7 Sharing volumes when containers run as different users
In chapter 6, we explained how volumes are used to share data between the pod’s
containers. You had no trouble writing files in one container and reading them in
the other. 
 But this was only because both containers were running as root, giving them full
access to all the files in the volume. Now imagine using the runAsUser option we
explained earlier. You may need to run the two containers as two different users (per-
haps you’re using two third-party container images, where each one runs its process
This container’s filesystem 
can’t be written to...
...but writing to /volume is 
allowed, becase a volume 
is mounted there.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="420">
  <data key="d0">Page_420</data>
  <data key="d5">Page_420</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_331">
  <data key="d0">388
CHAPTER 13
Securing cluster nodes and the network
under its own specific user). If those two containers use a volume to share files, they
may not necessarily be able to read or write files of one another. 
 That’s why Kubernetes allows you to specify supplemental groups for all the pods
running in the container, allowing them to share files, regardless of the user IDs
they’re running as. This is done using the following two properties:

fsGroup

supplementalGroups
What they do is best explained in an example, so let’s see how to use them in a pod
and then see what their effect is. The next listing describes a pod with two containers
sharing the same volume.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-shared-volume-fsgroup
spec:
  securityContext:                       
    fsGroup: 555                         
    supplementalGroups: [666, 777]       
  containers:
  - name: first
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:                     
      runAsUser: 1111                    
    volumeMounts:                               
    - name: shared-volume                       
      mountPath: /volume
      readOnly: false
  - name: second
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:                     
      runAsUser: 2222                    
    volumeMounts:                               
    - name: shared-volume                       
      mountPath: /volume
      readOnly: false
  volumes:                                      
  - name: shared-volume                         
    emptyDir:
After you create this pod, run a shell in its first container and see what user and group
IDs the container is running as:
$ kubectl exec -it pod-with-shared-volume-fsgroup -c first sh
/ $ id
uid=1111 gid=0(root) groups=555,666,777
Listing 13.14
fsGroup &amp; supplementalGroups: pod-with-shared-volume-fsgroup.yaml
The fsGroup and supplementalGroups 
are defined in the security context at 
the pod level.
The first container 
runs as user ID 1111.
Both containers 
use the same 
volume
The second
container
runs as user
ID 2222.
 
</data>
  <data key="d5">388
CHAPTER 13
Securing cluster nodes and the network
under its own specific user). If those two containers use a volume to share files, they
may not necessarily be able to read or write files of one another. 
 That’s why Kubernetes allows you to specify supplemental groups for all the pods
running in the container, allowing them to share files, regardless of the user IDs
they’re running as. This is done using the following two properties:

fsGroup

supplementalGroups
What they do is best explained in an example, so let’s see how to use them in a pod
and then see what their effect is. The next listing describes a pod with two containers
sharing the same volume.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-shared-volume-fsgroup
spec:
  securityContext:                       
    fsGroup: 555                         
    supplementalGroups: [666, 777]       
  containers:
  - name: first
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:                     
      runAsUser: 1111                    
    volumeMounts:                               
    - name: shared-volume                       
      mountPath: /volume
      readOnly: false
  - name: second
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:                     
      runAsUser: 2222                    
    volumeMounts:                               
    - name: shared-volume                       
      mountPath: /volume
      readOnly: false
  volumes:                                      
  - name: shared-volume                         
    emptyDir:
After you create this pod, run a shell in its first container and see what user and group
IDs the container is running as:
$ kubectl exec -it pod-with-shared-volume-fsgroup -c first sh
/ $ id
uid=1111 gid=0(root) groups=555,666,777
Listing 13.14
fsGroup &amp; supplementalGroups: pod-with-shared-volume-fsgroup.yaml
The fsGroup and supplementalGroups 
are defined in the security context at 
the pod level.
The first container 
runs as user ID 1111.
Both containers 
use the same 
volume
The second
container
runs as user
ID 2222.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="421">
  <data key="d0">Page_421</data>
  <data key="d5">Page_421</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_332">
  <data key="d0">389
Restricting the use of security-related features in pods
The id command shows the container is running with user ID 1111, as specified in the
pod definition. The effective group ID is 0(root), but group IDs 555, 666, and 777 are
also associated with the user. 
 In the pod definition, you set fsGroup to 555. Because of this, the mounted volume
will be owned by group ID 555, as shown here:
/ $ ls -l / | grep volume
drwxrwsrwx    2 root     555              6 May 29 12:23 volume
If you create a file in the mounted volume’s directory, the file is owned by user ID
1111 (that’s the user ID the container is running as) and by group ID 555:
/ $ echo foo &gt; /volume/foo
/ $ ls -l /volume
total 4
-rw-r--r--    1 1111     555              4 May 29 12:25 foo
This is different from how ownership is otherwise set up for newly created files. Usu-
ally, the user’s effective group ID, which is 0 in your case, is used when a user creates
files. You can see this by creating a file in the container’s filesystem instead of in the
volume:
/ $ echo foo &gt; /tmp/foo
/ $ ls -l /tmp
total 4
-rw-r--r--    1 1111     root             4 May 29 12:41 foo
As you can see, the fsGroup security context property is used when the process cre-
ates files in a volume (but this depends on the volume plugin used), whereas the
supplementalGroups property defines a list of additional group IDs the user is asso-
ciated with. 
 This concludes this section about the configuration of the container’s security con-
text. Next, we’ll see how a cluster administrator can restrict users from doing so.
13.3
Restricting the use of security-related features in pods
The examples in the previous sections have shown how a person deploying pods can
do whatever they want on any cluster node, by deploying a privileged pod to the
node, for example. Obviously, a mechanism must prevent users from doing part or
all of what’s been explained. The cluster admin can restrict the use of the previously
described security-related features by creating one or more PodSecurityPolicy
resources.
13.3.1 Introducing the PodSecurityPolicy resource
PodSecurityPolicy is a cluster-level (non-namespaced) resource, which defines what
security-related features users can or can’t use in their pods. The job of upholding
the policies configured in PodSecurityPolicy resources is performed by the
 
</data>
  <data key="d5">389
Restricting the use of security-related features in pods
The id command shows the container is running with user ID 1111, as specified in the
pod definition. The effective group ID is 0(root), but group IDs 555, 666, and 777 are
also associated with the user. 
 In the pod definition, you set fsGroup to 555. Because of this, the mounted volume
will be owned by group ID 555, as shown here:
/ $ ls -l / | grep volume
drwxrwsrwx    2 root     555              6 May 29 12:23 volume
If you create a file in the mounted volume’s directory, the file is owned by user ID
1111 (that’s the user ID the container is running as) and by group ID 555:
/ $ echo foo &gt; /volume/foo
/ $ ls -l /volume
total 4
-rw-r--r--    1 1111     555              4 May 29 12:25 foo
This is different from how ownership is otherwise set up for newly created files. Usu-
ally, the user’s effective group ID, which is 0 in your case, is used when a user creates
files. You can see this by creating a file in the container’s filesystem instead of in the
volume:
/ $ echo foo &gt; /tmp/foo
/ $ ls -l /tmp
total 4
-rw-r--r--    1 1111     root             4 May 29 12:41 foo
As you can see, the fsGroup security context property is used when the process cre-
ates files in a volume (but this depends on the volume plugin used), whereas the
supplementalGroups property defines a list of additional group IDs the user is asso-
ciated with. 
 This concludes this section about the configuration of the container’s security con-
text. Next, we’ll see how a cluster administrator can restrict users from doing so.
13.3
Restricting the use of security-related features in pods
The examples in the previous sections have shown how a person deploying pods can
do whatever they want on any cluster node, by deploying a privileged pod to the
node, for example. Obviously, a mechanism must prevent users from doing part or
all of what’s been explained. The cluster admin can restrict the use of the previously
described security-related features by creating one or more PodSecurityPolicy
resources.
13.3.1 Introducing the PodSecurityPolicy resource
PodSecurityPolicy is a cluster-level (non-namespaced) resource, which defines what
security-related features users can or can’t use in their pods. The job of upholding
the policies configured in PodSecurityPolicy resources is performed by the
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="422">
  <data key="d0">Page_422</data>
  <data key="d5">Page_422</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_333">
  <data key="d0">390
CHAPTER 13
Securing cluster nodes and the network
PodSecurityPolicy admission control plugin running in the API server (we explained
admission control plugins in chapter 11).
NOTE
The PodSecurityPolicy admission control plugin may not be enabled
in your cluster. Before running the following examples, ensure it’s enabled. If
you’re using Minikube, refer to the next sidebar.
When someone posts a pod resource to the API server, the PodSecurityPolicy admis-
sion control plugin validates the pod definition against the configured PodSecurity-
Policies. If the pod conforms to the cluster’s policies, it’s accepted and stored into
etcd; otherwise it’s rejected immediately. The plugin may also modify the pod
resource according to defaults configured in the policy.
UNDERSTANDING WHAT A PODSECURITYPOLICY CAN DO
A PodSecurityPolicy resource defines things like the following:
Whether a pod can use the host’s IPC, PID, or Network namespaces
Which host ports a pod can bind to
What user IDs a container can run as
Whether a pod with privileged containers can be created
Enabling RBAC and PodSecurityPolicy admission control in Minikube
I’m using Minikube version v0.19.0 to run these examples. That version doesn’t
enable either the PodSecurityPolicy admission control plugin or RBAC authorization,
which is required in part of the exercises. One exercise also requires authenticating
as a different user, so you’ll also need to enable the basic authentication plugin
where users are defined in a file.
To run Minikube with all these plugins enabled, you may need to use this (or a similar)
command, depending on the version you’re using: 
$ minikube start --extra-config apiserver.Authentication.PasswordFile.
➥ BasicAuthFile=/etc/kubernetes/passwd --extra-config=apiserver.
➥ Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRun
➥ Options.AdmissionControl=NamespaceLifecycle,LimitRanger,Service
➥ Account,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,
➥ DefaultTolerationSeconds,PodSecurityPolicy
The API server won’t start up until you create the password file you specified in the
command line options. This is how to create the file:
$ cat &lt;&lt;EOF | minikube ssh sudo tee /etc/kubernetes/passwd
password,alice,1000,basic-user
password,bob,2000,privileged-user
EOF
You’ll find a shell script that runs both commands in the book’s code archive in
Chapter13/minikube-with-rbac-and-psp-enabled.sh.
 
</data>
  <data key="d5">390
CHAPTER 13
Securing cluster nodes and the network
PodSecurityPolicy admission control plugin running in the API server (we explained
admission control plugins in chapter 11).
NOTE
The PodSecurityPolicy admission control plugin may not be enabled
in your cluster. Before running the following examples, ensure it’s enabled. If
you’re using Minikube, refer to the next sidebar.
When someone posts a pod resource to the API server, the PodSecurityPolicy admis-
sion control plugin validates the pod definition against the configured PodSecurity-
Policies. If the pod conforms to the cluster’s policies, it’s accepted and stored into
etcd; otherwise it’s rejected immediately. The plugin may also modify the pod
resource according to defaults configured in the policy.
UNDERSTANDING WHAT A PODSECURITYPOLICY CAN DO
A PodSecurityPolicy resource defines things like the following:
Whether a pod can use the host’s IPC, PID, or Network namespaces
Which host ports a pod can bind to
What user IDs a container can run as
Whether a pod with privileged containers can be created
Enabling RBAC and PodSecurityPolicy admission control in Minikube
I’m using Minikube version v0.19.0 to run these examples. That version doesn’t
enable either the PodSecurityPolicy admission control plugin or RBAC authorization,
which is required in part of the exercises. One exercise also requires authenticating
as a different user, so you’ll also need to enable the basic authentication plugin
where users are defined in a file.
To run Minikube with all these plugins enabled, you may need to use this (or a similar)
command, depending on the version you’re using: 
$ minikube start --extra-config apiserver.Authentication.PasswordFile.
➥ BasicAuthFile=/etc/kubernetes/passwd --extra-config=apiserver.
➥ Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRun
➥ Options.AdmissionControl=NamespaceLifecycle,LimitRanger,Service
➥ Account,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,
➥ DefaultTolerationSeconds,PodSecurityPolicy
The API server won’t start up until you create the password file you specified in the
command line options. This is how to create the file:
$ cat &lt;&lt;EOF | minikube ssh sudo tee /etc/kubernetes/passwd
password,alice,1000,basic-user
password,bob,2000,privileged-user
EOF
You’ll find a shell script that runs both commands in the book’s code archive in
Chapter13/minikube-with-rbac-and-psp-enabled.sh.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="423">
  <data key="d0">Page_423</data>
  <data key="d5">Page_423</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_334">
  <data key="d0">391
Restricting the use of security-related features in pods
Which kernel capabilities are allowed, which are added by default and which are
always dropped
What SELinux labels a container can use
Whether a container can use a writable root filesystem or not
Which filesystem groups the container can run as
Which volume types a pod can use
If you’ve read this chapter up to this point, everything but the last item in the previous
list should be familiar. The last item should also be fairly clear. 
EXAMINING A SAMPLE PODSECURITYPOLICY
The following listing shows a sample PodSecurityPolicy, which prevents pods from
using the host’s IPC, PID, and Network namespaces, and prevents running privileged
containers and the use of most host ports (except ports from 10000-11000 and 13000-
14000). The policy doesn’t set any constraints on what users, groups, or SELinux
groups the container can run as.
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: default
spec:
  hostIPC: false                 
  hostPID: false                 
  hostNetwork: false             
  hostPorts:                         
  - min: 10000                       
    max: 11000                       
  - min: 13000                       
    max: 14000                       
  privileged: false              
  readOnlyRootFilesystem: true   
  runAsUser:                      
    rule: RunAsAny                
  fsGroup:                        
    rule: RunAsAny                
  supplementalGroups:             
    rule: RunAsAny                
  seLinux:                      
    rule: RunAsAny              
  volumes:                  
  - '*'                     
Most of the options specified in the example should be self-explanatory, especially if
you’ve read the previous sections. After this PodSecurityPolicy resource is posted to
Listing 13.15
An example PodSecurityPolicy: pod-security-policy.yaml
Containers aren’t 
allowed to use the 
host’s IPC, PID, or 
network namespace.
They can only bind to host ports 
10000 to 11000 (inclusive) or 
host ports 13000 to 14000.
Containers cannot run 
in privileged mode.
Containers are forced to run 
with a read-only root filesystem.
Containers can 
run as any user 
and any group.
They can also use any 
SELinux groups they want.
All volume types can 
be used in pods.
 
</data>
  <data key="d5">391
Restricting the use of security-related features in pods
Which kernel capabilities are allowed, which are added by default and which are
always dropped
What SELinux labels a container can use
Whether a container can use a writable root filesystem or not
Which filesystem groups the container can run as
Which volume types a pod can use
If you’ve read this chapter up to this point, everything but the last item in the previous
list should be familiar. The last item should also be fairly clear. 
EXAMINING A SAMPLE PODSECURITYPOLICY
The following listing shows a sample PodSecurityPolicy, which prevents pods from
using the host’s IPC, PID, and Network namespaces, and prevents running privileged
containers and the use of most host ports (except ports from 10000-11000 and 13000-
14000). The policy doesn’t set any constraints on what users, groups, or SELinux
groups the container can run as.
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: default
spec:
  hostIPC: false                 
  hostPID: false                 
  hostNetwork: false             
  hostPorts:                         
  - min: 10000                       
    max: 11000                       
  - min: 13000                       
    max: 14000                       
  privileged: false              
  readOnlyRootFilesystem: true   
  runAsUser:                      
    rule: RunAsAny                
  fsGroup:                        
    rule: RunAsAny                
  supplementalGroups:             
    rule: RunAsAny                
  seLinux:                      
    rule: RunAsAny              
  volumes:                  
  - '*'                     
Most of the options specified in the example should be self-explanatory, especially if
you’ve read the previous sections. After this PodSecurityPolicy resource is posted to
Listing 13.15
An example PodSecurityPolicy: pod-security-policy.yaml
Containers aren’t 
allowed to use the 
host’s IPC, PID, or 
network namespace.
They can only bind to host ports 
10000 to 11000 (inclusive) or 
host ports 13000 to 14000.
Containers cannot run 
in privileged mode.
Containers are forced to run 
with a read-only root filesystem.
Containers can 
run as any user 
and any group.
They can also use any 
SELinux groups they want.
All volume types can 
be used in pods.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="424">
  <data key="d0">Page_424</data>
  <data key="d5">Page_424</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_335">
  <data key="d0">392
CHAPTER 13
Securing cluster nodes and the network
the cluster, the API server will no longer allow you to deploy the privileged pod used
earlier. For example
$ kubectl create -f pod-privileged.yaml
Error from server (Forbidden): error when creating "pod-privileged.yaml":
pods "pod-privileged" is forbidden: unable to validate against any pod 
security policy: [spec.containers[0].securityContext.privileged: Invalid 
value: true: Privileged containers are not allowed]
Likewise, you can no longer deploy pods that want to use the host’s PID, IPC, or Net-
work namespace. Also, because you set readOnlyRootFilesystem to true in the pol-
icy, the container filesystems in all pods will be read-only (containers can only write
to volumes).
13.3.2 Understanding runAsUser, fsGroup, and supplementalGroups 
policies
The policy in the previous example doesn’t impose any limits on which users and
groups containers can run as, because you’ve used the RunAsAny rule for the runAs-
User, fsGroup, and supplementalGroups fields. If you want to constrain the list of
allowed user or group IDs, you change the rule to MustRunAs and specify the range of
allowed IDs. 
USING THE MUSTRUNAS RULE
Let’s look at an example. To only allow containers to run as user ID 2 and constrain the
default filesystem group and supplemental group IDs to be anything from 2–10 or 20–
30 (all inclusive), you’d include the following snippet in the PodSecurityPolicy resource.
  runAsUser:
    rule: MustRunAs
    ranges:
    - min: 2                
      max: 2                
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 2                
      max: 10               
    - min: 20               
      max: 30               
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 2                
      max: 10               
    - min: 20               
      max: 30               
Listing 13.16
Specifying IDs containers must run as: psp-must-run-as.yaml
Add a single range with min equal 
to max to set one specific ID.
Multiple ranges are 
supported—here, 
group IDs can be 2–10 
or 20–30 (inclusive).
 
</data>
  <data key="d5">392
CHAPTER 13
Securing cluster nodes and the network
the cluster, the API server will no longer allow you to deploy the privileged pod used
earlier. For example
$ kubectl create -f pod-privileged.yaml
Error from server (Forbidden): error when creating "pod-privileged.yaml":
pods "pod-privileged" is forbidden: unable to validate against any pod 
security policy: [spec.containers[0].securityContext.privileged: Invalid 
value: true: Privileged containers are not allowed]
Likewise, you can no longer deploy pods that want to use the host’s PID, IPC, or Net-
work namespace. Also, because you set readOnlyRootFilesystem to true in the pol-
icy, the container filesystems in all pods will be read-only (containers can only write
to volumes).
13.3.2 Understanding runAsUser, fsGroup, and supplementalGroups 
policies
The policy in the previous example doesn’t impose any limits on which users and
groups containers can run as, because you’ve used the RunAsAny rule for the runAs-
User, fsGroup, and supplementalGroups fields. If you want to constrain the list of
allowed user or group IDs, you change the rule to MustRunAs and specify the range of
allowed IDs. 
USING THE MUSTRUNAS RULE
Let’s look at an example. To only allow containers to run as user ID 2 and constrain the
default filesystem group and supplemental group IDs to be anything from 2–10 or 20–
30 (all inclusive), you’d include the following snippet in the PodSecurityPolicy resource.
  runAsUser:
    rule: MustRunAs
    ranges:
    - min: 2                
      max: 2                
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 2                
      max: 10               
    - min: 20               
      max: 30               
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 2                
      max: 10               
    - min: 20               
      max: 30               
Listing 13.16
Specifying IDs containers must run as: psp-must-run-as.yaml
Add a single range with min equal 
to max to set one specific ID.
Multiple ranges are 
supported—here, 
group IDs can be 2–10 
or 20–30 (inclusive).
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="425">
  <data key="d0">Page_425</data>
  <data key="d5">Page_425</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_336">
  <data key="d0">393
Restricting the use of security-related features in pods
If the pod spec tries to set either of those fields to a value outside of these ranges, the
pod will not be accepted by the API server. To try this, delete the previous PodSecurity-
Policy and create the new one from the psp-must-run-as.yaml file. 
NOTE
Changing the policy has no effect on existing pods, because PodSecurity-
Policies are enforced only when creating or updating pods.
DEPLOYING A POD WITH RUNASUSER OUTSIDE OF THE POLICY’S RANGE
If you try deploying the pod-as-user-guest.yaml file from earlier, which says the con-
tainer should run as user ID 405, the API server rejects the pod:
$ kubectl create -f pod-as-user-guest.yaml
Error from server (Forbidden): error when creating "pod-as-user-guest.yaml"
: pods "pod-as-user-guest" is forbidden: unable to validate against any pod 
security policy: [securityContext.runAsUser: Invalid value: 405: UID on 
container main does not match required range.  Found 405, allowed: [{2 2}]]
Okay, that was obvious. But what happens if you deploy a pod without setting the runAs-
User property, but the user ID is baked into the container image (using the USER direc-
tive in the Dockerfile)?
DEPLOYING A POD WITH A CONTAINER IMAGE WITH AN OUT-OF-RANGE USER ID
I’ve created an alternative image for the Node.js app you’ve used throughout the
book. The image is configured so that the container will run as user ID 5. The Docker-
file for the image is shown in the following listing.
FROM node:7
ADD app.js /app.js
USER 5                         
ENTRYPOINT ["node", "app.js"]
I pushed the image to Docker Hub as luksa/kubia-run-as-user-5. If I deploy a pod
with that image, the API server doesn’t reject it:
$ kubectl run run-as-5 --image luksa/kubia-run-as-user-5 --restart Never
pod "run-as-5" created
Unlike before, the API server accepted the pod and the Kubelet has run its container.
Let’s see what user ID the container is running as:
$ kubectl exec run-as-5 -- id
uid=2(bin) gid=2(bin) groups=2(bin)
As you can see, the container is running as user ID 2, which is the ID you specified in
the PodSecurityPolicy. The PodSecurityPolicy can be used to override the user ID
hardcoded into a container image.
Listing 13.17
Dockerfile with a USER directive: kubia-run-as-user-5/Dockerfile
Containers run from 
this image will run 
as user ID 5.
 
</data>
  <data key="d5">393
Restricting the use of security-related features in pods
If the pod spec tries to set either of those fields to a value outside of these ranges, the
pod will not be accepted by the API server. To try this, delete the previous PodSecurity-
Policy and create the new one from the psp-must-run-as.yaml file. 
NOTE
Changing the policy has no effect on existing pods, because PodSecurity-
Policies are enforced only when creating or updating pods.
DEPLOYING A POD WITH RUNASUSER OUTSIDE OF THE POLICY’S RANGE
If you try deploying the pod-as-user-guest.yaml file from earlier, which says the con-
tainer should run as user ID 405, the API server rejects the pod:
$ kubectl create -f pod-as-user-guest.yaml
Error from server (Forbidden): error when creating "pod-as-user-guest.yaml"
: pods "pod-as-user-guest" is forbidden: unable to validate against any pod 
security policy: [securityContext.runAsUser: Invalid value: 405: UID on 
container main does not match required range.  Found 405, allowed: [{2 2}]]
Okay, that was obvious. But what happens if you deploy a pod without setting the runAs-
User property, but the user ID is baked into the container image (using the USER direc-
tive in the Dockerfile)?
DEPLOYING A POD WITH A CONTAINER IMAGE WITH AN OUT-OF-RANGE USER ID
I’ve created an alternative image for the Node.js app you’ve used throughout the
book. The image is configured so that the container will run as user ID 5. The Docker-
file for the image is shown in the following listing.
FROM node:7
ADD app.js /app.js
USER 5                         
ENTRYPOINT ["node", "app.js"]
I pushed the image to Docker Hub as luksa/kubia-run-as-user-5. If I deploy a pod
with that image, the API server doesn’t reject it:
$ kubectl run run-as-5 --image luksa/kubia-run-as-user-5 --restart Never
pod "run-as-5" created
Unlike before, the API server accepted the pod and the Kubelet has run its container.
Let’s see what user ID the container is running as:
$ kubectl exec run-as-5 -- id
uid=2(bin) gid=2(bin) groups=2(bin)
As you can see, the container is running as user ID 2, which is the ID you specified in
the PodSecurityPolicy. The PodSecurityPolicy can be used to override the user ID
hardcoded into a container image.
Listing 13.17
Dockerfile with a USER directive: kubia-run-as-user-5/Dockerfile
Containers run from 
this image will run 
as user ID 5.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="426">
  <data key="d0">Page_426</data>
  <data key="d5">Page_426</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_337">
  <data key="d0">394
CHAPTER 13
Securing cluster nodes and the network
USING THE MUSTRUNASNONROOT RULE IN THE RUNASUSER FIELD
For the runAsUser field an additional rule can be used: MustRunAsNonRoot. As the
name suggests, it prevents users from deploying containers that run as root. Either the
container spec must specify a runAsUser field, which can’t be zero (zero is the root
user’s ID), or the container image itself must run as a non-zero user ID. We explained
why this is a good thing earlier.
13.3.3 Configuring allowed, default, and disallowed capabilities
As you learned, containers can run in privileged mode or not, and you can define a
more fine-grained permission configuration by adding or dropping Linux kernel
capabilities in each container. Three fields influence which capabilities containers can
or cannot use:

allowedCapabilities

defaultAddCapabilities

requiredDropCapabilities
We’ll look at an example first, and then discuss what each of the three fields does. The
following listing shows a snippet of a PodSecurityPolicy resource defining three fields
related to capabilities.
apiVersion: extensions/v1beta1 
kind: PodSecurityPolicy
spec:
  allowedCapabilities:          
  - SYS_TIME                    
  defaultAddCapabilities:         
  - CHOWN                         
  requiredDropCapabilities:     
  - SYS_ADMIN                   
  - SYS_MODULE                  
  ...
NOTE
The SYS_ADMIN capability allows a range of administrative operations,
and the SYS_MODULE capability allows loading and unloading of Linux kernel
modules.
SPECIFYING WHICH CAPABILITIES CAN BE ADDED TO A CONTAINER
The allowedCapabilities field is used to specify which capabilities pod authors can
add in the securityContext.capabilities field in the container spec. In one of the
previous examples, you added the SYS_TIME capability to your container. If the Pod-
SecurityPolicy admission control plugin had been enabled, you wouldn’t have been
able to add that capability, unless it was specified in the PodSecurityPolicy as shown
in listing 13.18.
Listing 13.18
Specifying capabilities in a PodSecurityPolicy: psp-capabilities.yaml
Allow containers to 
add the SYS_TIME 
capability.
Automatically add the CHOWN 
capability to every container.
Require containers to 
drop the SYS_ADMIN and 
SYS_MODULE capabilities.
 
</data>
  <data key="d5">394
CHAPTER 13
Securing cluster nodes and the network
USING THE MUSTRUNASNONROOT RULE IN THE RUNASUSER FIELD
For the runAsUser field an additional rule can be used: MustRunAsNonRoot. As the
name suggests, it prevents users from deploying containers that run as root. Either the
container spec must specify a runAsUser field, which can’t be zero (zero is the root
user’s ID), or the container image itself must run as a non-zero user ID. We explained
why this is a good thing earlier.
13.3.3 Configuring allowed, default, and disallowed capabilities
As you learned, containers can run in privileged mode or not, and you can define a
more fine-grained permission configuration by adding or dropping Linux kernel
capabilities in each container. Three fields influence which capabilities containers can
or cannot use:

allowedCapabilities

defaultAddCapabilities

requiredDropCapabilities
We’ll look at an example first, and then discuss what each of the three fields does. The
following listing shows a snippet of a PodSecurityPolicy resource defining three fields
related to capabilities.
apiVersion: extensions/v1beta1 
kind: PodSecurityPolicy
spec:
  allowedCapabilities:          
  - SYS_TIME                    
  defaultAddCapabilities:         
  - CHOWN                         
  requiredDropCapabilities:     
  - SYS_ADMIN                   
  - SYS_MODULE                  
  ...
NOTE
The SYS_ADMIN capability allows a range of administrative operations,
and the SYS_MODULE capability allows loading and unloading of Linux kernel
modules.
SPECIFYING WHICH CAPABILITIES CAN BE ADDED TO A CONTAINER
The allowedCapabilities field is used to specify which capabilities pod authors can
add in the securityContext.capabilities field in the container spec. In one of the
previous examples, you added the SYS_TIME capability to your container. If the Pod-
SecurityPolicy admission control plugin had been enabled, you wouldn’t have been
able to add that capability, unless it was specified in the PodSecurityPolicy as shown
in listing 13.18.
Listing 13.18
Specifying capabilities in a PodSecurityPolicy: psp-capabilities.yaml
Allow containers to 
add the SYS_TIME 
capability.
Automatically add the CHOWN 
capability to every container.
Require containers to 
drop the SYS_ADMIN and 
SYS_MODULE capabilities.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="427">
  <data key="d0">Page_427</data>
  <data key="d5">Page_427</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_338">
  <data key="d0">395
Restricting the use of security-related features in pods
ADDING CAPABILITIES TO ALL CONTAINERS
All capabilities listed under the defaultAddCapabilities field will be added to
every deployed pod’s containers. If a user doesn’t want certain containers to have
those capabilities, they need to explicitly drop them in the specs of those containers.
 The example in listing 13.18 enables the automatic addition of the CAP_CHOWN capa-
bility to every container, thus allowing processes running in the container to change the
ownership of files in the container (with the chown command, for example).
DROPPING CAPABILITIES FROM A CONTAINER
The final field in this example is requiredDropCapabilities. I must admit, this was a
somewhat strange name for me at first, but it’s not that complicated. The capabilities
listed in this field are dropped automatically from every container (the PodSecurity-
Policy Admission Control plugin will add them to every container’s security-
Context.capabilities.drop field). 
 If a user tries to create a pod where they explicitly add one of the capabilities listed
in the policy’s requiredDropCapabilities field, the pod is rejected:
$ kubectl create -f pod-add-sysadmin-capability.yaml
Error from server (Forbidden): error when creating "pod-add-sysadmin-
capability.yaml": pods "pod-add-sysadmin-capability" is forbidden: unable 
to validate against any pod security policy: [capabilities.add: Invalid 
value: "SYS_ADMIN": capability may not be added]
13.3.4 Constraining the types of volumes pods can use
The last thing a PodSecurityPolicy resource can do is define which volume types users
can add to their pods. At the minimum, a PodSecurityPolicy should allow using at
least the emptyDir, configMap, secret, downwardAPI, and the persistentVolume-
Claim volumes. The pertinent part of such a PodSecurityPolicy resource is shown in
the following listing.
kind: PodSecurityPolicy
spec:
  volumes:
  - emptyDir
  - configMap
  - secret
  - downwardAPI
  - persistentVolumeClaim
If multiple PodSecurityPolicy resources are in place, pods can use any volume type
defined in any of the policies (the union of all volumes lists is used).
Listing 13.19
A PSP snippet allowing the use of only certain volume types: 
psp-volumes.yaml
 
</data>
  <data key="d5">395
Restricting the use of security-related features in pods
ADDING CAPABILITIES TO ALL CONTAINERS
All capabilities listed under the defaultAddCapabilities field will be added to
every deployed pod’s containers. If a user doesn’t want certain containers to have
those capabilities, they need to explicitly drop them in the specs of those containers.
 The example in listing 13.18 enables the automatic addition of the CAP_CHOWN capa-
bility to every container, thus allowing processes running in the container to change the
ownership of files in the container (with the chown command, for example).
DROPPING CAPABILITIES FROM A CONTAINER
The final field in this example is requiredDropCapabilities. I must admit, this was a
somewhat strange name for me at first, but it’s not that complicated. The capabilities
listed in this field are dropped automatically from every container (the PodSecurity-
Policy Admission Control plugin will add them to every container’s security-
Context.capabilities.drop field). 
 If a user tries to create a pod where they explicitly add one of the capabilities listed
in the policy’s requiredDropCapabilities field, the pod is rejected:
$ kubectl create -f pod-add-sysadmin-capability.yaml
Error from server (Forbidden): error when creating "pod-add-sysadmin-
capability.yaml": pods "pod-add-sysadmin-capability" is forbidden: unable 
to validate against any pod security policy: [capabilities.add: Invalid 
value: "SYS_ADMIN": capability may not be added]
13.3.4 Constraining the types of volumes pods can use
The last thing a PodSecurityPolicy resource can do is define which volume types users
can add to their pods. At the minimum, a PodSecurityPolicy should allow using at
least the emptyDir, configMap, secret, downwardAPI, and the persistentVolume-
Claim volumes. The pertinent part of such a PodSecurityPolicy resource is shown in
the following listing.
kind: PodSecurityPolicy
spec:
  volumes:
  - emptyDir
  - configMap
  - secret
  - downwardAPI
  - persistentVolumeClaim
If multiple PodSecurityPolicy resources are in place, pods can use any volume type
defined in any of the policies (the union of all volumes lists is used).
Listing 13.19
A PSP snippet allowing the use of only certain volume types: 
psp-volumes.yaml
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="428">
  <data key="d0">Page_428</data>
  <data key="d5">Page_428</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_339">
  <data key="d0">396
CHAPTER 13
Securing cluster nodes and the network
13.3.5 Assigning different PodSecurityPolicies to different users 
and groups
We mentioned that a PodSecurityPolicy is a cluster-level resource, which means it
can’t be stored in and applied to a specific namespace. Does that mean it always
applies across all namespaces? No, because that would make them relatively unus-
able. After all, system pods must often be allowed to do things that regular pods
shouldn’t.
 Assigning different policies to different users is done through the RBAC mecha-
nism described in the previous chapter. The idea is to create as many policies as you
need and make them available to individual users or groups by creating ClusterRole
resources and pointing them to the individual policies by name. By binding those
ClusterRoles to specific users or groups with ClusterRoleBindings, when the Pod-
SecurityPolicy Admission Control plugin needs to decide whether to admit a pod defi-
nition or not, it will only consider the policies accessible to the user creating the pod. 
 You’ll see how to do this in the next exercise. You’ll start by creating an additional
PodSecurityPolicy.
CREATING A PODSECURITYPOLICY ALLOWING PRIVILEGED CONTAINERS TO BE DEPLOYED
You’ll create a special PodSecurityPolicy that will allow privileged users to create pods
with privileged containers. The following listing shows the policy’s definition.
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged          
spec:
  privileged: true        
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  volumes:
  - '*'
After you post this policy to the API server, you have two policies in the cluster:
$ kubectl get psp
NAME         PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    ...  
default      false   []     RunAsAny   RunAsAny    RunAsAny   ...
privileged   true    []     RunAsAny   RunAsAny    RunAsAny   ...
NOTE
The shorthand for PodSecurityPolicy is psp.
Listing 13.20
A PodSecurityPolicy for privileged users: psp-privileged.yaml
The name of this 
policy is "privileged.”
It allows running 
privileged containers.
 
</data>
  <data key="d5">396
CHAPTER 13
Securing cluster nodes and the network
13.3.5 Assigning different PodSecurityPolicies to different users 
and groups
We mentioned that a PodSecurityPolicy is a cluster-level resource, which means it
can’t be stored in and applied to a specific namespace. Does that mean it always
applies across all namespaces? No, because that would make them relatively unus-
able. After all, system pods must often be allowed to do things that regular pods
shouldn’t.
 Assigning different policies to different users is done through the RBAC mecha-
nism described in the previous chapter. The idea is to create as many policies as you
need and make them available to individual users or groups by creating ClusterRole
resources and pointing them to the individual policies by name. By binding those
ClusterRoles to specific users or groups with ClusterRoleBindings, when the Pod-
SecurityPolicy Admission Control plugin needs to decide whether to admit a pod defi-
nition or not, it will only consider the policies accessible to the user creating the pod. 
 You’ll see how to do this in the next exercise. You’ll start by creating an additional
PodSecurityPolicy.
CREATING A PODSECURITYPOLICY ALLOWING PRIVILEGED CONTAINERS TO BE DEPLOYED
You’ll create a special PodSecurityPolicy that will allow privileged users to create pods
with privileged containers. The following listing shows the policy’s definition.
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged          
spec:
  privileged: true        
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  volumes:
  - '*'
After you post this policy to the API server, you have two policies in the cluster:
$ kubectl get psp
NAME         PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    ...  
default      false   []     RunAsAny   RunAsAny    RunAsAny   ...
privileged   true    []     RunAsAny   RunAsAny    RunAsAny   ...
NOTE
The shorthand for PodSecurityPolicy is psp.
Listing 13.20
A PodSecurityPolicy for privileged users: psp-privileged.yaml
The name of this 
policy is "privileged.”
It allows running 
privileged containers.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="429">
  <data key="d0">Page_429</data>
  <data key="d5">Page_429</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_340">
  <data key="d0">397
Restricting the use of security-related features in pods
As you can see in the PRIV column, the default policy doesn’t allow running privi-
leged containers, whereas the privileged policy does. Because you’re currently
logged in as a cluster-admin, you can see all the policies. When creating pods, if any
policy allows you to deploy a pod with certain features, the API server will accept
your pod.
 Now imagine two additional users are using your cluster: Alice and Bob. You want
Alice to only deploy restricted (non-privileged) pods, but you want to allow Bob to
also deploy privileged pods. You do this by making sure Alice can only use the default
PodSecurityPolicy, while allowing Bob to use both.
USING RBAC TO ASSIGN DIFFERENT PODSECURITYPOLICIES TO DIFFERENT USERS
In the previous chapter, you used RBAC to grant users access to only certain resource
types, but I mentioned that access can be granted to specific resource instances by ref-
erencing them by name. That’s what you’ll use to make users use different Pod-
SecurityPolicy resources.
 First, you’ll create two ClusterRoles, each allowing the use of one of the policies.
You’ll call the first one psp-default and in it allow the use of the default Pod-
SecurityPolicy resource. You can use kubectl create clusterrole to do that:
$ kubectl create clusterrole psp-default --verb=use 
➥  --resource=podsecuritypolicies --resource-name=default
clusterrole "psp-default" created
NOTE
You’re using the special verb use instead of get, list, watch, or similar.
As you can see, you’re referring to a specific instance of a PodSecurityPolicy resource by
using the --resource-name option. Now, create another ClusterRole called psp-
privileged, pointing to the privileged policy:
$ kubectl create clusterrole psp-privileged --verb=use
➥  --resource=podsecuritypolicies --resource-name=privileged
clusterrole "psp-privileged" created
Now, you need to bind these two policies to users. As you may remember from the pre-
vious chapter, if you’re binding a ClusterRole that grants access to cluster-level
resources (which is what PodSecurityPolicy resources are), you need to use a Cluster-
RoleBinding instead of a (namespaced) RoleBinding. 
 You’re going to bind the psp-default ClusterRole to all authenticated users, not
only to Alice. This is necessary because otherwise no one could create any pods,
because the Admission Control plugin would complain that no policy is in place.
Authenticated users all belong to the system:authenticated group, so you’ll bind
the ClusterRole to the group:
$ kubectl create clusterrolebinding psp-all-users 
➥ --clusterrole=psp-default --group=system:authenticated
clusterrolebinding "psp-all-users" created
 
</data>
  <data key="d5">397
Restricting the use of security-related features in pods
As you can see in the PRIV column, the default policy doesn’t allow running privi-
leged containers, whereas the privileged policy does. Because you’re currently
logged in as a cluster-admin, you can see all the policies. When creating pods, if any
policy allows you to deploy a pod with certain features, the API server will accept
your pod.
 Now imagine two additional users are using your cluster: Alice and Bob. You want
Alice to only deploy restricted (non-privileged) pods, but you want to allow Bob to
also deploy privileged pods. You do this by making sure Alice can only use the default
PodSecurityPolicy, while allowing Bob to use both.
USING RBAC TO ASSIGN DIFFERENT PODSECURITYPOLICIES TO DIFFERENT USERS
In the previous chapter, you used RBAC to grant users access to only certain resource
types, but I mentioned that access can be granted to specific resource instances by ref-
erencing them by name. That’s what you’ll use to make users use different Pod-
SecurityPolicy resources.
 First, you’ll create two ClusterRoles, each allowing the use of one of the policies.
You’ll call the first one psp-default and in it allow the use of the default Pod-
SecurityPolicy resource. You can use kubectl create clusterrole to do that:
$ kubectl create clusterrole psp-default --verb=use 
➥  --resource=podsecuritypolicies --resource-name=default
clusterrole "psp-default" created
NOTE
You’re using the special verb use instead of get, list, watch, or similar.
As you can see, you’re referring to a specific instance of a PodSecurityPolicy resource by
using the --resource-name option. Now, create another ClusterRole called psp-
privileged, pointing to the privileged policy:
$ kubectl create clusterrole psp-privileged --verb=use
➥  --resource=podsecuritypolicies --resource-name=privileged
clusterrole "psp-privileged" created
Now, you need to bind these two policies to users. As you may remember from the pre-
vious chapter, if you’re binding a ClusterRole that grants access to cluster-level
resources (which is what PodSecurityPolicy resources are), you need to use a Cluster-
RoleBinding instead of a (namespaced) RoleBinding. 
 You’re going to bind the psp-default ClusterRole to all authenticated users, not
only to Alice. This is necessary because otherwise no one could create any pods,
because the Admission Control plugin would complain that no policy is in place.
Authenticated users all belong to the system:authenticated group, so you’ll bind
the ClusterRole to the group:
$ kubectl create clusterrolebinding psp-all-users 
➥ --clusterrole=psp-default --group=system:authenticated
clusterrolebinding "psp-all-users" created
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="430">
  <data key="d0">Page_430</data>
  <data key="d5">Page_430</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_341">
  <data key="d0">398
CHAPTER 13
Securing cluster nodes and the network
You’ll bind the psp-privileged ClusterRole only to Bob:
$ kubectl create clusterrolebinding psp-bob 
➥ --clusterrole=psp-privileged --user=bob
clusterrolebinding "psp-bob" created
As an authenticated user, Alice should now have access to the default PodSecurity-
Policy, whereas Bob should have access to both the default and the privileged Pod-
SecurityPolicies. Alice shouldn’t be able to create privileged pods, whereas Bob
should. Let’s see if that’s true.
CREATING ADDITIONAL USERS FOR KUBECTL
But how do you authenticate as Alice or Bob instead of whatever you’re authenticated
as currently? The book’s appendix A explains how kubectl can be used with multiple
clusters, but also with multiple contexts. A context includes the user credentials used
for talking to a cluster. Turn to appendix A to find out more. Here we’ll show the bare
commands enabling you to use kubectl as Alice or Bob. 
 First, you’ll create two new users in kubectl’s config with the following two
commands:
$ kubectl config set-credentials alice --username=alice --password=password
User "alice" set.
$ kubectl config set-credentials bob --username=bob --password=password
User "bob" set.
It should be obvious what the commands do. Because you’re setting username and
password credentials, kubectl will use basic HTTP authentication for these two users
(other authentication methods include tokens, client certificates, and so on).
CREATING PODS AS A DIFFERENT USER
You can now try creating a privileged pod while authenticating as Alice. You can tell
kubectl which user credentials to use by using the --user option:
$ kubectl --user alice create -f pod-privileged.yaml
Error from server (Forbidden): error when creating "pod-privileged.yaml": 
pods "pod-privileged" is forbidden: unable to validate against any pod 
security policy: [spec.containers[0].securityContext.privileged: Invalid 
value: true: Privileged containers are not allowed]
As expected, the API server doesn’t allow Alice to create privileged pods. Now, let’s see
if it allows Bob to do that:
$ kubectl --user bob create -f pod-privileged.yaml
pod "pod-privileged" created
 
</data>
  <data key="d5">398
CHAPTER 13
Securing cluster nodes and the network
You’ll bind the psp-privileged ClusterRole only to Bob:
$ kubectl create clusterrolebinding psp-bob 
➥ --clusterrole=psp-privileged --user=bob
clusterrolebinding "psp-bob" created
As an authenticated user, Alice should now have access to the default PodSecurity-
Policy, whereas Bob should have access to both the default and the privileged Pod-
SecurityPolicies. Alice shouldn’t be able to create privileged pods, whereas Bob
should. Let’s see if that’s true.
CREATING ADDITIONAL USERS FOR KUBECTL
But how do you authenticate as Alice or Bob instead of whatever you’re authenticated
as currently? The book’s appendix A explains how kubectl can be used with multiple
clusters, but also with multiple contexts. A context includes the user credentials used
for talking to a cluster. Turn to appendix A to find out more. Here we’ll show the bare
commands enabling you to use kubectl as Alice or Bob. 
 First, you’ll create two new users in kubectl’s config with the following two
commands:
$ kubectl config set-credentials alice --username=alice --password=password
User "alice" set.
$ kubectl config set-credentials bob --username=bob --password=password
User "bob" set.
It should be obvious what the commands do. Because you’re setting username and
password credentials, kubectl will use basic HTTP authentication for these two users
(other authentication methods include tokens, client certificates, and so on).
CREATING PODS AS A DIFFERENT USER
You can now try creating a privileged pod while authenticating as Alice. You can tell
kubectl which user credentials to use by using the --user option:
$ kubectl --user alice create -f pod-privileged.yaml
Error from server (Forbidden): error when creating "pod-privileged.yaml": 
pods "pod-privileged" is forbidden: unable to validate against any pod 
security policy: [spec.containers[0].securityContext.privileged: Invalid 
value: true: Privileged containers are not allowed]
As expected, the API server doesn’t allow Alice to create privileged pods. Now, let’s see
if it allows Bob to do that:
$ kubectl --user bob create -f pod-privileged.yaml
pod "pod-privileged" created
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="431">
  <data key="d0">Page_431</data>
  <data key="d5">Page_431</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_342">
  <data key="d0">399
Isolating the pod network
And there you go. You’ve successfully used RBAC to make the Admission Control
plugin use different PodSecurityPolicy resources for different users.
13.4
Isolating the pod network
Up to now in this chapter, we’ve explored many security-related configuration options
that apply to individual pods and their containers. In the remainder of this chapter,
we’ll look at how the network between pods can be secured by limiting which pods can
talk to which pods.
 Whether this is configurable or not depends on which container networking
plugin is used in the cluster. If the networking plugin supports it, you can configure
network isolation by creating NetworkPolicy resources. 
 A NetworkPolicy applies to pods that match its label selector and specifies either
which sources can access the matched pods or which destinations can be accessed
from the matched pods. This is configured through ingress and egress rules, respec-
tively. Both types of rules can match only the pods that match a pod selector, all
pods in a namespace whose labels match a namespace selector, or a network IP
block specified using Classless Inter-Domain Routing (CIDR) notation (for example,
192.168.1.0/24). 
 We’ll look at both ingress and egress rules and all three matching options.
NOTE
Ingress rules in a NetworkPolicy have nothing to do with the Ingress
resource discussed in chapter 5.
13.4.1 Enabling network isolation in a namespace
By default, pods in a given namespace can be accessed by anyone. First, you’ll need
to change that. You’ll create a default-deny NetworkPolicy, which will prevent all
clients from connecting to any pod in your namespace. The NetworkPolicy defini-
tion is shown in the following listing.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector:        
When you create this NetworkPolicy in a certain namespace, no one can connect to
any pod in that namespace. 
 
 
 
Listing 13.21
A default-deny NetworkPolicy: network-policy-default-deny.yaml
Empty pod selector 
matches all pods in the 
same namespace
 
</data>
  <data key="d5">399
Isolating the pod network
And there you go. You’ve successfully used RBAC to make the Admission Control
plugin use different PodSecurityPolicy resources for different users.
13.4
Isolating the pod network
Up to now in this chapter, we’ve explored many security-related configuration options
that apply to individual pods and their containers. In the remainder of this chapter,
we’ll look at how the network between pods can be secured by limiting which pods can
talk to which pods.
 Whether this is configurable or not depends on which container networking
plugin is used in the cluster. If the networking plugin supports it, you can configure
network isolation by creating NetworkPolicy resources. 
 A NetworkPolicy applies to pods that match its label selector and specifies either
which sources can access the matched pods or which destinations can be accessed
from the matched pods. This is configured through ingress and egress rules, respec-
tively. Both types of rules can match only the pods that match a pod selector, all
pods in a namespace whose labels match a namespace selector, or a network IP
block specified using Classless Inter-Domain Routing (CIDR) notation (for example,
192.168.1.0/24). 
 We’ll look at both ingress and egress rules and all three matching options.
NOTE
Ingress rules in a NetworkPolicy have nothing to do with the Ingress
resource discussed in chapter 5.
13.4.1 Enabling network isolation in a namespace
By default, pods in a given namespace can be accessed by anyone. First, you’ll need
to change that. You’ll create a default-deny NetworkPolicy, which will prevent all
clients from connecting to any pod in your namespace. The NetworkPolicy defini-
tion is shown in the following listing.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector:        
When you create this NetworkPolicy in a certain namespace, no one can connect to
any pod in that namespace. 
 
 
 
Listing 13.21
A default-deny NetworkPolicy: network-policy-default-deny.yaml
Empty pod selector 
matches all pods in the 
same namespace
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="432">
  <data key="d0">Page_432</data>
  <data key="d5">Page_432</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_343">
  <data key="d0">400
CHAPTER 13
Securing cluster nodes and the network
NOTE
The CNI plugin or other type of networking solution used in the clus-
ter must support NetworkPolicy, or else there will be no effect on inter-pod
connectivity.
13.4.2 Allowing only some pods in the namespace to connect to 
a server pod
To let clients connect to the pods in the namespace, you must now explicitly say who
can connect to the pods. By who I mean which pods. Let’s explore how to do this
through an example. 
 Imagine having a PostgreSQL database pod running in namespace foo and a web-
server pod that uses the database. Other pods are also in the namespace, and you
don’t want to allow them to connect to the database. To secure the network, you need
to create the NetworkPolicy resource shown in the following listing in the same name-
space as the database pod.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: postgres-netpolicy
spec:
  podSelector:                     
    matchLabels:                   
      app: database                
  ingress:                           
  - from:                            
    - podSelector:                   
        matchLabels:                 
          app: webserver             
    ports:                     
    - port: 5432               
The example NetworkPolicy allows pods with the app=webserver label to connect to
pods with the app=database label, and only on port 5432. Other pods can’t connect to
the database pods, and no one (not even the webserver pods) can connect to anything
other than port 5432 of the database pods. This is shown in figure 13.4.
 Client pods usually connect to server pods through a Service instead of directly to
the pod, but that doesn’t change anything. The NetworkPolicy is enforced when con-
necting through a Service, as well.
 
 
 
 
Listing 13.22
A NetworkPolicy for the Postgres pod: network-policy-postgres.yaml
This policy secures 
access to pods with 
app=database label.
It allows incoming connections 
only from pods with the 
app=webserver label.
Connections to this 
port are allowed.
 
</data>
  <data key="d5">400
CHAPTER 13
Securing cluster nodes and the network
NOTE
The CNI plugin or other type of networking solution used in the clus-
ter must support NetworkPolicy, or else there will be no effect on inter-pod
connectivity.
13.4.2 Allowing only some pods in the namespace to connect to 
a server pod
To let clients connect to the pods in the namespace, you must now explicitly say who
can connect to the pods. By who I mean which pods. Let’s explore how to do this
through an example. 
 Imagine having a PostgreSQL database pod running in namespace foo and a web-
server pod that uses the database. Other pods are also in the namespace, and you
don’t want to allow them to connect to the database. To secure the network, you need
to create the NetworkPolicy resource shown in the following listing in the same name-
space as the database pod.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: postgres-netpolicy
spec:
  podSelector:                     
    matchLabels:                   
      app: database                
  ingress:                           
  - from:                            
    - podSelector:                   
        matchLabels:                 
          app: webserver             
    ports:                     
    - port: 5432               
The example NetworkPolicy allows pods with the app=webserver label to connect to
pods with the app=database label, and only on port 5432. Other pods can’t connect to
the database pods, and no one (not even the webserver pods) can connect to anything
other than port 5432 of the database pods. This is shown in figure 13.4.
 Client pods usually connect to server pods through a Service instead of directly to
the pod, but that doesn’t change anything. The NetworkPolicy is enforced when con-
necting through a Service, as well.
 
 
 
 
Listing 13.22
A NetworkPolicy for the Postgres pod: network-policy-postgres.yaml
This policy secures 
access to pods with 
app=database label.
It allows incoming connections 
only from pods with the 
app=webserver label.
Connections to this 
port are allowed.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="433">
  <data key="d0">Page_433</data>
  <data key="d5">Page_433</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_344">
  <data key="d0">401
Isolating the pod network
13.4.3 Isolating the network between Kubernetes namespaces
Now let’s look at another example, where multiple tenants are using the same Kuber-
netes cluster. Each tenant can use multiple namespaces, and each namespace has a
label specifying the tenant it belongs to. For example, one of those tenants is Man-
ning. All their namespaces have been labeled with tenant: manning. In one of their
namespaces, they run a Shopping Cart microservice that needs to be available to all
pods running in any of their namespaces. Obviously, they don’t want any other tenants
to access their microservice.
 To secure their microservice, they create the NetworkPolicy resource shown in the
following listing.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: shoppingcart-netpolicy
spec:
  podSelector:                       
    matchLabels:                     
      app: shopping-cart             
  ingress:
  - from:
    - namespaceSelector:            
        matchLabels:                
          tenant: manning           
    ports:
    - port: 80
Listing 13.23
NetworkPolicy for the shopping cart pod(s): network-policy-cart.yaml
app: database
Pod:
database
Port
5432
Port
9876
app: webserver
Pod:
webserver
Pod selector:
app=webserver
Pod selector:
app=database
app: webserver
Pod:
webserver
Other pods
NetworkPolicy: postgres-netpolicy
Figure 13.4
A NetworkPolicy allowing only some pods to access other pods and only on a specific 
port
This policy applies to pods 
labeled as microservice= 
shopping-cart.
Only pods running in namespaces 
labeled as tenant=manning are 
allowed to access the microservice.
 
</data>
  <data key="d5">401
Isolating the pod network
13.4.3 Isolating the network between Kubernetes namespaces
Now let’s look at another example, where multiple tenants are using the same Kuber-
netes cluster. Each tenant can use multiple namespaces, and each namespace has a
label specifying the tenant it belongs to. For example, one of those tenants is Man-
ning. All their namespaces have been labeled with tenant: manning. In one of their
namespaces, they run a Shopping Cart microservice that needs to be available to all
pods running in any of their namespaces. Obviously, they don’t want any other tenants
to access their microservice.
 To secure their microservice, they create the NetworkPolicy resource shown in the
following listing.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: shoppingcart-netpolicy
spec:
  podSelector:                       
    matchLabels:                     
      app: shopping-cart             
  ingress:
  - from:
    - namespaceSelector:            
        matchLabels:                
          tenant: manning           
    ports:
    - port: 80
Listing 13.23
NetworkPolicy for the shopping cart pod(s): network-policy-cart.yaml
app: database
Pod:
database
Port
5432
Port
9876
app: webserver
Pod:
webserver
Pod selector:
app=webserver
Pod selector:
app=database
app: webserver
Pod:
webserver
Other pods
NetworkPolicy: postgres-netpolicy
Figure 13.4
A NetworkPolicy allowing only some pods to access other pods and only on a specific 
port
This policy applies to pods 
labeled as microservice= 
shopping-cart.
Only pods running in namespaces 
labeled as tenant=manning are 
allowed to access the microservice.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="434">
  <data key="d0">Page_434</data>
  <data key="d5">Page_434</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_345">
  <data key="d0">402
CHAPTER 13
Securing cluster nodes and the network
This NetworkPolicy ensures only pods running in namespaces labeled as tenant:
manning can access their Shopping Cart microservice, as shown in figure 13.5.
If the shopping cart provider also wants to give access to other tenants (perhaps to
one of their partner companies), they can either create an additional NetworkPolicy
resource or add an additional ingress rule to their existing NetworkPolicy.
NOTE
In a multi-tenant Kubernetes cluster, tenants usually can’t add labels
(or annotations) to their namespaces themselves. If they could, they’d be able
to circumvent the namespaceSelector-based ingress rules.
13.4.4 Isolating using CIDR notation
Instead of specifying a pod- or namespace selector to define who can access the pods
targeted in the NetworkPolicy, you can also specify an IP block in CIDR notation. For
example, to allow the shopping-cart pods from the previous section to only be acces-
sible from IPs in the 192.168.1.1 to .255 range, you’d specify the ingress rule in the
next listing.
  ingress:
  - from:
    - ipBlock:                    
        cidr: 192.168.1.0/24      
Listing 13.24
Specifying an IP block in an ingress rule: network-policy-cidr.yaml
app: shopping-cart
Pod:
shopping-cart
Port
80
Namespace selector:
tenant=manning
Pod selector:
app=shopping-cart
Other pods
Pods
NetworkPolicy:
shoppingcart-netpolicy
Namespace: manningA
Namespace: ecommerce-ltd
Other namespaces
tenant: manning
Pods
Namespace: manningB
tenant: manning
Figure 13.5
A NetworkPolicy only allowing pods in namespaces matching a namespaceSelector to access a 
specific pod.
This ingress rule only allows traffic from 
clients in the 192.168.1.0/24 IP block. 
 
</data>
  <data key="d5">402
CHAPTER 13
Securing cluster nodes and the network
This NetworkPolicy ensures only pods running in namespaces labeled as tenant:
manning can access their Shopping Cart microservice, as shown in figure 13.5.
If the shopping cart provider also wants to give access to other tenants (perhaps to
one of their partner companies), they can either create an additional NetworkPolicy
resource or add an additional ingress rule to their existing NetworkPolicy.
NOTE
In a multi-tenant Kubernetes cluster, tenants usually can’t add labels
(or annotations) to their namespaces themselves. If they could, they’d be able
to circumvent the namespaceSelector-based ingress rules.
13.4.4 Isolating using CIDR notation
Instead of specifying a pod- or namespace selector to define who can access the pods
targeted in the NetworkPolicy, you can also specify an IP block in CIDR notation. For
example, to allow the shopping-cart pods from the previous section to only be acces-
sible from IPs in the 192.168.1.1 to .255 range, you’d specify the ingress rule in the
next listing.
  ingress:
  - from:
    - ipBlock:                    
        cidr: 192.168.1.0/24      
Listing 13.24
Specifying an IP block in an ingress rule: network-policy-cidr.yaml
app: shopping-cart
Pod:
shopping-cart
Port
80
Namespace selector:
tenant=manning
Pod selector:
app=shopping-cart
Other pods
Pods
NetworkPolicy:
shoppingcart-netpolicy
Namespace: manningA
Namespace: ecommerce-ltd
Other namespaces
tenant: manning
Pods
Namespace: manningB
tenant: manning
Figure 13.5
A NetworkPolicy only allowing pods in namespaces matching a namespaceSelector to access a 
specific pod.
This ingress rule only allows traffic from 
clients in the 192.168.1.0/24 IP block. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="435">
  <data key="d0">Page_435</data>
  <data key="d5">Page_435</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_346">
  <data key="d0">403
Summary
13.4.5 Limiting the outbound traffic of a set of pods
In all previous examples, you’ve been limiting the inbound traffic to the pods that
match the NetworkPolicy’s pod selector using ingress rules, but you can also limit
their outbound traffic through egress rules. An example is shown in the next listing.
spec:
  podSelector:               
    matchLabels:             
      app: webserver         
  egress:               
  - to:                       
    - podSelector:            
        matchLabels:          
          app: database       
The NetworkPolicy in the previous listing allows pods that have the app=webserver
label to only access pods that have the app=database label and nothing else (neither
other pods, nor any other IP, regardless of whether it’s internal or external to the
cluster).
13.5
Summary
In this chapter, you learned about securing cluster nodes from pods and pods from
other pods. You learned that
Pods can use the node’s Linux namespaces instead of using their own.
Containers can be configured to run as a different user and/or group than the
one defined in the container image.
Containers can also run in privileged mode, allowing them to access the node’s
devices that are otherwise not exposed to pods.
Containers can be run as read-only, preventing processes from writing to the
container’s filesystem (and only allowing them to write to mounted volumes).
Cluster-level PodSecurityPolicy resources can be created to prevent users from
creating pods that could compromise a node.
PodSecurityPolicy resources can be associated with specific users using RBAC’s
ClusterRoles and ClusterRoleBindings.
NetworkPolicy resources are used to limit a pod’s inbound and/or outbound
traffic.
In the next chapter, you’ll learn how computational resources available to pods can be
constrained and how a pod’s quality of service is configured.
Listing 13.25
Using egress rules in a NetworkPolicy: network-policy-egress.yaml
This policy applies to pods with 
the app=webserver label.
It limits
the pods’
outbound
traffic.
Webserver pods may only 
connect to pods with the 
app=database label.
 
</data>
  <data key="d5">403
Summary
13.4.5 Limiting the outbound traffic of a set of pods
In all previous examples, you’ve been limiting the inbound traffic to the pods that
match the NetworkPolicy’s pod selector using ingress rules, but you can also limit
their outbound traffic through egress rules. An example is shown in the next listing.
spec:
  podSelector:               
    matchLabels:             
      app: webserver         
  egress:               
  - to:                       
    - podSelector:            
        matchLabels:          
          app: database       
The NetworkPolicy in the previous listing allows pods that have the app=webserver
label to only access pods that have the app=database label and nothing else (neither
other pods, nor any other IP, regardless of whether it’s internal or external to the
cluster).
13.5
Summary
In this chapter, you learned about securing cluster nodes from pods and pods from
other pods. You learned that
Pods can use the node’s Linux namespaces instead of using their own.
Containers can be configured to run as a different user and/or group than the
one defined in the container image.
Containers can also run in privileged mode, allowing them to access the node’s
devices that are otherwise not exposed to pods.
Containers can be run as read-only, preventing processes from writing to the
container’s filesystem (and only allowing them to write to mounted volumes).
Cluster-level PodSecurityPolicy resources can be created to prevent users from
creating pods that could compromise a node.
PodSecurityPolicy resources can be associated with specific users using RBAC’s
ClusterRoles and ClusterRoleBindings.
NetworkPolicy resources are used to limit a pod’s inbound and/or outbound
traffic.
In the next chapter, you’ll learn how computational resources available to pods can be
constrained and how a pod’s quality of service is configured.
Listing 13.25
Using egress rules in a NetworkPolicy: network-policy-egress.yaml
This policy applies to pods with 
the app=webserver label.
It limits
the pods’
outbound
traffic.
Webserver pods may only 
connect to pods with the 
app=database label.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="436">
  <data key="d0">Page_436</data>
  <data key="d5">Page_436</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_347">
  <data key="d0">404
Managing pods’
 computational resources
Up to now you’ve created pods without caring about how much CPU and memory
they’re allowed to consume. But as you’ll see in this chapter, setting both how
much a pod is expected to consume and the maximum amount it’s allowed to con-
sume is a vital part of any pod definition. Setting these two sets of parameters
makes sure that a pod takes only its fair share of the resources provided by the
Kubernetes cluster and also affects how pods are scheduled across the cluster.
This chapter covers
Requesting CPU, memory, and other 
computational resources for containers
Setting a hard limit for CPU and memory
Understanding Quality of Service guarantees for 
pods
Setting default, min, and max resources for pods 
in a namespace
Limiting the total amount of resources available 
in a namespace
 
</data>
  <data key="d5">404
Managing pods’
 computational resources
Up to now you’ve created pods without caring about how much CPU and memory
they’re allowed to consume. But as you’ll see in this chapter, setting both how
much a pod is expected to consume and the maximum amount it’s allowed to con-
sume is a vital part of any pod definition. Setting these two sets of parameters
makes sure that a pod takes only its fair share of the resources provided by the
Kubernetes cluster and also affects how pods are scheduled across the cluster.
This chapter covers
Requesting CPU, memory, and other 
computational resources for containers
Setting a hard limit for CPU and memory
Understanding Quality of Service guarantees for 
pods
Setting default, min, and max resources for pods 
in a namespace
Limiting the total amount of resources available 
in a namespace
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="437">
  <data key="d0">Page_437</data>
  <data key="d5">Page_437</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_348">
  <data key="d0">405
Requesting resources for a pod’s containers
14.1
Requesting resources for a pod’s containers
When creating a pod, you can specify the amount of CPU and memory that a con-
tainer needs (these are called requests) and a hard limit on what it may consume
(known as limits). They’re specified for each container individually, not for the pod as
a whole. The pod’s resource requests and limits are the sum of the requests and lim-
its of all its containers. 
14.1.1 Creating pods with resource requests
Let’s look at an example pod manifest, which has the CPU and memory requests spec-
ified for its single container, as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: requests-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main              
    resources:              
      requests:             
        cpu: 200m          
        memory: 10Mi    
In the pod manifest, your single container requires one-fifth of a CPU core (200 mil-
licores) to run properly. Five such pods/containers can run sufficiently fast on a single
CPU core. 
 When you don’t specify a request for CPU, you’re saying you don’t care how much
CPU time the process running in your container is allotted. In the worst case, it may
not get any CPU time at all (this happens when a heavy demand by other processes
exists on the CPU). Although this may be fine for low-priority batch jobs, which aren’t
time-critical, it obviously isn’t appropriate for containers handling user requests.
 In the pod spec, you’re also requesting 10 mebibytes of memory for the container.
By doing that, you’re saying that you expect the processes running inside the con-
tainer to use at most 10 mebibytes of RAM. They might use less, but you’re not expect-
ing them to use more than that in normal circumstances. Later in this chapter you’ll
see what happens if they do.
 Now you’ll run the pod. When the pod starts, you can take a quick look at the pro-
cess’ CPU consumption by running the top command inside the container, as shown
in the following listing.
Listing 14.1
A pod with resource requests: requests-pod.yaml
You’re specifying resource 
requests for the main container.
The container requests 200 
millicores (that is, 1/5 of a 
single CPU core’s time).
The container also
requests 10 mebibytes
of memory.
 
</data>
  <data key="d5">405
Requesting resources for a pod’s containers
14.1
Requesting resources for a pod’s containers
When creating a pod, you can specify the amount of CPU and memory that a con-
tainer needs (these are called requests) and a hard limit on what it may consume
(known as limits). They’re specified for each container individually, not for the pod as
a whole. The pod’s resource requests and limits are the sum of the requests and lim-
its of all its containers. 
14.1.1 Creating pods with resource requests
Let’s look at an example pod manifest, which has the CPU and memory requests spec-
ified for its single container, as shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: requests-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main              
    resources:              
      requests:             
        cpu: 200m          
        memory: 10Mi    
In the pod manifest, your single container requires one-fifth of a CPU core (200 mil-
licores) to run properly. Five such pods/containers can run sufficiently fast on a single
CPU core. 
 When you don’t specify a request for CPU, you’re saying you don’t care how much
CPU time the process running in your container is allotted. In the worst case, it may
not get any CPU time at all (this happens when a heavy demand by other processes
exists on the CPU). Although this may be fine for low-priority batch jobs, which aren’t
time-critical, it obviously isn’t appropriate for containers handling user requests.
 In the pod spec, you’re also requesting 10 mebibytes of memory for the container.
By doing that, you’re saying that you expect the processes running inside the con-
tainer to use at most 10 mebibytes of RAM. They might use less, but you’re not expect-
ing them to use more than that in normal circumstances. Later in this chapter you’ll
see what happens if they do.
 Now you’ll run the pod. When the pod starts, you can take a quick look at the pro-
cess’ CPU consumption by running the top command inside the container, as shown
in the following listing.
Listing 14.1
A pod with resource requests: requests-pod.yaml
You’re specifying resource 
requests for the main container.
The container requests 200 
millicores (that is, 1/5 of a 
single CPU core’s time).
The container also
requests 10 mebibytes
of memory.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="438">
  <data key="d0">Page_438</data>
  <data key="d5">Page_438</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_349">
  <data key="d0">406
CHAPTER 14
Managing pods’ computational resources
$ kubectl exec -it requests-pod top
Mem: 1288116K used, 760368K free, 9196K shrd, 25748K buff, 814840K cached
CPU:  9.1% usr 42.1% sys  0.0% nic 48.4% idle  0.0% io  0.0% irq  0.2% sirq
Load average: 0.79 0.52 0.29 2/481 10
  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND
    1     0 root     R     1192  0.0   1 50.2 dd if /dev/zero of /dev/null
    7     0 root     R     1200  0.0   0  0.0 top
The dd command you’re running in the container consumes as much CPU as it can,
but it only runs a single thread so it can only use a single core. The Minikube VM,
which is where this example is running, has two CPU cores allotted to it. That’s why
the process is shown consuming 50% of the whole CPU. 
 Fifty percent of two cores is obviously one whole core, which means the container
is using more than the 200 millicores you requested in the pod specification. This is
expected, because requests don’t limit the amount of CPU a container can use. You’d
need to specify a CPU limit to do that. You’ll try that later, but first, let’s see how spec-
ifying resource requests in a pod affects the scheduling of the pod.
14.1.2 Understanding how resource requests affect scheduling
By specifying resource requests, you’re specifying the minimum amount of resources
your pod needs. This information is what the Scheduler uses when scheduling the pod
to a node. Each node has a certain amount of CPU and memory it can allocate to
pods. When scheduling a pod, the Scheduler will only consider nodes with enough
unallocated resources to meet the pod’s resource requirements. If the amount of
unallocated CPU or memory is less than what the pod requests, Kubernetes will not
schedule the pod to that node, because the node can’t provide the minimum amount
required by the pod.
UNDERSTANDING HOW THE SCHEDULER DETERMINES IF A POD CAN FIT ON A NODE
What’s important and somewhat surprising here is that the Scheduler doesn’t look at
how much of each individual resource is being used at the exact time of scheduling
but at the sum of resources requested by the existing pods deployed on the node.
Even though existing pods may be using less than what they’ve requested, scheduling
another pod based on actual resource consumption would break the guarantee given
to the already deployed pods.
 This is visualized in figure 14.1. Three pods are deployed on the node. Together,
they’ve requested 80% of the node’s CPU and 60% of the node’s memory. Pod D,
shown at the bottom right of the figure, cannot be scheduled onto the node because it
requests 25% of the CPU, which is more than the 20% of unallocated CPU. The fact
that the three pods are currently using only 70% of the CPU makes no difference.
Listing 14.2
Examining CPU and memory usage from within a container
 
</data>
  <data key="d5">406
CHAPTER 14
Managing pods’ computational resources
$ kubectl exec -it requests-pod top
Mem: 1288116K used, 760368K free, 9196K shrd, 25748K buff, 814840K cached
CPU:  9.1% usr 42.1% sys  0.0% nic 48.4% idle  0.0% io  0.0% irq  0.2% sirq
Load average: 0.79 0.52 0.29 2/481 10
  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND
    1     0 root     R     1192  0.0   1 50.2 dd if /dev/zero of /dev/null
    7     0 root     R     1200  0.0   0  0.0 top
The dd command you’re running in the container consumes as much CPU as it can,
but it only runs a single thread so it can only use a single core. The Minikube VM,
which is where this example is running, has two CPU cores allotted to it. That’s why
the process is shown consuming 50% of the whole CPU. 
 Fifty percent of two cores is obviously one whole core, which means the container
is using more than the 200 millicores you requested in the pod specification. This is
expected, because requests don’t limit the amount of CPU a container can use. You’d
need to specify a CPU limit to do that. You’ll try that later, but first, let’s see how spec-
ifying resource requests in a pod affects the scheduling of the pod.
14.1.2 Understanding how resource requests affect scheduling
By specifying resource requests, you’re specifying the minimum amount of resources
your pod needs. This information is what the Scheduler uses when scheduling the pod
to a node. Each node has a certain amount of CPU and memory it can allocate to
pods. When scheduling a pod, the Scheduler will only consider nodes with enough
unallocated resources to meet the pod’s resource requirements. If the amount of
unallocated CPU or memory is less than what the pod requests, Kubernetes will not
schedule the pod to that node, because the node can’t provide the minimum amount
required by the pod.
UNDERSTANDING HOW THE SCHEDULER DETERMINES IF A POD CAN FIT ON A NODE
What’s important and somewhat surprising here is that the Scheduler doesn’t look at
how much of each individual resource is being used at the exact time of scheduling
but at the sum of resources requested by the existing pods deployed on the node.
Even though existing pods may be using less than what they’ve requested, scheduling
another pod based on actual resource consumption would break the guarantee given
to the already deployed pods.
 This is visualized in figure 14.1. Three pods are deployed on the node. Together,
they’ve requested 80% of the node’s CPU and 60% of the node’s memory. Pod D,
shown at the bottom right of the figure, cannot be scheduled onto the node because it
requests 25% of the CPU, which is more than the 20% of unallocated CPU. The fact
that the three pods are currently using only 70% of the CPU makes no difference.
Listing 14.2
Examining CPU and memory usage from within a container
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="439">
  <data key="d0">Page_439</data>
  <data key="d5">Page_439</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_350">
  <data key="d0">407
Requesting resources for a pod’s containers
UNDERSTANDING HOW THE SCHEDULER USES PODS’ REQUESTS WHEN SELECTING THE BEST NODE 
FOR A POD
You may remember from chapter 11 that the Scheduler first filters the list of nodes to
exclude those that the pod can’t fit on and then prioritizes the remaining nodes per the
configured prioritization functions. Among others, two prioritization functions rank
nodes based on the amount of resources requested: LeastRequestedPriority and
MostRequestedPriority. The first one prefers nodes with fewer requested resources
(with a greater amount of unallocated resources), whereas the second one is the exact
opposite—it prefers nodes that have the most requested resources (a smaller amount of
unallocated CPU and memory). But, as we’ve discussed, they both consider the amount
of requested resources, not the amount of resources actually consumed.
 The Scheduler is configured to use only one of those functions. You may wonder
why anyone would want to use the MostRequestedPriority function. After all, if you
have a set of nodes, you usually want to spread CPU load evenly across them. However,
that’s not the case when running on cloud infrastructure, where you can add and
remove nodes whenever necessary. By configuring the Scheduler to use the Most-
RequestedPriority function, you guarantee that Kubernetes will use the smallest pos-
sible number of nodes while still providing each pod with the amount of CPU/memory
it requests. By keeping pods tightly packed, certain nodes are left vacant and can be
removed. Because you’re paying for individual nodes, this saves you money.
INSPECTING A NODE’S CAPACITY
Let’s see the Scheduler in action. You’ll deploy another pod with four times the
amount of requested resources as before. But before you do that, let’s see your node’s
capacity. Because the Scheduler needs to know how much CPU and memory each
node has, the Kubelet reports this data to the API server, making it available through
Pod C
Node
Pod A
Unallocated
CPU requests
Pod B
Pod A
Currently unused
CPU usage
Pod B
Pod C
0%
100%
Pod A
Memory requests
Pod B
Pod C
Pod A
Memory usage
Pod B
Pod C
CPU requests
Memory requests
Unallocated
Currently unused
Pod D
Pod D cannot be scheduled; its CPU
requests exceed unallocated CPU
Figure 14.1
The Scheduler only cares about requests, not actual usage.
 
</data>
  <data key="d5">407
Requesting resources for a pod’s containers
UNDERSTANDING HOW THE SCHEDULER USES PODS’ REQUESTS WHEN SELECTING THE BEST NODE 
FOR A POD
You may remember from chapter 11 that the Scheduler first filters the list of nodes to
exclude those that the pod can’t fit on and then prioritizes the remaining nodes per the
configured prioritization functions. Among others, two prioritization functions rank
nodes based on the amount of resources requested: LeastRequestedPriority and
MostRequestedPriority. The first one prefers nodes with fewer requested resources
(with a greater amount of unallocated resources), whereas the second one is the exact
opposite—it prefers nodes that have the most requested resources (a smaller amount of
unallocated CPU and memory). But, as we’ve discussed, they both consider the amount
of requested resources, not the amount of resources actually consumed.
 The Scheduler is configured to use only one of those functions. You may wonder
why anyone would want to use the MostRequestedPriority function. After all, if you
have a set of nodes, you usually want to spread CPU load evenly across them. However,
that’s not the case when running on cloud infrastructure, where you can add and
remove nodes whenever necessary. By configuring the Scheduler to use the Most-
RequestedPriority function, you guarantee that Kubernetes will use the smallest pos-
sible number of nodes while still providing each pod with the amount of CPU/memory
it requests. By keeping pods tightly packed, certain nodes are left vacant and can be
removed. Because you’re paying for individual nodes, this saves you money.
INSPECTING A NODE’S CAPACITY
Let’s see the Scheduler in action. You’ll deploy another pod with four times the
amount of requested resources as before. But before you do that, let’s see your node’s
capacity. Because the Scheduler needs to know how much CPU and memory each
node has, the Kubelet reports this data to the API server, making it available through
Pod C
Node
Pod A
Unallocated
CPU requests
Pod B
Pod A
Currently unused
CPU usage
Pod B
Pod C
0%
100%
Pod A
Memory requests
Pod B
Pod C
Pod A
Memory usage
Pod B
Pod C
CPU requests
Memory requests
Unallocated
Currently unused
Pod D
Pod D cannot be scheduled; its CPU
requests exceed unallocated CPU
Figure 14.1
The Scheduler only cares about requests, not actual usage.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="440">
  <data key="d0">Page_440</data>
  <data key="d5">Page_440</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_351">
  <data key="d0">408
CHAPTER 14
Managing pods’ computational resources
the Node resource. You can see it by using the kubectl describe command as in the
following listing.
$ kubectl describe nodes
Name:       minikube
...
Capacity:                       
  cpu:           2               
  memory:        2048484Ki       
  pods:          110             
Allocatable:                       
  cpu:           2                  
  memory:        1946084Ki          
  pods:          110                
...
The output shows two sets of amounts related to the available resources on the node:
the node’s capacity and allocatable resources. The capacity represents the total resources
of a node, which may not all be available to pods. Certain resources may be reserved
for Kubernetes and/or system components. The Scheduler bases its decisions only on
the allocatable resource amounts.
 In the previous example, the node called minikube runs in a VM with two cores
and has no CPU reserved, making the whole CPU allocatable to pods. Therefore,
the Scheduler should have no problem scheduling another pod requesting 800
millicores. 
 Run the pod now. You can use the YAML file in the code archive, or run the pod
with the kubectl run command like this:
$ kubectl run requests-pod-2 --image=busybox --restart Never
➥ --requests='cpu=800m,memory=20Mi' -- dd if=/dev/zero of=/dev/null
pod "requests-pod-2" created
Let’s see if it was scheduled:
$ kubectl get po requests-pod-2
NAME             READY     STATUS    RESTARTS   AGE
requests-pod-2   1/1       Running   0          3m
Okay, the pod has been scheduled and is running. 
CREATING A POD THAT DOESN’T FIT ON ANY NODE
You now have two pods deployed, which together have requested a total of 1,000 mil-
licores or exactly 1 core. You should therefore have another 1,000 millicores available
for additional pods, right? You can deploy another pod with a resource request of
1,000 millicores. Use a similar command as before:
$ kubectl run requests-pod-3 --image=busybox --restart Never
➥ --requests='cpu=1,memory=20Mi' -- dd if=/dev/zero of=/dev/null
pod "requests-pod-2" created
Listing 14.3
A node’s capacity and allocatable resources
The overall capacity 
of the node
The resources 
allocatable to pods
 
</data>
  <data key="d5">408
CHAPTER 14
Managing pods’ computational resources
the Node resource. You can see it by using the kubectl describe command as in the
following listing.
$ kubectl describe nodes
Name:       minikube
...
Capacity:                       
  cpu:           2               
  memory:        2048484Ki       
  pods:          110             
Allocatable:                       
  cpu:           2                  
  memory:        1946084Ki          
  pods:          110                
...
The output shows two sets of amounts related to the available resources on the node:
the node’s capacity and allocatable resources. The capacity represents the total resources
of a node, which may not all be available to pods. Certain resources may be reserved
for Kubernetes and/or system components. The Scheduler bases its decisions only on
the allocatable resource amounts.
 In the previous example, the node called minikube runs in a VM with two cores
and has no CPU reserved, making the whole CPU allocatable to pods. Therefore,
the Scheduler should have no problem scheduling another pod requesting 800
millicores. 
 Run the pod now. You can use the YAML file in the code archive, or run the pod
with the kubectl run command like this:
$ kubectl run requests-pod-2 --image=busybox --restart Never
➥ --requests='cpu=800m,memory=20Mi' -- dd if=/dev/zero of=/dev/null
pod "requests-pod-2" created
Let’s see if it was scheduled:
$ kubectl get po requests-pod-2
NAME             READY     STATUS    RESTARTS   AGE
requests-pod-2   1/1       Running   0          3m
Okay, the pod has been scheduled and is running. 
CREATING A POD THAT DOESN’T FIT ON ANY NODE
You now have two pods deployed, which together have requested a total of 1,000 mil-
licores or exactly 1 core. You should therefore have another 1,000 millicores available
for additional pods, right? You can deploy another pod with a resource request of
1,000 millicores. Use a similar command as before:
$ kubectl run requests-pod-3 --image=busybox --restart Never
➥ --requests='cpu=1,memory=20Mi' -- dd if=/dev/zero of=/dev/null
pod "requests-pod-2" created
Listing 14.3
A node’s capacity and allocatable resources
The overall capacity 
of the node
The resources 
allocatable to pods
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="441">
  <data key="d0">Page_441</data>
  <data key="d5">Page_441</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_352">
  <data key="d0">409
Requesting resources for a pod’s containers
NOTE
This time you’re specifying the CPU request in whole cores (cpu=1)
instead of millicores (cpu=1000m).
So far, so good. The pod has been accepted by the API server (you’ll remember from
the previous chapter that the API server can reject pods if they’re invalid in any way).
Now, check if the pod is running:
$ kubectl get po requests-pod-3
NAME             READY     STATUS    RESTARTS   AGE
requests-pod-3   0/1       Pending   0          4m
Even if you wait a while, the pod is still stuck at Pending. You can see more informa-
tion on why that’s the case by using the kubectl describe command, as shown in
the following listing.
$ kubectl describe po requests-pod-3
Name:       requests-pod-3
Namespace:  default
Node:       /                    
...
Conditions:
  Type           Status
  PodScheduled   False           
...
Events:
... Warning  FailedScheduling    No nodes are available      
                                 that match all of the       
                                 following predicates::      
                                 Insufficient cpu (1).       
The output shows that the pod hasn’t been scheduled because it can’t fit on any node
due to insufficient CPU on your single node. But why is that? The sum of the CPU
requests of all three pods equals 2,000 millicores or exactly two cores, which is exactly
what your node can provide. What’s wrong?
DETERMINING WHY A POD ISN’T BEING SCHEDULED
You can figure out why the pod isn’t being scheduled by inspecting the node resource.
Use the kubectl describe node command again and examine the output more
closely in the following listing.
$ kubectl describe node
Name:                   minikube
...
Non-terminated Pods:    (7 in total)
  Namespace    Name            CPU Requ.   CPU Lim.  Mem Req.    Mem Lim.
  ---------    ----            ----------  --------  ---------   --------
  default      requests-pod    200m (10%)  0 (0%)    10Mi (0%)   0 (0%)
Listing 14.4
Examining why a pod is stuck at Pending with kubectl describe pod
Listing 14.5
Inspecting allocated resources on a node with kubectl describe node
No node is 
associated 
with the pod.
The pod hasn’t 
been scheduled.
Scheduling has 
failed because of 
insufficient CPU.
 
</data>
  <data key="d5">409
Requesting resources for a pod’s containers
NOTE
This time you’re specifying the CPU request in whole cores (cpu=1)
instead of millicores (cpu=1000m).
So far, so good. The pod has been accepted by the API server (you’ll remember from
the previous chapter that the API server can reject pods if they’re invalid in any way).
Now, check if the pod is running:
$ kubectl get po requests-pod-3
NAME             READY     STATUS    RESTARTS   AGE
requests-pod-3   0/1       Pending   0          4m
Even if you wait a while, the pod is still stuck at Pending. You can see more informa-
tion on why that’s the case by using the kubectl describe command, as shown in
the following listing.
$ kubectl describe po requests-pod-3
Name:       requests-pod-3
Namespace:  default
Node:       /                    
...
Conditions:
  Type           Status
  PodScheduled   False           
...
Events:
... Warning  FailedScheduling    No nodes are available      
                                 that match all of the       
                                 following predicates::      
                                 Insufficient cpu (1).       
The output shows that the pod hasn’t been scheduled because it can’t fit on any node
due to insufficient CPU on your single node. But why is that? The sum of the CPU
requests of all three pods equals 2,000 millicores or exactly two cores, which is exactly
what your node can provide. What’s wrong?
DETERMINING WHY A POD ISN’T BEING SCHEDULED
You can figure out why the pod isn’t being scheduled by inspecting the node resource.
Use the kubectl describe node command again and examine the output more
closely in the following listing.
$ kubectl describe node
Name:                   minikube
...
Non-terminated Pods:    (7 in total)
  Namespace    Name            CPU Requ.   CPU Lim.  Mem Req.    Mem Lim.
  ---------    ----            ----------  --------  ---------   --------
  default      requests-pod    200m (10%)  0 (0%)    10Mi (0%)   0 (0%)
Listing 14.4
Examining why a pod is stuck at Pending with kubectl describe pod
Listing 14.5
Inspecting allocated resources on a node with kubectl describe node
No node is 
associated 
with the pod.
The pod hasn’t 
been scheduled.
Scheduling has 
failed because of 
insufficient CPU.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="442">
  <data key="d0">Page_442</data>
  <data key="d5">Page_442</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_353">
  <data key="d0">410
CHAPTER 14
Managing pods’ computational resources
  default      requests-pod-2  800m (40%)  0 (0%)    20Mi (1%)   0 (0%)
  kube-system  dflt-http-b...  10m (0%)    10m (0%)  20Mi (1%)   20Mi (1%)
  kube-system  kube-addon-...  5m (0%)     0 (0%)    50Mi (2%)   0 (0%)
  kube-system  kube-dns-26...  260m (13%)  0 (0%)    110Mi (5%)  170Mi (8%)
  kube-system  kubernetes-...  0 (0%)      0 (0%)    0 (0%)      0 (0%)
  kube-system  nginx-ingre...  0 (0%)      0 (0%)    0 (0%)      0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits      Memory Requests Memory Limits
  ------------  ----------      --------------- -------------
  1275m (63%)   10m (0%)        210Mi (11%)     190Mi (9%)
If you look at the bottom left of the listing, you’ll see a total of 1,275 millicores have
been requested by the running pods, which is 275 millicores more than what you
requested for the first two pods you deployed. Something is eating up additional
CPU resources. 
 You can find the culprit in the list of pods in the previous listing. Three pods in the
kube-system namespace have explicitly requested CPU resources. Those pods plus
your two pods leave only 725 millicores available for additional pods. Because your
third pod requested 1,000 millicores, the Scheduler won’t schedule it to this node, as
that would make the node overcommitted. 
FREEING RESOURCES TO GET THE POD SCHEDULED
The pod will only be scheduled when an adequate amount of CPU is freed (when one
of the first two pods is deleted, for example). If you delete your second pod, the
Scheduler will be notified of the deletion (through the watch mechanism described in
chapter 11) and will schedule your third pod as soon as the second pod terminates.
This is shown in the following listing.
$ kubectl delete po requests-pod-2
pod "requests-pod-2" deleted 
$ kubectl get po
NAME             READY     STATUS        RESTARTS   AGE
requests-pod     1/1       Running       0          2h
requests-pod-2   1/1       Terminating   0          1h
requests-pod-3   0/1       Pending       0          1h
$ kubectl get po
NAME             READY     STATUS    RESTARTS   AGE
requests-pod     1/1       Running   0          2h
requests-pod-3   1/1       Running   0          1h
In all these examples, you’ve specified a request for memory, but it hasn’t played any
role in the scheduling because your node has more than enough allocatable memory to
accommodate all your pods’ requests. Both CPU and memory requests are treated the
same way by the Scheduler, but in contrast to memory requests, a pod’s CPU requests
also play a role elsewhere—while the pod is running. You’ll learn about this next.
Listing 14.6
Pod is scheduled after deleting another pod
 
</data>
  <data key="d5">410
CHAPTER 14
Managing pods’ computational resources
  default      requests-pod-2  800m (40%)  0 (0%)    20Mi (1%)   0 (0%)
  kube-system  dflt-http-b...  10m (0%)    10m (0%)  20Mi (1%)   20Mi (1%)
  kube-system  kube-addon-...  5m (0%)     0 (0%)    50Mi (2%)   0 (0%)
  kube-system  kube-dns-26...  260m (13%)  0 (0%)    110Mi (5%)  170Mi (8%)
  kube-system  kubernetes-...  0 (0%)      0 (0%)    0 (0%)      0 (0%)
  kube-system  nginx-ingre...  0 (0%)      0 (0%)    0 (0%)      0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits      Memory Requests Memory Limits
  ------------  ----------      --------------- -------------
  1275m (63%)   10m (0%)        210Mi (11%)     190Mi (9%)
If you look at the bottom left of the listing, you’ll see a total of 1,275 millicores have
been requested by the running pods, which is 275 millicores more than what you
requested for the first two pods you deployed. Something is eating up additional
CPU resources. 
 You can find the culprit in the list of pods in the previous listing. Three pods in the
kube-system namespace have explicitly requested CPU resources. Those pods plus
your two pods leave only 725 millicores available for additional pods. Because your
third pod requested 1,000 millicores, the Scheduler won’t schedule it to this node, as
that would make the node overcommitted. 
FREEING RESOURCES TO GET THE POD SCHEDULED
The pod will only be scheduled when an adequate amount of CPU is freed (when one
of the first two pods is deleted, for example). If you delete your second pod, the
Scheduler will be notified of the deletion (through the watch mechanism described in
chapter 11) and will schedule your third pod as soon as the second pod terminates.
This is shown in the following listing.
$ kubectl delete po requests-pod-2
pod "requests-pod-2" deleted 
$ kubectl get po
NAME             READY     STATUS        RESTARTS   AGE
requests-pod     1/1       Running       0          2h
requests-pod-2   1/1       Terminating   0          1h
requests-pod-3   0/1       Pending       0          1h
$ kubectl get po
NAME             READY     STATUS    RESTARTS   AGE
requests-pod     1/1       Running   0          2h
requests-pod-3   1/1       Running   0          1h
In all these examples, you’ve specified a request for memory, but it hasn’t played any
role in the scheduling because your node has more than enough allocatable memory to
accommodate all your pods’ requests. Both CPU and memory requests are treated the
same way by the Scheduler, but in contrast to memory requests, a pod’s CPU requests
also play a role elsewhere—while the pod is running. You’ll learn about this next.
Listing 14.6
Pod is scheduled after deleting another pod
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="443">
  <data key="d0">Page_443</data>
  <data key="d5">Page_443</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_354">
  <data key="d0">411
Requesting resources for a pod’s containers
14.1.3 Understanding how CPU requests affect CPU time sharing
You now have two pods running in your cluster (you can disregard the system pods
right now, because they’re mostly idle). One has requested 200 millicores and the
other one five times as much. At the beginning of the chapter, we said Kubernetes dis-
tinguishes between resource requests and limits. You haven’t defined any limits yet, so
the two pods are in no way limited when it comes to how much CPU they can each
consume. If the process inside each pod consumes as much CPU time as it can, how
much CPU time does each pod get? 
 The CPU requests don’t only affect scheduling—they also determine how the
remaining (unused) CPU time is distributed between pods. Because your first pod
requested 200 millicores of CPU and the other one 1,000 millicores, any unused CPU
will be split among the two pods in a 1 to 5 ratio, as shown in figure 14.2. If both pods
consume as much CPU as they can, the first pod will get one sixth or 16.7% of the
CPU time and the other one the remaining five sixths or 83.3%.
But if one container wants to use up as much CPU as it can, while the other one is sit-
ting idle at a given moment, the first container will be allowed to use the whole CPU
time (minus the small amount of time used by the second container, if any). After all,
it makes sense to use all the available CPU if no one else is using it, right? As soon as
the second container needs CPU time, it will get it and the first container will be throt-
tled back.
14.1.4 Defining and requesting custom resources
Kubernetes also allows you to add your own custom resources to a node and request
them in the pod’s resource requests. Initially these were known as Opaque Integer
Resources, but were replaced with Extended Resources in Kubernetes version 1.8.
Pod A:
200 m
CPU
requests
Pod B: 1000 m
800 m available
CPU
usage
2000 m
1000 m
0 m
Pod A and B requests
are in 1:5 ratio.
Available CPU time is
distributed in same ratio.
Pod B: 1667 m
133 m
(1/6)
667 m
(5/6)
Pod A:
333 m
Figure 14.2
Unused CPU time is distributed to containers based on their CPU requests.
 
</data>
  <data key="d5">411
Requesting resources for a pod’s containers
14.1.3 Understanding how CPU requests affect CPU time sharing
You now have two pods running in your cluster (you can disregard the system pods
right now, because they’re mostly idle). One has requested 200 millicores and the
other one five times as much. At the beginning of the chapter, we said Kubernetes dis-
tinguishes between resource requests and limits. You haven’t defined any limits yet, so
the two pods are in no way limited when it comes to how much CPU they can each
consume. If the process inside each pod consumes as much CPU time as it can, how
much CPU time does each pod get? 
 The CPU requests don’t only affect scheduling—they also determine how the
remaining (unused) CPU time is distributed between pods. Because your first pod
requested 200 millicores of CPU and the other one 1,000 millicores, any unused CPU
will be split among the two pods in a 1 to 5 ratio, as shown in figure 14.2. If both pods
consume as much CPU as they can, the first pod will get one sixth or 16.7% of the
CPU time and the other one the remaining five sixths or 83.3%.
But if one container wants to use up as much CPU as it can, while the other one is sit-
ting idle at a given moment, the first container will be allowed to use the whole CPU
time (minus the small amount of time used by the second container, if any). After all,
it makes sense to use all the available CPU if no one else is using it, right? As soon as
the second container needs CPU time, it will get it and the first container will be throt-
tled back.
14.1.4 Defining and requesting custom resources
Kubernetes also allows you to add your own custom resources to a node and request
them in the pod’s resource requests. Initially these were known as Opaque Integer
Resources, but were replaced with Extended Resources in Kubernetes version 1.8.
Pod A:
200 m
CPU
requests
Pod B: 1000 m
800 m available
CPU
usage
2000 m
1000 m
0 m
Pod A and B requests
are in 1:5 ratio.
Available CPU time is
distributed in same ratio.
Pod B: 1667 m
133 m
(1/6)
667 m
(5/6)
Pod A:
333 m
Figure 14.2
Unused CPU time is distributed to containers based on their CPU requests.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="444">
  <data key="d0">Page_444</data>
  <data key="d5">Page_444</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_355">
  <data key="d0">412
CHAPTER 14
Managing pods’ computational resources
 First, you obviously need to make Kubernetes aware of your custom resource by
adding it to the Node object’s capacity field. This can be done by performing a
PATCH HTTP request. The resource name can be anything, such as example.org/my-
resource, as long as it doesn’t start with the kubernetes.io domain. The quantity
must be an integer (for example, you can’t set it to 100 millis, because 0.1 isn’t an inte-
ger; but you can set it to 1000m or 2000m or, simply, 1 or 2). The value will be copied
from the capacity to the allocatable field automatically.
 Then, when creating pods, you specify the same resource name and the requested
quantity under the resources.requests field in the container spec or with --requests
when using kubectl run like you did in previous examples. The Scheduler will make
sure the pod is only deployed to a node that has the requested amount of the custom
resource available. Every deployed pod obviously reduces the number of allocatable
units of the resource.
 An example of a custom resource could be the number of GPU units available on the
node. Pods requiring the use of a GPU specify that in their requests. The Scheduler then
makes sure the pod is only scheduled to nodes with at least one GPU still unallocated.
14.2
Limiting resources available to a container
Setting resource requests for containers in a pod ensures each container gets the min-
imum amount of resources it needs. Now let’s see the other side of the coin—the
maximum amount the container will be allowed to consume. 
14.2.1 Setting a hard limit for the amount of resources a container can use
We’ve seen how containers are allowed to use up all the CPU if all the other processes
are sitting idle. But you may want to prevent certain containers from using up more
than a specific amount of CPU. And you’ll always want to limit the amount of memory
a container can consume. 
 CPU is a compressible resource, which means the amount used by a container can
be throttled without affecting the process running in the container in an adverse way.
Memory is obviously different—it’s incompressible. Once a process is given a chunk of
memory, that memory can’t be taken away from it until it’s released by the process
itself. That’s why you need to limit the maximum amount of memory a container can
be given. 
 Without limiting memory, a container (or a pod) running on a worker node may
eat up all the available memory and affect all other pods on the node and any new
pods scheduled to the node (remember that new pods are scheduled to the node
based on the memory requests and not actual memory usage). A single malfunction-
ing or malicious pod can practically make the whole node unusable.
CREATING A POD WITH RESOURCE LIMITS
To prevent this from happening, Kubernetes allows you to specify resource limits for
every container (along with, and virtually in the same way as, resource requests). The
following listing shows an example pod manifest with resource limits.
 
</data>
  <data key="d5">412
CHAPTER 14
Managing pods’ computational resources
 First, you obviously need to make Kubernetes aware of your custom resource by
adding it to the Node object’s capacity field. This can be done by performing a
PATCH HTTP request. The resource name can be anything, such as example.org/my-
resource, as long as it doesn’t start with the kubernetes.io domain. The quantity
must be an integer (for example, you can’t set it to 100 millis, because 0.1 isn’t an inte-
ger; but you can set it to 1000m or 2000m or, simply, 1 or 2). The value will be copied
from the capacity to the allocatable field automatically.
 Then, when creating pods, you specify the same resource name and the requested
quantity under the resources.requests field in the container spec or with --requests
when using kubectl run like you did in previous examples. The Scheduler will make
sure the pod is only deployed to a node that has the requested amount of the custom
resource available. Every deployed pod obviously reduces the number of allocatable
units of the resource.
 An example of a custom resource could be the number of GPU units available on the
node. Pods requiring the use of a GPU specify that in their requests. The Scheduler then
makes sure the pod is only scheduled to nodes with at least one GPU still unallocated.
14.2
Limiting resources available to a container
Setting resource requests for containers in a pod ensures each container gets the min-
imum amount of resources it needs. Now let’s see the other side of the coin—the
maximum amount the container will be allowed to consume. 
14.2.1 Setting a hard limit for the amount of resources a container can use
We’ve seen how containers are allowed to use up all the CPU if all the other processes
are sitting idle. But you may want to prevent certain containers from using up more
than a specific amount of CPU. And you’ll always want to limit the amount of memory
a container can consume. 
 CPU is a compressible resource, which means the amount used by a container can
be throttled without affecting the process running in the container in an adverse way.
Memory is obviously different—it’s incompressible. Once a process is given a chunk of
memory, that memory can’t be taken away from it until it’s released by the process
itself. That’s why you need to limit the maximum amount of memory a container can
be given. 
 Without limiting memory, a container (or a pod) running on a worker node may
eat up all the available memory and affect all other pods on the node and any new
pods scheduled to the node (remember that new pods are scheduled to the node
based on the memory requests and not actual memory usage). A single malfunction-
ing or malicious pod can practically make the whole node unusable.
CREATING A POD WITH RESOURCE LIMITS
To prevent this from happening, Kubernetes allows you to specify resource limits for
every container (along with, and virtually in the same way as, resource requests). The
following listing shows an example pod manifest with resource limits.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="445">
  <data key="d0">Page_445</data>
  <data key="d5">Page_445</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_356">
  <data key="d0">413
Limiting resources available to a container
apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:            
      limits:             
        cpu: 1             
        memory: 20Mi       
This pod’s container has resource limits configured for both CPU and memory. The
process or processes running inside the container will not be allowed to consume
more than 1 CPU core and 20 mebibytes of memory. 
NOTE
Because you haven’t specified any resource requests, they’ll be set to
the same values as the resource limits.
OVERCOMMITTING LIMITS
Unlike resource requests, resource limits aren’t constrained by the node’s allocatable
resource amounts. The sum of all limits of all the pods on a node is allowed to exceed
100% of the node’s capacity (figure 14.3). Restated, resource limits can be overcom-
mitted. This has an important consequence—when 100% of the node’s resources are
used up, certain containers will need to be killed.
You’ll see how Kubernetes decides which containers to kill in section 14.3, but individ-
ual containers can be killed even if they try to use more than their resource limits
specify. You’ll learn more about this next.
Listing 14.7
A pod with a hard limit on CPU and memory: limited-pod.yaml
Specifying resource 
limits for the container
This container will be 
allowed to use at 
most 1 CPU core.
The container will be
allowed to use up to 20
mebibytes of memory.
Node
0%
136%
100%
Pod A
Memory requests
Pod B
Pod C
Pod A
Memory limits
Pod B
Unallocated
Pod C
Figure 14.3
The sum of resource limits of all pods on a node can exceed 100% of the node’s 
capacity.
 
</data>
  <data key="d5">413
Limiting resources available to a container
apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:            
      limits:             
        cpu: 1             
        memory: 20Mi       
This pod’s container has resource limits configured for both CPU and memory. The
process or processes running inside the container will not be allowed to consume
more than 1 CPU core and 20 mebibytes of memory. 
NOTE
Because you haven’t specified any resource requests, they’ll be set to
the same values as the resource limits.
OVERCOMMITTING LIMITS
Unlike resource requests, resource limits aren’t constrained by the node’s allocatable
resource amounts. The sum of all limits of all the pods on a node is allowed to exceed
100% of the node’s capacity (figure 14.3). Restated, resource limits can be overcom-
mitted. This has an important consequence—when 100% of the node’s resources are
used up, certain containers will need to be killed.
You’ll see how Kubernetes decides which containers to kill in section 14.3, but individ-
ual containers can be killed even if they try to use more than their resource limits
specify. You’ll learn more about this next.
Listing 14.7
A pod with a hard limit on CPU and memory: limited-pod.yaml
Specifying resource 
limits for the container
This container will be 
allowed to use at 
most 1 CPU core.
The container will be
allowed to use up to 20
mebibytes of memory.
Node
0%
136%
100%
Pod A
Memory requests
Pod B
Pod C
Pod A
Memory limits
Pod B
Unallocated
Pod C
Figure 14.3
The sum of resource limits of all pods on a node can exceed 100% of the node’s 
capacity.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="446">
  <data key="d0">Page_446</data>
  <data key="d5">Page_446</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_357">
  <data key="d0">414
CHAPTER 14
Managing pods’ computational resources
14.2.2 Exceeding the limits
What happens when a process running in a container tries to use a greater amount of
resources than it’s allowed to? 
 You’ve already learned that CPU is a compressible resource, and it’s only natural
for a process to want to consume all of the CPU time when not waiting for an I/O
operation. As you’ve learned, a process’ CPU usage is throttled, so when a CPU
limit is set for a container, the process isn’t given more CPU time than the config-
ured limit. 
 With memory, it’s different. When a process tries to allocate memory over its
limit, the process is killed (it’s said the container is OOMKilled, where OOM stands
for Out Of Memory). If the pod’s restart policy is set to Always or OnFailure, the
process is restarted immediately, so you may not even notice it getting killed. But if it
keeps going over the memory limit and getting killed, Kubernetes will begin restart-
ing it with increasing delays between restarts. You’ll see a CrashLoopBackOff status
in that case:
$ kubectl get po
NAME        READY     STATUS             RESTARTS   AGE
memoryhog   0/1       CrashLoopBackOff   3          1m
The CrashLoopBackOff status doesn’t mean the Kubelet has given up. It means that
after each crash, the Kubelet is increasing the time period before restarting the con-
tainer. After the first crash, it restarts the container immediately and then, if it crashes
again, waits for 10 seconds before restarting it again. On subsequent crashes, this
delay is then increased exponentially to 20, 40, 80, and 160 seconds, and finally lim-
ited to 300 seconds. Once the interval hits the 300-second limit, the Kubelet keeps
restarting the container indefinitely every five minutes until the pod either stops
crashing or is deleted. 
 To examine why the container crashed, you can check the pod’s log and/or use
the kubectl describe pod command, as shown in the following listing.
$ kubectl describe pod
Name:       memoryhog
...
Containers:
  main:
    ...
    State:          Terminated          
      Reason:       OOMKilled           
      Exit Code:    137
      Started:      Tue, 27 Dec 2016 14:55:53 +0100
      Finished:     Tue, 27 Dec 2016 14:55:58 +0100
    Last State:     Terminated            
      Reason:       OOMKilled             
      Exit Code:    137
Listing 14.8
Inspecting why a container terminated with kubectl describe pod
The current container was 
killed because it was out 
of memory (OOM).
The previous container 
was also killed because 
it was  OOM
 
</data>
  <data key="d5">414
CHAPTER 14
Managing pods’ computational resources
14.2.2 Exceeding the limits
What happens when a process running in a container tries to use a greater amount of
resources than it’s allowed to? 
 You’ve already learned that CPU is a compressible resource, and it’s only natural
for a process to want to consume all of the CPU time when not waiting for an I/O
operation. As you’ve learned, a process’ CPU usage is throttled, so when a CPU
limit is set for a container, the process isn’t given more CPU time than the config-
ured limit. 
 With memory, it’s different. When a process tries to allocate memory over its
limit, the process is killed (it’s said the container is OOMKilled, where OOM stands
for Out Of Memory). If the pod’s restart policy is set to Always or OnFailure, the
process is restarted immediately, so you may not even notice it getting killed. But if it
keeps going over the memory limit and getting killed, Kubernetes will begin restart-
ing it with increasing delays between restarts. You’ll see a CrashLoopBackOff status
in that case:
$ kubectl get po
NAME        READY     STATUS             RESTARTS   AGE
memoryhog   0/1       CrashLoopBackOff   3          1m
The CrashLoopBackOff status doesn’t mean the Kubelet has given up. It means that
after each crash, the Kubelet is increasing the time period before restarting the con-
tainer. After the first crash, it restarts the container immediately and then, if it crashes
again, waits for 10 seconds before restarting it again. On subsequent crashes, this
delay is then increased exponentially to 20, 40, 80, and 160 seconds, and finally lim-
ited to 300 seconds. Once the interval hits the 300-second limit, the Kubelet keeps
restarting the container indefinitely every five minutes until the pod either stops
crashing or is deleted. 
 To examine why the container crashed, you can check the pod’s log and/or use
the kubectl describe pod command, as shown in the following listing.
$ kubectl describe pod
Name:       memoryhog
...
Containers:
  main:
    ...
    State:          Terminated          
      Reason:       OOMKilled           
      Exit Code:    137
      Started:      Tue, 27 Dec 2016 14:55:53 +0100
      Finished:     Tue, 27 Dec 2016 14:55:58 +0100
    Last State:     Terminated            
      Reason:       OOMKilled             
      Exit Code:    137
Listing 14.8
Inspecting why a container terminated with kubectl describe pod
The current container was 
killed because it was out 
of memory (OOM).
The previous container 
was also killed because 
it was  OOM
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="447">
  <data key="d0">Page_447</data>
  <data key="d5">Page_447</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_358">
  <data key="d0">415
Limiting resources available to a container
      Started:      Tue, 27 Dec 2016 14:55:37 +0100
      Finished:     Tue, 27 Dec 2016 14:55:50 +0100
    Ready:          False
...
The OOMKilled status tells you that the container was killed because it was out of mem-
ory. In the previous listing, the container went over its memory limit and was killed
immediately. 
 It’s important not to set memory limits too low if you don’t want your container to
be killed. But containers can get OOMKilled even if they aren’t over their limit. You’ll
see why in section 14.3.2, but first, let’s discuss something that catches most users off-
guard the first time they start specifying limits for their containers.
14.2.3 Understanding how apps in containers see limits
If you haven’t deployed the pod from listing 14.7, deploy it now:
$ kubectl create -f limited-pod.yaml
pod "limited-pod" created
Now, run the top command in the container, the way you did at the beginning of the
chapter. The command’s output is shown in the following listing.
$ kubectl exec -it limited-pod top
Mem: 1450980K used, 597504K free, 22012K shrd, 65876K buff, 857552K cached
CPU: 10.0% usr 40.0% sys  0.0% nic 50.0% idle  0.0% io  0.0% irq  0.0% sirq
Load average: 0.17 1.19 2.47 4/503 10
  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND
    1     0 root     R     1192  0.0   1 49.9 dd if /dev/zero of /dev/null
    5     0 root     R     1196  0.0   0  0.0 top
First, let me remind you that the pod’s CPU limit is set to 1 core and its memory limit
is set to 20 MiB. Now, examine the output of the top command closely. Is there any-
thing that strikes you as odd?
 Look at the amount of used and free memory. Those numbers are nowhere near
the 20 MiB you set as the limit for the container. Similarly, you set the CPU limit to
one core and it seems like the main process is using only 50% of the available CPU
time, even though the dd command, when used like you’re using it, usually uses all the
CPU it has available. What’s going on?
UNDERSTANDING THAT CONTAINERS ALWAYS SEE THE NODE’S MEMORY, NOT THE CONTAINER’S
The top command shows the memory amounts of the whole node the container is
running on. Even though you set a limit on how much memory is available to a con-
tainer, the container will not be aware of this limit. 
Listing 14.9
Running the top command in a CPU- and memory-limited container
 
</data>
  <data key="d5">415
Limiting resources available to a container
      Started:      Tue, 27 Dec 2016 14:55:37 +0100
      Finished:     Tue, 27 Dec 2016 14:55:50 +0100
    Ready:          False
...
The OOMKilled status tells you that the container was killed because it was out of mem-
ory. In the previous listing, the container went over its memory limit and was killed
immediately. 
 It’s important not to set memory limits too low if you don’t want your container to
be killed. But containers can get OOMKilled even if they aren’t over their limit. You’ll
see why in section 14.3.2, but first, let’s discuss something that catches most users off-
guard the first time they start specifying limits for their containers.
14.2.3 Understanding how apps in containers see limits
If you haven’t deployed the pod from listing 14.7, deploy it now:
$ kubectl create -f limited-pod.yaml
pod "limited-pod" created
Now, run the top command in the container, the way you did at the beginning of the
chapter. The command’s output is shown in the following listing.
$ kubectl exec -it limited-pod top
Mem: 1450980K used, 597504K free, 22012K shrd, 65876K buff, 857552K cached
CPU: 10.0% usr 40.0% sys  0.0% nic 50.0% idle  0.0% io  0.0% irq  0.0% sirq
Load average: 0.17 1.19 2.47 4/503 10
  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND
    1     0 root     R     1192  0.0   1 49.9 dd if /dev/zero of /dev/null
    5     0 root     R     1196  0.0   0  0.0 top
First, let me remind you that the pod’s CPU limit is set to 1 core and its memory limit
is set to 20 MiB. Now, examine the output of the top command closely. Is there any-
thing that strikes you as odd?
 Look at the amount of used and free memory. Those numbers are nowhere near
the 20 MiB you set as the limit for the container. Similarly, you set the CPU limit to
one core and it seems like the main process is using only 50% of the available CPU
time, even though the dd command, when used like you’re using it, usually uses all the
CPU it has available. What’s going on?
UNDERSTANDING THAT CONTAINERS ALWAYS SEE THE NODE’S MEMORY, NOT THE CONTAINER’S
The top command shows the memory amounts of the whole node the container is
running on. Even though you set a limit on how much memory is available to a con-
tainer, the container will not be aware of this limit. 
Listing 14.9
Running the top command in a CPU- and memory-limited container
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="448">
  <data key="d0">Page_448</data>
  <data key="d5">Page_448</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_359">
  <data key="d0">416
CHAPTER 14
Managing pods’ computational resources
 This has an unfortunate effect on any application that looks up the amount of
memory available on the system and uses that information to decide how much mem-
ory it wants to reserve. 
 The problem is visible when running Java apps, especially if you don’t specify the
maximum heap size for the Java Virtual Machine with the -Xmx option. In that case,
the JVM will set the maximum heap size based on the host’s total memory instead of
the memory available to the container. When you run your containerized Java apps in
a Kubernetes cluster on your laptop, the problem doesn’t manifest itself, because the
difference between the memory limits you set for the pod and the total memory avail-
able on your laptop is not that great. 
 But when you deploy your pod onto a production system, where nodes have much
more physical memory, the JVM may go over the container’s memory limit you config-
ured and will be OOMKilled. 
 And if you think setting the -Xmx option properly solves the issue, you’re wrong,
unfortunately. The -Xmx option only constrains the heap size, but does nothing about
the JVM’s off-heap memory. Luckily, new versions of Java alleviate that problem by tak-
ing the configured container limits into account.
UNDERSTANDING THAT CONTAINERS ALSO SEE ALL THE NODE’S CPU CORES
Exactly like with memory, containers will also see all the node’s CPUs, regardless of
the CPU limits configured for the container. Setting a CPU limit to one core doesn’t
magically only expose only one CPU core to the container. All the CPU limit does is
constrain the amount of CPU time the container can use. 
 A container with a one-core CPU limit running on a 64-core CPU will get 1/64th
of the overall CPU time. And even though its limit is set to one core, the container’s
processes will not run on only one core. At different points in time, its code may be
executed on different cores.
 Nothing is wrong with this, right? While that’s generally the case, at least one sce-
nario exists where this situation is catastrophic.
 Certain applications look up the number of CPUs on the system to decide how
many worker threads they should run. Again, such an app will run fine on a develop-
ment laptop, but when deployed on a node with a much bigger number of cores, it’s
going to spin up too many threads, all competing for the (possibly) limited CPU time.
Also, each thread requires additional memory, causing the apps memory usage to sky-
rocket. 
 You may want to use the Downward API to pass the CPU limit to the container and
use it instead of relying on the number of CPUs your app can see on the system. You
can also tap into the cgroups system directly to get the configured CPU limit by read-
ing the following files:
/sys/fs/cgroup/cpu/cpu.cfs_quota_us
/sys/fs/cgroup/cpu/cpu.cfs_period_us
 
</data>
  <data key="d5">416
CHAPTER 14
Managing pods’ computational resources
 This has an unfortunate effect on any application that looks up the amount of
memory available on the system and uses that information to decide how much mem-
ory it wants to reserve. 
 The problem is visible when running Java apps, especially if you don’t specify the
maximum heap size for the Java Virtual Machine with the -Xmx option. In that case,
the JVM will set the maximum heap size based on the host’s total memory instead of
the memory available to the container. When you run your containerized Java apps in
a Kubernetes cluster on your laptop, the problem doesn’t manifest itself, because the
difference between the memory limits you set for the pod and the total memory avail-
able on your laptop is not that great. 
 But when you deploy your pod onto a production system, where nodes have much
more physical memory, the JVM may go over the container’s memory limit you config-
ured and will be OOMKilled. 
 And if you think setting the -Xmx option properly solves the issue, you’re wrong,
unfortunately. The -Xmx option only constrains the heap size, but does nothing about
the JVM’s off-heap memory. Luckily, new versions of Java alleviate that problem by tak-
ing the configured container limits into account.
UNDERSTANDING THAT CONTAINERS ALSO SEE ALL THE NODE’S CPU CORES
Exactly like with memory, containers will also see all the node’s CPUs, regardless of
the CPU limits configured for the container. Setting a CPU limit to one core doesn’t
magically only expose only one CPU core to the container. All the CPU limit does is
constrain the amount of CPU time the container can use. 
 A container with a one-core CPU limit running on a 64-core CPU will get 1/64th
of the overall CPU time. And even though its limit is set to one core, the container’s
processes will not run on only one core. At different points in time, its code may be
executed on different cores.
 Nothing is wrong with this, right? While that’s generally the case, at least one sce-
nario exists where this situation is catastrophic.
 Certain applications look up the number of CPUs on the system to decide how
many worker threads they should run. Again, such an app will run fine on a develop-
ment laptop, but when deployed on a node with a much bigger number of cores, it’s
going to spin up too many threads, all competing for the (possibly) limited CPU time.
Also, each thread requires additional memory, causing the apps memory usage to sky-
rocket. 
 You may want to use the Downward API to pass the CPU limit to the container and
use it instead of relying on the number of CPUs your app can see on the system. You
can also tap into the cgroups system directly to get the configured CPU limit by read-
ing the following files:
/sys/fs/cgroup/cpu/cpu.cfs_quota_us
/sys/fs/cgroup/cpu/cpu.cfs_period_us
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="449">
  <data key="d0">Page_449</data>
  <data key="d5">Page_449</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_360">
  <data key="d0">417
Understanding pod QoS classes
14.3
Understanding pod QoS classes
We’ve already mentioned that resource limits can be overcommitted and that a
node can’t necessarily provide all its pods the amount of resources specified in their
resource limits. 
 Imagine having two pods, where pod A is using, let’s say, 90% of the node’s mem-
ory and then pod B suddenly requires more memory than what it had been using up
to that point and the node can’t provide the required amount of memory. Which
container should be killed? Should it be pod B, because its request for memory can’t
be satisfied, or should pod A be killed to free up memory, so it can be provided to
pod B? 
 Obviously, it depends. Kubernetes can’t make a proper decision on its own. You
need a way to specify which pods have priority in such cases. Kubernetes does this by
categorizing pods into three Quality of Service (QoS) classes:

BestEffort (the lowest priority)

Burstable

Guaranteed (the highest)
14.3.1 Defining the QoS class for a pod
You might expect these classes to be assignable to pods through a separate field in the
manifest, but they aren’t. The QoS class is derived from the combination of resource
requests and limits for the pod’s containers. Here’s how.
ASSIGNING A POD TO THE BESTEFFORT CLASS
The lowest priority QoS class is the BestEffort class. It’s assigned to pods that don’t
have any requests or limits set at all (in any of their containers). This is the QoS class
that has been assigned to all the pods you created in previous chapters. Containers
running in these pods have had no resource guarantees whatsoever. In the worst
case, they may get almost no CPU time at all and will be the first ones killed when
memory needs to be freed for other pods. But because a BestEffort pod has no
memory limits set, its containers may use as much memory as they want, if enough
memory is available.
ASSIGNING A POD TO THE GUARANTEED CLASS
On the other end of the spectrum is the Guaranteed QoS class. This class is given to
pods whose containers’ requests are equal to the limits for all resources. For a pod’s
class to be Guaranteed, three things need to be true:
Requests and limits need to be set for both CPU and memory.
They need to be set for each container.
They need to be equal (the limit needs to match the request for each resource
in each container).
Because a container’s resource requests, if not set explicitly, default to the limits,
specifying the limits for all resources (for each container in the pod) is enough for
 
</data>
  <data key="d5">417
Understanding pod QoS classes
14.3
Understanding pod QoS classes
We’ve already mentioned that resource limits can be overcommitted and that a
node can’t necessarily provide all its pods the amount of resources specified in their
resource limits. 
 Imagine having two pods, where pod A is using, let’s say, 90% of the node’s mem-
ory and then pod B suddenly requires more memory than what it had been using up
to that point and the node can’t provide the required amount of memory. Which
container should be killed? Should it be pod B, because its request for memory can’t
be satisfied, or should pod A be killed to free up memory, so it can be provided to
pod B? 
 Obviously, it depends. Kubernetes can’t make a proper decision on its own. You
need a way to specify which pods have priority in such cases. Kubernetes does this by
categorizing pods into three Quality of Service (QoS) classes:

BestEffort (the lowest priority)

Burstable

Guaranteed (the highest)
14.3.1 Defining the QoS class for a pod
You might expect these classes to be assignable to pods through a separate field in the
manifest, but they aren’t. The QoS class is derived from the combination of resource
requests and limits for the pod’s containers. Here’s how.
ASSIGNING A POD TO THE BESTEFFORT CLASS
The lowest priority QoS class is the BestEffort class. It’s assigned to pods that don’t
have any requests or limits set at all (in any of their containers). This is the QoS class
that has been assigned to all the pods you created in previous chapters. Containers
running in these pods have had no resource guarantees whatsoever. In the worst
case, they may get almost no CPU time at all and will be the first ones killed when
memory needs to be freed for other pods. But because a BestEffort pod has no
memory limits set, its containers may use as much memory as they want, if enough
memory is available.
ASSIGNING A POD TO THE GUARANTEED CLASS
On the other end of the spectrum is the Guaranteed QoS class. This class is given to
pods whose containers’ requests are equal to the limits for all resources. For a pod’s
class to be Guaranteed, three things need to be true:
Requests and limits need to be set for both CPU and memory.
They need to be set for each container.
They need to be equal (the limit needs to match the request for each resource
in each container).
Because a container’s resource requests, if not set explicitly, default to the limits,
specifying the limits for all resources (for each container in the pod) is enough for
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="450">
  <data key="d0">Page_450</data>
  <data key="d5">Page_450</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_361">
  <data key="d0">418
CHAPTER 14
Managing pods’ computational resources
the pod to be Guaranteed. Containers in those pods get the requested amount of
resources, but cannot consume additional ones (because their limits are no higher
than their requests). 
ASSIGNING THE BURSTABLE QOS CLASS TO A POD
In between BestEffort and Guaranteed is the Burstable QoS class. All other pods
fall into this class. This includes single-container pods where the container’s limits
don’t match its requests and all pods where at least one container has a resource
request specified, but not the limit. It also includes pods where one container’s
requests match their limits, but another container has no requests or limits specified.
Burstable pods get the amount of resources they request, but are allowed to use addi-
tional resources (up to the limit) if needed.
UNDERSTANDING HOW THE RELATIONSHIP BETWEEN REQUESTS AND LIMITS DEFINES THE QOS CLASS
All three QoS classes and their relationships with requests and limits are shown in fig-
ure 14.4.
Thinking about what QoS class a pod has can make your head spin, because it involves
multiple containers, multiple resources, and all the possible relationships between
requests and limits. It’s easier if you start by thinking about QoS at the container level
(although QoS classes are a property of pods, not containers) and then derive the
pod’s QoS class from the QoS classes of containers. 
FIGURING OUT A CONTAINER’S QOS CLASS
Table 14.1 shows the QoS class based on how resource requests and limits are
defined on a single container. For single-container pods, the QoS class applies to
the pod as well.
 
BestEffort
QoS
Requests
Limits
Burstable
QoS
Requests
Limits
Guaranteed
QoS
Requests
Limits
Requests and
limits are not set
Requests are
below limits
Requests
equal limits
Figure 14.4
Resource requests, limits and QoS classes
 
</data>
  <data key="d5">418
CHAPTER 14
Managing pods’ computational resources
the pod to be Guaranteed. Containers in those pods get the requested amount of
resources, but cannot consume additional ones (because their limits are no higher
than their requests). 
ASSIGNING THE BURSTABLE QOS CLASS TO A POD
In between BestEffort and Guaranteed is the Burstable QoS class. All other pods
fall into this class. This includes single-container pods where the container’s limits
don’t match its requests and all pods where at least one container has a resource
request specified, but not the limit. It also includes pods where one container’s
requests match their limits, but another container has no requests or limits specified.
Burstable pods get the amount of resources they request, but are allowed to use addi-
tional resources (up to the limit) if needed.
UNDERSTANDING HOW THE RELATIONSHIP BETWEEN REQUESTS AND LIMITS DEFINES THE QOS CLASS
All three QoS classes and their relationships with requests and limits are shown in fig-
ure 14.4.
Thinking about what QoS class a pod has can make your head spin, because it involves
multiple containers, multiple resources, and all the possible relationships between
requests and limits. It’s easier if you start by thinking about QoS at the container level
(although QoS classes are a property of pods, not containers) and then derive the
pod’s QoS class from the QoS classes of containers. 
FIGURING OUT A CONTAINER’S QOS CLASS
Table 14.1 shows the QoS class based on how resource requests and limits are
defined on a single container. For single-container pods, the QoS class applies to
the pod as well.
 
BestEffort
QoS
Requests
Limits
Burstable
QoS
Requests
Limits
Guaranteed
QoS
Requests
Limits
Requests and
limits are not set
Requests are
below limits
Requests
equal limits
Figure 14.4
Resource requests, limits and QoS classes
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="451">
  <data key="d0">Page_451</data>
  <data key="d5">Page_451</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_362">
  <data key="d0">419
Understanding pod QoS classes
NOTE
If only requests are set, but not limits, refer to the table rows where
requests are less than the limits. If only limits are set, requests default to the
limits, so refer to the rows where requests equal limits.
FIGURING OUT THE QOS CLASS OF A POD WITH MULTIPLE CONTAINERS
For multi-container pods, if all the containers have the same QoS class, that’s also the
pod’s QoS class. If at least one container has a different class, the pod’s QoS class is
Burstable, regardless of what the container classes are. Table 14.2 shows how a two-
container pod’s QoS class relates to the classes of its two containers. You can easily
extend this to pods with more than two containers.
NOTE
A pod’s QoS class is shown when running kubectl describe pod and
in the pod’s YAML/JSON manifest in the status.qosClass field.
We’ve explained how QoS classes are determined, but we still need to look at how they
determine which container gets killed in an overcommitted system.
Table 14.1
The QoS class of a single-container pod based on resource requests and limits
CPU requests vs. limits
Memory requests vs. limits
Container QoS class
None set
None set
BestEffort
None set
Requests &lt; Limits
Burstable
None set
Requests = Limits
Burstable
Requests &lt; Limits
None set
Burstable
Requests &lt; Limits
Requests &lt; Limits
Burstable
Requests &lt; Limits
Requests = Limits
Burstable
Requests = Limits
Requests = Limits
Guaranteed
Table 14.2
A Pod’s QoS class derived from the classes of its containers
Container 1 QoS class
Container 2 QoS class
Pod’s QoS class
BestEffort
BestEffort
BestEffort
BestEffort
Burstable
Burstable
BestEffort
Guaranteed
Burstable
Burstable
Burstable
Burstable
Burstable
Guaranteed
Burstable
Guaranteed
Guaranteed
Guaranteed
 
</data>
  <data key="d5">419
Understanding pod QoS classes
NOTE
If only requests are set, but not limits, refer to the table rows where
requests are less than the limits. If only limits are set, requests default to the
limits, so refer to the rows where requests equal limits.
FIGURING OUT THE QOS CLASS OF A POD WITH MULTIPLE CONTAINERS
For multi-container pods, if all the containers have the same QoS class, that’s also the
pod’s QoS class. If at least one container has a different class, the pod’s QoS class is
Burstable, regardless of what the container classes are. Table 14.2 shows how a two-
container pod’s QoS class relates to the classes of its two containers. You can easily
extend this to pods with more than two containers.
NOTE
A pod’s QoS class is shown when running kubectl describe pod and
in the pod’s YAML/JSON manifest in the status.qosClass field.
We’ve explained how QoS classes are determined, but we still need to look at how they
determine which container gets killed in an overcommitted system.
Table 14.1
The QoS class of a single-container pod based on resource requests and limits
CPU requests vs. limits
Memory requests vs. limits
Container QoS class
None set
None set
BestEffort
None set
Requests &lt; Limits
Burstable
None set
Requests = Limits
Burstable
Requests &lt; Limits
None set
Burstable
Requests &lt; Limits
Requests &lt; Limits
Burstable
Requests &lt; Limits
Requests = Limits
Burstable
Requests = Limits
Requests = Limits
Guaranteed
Table 14.2
A Pod’s QoS class derived from the classes of its containers
Container 1 QoS class
Container 2 QoS class
Pod’s QoS class
BestEffort
BestEffort
BestEffort
BestEffort
Burstable
Burstable
BestEffort
Guaranteed
Burstable
Burstable
Burstable
Burstable
Burstable
Guaranteed
Burstable
Guaranteed
Guaranteed
Guaranteed
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="452">
  <data key="d0">Page_452</data>
  <data key="d5">Page_452</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_363">
  <data key="d0">420
CHAPTER 14
Managing pods’ computational resources
14.3.2 Understanding which process gets killed when memory is low
When the system is overcommitted, the QoS classes determine which container gets
killed first so the freed resources can be given to higher priority pods. First in line to
get killed are pods in the BestEffort class, followed by Burstable pods, and finally
Guaranteed pods, which only get killed if system processes need memory.
UNDERSTANDING HOW QOS CLASSES LINE UP
Let’s look at the example shown in figure 14.5. Imagine having two single-container
pods, where the first one has the BestEffort QoS class, and the second one’s is
Burstable. When the node’s whole memory is already maxed out and one of the pro-
cesses on the node tries to allocate more memory, the system will need to kill one of
the processes (perhaps even the process trying to allocate additional memory) to
honor the allocation request. In this case, the process running in the BestEffort pod
will always be killed before the one in the Burstable pod.
Obviously, a BestEffort pod’s process will also be killed before any Guaranteed pods’
processes are killed. Likewise, a Burstable pod’s process will also be killed before that
of a Guaranteed pod. But what happens if there are only two Burstable pods? Clearly,
the selection process needs to prefer one over the other.
UNDERSTANDING HOW CONTAINERS WITH THE SAME QOS CLASS ARE HANDLED
Each running process has an OutOfMemory (OOM) score. The system selects the
process to kill by comparing OOM scores of all the running processes. When memory
needs to be freed, the process with the highest score gets killed.
 OOM scores are calculated from two things: the percentage of the available mem-
ory the process is consuming and a fixed OOM score adjustment, which is based on the
pod’s QoS class and the container’s requested memory. When two single-container pods
exist, both in the Burstable class, the system will kill the one using more of its requested
BestEffort
QoS pod
Pod A
First in line
to be killed
Actual usage
Requests
Limits
Burstable
QoS pod
Pod B
Second in line
to be killed
90% used
Requests
Limits
Burstable
QoS pod
Pod C
Third in line
to be killed
70% used
Requests
Limits
Guaranteed
QoS pod
Pod D
Last to
be killed
99% used
Requests
Limits
Figure 14.5
Which pods get killed first
 
</data>
  <data key="d5">420
CHAPTER 14
Managing pods’ computational resources
14.3.2 Understanding which process gets killed when memory is low
When the system is overcommitted, the QoS classes determine which container gets
killed first so the freed resources can be given to higher priority pods. First in line to
get killed are pods in the BestEffort class, followed by Burstable pods, and finally
Guaranteed pods, which only get killed if system processes need memory.
UNDERSTANDING HOW QOS CLASSES LINE UP
Let’s look at the example shown in figure 14.5. Imagine having two single-container
pods, where the first one has the BestEffort QoS class, and the second one’s is
Burstable. When the node’s whole memory is already maxed out and one of the pro-
cesses on the node tries to allocate more memory, the system will need to kill one of
the processes (perhaps even the process trying to allocate additional memory) to
honor the allocation request. In this case, the process running in the BestEffort pod
will always be killed before the one in the Burstable pod.
Obviously, a BestEffort pod’s process will also be killed before any Guaranteed pods’
processes are killed. Likewise, a Burstable pod’s process will also be killed before that
of a Guaranteed pod. But what happens if there are only two Burstable pods? Clearly,
the selection process needs to prefer one over the other.
UNDERSTANDING HOW CONTAINERS WITH THE SAME QOS CLASS ARE HANDLED
Each running process has an OutOfMemory (OOM) score. The system selects the
process to kill by comparing OOM scores of all the running processes. When memory
needs to be freed, the process with the highest score gets killed.
 OOM scores are calculated from two things: the percentage of the available mem-
ory the process is consuming and a fixed OOM score adjustment, which is based on the
pod’s QoS class and the container’s requested memory. When two single-container pods
exist, both in the Burstable class, the system will kill the one using more of its requested
BestEffort
QoS pod
Pod A
First in line
to be killed
Actual usage
Requests
Limits
Burstable
QoS pod
Pod B
Second in line
to be killed
90% used
Requests
Limits
Burstable
QoS pod
Pod C
Third in line
to be killed
70% used
Requests
Limits
Guaranteed
QoS pod
Pod D
Last to
be killed
99% used
Requests
Limits
Figure 14.5
Which pods get killed first
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="453">
  <data key="d0">Page_453</data>
  <data key="d5">Page_453</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_364">
  <data key="d0">421
Setting default requests and limits for pods per namespace
memory than the other, percentage-wise. That’s why in figure 14.5, pod B, using 90%
of its requested memory, gets killed before pod C, which is only using 70%, even
though it’s using more megabytes of memory than pod B. 
 This shows you need to be mindful of not only the relationship between requests
and limits, but also of requests and the expected actual memory consumption. 
14.4
Setting default requests and limits for pods per 
namespace
We’ve looked at how resource requests and limits can be set for each individual con-
tainer. If you don’t set them, the container is at the mercy of all other containers that
do specify resource requests and limits. It’s a good idea to set requests and limits on
every container.
14.4.1 Introducing the LimitRange resource
Instead of having to do this for every container, you can also do it by creating a Limit-
Range resource. It allows you to specify (for each namespace) not only the minimum
and maximum limit you can set on a container for each resource, but also the default
resource requests for containers that don’t specify requests explicitly, as depicted in
figure 14.6.
API server
Validation
Pod A
manifest
- Requests
- Limits
Pod A
manifest
- Requests
- Limits
Pod B
manifest
- No
requests
or limits
Pod B
manifest
- No
requests
or limits
Defaulting
Rejected because
requests and limits are
outside min/max values
Defaults
applied
Namespace XYZ
LimitRange
Pod B
manifest
- Default
requests
- Default
limits
Pod B
- Default requests
- Default limits
- Min/max CPU
- Min/max memory
- Default requests
- Default limits
Figure 14.6
A LimitRange is used for validation and defaulting pods.
 
</data>
  <data key="d5">421
Setting default requests and limits for pods per namespace
memory than the other, percentage-wise. That’s why in figure 14.5, pod B, using 90%
of its requested memory, gets killed before pod C, which is only using 70%, even
though it’s using more megabytes of memory than pod B. 
 This shows you need to be mindful of not only the relationship between requests
and limits, but also of requests and the expected actual memory consumption. 
14.4
Setting default requests and limits for pods per 
namespace
We’ve looked at how resource requests and limits can be set for each individual con-
tainer. If you don’t set them, the container is at the mercy of all other containers that
do specify resource requests and limits. It’s a good idea to set requests and limits on
every container.
14.4.1 Introducing the LimitRange resource
Instead of having to do this for every container, you can also do it by creating a Limit-
Range resource. It allows you to specify (for each namespace) not only the minimum
and maximum limit you can set on a container for each resource, but also the default
resource requests for containers that don’t specify requests explicitly, as depicted in
figure 14.6.
API server
Validation
Pod A
manifest
- Requests
- Limits
Pod A
manifest
- Requests
- Limits
Pod B
manifest
- No
requests
or limits
Pod B
manifest
- No
requests
or limits
Defaulting
Rejected because
requests and limits are
outside min/max values
Defaults
applied
Namespace XYZ
LimitRange
Pod B
manifest
- Default
requests
- Default
limits
Pod B
- Default requests
- Default limits
- Min/max CPU
- Min/max memory
- Default requests
- Default limits
Figure 14.6
A LimitRange is used for validation and defaulting pods.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="454">
  <data key="d0">Page_454</data>
  <data key="d5">Page_454</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_365">
  <data key="d0">422
CHAPTER 14
Managing pods’ computational resources
LimitRange resources are used by the LimitRanger Admission Control plugin (we
explained what those plugins are in chapter 11). When a pod manifest is posted to the
API server, the LimitRanger plugin validates the pod spec. If validation fails, the mani-
fest is rejected immediately. Because of this, a great use-case for LimitRange objects is
to prevent users from creating pods that are bigger than any node in the cluster. With-
out such a LimitRange, the API server will gladly accept the pod, but then never
schedule it. 
 The limits specified in a LimitRange resource apply to each individual pod/con-
tainer or other kind of object created in the same namespace as the LimitRange
object. They don’t limit the total amount of resources available across all the pods in
the namespace. This is specified through ResourceQuota objects, which are explained
in section 14.5. 
14.4.2 Creating a LimitRange object
Let’s look at a full example of a LimitRange and see what the individual properties do.
The following listing shows the full definition of a LimitRange resource.
apiVersion: v1
kind: LimitRange
metadata:
  name: example
spec:
  limits:
  - type: Pod           
    min:                         
      cpu: 50m                   
      memory: 5Mi                
    max:                          
      cpu: 1                      
      memory: 1Gi                 
  - type: Container             
    defaultRequest:             
      cpu: 100m                 
      memory: 10Mi              
    default:                      
      cpu: 200m                   
      memory: 100Mi               
    min:                         
      cpu: 50m                   
      memory: 5Mi                
    max:                         
      cpu: 1                     
      memory: 1Gi                
    maxLimitRequestRatio:         
      cpu: 4                      
      memory: 10                  
Listing 14.10
A LimitRange resource: limits.yaml
Specifies the 
limits for a pod 
as a whole
Minimum CPU and memory all the 
pod’s containers can request in total
Maximum CPU and memory all the pod’s 
containers can request (and limit)
The
container
limits are
specified
below this
line.
Default requests for CPU and memory 
that will be applied to containers that 
don’t specify them explicitly
Default limits for containers 
that don’t specify them
Minimum and maximum 
requests/limits that a 
container can have
Maximum ratio between 
the limit and request 
for each resource
 
</data>
  <data key="d5">422
CHAPTER 14
Managing pods’ computational resources
LimitRange resources are used by the LimitRanger Admission Control plugin (we
explained what those plugins are in chapter 11). When a pod manifest is posted to the
API server, the LimitRanger plugin validates the pod spec. If validation fails, the mani-
fest is rejected immediately. Because of this, a great use-case for LimitRange objects is
to prevent users from creating pods that are bigger than any node in the cluster. With-
out such a LimitRange, the API server will gladly accept the pod, but then never
schedule it. 
 The limits specified in a LimitRange resource apply to each individual pod/con-
tainer or other kind of object created in the same namespace as the LimitRange
object. They don’t limit the total amount of resources available across all the pods in
the namespace. This is specified through ResourceQuota objects, which are explained
in section 14.5. 
14.4.2 Creating a LimitRange object
Let’s look at a full example of a LimitRange and see what the individual properties do.
The following listing shows the full definition of a LimitRange resource.
apiVersion: v1
kind: LimitRange
metadata:
  name: example
spec:
  limits:
  - type: Pod           
    min:                         
      cpu: 50m                   
      memory: 5Mi                
    max:                          
      cpu: 1                      
      memory: 1Gi                 
  - type: Container             
    defaultRequest:             
      cpu: 100m                 
      memory: 10Mi              
    default:                      
      cpu: 200m                   
      memory: 100Mi               
    min:                         
      cpu: 50m                   
      memory: 5Mi                
    max:                         
      cpu: 1                     
      memory: 1Gi                
    maxLimitRequestRatio:         
      cpu: 4                      
      memory: 10                  
Listing 14.10
A LimitRange resource: limits.yaml
Specifies the 
limits for a pod 
as a whole
Minimum CPU and memory all the 
pod’s containers can request in total
Maximum CPU and memory all the pod’s 
containers can request (and limit)
The
container
limits are
specified
below this
line.
Default requests for CPU and memory 
that will be applied to containers that 
don’t specify them explicitly
Default limits for containers 
that don’t specify them
Minimum and maximum 
requests/limits that a 
container can have
Maximum ratio between 
the limit and request 
for each resource
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="455">
  <data key="d0">Page_455</data>
  <data key="d5">Page_455</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_366">
  <data key="d0">423
Setting default requests and limits for pods per namespace
  - type: PersistentVolumeClaim      
    min:                             
      storage: 1Gi                   
    max:                             
      storage: 10Gi                  
As you can see from the previous example, the minimum and maximum limits for a
whole pod can be configured. They apply to the sum of all the pod’s containers’
requests and limits. 
 Lower down, at the container level, you can set not only the minimum and maxi-
mum, but also default resource requests (defaultRequest) and default limits
(default) that will be applied to each container that doesn’t specify them explicitly. 
 Beside the min, max, and default values, you can even set the maximum ratio of
limits vs. requests. The previous listing sets the CPU maxLimitRequestRatio to 4,
which means a container’s CPU limits will not be allowed to be more than four times
greater than its CPU requests. A container requesting 200 millicores will not be
accepted if its CPU limit is set to 801 millicores or higher. For memory, the maximum
ratio is set to 10.
 In chapter 6 we looked at PersistentVolumeClaims (PVC), which allow you to claim
a certain amount of persistent storage similarly to how a pod’s containers claim CPU
and memory. In the same way you’re limiting the minimum and maximum amount of
CPU a container can request, you should also limit the amount of storage a single
PVC can request. A LimitRange object allows you to do that as well, as you can see at
the bottom of the example.
 The example shows a single LimitRange object containing limits for everything,
but you could also split them into multiple objects if you prefer to have them orga-
nized per type (one for pod limits, another for container limits, and yet another for
PVCs, for example). Limits from multiple LimitRange objects are all consolidated
when validating a pod or PVC.
 Because the validation (and defaults) configured in a LimitRange object is per-
formed by the API server when it receives a new pod or PVC manifest, if you modify
the limits afterwards, existing pods and PVCs will not be revalidated—the new limits
will only apply to pods and PVCs created afterward. 
14.4.3 Enforcing the limits
With your limits in place, you can now try creating a pod that requests more CPU than
allowed by the LimitRange. You’ll find the YAML for the pod in the code archive. The
next listing only shows the part relevant to the discussion.
    resources:
      requests:
        cpu: 2
Listing 14.11
A pod with CPU requests greater than the limit: limits-pod-too-big.yaml
A LimitRange can also set 
the minimum and maximum 
amount of storage a PVC 
can request.
 
</data>
  <data key="d5">423
Setting default requests and limits for pods per namespace
  - type: PersistentVolumeClaim      
    min:                             
      storage: 1Gi                   
    max:                             
      storage: 10Gi                  
As you can see from the previous example, the minimum and maximum limits for a
whole pod can be configured. They apply to the sum of all the pod’s containers’
requests and limits. 
 Lower down, at the container level, you can set not only the minimum and maxi-
mum, but also default resource requests (defaultRequest) and default limits
(default) that will be applied to each container that doesn’t specify them explicitly. 
 Beside the min, max, and default values, you can even set the maximum ratio of
limits vs. requests. The previous listing sets the CPU maxLimitRequestRatio to 4,
which means a container’s CPU limits will not be allowed to be more than four times
greater than its CPU requests. A container requesting 200 millicores will not be
accepted if its CPU limit is set to 801 millicores or higher. For memory, the maximum
ratio is set to 10.
 In chapter 6 we looked at PersistentVolumeClaims (PVC), which allow you to claim
a certain amount of persistent storage similarly to how a pod’s containers claim CPU
and memory. In the same way you’re limiting the minimum and maximum amount of
CPU a container can request, you should also limit the amount of storage a single
PVC can request. A LimitRange object allows you to do that as well, as you can see at
the bottom of the example.
 The example shows a single LimitRange object containing limits for everything,
but you could also split them into multiple objects if you prefer to have them orga-
nized per type (one for pod limits, another for container limits, and yet another for
PVCs, for example). Limits from multiple LimitRange objects are all consolidated
when validating a pod or PVC.
 Because the validation (and defaults) configured in a LimitRange object is per-
formed by the API server when it receives a new pod or PVC manifest, if you modify
the limits afterwards, existing pods and PVCs will not be revalidated—the new limits
will only apply to pods and PVCs created afterward. 
14.4.3 Enforcing the limits
With your limits in place, you can now try creating a pod that requests more CPU than
allowed by the LimitRange. You’ll find the YAML for the pod in the code archive. The
next listing only shows the part relevant to the discussion.
    resources:
      requests:
        cpu: 2
Listing 14.11
A pod with CPU requests greater than the limit: limits-pod-too-big.yaml
A LimitRange can also set 
the minimum and maximum 
amount of storage a PVC 
can request.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="456">
  <data key="d0">Page_456</data>
  <data key="d5">Page_456</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_367">
  <data key="d0">424
CHAPTER 14
Managing pods’ computational resources
The pod’s single container is requesting two CPUs, which is more than the maximum
you set in the LimitRange earlier. Creating the pod yields the following result:
$ kubectl create -f limits-pod-too-big.yaml 
Error from server (Forbidden): error when creating "limits-pod-too-big.yaml": 
pods "too-big" is forbidden: [
  maximum cpu usage per Pod is 1, but request is 2., 
  maximum cpu usage per Container is 1, but request is 2.]
I’ve modified the output slightly to make it more legible. The nice thing about the
error message from the server is that it lists all the reasons why the pod was rejected,
not only the first one it encountered. As you can see, the pod was rejected for two rea-
sons: you requested two CPUs for the container, but the maximum CPU limit for a
container is one. Likewise, the pod as a whole requested two CPUs, but the maximum
is one CPU (if this was a multi-container pod, even if each individual container
requested less than the maximum amount of CPU, together they’d still need to
request less than two CPUs to pass the maximum CPU for pods). 
14.4.4 Applying default resource requests and limits
Now let’s also see how default resource requests and limits are set on containers that
don’t specify them. Deploy the kubia-manual pod from chapter 3 again:
$ kubectl create -f ../Chapter03/kubia-manual.yaml
pod "kubia-manual" created
Before you set up your LimitRange object, all your pods were created without any
resource requests or limits, but now the defaults are applied automatically when creat-
ing the pod. You can confirm this by describing the kubia-manual pod, as shown in
the following listing.
$ kubectl describe po kubia-manual
Name:           kubia-manual
...
Containers:
  kubia:
    Limits:
      cpu:      200m
      memory:   100Mi
    Requests:
      cpu:      100m
      memory:   10Mi
The container’s requests and limits match the ones you specified in the LimitRange
object. If you used a different LimitRange specification in another namespace, pods
created in that namespace would obviously have different requests and limits. This
allows admins to configure default, min, and max resources for pods per namespace.
Listing 14.12
Inspecting limits that were applied to a pod automatically
 
</data>
  <data key="d5">424
CHAPTER 14
Managing pods’ computational resources
The pod’s single container is requesting two CPUs, which is more than the maximum
you set in the LimitRange earlier. Creating the pod yields the following result:
$ kubectl create -f limits-pod-too-big.yaml 
Error from server (Forbidden): error when creating "limits-pod-too-big.yaml": 
pods "too-big" is forbidden: [
  maximum cpu usage per Pod is 1, but request is 2., 
  maximum cpu usage per Container is 1, but request is 2.]
I’ve modified the output slightly to make it more legible. The nice thing about the
error message from the server is that it lists all the reasons why the pod was rejected,
not only the first one it encountered. As you can see, the pod was rejected for two rea-
sons: you requested two CPUs for the container, but the maximum CPU limit for a
container is one. Likewise, the pod as a whole requested two CPUs, but the maximum
is one CPU (if this was a multi-container pod, even if each individual container
requested less than the maximum amount of CPU, together they’d still need to
request less than two CPUs to pass the maximum CPU for pods). 
14.4.4 Applying default resource requests and limits
Now let’s also see how default resource requests and limits are set on containers that
don’t specify them. Deploy the kubia-manual pod from chapter 3 again:
$ kubectl create -f ../Chapter03/kubia-manual.yaml
pod "kubia-manual" created
Before you set up your LimitRange object, all your pods were created without any
resource requests or limits, but now the defaults are applied automatically when creat-
ing the pod. You can confirm this by describing the kubia-manual pod, as shown in
the following listing.
$ kubectl describe po kubia-manual
Name:           kubia-manual
...
Containers:
  kubia:
    Limits:
      cpu:      200m
      memory:   100Mi
    Requests:
      cpu:      100m
      memory:   10Mi
The container’s requests and limits match the ones you specified in the LimitRange
object. If you used a different LimitRange specification in another namespace, pods
created in that namespace would obviously have different requests and limits. This
allows admins to configure default, min, and max resources for pods per namespace.
Listing 14.12
Inspecting limits that were applied to a pod automatically
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="457">
  <data key="d0">Page_457</data>
  <data key="d5">Page_457</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_368">
  <data key="d0">425
Limiting the total resources available in a namespace
If namespaces are used to separate different teams or to separate development, QA,
staging, and production pods running in the same Kubernetes cluster, using a differ-
ent LimitRange in each namespace ensures large pods can only be created in certain
namespaces, whereas others are constrained to smaller pods.
 But remember, the limits configured in a LimitRange only apply to each individual
pod/container. It’s still possible to create many pods and eat up all the resources avail-
able in the cluster. LimitRanges don’t provide any protection from that. A Resource-
Quota object, on the other hand, does. You’ll learn about them next.
14.5
Limiting the total resources available in a namespace
As you’ve seen, LimitRanges only apply to individual pods, but cluster admins also
need a way to limit the total amount of resources available in a namespace. This is
achieved by creating a ResourceQuota object. 
14.5.1 Introducing the ResourceQuota object
In chapter 10 we said that several Admission Control plugins running inside the API
server verify whether the pod may be created or not. In the previous section, I said
that the LimitRanger plugin enforces the policies configured in LimitRange resources.
Similarly, the ResourceQuota Admission Control plugin checks whether the pod
being created would cause the configured ResourceQuota to be exceeded. If that’s
the case, the pod’s creation is rejected. Because resource quotas are enforced at pod
creation time, a ResourceQuota object only affects pods created after the Resource-
Quota object is created—creating it has no effect on existing pods.
 A ResourceQuota limits the amount of computational resources the pods and the
amount of storage PersistentVolumeClaims in a namespace can consume. It can also
limit the number of pods, claims, and other API objects users are allowed to create
inside the namespace. Because you’ve mostly dealt with CPU and memory so far, let’s
start by looking at how to specify quotas for them.
CREATING A RESOURCEQUOTA FOR CPU AND MEMORY
The overall CPU and memory all the pods in a namespace are allowed to consume is
defined by creating a ResourceQuota object as shown in the following listing.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cpu-and-mem
spec:
  hard:
    requests.cpu: 400m
    requests.memory: 200Mi
    limits.cpu: 600m
    limits.memory: 500Mi
Listing 14.13
A ResourceQuota resource for CPU and memory: quota-cpu-memory.yaml
 
</data>
  <data key="d5">425
Limiting the total resources available in a namespace
If namespaces are used to separate different teams or to separate development, QA,
staging, and production pods running in the same Kubernetes cluster, using a differ-
ent LimitRange in each namespace ensures large pods can only be created in certain
namespaces, whereas others are constrained to smaller pods.
 But remember, the limits configured in a LimitRange only apply to each individual
pod/container. It’s still possible to create many pods and eat up all the resources avail-
able in the cluster. LimitRanges don’t provide any protection from that. A Resource-
Quota object, on the other hand, does. You’ll learn about them next.
14.5
Limiting the total resources available in a namespace
As you’ve seen, LimitRanges only apply to individual pods, but cluster admins also
need a way to limit the total amount of resources available in a namespace. This is
achieved by creating a ResourceQuota object. 
14.5.1 Introducing the ResourceQuota object
In chapter 10 we said that several Admission Control plugins running inside the API
server verify whether the pod may be created or not. In the previous section, I said
that the LimitRanger plugin enforces the policies configured in LimitRange resources.
Similarly, the ResourceQuota Admission Control plugin checks whether the pod
being created would cause the configured ResourceQuota to be exceeded. If that’s
the case, the pod’s creation is rejected. Because resource quotas are enforced at pod
creation time, a ResourceQuota object only affects pods created after the Resource-
Quota object is created—creating it has no effect on existing pods.
 A ResourceQuota limits the amount of computational resources the pods and the
amount of storage PersistentVolumeClaims in a namespace can consume. It can also
limit the number of pods, claims, and other API objects users are allowed to create
inside the namespace. Because you’ve mostly dealt with CPU and memory so far, let’s
start by looking at how to specify quotas for them.
CREATING A RESOURCEQUOTA FOR CPU AND MEMORY
The overall CPU and memory all the pods in a namespace are allowed to consume is
defined by creating a ResourceQuota object as shown in the following listing.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cpu-and-mem
spec:
  hard:
    requests.cpu: 400m
    requests.memory: 200Mi
    limits.cpu: 600m
    limits.memory: 500Mi
Listing 14.13
A ResourceQuota resource for CPU and memory: quota-cpu-memory.yaml
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="458">
  <data key="d0">Page_458</data>
  <data key="d5">Page_458</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_369">
  <data key="d0">426
CHAPTER 14
Managing pods’ computational resources
Instead of defining a single total for each resource, you define separate totals for
requests and limits for both CPU and memory. You’ll notice the structure is a bit dif-
ferent, compared to that of a LimitRange. Here, both the requests and the limits for
all resources are defined in a single place. 
 This ResourceQuota sets the maximum amount of CPU pods in the namespace
can request to 400 millicores. The maximum total CPU limits in the namespace are
set to 600 millicores. For memory, the maximum total requests are set to 200 MiB,
whereas the limits are set to 500 MiB.
 A ResourceQuota object applies to the namespace it’s created in, like a Limit-
Range, but it applies to all the pods’ resource requests and limits in total and not to
each individual pod or container separately, as shown in figure 14.7.
INSPECTING THE QUOTA AND QUOTA USAGE
After you post the ResourceQuota object to the API server, you can use the kubectl
describe command to see how much of the quota is already used up, as shown in
the following listing.
$ kubectl describe quota
Name:           cpu-and-mem
Namespace:      default
Resource        Used   Hard
--------        ----   ----
limits.cpu      200m   600m
limits.memory   100Mi  500Mi
requests.cpu    100m   400m
requests.memory 10Mi   200Mi
I only have the kubia-manual pod running, so the Used column matches its resource
requests and limits. When I run additional pods, their requests and limits are added to
the used amounts.
Listing 14.14
Inspecting the ResourceQuota with kubectl describe quota
LimitRange
ResourceQuota
Namespace: FOO
Pod A
Pod B
Pod C
LimitRange
ResourceQuota
Namespace: BAR
Pod D
Pod E
Pod F
Figure 14.7
LimitRanges apply to individual pods; ResourceQuotas apply to all pods in the 
namespace.
 
</data>
  <data key="d5">426
CHAPTER 14
Managing pods’ computational resources
Instead of defining a single total for each resource, you define separate totals for
requests and limits for both CPU and memory. You’ll notice the structure is a bit dif-
ferent, compared to that of a LimitRange. Here, both the requests and the limits for
all resources are defined in a single place. 
 This ResourceQuota sets the maximum amount of CPU pods in the namespace
can request to 400 millicores. The maximum total CPU limits in the namespace are
set to 600 millicores. For memory, the maximum total requests are set to 200 MiB,
whereas the limits are set to 500 MiB.
 A ResourceQuota object applies to the namespace it’s created in, like a Limit-
Range, but it applies to all the pods’ resource requests and limits in total and not to
each individual pod or container separately, as shown in figure 14.7.
INSPECTING THE QUOTA AND QUOTA USAGE
After you post the ResourceQuota object to the API server, you can use the kubectl
describe command to see how much of the quota is already used up, as shown in
the following listing.
$ kubectl describe quota
Name:           cpu-and-mem
Namespace:      default
Resource        Used   Hard
--------        ----   ----
limits.cpu      200m   600m
limits.memory   100Mi  500Mi
requests.cpu    100m   400m
requests.memory 10Mi   200Mi
I only have the kubia-manual pod running, so the Used column matches its resource
requests and limits. When I run additional pods, their requests and limits are added to
the used amounts.
Listing 14.14
Inspecting the ResourceQuota with kubectl describe quota
LimitRange
ResourceQuota
Namespace: FOO
Pod A
Pod B
Pod C
LimitRange
ResourceQuota
Namespace: BAR
Pod D
Pod E
Pod F
Figure 14.7
LimitRanges apply to individual pods; ResourceQuotas apply to all pods in the 
namespace.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="459">
  <data key="d0">Page_459</data>
  <data key="d5">Page_459</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_370">
  <data key="d0">427
Limiting the total resources available in a namespace
CREATING A LIMITRANGE ALONG WITH A RESOURCEQUOTA
One caveat when creating a ResourceQuota is that you will also want to create a Limit-
Range object alongside it. In your case, you have a LimitRange configured from the
previous section, but if you didn’t have one, you couldn’t run the kubia-manual pod,
because it doesn’t specify any resource requests or limits. Here’s what would happen
in that case:
$ kubectl create -f ../Chapter03/kubia-manual.yaml
Error from server (Forbidden): error when creating "../Chapter03/kubia-
manual.yaml": pods "kubia-manual" is forbidden: failed quota: cpu-and-
mem: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
When a quota for a specific resource (CPU or memory) is configured (request or
limit), pods need to have the request or limit (respectively) set for that same resource;
otherwise the API server will not accept the pod. That’s why having a LimitRange with
defaults for those resources can make life a bit easier for people creating pods.
14.5.2 Specifying a quota for persistent storage
A ResourceQuota object can also limit the amount of persistent storage that can be
claimed in the namespace, as shown in the following listing.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage
spec:
  hard:
    requests.storage: 500Gi                               
    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi     
    standard.storageclass.storage.k8s.io/requests.storage: 1Ti
In this example, the amount of storage all PersistentVolumeClaims in a namespace
can request is limited to 500 GiB (by the requests.storage entry in the Resource-
Quota object). But as you’ll remember from chapter 6, PersistentVolumeClaims can
request a dynamically provisioned PersistentVolume of a specific StorageClass. That’s
why Kubernetes also makes it possible to define storage quotas for each StorageClass
individually. The previous example limits the total amount of claimable SSD storage
(designated by the ssd StorageClass) to 300 GiB. The less-performant HDD storage
(StorageClass standard) is limited to 1 TiB.
14.5.3 Limiting the number of objects that can be created
A ResourceQuota can also be configured to limit the number of Pods, Replication-
Controllers, Services, and other objects inside a single namespace. This allows the
cluster admin to limit the number of objects users can create based on their payment
Listing 14.15
A ResourceQuota for storage: quota-storage.yaml
The amount of 
storage claimable 
overall
The amount 
of claimable 
storage in 
StorageClass ssd
 
</data>
  <data key="d5">427
Limiting the total resources available in a namespace
CREATING A LIMITRANGE ALONG WITH A RESOURCEQUOTA
One caveat when creating a ResourceQuota is that you will also want to create a Limit-
Range object alongside it. In your case, you have a LimitRange configured from the
previous section, but if you didn’t have one, you couldn’t run the kubia-manual pod,
because it doesn’t specify any resource requests or limits. Here’s what would happen
in that case:
$ kubectl create -f ../Chapter03/kubia-manual.yaml
Error from server (Forbidden): error when creating "../Chapter03/kubia-
manual.yaml": pods "kubia-manual" is forbidden: failed quota: cpu-and-
mem: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
When a quota for a specific resource (CPU or memory) is configured (request or
limit), pods need to have the request or limit (respectively) set for that same resource;
otherwise the API server will not accept the pod. That’s why having a LimitRange with
defaults for those resources can make life a bit easier for people creating pods.
14.5.2 Specifying a quota for persistent storage
A ResourceQuota object can also limit the amount of persistent storage that can be
claimed in the namespace, as shown in the following listing.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage
spec:
  hard:
    requests.storage: 500Gi                               
    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi     
    standard.storageclass.storage.k8s.io/requests.storage: 1Ti
In this example, the amount of storage all PersistentVolumeClaims in a namespace
can request is limited to 500 GiB (by the requests.storage entry in the Resource-
Quota object). But as you’ll remember from chapter 6, PersistentVolumeClaims can
request a dynamically provisioned PersistentVolume of a specific StorageClass. That’s
why Kubernetes also makes it possible to define storage quotas for each StorageClass
individually. The previous example limits the total amount of claimable SSD storage
(designated by the ssd StorageClass) to 300 GiB. The less-performant HDD storage
(StorageClass standard) is limited to 1 TiB.
14.5.3 Limiting the number of objects that can be created
A ResourceQuota can also be configured to limit the number of Pods, Replication-
Controllers, Services, and other objects inside a single namespace. This allows the
cluster admin to limit the number of objects users can create based on their payment
Listing 14.15
A ResourceQuota for storage: quota-storage.yaml
The amount of 
storage claimable 
overall
The amount 
of claimable 
storage in 
StorageClass ssd
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="460">
  <data key="d0">Page_460</data>
  <data key="d5">Page_460</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_371">
  <data key="d0">428
CHAPTER 14
Managing pods’ computational resources
plan, for example, and can also limit the number of public IPs or node ports Ser-
vices can use. 
 The following listing shows what a ResourceQuota object that limits the number of
objects may look like.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: objects
spec:
  hard:
    pods: 10                        
    replicationcontrollers: 5       
    secrets: 10                     
    configmaps: 10                  
    persistentvolumeclaims: 4       
    services: 5                      
    services.loadbalancers: 1        
    services.nodeports: 2            
    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2   
The ResourceQuota in this listing allows users to create at most 10 Pods in the name-
space, regardless if they’re created manually or by a ReplicationController, Replica-
Set, DaemonSet, Job, and so on. It also limits the number of ReplicationControllers to
five. A maximum of five Services can be created, of which only one can be a LoadBal-
ancer-type Service, and only two can be NodePort Services. Similar to how the maxi-
mum amount of requested storage can be specified per StorageClass, the number of
PersistentVolumeClaims can also be limited per StorageClass.
 Object count quotas can currently be set for the following objects: 
Pods
ReplicationControllers 
Secrets
ConfigMaps
PersistentVolumeClaims
Services (in general), and for two specific types of Services, such as Load-
Balancer Services (services.loadbalancers) and NodePort Services (ser-
vices.nodeports) 
Finally, you can even set an object count quota for ResourceQuota objects themselves.
The number of other objects, such as ReplicaSets, Jobs, Deployments, Ingresses, and
so on, cannot be limited yet (but this may have changed since the book was published,
so please check the documentation for up-to-date information).
Listing 14.16
A ResourceQuota for max number of resources: quota-object-count.yaml
Only 10 Pods, 5 ReplicationControllers, 
10 Secrets, 10 ConfigMaps, and 
4 PersistentVolumeClaims can be 
created in the namespace.
Five Services overall can be created, 
of which at most one can be a 
LoadBalancer Service and at most 
two can be NodePort Services.
Only two PVCs can claim storage
with the ssd StorageClass.
 
</data>
  <data key="d5">428
CHAPTER 14
Managing pods’ computational resources
plan, for example, and can also limit the number of public IPs or node ports Ser-
vices can use. 
 The following listing shows what a ResourceQuota object that limits the number of
objects may look like.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: objects
spec:
  hard:
    pods: 10                        
    replicationcontrollers: 5       
    secrets: 10                     
    configmaps: 10                  
    persistentvolumeclaims: 4       
    services: 5                      
    services.loadbalancers: 1        
    services.nodeports: 2            
    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2   
The ResourceQuota in this listing allows users to create at most 10 Pods in the name-
space, regardless if they’re created manually or by a ReplicationController, Replica-
Set, DaemonSet, Job, and so on. It also limits the number of ReplicationControllers to
five. A maximum of five Services can be created, of which only one can be a LoadBal-
ancer-type Service, and only two can be NodePort Services. Similar to how the maxi-
mum amount of requested storage can be specified per StorageClass, the number of
PersistentVolumeClaims can also be limited per StorageClass.
 Object count quotas can currently be set for the following objects: 
Pods
ReplicationControllers 
Secrets
ConfigMaps
PersistentVolumeClaims
Services (in general), and for two specific types of Services, such as Load-
Balancer Services (services.loadbalancers) and NodePort Services (ser-
vices.nodeports) 
Finally, you can even set an object count quota for ResourceQuota objects themselves.
The number of other objects, such as ReplicaSets, Jobs, Deployments, Ingresses, and
so on, cannot be limited yet (but this may have changed since the book was published,
so please check the documentation for up-to-date information).
Listing 14.16
A ResourceQuota for max number of resources: quota-object-count.yaml
Only 10 Pods, 5 ReplicationControllers, 
10 Secrets, 10 ConfigMaps, and 
4 PersistentVolumeClaims can be 
created in the namespace.
Five Services overall can be created, 
of which at most one can be a 
LoadBalancer Service and at most 
two can be NodePort Services.
Only two PVCs can claim storage
with the ssd StorageClass.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="461">
  <data key="d0">Page_461</data>
  <data key="d5">Page_461</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_372">
  <data key="d0">429
Limiting the total resources available in a namespace
14.5.4 Specifying quotas for specific pod states and/or QoS classes
The quotas you’ve created so far have applied to all pods, regardless of their current
state and QoS class. But quotas can also be limited to a set of quota scopes. Four scopes are
currently available: BestEffort, NotBestEffort, Terminating, and NotTerminating. 
 The BestEffort and NotBestEffort scopes determine whether the quota applies
to pods with the BestEffort QoS class or with one of the other two classes (that is,
Burstable and Guaranteed). 
 The other two scopes (Terminating and NotTerminating) don’t apply to pods
that are (or aren’t) in the process of shutting down, as the name might lead you to
believe. We haven’t talked about this, but you can specify how long each pod is
allowed to run before it’s terminated and marked as Failed. This is done by setting
the activeDeadlineSeconds field in the pod spec. This property defines the number
of seconds a pod is allowed to be active on the node relative to its start time before it’s
marked as Failed and then terminated. The Terminating quota scope applies to pods
that have the activeDeadlineSeconds set, whereas the NotTerminating applies to
those that don’t. 
 When creating a ResourceQuota, you can specify the scopes that it applies to. A
pod must match all the specified scopes for the quota to apply to it. Additionally, what
a quota can limit depends on the quota’s scope. BestEffort scope can only limit the
number of pods, whereas the other three scopes can limit the number of pods,
CPU/memory requests, and CPU/memory limits. 
 If, for example, you want the quota to apply only to BestEffort, NotTerminating
pods, you can create the ResourceQuota object shown in the following listing.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-notterminating-pods
spec:
  scopes:                 
  - BestEffort            
  - NotTerminating        
  hard: 
    pods: 4          
This quota ensures that at most four pods exist with the BestEffort QoS class,
which don’t have an active deadline. If the quota was targeting NotBestEffort pods
instead, you could also specify requests.cpu, requests.memory, limits.cpu, and
limits.memory.
NOTE
Before you move on to the next section of this chapter, please delete
all the ResourceQuota and LimitRange resources you created. You won’t
Listing 14.17
ResourceQuota for BestEffort/NotTerminating pods: 
quota-scoped.yaml
This quota only applies to pods 
that have the BestEffort QoS and 
don’t have an active deadline set.
Only four such 
pods can exist.
 
</data>
  <data key="d5">429
Limiting the total resources available in a namespace
14.5.4 Specifying quotas for specific pod states and/or QoS classes
The quotas you’ve created so far have applied to all pods, regardless of their current
state and QoS class. But quotas can also be limited to a set of quota scopes. Four scopes are
currently available: BestEffort, NotBestEffort, Terminating, and NotTerminating. 
 The BestEffort and NotBestEffort scopes determine whether the quota applies
to pods with the BestEffort QoS class or with one of the other two classes (that is,
Burstable and Guaranteed). 
 The other two scopes (Terminating and NotTerminating) don’t apply to pods
that are (or aren’t) in the process of shutting down, as the name might lead you to
believe. We haven’t talked about this, but you can specify how long each pod is
allowed to run before it’s terminated and marked as Failed. This is done by setting
the activeDeadlineSeconds field in the pod spec. This property defines the number
of seconds a pod is allowed to be active on the node relative to its start time before it’s
marked as Failed and then terminated. The Terminating quota scope applies to pods
that have the activeDeadlineSeconds set, whereas the NotTerminating applies to
those that don’t. 
 When creating a ResourceQuota, you can specify the scopes that it applies to. A
pod must match all the specified scopes for the quota to apply to it. Additionally, what
a quota can limit depends on the quota’s scope. BestEffort scope can only limit the
number of pods, whereas the other three scopes can limit the number of pods,
CPU/memory requests, and CPU/memory limits. 
 If, for example, you want the quota to apply only to BestEffort, NotTerminating
pods, you can create the ResourceQuota object shown in the following listing.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-notterminating-pods
spec:
  scopes:                 
  - BestEffort            
  - NotTerminating        
  hard: 
    pods: 4          
This quota ensures that at most four pods exist with the BestEffort QoS class,
which don’t have an active deadline. If the quota was targeting NotBestEffort pods
instead, you could also specify requests.cpu, requests.memory, limits.cpu, and
limits.memory.
NOTE
Before you move on to the next section of this chapter, please delete
all the ResourceQuota and LimitRange resources you created. You won’t
Listing 14.17
ResourceQuota for BestEffort/NotTerminating pods: 
quota-scoped.yaml
This quota only applies to pods 
that have the BestEffort QoS and 
don’t have an active deadline set.
Only four such 
pods can exist.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="462">
  <data key="d0">Page_462</data>
  <data key="d5">Page_462</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_373">
  <data key="d0">430
CHAPTER 14
Managing pods’ computational resources
need them anymore and they may interfere with examples in the following
chapters.
14.6
Monitoring pod resource usage
Properly setting resource requests and limits is crucial for getting the most out of your
Kubernetes cluster. If requests are set too high, your cluster nodes will be underuti-
lized and you’ll be throwing money away. If you set them too low, your apps will be
CPU-starved or even killed by the OOM Killer. How do you find the sweet spot for
requests and limits?
 You find it by monitoring the actual resource usage of your containers under the
expected load levels. Once the application is exposed to the public, you should keep
monitoring it and adjust the resource requests and limits if required.
14.6.1 Collecting and retrieving actual resource usages
How does one monitor apps running in Kubernetes? Luckily, the Kubelet itself
already contains an agent called cAdvisor, which performs the basic collection of
resource consumption data for both individual containers running on the node and
the node as a whole. Gathering those statistics centrally for the whole cluster requires
you to run an additional component called Heapster. 
 Heapster runs as a pod on one of the nodes and is exposed through a regular
Kubernetes Service, making it accessible at a stable IP address. It collects the data
from all cAdvisors in the cluster and exposes it in a single location. Figure 14.8
shows the flow of the metrics data from the pods, through cAdvisor and finally into
Heapster.
Kubelet
cAdvisor
Node 1
Pod
Pod
Kubelet
cAdvisor
Node 2
Pod
Kubelet
cAdvisor
Node X
Pod
Heapster
Each cAdvisor collects metrics from
containers running on its node.
Heapster runs on one of the nodes as a
pod and collects metrics from all nodes.
Figure 14.8
The flow of metrics data into Heapster
 
</data>
  <data key="d5">430
CHAPTER 14
Managing pods’ computational resources
need them anymore and they may interfere with examples in the following
chapters.
14.6
Monitoring pod resource usage
Properly setting resource requests and limits is crucial for getting the most out of your
Kubernetes cluster. If requests are set too high, your cluster nodes will be underuti-
lized and you’ll be throwing money away. If you set them too low, your apps will be
CPU-starved or even killed by the OOM Killer. How do you find the sweet spot for
requests and limits?
 You find it by monitoring the actual resource usage of your containers under the
expected load levels. Once the application is exposed to the public, you should keep
monitoring it and adjust the resource requests and limits if required.
14.6.1 Collecting and retrieving actual resource usages
How does one monitor apps running in Kubernetes? Luckily, the Kubelet itself
already contains an agent called cAdvisor, which performs the basic collection of
resource consumption data for both individual containers running on the node and
the node as a whole. Gathering those statistics centrally for the whole cluster requires
you to run an additional component called Heapster. 
 Heapster runs as a pod on one of the nodes and is exposed through a regular
Kubernetes Service, making it accessible at a stable IP address. It collects the data
from all cAdvisors in the cluster and exposes it in a single location. Figure 14.8
shows the flow of the metrics data from the pods, through cAdvisor and finally into
Heapster.
Kubelet
cAdvisor
Node 1
Pod
Pod
Kubelet
cAdvisor
Node 2
Pod
Kubelet
cAdvisor
Node X
Pod
Heapster
Each cAdvisor collects metrics from
containers running on its node.
Heapster runs on one of the nodes as a
pod and collects metrics from all nodes.
Figure 14.8
The flow of metrics data into Heapster
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="463">
  <data key="d0">Page_463</data>
  <data key="d5">Page_463</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_374">
  <data key="d0">431
Monitoring pod resource usage
The arrows in the figure show how the metrics data flows. They don’t show which com-
ponent connects to which to get the data. The pods (or the containers running
therein) don’t know anything about cAdvisor, and cAdvisor doesn’t know anything
about Heapster. It’s Heapster that connects to all the cAdvisors, and it’s the cAdvisors
that collect the container and node usage data without having to talk to the processes
running inside the pods’ containers.
ENABLING HEAPSTER
If you’re running a cluster in Google Kubernetes Engine, Heapster is enabled by
default. If you’re using Minikube, it’s available as an add-on and can be enabled with
the following command:
$ minikube addons enable heapster
heapster was successfully enabled
To run Heapster manually in other types of Kubernetes clusters, you can refer to
instructions located at https:/
/github.com/kubernetes/heapster. 
 After enabling Heapster, you’ll need to wait a few minutes for it to collect metrics
before you can see resource usage statistics for your cluster, so be patient. 
DISPLAYING CPU AND MEMORY USAGE FOR CLUSTER NODES
Running Heapster in your cluster makes it possible to obtain resource usages for
nodes and individual pods through the kubectl top command. To see how much
CPU and memory is being used on your nodes, you can run the command shown in
the following listing.
$ kubectl top node
NAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
minikube   170m         8%        556Mi           27%
This shows the actual, current CPU and memory usage of all the pods running on the
node, unlike the kubectl describe node command, which shows the amount of CPU
and memory requests and limits instead of actual runtime usage data. 
DISPLAYING CPU AND MEMORY USAGE FOR INDIVIDUAL PODS
To see how much each individual pod is using, you can use the kubectl top pod com-
mand, as shown in the following listing.
$ kubectl top pod --all-namespaces
NAMESPACE      NAME                             CPU(cores)   MEMORY(bytes)
kube-system    influxdb-grafana-2r2w9           1m           32Mi
kube-system    heapster-40j6d                   0m           18Mi
Listing 14.18
Actual CPU and memory usage of nodes
Listing 14.19
Actual CPU and memory usages of pods
 
</data>
  <data key="d5">431
Monitoring pod resource usage
The arrows in the figure show how the metrics data flows. They don’t show which com-
ponent connects to which to get the data. The pods (or the containers running
therein) don’t know anything about cAdvisor, and cAdvisor doesn’t know anything
about Heapster. It’s Heapster that connects to all the cAdvisors, and it’s the cAdvisors
that collect the container and node usage data without having to talk to the processes
running inside the pods’ containers.
ENABLING HEAPSTER
If you’re running a cluster in Google Kubernetes Engine, Heapster is enabled by
default. If you’re using Minikube, it’s available as an add-on and can be enabled with
the following command:
$ minikube addons enable heapster
heapster was successfully enabled
To run Heapster manually in other types of Kubernetes clusters, you can refer to
instructions located at https:/
/github.com/kubernetes/heapster. 
 After enabling Heapster, you’ll need to wait a few minutes for it to collect metrics
before you can see resource usage statistics for your cluster, so be patient. 
DISPLAYING CPU AND MEMORY USAGE FOR CLUSTER NODES
Running Heapster in your cluster makes it possible to obtain resource usages for
nodes and individual pods through the kubectl top command. To see how much
CPU and memory is being used on your nodes, you can run the command shown in
the following listing.
$ kubectl top node
NAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
minikube   170m         8%        556Mi           27%
This shows the actual, current CPU and memory usage of all the pods running on the
node, unlike the kubectl describe node command, which shows the amount of CPU
and memory requests and limits instead of actual runtime usage data. 
DISPLAYING CPU AND MEMORY USAGE FOR INDIVIDUAL PODS
To see how much each individual pod is using, you can use the kubectl top pod com-
mand, as shown in the following listing.
$ kubectl top pod --all-namespaces
NAMESPACE      NAME                             CPU(cores)   MEMORY(bytes)
kube-system    influxdb-grafana-2r2w9           1m           32Mi
kube-system    heapster-40j6d                   0m           18Mi
Listing 14.18
Actual CPU and memory usage of nodes
Listing 14.19
Actual CPU and memory usages of pods
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="464">
  <data key="d0">Page_464</data>
  <data key="d5">Page_464</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_375">
  <data key="d0">432
CHAPTER 14
Managing pods’ computational resources
default        kubia-3773182134-63bmb           0m           9Mi
kube-system    kube-dns-v20-z0hq6               1m           11Mi
kube-system    kubernetes-dashboard-r53mc       0m           14Mi
kube-system    kube-addon-manager-minikube      7m           33Mi
The outputs of both these commands are fairly simple, so you probably don’t need me
to explain them, but I do need to warn you about one thing. Sometimes the top pod
command will refuse to show any metrics and instead print out an error like this:
$ kubectl top pod
W0312 22:12:58.021885   15126 top_pod.go:186] Metrics not available for pod 
default/kubia-3773182134-63bmb, age: 1h24m19.021873823s
error: Metrics not available for pod default/kubia-3773182134-63bmb, age: 
1h24m19.021873823s
If this happens, don’t start looking for the cause of the error yet. Relax, wait a while,
and rerun the command—it may take a few minutes, but the metrics should appear
eventually. The kubectl top command gets the metrics from Heapster, which aggre-
gates the data over a few minutes and doesn’t expose it immediately. 
TIP
To see resource usages across individual containers instead of pods, you
can use the --containers option. 
14.6.2 Storing and analyzing historical resource consumption statistics
The top command only shows current resource usages—it doesn’t show you how
much CPU or memory your pods consumed throughout the last hour, yesterday, or a
week ago, for example. In fact, both cAdvisor and Heapster only hold resource usage
data for a short window of time. If you want to analyze your pods’ resource consump-
tion over longer time periods, you’ll need to run additional tools.
 When using Google Kubernetes Engine, you can monitor your cluster with Google
Cloud Monitoring, but when you’re running your own local Kubernetes cluster
(either through Minikube or other means), people usually use InfluxDB for storing
statistics data and Grafana for visualizing and analyzing them. 
INTRODUCING INFLUXDB AND GRAFANA
InfluxDB is an open source time-series database ideal for storing application metrics
and other monitoring data. Grafana, also open source, is an analytics and visualization
suite with a nice-looking web console that allows you to visualize the data stored in
InfluxDB and discover how your application’s resource usage behaves over time (an
example showing three Grafana charts is shown in figure 14.9).
 
 
</data>
  <data key="d5">432
CHAPTER 14
Managing pods’ computational resources
default        kubia-3773182134-63bmb           0m           9Mi
kube-system    kube-dns-v20-z0hq6               1m           11Mi
kube-system    kubernetes-dashboard-r53mc       0m           14Mi
kube-system    kube-addon-manager-minikube      7m           33Mi
The outputs of both these commands are fairly simple, so you probably don’t need me
to explain them, but I do need to warn you about one thing. Sometimes the top pod
command will refuse to show any metrics and instead print out an error like this:
$ kubectl top pod
W0312 22:12:58.021885   15126 top_pod.go:186] Metrics not available for pod 
default/kubia-3773182134-63bmb, age: 1h24m19.021873823s
error: Metrics not available for pod default/kubia-3773182134-63bmb, age: 
1h24m19.021873823s
If this happens, don’t start looking for the cause of the error yet. Relax, wait a while,
and rerun the command—it may take a few minutes, but the metrics should appear
eventually. The kubectl top command gets the metrics from Heapster, which aggre-
gates the data over a few minutes and doesn’t expose it immediately. 
TIP
To see resource usages across individual containers instead of pods, you
can use the --containers option. 
14.6.2 Storing and analyzing historical resource consumption statistics
The top command only shows current resource usages—it doesn’t show you how
much CPU or memory your pods consumed throughout the last hour, yesterday, or a
week ago, for example. In fact, both cAdvisor and Heapster only hold resource usage
data for a short window of time. If you want to analyze your pods’ resource consump-
tion over longer time periods, you’ll need to run additional tools.
 When using Google Kubernetes Engine, you can monitor your cluster with Google
Cloud Monitoring, but when you’re running your own local Kubernetes cluster
(either through Minikube or other means), people usually use InfluxDB for storing
statistics data and Grafana for visualizing and analyzing them. 
INTRODUCING INFLUXDB AND GRAFANA
InfluxDB is an open source time-series database ideal for storing application metrics
and other monitoring data. Grafana, also open source, is an analytics and visualization
suite with a nice-looking web console that allows you to visualize the data stored in
InfluxDB and discover how your application’s resource usage behaves over time (an
example showing three Grafana charts is shown in figure 14.9).
 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="465">
  <data key="d0">Page_465</data>
  <data key="d5">Page_465</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_376">
  <data key="d0">433
Monitoring pod resource usage
RUNNING INFLUXDB AND GRAFANA IN YOUR CLUSTER
Both InfluxDB and Grafana can run as pods. Deploying them is straightforward. All
the necessary manifests are available in the Heapster Git repository at http:/
/github
.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb.
 When using Minikube, you don’t even need to deploy them manually, because
they’re deployed along with Heapster when you enable the Heapster add-on.
ANALYZING RESOURCE USAGE WITH GRAFANA
To discover how much of each resource your pod requires over time, open the
Grafana web console and explore the predefined dashboards. Generally, you can find
out the URL of Grafana’s web console with kubectl cluster-info:
$ kubectl cluster-info
...
monitoring-grafana is running at 
https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-
system/services/monitoring-grafana
Figure 14.9
Grafana dashboard showing CPU usage across the cluster
 
</data>
  <data key="d5">433
Monitoring pod resource usage
RUNNING INFLUXDB AND GRAFANA IN YOUR CLUSTER
Both InfluxDB and Grafana can run as pods. Deploying them is straightforward. All
the necessary manifests are available in the Heapster Git repository at http:/
/github
.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb.
 When using Minikube, you don’t even need to deploy them manually, because
they’re deployed along with Heapster when you enable the Heapster add-on.
ANALYZING RESOURCE USAGE WITH GRAFANA
To discover how much of each resource your pod requires over time, open the
Grafana web console and explore the predefined dashboards. Generally, you can find
out the URL of Grafana’s web console with kubectl cluster-info:
$ kubectl cluster-info
...
monitoring-grafana is running at 
https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-
system/services/monitoring-grafana
Figure 14.9
Grafana dashboard showing CPU usage across the cluster
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="466">
  <data key="d0">Page_466</data>
  <data key="d5">Page_466</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_377">
  <data key="d0">434
CHAPTER 14
Managing pods’ computational resources
When using Minikube, Grafana’s web console is exposed through a NodePort Service,
so you can open it in your browser with the following command:
$ minikube service monitoring-grafana -n kube-system
Opening kubernetes service kube-system/monitoring-grafana in default 
browser...
A new browser window or tab will open and show the Grafana Home screen. On the
right-hand side, you’ll see a list of dashboards containing two entries:
Cluster
Pods
To see the resource usage statistics of the nodes, open the Cluster dashboard. There
you’ll see several charts showing the overall cluster usage, usage by node, and the
individual usage for CPU, memory, network, and filesystem. The charts will not only
show the actual usage, but also the requests and limits for those resources (where
they apply).
 If you then switch over to the Pods dashboard, you can examine the resource
usages for each individual pod, again with both requests and limits shown alongside
the actual usage. 
 Initially, the charts show the statistics for the last 30 minutes, but you can zoom out
and see the data for much longer time periods: days, months, or even years.
USING THE INFORMATION SHOWN IN THE CHARTS
By looking at the charts, you can quickly see if the resource requests or limits you’ve
set for your pods need to be raised or whether they can be lowered to allow more pods
to fit on your nodes. Let’s look at an example. Figure 14.10 shows the CPU and mem-
ory charts for a pod.
 At the far right of the top chart, you can see the pod is using more CPU than was
requested in the pod’s manifest. Although this isn’t problematic when this is the only
pod running on the node, you should keep in mind that a pod is only guaranteed as
much of a resource as it requests through resource requests. Your pod may be running
fine now, but when other pods are deployed to the same node and start using the
CPU, your pod’s CPU time may be throttled. Because of this, to ensure the pod can
use as much CPU as it needs to at any time, you should raise the CPU resource request
for the pod’s container.
 The bottom chart shows the pod’s memory usage and request. Here the situation is
the exact opposite. The amount of memory the pod is using is well below what was
requested in the pod’s spec. The requested memory is reserved for the pod and won’t
be available to other pods. The unused memory is therefore wasted. You should
decrease the pod’s memory request to make the memory available to other pods run-
ning on the node. 
 
</data>
  <data key="d5">434
CHAPTER 14
Managing pods’ computational resources
When using Minikube, Grafana’s web console is exposed through a NodePort Service,
so you can open it in your browser with the following command:
$ minikube service monitoring-grafana -n kube-system
Opening kubernetes service kube-system/monitoring-grafana in default 
browser...
A new browser window or tab will open and show the Grafana Home screen. On the
right-hand side, you’ll see a list of dashboards containing two entries:
Cluster
Pods
To see the resource usage statistics of the nodes, open the Cluster dashboard. There
you’ll see several charts showing the overall cluster usage, usage by node, and the
individual usage for CPU, memory, network, and filesystem. The charts will not only
show the actual usage, but also the requests and limits for those resources (where
they apply).
 If you then switch over to the Pods dashboard, you can examine the resource
usages for each individual pod, again with both requests and limits shown alongside
the actual usage. 
 Initially, the charts show the statistics for the last 30 minutes, but you can zoom out
and see the data for much longer time periods: days, months, or even years.
USING THE INFORMATION SHOWN IN THE CHARTS
By looking at the charts, you can quickly see if the resource requests or limits you’ve
set for your pods need to be raised or whether they can be lowered to allow more pods
to fit on your nodes. Let’s look at an example. Figure 14.10 shows the CPU and mem-
ory charts for a pod.
 At the far right of the top chart, you can see the pod is using more CPU than was
requested in the pod’s manifest. Although this isn’t problematic when this is the only
pod running on the node, you should keep in mind that a pod is only guaranteed as
much of a resource as it requests through resource requests. Your pod may be running
fine now, but when other pods are deployed to the same node and start using the
CPU, your pod’s CPU time may be throttled. Because of this, to ensure the pod can
use as much CPU as it needs to at any time, you should raise the CPU resource request
for the pod’s container.
 The bottom chart shows the pod’s memory usage and request. Here the situation is
the exact opposite. The amount of memory the pod is using is well below what was
requested in the pod’s spec. The requested memory is reserved for the pod and won’t
be available to other pods. The unused memory is therefore wasted. You should
decrease the pod’s memory request to make the memory available to other pods run-
ning on the node. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="467">
  <data key="d0">Page_467</data>
  <data key="d5">Page_467</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_378">
  <data key="d0">435
Summary
14.7
Summary
This chapter has shown you that you need to consider your pod’s resource usage and
configure both the resource requests and the limits for your pod to keep everything
running smoothly. The key takeaways from this chapter are
Specifying resource requests helps Kubernetes schedule pods across the cluster.
Specifying resource limits keeps pods from starving other pods of resources.
Unused CPU time is allocated based on containers’ CPU requests.
Containers never get killed if they try to use too much CPU, but they are killed
if they try to use too much memory.
In an overcommitted system, containers also get killed to free memory for more
important pods, based on the pods’ QoS classes and actual memory usage.
Actual CPU usage is higher
than what was requested.
The application’s CPU time
will be throttled when other
apps demand more CPU.
You should increase the
CPU request.
Actual memory usage is well
below requested memory.
You’ve reserved too much
memory for this app. You’re
wasting memory, because it
won’t ever be used by this
app and also can’t be used
by other apps. You should
decrease the memory
request.
CPU request
CPU usage
Memory request
Memory usage
Figure 14.10
CPU and memory usage chart for a pod
 
</data>
  <data key="d5">435
Summary
14.7
Summary
This chapter has shown you that you need to consider your pod’s resource usage and
configure both the resource requests and the limits for your pod to keep everything
running smoothly. The key takeaways from this chapter are
Specifying resource requests helps Kubernetes schedule pods across the cluster.
Specifying resource limits keeps pods from starving other pods of resources.
Unused CPU time is allocated based on containers’ CPU requests.
Containers never get killed if they try to use too much CPU, but they are killed
if they try to use too much memory.
In an overcommitted system, containers also get killed to free memory for more
important pods, based on the pods’ QoS classes and actual memory usage.
Actual CPU usage is higher
than what was requested.
The application’s CPU time
will be throttled when other
apps demand more CPU.
You should increase the
CPU request.
Actual memory usage is well
below requested memory.
You’ve reserved too much
memory for this app. You’re
wasting memory, because it
won’t ever be used by this
app and also can’t be used
by other apps. You should
decrease the memory
request.
CPU request
CPU usage
Memory request
Memory usage
Figure 14.10
CPU and memory usage chart for a pod
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="468">
  <data key="d0">Page_468</data>
  <data key="d5">Page_468</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_379">
  <data key="d0">436
CHAPTER 14
Managing pods’ computational resources
You can use LimitRange objects to define the minimum, maximum, and default
resource requests and limits for individual pods.
You can use ResourceQuota objects to limit the amount of resources available
to all the pods in a namespace.
To know how high to set a pod’s resource requests and limits, you need to mon-
itor how the pod uses resources over a long-enough time period.
In the next chapter, you’ll see how these metrics can be used by Kubernetes to auto-
matically scale your pods.
 
</data>
  <data key="d5">436
CHAPTER 14
Managing pods’ computational resources
You can use LimitRange objects to define the minimum, maximum, and default
resource requests and limits for individual pods.
You can use ResourceQuota objects to limit the amount of resources available
to all the pods in a namespace.
To know how high to set a pod’s resource requests and limits, you need to mon-
itor how the pod uses resources over a long-enough time period.
In the next chapter, you’ll see how these metrics can be used by Kubernetes to auto-
matically scale your pods.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="469">
  <data key="d0">Page_469</data>
  <data key="d5">Page_469</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_380">
  <data key="d0">437
Automatic scaling
of pods and cluster nodes
Applications running in pods can be scaled out manually by increasing the
replicas field in the ReplicationController, ReplicaSet, Deployment, or other
scalable resource. Pods can also be scaled vertically by increasing their container’s
resource requests and limits (though this can currently only be done at pod cre-
ation time, not while the pod is running). Although manual scaling is okay for
times when you can anticipate load spikes in advance or when the load changes
gradually over longer periods of time, requiring manual intervention to handle
sudden, unpredictable traffic increases isn’t ideal. 
This chapter covers
Configuring automatic horizontal scaling of pods 
based on CPU utilization
Configuring automatic horizontal scaling of pods 
based on custom metrics
Understanding why vertical scaling of pods isn’t 
possible yet
Understanding automatic horizontal scaling of 
cluster nodes
 
</data>
  <data key="d5">437
Automatic scaling
of pods and cluster nodes
Applications running in pods can be scaled out manually by increasing the
replicas field in the ReplicationController, ReplicaSet, Deployment, or other
scalable resource. Pods can also be scaled vertically by increasing their container’s
resource requests and limits (though this can currently only be done at pod cre-
ation time, not while the pod is running). Although manual scaling is okay for
times when you can anticipate load spikes in advance or when the load changes
gradually over longer periods of time, requiring manual intervention to handle
sudden, unpredictable traffic increases isn’t ideal. 
This chapter covers
Configuring automatic horizontal scaling of pods 
based on CPU utilization
Configuring automatic horizontal scaling of pods 
based on custom metrics
Understanding why vertical scaling of pods isn’t 
possible yet
Understanding automatic horizontal scaling of 
cluster nodes
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="470">
  <data key="d0">Page_470</data>
  <data key="d5">Page_470</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_381">
  <data key="d0">438
CHAPTER 15
Automatic scaling of pods and cluster nodes
 Luckily, Kubernetes can monitor your pods and scale them up automatically as
soon as it detects an increase in the CPU usage or some other metric. If running on a
cloud infrastructure, it can even spin up additional nodes if the existing ones can’t
accept any more pods. This chapter will explain how to get Kubernetes to do both pod
and node autoscaling.
 The autoscaling feature in Kubernetes was completely rewritten between the 1.6
and the 1.7 version, so be aware you may find outdated information on this subject
online.
15.1
Horizontal pod autoscaling
Horizontal pod autoscaling is the automatic scaling of the number of pod replicas man-
aged by a controller. It’s performed by the Horizontal controller, which is enabled and
configured by creating a HorizontalPodAutoscaler (HPA) resource. The controller
periodically checks pod metrics, calculates the number of replicas required to meet
the target metric value configured in the HorizontalPodAutoscaler resource, and
adjusts the replicas field on the target resource (Deployment, ReplicaSet, Replication-
Controller, or StatefulSet). 
15.1.1 Understanding the autoscaling process
The autoscaling process can be split into three steps:
Obtain metrics of all the pods managed by the scaled resource object.
Calculate the number of pods required to bring the metrics to (or close to) the
specified target value.
Update the replicas field of the scaled resource.
Let’s examine all three steps next.
OBTAINING POD METRICS
The Autoscaler doesn’t perform the gathering of the pod metrics itself. It gets the
metrics from a different source. As we saw in the previous chapter, pod and node met-
rics are collected by an agent called cAdvisor, which runs in the Kubelet on each node,
and then aggregated by the cluster-wide component called Heapster. The horizontal
pod autoscaler controller gets the metrics of all the pods by querying Heapster
through REST calls. The flow of metrics data is shown in figure 15.1 (although all the
connections are initiated in the opposite direction).
This implies that Heapster must be running in the cluster for autoscaling to work. If
you’re using Minikube and were following along in the previous chapter, Heapster
Pod(s)
cAdvisor(s)
Horizontal Pod Autoscaler(s)
Heapster
Figure 15.1
Flow of metrics from the pod(s) to the HorizontalPodAutoscaler(s)
 
</data>
  <data key="d5">438
CHAPTER 15
Automatic scaling of pods and cluster nodes
 Luckily, Kubernetes can monitor your pods and scale them up automatically as
soon as it detects an increase in the CPU usage or some other metric. If running on a
cloud infrastructure, it can even spin up additional nodes if the existing ones can’t
accept any more pods. This chapter will explain how to get Kubernetes to do both pod
and node autoscaling.
 The autoscaling feature in Kubernetes was completely rewritten between the 1.6
and the 1.7 version, so be aware you may find outdated information on this subject
online.
15.1
Horizontal pod autoscaling
Horizontal pod autoscaling is the automatic scaling of the number of pod replicas man-
aged by a controller. It’s performed by the Horizontal controller, which is enabled and
configured by creating a HorizontalPodAutoscaler (HPA) resource. The controller
periodically checks pod metrics, calculates the number of replicas required to meet
the target metric value configured in the HorizontalPodAutoscaler resource, and
adjusts the replicas field on the target resource (Deployment, ReplicaSet, Replication-
Controller, or StatefulSet). 
15.1.1 Understanding the autoscaling process
The autoscaling process can be split into three steps:
Obtain metrics of all the pods managed by the scaled resource object.
Calculate the number of pods required to bring the metrics to (or close to) the
specified target value.
Update the replicas field of the scaled resource.
Let’s examine all three steps next.
OBTAINING POD METRICS
The Autoscaler doesn’t perform the gathering of the pod metrics itself. It gets the
metrics from a different source. As we saw in the previous chapter, pod and node met-
rics are collected by an agent called cAdvisor, which runs in the Kubelet on each node,
and then aggregated by the cluster-wide component called Heapster. The horizontal
pod autoscaler controller gets the metrics of all the pods by querying Heapster
through REST calls. The flow of metrics data is shown in figure 15.1 (although all the
connections are initiated in the opposite direction).
This implies that Heapster must be running in the cluster for autoscaling to work. If
you’re using Minikube and were following along in the previous chapter, Heapster
Pod(s)
cAdvisor(s)
Horizontal Pod Autoscaler(s)
Heapster
Figure 15.1
Flow of metrics from the pod(s) to the HorizontalPodAutoscaler(s)
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="471">
  <data key="d0">Page_471</data>
  <data key="d5">Page_471</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_382">
  <data key="d0">439
Horizontal pod autoscaling
should already be enabled in your cluster. If not, make sure to enable the Heapster
add-on before trying out any autoscaling examples.
 Although you don’t need to query Heapster directly, if you’re interested in doing
so, you’ll find both the Heapster Pod and the Service it’s exposed through in the
kube-system namespace. 
CALCULATING THE REQUIRED NUMBER OF PODS
Once the Autoscaler has metrics for all the pods belonging to the resource the Auto-
scaler is scaling (the Deployment, ReplicaSet, ReplicationController, or StatefulSet
resource), it can use those metrics to figure out the required number of replicas. It
needs to find the number that will bring the average value of the metric across all
those replicas as close to the configured target value as possible. The input to this cal-
culation is a set of pod metrics (possibly multiple metrics per pod) and the output is a
single integer (the number of pod replicas). 
 When the Autoscaler is configured to consider only a single metric, calculating the
required replica count is simple. All it takes is summing up the metrics values of all
the pods, dividing that by the target value set on the HorizontalPodAutoscaler
resource, and then rounding it up to the next-larger integer. The actual calculation is
a bit more involved than this, because it also makes sure the Autoscaler doesn’t thrash
around when the metric value is unstable and changes rapidly. 
 When autoscaling is based on multiple pod metrics (for example, both CPU usage
and Queries-Per-Second [QPS]), the calculation isn’t that much more complicated.
The Autoscaler calculates the replica count for each metric individually and then
takes the highest value (for example, if four pods are required to achieve the target
CPU usage, and three pods are required to achieve the target QPS, the Autoscaler will
scale to four pods). Figure 15.2 shows this example.
A look at changes related to how the Autoscaler obtains metrics
Prior to Kubernetes version 1.6, the HorizontalPodAutoscaler obtained the metrics
from Heapster directly. In version 1.8, the Autoscaler can get the metrics through an
aggregated version of the resource metrics API by starting the Controller Manager
with the --horizontal-pod-autoscaler-use-rest-clients=true flag. From ver-
sion 1.9, this behavior will be enabled by default.
The core API server will not expose the metrics itself. From version 1.7, Kubernetes
allows registering multiple API servers and making them appear as a single API
server. This allows it to expose metrics through one of those underlying API servers.
We’ll explain API server aggregation in the last chapter. 
Selecting what metrics collector to use in their clusters will be up to cluster adminis-
trators. A simple translation layer is usually required to expose the metrics in the
appropriate API paths and in the appropriate format.
 
</data>
  <data key="d5">439
Horizontal pod autoscaling
should already be enabled in your cluster. If not, make sure to enable the Heapster
add-on before trying out any autoscaling examples.
 Although you don’t need to query Heapster directly, if you’re interested in doing
so, you’ll find both the Heapster Pod and the Service it’s exposed through in the
kube-system namespace. 
CALCULATING THE REQUIRED NUMBER OF PODS
Once the Autoscaler has metrics for all the pods belonging to the resource the Auto-
scaler is scaling (the Deployment, ReplicaSet, ReplicationController, or StatefulSet
resource), it can use those metrics to figure out the required number of replicas. It
needs to find the number that will bring the average value of the metric across all
those replicas as close to the configured target value as possible. The input to this cal-
culation is a set of pod metrics (possibly multiple metrics per pod) and the output is a
single integer (the number of pod replicas). 
 When the Autoscaler is configured to consider only a single metric, calculating the
required replica count is simple. All it takes is summing up the metrics values of all
the pods, dividing that by the target value set on the HorizontalPodAutoscaler
resource, and then rounding it up to the next-larger integer. The actual calculation is
a bit more involved than this, because it also makes sure the Autoscaler doesn’t thrash
around when the metric value is unstable and changes rapidly. 
 When autoscaling is based on multiple pod metrics (for example, both CPU usage
and Queries-Per-Second [QPS]), the calculation isn’t that much more complicated.
The Autoscaler calculates the replica count for each metric individually and then
takes the highest value (for example, if four pods are required to achieve the target
CPU usage, and three pods are required to achieve the target QPS, the Autoscaler will
scale to four pods). Figure 15.2 shows this example.
A look at changes related to how the Autoscaler obtains metrics
Prior to Kubernetes version 1.6, the HorizontalPodAutoscaler obtained the metrics
from Heapster directly. In version 1.8, the Autoscaler can get the metrics through an
aggregated version of the resource metrics API by starting the Controller Manager
with the --horizontal-pod-autoscaler-use-rest-clients=true flag. From ver-
sion 1.9, this behavior will be enabled by default.
The core API server will not expose the metrics itself. From version 1.7, Kubernetes
allows registering multiple API servers and making them appear as a single API
server. This allows it to expose metrics through one of those underlying API servers.
We’ll explain API server aggregation in the last chapter. 
Selecting what metrics collector to use in their clusters will be up to cluster adminis-
trators. A simple translation layer is usually required to expose the metrics in the
appropriate API paths and in the appropriate format.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="472">
  <data key="d0">Page_472</data>
  <data key="d5">Page_472</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_383">
  <data key="d0">440
CHAPTER 15
Automatic scaling of pods and cluster nodes
UPDATING THE DESIRED REPLICA COUNT ON THE SCALED RESOURCE
The final step of an autoscaling operation is updating the desired replica count field
on the scaled resource object (a ReplicaSet, for example) and then letting the Replica-
Set controller take care of spinning up additional pods or deleting excess ones.
 The Autoscaler controller modifies the replicas field of the scaled resource
through the Scale sub-resource. It enables the Autoscaler to do its work without know-
ing any details of the resource it’s scaling, except for what’s exposed through the Scale
sub-resource (see figure 15.3).
This allows the Autoscaler to operate on any scalable resource, as long as the API
server exposes the Scale sub-resource for it. Currently, it’s exposed for
Deployments
ReplicaSets
ReplicationControllers
StatefulSets
These are currently the only objects you can attach an Autoscaler to.
Pod 1
CPU
utilization
QPS
Pod 2
Pod 3
Target
CPU utilization
Target QPS
Replicas: 4
Replicas: 3
Replicas: 4
30
12
15
20
(15 + 30 + 12) / 20 = 57 / 20
(60 + 90 + 50) / 50 = 200 / 50
Max(4, 3)
50%
60%
90%
50%
Figure 15.2
Calculating the number of replicas from two metrics
Autoscaler adjusts replicas (++ or --)
Horizontal Pod Autoscaler
Deployment, ReplicaSet,
StatefulSet, or
ReplicationController
Scale
sub-resource
Figure 15.3
The Horizontal Pod Autoscaler modifies only on the Scale sub-resource.
 
</data>
  <data key="d5">440
CHAPTER 15
Automatic scaling of pods and cluster nodes
UPDATING THE DESIRED REPLICA COUNT ON THE SCALED RESOURCE
The final step of an autoscaling operation is updating the desired replica count field
on the scaled resource object (a ReplicaSet, for example) and then letting the Replica-
Set controller take care of spinning up additional pods or deleting excess ones.
 The Autoscaler controller modifies the replicas field of the scaled resource
through the Scale sub-resource. It enables the Autoscaler to do its work without know-
ing any details of the resource it’s scaling, except for what’s exposed through the Scale
sub-resource (see figure 15.3).
This allows the Autoscaler to operate on any scalable resource, as long as the API
server exposes the Scale sub-resource for it. Currently, it’s exposed for
Deployments
ReplicaSets
ReplicationControllers
StatefulSets
These are currently the only objects you can attach an Autoscaler to.
Pod 1
CPU
utilization
QPS
Pod 2
Pod 3
Target
CPU utilization
Target QPS
Replicas: 4
Replicas: 3
Replicas: 4
30
12
15
20
(15 + 30 + 12) / 20 = 57 / 20
(60 + 90 + 50) / 50 = 200 / 50
Max(4, 3)
50%
60%
90%
50%
Figure 15.2
Calculating the number of replicas from two metrics
Autoscaler adjusts replicas (++ or --)
Horizontal Pod Autoscaler
Deployment, ReplicaSet,
StatefulSet, or
ReplicationController
Scale
sub-resource
Figure 15.3
The Horizontal Pod Autoscaler modifies only on the Scale sub-resource.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="473">
  <data key="d0">Page_473</data>
  <data key="d5">Page_473</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_384">
  <data key="d0">441
Horizontal pod autoscaling
UNDERSTANDING THE WHOLE AUTOSCALING PROCESS
You now understand the three steps involved in autoscaling, so let’s visualize all the
components involved in the autoscaling process. They’re shown in figure 15.4.
The arrows leading from the pods to the cAdvisors, which continue on to Heapster
and finally to the Horizontal Pod Autoscaler, indicate the direction of the flow of met-
rics data. It’s important to be aware that each component gets the metrics from the
other components periodically (that is, cAdvisor gets the metrics from the pods in a
continuous loop; the same is also true for Heapster and for the HPA controller). The
end effect is that it takes quite a while for the metrics data to be propagated and a res-
caling action to be performed. It isn’t immediate. Keep this in mind when you observe
the Autoscaler in action next.
15.1.2 Scaling based on CPU utilization
Perhaps the most important metric you’ll want to base autoscaling on is the amount of
CPU consumed by the processes running inside your pods. Imagine having a few pods
providing a service. When their CPU usage reaches 100% it’s obvious they can’t cope
with the demand anymore and need to be scaled either up (vertical scaling—increas-
ing the amount of CPU the pods can use) or out (horizontal scaling—increasing the
number of pods). Because we’re talking about the horizontal pod autoscaler here,
Autoscaler adjusts
replicas (++ or --)
Heapster collects
metrics from all nodes
cAdvisor collects metrics
from all containers on a node
Deployment
ReplicaSet
Autoscaler collects
metrics from Heapster
Kubelet
cAdvisor
Node 1
Pod
Pod
Kubelet
cAdvisor
Node 2
Pod
Node X
Heapster
Horizontal Pod
Autoscaler
Figure 15.4
How the autoscaler obtains metrics and rescales the target deployment 
 
</data>
  <data key="d5">441
Horizontal pod autoscaling
UNDERSTANDING THE WHOLE AUTOSCALING PROCESS
You now understand the three steps involved in autoscaling, so let’s visualize all the
components involved in the autoscaling process. They’re shown in figure 15.4.
The arrows leading from the pods to the cAdvisors, which continue on to Heapster
and finally to the Horizontal Pod Autoscaler, indicate the direction of the flow of met-
rics data. It’s important to be aware that each component gets the metrics from the
other components periodically (that is, cAdvisor gets the metrics from the pods in a
continuous loop; the same is also true for Heapster and for the HPA controller). The
end effect is that it takes quite a while for the metrics data to be propagated and a res-
caling action to be performed. It isn’t immediate. Keep this in mind when you observe
the Autoscaler in action next.
15.1.2 Scaling based on CPU utilization
Perhaps the most important metric you’ll want to base autoscaling on is the amount of
CPU consumed by the processes running inside your pods. Imagine having a few pods
providing a service. When their CPU usage reaches 100% it’s obvious they can’t cope
with the demand anymore and need to be scaled either up (vertical scaling—increas-
ing the amount of CPU the pods can use) or out (horizontal scaling—increasing the
number of pods). Because we’re talking about the horizontal pod autoscaler here,
Autoscaler adjusts
replicas (++ or --)
Heapster collects
metrics from all nodes
cAdvisor collects metrics
from all containers on a node
Deployment
ReplicaSet
Autoscaler collects
metrics from Heapster
Kubelet
cAdvisor
Node 1
Pod
Pod
Kubelet
cAdvisor
Node 2
Pod
Node X
Heapster
Horizontal Pod
Autoscaler
Figure 15.4
How the autoscaler obtains metrics and rescales the target deployment 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="474">
  <data key="d0">Page_474</data>
  <data key="d5">Page_474</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_385">
  <data key="d0">442
CHAPTER 15
Automatic scaling of pods and cluster nodes
we’re only focusing on scaling out (increasing the number of pods). By doing that,
the average CPU usage should come down. 
 Because CPU usage is usually unstable, it makes sense to scale out even before the
CPU is completely swamped—perhaps when the average CPU load across the pods
reaches or exceeds 80%. But 80% of what, exactly?
TIP
Always set the target CPU usage well below 100% (and definitely never
above 90%) to leave enough room for handling sudden load spikes.
As you may remember from the previous chapter, the process running inside a con-
tainer is guaranteed the amount of CPU requested through the resource requests
specified for the container. But at times when no other processes need CPU, the pro-
cess may use all the available CPU on the node. When someone says a pod is consum-
ing 80% of the CPU, it’s not clear if they mean 80% of the node’s CPU, 80% of the
pod’s guaranteed CPU (the resource request), or 80% of the hard limit configured
for the pod through resource limits. 
 As far as the Autoscaler is concerned, only the pod’s guaranteed CPU amount (the
CPU requests) is important when determining the CPU utilization of a pod. The Auto-
scaler compares the pod’s actual CPU consumption and its CPU requests, which
means the pods you’re autoscaling need to have CPU requests set (either directly or
indirectly through a LimitRange object) for the Autoscaler to determine the CPU uti-
lization percentage.
CREATING A HORIZONTALPODAUTOSCALER BASED ON CPU USAGE
Let’s see how to create a HorizontalPodAutoscaler now and configure it to scale pods
based on their CPU utilization. You’ll create a Deployment similar to the one in chap-
ter 9, but as we’ve discussed, you’ll need to make sure the pods created by the Deploy-
ment all have the CPU resource requests specified in order to make autoscaling
possible. You’ll have to add a CPU resource request to the Deployment’s pod tem-
plate, as shown in the following listing.
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3                
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1     
        name: nodejs
Listing 15.1
Deployment with CPU requests set: deployment.yaml
Manually setting the 
(initial) desired number 
of replicas to three
Running the 
kubia:v1 image
 
</data>
  <data key="d5">442
CHAPTER 15
Automatic scaling of pods and cluster nodes
we’re only focusing on scaling out (increasing the number of pods). By doing that,
the average CPU usage should come down. 
 Because CPU usage is usually unstable, it makes sense to scale out even before the
CPU is completely swamped—perhaps when the average CPU load across the pods
reaches or exceeds 80%. But 80% of what, exactly?
TIP
Always set the target CPU usage well below 100% (and definitely never
above 90%) to leave enough room for handling sudden load spikes.
As you may remember from the previous chapter, the process running inside a con-
tainer is guaranteed the amount of CPU requested through the resource requests
specified for the container. But at times when no other processes need CPU, the pro-
cess may use all the available CPU on the node. When someone says a pod is consum-
ing 80% of the CPU, it’s not clear if they mean 80% of the node’s CPU, 80% of the
pod’s guaranteed CPU (the resource request), or 80% of the hard limit configured
for the pod through resource limits. 
 As far as the Autoscaler is concerned, only the pod’s guaranteed CPU amount (the
CPU requests) is important when determining the CPU utilization of a pod. The Auto-
scaler compares the pod’s actual CPU consumption and its CPU requests, which
means the pods you’re autoscaling need to have CPU requests set (either directly or
indirectly through a LimitRange object) for the Autoscaler to determine the CPU uti-
lization percentage.
CREATING A HORIZONTALPODAUTOSCALER BASED ON CPU USAGE
Let’s see how to create a HorizontalPodAutoscaler now and configure it to scale pods
based on their CPU utilization. You’ll create a Deployment similar to the one in chap-
ter 9, but as we’ve discussed, you’ll need to make sure the pods created by the Deploy-
ment all have the CPU resource requests specified in order to make autoscaling
possible. You’ll have to add a CPU resource request to the Deployment’s pod tem-
plate, as shown in the following listing.
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3                
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1     
        name: nodejs
Listing 15.1
Deployment with CPU requests set: deployment.yaml
Manually setting the 
(initial) desired number 
of replicas to three
Running the 
kubia:v1 image
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="475">
  <data key="d0">Page_475</data>
  <data key="d5">Page_475</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_386">
  <data key="d0">443
Horizontal pod autoscaling
        resources:              
          requests:             
            cpu: 100m           
This is a regular Deployment object—it doesn’t use autoscaling yet. It will run three
instances of the kubia NodeJS app, with each instance requesting 100 millicores
of CPU. 
 After creating the Deployment, to enable horizontal autoscaling of its pods, you
need to create a HorizontalPodAutoscaler (HPA) object and point it to the Deploy-
ment. You could prepare and post the YAML manifest for the HPA, but an easier way
exists—using the kubectl autoscale command:
$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5
deployment "kubia" autoscaled
This creates the HPA object for you and sets the Deployment called kubia as the scal-
ing target. You’re setting the target CPU utilization of the pods to 30% and specifying
the minimum and maximum number of replicas. The Autoscaler will constantly keep
adjusting the number of replicas to keep their CPU utilization around 30%, but it will
never scale down to less than one or scale up to more than five replicas. 
TIP
Always make sure to autoscale Deployments instead of the underlying
ReplicaSets. This way, you ensure the desired replica count is preserved across
application updates (remember that a Deployment creates a new ReplicaSet
for each version). The same rule applies to manual scaling, as well.
Let’s look at the definition of the HorizontalPodAutoscaler resource to gain a better
understanding of it. It’s shown in the following listing.
$ kubectl get hpa.v2beta1.autoscaling kubia -o yaml
apiVersion: autoscaling/v2beta1            
kind: HorizontalPodAutoscaler              
metadata:
  name: kubia               
  ...
spec:
  maxReplicas: 5                   
  metrics:                              
  - resource:                           
      name: cpu                         
      targetAverageUtilization: 30      
    type: Resource                      
  minReplicas: 1                   
  scaleTargetRef:                          
    apiVersion: extensions/v1beta1         
    kind: Deployment                       
    name: kubia                            
Listing 15.2
A HorizontalPodAutoscaler YAML definition
Requesting 100 millicores 
of CPU per pod
HPA resources are in the 
autoscaling API group.
Each HPA has a name (it doesn’t 
need to match the name of the 
Deployment as in this case).
The
minimum
and
maximum
number of
replicas
you
specified
You’d like the Autoscaler to 
adjust the number of pods 
so they each utilize 30% of 
requested CPU.
The target resource 
which this Autoscaler 
will act upon
 
</data>
  <data key="d5">443
Horizontal pod autoscaling
        resources:              
          requests:             
            cpu: 100m           
This is a regular Deployment object—it doesn’t use autoscaling yet. It will run three
instances of the kubia NodeJS app, with each instance requesting 100 millicores
of CPU. 
 After creating the Deployment, to enable horizontal autoscaling of its pods, you
need to create a HorizontalPodAutoscaler (HPA) object and point it to the Deploy-
ment. You could prepare and post the YAML manifest for the HPA, but an easier way
exists—using the kubectl autoscale command:
$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5
deployment "kubia" autoscaled
This creates the HPA object for you and sets the Deployment called kubia as the scal-
ing target. You’re setting the target CPU utilization of the pods to 30% and specifying
the minimum and maximum number of replicas. The Autoscaler will constantly keep
adjusting the number of replicas to keep their CPU utilization around 30%, but it will
never scale down to less than one or scale up to more than five replicas. 
TIP
Always make sure to autoscale Deployments instead of the underlying
ReplicaSets. This way, you ensure the desired replica count is preserved across
application updates (remember that a Deployment creates a new ReplicaSet
for each version). The same rule applies to manual scaling, as well.
Let’s look at the definition of the HorizontalPodAutoscaler resource to gain a better
understanding of it. It’s shown in the following listing.
$ kubectl get hpa.v2beta1.autoscaling kubia -o yaml
apiVersion: autoscaling/v2beta1            
kind: HorizontalPodAutoscaler              
metadata:
  name: kubia               
  ...
spec:
  maxReplicas: 5                   
  metrics:                              
  - resource:                           
      name: cpu                         
      targetAverageUtilization: 30      
    type: Resource                      
  minReplicas: 1                   
  scaleTargetRef:                          
    apiVersion: extensions/v1beta1         
    kind: Deployment                       
    name: kubia                            
Listing 15.2
A HorizontalPodAutoscaler YAML definition
Requesting 100 millicores 
of CPU per pod
HPA resources are in the 
autoscaling API group.
Each HPA has a name (it doesn’t 
need to match the name of the 
Deployment as in this case).
The
minimum
and
maximum
number of
replicas
you
specified
You’d like the Autoscaler to 
adjust the number of pods 
so they each utilize 30% of 
requested CPU.
The target resource 
which this Autoscaler 
will act upon
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="476">
  <data key="d0">Page_476</data>
  <data key="d5">Page_476</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_387">
  <data key="d0">444
CHAPTER 15
Automatic scaling of pods and cluster nodes
status:
  currentMetrics: []        
  currentReplicas: 3        
  desiredReplicas: 0        
NOTE
Multiple versions of HPA resources exist: the new autoscaling/v2beta1
and the old autoscaling/v1. You’re requesting the new version here.
SEEING THE FIRST AUTOMATIC RESCALE EVENT
It takes a while for cAdvisor to get the CPU metrics and for Heapster to collect them
before the Autoscaler can take action. During that time, if you display the HPA resource
with kubectl get, the TARGETS column will show &lt;unknown&gt;:
$ kubectl get hpa
NAME      REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS
kubia     Deployment/kubia   &lt;unknown&gt; / 30%   1         5         0       
Because you’re running three pods that are currently receiving no requests, which
means their CPU usage should be close to zero, you should expect the Autoscaler to
scale them down to a single pod, because even with a single pod, the CPU utilization
will still be below the 30% target. 
 And sure enough, the autoscaler does exactly that. It soon scales the Deployment
down to a single replica:
$ kubectl get deployment
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubia     1         1         1            1           23m
Remember, the autoscaler only adjusts the desired replica count on the Deployment.
The Deployment controller then takes care of updating the desired replica count on
the ReplicaSet object, which then causes the ReplicaSet controller to delete two excess
pods, leaving one pod running.
 You can use kubectl describe to see more information on the HorizontalPod-
Autoscaler and the operation of the underlying controller, as the following listing shows.
$ kubectl describe hpa
Name:                             kubia
Namespace:                        default
Labels:                           &lt;none&gt;
Annotations:                      &lt;none&gt;
CreationTimestamp:                Sat, 03 Jun 2017 12:59:57 +0200
Reference:                        Deployment/kubia
Metrics:                          ( current / target )
  resource cpu on pods  
  (as a percentage of request):   0% (0) / 30%
Min replicas:                     1
Max replicas:                     5
Listing 15.3
Inspecting a HorizontalPodAutoscaler with kubectl describe
The current status 
of the Autoscaler
 
</data>
  <data key="d5">444
CHAPTER 15
Automatic scaling of pods and cluster nodes
status:
  currentMetrics: []        
  currentReplicas: 3        
  desiredReplicas: 0        
NOTE
Multiple versions of HPA resources exist: the new autoscaling/v2beta1
and the old autoscaling/v1. You’re requesting the new version here.
SEEING THE FIRST AUTOMATIC RESCALE EVENT
It takes a while for cAdvisor to get the CPU metrics and for Heapster to collect them
before the Autoscaler can take action. During that time, if you display the HPA resource
with kubectl get, the TARGETS column will show &lt;unknown&gt;:
$ kubectl get hpa
NAME      REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS
kubia     Deployment/kubia   &lt;unknown&gt; / 30%   1         5         0       
Because you’re running three pods that are currently receiving no requests, which
means their CPU usage should be close to zero, you should expect the Autoscaler to
scale them down to a single pod, because even with a single pod, the CPU utilization
will still be below the 30% target. 
 And sure enough, the autoscaler does exactly that. It soon scales the Deployment
down to a single replica:
$ kubectl get deployment
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubia     1         1         1            1           23m
Remember, the autoscaler only adjusts the desired replica count on the Deployment.
The Deployment controller then takes care of updating the desired replica count on
the ReplicaSet object, which then causes the ReplicaSet controller to delete two excess
pods, leaving one pod running.
 You can use kubectl describe to see more information on the HorizontalPod-
Autoscaler and the operation of the underlying controller, as the following listing shows.
$ kubectl describe hpa
Name:                             kubia
Namespace:                        default
Labels:                           &lt;none&gt;
Annotations:                      &lt;none&gt;
CreationTimestamp:                Sat, 03 Jun 2017 12:59:57 +0200
Reference:                        Deployment/kubia
Metrics:                          ( current / target )
  resource cpu on pods  
  (as a percentage of request):   0% (0) / 30%
Min replicas:                     1
Max replicas:                     5
Listing 15.3
Inspecting a HorizontalPodAutoscaler with kubectl describe
The current status 
of the Autoscaler
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="477">
  <data key="d0">Page_477</data>
  <data key="d5">Page_477</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_388">
  <data key="d0">445
Horizontal pod autoscaling
Events:
From                        Reason              Message
----                        ------              ---
horizontal-pod-autoscaler   SuccessfulRescale   New size: 1; reason: All 
                                                metrics below target
NOTE
The output has been modified to make it more readable.
Turn your focus to the table of events at the bottom of the listing. You see the horizon-
tal pod autoscaler has successfully rescaled to one replica, because all metrics were
below target. 
TRIGGERING A SCALE-UP
You’ve already witnessed your first automatic rescale event (a scale-down). Now, you’ll
start sending requests to your pod, thereby increasing its CPU usage, and you should
see the autoscaler detect this and start up additional pods.
 You’ll need to expose the pods through a Service, so you can hit all of them through
a single URL. You may remember that the easiest way to do that is with kubectl expose:
$ kubectl expose deployment kubia --port=80 --target-port=8080
service "kubia" exposed
Before you start hitting your pod(s) with requests, you may want to run the follow-
ing command in a separate terminal to keep an eye on what’s happening with the
HorizontalPodAutoscaler and the Deployment, as shown in the following listing.
$ watch -n 1 kubectl get hpa,deployment
Every 
1.0s: 
kubectl 
get 
hpa,deployment 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
NAME        REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS  AGE
hpa/kubia   Deployment/kubia   0% / 30%   1         5         1         45m
NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/kubia   1         1         1            1           56m
TIP
List multiple resource types with kubectl get by delimiting them with
a comma. 
If you’re using OSX, you’ll have to replace the watch command with a loop, manually
run kubectl get periodically, or use kubectl’s --watch option. But although a plain
kubectl get can show multiple types of resources at once, that’s not the case when
using the aforementioned --watch option, so you’ll need to use two terminals if you
want to watch both the HPA and the Deployment objects. 
 Keep an eye on the state of those two objects while you run a load-generating pod.
You’ll run the following command in another terminal:
$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox 
➥ -- sh -c "while true; do wget -O - -q http://kubia.default; done"
Listing 15.4
Watching multiple resources in parallel
 
</data>
  <data key="d5">445
Horizontal pod autoscaling
Events:
From                        Reason              Message
----                        ------              ---
horizontal-pod-autoscaler   SuccessfulRescale   New size: 1; reason: All 
                                                metrics below target
NOTE
The output has been modified to make it more readable.
Turn your focus to the table of events at the bottom of the listing. You see the horizon-
tal pod autoscaler has successfully rescaled to one replica, because all metrics were
below target. 
TRIGGERING A SCALE-UP
You’ve already witnessed your first automatic rescale event (a scale-down). Now, you’ll
start sending requests to your pod, thereby increasing its CPU usage, and you should
see the autoscaler detect this and start up additional pods.
 You’ll need to expose the pods through a Service, so you can hit all of them through
a single URL. You may remember that the easiest way to do that is with kubectl expose:
$ kubectl expose deployment kubia --port=80 --target-port=8080
service "kubia" exposed
Before you start hitting your pod(s) with requests, you may want to run the follow-
ing command in a separate terminal to keep an eye on what’s happening with the
HorizontalPodAutoscaler and the Deployment, as shown in the following listing.
$ watch -n 1 kubectl get hpa,deployment
Every 
1.0s: 
kubectl 
get 
hpa,deployment 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
NAME        REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS  AGE
hpa/kubia   Deployment/kubia   0% / 30%   1         5         1         45m
NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/kubia   1         1         1            1           56m
TIP
List multiple resource types with kubectl get by delimiting them with
a comma. 
If you’re using OSX, you’ll have to replace the watch command with a loop, manually
run kubectl get periodically, or use kubectl’s --watch option. But although a plain
kubectl get can show multiple types of resources at once, that’s not the case when
using the aforementioned --watch option, so you’ll need to use two terminals if you
want to watch both the HPA and the Deployment objects. 
 Keep an eye on the state of those two objects while you run a load-generating pod.
You’ll run the following command in another terminal:
$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox 
➥ -- sh -c "while true; do wget -O - -q http://kubia.default; done"
Listing 15.4
Watching multiple resources in parallel
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="478">
  <data key="d0">Page_478</data>
  <data key="d5">Page_478</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_389">
  <data key="d0">446
CHAPTER 15
Automatic scaling of pods and cluster nodes
This will run a pod which repeatedly hits the kubia Service. You’ve seen the -it
option a few times when running the kubectl exec command. As you can see, it can
also be used with kubectl run. It allows you to attach the console to the process,
which will not only show you the process’ output directly, but will also terminate the
process as soon as you press CTRL+C. The --rm option causes the pod to be deleted
afterward, and the --restart=Never option causes kubectl run to create an unman-
aged pod directly instead of through a Deployment object, which you don’t need.
This combination of options is useful for running commands inside the cluster with-
out having to piggyback on an existing pod. It not only behaves the same as if you
were running the command locally, it even cleans up everything when the command
terminates. 
SEEING THE AUTOSCALER SCALE UP THE DEPLOYMENT
As the load-generator pod runs, you’ll see it initially hitting the single pod. As before,
it takes time for the metrics to be updated, but when they are, you’ll see the autoscaler
increase the number of replicas. In my case, the pod’s CPU utilization initially jumped
to 108%, which caused the autoscaler to increase the number of pods to four. The
utilization on the individual pods then decreased to 74% and then stabilized at
around 26%. 
NOTE
If the CPU load in your case doesn’t exceed 30%, try running addi-
tional load-generators.
Again, you can inspect autoscaler events with kubectl describe to see what the
autoscaler has done (only the most important information is shown in the following
listing).
From    Reason              Message
----    ------              -------
h-p-a   SuccessfulRescale   New size: 1; reason: All metrics below target
h-p-a   SuccessfulRescale   New size: 4; reason: cpu resource utilization 
                            (percentage of request) above target
Does it strike you as odd that the initial average CPU utilization in my case, when I
only had one pod, was 108%, which is more than 100%? Remember, a container’s
CPU utilization is the container’s actual CPU usage divided by its requested CPU. The
requested CPU defines the minimum, not maximum amount of CPU available to the
container, so a container may consume more than the requested CPU, bringing the
percentage over 100. 
 Before we go on, let’s do a little math and see how the autoscaler concluded that
four replicas are needed. Initially, there was one replica handling requests and its
CPU usage spiked to 108%. Dividing 108 by 30 (the target CPU utilization percent-
age) gives 3.6, which the autoscaler then rounded up to 4. If you divide 108 by 4, you
Listing 15.5
Events of a HorizontalPodAutoscaler
 
</data>
  <data key="d5">446
CHAPTER 15
Automatic scaling of pods and cluster nodes
This will run a pod which repeatedly hits the kubia Service. You’ve seen the -it
option a few times when running the kubectl exec command. As you can see, it can
also be used with kubectl run. It allows you to attach the console to the process,
which will not only show you the process’ output directly, but will also terminate the
process as soon as you press CTRL+C. The --rm option causes the pod to be deleted
afterward, and the --restart=Never option causes kubectl run to create an unman-
aged pod directly instead of through a Deployment object, which you don’t need.
This combination of options is useful for running commands inside the cluster with-
out having to piggyback on an existing pod. It not only behaves the same as if you
were running the command locally, it even cleans up everything when the command
terminates. 
SEEING THE AUTOSCALER SCALE UP THE DEPLOYMENT
As the load-generator pod runs, you’ll see it initially hitting the single pod. As before,
it takes time for the metrics to be updated, but when they are, you’ll see the autoscaler
increase the number of replicas. In my case, the pod’s CPU utilization initially jumped
to 108%, which caused the autoscaler to increase the number of pods to four. The
utilization on the individual pods then decreased to 74% and then stabilized at
around 26%. 
NOTE
If the CPU load in your case doesn’t exceed 30%, try running addi-
tional load-generators.
Again, you can inspect autoscaler events with kubectl describe to see what the
autoscaler has done (only the most important information is shown in the following
listing).
From    Reason              Message
----    ------              -------
h-p-a   SuccessfulRescale   New size: 1; reason: All metrics below target
h-p-a   SuccessfulRescale   New size: 4; reason: cpu resource utilization 
                            (percentage of request) above target
Does it strike you as odd that the initial average CPU utilization in my case, when I
only had one pod, was 108%, which is more than 100%? Remember, a container’s
CPU utilization is the container’s actual CPU usage divided by its requested CPU. The
requested CPU defines the minimum, not maximum amount of CPU available to the
container, so a container may consume more than the requested CPU, bringing the
percentage over 100. 
 Before we go on, let’s do a little math and see how the autoscaler concluded that
four replicas are needed. Initially, there was one replica handling requests and its
CPU usage spiked to 108%. Dividing 108 by 30 (the target CPU utilization percent-
age) gives 3.6, which the autoscaler then rounded up to 4. If you divide 108 by 4, you
Listing 15.5
Events of a HorizontalPodAutoscaler
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="479">
  <data key="d0">Page_479</data>
  <data key="d5">Page_479</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_390">
  <data key="d0">447
Horizontal pod autoscaling
get 27%. If the autoscaler scales up to four pods, their average CPU utilization is
expected to be somewhere in the neighborhood of 27%, which is close to the target
value of 30% and almost exactly what the observed CPU utilization was.
UNDERSTANDING THE MAXIMUM RATE OF SCALING
In my case, the CPU usage shot up to 108%, but in general, the initial CPU usage
could spike even higher. Even if the initial average CPU utilization was higher (say
150%), requiring five replicas to achieve the 30% target, the autoscaler would still
only scale up to four pods in the first step, because it has a limit on how many repli-
cas can be added in a single scale-up operation. The autoscaler will at most double
the number of replicas in a single operation, if more than two current replicas
exist. If only one or two exist, it will scale up to a maximum of four replicas in a sin-
gle step. 
 Additionally, it has a limit on how soon a subsequent autoscale operation can
occur after the previous one. Currently, a scale-up will occur only if no rescaling
event occurred in the last three minutes. A scale-down event is performed even less
frequently—every five minutes. Keep this in mind so you don’t wonder why the
autoscaler refuses to perform a rescale operation even if the metrics clearly show
that it should.
MODIFYING THE TARGET METRIC VALUE ON AN EXISTING HPA OBJECT
To wrap up this section, let’s do one last exercise. Maybe your initial CPU utilization
target of 30% was a bit too low, so increase it to 60%. You do this by editing the HPA
resource with the kubectl edit command. When the text editor opens, change the
targetAverageUtilization field to 60, as shown in the following listing.
...
spec:
  maxReplicas: 5
  metrics:
  - resource:
      name: cpu
      targetAverageUtilization: 60    
    type: Resource
...
As with most other resources, after you modify the resource, your changes will be
detected by the autoscaler controller and acted upon. You could also delete the
resource and recreate it with different target values, because by deleting the HPA
resource, you only disable autoscaling of the target resource (a Deployment in this
case) and leave it at the scale it is at that time. The automatic scaling will resume after
you create a new HPA resource for the Deployment.
Listing 15.6
Increasing the target CPU utilization by editing the HPA resource
Change this 
from 30 to 60.
 
</data>
  <data key="d5">447
Horizontal pod autoscaling
get 27%. If the autoscaler scales up to four pods, their average CPU utilization is
expected to be somewhere in the neighborhood of 27%, which is close to the target
value of 30% and almost exactly what the observed CPU utilization was.
UNDERSTANDING THE MAXIMUM RATE OF SCALING
In my case, the CPU usage shot up to 108%, but in general, the initial CPU usage
could spike even higher. Even if the initial average CPU utilization was higher (say
150%), requiring five replicas to achieve the 30% target, the autoscaler would still
only scale up to four pods in the first step, because it has a limit on how many repli-
cas can be added in a single scale-up operation. The autoscaler will at most double
the number of replicas in a single operation, if more than two current replicas
exist. If only one or two exist, it will scale up to a maximum of four replicas in a sin-
gle step. 
 Additionally, it has a limit on how soon a subsequent autoscale operation can
occur after the previous one. Currently, a scale-up will occur only if no rescaling
event occurred in the last three minutes. A scale-down event is performed even less
frequently—every five minutes. Keep this in mind so you don’t wonder why the
autoscaler refuses to perform a rescale operation even if the metrics clearly show
that it should.
MODIFYING THE TARGET METRIC VALUE ON AN EXISTING HPA OBJECT
To wrap up this section, let’s do one last exercise. Maybe your initial CPU utilization
target of 30% was a bit too low, so increase it to 60%. You do this by editing the HPA
resource with the kubectl edit command. When the text editor opens, change the
targetAverageUtilization field to 60, as shown in the following listing.
...
spec:
  maxReplicas: 5
  metrics:
  - resource:
      name: cpu
      targetAverageUtilization: 60    
    type: Resource
...
As with most other resources, after you modify the resource, your changes will be
detected by the autoscaler controller and acted upon. You could also delete the
resource and recreate it with different target values, because by deleting the HPA
resource, you only disable autoscaling of the target resource (a Deployment in this
case) and leave it at the scale it is at that time. The automatic scaling will resume after
you create a new HPA resource for the Deployment.
Listing 15.6
Increasing the target CPU utilization by editing the HPA resource
Change this 
from 30 to 60.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="480">
  <data key="d0">Page_480</data>
  <data key="d5">Page_480</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_391">
  <data key="d0">448
CHAPTER 15
Automatic scaling of pods and cluster nodes
15.1.3 Scaling based on memory consumption
You’ve seen how easily the horizontal Autoscaler can be configured to keep CPU uti-
lization at the target level. But what about autoscaling based on the pods’ memory
usage? 
 Memory-based autoscaling is much more problematic than CPU-based autoscal-
ing. The main reason is because after scaling up, the old pods would somehow need to
be forced to release memory. This needs to be done by the app itself—it can’t be done
by the system. All the system could do is kill and restart the app, hoping it would use
less memory than before. But if the app then uses the same amount as before, the
Autoscaler would scale it up again. And again, and again, until it reaches the maxi-
mum number of pods configured on the HPA resource. Obviously, this isn’t what any-
one wants. Memory-based autoscaling was introduced in Kubernetes version 1.8, and
is configured exactly like CPU-based autoscaling. Exploring it is left up to the reader.
15.1.4 Scaling based on other and custom metrics
You’ve seen how easy it is to scale pods based on their CPU usage. Initially, this was the
only autoscaling option that was usable in practice. To have the autoscaler use custom,
app-defined metrics to drive its autoscaling decisions was fairly complicated. The ini-
tial design of the autoscaler didn’t make it easy to move beyond simple CPU-based
scaling. This prompted the Kubernetes Autoscaling Special Interest Group (SIG) to
redesign the autoscaler completely. 
 If you’re interested in learning how complicated it was to use the initial autoscaler
with custom metrics, I invite you to read my blog post entitled “Kubernetes autoscal-
ing based on custom metrics without using a host port,” which you’ll find online at
http:/
/medium.com/@marko.luksa. You’ll learn about all the other problems I
encountered when trying to set up autoscaling based on custom metrics. Luckily,
newer versions of Kubernetes don’t have those problems. I’ll cover the subject in a
new blog post. 
 Instead of going through a complete example here, let’s quickly go over how to
configure the autoscaler to use different metrics sources. We’ll start by examining how
we defined what metric to use in our previous example. The following listing shows
how your previous HPA object was configured to use the CPU usage metric.
...
spec:
  maxReplicas: 5
  metrics:
  - type: Resource      
    resource:
      name: cpu                      
      targetAverageUtilization: 30    
...
Listing 15.7
HorizontalPodAutoscaler definition for CPU-based autoscaling
Defines the type 
of metric
The resource, whose 
utilization will be monitored
The target utilization 
of this resource
 
</data>
  <data key="d5">448
CHAPTER 15
Automatic scaling of pods and cluster nodes
15.1.3 Scaling based on memory consumption
You’ve seen how easily the horizontal Autoscaler can be configured to keep CPU uti-
lization at the target level. But what about autoscaling based on the pods’ memory
usage? 
 Memory-based autoscaling is much more problematic than CPU-based autoscal-
ing. The main reason is because after scaling up, the old pods would somehow need to
be forced to release memory. This needs to be done by the app itself—it can’t be done
by the system. All the system could do is kill and restart the app, hoping it would use
less memory than before. But if the app then uses the same amount as before, the
Autoscaler would scale it up again. And again, and again, until it reaches the maxi-
mum number of pods configured on the HPA resource. Obviously, this isn’t what any-
one wants. Memory-based autoscaling was introduced in Kubernetes version 1.8, and
is configured exactly like CPU-based autoscaling. Exploring it is left up to the reader.
15.1.4 Scaling based on other and custom metrics
You’ve seen how easy it is to scale pods based on their CPU usage. Initially, this was the
only autoscaling option that was usable in practice. To have the autoscaler use custom,
app-defined metrics to drive its autoscaling decisions was fairly complicated. The ini-
tial design of the autoscaler didn’t make it easy to move beyond simple CPU-based
scaling. This prompted the Kubernetes Autoscaling Special Interest Group (SIG) to
redesign the autoscaler completely. 
 If you’re interested in learning how complicated it was to use the initial autoscaler
with custom metrics, I invite you to read my blog post entitled “Kubernetes autoscal-
ing based on custom metrics without using a host port,” which you’ll find online at
http:/
/medium.com/@marko.luksa. You’ll learn about all the other problems I
encountered when trying to set up autoscaling based on custom metrics. Luckily,
newer versions of Kubernetes don’t have those problems. I’ll cover the subject in a
new blog post. 
 Instead of going through a complete example here, let’s quickly go over how to
configure the autoscaler to use different metrics sources. We’ll start by examining how
we defined what metric to use in our previous example. The following listing shows
how your previous HPA object was configured to use the CPU usage metric.
...
spec:
  maxReplicas: 5
  metrics:
  - type: Resource      
    resource:
      name: cpu                      
      targetAverageUtilization: 30    
...
Listing 15.7
HorizontalPodAutoscaler definition for CPU-based autoscaling
Defines the type 
of metric
The resource, whose 
utilization will be monitored
The target utilization 
of this resource
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="481">
  <data key="d0">Page_481</data>
  <data key="d5">Page_481</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_392">
  <data key="d0">449
Horizontal pod autoscaling
As you can see, the metrics field allows you to define more than one metric to use.
In the listing, you’re using a single metric. Each entry defines the type of metric—
in this case, a Resource metric. You have three types of metrics you can use in an
HPA object:

Resource

Pods

Object
UNDERSTANDING THE RESOURCE METRIC TYPE
The Resource type makes the autoscaler base its autoscaling decisions on a resource
metric, like the ones specified in a container’s resource requests. We’ve already seen
how to do that, so let’s focus on the other two types.
UNDERSTANDING THE PODS METRIC TYPE
The Pods type is used to refer to any other (including custom) metric related to the
pod directly. An example of such a metric could be the already mentioned Queries-
Per-Second (QPS) or the number of messages in a message broker’s queue (when the
message broker is running as a pod). To configure the autoscaler to use the pod’s QPS
metric, the HPA object would need to include the entry shown in the following listing
under its metrics field.
...
spec:
  metrics:
  - type: Pods              
    resource:
      metricName: qps             
      targetAverageValue: 100    
...
The example in the listing configures the autoscaler to keep the average QPS of all
the pods managed by the ReplicaSet (or other) controller targeted by this HPA
resource at 100. 
UNDERSTANDING THE OBJECT METRIC TYPE
The Object metric type is used when you want to make the autoscaler scale pods
based on a metric that doesn’t pertain directly to those pods. For example, you may
want to scale pods according to a metric of another cluster object, such as an Ingress
object. The metric could be QPS as in listing 15.8, the average request latency, or
something else completely. 
 Unlike in the previous case, where the autoscaler needed to obtain the metric for
all targeted pods and then use the average of those values, when you use an Object
metric type, the autoscaler obtains a single metric from the single object. In the HPA
Listing 15.8
Referring to a custom pod metric in the HPA
Defines a pod metric
The name of 
the metric
The target average value 
across all targeted pods
 
</data>
  <data key="d5">449
Horizontal pod autoscaling
As you can see, the metrics field allows you to define more than one metric to use.
In the listing, you’re using a single metric. Each entry defines the type of metric—
in this case, a Resource metric. You have three types of metrics you can use in an
HPA object:

Resource

Pods

Object
UNDERSTANDING THE RESOURCE METRIC TYPE
The Resource type makes the autoscaler base its autoscaling decisions on a resource
metric, like the ones specified in a container’s resource requests. We’ve already seen
how to do that, so let’s focus on the other two types.
UNDERSTANDING THE PODS METRIC TYPE
The Pods type is used to refer to any other (including custom) metric related to the
pod directly. An example of such a metric could be the already mentioned Queries-
Per-Second (QPS) or the number of messages in a message broker’s queue (when the
message broker is running as a pod). To configure the autoscaler to use the pod’s QPS
metric, the HPA object would need to include the entry shown in the following listing
under its metrics field.
...
spec:
  metrics:
  - type: Pods              
    resource:
      metricName: qps             
      targetAverageValue: 100    
...
The example in the listing configures the autoscaler to keep the average QPS of all
the pods managed by the ReplicaSet (or other) controller targeted by this HPA
resource at 100. 
UNDERSTANDING THE OBJECT METRIC TYPE
The Object metric type is used when you want to make the autoscaler scale pods
based on a metric that doesn’t pertain directly to those pods. For example, you may
want to scale pods according to a metric of another cluster object, such as an Ingress
object. The metric could be QPS as in listing 15.8, the average request latency, or
something else completely. 
 Unlike in the previous case, where the autoscaler needed to obtain the metric for
all targeted pods and then use the average of those values, when you use an Object
metric type, the autoscaler obtains a single metric from the single object. In the HPA
Listing 15.8
Referring to a custom pod metric in the HPA
Defines a pod metric
The name of 
the metric
The target average value 
across all targeted pods
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="482">
  <data key="d0">Page_482</data>
  <data key="d5">Page_482</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_393">
  <data key="d0">450
CHAPTER 15
Automatic scaling of pods and cluster nodes
definition, you need to specify the target object and the target value. The following
listing shows an example.
...
spec:
  metrics:
  - type: Object                   
    resource:
      metricName: latencyMillis           
      target: 
        apiVersion: extensions/v1beta1     
        kind: Ingress                      
        name: frontend                     
      targetValue: 20                   
  scaleTargetRef:                          
    apiVersion: extensions/v1beta1         
    kind: Deployment                       
    name: kubia                            
...
In this example, the HPA is configured to use the latencyMillis metric of the
frontend Ingress object. The target value for the metric is 20. The horizontal pod
autoscaler will monitor the Ingress’ metric and if it rises too far above the target value,
the autoscaler will scale the kubia Deployment resource. 
15.1.5 Determining which metrics are appropriate for autoscaling
You need to understand that not all metrics are appropriate for use as the basis of
autoscaling. As mentioned previously, the pods’ containers’ memory consumption isn’t
a good metric for autoscaling. The autoscaler won’t function properly if increasing
the number of replicas doesn’t result in a linear decrease of the average value of the
observed metric (or at least close to linear). 
 For example, if you have only a single pod instance and the value of the metric is X
and the autoscaler scales up to two replicas, the metric needs to fall to somewhere
close to X/2. An example of such a custom metric is Queries per Second (QPS),
which in the case of web applications reports the number of requests the application
is receiving per second. Increasing the number of replicas will always result in a pro-
portionate decrease of QPS, because a greater number of pods will be handling the
same total number of requests. 
 Before you decide to base the autoscaler on your app’s own custom metric, be sure
to think about how its value will behave when the number of pods increases or
decreases.
15.1.6 Scaling down to zero replicas
The horizontal pod autoscaler currently doesn’t allow setting the minReplicas field
to 0, so the autoscaler will never scale down to zero, even if the pods aren’t doing
Listing 15.9
Referring to a metric of a different object in the HPA
Use metric of a 
specific object
The name of 
the metric
The specific object whose metric 
the autoscaler should obtain
The
Autoscaler
should
scale so
the value
of the
metric
stays close
to this.
The scalable resource the 
autoscaler will scale
 
</data>
  <data key="d5">450
CHAPTER 15
Automatic scaling of pods and cluster nodes
definition, you need to specify the target object and the target value. The following
listing shows an example.
...
spec:
  metrics:
  - type: Object                   
    resource:
      metricName: latencyMillis           
      target: 
        apiVersion: extensions/v1beta1     
        kind: Ingress                      
        name: frontend                     
      targetValue: 20                   
  scaleTargetRef:                          
    apiVersion: extensions/v1beta1         
    kind: Deployment                       
    name: kubia                            
...
In this example, the HPA is configured to use the latencyMillis metric of the
frontend Ingress object. The target value for the metric is 20. The horizontal pod
autoscaler will monitor the Ingress’ metric and if it rises too far above the target value,
the autoscaler will scale the kubia Deployment resource. 
15.1.5 Determining which metrics are appropriate for autoscaling
You need to understand that not all metrics are appropriate for use as the basis of
autoscaling. As mentioned previously, the pods’ containers’ memory consumption isn’t
a good metric for autoscaling. The autoscaler won’t function properly if increasing
the number of replicas doesn’t result in a linear decrease of the average value of the
observed metric (or at least close to linear). 
 For example, if you have only a single pod instance and the value of the metric is X
and the autoscaler scales up to two replicas, the metric needs to fall to somewhere
close to X/2. An example of such a custom metric is Queries per Second (QPS),
which in the case of web applications reports the number of requests the application
is receiving per second. Increasing the number of replicas will always result in a pro-
portionate decrease of QPS, because a greater number of pods will be handling the
same total number of requests. 
 Before you decide to base the autoscaler on your app’s own custom metric, be sure
to think about how its value will behave when the number of pods increases or
decreases.
15.1.6 Scaling down to zero replicas
The horizontal pod autoscaler currently doesn’t allow setting the minReplicas field
to 0, so the autoscaler will never scale down to zero, even if the pods aren’t doing
Listing 15.9
Referring to a metric of a different object in the HPA
Use metric of a 
specific object
The name of 
the metric
The specific object whose metric 
the autoscaler should obtain
The
Autoscaler
should
scale so
the value
of the
metric
stays close
to this.
The scalable resource the 
autoscaler will scale
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="483">
  <data key="d0">Page_483</data>
  <data key="d5">Page_483</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_394">
  <data key="d0">451
Vertical pod autoscaling
anything. Allowing the number of pods to be scaled down to zero can dramatically
increase the utilization of your hardware. When you run services that get requests only
once every few hours or even days, it doesn’t make sense to have them running all the
time, eating up resources that could be used by other pods. But you still want to have
those services available immediately when a client request comes in. 
 This is known as idling and un-idling. It allows pods that provide a certain service
to be scaled down to zero. When a new request comes in, the request is blocked until
the pod is brought up and then the request is finally forwarded to the pod. 
 Kubernetes currently doesn’t provide this feature yet, but it will eventually. Check
the documentation to see if idling has been implemented yet. 
15.2
Vertical pod autoscaling
Horizontal scaling is great, but not every application can be scaled horizontally. For
such applications, the only option is to scale them vertically—give them more CPU
and/or memory. Because a node usually has more resources than a single pod
requests, it should almost always be possible to scale a pod vertically, right? 
 Because a pod’s resource requests are configured through fields in the pod
manifest, vertically scaling a pod would be performed by changing those fields. I
say “would” because it’s currently not possible to change either resource requests
or limits of existing pods. Before I started writing the book (well over a year ago), I
was sure that by the time I wrote this chapter, Kubernetes would already support
proper vertical pod autoscaling, so I included it in my proposal for the table of con-
tents. Sadly, what seems like a lifetime later, vertical pod autoscaling is still not
available yet. 
15.2.1 Automatically configuring resource requests
An experimental feature sets the CPU and memory requests on newly created pods, if
their containers don’t have them set explicitly. The feature is provided by an Admission
Control plugin called InitialResources. When a new pod without resource requests is
created, the plugin looks at historical resource usage data of the pod’s containers (per
the underlying container image and tag) and sets the requests accordingly. 
 You can deploy pods without specifying resource requests and rely on Kubernetes
to eventually figure out what each container’s resource needs are. Effectively, Kuber-
netes is vertically scaling the pod. For example, if a container keeps running out of
memory, the next time a pod with that container image is created, its resource request
for memory will be set higher automatically.
15.2.2 Modifying resource requests while a pod is running
Eventually, the same mechanism will be used to modify an existing pod’s resource
requests, which means it will vertically scale the pod while it’s running. As I’m writing
this, a new vertical pod autoscaling proposal is being finalized. Please refer to the
 
</data>
  <data key="d5">451
Vertical pod autoscaling
anything. Allowing the number of pods to be scaled down to zero can dramatically
increase the utilization of your hardware. When you run services that get requests only
once every few hours or even days, it doesn’t make sense to have them running all the
time, eating up resources that could be used by other pods. But you still want to have
those services available immediately when a client request comes in. 
 This is known as idling and un-idling. It allows pods that provide a certain service
to be scaled down to zero. When a new request comes in, the request is blocked until
the pod is brought up and then the request is finally forwarded to the pod. 
 Kubernetes currently doesn’t provide this feature yet, but it will eventually. Check
the documentation to see if idling has been implemented yet. 
15.2
Vertical pod autoscaling
Horizontal scaling is great, but not every application can be scaled horizontally. For
such applications, the only option is to scale them vertically—give them more CPU
and/or memory. Because a node usually has more resources than a single pod
requests, it should almost always be possible to scale a pod vertically, right? 
 Because a pod’s resource requests are configured through fields in the pod
manifest, vertically scaling a pod would be performed by changing those fields. I
say “would” because it’s currently not possible to change either resource requests
or limits of existing pods. Before I started writing the book (well over a year ago), I
was sure that by the time I wrote this chapter, Kubernetes would already support
proper vertical pod autoscaling, so I included it in my proposal for the table of con-
tents. Sadly, what seems like a lifetime later, vertical pod autoscaling is still not
available yet. 
15.2.1 Automatically configuring resource requests
An experimental feature sets the CPU and memory requests on newly created pods, if
their containers don’t have them set explicitly. The feature is provided by an Admission
Control plugin called InitialResources. When a new pod without resource requests is
created, the plugin looks at historical resource usage data of the pod’s containers (per
the underlying container image and tag) and sets the requests accordingly. 
 You can deploy pods without specifying resource requests and rely on Kubernetes
to eventually figure out what each container’s resource needs are. Effectively, Kuber-
netes is vertically scaling the pod. For example, if a container keeps running out of
memory, the next time a pod with that container image is created, its resource request
for memory will be set higher automatically.
15.2.2 Modifying resource requests while a pod is running
Eventually, the same mechanism will be used to modify an existing pod’s resource
requests, which means it will vertically scale the pod while it’s running. As I’m writing
this, a new vertical pod autoscaling proposal is being finalized. Please refer to the
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="484">
  <data key="d0">Page_484</data>
  <data key="d5">Page_484</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_395">
  <data key="d0">452
CHAPTER 15
Automatic scaling of pods and cluster nodes
Kubernetes documentation to find out whether vertical pod autoscaling is already
implemented or not.
15.3
Horizontal scaling of cluster nodes
The Horizontal Pod Autoscaler creates additional pod instances when the need for
them arises. But what about when all your nodes are at capacity and can’t run any
more pods? Obviously, this problem isn’t limited only to when new pod instances are
created by the Autoscaler. Even when creating pods manually, you may encounter the
problem where none of the nodes can accept the new pods, because the node’s
resources are used up by existing pods. 
 In that case, you’d need to delete several of those existing pods, scale them down
vertically, or add additional nodes to your cluster. If your Kubernetes cluster is run-
ning on premises, you’d need to physically add a new machine and make it part of the
Kubernetes cluster. But if your cluster is running on a cloud infrastructure, adding
additional nodes is usually a matter of a few clicks or an API call to the cloud infra-
structure. This can be done automatically, right?
 Kubernetes includes the feature to automatically request additional nodes from
the cloud provider as soon as it detects additional nodes are needed. This is per-
formed by the Cluster Autoscaler.
15.3.1 Introducing the Cluster Autoscaler
The Cluster Autoscaler takes care of automatically provisioning additional nodes
when it notices a pod that can’t be scheduled to existing nodes because of a lack of
resources on those nodes. It also de-provisions nodes when they’re underutilized for
longer periods of time. 
REQUESTING ADDITIONAL NODES FROM THE CLOUD INFRASTRUCTURE
A new node will be provisioned if, after a new pod is created, the Scheduler can’t
schedule it to any of the existing nodes. The Cluster Autoscaler looks out for such
pods and asks the cloud provider to start up an additional node. But before doing
that, it checks whether the new node can even accommodate the pod. After all, if
that’s not the case, it makes no sense to start up such a node.
 Cloud providers usually group nodes into groups (or pools) of same-sized nodes
(or nodes having the same features). The Cluster Autoscaler thus can’t simply say
“Give me an additional node.” It needs to also specify the node type.
 The Cluster Autoscaler does this by examining the available node groups to see if
at least one node type would be able to fit the unscheduled pod. If exactly one such
node group exists, the Autoscaler can increase the size of the node group to have the
cloud provider add another node to the group. If more than one option is available,
the Autoscaler must pick the best one. The exact meaning of “best” will obviously
need to be configurable. In the worst case, it selects a random one. A simple overview
of how the cluster Autoscaler reacts to an unschedulable pod is shown in figure 15.5.
 
</data>
  <data key="d5">452
CHAPTER 15
Automatic scaling of pods and cluster nodes
Kubernetes documentation to find out whether vertical pod autoscaling is already
implemented or not.
15.3
Horizontal scaling of cluster nodes
The Horizontal Pod Autoscaler creates additional pod instances when the need for
them arises. But what about when all your nodes are at capacity and can’t run any
more pods? Obviously, this problem isn’t limited only to when new pod instances are
created by the Autoscaler. Even when creating pods manually, you may encounter the
problem where none of the nodes can accept the new pods, because the node’s
resources are used up by existing pods. 
 In that case, you’d need to delete several of those existing pods, scale them down
vertically, or add additional nodes to your cluster. If your Kubernetes cluster is run-
ning on premises, you’d need to physically add a new machine and make it part of the
Kubernetes cluster. But if your cluster is running on a cloud infrastructure, adding
additional nodes is usually a matter of a few clicks or an API call to the cloud infra-
structure. This can be done automatically, right?
 Kubernetes includes the feature to automatically request additional nodes from
the cloud provider as soon as it detects additional nodes are needed. This is per-
formed by the Cluster Autoscaler.
15.3.1 Introducing the Cluster Autoscaler
The Cluster Autoscaler takes care of automatically provisioning additional nodes
when it notices a pod that can’t be scheduled to existing nodes because of a lack of
resources on those nodes. It also de-provisions nodes when they’re underutilized for
longer periods of time. 
REQUESTING ADDITIONAL NODES FROM THE CLOUD INFRASTRUCTURE
A new node will be provisioned if, after a new pod is created, the Scheduler can’t
schedule it to any of the existing nodes. The Cluster Autoscaler looks out for such
pods and asks the cloud provider to start up an additional node. But before doing
that, it checks whether the new node can even accommodate the pod. After all, if
that’s not the case, it makes no sense to start up such a node.
 Cloud providers usually group nodes into groups (or pools) of same-sized nodes
(or nodes having the same features). The Cluster Autoscaler thus can’t simply say
“Give me an additional node.” It needs to also specify the node type.
 The Cluster Autoscaler does this by examining the available node groups to see if
at least one node type would be able to fit the unscheduled pod. If exactly one such
node group exists, the Autoscaler can increase the size of the node group to have the
cloud provider add another node to the group. If more than one option is available,
the Autoscaler must pick the best one. The exact meaning of “best” will obviously
need to be configurable. In the worst case, it selects a random one. A simple overview
of how the cluster Autoscaler reacts to an unschedulable pod is shown in figure 15.5.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="485">
  <data key="d0">Page_485</data>
  <data key="d5">Page_485</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_396">
  <data key="d0">453
Horizontal scaling of cluster nodes
When the new node starts up, the Kubelet on that node contacts the API server and
registers the node by creating a Node resource. From then on, the node is part of the
Kubernetes cluster and pods can be scheduled to it.
 Simple, right? What about scaling down?
RELINQUISHING NODES
The Cluster Autoscaler also needs to scale down the number of nodes when they
aren’t being utilized enough. The Autoscaler does this by monitoring the requested
CPU and memory on all the nodes. If the CPU and memory requests of all the pods
running on a given node are below 50%, the node is considered unnecessary. 
 That’s not the only determining factor in deciding whether to bring a node down.
The Autoscaler also checks to see if any system pods are running (only) on that node
(apart from those that are run on every node, because they’re deployed by a Daemon-
Set, for example). If a system pod is running on a node, the node won’t be relinquished.
The same is also true if an unmanaged pod or a pod with local storage is running on the
node, because that would cause disruption to the service the pod is providing. In other
words, a node will only be returned to the cloud provider if the Cluster Autoscaler
knows the pods running on the node will be rescheduled to other nodes.
 When a node is selected to be shut down, the node is first marked as unschedula-
ble and then all the pods running on the node are evicted. Because all those pods
belong to ReplicaSets or other controllers, their replacements are created and sched-
uled to the remaining nodes (that’s why the node that’s being shut down is first
marked as unschedulable).
Node group X
Node X1
1. Autoscaler notices a
Pod can’t be scheduled
to existing nodes
3. Autoscaler scales up the
node group selected in
previous step
2. Autoscaler determines which node
type (if any) would be able to ﬁt the
pod. If multiple types could ﬁt the
pod, it selects one of them.
Cluster
Autoscaler
Pods
Node X2
Pods
Node group Y
Node Y1
Pods
Unschedulable
pod
Figure 15.5
The Cluster Autoscaler scales up when it finds a pod that can’t be scheduled to 
existing nodes.
 
</data>
  <data key="d5">453
Horizontal scaling of cluster nodes
When the new node starts up, the Kubelet on that node contacts the API server and
registers the node by creating a Node resource. From then on, the node is part of the
Kubernetes cluster and pods can be scheduled to it.
 Simple, right? What about scaling down?
RELINQUISHING NODES
The Cluster Autoscaler also needs to scale down the number of nodes when they
aren’t being utilized enough. The Autoscaler does this by monitoring the requested
CPU and memory on all the nodes. If the CPU and memory requests of all the pods
running on a given node are below 50%, the node is considered unnecessary. 
 That’s not the only determining factor in deciding whether to bring a node down.
The Autoscaler also checks to see if any system pods are running (only) on that node
(apart from those that are run on every node, because they’re deployed by a Daemon-
Set, for example). If a system pod is running on a node, the node won’t be relinquished.
The same is also true if an unmanaged pod or a pod with local storage is running on the
node, because that would cause disruption to the service the pod is providing. In other
words, a node will only be returned to the cloud provider if the Cluster Autoscaler
knows the pods running on the node will be rescheduled to other nodes.
 When a node is selected to be shut down, the node is first marked as unschedula-
ble and then all the pods running on the node are evicted. Because all those pods
belong to ReplicaSets or other controllers, their replacements are created and sched-
uled to the remaining nodes (that’s why the node that’s being shut down is first
marked as unschedulable).
Node group X
Node X1
1. Autoscaler notices a
Pod can’t be scheduled
to existing nodes
3. Autoscaler scales up the
node group selected in
previous step
2. Autoscaler determines which node
type (if any) would be able to ﬁt the
pod. If multiple types could ﬁt the
pod, it selects one of them.
Cluster
Autoscaler
Pods
Node X2
Pods
Node group Y
Node Y1
Pods
Unschedulable
pod
Figure 15.5
The Cluster Autoscaler scales up when it finds a pod that can’t be scheduled to 
existing nodes.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="486">
  <data key="d0">Page_486</data>
  <data key="d5">Page_486</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_397">
  <data key="d0">454
CHAPTER 15
Automatic scaling of pods and cluster nodes
15.3.2 Enabling the Cluster Autoscaler
Cluster autoscaling is currently available on
Google Kubernetes Engine (GKE)
Google Compute Engine (GCE)
Amazon Web Services (AWS)
Microsoft Azure
How you start the Autoscaler depends on where your Kubernetes cluster is running.
For your kubia cluster running on GKE, you can enable the Cluster Autoscaler like
this:
$ gcloud container clusters update kubia --enable-autoscaling \
  --min-nodes=3 --max-nodes=5
If your cluster is running on GCE, you need to set three environment variables before
running kube-up.sh: 

KUBE_ENABLE_CLUSTER_AUTOSCALER=true

KUBE_AUTOSCALER_MIN_NODES=3

KUBE_AUTOSCALER_MAX_NODES=5
Refer to the Cluster Autoscaler GitHub repo at https:/
/github.com/kubernetes/auto-
scaler/tree/master/cluster-autoscaler for information on how to enable it on other
platforms. 
NOTE
The Cluster Autoscaler publishes its status to the cluster-autoscaler-
status ConfigMap in the kube-system namespace.
15.3.3 Limiting service disruption during cluster scale-down
When a node fails unexpectedly, nothing you can do will prevent its pods from becom-
ing unavailable. But when a node is shut down voluntarily, either by the Cluster Auto-
scaler or by a human operator, you can make sure the operation doesn’t disrupt the
service provided by the pods running on that node through an additional feature.
Manually cordoning and draining nodes
A node can also be marked as unschedulable and drained manually. Without going
into specifics, this is done with the following kubectl commands:

kubectl cordon &lt;node&gt; marks the node as unschedulable (but doesn’t do
anything with pods running on that node).

kubectl drain &lt;node&gt; marks the node as unschedulable and then evicts all
the pods from the node.
In both cases, no new pods are scheduled to the node until you uncordon it again
with kubectl uncordon &lt;node&gt;.
 
</data>
  <data key="d5">454
CHAPTER 15
Automatic scaling of pods and cluster nodes
15.3.2 Enabling the Cluster Autoscaler
Cluster autoscaling is currently available on
Google Kubernetes Engine (GKE)
Google Compute Engine (GCE)
Amazon Web Services (AWS)
Microsoft Azure
How you start the Autoscaler depends on where your Kubernetes cluster is running.
For your kubia cluster running on GKE, you can enable the Cluster Autoscaler like
this:
$ gcloud container clusters update kubia --enable-autoscaling \
  --min-nodes=3 --max-nodes=5
If your cluster is running on GCE, you need to set three environment variables before
running kube-up.sh: 

KUBE_ENABLE_CLUSTER_AUTOSCALER=true

KUBE_AUTOSCALER_MIN_NODES=3

KUBE_AUTOSCALER_MAX_NODES=5
Refer to the Cluster Autoscaler GitHub repo at https:/
/github.com/kubernetes/auto-
scaler/tree/master/cluster-autoscaler for information on how to enable it on other
platforms. 
NOTE
The Cluster Autoscaler publishes its status to the cluster-autoscaler-
status ConfigMap in the kube-system namespace.
15.3.3 Limiting service disruption during cluster scale-down
When a node fails unexpectedly, nothing you can do will prevent its pods from becom-
ing unavailable. But when a node is shut down voluntarily, either by the Cluster Auto-
scaler or by a human operator, you can make sure the operation doesn’t disrupt the
service provided by the pods running on that node through an additional feature.
Manually cordoning and draining nodes
A node can also be marked as unschedulable and drained manually. Without going
into specifics, this is done with the following kubectl commands:

kubectl cordon &lt;node&gt; marks the node as unschedulable (but doesn’t do
anything with pods running on that node).

kubectl drain &lt;node&gt; marks the node as unschedulable and then evicts all
the pods from the node.
In both cases, no new pods are scheduled to the node until you uncordon it again
with kubectl uncordon &lt;node&gt;.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="487">
  <data key="d0">Page_487</data>
  <data key="d5">Page_487</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_398">
  <data key="d0">455
Horizontal scaling of cluster nodes
 Certain services require that a minimum number of pods always keeps running;
this is especially true for quorum-based clustered applications. For this reason, Kuber-
netes provides a way of specifying the minimum number of pods that need to keep
running while performing these types of operations. This is done by creating a Pod-
DisruptionBudget resource.
 Even though the name of the resource sounds complex, it’s one of the simplest
Kubernetes resources available. It contains only a pod label selector and a number
specifying the minimum number of pods that must always be available or, starting
from Kubernetes version 1.7, the maximum number of pods that can be unavailable.
We’ll look at what a PodDisruptionBudget (PDB) resource manifest looks like, but
instead of creating it from a YAML file, you’ll create it with kubectl create pod-
disruptionbudget and then obtain and examine the YAML later.
 If you want to ensure three instances of your kubia pod are always running (they
have the label app=kubia), create the PodDisruptionBudget resource like this:
$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3
poddisruptionbudget "kubia-pdb" created
Simple, right? Now, retrieve the PDB’s YAML. It’s shown in the next listing.
$ kubectl get pdb kubia-pdb -o yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: kubia-pdb
spec:
  minAvailable: 3         
  selector:                
    matchLabels:           
      app: kubia           
status:
  ...
You can also use a percentage instead of an absolute number in the minAvailable
field. For example, you could state that 60% of all pods with the app=kubia label need
to be running at all times.
NOTE
Starting with Kubernetes 1.7, the PodDisruptionBudget resource also
supports the maxUnavailable field, which you can use instead of min-
Available if you want to block evictions when more than that many pods are
unavailable. 
We don’t have much more to say about this resource. As long as it exists, both the
Cluster Autoscaler and the kubectl drain command will adhere to it and will never
evict a pod with the app=kubia label if that would bring the number of such pods
below three. 
Listing 15.10
A PodDisruptionBudget definition
How many pods should 
always be available
The label selector that 
determines which pods 
this budget applies to
 
</data>
  <data key="d5">455
Horizontal scaling of cluster nodes
 Certain services require that a minimum number of pods always keeps running;
this is especially true for quorum-based clustered applications. For this reason, Kuber-
netes provides a way of specifying the minimum number of pods that need to keep
running while performing these types of operations. This is done by creating a Pod-
DisruptionBudget resource.
 Even though the name of the resource sounds complex, it’s one of the simplest
Kubernetes resources available. It contains only a pod label selector and a number
specifying the minimum number of pods that must always be available or, starting
from Kubernetes version 1.7, the maximum number of pods that can be unavailable.
We’ll look at what a PodDisruptionBudget (PDB) resource manifest looks like, but
instead of creating it from a YAML file, you’ll create it with kubectl create pod-
disruptionbudget and then obtain and examine the YAML later.
 If you want to ensure three instances of your kubia pod are always running (they
have the label app=kubia), create the PodDisruptionBudget resource like this:
$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3
poddisruptionbudget "kubia-pdb" created
Simple, right? Now, retrieve the PDB’s YAML. It’s shown in the next listing.
$ kubectl get pdb kubia-pdb -o yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: kubia-pdb
spec:
  minAvailable: 3         
  selector:                
    matchLabels:           
      app: kubia           
status:
  ...
You can also use a percentage instead of an absolute number in the minAvailable
field. For example, you could state that 60% of all pods with the app=kubia label need
to be running at all times.
NOTE
Starting with Kubernetes 1.7, the PodDisruptionBudget resource also
supports the maxUnavailable field, which you can use instead of min-
Available if you want to block evictions when more than that many pods are
unavailable. 
We don’t have much more to say about this resource. As long as it exists, both the
Cluster Autoscaler and the kubectl drain command will adhere to it and will never
evict a pod with the app=kubia label if that would bring the number of such pods
below three. 
Listing 15.10
A PodDisruptionBudget definition
How many pods should 
always be available
The label selector that 
determines which pods 
this budget applies to
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="488">
  <data key="d0">Page_488</data>
  <data key="d5">Page_488</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_399">
  <data key="d0">456
CHAPTER 15
Automatic scaling of pods and cluster nodes
 For example, if there were four pods altogether and minAvailable was set to three
as in the example, the pod eviction process would evict pods one by one, waiting for
the evicted pod to be replaced with a new one by the ReplicaSet controller, before
evicting another pod. 
15.4
Summary
This chapter has shown you how Kubernetes can scale not only your pods, but also
your nodes. You’ve learned that
Configuring the automatic horizontal scaling of pods is as easy as creating a
HorizontalPodAutoscaler object and pointing it to a Deployment, ReplicaSet,
or ReplicationController and specifying the target CPU utilization for the pods.
Besides having the Horizontal Pod Autoscaler perform scaling operations based
on the pods’ CPU utilization, you can also configure it to scale based on your
own application-provided custom metrics or metrics related to other objects
deployed in the cluster.
Vertical pod autoscaling isn’t possible yet.
Even cluster nodes can be scaled automatically if your Kubernetes cluster runs
on a supported cloud provider.
You can run one-off processes in a pod and have the pod stopped and deleted
automatically as soon you press CTRL+C by using kubectl run with the -it and
--rm options.
In the next chapter, you’ll explore advanced scheduling features, such as how to keep
certain pods away from certain nodes and how to schedule pods either close together
or apart.
 
</data>
  <data key="d5">456
CHAPTER 15
Automatic scaling of pods and cluster nodes
 For example, if there were four pods altogether and minAvailable was set to three
as in the example, the pod eviction process would evict pods one by one, waiting for
the evicted pod to be replaced with a new one by the ReplicaSet controller, before
evicting another pod. 
15.4
Summary
This chapter has shown you how Kubernetes can scale not only your pods, but also
your nodes. You’ve learned that
Configuring the automatic horizontal scaling of pods is as easy as creating a
HorizontalPodAutoscaler object and pointing it to a Deployment, ReplicaSet,
or ReplicationController and specifying the target CPU utilization for the pods.
Besides having the Horizontal Pod Autoscaler perform scaling operations based
on the pods’ CPU utilization, you can also configure it to scale based on your
own application-provided custom metrics or metrics related to other objects
deployed in the cluster.
Vertical pod autoscaling isn’t possible yet.
Even cluster nodes can be scaled automatically if your Kubernetes cluster runs
on a supported cloud provider.
You can run one-off processes in a pod and have the pod stopped and deleted
automatically as soon you press CTRL+C by using kubectl run with the -it and
--rm options.
In the next chapter, you’ll explore advanced scheduling features, such as how to keep
certain pods away from certain nodes and how to schedule pods either close together
or apart.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="489">
  <data key="d0">Page_489</data>
  <data key="d5">Page_489</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_400">
  <data key="d0">457
Advanced scheduling
Kubernetes allows you to affect where pods are scheduled. Initially, this was only
done by specifying a node selector in the pod specification, but additional mech-
anisms were later added that expanded this functionality. They’re covered in this
chapter.
16.1
Using taints and tolerations to repel pods from 
certain nodes
The first two features related to advanced scheduling that we’ll explore here are
the node taints and pods’ tolerations of those taints. They’re used for restricting
This chapter covers
Using node taints and pod tolerations to keep 
pods away from certain nodes
Defining node affinity rules as an alternative to 
node selectors
Co-locating pods using pod affinity 
Keeping pods away from each other using pod 
anti-affinity
 
</data>
  <data key="d5">457
Advanced scheduling
Kubernetes allows you to affect where pods are scheduled. Initially, this was only
done by specifying a node selector in the pod specification, but additional mech-
anisms were later added that expanded this functionality. They’re covered in this
chapter.
16.1
Using taints and tolerations to repel pods from 
certain nodes
The first two features related to advanced scheduling that we’ll explore here are
the node taints and pods’ tolerations of those taints. They’re used for restricting
This chapter covers
Using node taints and pod tolerations to keep 
pods away from certain nodes
Defining node affinity rules as an alternative to 
node selectors
Co-locating pods using pod affinity 
Keeping pods away from each other using pod 
anti-affinity
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="490">
  <data key="d0">Page_490</data>
  <data key="d5">Page_490</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_401">
  <data key="d0">458
CHAPTER 16
Advanced scheduling
which pods can use a certain node. A pod can only be scheduled to a node if it toler-
ates the node’s taints.
 This is somewhat different from using node selectors and node affinity, which
you’ll learn about later in this chapter. Node selectors and node affinity rules make
it possible to select which nodes a pod can or can’t be scheduled to by specifically
adding that information to the pod, whereas taints allow rejecting deployment of
pods to certain nodes by only adding taints to the node without having to modify
existing pods. Pods that you want deployed on a tainted node need to opt in to use
the node, whereas with node selectors, pods explicitly specify which node(s) they
want to be deployed to.
16.1.1 Introducing taints and tolerations
The best path to learn about node taints is to see an existing taint. Appendix B shows
how to set up a multi-node cluster with the kubeadm tool. By default, the master node
in such a cluster is tainted, so only Control Plane pods can be deployed on it. 
DISPLAYING A NODE’S TAINTS
You can see the node’s taints using kubectl describe node, as shown in the follow-
ing listing.
$ kubectl describe node master.k8s
Name:         master.k8s
Role:
Labels:       beta.kubernetes.io/arch=amd64
              beta.kubernetes.io/os=linux
              kubernetes.io/hostname=master.k8s
              node-role.kubernetes.io/master=
Annotations:  node.alpha.kubernetes.io/ttl=0
              volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:       node-role.kubernetes.io/master:NoSchedule      
...
The master node has a single taint. Taints have a key, value, and an effect, and are repre-
sented as &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;. The master node’s taint shown in the previous
listing has the key node-role.kubernetes.io/master, a null value (not shown in the
taint), and the effect of NoSchedule. 
 This taint prevents pods from being scheduled to the master node, unless those pods
tolerate this taint. The pods that tolerate it are usually system pods (see figure 16.1).
 
 
 
 
Listing 16.1
Describing the master node in a cluster created with kubeadm
The master node 
has one taint.
 
</data>
  <data key="d5">458
CHAPTER 16
Advanced scheduling
which pods can use a certain node. A pod can only be scheduled to a node if it toler-
ates the node’s taints.
 This is somewhat different from using node selectors and node affinity, which
you’ll learn about later in this chapter. Node selectors and node affinity rules make
it possible to select which nodes a pod can or can’t be scheduled to by specifically
adding that information to the pod, whereas taints allow rejecting deployment of
pods to certain nodes by only adding taints to the node without having to modify
existing pods. Pods that you want deployed on a tainted node need to opt in to use
the node, whereas with node selectors, pods explicitly specify which node(s) they
want to be deployed to.
16.1.1 Introducing taints and tolerations
The best path to learn about node taints is to see an existing taint. Appendix B shows
how to set up a multi-node cluster with the kubeadm tool. By default, the master node
in such a cluster is tainted, so only Control Plane pods can be deployed on it. 
DISPLAYING A NODE’S TAINTS
You can see the node’s taints using kubectl describe node, as shown in the follow-
ing listing.
$ kubectl describe node master.k8s
Name:         master.k8s
Role:
Labels:       beta.kubernetes.io/arch=amd64
              beta.kubernetes.io/os=linux
              kubernetes.io/hostname=master.k8s
              node-role.kubernetes.io/master=
Annotations:  node.alpha.kubernetes.io/ttl=0
              volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:       node-role.kubernetes.io/master:NoSchedule      
...
The master node has a single taint. Taints have a key, value, and an effect, and are repre-
sented as &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;. The master node’s taint shown in the previous
listing has the key node-role.kubernetes.io/master, a null value (not shown in the
taint), and the effect of NoSchedule. 
 This taint prevents pods from being scheduled to the master node, unless those pods
tolerate this taint. The pods that tolerate it are usually system pods (see figure 16.1).
 
 
 
 
Listing 16.1
Describing the master node in a cluster created with kubeadm
The master node 
has one taint.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="491">
  <data key="d0">Page_491</data>
  <data key="d5">Page_491</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_402">
  <data key="d0">459
Using taints and tolerations to repel pods from certain nodes
DISPLAYING A POD’S TOLERATIONS
In a cluster installed with kubeadm, the kube-proxy cluster component runs as a pod
on every node, including the master node, because master components that run as
pods may also need to access Kubernetes Services. To make sure the kube-proxy pod
also runs on the master node, it includes the appropriate toleration. In total, the pod
has three tolerations, which are shown in the following listing.
$ kubectl describe po kube-proxy-80wqm -n kube-system
...
Tolerations:    node-role.kubernetes.io/master=:NoSchedule
                node.alpha.kubernetes.io/notReady=:Exists:NoExecute
                node.alpha.kubernetes.io/unreachable=:Exists:NoExecute
...
As you can see, the first toleration matches the master node’s taint, allowing this kube-
proxy pod to be scheduled to the master node. 
NOTE
Disregard the equal sign, which is shown in the pod’s tolerations, but
not in the node’s taints. Kubectl apparently displays taints and tolerations dif-
ferently when the taint’s/toleration’s value is null.
UNDERSTANDING TAINT EFFECTS
The two other tolerations on the kube-proxy pod define how long the pod is allowed
to run on nodes that aren’t ready or are unreachable (the time in seconds isn’t shown,
Listing 16.2
A pod’s tolerations
System pod may be
scheduled to master
node because its
toleration matches
the node’s taint.
System pod
Master node
Taint:
node-role.kubernetes.io
/master:NoSchedule
Toleration:
node-role.kubernetes.io
/master:NoSchedule
Regular pod
Regular node
No taints
No tolerations
Pods with no tolerations
may only be scheduled
to nodes without taints.
Figure 16.1
A pod is only scheduled to a node if it tolerates the node’s taints.
 
</data>
  <data key="d5">459
Using taints and tolerations to repel pods from certain nodes
DISPLAYING A POD’S TOLERATIONS
In a cluster installed with kubeadm, the kube-proxy cluster component runs as a pod
on every node, including the master node, because master components that run as
pods may also need to access Kubernetes Services. To make sure the kube-proxy pod
also runs on the master node, it includes the appropriate toleration. In total, the pod
has three tolerations, which are shown in the following listing.
$ kubectl describe po kube-proxy-80wqm -n kube-system
...
Tolerations:    node-role.kubernetes.io/master=:NoSchedule
                node.alpha.kubernetes.io/notReady=:Exists:NoExecute
                node.alpha.kubernetes.io/unreachable=:Exists:NoExecute
...
As you can see, the first toleration matches the master node’s taint, allowing this kube-
proxy pod to be scheduled to the master node. 
NOTE
Disregard the equal sign, which is shown in the pod’s tolerations, but
not in the node’s taints. Kubectl apparently displays taints and tolerations dif-
ferently when the taint’s/toleration’s value is null.
UNDERSTANDING TAINT EFFECTS
The two other tolerations on the kube-proxy pod define how long the pod is allowed
to run on nodes that aren’t ready or are unreachable (the time in seconds isn’t shown,
Listing 16.2
A pod’s tolerations
System pod may be
scheduled to master
node because its
toleration matches
the node’s taint.
System pod
Master node
Taint:
node-role.kubernetes.io
/master:NoSchedule
Toleration:
node-role.kubernetes.io
/master:NoSchedule
Regular pod
Regular node
No taints
No tolerations
Pods with no tolerations
may only be scheduled
to nodes without taints.
Figure 16.1
A pod is only scheduled to a node if it tolerates the node’s taints.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="492">
  <data key="d0">Page_492</data>
  <data key="d5">Page_492</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_403">
  <data key="d0">460
CHAPTER 16
Advanced scheduling
but can be seen in the pod’s YAML). Those two tolerations refer to the NoExecute
instead of the NoSchedule effect. 
 Each taint has an effect associated with it. Three possible effects exist:

NoSchedule, which means pods won’t be scheduled to the node if they don’t tol-
erate the taint.

PreferNoSchedule is a soft version of NoSchedule, meaning the scheduler will
try to avoid scheduling the pod to the node, but will schedule it to the node if it
can’t schedule it somewhere else. 

NoExecute, unlike NoSchedule and PreferNoSchedule that only affect schedul-
ing, also affects pods already running on the node. If you add a NoExecute taint
to a node, pods that are already running on that node and don’t tolerate the
NoExecute taint will be evicted from the node. 
16.1.2 Adding custom taints to a node
Imagine having a single Kubernetes cluster where you run both production and non-
production workloads. It’s of the utmost importance that non-production pods never
run on the production nodes. This can be achieved by adding a taint to your produc-
tion nodes. To add a taint, you use the kubectl taint command:
$ kubectl taint node node1.k8s node-type=production:NoSchedule
node "node1.k8s" tainted
This adds a taint with key node-type, value production and the NoSchedule effect. If
you now deploy multiple replicas of a regular pod, you’ll see none of them are sched-
uled to the node you tainted, as shown in the following listing.
$ kubectl run test --image busybox --replicas 5 -- sleep 99999
deployment "test" created
$ kubectl get po -o wide
NAME                READY  STATUS    RESTARTS   AGE   IP          NODE
test-196686-46ngl   1/1    Running   0          12s   10.47.0.1   node2.k8s
test-196686-73p89   1/1    Running   0          12s   10.47.0.7   node2.k8s
test-196686-77280   1/1    Running   0          12s   10.47.0.6   node2.k8s
test-196686-h9m8f   1/1    Running   0          12s   10.47.0.5   node2.k8s
test-196686-p85ll   1/1    Running   0          12s   10.47.0.4   node2.k8s
Now, no one can inadvertently deploy pods onto the production nodes. 
16.1.3 Adding tolerations to pods
To deploy production pods to the production nodes, they need to tolerate the taint
you added to the nodes. The manifests of your production pods need to include the
YAML snippet shown in the following listing.
 
Listing 16.3
Deploying pods without a toleration
 
</data>
  <data key="d5">460
CHAPTER 16
Advanced scheduling
but can be seen in the pod’s YAML). Those two tolerations refer to the NoExecute
instead of the NoSchedule effect. 
 Each taint has an effect associated with it. Three possible effects exist:

NoSchedule, which means pods won’t be scheduled to the node if they don’t tol-
erate the taint.

PreferNoSchedule is a soft version of NoSchedule, meaning the scheduler will
try to avoid scheduling the pod to the node, but will schedule it to the node if it
can’t schedule it somewhere else. 

NoExecute, unlike NoSchedule and PreferNoSchedule that only affect schedul-
ing, also affects pods already running on the node. If you add a NoExecute taint
to a node, pods that are already running on that node and don’t tolerate the
NoExecute taint will be evicted from the node. 
16.1.2 Adding custom taints to a node
Imagine having a single Kubernetes cluster where you run both production and non-
production workloads. It’s of the utmost importance that non-production pods never
run on the production nodes. This can be achieved by adding a taint to your produc-
tion nodes. To add a taint, you use the kubectl taint command:
$ kubectl taint node node1.k8s node-type=production:NoSchedule
node "node1.k8s" tainted
This adds a taint with key node-type, value production and the NoSchedule effect. If
you now deploy multiple replicas of a regular pod, you’ll see none of them are sched-
uled to the node you tainted, as shown in the following listing.
$ kubectl run test --image busybox --replicas 5 -- sleep 99999
deployment "test" created
$ kubectl get po -o wide
NAME                READY  STATUS    RESTARTS   AGE   IP          NODE
test-196686-46ngl   1/1    Running   0          12s   10.47.0.1   node2.k8s
test-196686-73p89   1/1    Running   0          12s   10.47.0.7   node2.k8s
test-196686-77280   1/1    Running   0          12s   10.47.0.6   node2.k8s
test-196686-h9m8f   1/1    Running   0          12s   10.47.0.5   node2.k8s
test-196686-p85ll   1/1    Running   0          12s   10.47.0.4   node2.k8s
Now, no one can inadvertently deploy pods onto the production nodes. 
16.1.3 Adding tolerations to pods
To deploy production pods to the production nodes, they need to tolerate the taint
you added to the nodes. The manifests of your production pods need to include the
YAML snippet shown in the following listing.
 
Listing 16.3
Deploying pods without a toleration
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="493">
  <data key="d0">Page_493</data>
  <data key="d5">Page_493</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_404">
  <data key="d0">461
Using taints and tolerations to repel pods from certain nodes
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prod
spec:
  replicas: 5
  template:
    spec:
      ...
      tolerations:
      - key: node-type         
        Operator: Equal        
        value: production      
        effect: NoSchedule     
If you deploy this Deployment, you’ll see its pods get deployed to the production
node, as shown in the next listing.
$ kubectl get po -o wide
NAME                READY  STATUS    RESTARTS   AGE   IP          NODE
prod-350605-1ph5h   0/1    Running   0          16s   10.44.0.3   node1.k8s
prod-350605-ctqcr   1/1    Running   0          16s   10.47.0.4   node2.k8s
prod-350605-f7pcc   0/1    Running   0          17s   10.44.0.6   node1.k8s
prod-350605-k7c8g   1/1    Running   0          17s   10.47.0.9   node2.k8s
prod-350605-rp1nv   0/1    Running   0          17s   10.44.0.4   node1.k8s
As you can see in the listing, production pods were also deployed to node2, which isn’t
a production node. To prevent that from happening, you’d also need to taint the non-
production nodes with a taint such as node-type=non-production:NoSchedule. Then
you’d also need to add the matching toleration to all your non-production pods.
16.1.4 Understanding what taints and tolerations can be used for
Nodes can have more than one taint and pods can have more than one toleration. As
you’ve seen, taints can only have a key and an effect and don’t require a value. Tolera-
tions can tolerate a specific value by specifying the Equal operator (that’s also the
default operator if you don’t specify one), or they can tolerate any value for a specific
taint key if you use the Exists operator.
USING TAINTS AND TOLERATIONS DURING SCHEDULING
Taints can be used to prevent scheduling of new pods (NoSchedule effect) and to
define unpreferred nodes (PreferNoSchedule effect) and even evict existing pods
from a node (NoExecute).
 You can set up taints and tolerations any way you see fit. For example, you could
partition your cluster into multiple partitions, allowing your development teams to
schedule pods only to their respective nodes. You can also use taints and tolerations
Listing 16.4
A production Deployment with a toleration: production-deployment.yaml
Listing 16.5
Pods with the toleration are deployed on production node1
This toleration allows the 
pod to be scheduled to 
production nodes.
 
</data>
  <data key="d5">461
Using taints and tolerations to repel pods from certain nodes
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prod
spec:
  replicas: 5
  template:
    spec:
      ...
      tolerations:
      - key: node-type         
        Operator: Equal        
        value: production      
        effect: NoSchedule     
If you deploy this Deployment, you’ll see its pods get deployed to the production
node, as shown in the next listing.
$ kubectl get po -o wide
NAME                READY  STATUS    RESTARTS   AGE   IP          NODE
prod-350605-1ph5h   0/1    Running   0          16s   10.44.0.3   node1.k8s
prod-350605-ctqcr   1/1    Running   0          16s   10.47.0.4   node2.k8s
prod-350605-f7pcc   0/1    Running   0          17s   10.44.0.6   node1.k8s
prod-350605-k7c8g   1/1    Running   0          17s   10.47.0.9   node2.k8s
prod-350605-rp1nv   0/1    Running   0          17s   10.44.0.4   node1.k8s
As you can see in the listing, production pods were also deployed to node2, which isn’t
a production node. To prevent that from happening, you’d also need to taint the non-
production nodes with a taint such as node-type=non-production:NoSchedule. Then
you’d also need to add the matching toleration to all your non-production pods.
16.1.4 Understanding what taints and tolerations can be used for
Nodes can have more than one taint and pods can have more than one toleration. As
you’ve seen, taints can only have a key and an effect and don’t require a value. Tolera-
tions can tolerate a specific value by specifying the Equal operator (that’s also the
default operator if you don’t specify one), or they can tolerate any value for a specific
taint key if you use the Exists operator.
USING TAINTS AND TOLERATIONS DURING SCHEDULING
Taints can be used to prevent scheduling of new pods (NoSchedule effect) and to
define unpreferred nodes (PreferNoSchedule effect) and even evict existing pods
from a node (NoExecute).
 You can set up taints and tolerations any way you see fit. For example, you could
partition your cluster into multiple partitions, allowing your development teams to
schedule pods only to their respective nodes. You can also use taints and tolerations
Listing 16.4
A production Deployment with a toleration: production-deployment.yaml
Listing 16.5
Pods with the toleration are deployed on production node1
This toleration allows the 
pod to be scheduled to 
production nodes.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="494">
  <data key="d0">Page_494</data>
  <data key="d5">Page_494</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_405">
  <data key="d0">462
CHAPTER 16
Advanced scheduling
when several of your nodes provide special hardware and only part of your pods need
to use it.
CONFIGURING HOW LONG AFTER A NODE FAILURE A POD IS RESCHEDULED
You can also use a toleration to specify how long Kubernetes should wait before
rescheduling a pod to another node if the node the pod is running on becomes
unready or unreachable. If you look at the tolerations of one of your pods, you’ll see
two tolerations, which are shown in the following listing.
$ kubectl get po prod-350605-1ph5h -o yaml
...
  tolerations:
  - effect: NoExecute                            
    key: node.alpha.kubernetes.io/notReady       
    operator: Exists                             
    tolerationSeconds: 300                       
  - effect: NoExecute                              
    key: node.alpha.kubernetes.io/unreachable      
    operator: Exists                               
    tolerationSeconds: 300                         
These two tolerations say that this pod tolerates a node being notReady or unreach-
able for 300 seconds. The Kubernetes Control Plane, when it detects that a node is no
longer ready or no longer reachable, will wait for 300 seconds before it deletes the
pod and reschedules it to another node.
 These two tolerations are automatically added to pods that don’t define them. If
that five-minute delay is too long for your pods, you can make the delay shorter by
adding those two tolerations to the pod’s spec.
NOTE
This is currently an alpha feature, so it may change in future versions
of Kubernetes. Taint-based evictions also aren’t enabled by default. You enable
them by running the Controller Manager with the --feature-gates=Taint-
BasedEvictions=true option.
16.2
Using node affinity to attract pods to certain nodes
As you’ve learned, taints are used to keep pods away from certain nodes. Now you’ll
learn about a newer mechanism called node affinity, which allows you to tell Kuberne-
tes to schedule pods only to specific subsets of nodes.
COMPARING NODE AFFINITY TO NODE SELECTORS
The initial node affinity mechanism in early versions of Kubernetes was the node-
Selector field in the pod specification. The node had to include all the labels speci-
fied in that field to be eligible to become the target for the pod. 
 Node selectors get the job done and are simple, but they don’t offer everything
that you may need. Because of that, a more powerful mechanism was introduced.
Listing 16.6
Pod with default tolerations
The pod tolerates the node being 
notReady for 300 seconds, before 
it needs to be rescheduled.
The same applies to the 
node being unreachable.
 
</data>
  <data key="d5">462
CHAPTER 16
Advanced scheduling
when several of your nodes provide special hardware and only part of your pods need
to use it.
CONFIGURING HOW LONG AFTER A NODE FAILURE A POD IS RESCHEDULED
You can also use a toleration to specify how long Kubernetes should wait before
rescheduling a pod to another node if the node the pod is running on becomes
unready or unreachable. If you look at the tolerations of one of your pods, you’ll see
two tolerations, which are shown in the following listing.
$ kubectl get po prod-350605-1ph5h -o yaml
...
  tolerations:
  - effect: NoExecute                            
    key: node.alpha.kubernetes.io/notReady       
    operator: Exists                             
    tolerationSeconds: 300                       
  - effect: NoExecute                              
    key: node.alpha.kubernetes.io/unreachable      
    operator: Exists                               
    tolerationSeconds: 300                         
These two tolerations say that this pod tolerates a node being notReady or unreach-
able for 300 seconds. The Kubernetes Control Plane, when it detects that a node is no
longer ready or no longer reachable, will wait for 300 seconds before it deletes the
pod and reschedules it to another node.
 These two tolerations are automatically added to pods that don’t define them. If
that five-minute delay is too long for your pods, you can make the delay shorter by
adding those two tolerations to the pod’s spec.
NOTE
This is currently an alpha feature, so it may change in future versions
of Kubernetes. Taint-based evictions also aren’t enabled by default. You enable
them by running the Controller Manager with the --feature-gates=Taint-
BasedEvictions=true option.
16.2
Using node affinity to attract pods to certain nodes
As you’ve learned, taints are used to keep pods away from certain nodes. Now you’ll
learn about a newer mechanism called node affinity, which allows you to tell Kuberne-
tes to schedule pods only to specific subsets of nodes.
COMPARING NODE AFFINITY TO NODE SELECTORS
The initial node affinity mechanism in early versions of Kubernetes was the node-
Selector field in the pod specification. The node had to include all the labels speci-
fied in that field to be eligible to become the target for the pod. 
 Node selectors get the job done and are simple, but they don’t offer everything
that you may need. Because of that, a more powerful mechanism was introduced.
Listing 16.6
Pod with default tolerations
The pod tolerates the node being 
notReady for 300 seconds, before 
it needs to be rescheduled.
The same applies to the 
node being unreachable.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="495">
  <data key="d0">Page_495</data>
  <data key="d5">Page_495</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_406">
  <data key="d0">463
Using node affinity to attract pods to certain nodes
Node selectors will eventually be deprecated, so it’s important you understand the
new node affinity rules.
 Similar to node selectors, each pod can define its own node affinity rules. These
allow you to specify either hard requirements or preferences. By specifying a prefer-
ence, you tell Kubernetes which nodes you prefer for a specific pod, and Kubernetes
will try to schedule the pod to one of those nodes. If that’s not possible, it will choose
one of the other nodes. 
EXAMINING THE DEFAULT NODE LABELS
Node affinity selects nodes based on their labels, the same way node selectors do.
Before you see how to use node affinity, let’s examine the labels of one of the nodes in
a Google Kubernetes Engine cluster (GKE) to see what the default node labels are.
They’re shown in the following listing.
$ kubectl describe node gke-kubia-default-pool-db274c5a-mjnf
Name:     gke-kubia-default-pool-db274c5a-mjnf
Role:
Labels:   beta.kubernetes.io/arch=amd64
          beta.kubernetes.io/fluentd-ds-ready=true
          beta.kubernetes.io/instance-type=f1-micro
          beta.kubernetes.io/os=linux
          cloud.google.com/gke-nodepool=default-pool
          failure-domain.beta.kubernetes.io/region=europe-west1         
          failure-domain.beta.kubernetes.io/zone=europe-west1-d         
          kubernetes.io/hostname=gke-kubia-default-pool-db274c5a-mjnf   
The node has many labels, but the last three are the most important when it comes to
node affinity and pod affinity, which you’ll learn about later. The meaning of those
three labels is as follows:

failure-domain.beta.kubernetes.io/region specifies the geographical region
the node is located in.

failure-domain.beta.kubernetes.io/zone specifies the availability zone the
node is in.

kubernetes.io/hostname is obviously the node’s hostname.
These and other labels can be used in pod affinity rules. In chapter 3, you already
learned how you can add a custom label to nodes and use it in a pod’s node selector.
You used the custom label to deploy pods only to nodes with that label by adding a node
selector to the pods. Now, you’ll see how to do the same using node affinity rules.
16.2.1 Specifying hard node affinity rules
In the example in chapter 3, you used the node selector to deploy a pod that requires
a GPU only to nodes that have a GPU. The pod spec included the nodeSelector field
shown in the following listing.
Listing 16.7
Default labels of a node in GKE
These three
labels are the
most important
ones related to
node affinity.
 
</data>
  <data key="d5">463
Using node affinity to attract pods to certain nodes
Node selectors will eventually be deprecated, so it’s important you understand the
new node affinity rules.
 Similar to node selectors, each pod can define its own node affinity rules. These
allow you to specify either hard requirements or preferences. By specifying a prefer-
ence, you tell Kubernetes which nodes you prefer for a specific pod, and Kubernetes
will try to schedule the pod to one of those nodes. If that’s not possible, it will choose
one of the other nodes. 
EXAMINING THE DEFAULT NODE LABELS
Node affinity selects nodes based on their labels, the same way node selectors do.
Before you see how to use node affinity, let’s examine the labels of one of the nodes in
a Google Kubernetes Engine cluster (GKE) to see what the default node labels are.
They’re shown in the following listing.
$ kubectl describe node gke-kubia-default-pool-db274c5a-mjnf
Name:     gke-kubia-default-pool-db274c5a-mjnf
Role:
Labels:   beta.kubernetes.io/arch=amd64
          beta.kubernetes.io/fluentd-ds-ready=true
          beta.kubernetes.io/instance-type=f1-micro
          beta.kubernetes.io/os=linux
          cloud.google.com/gke-nodepool=default-pool
          failure-domain.beta.kubernetes.io/region=europe-west1         
          failure-domain.beta.kubernetes.io/zone=europe-west1-d         
          kubernetes.io/hostname=gke-kubia-default-pool-db274c5a-mjnf   
The node has many labels, but the last three are the most important when it comes to
node affinity and pod affinity, which you’ll learn about later. The meaning of those
three labels is as follows:

failure-domain.beta.kubernetes.io/region specifies the geographical region
the node is located in.

failure-domain.beta.kubernetes.io/zone specifies the availability zone the
node is in.

kubernetes.io/hostname is obviously the node’s hostname.
These and other labels can be used in pod affinity rules. In chapter 3, you already
learned how you can add a custom label to nodes and use it in a pod’s node selector.
You used the custom label to deploy pods only to nodes with that label by adding a node
selector to the pods. Now, you’ll see how to do the same using node affinity rules.
16.2.1 Specifying hard node affinity rules
In the example in chapter 3, you used the node selector to deploy a pod that requires
a GPU only to nodes that have a GPU. The pod spec included the nodeSelector field
shown in the following listing.
Listing 16.7
Default labels of a node in GKE
These three
labels are the
most important
ones related to
node affinity.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="496">
  <data key="d0">Page_496</data>
  <data key="d5">Page_496</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_407">
  <data key="d0">464
CHAPTER 16
Advanced scheduling
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  nodeSelector:          
    gpu: "true"          
  ...
The nodeSelector field specifies that the pod should only be deployed on nodes that
include the gpu=true label. If you replace the node selector with a node affinity rule,
the pod definition will look like the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu
            operator: In
            values:
            - "true"
The first thing you’ll notice is that this is much more complicated than a simple node
selector. But that’s because it’s much more expressive. Let’s examine the rule in detail. 
MAKING SENSE OF THE LONG NODEAFFINITY ATTRIBUTE NAME
As you can see, the pod’s spec section contains an affinity field that contains a node-
Affinity field, which contains a field with an extremely long name, so let’s focus on
that first.
 Let’s break it down into two parts and examine what they mean:

requiredDuringScheduling... means the rules defined under this field spec-
ify the labels the node must have for the pod to be scheduled to the node.

...IgnoredDuringExecution means the rules defined under the field don’t
affect pods already executing on the node. 
At this point, let me make things easier for you by letting you know that affinity cur-
rently only affects pod scheduling and never causes a pod to be evicted from a node.
That’s why all the rules right now always end with IgnoredDuringExecution. Eventu-
ally, Kubernetes will also support RequiredDuringExecution, which means that if you
Listing 16.8
A pod using a node selector: kubia-gpu-nodeselector.yaml
Listing 16.9
A pod using a nodeAffinity rule: kubia-gpu-nodeaffinity.yaml
This pod is only scheduled 
to nodes that have the 
gpu=true label.
 
</data>
  <data key="d5">464
CHAPTER 16
Advanced scheduling
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  nodeSelector:          
    gpu: "true"          
  ...
The nodeSelector field specifies that the pod should only be deployed on nodes that
include the gpu=true label. If you replace the node selector with a node affinity rule,
the pod definition will look like the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu
            operator: In
            values:
            - "true"
The first thing you’ll notice is that this is much more complicated than a simple node
selector. But that’s because it’s much more expressive. Let’s examine the rule in detail. 
MAKING SENSE OF THE LONG NODEAFFINITY ATTRIBUTE NAME
As you can see, the pod’s spec section contains an affinity field that contains a node-
Affinity field, which contains a field with an extremely long name, so let’s focus on
that first.
 Let’s break it down into two parts and examine what they mean:

requiredDuringScheduling... means the rules defined under this field spec-
ify the labels the node must have for the pod to be scheduled to the node.

...IgnoredDuringExecution means the rules defined under the field don’t
affect pods already executing on the node. 
At this point, let me make things easier for you by letting you know that affinity cur-
rently only affects pod scheduling and never causes a pod to be evicted from a node.
That’s why all the rules right now always end with IgnoredDuringExecution. Eventu-
ally, Kubernetes will also support RequiredDuringExecution, which means that if you
Listing 16.8
A pod using a node selector: kubia-gpu-nodeselector.yaml
Listing 16.9
A pod using a nodeAffinity rule: kubia-gpu-nodeaffinity.yaml
This pod is only scheduled 
to nodes that have the 
gpu=true label.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="497">
  <data key="d0">Page_497</data>
  <data key="d5">Page_497</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_408">
  <data key="d0">465
Using node affinity to attract pods to certain nodes
remove a label from a node, pods that require the node to have that label will be
evicted from such a node. As I’ve said, that’s not yet supported in Kubernetes, so let’s
not concern ourselves with the second part of that long field any longer.
UNDERSTANDING NODESELECTORTERMS
By keeping what was explained in the previous section in mind, it’s easy to understand
that the nodeSelectorTerms field and the matchExpressions field define which
expressions the node’s labels must match for the pod to be scheduled to the node.
The single expression in the example is simple to understand. The node must have a
gpu label whose value is set to true. 
 This pod will therefore only be scheduled to nodes that have the gpu=true label, as
shown in figure 16.2.
Now comes the more interesting part. Node also affinity allows you to prioritize nodes
during scheduling. We’ll look at that next.
16.2.2 Prioritizing nodes when scheduling a pod
The biggest benefit of the newly introduced node affinity feature is the ability to spec-
ify which nodes the Scheduler should prefer when scheduling a specific pod. This is
done through the preferredDuringSchedulingIgnoredDuringExecution field.
 Imagine having multiple datacenters across different countries. Each datacenter
represents a separate availability zone. In each zone, you have certain machines meant
only for your own use and others that your partner companies can use. You now want
to deploy a few pods and you’d prefer them to be scheduled to zone1 and to the
Node with a GPU
Pod
Node afﬁnity
Required label:
gpu=true
Pod
No node afﬁnity
gpu: true
Node with a GPU
Node without a GPU
Node without a GPU
gpu: true
This pod may be scheduled only
to nodes with gpu=true label
This pod may be
scheduled to any node
Figure 16.2
A pod’s node affinity specifies which labels a node must have for the pod to be 
scheduled to it.
 
</data>
  <data key="d5">465
Using node affinity to attract pods to certain nodes
remove a label from a node, pods that require the node to have that label will be
evicted from such a node. As I’ve said, that’s not yet supported in Kubernetes, so let’s
not concern ourselves with the second part of that long field any longer.
UNDERSTANDING NODESELECTORTERMS
By keeping what was explained in the previous section in mind, it’s easy to understand
that the nodeSelectorTerms field and the matchExpressions field define which
expressions the node’s labels must match for the pod to be scheduled to the node.
The single expression in the example is simple to understand. The node must have a
gpu label whose value is set to true. 
 This pod will therefore only be scheduled to nodes that have the gpu=true label, as
shown in figure 16.2.
Now comes the more interesting part. Node also affinity allows you to prioritize nodes
during scheduling. We’ll look at that next.
16.2.2 Prioritizing nodes when scheduling a pod
The biggest benefit of the newly introduced node affinity feature is the ability to spec-
ify which nodes the Scheduler should prefer when scheduling a specific pod. This is
done through the preferredDuringSchedulingIgnoredDuringExecution field.
 Imagine having multiple datacenters across different countries. Each datacenter
represents a separate availability zone. In each zone, you have certain machines meant
only for your own use and others that your partner companies can use. You now want
to deploy a few pods and you’d prefer them to be scheduled to zone1 and to the
Node with a GPU
Pod
Node afﬁnity
Required label:
gpu=true
Pod
No node afﬁnity
gpu: true
Node with a GPU
Node without a GPU
Node without a GPU
gpu: true
This pod may be scheduled only
to nodes with gpu=true label
This pod may be
scheduled to any node
Figure 16.2
A pod’s node affinity specifies which labels a node must have for the pod to be 
scheduled to it.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="498">
  <data key="d0">Page_498</data>
  <data key="d5">Page_498</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_409">
  <data key="d0">466
CHAPTER 16
Advanced scheduling
machines reserved for your company’s deployments. If those machines don’t have
enough room for the pods or if other important reasons exist that prevent them from
being scheduled there, you’re okay with them being scheduled to the machines your
partners use and to the other zones. Node affinity allows you to do that.
LABELING NODES
First, the nodes need to be labeled appropriately. Each node needs to have a label that
designates the availability zone the node belongs to and a label marking it as either a
dedicated or a shared node.
 Appendix B explains how to set up a three-node cluster (one master and two
worker nodes) in VMs running locally. In the following examples, I’ll use the two worker
nodes in that cluster, but you can also use Google Kubernetes Engine or any other
multi-node cluster. 
NOTE
Minikube isn’t the best choice for running these examples, because it
runs only one node.
First, label the nodes, as shown in the next listing.
$ kubectl label node node1.k8s availability-zone=zone1
node "node1.k8s" labeled
$ kubectl label node node1.k8s share-type=dedicated
node "node1.k8s" labeled
$ kubectl label node node2.k8s availability-zone=zone2
node "node2.k8s" labeled
$ kubectl label node node2.k8s share-type=shared
node "node2.k8s" labeled
$ kubectl get node -L availability-zone -L share-type
NAME         STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE
master.k8s   Ready     4d        v1.6.4    &lt;none&gt;              &lt;none&gt;
node1.k8s    Ready     4d        v1.6.4    zone1               dedicated
node2.k8s    Ready     4d        v1.6.4    zone2               shared
SPECIFYING PREFERENTIAL NODE AFFINITY RULES
With the node labels set up, you can now create a Deployment that prefers dedicated
nodes in zone1. The following listing shows the Deployment manifest.
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pref
spec:
  template:
    ...
    spec:
      affinity:
        nodeAffinity:
Listing 16.10
Labeling nodes
Listing 16.11
Deployment with preferred node affinity: preferred-deployment.yaml
 
</data>
  <data key="d5">466
CHAPTER 16
Advanced scheduling
machines reserved for your company’s deployments. If those machines don’t have
enough room for the pods or if other important reasons exist that prevent them from
being scheduled there, you’re okay with them being scheduled to the machines your
partners use and to the other zones. Node affinity allows you to do that.
LABELING NODES
First, the nodes need to be labeled appropriately. Each node needs to have a label that
designates the availability zone the node belongs to and a label marking it as either a
dedicated or a shared node.
 Appendix B explains how to set up a three-node cluster (one master and two
worker nodes) in VMs running locally. In the following examples, I’ll use the two worker
nodes in that cluster, but you can also use Google Kubernetes Engine or any other
multi-node cluster. 
NOTE
Minikube isn’t the best choice for running these examples, because it
runs only one node.
First, label the nodes, as shown in the next listing.
$ kubectl label node node1.k8s availability-zone=zone1
node "node1.k8s" labeled
$ kubectl label node node1.k8s share-type=dedicated
node "node1.k8s" labeled
$ kubectl label node node2.k8s availability-zone=zone2
node "node2.k8s" labeled
$ kubectl label node node2.k8s share-type=shared
node "node2.k8s" labeled
$ kubectl get node -L availability-zone -L share-type
NAME         STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE
master.k8s   Ready     4d        v1.6.4    &lt;none&gt;              &lt;none&gt;
node1.k8s    Ready     4d        v1.6.4    zone1               dedicated
node2.k8s    Ready     4d        v1.6.4    zone2               shared
SPECIFYING PREFERENTIAL NODE AFFINITY RULES
With the node labels set up, you can now create a Deployment that prefers dedicated
nodes in zone1. The following listing shows the Deployment manifest.
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pref
spec:
  template:
    ...
    spec:
      affinity:
        nodeAffinity:
Listing 16.10
Labeling nodes
Listing 16.11
Deployment with preferred node affinity: preferred-deployment.yaml
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="499">
  <data key="d0">Page_499</data>
  <data key="d5">Page_499</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_410">
  <data key="d0">467
Using node affinity to attract pods to certain nodes
          preferredDuringSchedulingIgnoredDuringExecution:    
          - weight: 80                               
            preference:                              
              matchExpressions:                      
              - key: availability-zone               
                operator: In                         
                values:                              
                - zone1                              
          - weight: 20                     
            preference:                    
              matchExpressions:            
              - key: share-type            
                operator: In               
                values:                    
                - dedicated                
      ...
Let’s examine the listing closely. You’re defining a node affinity preference, instead of
a hard requirement. You want the pods scheduled to nodes that include the labels
availability-zone=zone1 and share-type=dedicated. You’re saying that the first
preference rule is important by setting its weight to 80, whereas the second one is
much less important (weight is set to 20).
UNDERSTANDING HOW NODE PREFERENCES WORK
If your cluster had many nodes, when scheduling the pods of the Deployment in the
previous listing, the nodes would be split into four groups, as shown in figure 16.3.
Nodes whose availability-zone and share-type labels match the pod’s node affin-
ity are ranked the highest. Then, because of how the weights in the pod’s node affinity
rules are configured, next come the shared nodes in zone1, then come the dedicated
nodes in the other zones, and at the lowest priority are all the other nodes.
You’re
specifying
preferences,
not hard
requirements.
You prefer the pod to be 
scheduled to zone1. This 
is your most important 
preference.
You also prefer that your 
pods be scheduled to 
dedicated nodes, but this is 
four times less important 
than your zone preference.
Node
Top priority
Availability zone 1
Pod
Priority: 2
Priority: 3
Priority: 4
Node afﬁnity
Preferred labels:
avail-zone: zone1 (weight 80)
share: dedicated (weight 20)
avail-zone: zone1
share: dedicated
Node
avail-zone: zone1
share: shared
Node
Availability zone 2
avail-zone: zone2
share: dedicated
Node
avail-zone: zone2
share: shared
This pod may be scheduled to
any node, but certain nodes are
preferred based on their labels.
Figure 16.3
Prioritizing nodes based on a pod’s node affinity preferences
 
</data>
  <data key="d5">467
Using node affinity to attract pods to certain nodes
          preferredDuringSchedulingIgnoredDuringExecution:    
          - weight: 80                               
            preference:                              
              matchExpressions:                      
              - key: availability-zone               
                operator: In                         
                values:                              
                - zone1                              
          - weight: 20                     
            preference:                    
              matchExpressions:            
              - key: share-type            
                operator: In               
                values:                    
                - dedicated                
      ...
Let’s examine the listing closely. You’re defining a node affinity preference, instead of
a hard requirement. You want the pods scheduled to nodes that include the labels
availability-zone=zone1 and share-type=dedicated. You’re saying that the first
preference rule is important by setting its weight to 80, whereas the second one is
much less important (weight is set to 20).
UNDERSTANDING HOW NODE PREFERENCES WORK
If your cluster had many nodes, when scheduling the pods of the Deployment in the
previous listing, the nodes would be split into four groups, as shown in figure 16.3.
Nodes whose availability-zone and share-type labels match the pod’s node affin-
ity are ranked the highest. Then, because of how the weights in the pod’s node affinity
rules are configured, next come the shared nodes in zone1, then come the dedicated
nodes in the other zones, and at the lowest priority are all the other nodes.
You’re
specifying
preferences,
not hard
requirements.
You prefer the pod to be 
scheduled to zone1. This 
is your most important 
preference.
You also prefer that your 
pods be scheduled to 
dedicated nodes, but this is 
four times less important 
than your zone preference.
Node
Top priority
Availability zone 1
Pod
Priority: 2
Priority: 3
Priority: 4
Node afﬁnity
Preferred labels:
avail-zone: zone1 (weight 80)
share: dedicated (weight 20)
avail-zone: zone1
share: dedicated
Node
avail-zone: zone1
share: shared
Node
Availability zone 2
avail-zone: zone2
share: dedicated
Node
avail-zone: zone2
share: shared
This pod may be scheduled to
any node, but certain nodes are
preferred based on their labels.
Figure 16.3
Prioritizing nodes based on a pod’s node affinity preferences
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="500">
  <data key="d0">Page_500</data>
  <data key="d5">Page_500</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_411">
  <data key="d0">468
CHAPTER 16
Advanced scheduling
DEPLOYING THE PODS IN THE TWO-NODE CLUSTER
If you create this Deployment in your two-node cluster, you should see most (if not
all) of your pods deployed to node1. Examine the following listing to see if that’s true.
$ kubectl get po -o wide
NAME                READY   STATUS    RESTARTS  AGE   IP          NODE
pref-607515-1rnwv   1/1     Running   0         4m    10.47.0.1   node2.k8s
pref-607515-27wp0   1/1     Running   0         4m    10.44.0.8   node1.k8s
pref-607515-5xd0z   1/1     Running   0         4m    10.44.0.5   node1.k8s
pref-607515-jx9wt   1/1     Running   0         4m    10.44.0.4   node1.k8s
pref-607515-mlgqm   1/1     Running   0         4m    10.44.0.6   node1.k8s
Out of the five pods that were created, four of them landed on node1 and only one
landed on node2. Why did one of them land on node2 instead of node1? The reason is
that besides the node affinity prioritization function, the Scheduler also uses other pri-
oritization functions to decide where to schedule a pod. One of those is the Selector-
SpreadPriority function, which makes sure pods belonging to the same ReplicaSet or
Service are spread around different nodes so a node failure won’t bring the whole ser-
vice down. That’s most likely what caused one of the pods to be scheduled to node2.
 You can try scaling the Deployment up to 20 or more and you’ll see the majority of
pods will be scheduled to node1. In my test, only two out of the 20 were scheduled to
node2. If you hadn’t defined any node affinity preferences, the pods would have been
spread around the two nodes evenly.
16.3
Co-locating pods with pod affinity and anti-affinity
You’ve seen how node affinity rules are used to influence which node a pod is scheduled
to. But these rules only affect the affinity between a pod and a node, whereas sometimes
you’d like to have the ability to specify the affinity between pods themselves. 
 For example, imagine having a frontend and a backend pod. Having those pods
deployed near to each other reduces latency and improves the performance of the
app. You could use node affinity rules to ensure both are deployed to the same node,
rack, or datacenter, but then you’d have to specify exactly which node, rack, or data-
center to schedule them to, which is not the best solution. It’s better to let Kubernetes
deploy your pods anywhere it sees fit, while keeping the frontend and backend pods
close together. This can be achieved using pod affinity. Let’s learn more about it with
an example.
16.3.1 Using inter-pod affinity to deploy pods on the same node
You’ll deploy a backend pod and five frontend pod replicas with pod affinity config-
ured so that they’re all deployed on the same node as the backend pod.
 First, deploy the backend pod:
$ kubectl run backend -l app=backend --image busybox -- sleep 999999
deployment "backend" created
Listing 16.12
Seeing where pods were scheduled
 
</data>
  <data key="d5">468
CHAPTER 16
Advanced scheduling
DEPLOYING THE PODS IN THE TWO-NODE CLUSTER
If you create this Deployment in your two-node cluster, you should see most (if not
all) of your pods deployed to node1. Examine the following listing to see if that’s true.
$ kubectl get po -o wide
NAME                READY   STATUS    RESTARTS  AGE   IP          NODE
pref-607515-1rnwv   1/1     Running   0         4m    10.47.0.1   node2.k8s
pref-607515-27wp0   1/1     Running   0         4m    10.44.0.8   node1.k8s
pref-607515-5xd0z   1/1     Running   0         4m    10.44.0.5   node1.k8s
pref-607515-jx9wt   1/1     Running   0         4m    10.44.0.4   node1.k8s
pref-607515-mlgqm   1/1     Running   0         4m    10.44.0.6   node1.k8s
Out of the five pods that were created, four of them landed on node1 and only one
landed on node2. Why did one of them land on node2 instead of node1? The reason is
that besides the node affinity prioritization function, the Scheduler also uses other pri-
oritization functions to decide where to schedule a pod. One of those is the Selector-
SpreadPriority function, which makes sure pods belonging to the same ReplicaSet or
Service are spread around different nodes so a node failure won’t bring the whole ser-
vice down. That’s most likely what caused one of the pods to be scheduled to node2.
 You can try scaling the Deployment up to 20 or more and you’ll see the majority of
pods will be scheduled to node1. In my test, only two out of the 20 were scheduled to
node2. If you hadn’t defined any node affinity preferences, the pods would have been
spread around the two nodes evenly.
16.3
Co-locating pods with pod affinity and anti-affinity
You’ve seen how node affinity rules are used to influence which node a pod is scheduled
to. But these rules only affect the affinity between a pod and a node, whereas sometimes
you’d like to have the ability to specify the affinity between pods themselves. 
 For example, imagine having a frontend and a backend pod. Having those pods
deployed near to each other reduces latency and improves the performance of the
app. You could use node affinity rules to ensure both are deployed to the same node,
rack, or datacenter, but then you’d have to specify exactly which node, rack, or data-
center to schedule them to, which is not the best solution. It’s better to let Kubernetes
deploy your pods anywhere it sees fit, while keeping the frontend and backend pods
close together. This can be achieved using pod affinity. Let’s learn more about it with
an example.
16.3.1 Using inter-pod affinity to deploy pods on the same node
You’ll deploy a backend pod and five frontend pod replicas with pod affinity config-
ured so that they’re all deployed on the same node as the backend pod.
 First, deploy the backend pod:
$ kubectl run backend -l app=backend --image busybox -- sleep 999999
deployment "backend" created
Listing 16.12
Seeing where pods were scheduled
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="501">
  <data key="d0">Page_501</data>
  <data key="d5">Page_501</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_412">
  <data key="d0">469
Co-locating pods with pod affinity and anti-affinity
This Deployment is not special in any way. The only thing you need to note is the
app=backend label you added to the pod using the -l option. This label is what you’ll
use in the frontend pod’s podAffinity configuration. 
SPECIFYING POD AFFINITY IN A POD DEFINITION
The frontend pod’s definition is shown in the following listing.
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    ...
    spec:
      affinity:
        podAffinity:                                 
          requiredDuringSchedulingIgnoredDuringExecution:   
          - topologyKey: kubernetes.io/hostname           
            labelSelector:                                
              matchLabels:                                
                app: backend                              
      ...
The listing shows that this Deployment will create pods that have a hard requirement
to be deployed on the same node (specified by the topologyKey field) as pods that
have the app=backend label (see figure 16.4).
Listing 16.13
Pod using podAffinity: frontend-podaffinity-host.yaml
Defining 
podAffinity rules
Defining a hard 
requirement, not 
a preference
The pods of this Deployment 
must be deployed on the 
same node as the pods that 
match the selector.
All frontend pods will
be scheduled only to
the node the backend
pod was scheduled to.
Some node
Other nodes
Frontend pods
Backend
pod
Pod afﬁnity
Label selector: app=backend
Topology key: hostname
app: backend
Figure 16.4
Pod affinity allows scheduling pods to the node where other pods 
with a specific label are.
 
</data>
  <data key="d5">469
Co-locating pods with pod affinity and anti-affinity
This Deployment is not special in any way. The only thing you need to note is the
app=backend label you added to the pod using the -l option. This label is what you’ll
use in the frontend pod’s podAffinity configuration. 
SPECIFYING POD AFFINITY IN A POD DEFINITION
The frontend pod’s definition is shown in the following listing.
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    ...
    spec:
      affinity:
        podAffinity:                                 
          requiredDuringSchedulingIgnoredDuringExecution:   
          - topologyKey: kubernetes.io/hostname           
            labelSelector:                                
              matchLabels:                                
                app: backend                              
      ...
The listing shows that this Deployment will create pods that have a hard requirement
to be deployed on the same node (specified by the topologyKey field) as pods that
have the app=backend label (see figure 16.4).
Listing 16.13
Pod using podAffinity: frontend-podaffinity-host.yaml
Defining 
podAffinity rules
Defining a hard 
requirement, not 
a preference
The pods of this Deployment 
must be deployed on the 
same node as the pods that 
match the selector.
All frontend pods will
be scheduled only to
the node the backend
pod was scheduled to.
Some node
Other nodes
Frontend pods
Backend
pod
Pod afﬁnity
Label selector: app=backend
Topology key: hostname
app: backend
Figure 16.4
Pod affinity allows scheduling pods to the node where other pods 
with a specific label are.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="502">
  <data key="d0">Page_502</data>
  <data key="d5">Page_502</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_413">
  <data key="d0">470
CHAPTER 16
Advanced scheduling
NOTE
Instead of the simpler matchLabels field, you could also use the more
expressive matchExpressions field.
DEPLOYING A POD WITH POD AFFINITY
Before you create this Deployment, let’s see which node the backend pod was sched-
uled to earlier:
$ kubectl get po -o wide
NAME                   READY  STATUS   RESTARTS  AGE  IP         NODE
backend-257820-qhqj6   1/1    Running  0         8m   10.47.0.1  node2.k8s
When you create the frontend pods, they should be deployed to node2 as well. You’re
going to create the Deployment and see where the pods are deployed. This is shown
in the next listing.
$ kubectl create -f frontend-podaffinity-host.yaml
deployment "frontend" created
$ kubectl get po -o wide
NAME                   READY  STATUS    RESTARTS  AGE  IP         NODE
backend-257820-qhqj6   1/1    Running   0         8m   10.47.0.1  node2.k8s
frontend-121895-2c1ts  1/1    Running   0         13s  10.47.0.6  node2.k8s
frontend-121895-776m7  1/1    Running   0         13s  10.47.0.4  node2.k8s
frontend-121895-7ffsm  1/1    Running   0         13s  10.47.0.8  node2.k8s
frontend-121895-fpgm6  1/1    Running   0         13s  10.47.0.7  node2.k8s
frontend-121895-vb9ll  1/1    Running   0         13s  10.47.0.5  node2.k8s
All the frontend pods were indeed scheduled to the same node as the backend pod.
When scheduling the frontend pod, the Scheduler first found all the pods that match
the labelSelector defined in the frontend pod’s podAffinity configuration and
then scheduled the frontend pod to the same node.
UNDERSTANDING HOW THE SCHEDULER USES POD AFFINITY RULES
What’s interesting is that if you now delete the backend pod, the Scheduler will sched-
ule the pod to node2 even though it doesn’t define any pod affinity rules itself (the
rules are only on the frontend pods). This makes sense, because otherwise if the back-
end pod were to be deleted by accident and rescheduled to a different node, the fron-
tend pods’ affinity rules would be broken. 
 You can confirm the Scheduler takes other pods’ pod affinity rules into account, if
you increase the Scheduler’s logging level and then check its log. The following listing
shows the relevant log lines.
... Attempting to schedule pod: default/backend-257820-qhqj6
... ...
... backend-qhqj6 -&gt; node2.k8s: Taint Toleration Priority, Score: (10)
Listing 16.14
Deploying frontend pods and seeing which node they’re scheduled to
Listing 16.15
Scheduler log showing why the backend pod is scheduled to node2
 
</data>
  <data key="d5">470
CHAPTER 16
Advanced scheduling
NOTE
Instead of the simpler matchLabels field, you could also use the more
expressive matchExpressions field.
DEPLOYING A POD WITH POD AFFINITY
Before you create this Deployment, let’s see which node the backend pod was sched-
uled to earlier:
$ kubectl get po -o wide
NAME                   READY  STATUS   RESTARTS  AGE  IP         NODE
backend-257820-qhqj6   1/1    Running  0         8m   10.47.0.1  node2.k8s
When you create the frontend pods, they should be deployed to node2 as well. You’re
going to create the Deployment and see where the pods are deployed. This is shown
in the next listing.
$ kubectl create -f frontend-podaffinity-host.yaml
deployment "frontend" created
$ kubectl get po -o wide
NAME                   READY  STATUS    RESTARTS  AGE  IP         NODE
backend-257820-qhqj6   1/1    Running   0         8m   10.47.0.1  node2.k8s
frontend-121895-2c1ts  1/1    Running   0         13s  10.47.0.6  node2.k8s
frontend-121895-776m7  1/1    Running   0         13s  10.47.0.4  node2.k8s
frontend-121895-7ffsm  1/1    Running   0         13s  10.47.0.8  node2.k8s
frontend-121895-fpgm6  1/1    Running   0         13s  10.47.0.7  node2.k8s
frontend-121895-vb9ll  1/1    Running   0         13s  10.47.0.5  node2.k8s
All the frontend pods were indeed scheduled to the same node as the backend pod.
When scheduling the frontend pod, the Scheduler first found all the pods that match
the labelSelector defined in the frontend pod’s podAffinity configuration and
then scheduled the frontend pod to the same node.
UNDERSTANDING HOW THE SCHEDULER USES POD AFFINITY RULES
What’s interesting is that if you now delete the backend pod, the Scheduler will sched-
ule the pod to node2 even though it doesn’t define any pod affinity rules itself (the
rules are only on the frontend pods). This makes sense, because otherwise if the back-
end pod were to be deleted by accident and rescheduled to a different node, the fron-
tend pods’ affinity rules would be broken. 
 You can confirm the Scheduler takes other pods’ pod affinity rules into account, if
you increase the Scheduler’s logging level and then check its log. The following listing
shows the relevant log lines.
... Attempting to schedule pod: default/backend-257820-qhqj6
... ...
... backend-qhqj6 -&gt; node2.k8s: Taint Toleration Priority, Score: (10)
Listing 16.14
Deploying frontend pods and seeing which node they’re scheduled to
Listing 16.15
Scheduler log showing why the backend pod is scheduled to node2
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="503">
  <data key="d0">Page_503</data>
  <data key="d5">Page_503</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_414">
  <data key="d0">471
Co-locating pods with pod affinity and anti-affinity
... backend-qhqj6 -&gt; node1.k8s: Taint Toleration Priority, Score: (10)
... backend-qhqj6 -&gt; node2.k8s: InterPodAffinityPriority, Score: (10)
... backend-qhqj6 -&gt; node1.k8s: InterPodAffinityPriority, Score: (0)
... backend-qhqj6 -&gt; node2.k8s: SelectorSpreadPriority, Score: (10)
... backend-qhqj6 -&gt; node1.k8s: SelectorSpreadPriority, Score: (10)
... backend-qhqj6 -&gt; node2.k8s: NodeAffinityPriority, Score: (0)
... backend-qhqj6 -&gt; node1.k8s: NodeAffinityPriority, Score: (0)
... Host node2.k8s =&gt; Score 100030
... Host node1.k8s =&gt; Score 100022
... Attempting to bind backend-257820-qhqj6 to node2.k8s
If you focus on the two lines in bold, you’ll see that during the scheduling of the back-
end pod, node2 received a higher score than node1 because of inter-pod affinity. 
16.3.2 Deploying pods in the same rack, availability zone, or 
geographic region
In the previous example, you used podAffinity to deploy frontend pods onto the
same node as the backend pods. You probably don’t want all your frontend pods to
run on the same machine, but you’d still like to keep them close to the backend
pod—for example, run them in the same availability zone. 
CO-LOCATING PODS IN THE SAME AVAILABILITY ZONE
The cluster I’m using runs in three VMs on my local machine, so all the nodes are in
the same availability zone, so to speak. But if the nodes were in different zones, all I’d
need to do to run the frontend pods in the same zone as the backend pod would be to
change the topologyKey property to failure-domain.beta.kubernetes.io/zone. 
CO-LOCATING PODS IN THE SAME GEOGRAPHICAL REGION
To allow the pods to be deployed in the same region instead of the same zone (cloud
providers usually have datacenters located in different geographical regions and split
into multiple availability zones in each region), the topologyKey would be set to
failure-domain.beta.kubernetes.io/region.
UNDERSTANDING HOW TOPOLOGYKEY WORKS
The way topologyKey works is simple. The three keys we’ve mentioned so far aren’t
special. If you want, you can easily use your own topologyKey, such as rack, to have
the pods scheduled to the same server rack. The only prerequisite is to add a rack
label to your nodes. This scenario is shown in figure 16.5.
 For example, if you had 20 nodes, with 10 in each rack, you’d label the first ten as
rack=rack1 and the others as rack=rack2. Then, when defining a pod’s podAffinity,
you’d set the toplogyKey to rack. 
 When the Scheduler is deciding where to deploy a pod, it checks the pod’s pod-
Affinity config, finds the pods that match the label selector, and looks up the nodes
they’re running on. Specifically, it looks up the nodes’ label whose key matches the
topologyKey field specified in podAffinity. Then it selects all the nodes whose label
 
</data>
  <data key="d5">471
Co-locating pods with pod affinity and anti-affinity
... backend-qhqj6 -&gt; node1.k8s: Taint Toleration Priority, Score: (10)
... backend-qhqj6 -&gt; node2.k8s: InterPodAffinityPriority, Score: (10)
... backend-qhqj6 -&gt; node1.k8s: InterPodAffinityPriority, Score: (0)
... backend-qhqj6 -&gt; node2.k8s: SelectorSpreadPriority, Score: (10)
... backend-qhqj6 -&gt; node1.k8s: SelectorSpreadPriority, Score: (10)
... backend-qhqj6 -&gt; node2.k8s: NodeAffinityPriority, Score: (0)
... backend-qhqj6 -&gt; node1.k8s: NodeAffinityPriority, Score: (0)
... Host node2.k8s =&gt; Score 100030
... Host node1.k8s =&gt; Score 100022
... Attempting to bind backend-257820-qhqj6 to node2.k8s
If you focus on the two lines in bold, you’ll see that during the scheduling of the back-
end pod, node2 received a higher score than node1 because of inter-pod affinity. 
16.3.2 Deploying pods in the same rack, availability zone, or 
geographic region
In the previous example, you used podAffinity to deploy frontend pods onto the
same node as the backend pods. You probably don’t want all your frontend pods to
run on the same machine, but you’d still like to keep them close to the backend
pod—for example, run them in the same availability zone. 
CO-LOCATING PODS IN THE SAME AVAILABILITY ZONE
The cluster I’m using runs in three VMs on my local machine, so all the nodes are in
the same availability zone, so to speak. But if the nodes were in different zones, all I’d
need to do to run the frontend pods in the same zone as the backend pod would be to
change the topologyKey property to failure-domain.beta.kubernetes.io/zone. 
CO-LOCATING PODS IN THE SAME GEOGRAPHICAL REGION
To allow the pods to be deployed in the same region instead of the same zone (cloud
providers usually have datacenters located in different geographical regions and split
into multiple availability zones in each region), the topologyKey would be set to
failure-domain.beta.kubernetes.io/region.
UNDERSTANDING HOW TOPOLOGYKEY WORKS
The way topologyKey works is simple. The three keys we’ve mentioned so far aren’t
special. If you want, you can easily use your own topologyKey, such as rack, to have
the pods scheduled to the same server rack. The only prerequisite is to add a rack
label to your nodes. This scenario is shown in figure 16.5.
 For example, if you had 20 nodes, with 10 in each rack, you’d label the first ten as
rack=rack1 and the others as rack=rack2. Then, when defining a pod’s podAffinity,
you’d set the toplogyKey to rack. 
 When the Scheduler is deciding where to deploy a pod, it checks the pod’s pod-
Affinity config, finds the pods that match the label selector, and looks up the nodes
they’re running on. Specifically, it looks up the nodes’ label whose key matches the
topologyKey field specified in podAffinity. Then it selects all the nodes whose label
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="504">
  <data key="d0">Page_504</data>
  <data key="d5">Page_504</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_415">
  <data key="d0">472
CHAPTER 16
Advanced scheduling
matches the values of the pods it found earlier. In figure 16.5, the label selector
matched the backend pod, which runs on Node 12. The value of the rack label on
that node equals rack2, so when scheduling a frontend pod, the Scheduler will only
select among the nodes that have the rack=rack2 label.
NOTE
By default, the label selector only matches pods in the same name-
space as the pod that’s being scheduled. But you can also select pods from
other namespaces by adding a namespaces field at the same level as label-
Selector.
16.3.3 Expressing pod affinity preferences instead of hard requirements
Earlier, when we talked about node affinity, you saw that nodeAffinity can be used to
express a hard requirement, which means a pod is only scheduled to nodes that match
the node affinity rules. It can also be used to specify node preferences, to instruct the
Scheduler to schedule the pod to certain nodes, while allowing it to schedule it any-
where else if those nodes can’t fit the pod for any reason.
 The same also applies to podAffinity. You can tell the Scheduler you’d prefer to
have your frontend pods scheduled onto the same node as your backend pod, but if
that’s not possible, you’re okay with them being scheduled elsewhere. An example of
a Deployment using the preferredDuringSchedulingIgnoredDuringExecution pod
affinity rule is shown in the next listing.
Frontend pods will be
scheduled to nodes in
the same rack as the
backend pod.
Node 1
Rack 1
rack: rack1
Node 2
rack: rack1
Node 3
...
rack: rack1
Node 10
rack: rack1
Node 11
Rack 2
rack: rack2
Node 12
rack: rack2
...
Node 20
rack: rack2
Backend
pod
app: backend
Frontend pods
Pod afﬁnity (required)
Label selector: app=backend
Topology key: rack
Figure 16.5
The topologyKey in podAffinity determines the scope of where the pod 
should be scheduled to.
 
</data>
  <data key="d5">472
CHAPTER 16
Advanced scheduling
matches the values of the pods it found earlier. In figure 16.5, the label selector
matched the backend pod, which runs on Node 12. The value of the rack label on
that node equals rack2, so when scheduling a frontend pod, the Scheduler will only
select among the nodes that have the rack=rack2 label.
NOTE
By default, the label selector only matches pods in the same name-
space as the pod that’s being scheduled. But you can also select pods from
other namespaces by adding a namespaces field at the same level as label-
Selector.
16.3.3 Expressing pod affinity preferences instead of hard requirements
Earlier, when we talked about node affinity, you saw that nodeAffinity can be used to
express a hard requirement, which means a pod is only scheduled to nodes that match
the node affinity rules. It can also be used to specify node preferences, to instruct the
Scheduler to schedule the pod to certain nodes, while allowing it to schedule it any-
where else if those nodes can’t fit the pod for any reason.
 The same also applies to podAffinity. You can tell the Scheduler you’d prefer to
have your frontend pods scheduled onto the same node as your backend pod, but if
that’s not possible, you’re okay with them being scheduled elsewhere. An example of
a Deployment using the preferredDuringSchedulingIgnoredDuringExecution pod
affinity rule is shown in the next listing.
Frontend pods will be
scheduled to nodes in
the same rack as the
backend pod.
Node 1
Rack 1
rack: rack1
Node 2
rack: rack1
Node 3
...
rack: rack1
Node 10
rack: rack1
Node 11
Rack 2
rack: rack2
Node 12
rack: rack2
...
Node 20
rack: rack2
Backend
pod
app: backend
Frontend pods
Pod afﬁnity (required)
Label selector: app=backend
Topology key: rack
Figure 16.5
The topologyKey in podAffinity determines the scope of where the pod 
should be scheduled to.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="505">
  <data key="d0">Page_505</data>
  <data key="d5">Page_505</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_416">
  <data key="d0">473
Co-locating pods with pod affinity and anti-affinity
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    ...
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:  
          - weight: 80                                        
            podAffinityTerm:                                  
              topologyKey: kubernetes.io/hostname             
              labelSelector:                                  
                matchLabels:                                  
                  app: backend                                
      containers: ...
As in nodeAffinity preference rules, you need to define a weight for each rule. You
also need to specify the topologyKey and labelSelector, as in the hard-requirement
podAffinity rules. Figure 16.6 shows this scenario.
Deploying this pod, as with your nodeAffinity example, deploys four pods on the same
node as the backend pod, and one pod on the other node (see the following listing).
Listing 16.16
Pod affinity preference
Preferred 
instead of 
Required
A weight and a 
podAffinity term is 
specified as in the 
previous example
The Scheduler will prefer
Node 2 for frontend pods,
but may schedule pods
to Node 1 as well.
Node 1
Node 2
Backend
pod
app: backend
Frontend pod
Pod afﬁnity (preferred)
Label selector: app=backend
Topology key: hostname
hostname: node2
hostname: node1
Figure 16.6
Pod affinity can be used to make the Scheduler prefer nodes where 
pods with a certain label are running. 
 
</data>
  <data key="d5">473
Co-locating pods with pod affinity and anti-affinity
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    ...
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:  
          - weight: 80                                        
            podAffinityTerm:                                  
              topologyKey: kubernetes.io/hostname             
              labelSelector:                                  
                matchLabels:                                  
                  app: backend                                
      containers: ...
As in nodeAffinity preference rules, you need to define a weight for each rule. You
also need to specify the topologyKey and labelSelector, as in the hard-requirement
podAffinity rules. Figure 16.6 shows this scenario.
Deploying this pod, as with your nodeAffinity example, deploys four pods on the same
node as the backend pod, and one pod on the other node (see the following listing).
Listing 16.16
Pod affinity preference
Preferred 
instead of 
Required
A weight and a 
podAffinity term is 
specified as in the 
previous example
The Scheduler will prefer
Node 2 for frontend pods,
but may schedule pods
to Node 1 as well.
Node 1
Node 2
Backend
pod
app: backend
Frontend pod
Pod afﬁnity (preferred)
Label selector: app=backend
Topology key: hostname
hostname: node2
hostname: node1
Figure 16.6
Pod affinity can be used to make the Scheduler prefer nodes where 
pods with a certain label are running. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="506">
  <data key="d0">Page_506</data>
  <data key="d5">Page_506</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_417">
  <data key="d0">474
CHAPTER 16
Advanced scheduling
$ kubectl get po -o wide
NAME                   READY  STATUS   RESTARTS  AGE  IP          NODE
backend-257820-ssrgj   1/1    Running  0         1h   10.47.0.9   node2.k8s
frontend-941083-3mff9  1/1    Running  0         8m   10.44.0.4   node1.k8s
frontend-941083-7fp7d  1/1    Running  0         8m   10.47.0.6   node2.k8s
frontend-941083-cq23b  1/1    Running  0         8m   10.47.0.1   node2.k8s
frontend-941083-m70sw  1/1    Running  0         8m   10.47.0.5   node2.k8s
frontend-941083-wsjv8  1/1    Running  0         8m   10.47.0.4   node2.k8s
16.3.4 Scheduling pods away from each other with pod anti-affinity
You’ve seen how to tell the Scheduler to co-locate pods, but sometimes you may want
the exact opposite. You may want to keep pods away from each other. This is called
pod anti-affinity. It’s specified the same way as pod affinity, except that you use the
podAntiAffinity property instead of podAffinity, which results in the Scheduler
never choosing nodes where pods matching the podAntiAffinity’s label selector are
running, as shown in figure 16.7.
An example of why you’d want to use pod anti-affinity is when two sets of pods inter-
fere with each other’s performance if they run on the same node. In that case, you
want to tell the Scheduler to never schedule those pods on the same node. Another
example would be to force the Scheduler to spread pods of the same group across dif-
ferent availability zones or regions, so that a failure of a whole zone (or region) never
brings the service down completely. 
Listing 16.17
Pods deployed with podAffinity preferences
These pods will NOT be scheduled
to the same node(s) where pods
with app=foo label are running.
Some node
Other nodes
Pods
Pod: foo
Pod
(required)
anti-afﬁnity
Label selector: app=foo
Topology key: hostname
app: foo
Figure 16.7
Using pod anti-affinity to keep pods away from nodes that run pods 
with a certain label.
 
</data>
  <data key="d5">474
CHAPTER 16
Advanced scheduling
$ kubectl get po -o wide
NAME                   READY  STATUS   RESTARTS  AGE  IP          NODE
backend-257820-ssrgj   1/1    Running  0         1h   10.47.0.9   node2.k8s
frontend-941083-3mff9  1/1    Running  0         8m   10.44.0.4   node1.k8s
frontend-941083-7fp7d  1/1    Running  0         8m   10.47.0.6   node2.k8s
frontend-941083-cq23b  1/1    Running  0         8m   10.47.0.1   node2.k8s
frontend-941083-m70sw  1/1    Running  0         8m   10.47.0.5   node2.k8s
frontend-941083-wsjv8  1/1    Running  0         8m   10.47.0.4   node2.k8s
16.3.4 Scheduling pods away from each other with pod anti-affinity
You’ve seen how to tell the Scheduler to co-locate pods, but sometimes you may want
the exact opposite. You may want to keep pods away from each other. This is called
pod anti-affinity. It’s specified the same way as pod affinity, except that you use the
podAntiAffinity property instead of podAffinity, which results in the Scheduler
never choosing nodes where pods matching the podAntiAffinity’s label selector are
running, as shown in figure 16.7.
An example of why you’d want to use pod anti-affinity is when two sets of pods inter-
fere with each other’s performance if they run on the same node. In that case, you
want to tell the Scheduler to never schedule those pods on the same node. Another
example would be to force the Scheduler to spread pods of the same group across dif-
ferent availability zones or regions, so that a failure of a whole zone (or region) never
brings the service down completely. 
Listing 16.17
Pods deployed with podAffinity preferences
These pods will NOT be scheduled
to the same node(s) where pods
with app=foo label are running.
Some node
Other nodes
Pods
Pod: foo
Pod
(required)
anti-afﬁnity
Label selector: app=foo
Topology key: hostname
app: foo
Figure 16.7
Using pod anti-affinity to keep pods away from nodes that run pods 
with a certain label.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="507">
  <data key="d0">Page_507</data>
  <data key="d5">Page_507</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_418">
  <data key="d0">475
Co-locating pods with pod affinity and anti-affinity
USING ANTI-AFFINITY TO SPREAD APART PODS OF THE SAME DEPLOYMENT
Let’s see how to force your frontend pods to be scheduled to different nodes. The fol-
lowing listing shows how the pods’ anti-affinity is configured.
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    metadata:
      labels:                  
        app: frontend          
    spec:
      affinity:
        podAntiAffinity:                                      
          requiredDuringSchedulingIgnoredDuringExecution:     
          - topologyKey: kubernetes.io/hostname            
            labelSelector:                                 
              matchLabels:                                 
                app: frontend                              
      containers: ...
This time, you’re defining podAntiAffinity instead of podAffinity, and you’re mak-
ing the labelSelector match the same pods that the Deployment creates. Let’s see
what happens when you create this Deployment. The pods created by it are shown in
the following listing.
$ kubectl get po -l app=frontend -o wide
NAME                    READY  STATUS   RESTARTS  AGE  IP         NODE
frontend-286632-0lffz   0/1    Pending  0         1m   &lt;none&gt;
frontend-286632-2rkcz   1/1    Running  0         1m   10.47.0.1  node2.k8s
frontend-286632-4nwhp   0/1    Pending  0         1m   &lt;none&gt;
frontend-286632-h4686   0/1    Pending  0         1m   &lt;none&gt;
frontend-286632-st222   1/1    Running  0         1m   10.44.0.4  node1.k8s
As you can see, only two pods were scheduled—one to node1, the other to node2. The
three remaining pods are all Pending, because the Scheduler isn’t allowed to schedule
them to the same nodes.
USING PREFERENTIAL POD ANTI-AFFINITY
In this case, you probably should have specified a soft requirement instead (using the
preferredDuringSchedulingIgnoredDuringExecution property). After all, it’s not
such a big problem if two frontend pods run on the same node. But in scenarios where
that’s a problem, using requiredDuringScheduling is appropriate. 
Listing 16.18
Pods with anti-affinity: frontend-podantiaffinity-host.yaml
Listing 16.19
Pods created by the Deployment
The frontend pods have 
the app=frontend label.
Defining hard-
requirements for 
pod anti-affinity
A frontend pod must not 
be scheduled to the same 
machine as a pod with 
app=frontend label.
 
</data>
  <data key="d5">475
Co-locating pods with pod affinity and anti-affinity
USING ANTI-AFFINITY TO SPREAD APART PODS OF THE SAME DEPLOYMENT
Let’s see how to force your frontend pods to be scheduled to different nodes. The fol-
lowing listing shows how the pods’ anti-affinity is configured.
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    metadata:
      labels:                  
        app: frontend          
    spec:
      affinity:
        podAntiAffinity:                                      
          requiredDuringSchedulingIgnoredDuringExecution:     
          - topologyKey: kubernetes.io/hostname            
            labelSelector:                                 
              matchLabels:                                 
                app: frontend                              
      containers: ...
This time, you’re defining podAntiAffinity instead of podAffinity, and you’re mak-
ing the labelSelector match the same pods that the Deployment creates. Let’s see
what happens when you create this Deployment. The pods created by it are shown in
the following listing.
$ kubectl get po -l app=frontend -o wide
NAME                    READY  STATUS   RESTARTS  AGE  IP         NODE
frontend-286632-0lffz   0/1    Pending  0         1m   &lt;none&gt;
frontend-286632-2rkcz   1/1    Running  0         1m   10.47.0.1  node2.k8s
frontend-286632-4nwhp   0/1    Pending  0         1m   &lt;none&gt;
frontend-286632-h4686   0/1    Pending  0         1m   &lt;none&gt;
frontend-286632-st222   1/1    Running  0         1m   10.44.0.4  node1.k8s
As you can see, only two pods were scheduled—one to node1, the other to node2. The
three remaining pods are all Pending, because the Scheduler isn’t allowed to schedule
them to the same nodes.
USING PREFERENTIAL POD ANTI-AFFINITY
In this case, you probably should have specified a soft requirement instead (using the
preferredDuringSchedulingIgnoredDuringExecution property). After all, it’s not
such a big problem if two frontend pods run on the same node. But in scenarios where
that’s a problem, using requiredDuringScheduling is appropriate. 
Listing 16.18
Pods with anti-affinity: frontend-podantiaffinity-host.yaml
Listing 16.19
Pods created by the Deployment
The frontend pods have 
the app=frontend label.
Defining hard-
requirements for 
pod anti-affinity
A frontend pod must not 
be scheduled to the same 
machine as a pod with 
app=frontend label.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="508">
  <data key="d0">Page_508</data>
  <data key="d5">Page_508</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_419">
  <data key="d0">476
CHAPTER 16
Advanced scheduling
 As with pod affinity, the topologyKey property determines the scope of where the
pod shouldn’t be deployed to. You can use it to ensure pods aren’t deployed to the
same rack, availability zone, region, or any custom scope you create using custom
node labels.
16.4
Summary
In this chapter, we looked at how to ensure pods aren’t scheduled to certain nodes or
are only scheduled to specific nodes, either because of the node’s labels or because of
the pods running on them.
 You learned that
If you add a taint to a node, pods won’t be scheduled to that node unless they
tolerate that taint.
Three types of taints exist: NoSchedule completely prevents scheduling, Prefer-
NoSchedule isn’t as strict, and NoExecute even evicts existing pods from a node.
The NoExecute taint is also used to specify how long the Control Plane should
wait before rescheduling the pod when the node it runs on becomes unreach-
able or unready.
Node affinity allows you to specify which nodes a pod should be scheduled to. It
can be used to specify a hard requirement or to only express a node preference.
Pod affinity is used to make the Scheduler deploy pods to the same node where
another pod is running (based on the pod’s labels). 
Pod affinity’s topologyKey specifies how close the pod should be deployed to
the other pod (onto the same node or onto a node in the same rack, availability
zone, or availability region).
Pod anti-affinity can be used to keep certain pods away from each other. 
Both pod affinity and anti-affinity, like node affinity, can either specify hard
requirements or preferences.
In the next chapter, you’ll learn about best practices for developing apps and how to
make them run smoothly in a Kubernetes environment.
 
</data>
  <data key="d5">476
CHAPTER 16
Advanced scheduling
 As with pod affinity, the topologyKey property determines the scope of where the
pod shouldn’t be deployed to. You can use it to ensure pods aren’t deployed to the
same rack, availability zone, region, or any custom scope you create using custom
node labels.
16.4
Summary
In this chapter, we looked at how to ensure pods aren’t scheduled to certain nodes or
are only scheduled to specific nodes, either because of the node’s labels or because of
the pods running on them.
 You learned that
If you add a taint to a node, pods won’t be scheduled to that node unless they
tolerate that taint.
Three types of taints exist: NoSchedule completely prevents scheduling, Prefer-
NoSchedule isn’t as strict, and NoExecute even evicts existing pods from a node.
The NoExecute taint is also used to specify how long the Control Plane should
wait before rescheduling the pod when the node it runs on becomes unreach-
able or unready.
Node affinity allows you to specify which nodes a pod should be scheduled to. It
can be used to specify a hard requirement or to only express a node preference.
Pod affinity is used to make the Scheduler deploy pods to the same node where
another pod is running (based on the pod’s labels). 
Pod affinity’s topologyKey specifies how close the pod should be deployed to
the other pod (onto the same node or onto a node in the same rack, availability
zone, or availability region).
Pod anti-affinity can be used to keep certain pods away from each other. 
Both pod affinity and anti-affinity, like node affinity, can either specify hard
requirements or preferences.
In the next chapter, you’ll learn about best practices for developing apps and how to
make them run smoothly in a Kubernetes environment.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="509">
  <data key="d0">Page_509</data>
  <data key="d5">Page_509</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_420">
  <data key="d0">477
Best practices
for developing apps
We’ve now covered most of what you need to know to run your apps in Kubernetes.
We’ve explored what each individual resource does and how it’s used. Now we’ll see
how to combine them in a typical application running on Kubernetes. We’ll also
look at how to make an application run smoothly. After all, that’s the whole point
of using Kubernetes, isn’t it? 
 Hopefully, this chapter will help to clear up any misunderstandings and explain
things that weren’t explained clearly yet. Along the way, we’ll also introduce a few
additional concepts that haven’t been mentioned up to this point.
This chapter covers
Understanding which Kubernetes resources 
appear in a typical application
Adding post-start and pre-stop pod lifecycle hooks
Properly terminating an app without breaking 
client requests
Making apps easy to manage in Kubernetes
Using init containers in a pod
Developing locally with Minikube
 
</data>
  <data key="d5">477
Best practices
for developing apps
We’ve now covered most of what you need to know to run your apps in Kubernetes.
We’ve explored what each individual resource does and how it’s used. Now we’ll see
how to combine them in a typical application running on Kubernetes. We’ll also
look at how to make an application run smoothly. After all, that’s the whole point
of using Kubernetes, isn’t it? 
 Hopefully, this chapter will help to clear up any misunderstandings and explain
things that weren’t explained clearly yet. Along the way, we’ll also introduce a few
additional concepts that haven’t been mentioned up to this point.
This chapter covers
Understanding which Kubernetes resources 
appear in a typical application
Adding post-start and pre-stop pod lifecycle hooks
Properly terminating an app without breaking 
client requests
Making apps easy to manage in Kubernetes
Using init containers in a pod
Developing locally with Minikube
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="510">
  <data key="d0">Page_510</data>
  <data key="d5">Page_510</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_421">
  <data key="d0">478
CHAPTER 17
Best practices for developing apps
17.1
Bringing everything together
Let’s start by looking at what an actual application consists of. This will also give you a
chance to see if you remember everything you’ve learned so far and look at the big
picture. Figure 17.1 shows the Kubernetes components used in a typical application.
A typical application manifest contains one or more Deployment and/or StatefulSet
objects. Those include a pod template containing one or more containers, with a live-
ness probe for each of them and a readiness probe for the service(s) the container
provides (if any). Pods that provide services to others are exposed through one or
more Services. When they need to be reachable from outside the cluster, the Services
are either configured to be LoadBalancer or NodePort-type Services, or exposed
through an Ingress resource. 
 The pod templates (and the pods created from them) usually reference two types
of Secrets—those for pulling container images from private image registries and those
used directly by the process running inside the pods. The Secrets themselves are
usually not part of the application manifest, because they aren’t configured by the
application developers but by the operations team. Secrets are usually assigned to
ServiceAccounts, which are assigned to individual pods. 
Deﬁned in the app manifest by the developer
Pod template
Deployment
labels
Pod(s)
Label selector
labels
Created automatically at runtime
Created by a cluster admin beforehand
Container(s)
Volume(s)
ReplicaSet(s)
Endpoints
• Health probes
• Environment variables
• Volume mounts
• Resource reqs/limits
Horizontal
PodAutoscaler
StatefulSet
DaemonSet
Job
CronJob
Persistent
Volume
ConﬁgMap
Service
Persistent
Volume
Claim
Secret(s)
Service
account
Storage
Class
LimitRange
ResourceQuota
Ingress
imagePullSecret
Figure 17.1
Resources in a typical application
 
</data>
  <data key="d5">478
CHAPTER 17
Best practices for developing apps
17.1
Bringing everything together
Let’s start by looking at what an actual application consists of. This will also give you a
chance to see if you remember everything you’ve learned so far and look at the big
picture. Figure 17.1 shows the Kubernetes components used in a typical application.
A typical application manifest contains one or more Deployment and/or StatefulSet
objects. Those include a pod template containing one or more containers, with a live-
ness probe for each of them and a readiness probe for the service(s) the container
provides (if any). Pods that provide services to others are exposed through one or
more Services. When they need to be reachable from outside the cluster, the Services
are either configured to be LoadBalancer or NodePort-type Services, or exposed
through an Ingress resource. 
 The pod templates (and the pods created from them) usually reference two types
of Secrets—those for pulling container images from private image registries and those
used directly by the process running inside the pods. The Secrets themselves are
usually not part of the application manifest, because they aren’t configured by the
application developers but by the operations team. Secrets are usually assigned to
ServiceAccounts, which are assigned to individual pods. 
Deﬁned in the app manifest by the developer
Pod template
Deployment
labels
Pod(s)
Label selector
labels
Created automatically at runtime
Created by a cluster admin beforehand
Container(s)
Volume(s)
ReplicaSet(s)
Endpoints
• Health probes
• Environment variables
• Volume mounts
• Resource reqs/limits
Horizontal
PodAutoscaler
StatefulSet
DaemonSet
Job
CronJob
Persistent
Volume
ConﬁgMap
Service
Persistent
Volume
Claim
Secret(s)
Service
account
Storage
Class
LimitRange
ResourceQuota
Ingress
imagePullSecret
Figure 17.1
Resources in a typical application
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="511">
  <data key="d0">Page_511</data>
  <data key="d5">Page_511</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_422">
  <data key="d0">479
Understanding the pod’s lifecycle
 The application also contains one or more ConfigMaps, which are either used to
initialize environment variables or mounted as a configMap volume in the pod. Cer-
tain pods use additional volumes, such as an emptyDir or a gitRepo volume, whereas
pods requiring persistent storage use persistentVolumeClaim volumes. The Persistent-
VolumeClaims are also part of the application manifest, whereas StorageClasses refer-
enced by them are created by system administrators upfront. 
 In certain cases, an application also requires the use of Jobs or CronJobs. Daemon-
Sets aren’t normally part of application deployments, but are usually created by sysad-
mins to run system services on all or a subset of nodes. HorizontalPodAutoscalers
are either included in the manifest by the developers or added to the system later by
the ops team. The cluster administrator also creates LimitRange and ResourceQuota
objects to keep compute resource usage of individual pods and all the pods (as a
whole) under control.
 After the application is deployed, additional objects are created automatically by
the various Kubernetes controllers. These include service Endpoints objects created
by the Endpoints controller, ReplicaSets created by the Deployment controller, and
the actual pods created by the ReplicaSet (or Job, CronJob, StatefulSet, or DaemonSet)
controllers.
 Resources are often labeled with one or more labels to keep them organized. This
doesn’t apply only to pods but to all other resources as well. In addition to labels, most
resources also contain annotations that describe each resource, list the contact infor-
mation of the person or team responsible for it, or provide additional metadata for
management and other tools. 
 At the center of all this is the Pod, which arguably is the most important Kuberne-
tes resource. After all, each of your applications runs inside it. To make sure you know
how to develop apps that make the most out of their environment, let’s take one last
close look at pods—this time from the application’s perspective. 
17.2
Understanding the pod’s lifecycle
We’ve said that pods can be compared to VMs dedicated to running only a single
application. Although an application running inside a pod is not unlike an application
running in a VM, significant differences do exist. One example is that apps running in
a pod can be killed any time, because Kubernetes needs to relocate the pod to
another node for a reason or because of a scale-down request. We’ll explore this
aspect next.
17.2.1 Applications must expect to be killed and relocated
Outside Kubernetes, apps running in VMs are seldom moved from one machine to
another. When an operator moves the app, they can also reconfigure the app and
manually check that the app is running fine in the new location. With Kubernetes,
apps are relocated much more frequently and automatically—no human operator
 
</data>
  <data key="d5">479
Understanding the pod’s lifecycle
 The application also contains one or more ConfigMaps, which are either used to
initialize environment variables or mounted as a configMap volume in the pod. Cer-
tain pods use additional volumes, such as an emptyDir or a gitRepo volume, whereas
pods requiring persistent storage use persistentVolumeClaim volumes. The Persistent-
VolumeClaims are also part of the application manifest, whereas StorageClasses refer-
enced by them are created by system administrators upfront. 
 In certain cases, an application also requires the use of Jobs or CronJobs. Daemon-
Sets aren’t normally part of application deployments, but are usually created by sysad-
mins to run system services on all or a subset of nodes. HorizontalPodAutoscalers
are either included in the manifest by the developers or added to the system later by
the ops team. The cluster administrator also creates LimitRange and ResourceQuota
objects to keep compute resource usage of individual pods and all the pods (as a
whole) under control.
 After the application is deployed, additional objects are created automatically by
the various Kubernetes controllers. These include service Endpoints objects created
by the Endpoints controller, ReplicaSets created by the Deployment controller, and
the actual pods created by the ReplicaSet (or Job, CronJob, StatefulSet, or DaemonSet)
controllers.
 Resources are often labeled with one or more labels to keep them organized. This
doesn’t apply only to pods but to all other resources as well. In addition to labels, most
resources also contain annotations that describe each resource, list the contact infor-
mation of the person or team responsible for it, or provide additional metadata for
management and other tools. 
 At the center of all this is the Pod, which arguably is the most important Kuberne-
tes resource. After all, each of your applications runs inside it. To make sure you know
how to develop apps that make the most out of their environment, let’s take one last
close look at pods—this time from the application’s perspective. 
17.2
Understanding the pod’s lifecycle
We’ve said that pods can be compared to VMs dedicated to running only a single
application. Although an application running inside a pod is not unlike an application
running in a VM, significant differences do exist. One example is that apps running in
a pod can be killed any time, because Kubernetes needs to relocate the pod to
another node for a reason or because of a scale-down request. We’ll explore this
aspect next.
17.2.1 Applications must expect to be killed and relocated
Outside Kubernetes, apps running in VMs are seldom moved from one machine to
another. When an operator moves the app, they can also reconfigure the app and
manually check that the app is running fine in the new location. With Kubernetes,
apps are relocated much more frequently and automatically—no human operator
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="512">
  <data key="d0">Page_512</data>
  <data key="d5">Page_512</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_423">
  <data key="d0">480
CHAPTER 17
Best practices for developing apps
reconfigures them and makes sure they still run properly after the move. This means
application developers need to make sure their apps allow being moved relatively
often. 
EXPECTING THE LOCAL IP AND HOSTNAME TO CHANGE
When a pod is killed and run elsewhere (technically, it’s a new pod instance replac-
ing the old one; the pod isn’t relocated), it not only has a new IP address but also a
new name and hostname. Most stateless apps can usually handle this without any
adverse effects, but stateful apps usually can’t. We’ve learned that stateful apps can
be run through a StatefulSet, which ensures that when the app starts up on a new
node after being rescheduled, it will still see the same host name and persistent state
as before. The pod’s IP will change nevertheless. Apps need to be prepared for that
to happen. The application developer therefore should never base membership in a
clustered app on the member’s IP address, and if basing it on the hostname, should
always use a StatefulSet.
EXPECTING THE DATA WRITTEN TO DISK TO DISAPPEAR
Another thing to keep in mind is that if the app writes data to disk, that data may not be
available after the app is started inside a new pod, unless you mount persistent storage at
the location the app is writing to. It should be clear this happens when the pod is
rescheduled, but files written to disk will disappear even in scenarios that don’t involve
any rescheduling. Even during the lifetime of a single pod, the files written to disk by
the app running in the pod may disappear. Let me explain this with an example.
 Imagine an app that has a long and computationally intensive initial startup proce-
dure. To help the app come up faster on subsequent startups, the developers make
the app cache the results of the initial startup on disk (an example of this would be
the scanning of all Java classes for annotations at startup and then writing the results
to an index file). Because apps in Kubernetes run in containers by default, these files
are written to the container’s filesystem. If the container is then restarted, they’re all
lost, because the new container starts off with a completely new writable layer (see fig-
ure 17.2).
 Don’t forget that individual containers may be restarted for several reasons, such
as because the process crashes, because the liveness probe returned a failure, or
because the node started running out of memory and the process was killed by the
OOMKiller. When this happens, the pod is still the same, but the container itself is
completely new. The Kubelet doesn’t run the same container again; it always creates a
new container. 
USING VOLUMES TO PRESERVE DATA ACROSS CONTAINER RESTARTS
When its container is restarted, the app in the example will need to perform the
intensive startup procedure again. This may or may not be desired. To make sure data
like this isn’t lost, you need to use at least a pod-scoped volume. Because volumes live
and die together with the pod, the new container will be able to reuse the data written
to the volume by the previous container (figure 17.3).
 
</data>
  <data key="d5">480
CHAPTER 17
Best practices for developing apps
reconfigures them and makes sure they still run properly after the move. This means
application developers need to make sure their apps allow being moved relatively
often. 
EXPECTING THE LOCAL IP AND HOSTNAME TO CHANGE
When a pod is killed and run elsewhere (technically, it’s a new pod instance replac-
ing the old one; the pod isn’t relocated), it not only has a new IP address but also a
new name and hostname. Most stateless apps can usually handle this without any
adverse effects, but stateful apps usually can’t. We’ve learned that stateful apps can
be run through a StatefulSet, which ensures that when the app starts up on a new
node after being rescheduled, it will still see the same host name and persistent state
as before. The pod’s IP will change nevertheless. Apps need to be prepared for that
to happen. The application developer therefore should never base membership in a
clustered app on the member’s IP address, and if basing it on the hostname, should
always use a StatefulSet.
EXPECTING THE DATA WRITTEN TO DISK TO DISAPPEAR
Another thing to keep in mind is that if the app writes data to disk, that data may not be
available after the app is started inside a new pod, unless you mount persistent storage at
the location the app is writing to. It should be clear this happens when the pod is
rescheduled, but files written to disk will disappear even in scenarios that don’t involve
any rescheduling. Even during the lifetime of a single pod, the files written to disk by
the app running in the pod may disappear. Let me explain this with an example.
 Imagine an app that has a long and computationally intensive initial startup proce-
dure. To help the app come up faster on subsequent startups, the developers make
the app cache the results of the initial startup on disk (an example of this would be
the scanning of all Java classes for annotations at startup and then writing the results
to an index file). Because apps in Kubernetes run in containers by default, these files
are written to the container’s filesystem. If the container is then restarted, they’re all
lost, because the new container starts off with a completely new writable layer (see fig-
ure 17.2).
 Don’t forget that individual containers may be restarted for several reasons, such
as because the process crashes, because the liveness probe returned a failure, or
because the node started running out of memory and the process was killed by the
OOMKiller. When this happens, the pod is still the same, but the container itself is
completely new. The Kubelet doesn’t run the same container again; it always creates a
new container. 
USING VOLUMES TO PRESERVE DATA ACROSS CONTAINER RESTARTS
When its container is restarted, the app in the example will need to perform the
intensive startup procedure again. This may or may not be desired. To make sure data
like this isn’t lost, you need to use at least a pod-scoped volume. Because volumes live
and die together with the pod, the new container will be able to reuse the data written
to the volume by the previous container (figure 17.3).
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="513">
  <data key="d0">Page_513</data>
  <data key="d5">Page_513</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_424">
  <data key="d0">481
Understanding the pod’s lifecycle
Container
Process
Writes to
Filesystem
Writable layer
Read-only layer
Read-only layer
Image layers
Container crashes
or is killed
Pod
New container
New process
Filesystem
New writable layer
Read-only layer
Read-only layer
Image layers
New container started
(part of the same pod)
New container
starts with new
writeable layer:
all ﬁles are lost
Figure 17.2
Files written to the container’s filesystem are lost when the container is restarted.
Container
Process
Writes to
Can read
the same ﬁles
Filesystem
volumeMount
Container crashes
or is killed
Pod
New container
New process
Filesystem
volumeMount
New container started
(part of the same pod)
New process can
use data preserved
in the volume
Volume
Figure 17.3
Using a volume to persist data across container restarts
 
</data>
  <data key="d5">481
Understanding the pod’s lifecycle
Container
Process
Writes to
Filesystem
Writable layer
Read-only layer
Read-only layer
Image layers
Container crashes
or is killed
Pod
New container
New process
Filesystem
New writable layer
Read-only layer
Read-only layer
Image layers
New container started
(part of the same pod)
New container
starts with new
writeable layer:
all ﬁles are lost
Figure 17.2
Files written to the container’s filesystem are lost when the container is restarted.
Container
Process
Writes to
Can read
the same ﬁles
Filesystem
volumeMount
Container crashes
or is killed
Pod
New container
New process
Filesystem
volumeMount
New container started
(part of the same pod)
New process can
use data preserved
in the volume
Volume
Figure 17.3
Using a volume to persist data across container restarts
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="514">
  <data key="d0">Page_514</data>
  <data key="d5">Page_514</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_425">
  <data key="d0">482
CHAPTER 17
Best practices for developing apps
Using a volume to preserve files across container restarts is a great idea sometimes,
but not always. What if the data gets corrupted and causes the newly created process
to crash again? This will result in a continuous crash loop (the pod will show the
CrashLoopBackOff status). If you hadn’t used a volume, the new container would start
from scratch and most likely not crash. Using volumes to preserve files across con-
tainer restarts like this is a double-edged sword. You need to think carefully about
whether to use them or not.
17.2.2 Rescheduling of dead or partially dead pods
If a pod’s container keeps crashing, the Kubelet will keep restarting it indefinitely.
The time between restarts will be increased exponentially until it reaches five minutes.
During those five minute intervals, the pod is essentially dead, because its container’s
process isn’t running. To be fair, if it’s a multi-container pod, certain containers may
be running normally, so the pod is only partially dead. But if a pod contains only a sin-
gle container, the pod is effectively dead and completely useless, because no process is
running in it anymore.
 You may find it surprising to learn that such pods aren’t automatically removed
and rescheduled, even if they’re part of a ReplicaSet or similar controller. If you cre-
ate a ReplicaSet with a desired replica count of three, and then one of the containers
in one of those pods starts crashing, Kubernetes will not delete and replace the pod.
The end result is a ReplicaSet with only two properly running replicas instead of the
desired three (figure 17.4).
You’d probably expect the pod to be deleted and replaced with another pod instance
that might run successfully on another node. After all, the container may be crashing
because of a node-related problem that doesn’t manifest itself on other nodes. Sadly,
that isn’t the case. The ReplicaSet controller doesn’t care if the pods are dead—all it
ReplicaSet
Desired replicas: 3
Actual replicas: 3
Only two pods are actually
performing their jobs
Third pod’s status is Running,
but its container keeps crashing,
with signiﬁcant delays between
restarts (CrashLoopBackOff)
We want
three pods
Pod
Running
container
Pod
Running
container
Pod
Dead
container
Figure 17.4
A ReplicaSet controller doesn’t reschedule dead pods.
 
</data>
  <data key="d5">482
CHAPTER 17
Best practices for developing apps
Using a volume to preserve files across container restarts is a great idea sometimes,
but not always. What if the data gets corrupted and causes the newly created process
to crash again? This will result in a continuous crash loop (the pod will show the
CrashLoopBackOff status). If you hadn’t used a volume, the new container would start
from scratch and most likely not crash. Using volumes to preserve files across con-
tainer restarts like this is a double-edged sword. You need to think carefully about
whether to use them or not.
17.2.2 Rescheduling of dead or partially dead pods
If a pod’s container keeps crashing, the Kubelet will keep restarting it indefinitely.
The time between restarts will be increased exponentially until it reaches five minutes.
During those five minute intervals, the pod is essentially dead, because its container’s
process isn’t running. To be fair, if it’s a multi-container pod, certain containers may
be running normally, so the pod is only partially dead. But if a pod contains only a sin-
gle container, the pod is effectively dead and completely useless, because no process is
running in it anymore.
 You may find it surprising to learn that such pods aren’t automatically removed
and rescheduled, even if they’re part of a ReplicaSet or similar controller. If you cre-
ate a ReplicaSet with a desired replica count of three, and then one of the containers
in one of those pods starts crashing, Kubernetes will not delete and replace the pod.
The end result is a ReplicaSet with only two properly running replicas instead of the
desired three (figure 17.4).
You’d probably expect the pod to be deleted and replaced with another pod instance
that might run successfully on another node. After all, the container may be crashing
because of a node-related problem that doesn’t manifest itself on other nodes. Sadly,
that isn’t the case. The ReplicaSet controller doesn’t care if the pods are dead—all it
ReplicaSet
Desired replicas: 3
Actual replicas: 3
Only two pods are actually
performing their jobs
Third pod’s status is Running,
but its container keeps crashing,
with signiﬁcant delays between
restarts (CrashLoopBackOff)
We want
three pods
Pod
Running
container
Pod
Running
container
Pod
Dead
container
Figure 17.4
A ReplicaSet controller doesn’t reschedule dead pods.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="515">
  <data key="d0">Page_515</data>
  <data key="d5">Page_515</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_426">
  <data key="d0">483
Understanding the pod’s lifecycle
cares about is that the number of pods matches the desired replica count, which in
this case, it does.
 If you’d like to see for yourself, I’ve included a YAML manifest for a ReplicaSet
whose pods will keep crashing (see file replicaset-crashingpods.yaml in the code
archive). If you create the ReplicaSet and inspect the pods that are created, the follow-
ing listing is what you’ll see.
$ kubectl get po
NAME                  READY     STATUS             RESTARTS   AGE
crashing-pods-f1tcd   0/1       CrashLoopBackOff   5          6m     
crashing-pods-k7l6k   0/1       CrashLoopBackOff   5          6m
crashing-pods-z7l3v   0/1       CrashLoopBackOff   5          6m
$ kubectl describe rs crashing-pods
Name:           crashing-pods
Replicas:       3 current / 3 desired                       
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed      
$ kubectl describe po crashing-pods-f1tcd
Name:           crashing-pods-f1tcd
Namespace:      default
Node:           minikube/192.168.99.102
Start Time:     Thu, 02 Mar 2017 14:02:23 +0100
Labels:         app=crashing-pods
Status:         Running                      
In a way, it’s understandable that Kubernetes behaves this way. The container will be
restarted every five minutes in the hope that the underlying cause of the crash will be
resolved. The rationale is that rescheduling the pod to another node most likely
wouldn’t fix the problem anyway, because the app is running inside a container and
all the nodes should be mostly equivalent. That’s not always the case, but it is most of
the time. 
17.2.3 Starting pods in a specific order
One other difference between apps running in pods and those managed manually is
that the ops person deploying those apps knows about the dependencies between
them. This allows them to start the apps in order. 
UNDERSTANDING HOW PODS ARE STARTED
When you use Kubernetes to run your multi-pod applications, you don’t have a built-
in way to tell Kubernetes to run certain pods first and the rest only when the first pods
are already up and ready to serve. Sure, you could post the manifest for the first app
and then wait for the pod(s) to be ready before you post the second manifest, but your
Listing 17.1
ReplicaSet and pods that keep crashing
The pod’s status shows the Kubelet is
delaying the restart because the
container keeps crashing.
No action taken 
by the controller, 
because current 
replicas match 
desired replicas
Three 
replicas are 
shown as 
running.
kubectl describe 
also shows pod’s 
status as running
 
</data>
  <data key="d5">483
Understanding the pod’s lifecycle
cares about is that the number of pods matches the desired replica count, which in
this case, it does.
 If you’d like to see for yourself, I’ve included a YAML manifest for a ReplicaSet
whose pods will keep crashing (see file replicaset-crashingpods.yaml in the code
archive). If you create the ReplicaSet and inspect the pods that are created, the follow-
ing listing is what you’ll see.
$ kubectl get po
NAME                  READY     STATUS             RESTARTS   AGE
crashing-pods-f1tcd   0/1       CrashLoopBackOff   5          6m     
crashing-pods-k7l6k   0/1       CrashLoopBackOff   5          6m
crashing-pods-z7l3v   0/1       CrashLoopBackOff   5          6m
$ kubectl describe rs crashing-pods
Name:           crashing-pods
Replicas:       3 current / 3 desired                       
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed      
$ kubectl describe po crashing-pods-f1tcd
Name:           crashing-pods-f1tcd
Namespace:      default
Node:           minikube/192.168.99.102
Start Time:     Thu, 02 Mar 2017 14:02:23 +0100
Labels:         app=crashing-pods
Status:         Running                      
In a way, it’s understandable that Kubernetes behaves this way. The container will be
restarted every five minutes in the hope that the underlying cause of the crash will be
resolved. The rationale is that rescheduling the pod to another node most likely
wouldn’t fix the problem anyway, because the app is running inside a container and
all the nodes should be mostly equivalent. That’s not always the case, but it is most of
the time. 
17.2.3 Starting pods in a specific order
One other difference between apps running in pods and those managed manually is
that the ops person deploying those apps knows about the dependencies between
them. This allows them to start the apps in order. 
UNDERSTANDING HOW PODS ARE STARTED
When you use Kubernetes to run your multi-pod applications, you don’t have a built-
in way to tell Kubernetes to run certain pods first and the rest only when the first pods
are already up and ready to serve. Sure, you could post the manifest for the first app
and then wait for the pod(s) to be ready before you post the second manifest, but your
Listing 17.1
ReplicaSet and pods that keep crashing
The pod’s status shows the Kubelet is
delaying the restart because the
container keeps crashing.
No action taken 
by the controller, 
because current 
replicas match 
desired replicas
Three 
replicas are 
shown as 
running.
kubectl describe 
also shows pod’s 
status as running
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="516">
  <data key="d0">Page_516</data>
  <data key="d5">Page_516</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_427">
  <data key="d0">484
CHAPTER 17
Best practices for developing apps
whole system is usually defined in a single YAML or JSON containing multiple Pods,
Services, and other objects. 
 The Kubernetes API server does process the objects in the YAML/JSON in the
order they’re listed, but this only means they’re written to etcd in that order. You have
no guarantee that pods will also be started in that order. 
 But you can prevent a pod’s main container from starting until a precondition is
met. This is done by including an init containers in the pod. 
INTRODUCING INIT CONTAINERS
In addition to regular containers, pods can also include init containers. As the name
suggests, they can be used to initialize the pod—this often means writing data to the
pod’s volumes, which are then mounted into the pod’s main container(s).
 A pod may have any number of init containers. They’re executed sequentially and
only after the last one completes are the pod’s main containers started. This means
init containers can also be used to delay the start of the pod’s main container(s)—for
example, until a certain precondition is met. An init container could wait for a service
required by the pod’s main container to be up and ready. When it is, the init container
terminates and allows the main container(s) to be started. This way, the main con-
tainer wouldn’t use the service before it’s ready.
 Let’s look at an example of a pod using an init container to delay the start of the
main container. Remember the fortune pod you created in chapter 7? It’s a web
server that returns a fortune quote as a response to client requests. Now, let’s imagine
you have a fortune-client pod that requires the fortune Service to be up and run-
ning before its main container starts. You can add an init container, which checks
whether the Service is responding to requests. Until that’s the case, the init container
keeps retrying. Once it gets a response, the init container terminates and lets the main
container start.
ADDING AN INIT CONTAINER TO A POD
Init containers can be defined in the pod spec like main containers but through the
spec.initContainers field. You’ll find the complete YAML for the fortune-client pod
in the book’s code archive. The following listing shows the part where the init con-
tainer is defined.
spec:
  initContainers:      
  - name: init
    image: busybox
    command:
    - sh
    - -c
    - 'while true; do echo "Waiting for fortune service to come up...";  
    ➥ wget http://fortune -q -T 1 -O /dev/null &gt;/dev/null 2&gt;/dev/null   
    ➥ &amp;&amp; break; sleep 1; done; echo "Service is up! Starting main       
    ➥ container."'
Listing 17.2
An init container defined in a pod: fortune-client.yaml
You’re defining 
an init container, 
not a regular 
container.
The init container runs a
loop that runs until the
fortune Service is up.
 
</data>
  <data key="d5">484
CHAPTER 17
Best practices for developing apps
whole system is usually defined in a single YAML or JSON containing multiple Pods,
Services, and other objects. 
 The Kubernetes API server does process the objects in the YAML/JSON in the
order they’re listed, but this only means they’re written to etcd in that order. You have
no guarantee that pods will also be started in that order. 
 But you can prevent a pod’s main container from starting until a precondition is
met. This is done by including an init containers in the pod. 
INTRODUCING INIT CONTAINERS
In addition to regular containers, pods can also include init containers. As the name
suggests, they can be used to initialize the pod—this often means writing data to the
pod’s volumes, which are then mounted into the pod’s main container(s).
 A pod may have any number of init containers. They’re executed sequentially and
only after the last one completes are the pod’s main containers started. This means
init containers can also be used to delay the start of the pod’s main container(s)—for
example, until a certain precondition is met. An init container could wait for a service
required by the pod’s main container to be up and ready. When it is, the init container
terminates and allows the main container(s) to be started. This way, the main con-
tainer wouldn’t use the service before it’s ready.
 Let’s look at an example of a pod using an init container to delay the start of the
main container. Remember the fortune pod you created in chapter 7? It’s a web
server that returns a fortune quote as a response to client requests. Now, let’s imagine
you have a fortune-client pod that requires the fortune Service to be up and run-
ning before its main container starts. You can add an init container, which checks
whether the Service is responding to requests. Until that’s the case, the init container
keeps retrying. Once it gets a response, the init container terminates and lets the main
container start.
ADDING AN INIT CONTAINER TO A POD
Init containers can be defined in the pod spec like main containers but through the
spec.initContainers field. You’ll find the complete YAML for the fortune-client pod
in the book’s code archive. The following listing shows the part where the init con-
tainer is defined.
spec:
  initContainers:      
  - name: init
    image: busybox
    command:
    - sh
    - -c
    - 'while true; do echo "Waiting for fortune service to come up...";  
    ➥ wget http://fortune -q -T 1 -O /dev/null &gt;/dev/null 2&gt;/dev/null   
    ➥ &amp;&amp; break; sleep 1; done; echo "Service is up! Starting main       
    ➥ container."'
Listing 17.2
An init container defined in a pod: fortune-client.yaml
You’re defining 
an init container, 
not a regular 
container.
The init container runs a
loop that runs until the
fortune Service is up.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="517">
  <data key="d0">Page_517</data>
  <data key="d5">Page_517</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_428">
  <data key="d0">485
Understanding the pod’s lifecycle
When you deploy this pod, only its init container is started. This is shown in the pod’s
status when you list pods with kubectl get:
$ kubectl get po
NAME             READY     STATUS     RESTARTS   AGE
fortune-client   0/1       Init:0/1   0          1m
The STATUS column shows that zero of one init containers have finished. You can see
the log of the init container with kubectl logs:
$ kubectl logs fortune-client -c init
Waiting for fortune service to come up...
When running the kubectl logs command, you need to specify the name of the init
container with the -c switch (in the example, the name of the pod’s init container is
init, as you can see in listing 17.2).
 The main container won’t run until you deploy the fortune Service and the
fortune-server pod. You’ll find them in the fortune-server.yaml file. 
BEST PRACTICES FOR HANDLING INTER-POD DEPENDENCIES
You’ve seen how an init container can be used to delay starting the pod’s main con-
tainer(s) until a precondition is met (making sure the Service the pod depends on is
ready, for example), but it’s much better to write apps that don’t require every service
they rely on to be ready before the app starts up. After all, the service may also go
offline later, while the app is already running.
 The application needs to handle internally the possibility that its dependencies
aren’t ready. And don’t forget readiness probes. If an app can’t do its job because one
of its dependencies is missing, it should signal that through its readiness probe, so
Kubernetes knows it, too, isn’t ready. You’ll want to do this not only because it pre-
vents the app from being added as a service endpoint, but also because the app’s read-
iness is also used by the Deployment controller when performing a rolling update,
thereby preventing a rollout of a bad version. 
17.2.4 Adding lifecycle hooks
We’ve talked about how init containers can be used to hook into the startup of the
pod, but pods also allow you to define two lifecycle hooks:
Post-start hooks
Pre-stop hooks
These lifecycle hooks are specified per container, unlike init containers, which apply
to the whole pod. As their names suggest, they’re executed when the container starts
and before it stops. 
 Lifecycle hooks are similar to liveness and readiness probes in that they can either
Execute a command inside the container
Perform an HTTP GET request against a URL
 
</data>
  <data key="d5">485
Understanding the pod’s lifecycle
When you deploy this pod, only its init container is started. This is shown in the pod’s
status when you list pods with kubectl get:
$ kubectl get po
NAME             READY     STATUS     RESTARTS   AGE
fortune-client   0/1       Init:0/1   0          1m
The STATUS column shows that zero of one init containers have finished. You can see
the log of the init container with kubectl logs:
$ kubectl logs fortune-client -c init
Waiting for fortune service to come up...
When running the kubectl logs command, you need to specify the name of the init
container with the -c switch (in the example, the name of the pod’s init container is
init, as you can see in listing 17.2).
 The main container won’t run until you deploy the fortune Service and the
fortune-server pod. You’ll find them in the fortune-server.yaml file. 
BEST PRACTICES FOR HANDLING INTER-POD DEPENDENCIES
You’ve seen how an init container can be used to delay starting the pod’s main con-
tainer(s) until a precondition is met (making sure the Service the pod depends on is
ready, for example), but it’s much better to write apps that don’t require every service
they rely on to be ready before the app starts up. After all, the service may also go
offline later, while the app is already running.
 The application needs to handle internally the possibility that its dependencies
aren’t ready. And don’t forget readiness probes. If an app can’t do its job because one
of its dependencies is missing, it should signal that through its readiness probe, so
Kubernetes knows it, too, isn’t ready. You’ll want to do this not only because it pre-
vents the app from being added as a service endpoint, but also because the app’s read-
iness is also used by the Deployment controller when performing a rolling update,
thereby preventing a rollout of a bad version. 
17.2.4 Adding lifecycle hooks
We’ve talked about how init containers can be used to hook into the startup of the
pod, but pods also allow you to define two lifecycle hooks:
Post-start hooks
Pre-stop hooks
These lifecycle hooks are specified per container, unlike init containers, which apply
to the whole pod. As their names suggest, they’re executed when the container starts
and before it stops. 
 Lifecycle hooks are similar to liveness and readiness probes in that they can either
Execute a command inside the container
Perform an HTTP GET request against a URL
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="518">
  <data key="d0">Page_518</data>
  <data key="d5">Page_518</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_429">
  <data key="d0">486
CHAPTER 17
Best practices for developing apps
Let’s look at the two hooks individually to see what effect they have on the container
lifecycle.
USING A POST-START CONTAINER LIFECYCLE HOOK
A post-start hook is executed immediately after the container’s main process is started.
You use it to perform additional operations when the application starts. Sure, if you’re
the author of the application running in the container, you can always perform those
operations inside the application code itself. But when you’re running an application
developed by someone else, you mostly don’t want to (or can’t) modify its source
code. Post-start hooks allow you to run additional commands without having to touch
the app. These may signal to an external listener that the app is starting, or they may
initialize the application so it can start doing its job.
 The hook is run in parallel with the main process. The name might be somewhat
misleading, because it doesn’t wait for the main process to start up fully (if the process
has an initialization procedure, the Kubelet obviously can’t wait for the procedure to
complete, because it has no way of knowing when that is). 
 But even though the hook runs asynchronously, it does affect the container in two
ways. Until the hook completes, the container will stay in the Waiting state with the
reason ContainerCreating. Because of this, the pod’s status will be Pending instead of
Running. If the hook fails to run or returns a non-zero exit code, the main container
will be killed. 
 A pod manifest containing a post-start hook looks like the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-poststart-hook
spec:
  containers:
  - image: luksa/kubia
    name: kubia
    lifecycle:          
      postStart:        
        exec:                                                               
          command:                                                          
          - sh                                                              
          - -c                                                              
          - "echo 'hook will fail with exit code 15'; sleep 5; exit 15"     
In the example, the echo, sleep, and exit commands are executed along with the
container’s main process as soon as the container is created. Rather than run a com-
mand like this, you’d typically run a shell script or a binary executable file stored in
the container image. 
 Sadly, if the process started by the hook logs to the standard output, you can’t see
the output anywhere. This makes debugging lifecycle hooks painful. If the hook fails,
Listing 17.3
A pod with a post-start lifecycle hook: post-start-hook.yaml
The hook is executed as 
the container starts.
It executes the
postStart.sh
script in the /bin
directory inside
the container.
 
</data>
  <data key="d5">486
CHAPTER 17
Best practices for developing apps
Let’s look at the two hooks individually to see what effect they have on the container
lifecycle.
USING A POST-START CONTAINER LIFECYCLE HOOK
A post-start hook is executed immediately after the container’s main process is started.
You use it to perform additional operations when the application starts. Sure, if you’re
the author of the application running in the container, you can always perform those
operations inside the application code itself. But when you’re running an application
developed by someone else, you mostly don’t want to (or can’t) modify its source
code. Post-start hooks allow you to run additional commands without having to touch
the app. These may signal to an external listener that the app is starting, or they may
initialize the application so it can start doing its job.
 The hook is run in parallel with the main process. The name might be somewhat
misleading, because it doesn’t wait for the main process to start up fully (if the process
has an initialization procedure, the Kubelet obviously can’t wait for the procedure to
complete, because it has no way of knowing when that is). 
 But even though the hook runs asynchronously, it does affect the container in two
ways. Until the hook completes, the container will stay in the Waiting state with the
reason ContainerCreating. Because of this, the pod’s status will be Pending instead of
Running. If the hook fails to run or returns a non-zero exit code, the main container
will be killed. 
 A pod manifest containing a post-start hook looks like the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-poststart-hook
spec:
  containers:
  - image: luksa/kubia
    name: kubia
    lifecycle:          
      postStart:        
        exec:                                                               
          command:                                                          
          - sh                                                              
          - -c                                                              
          - "echo 'hook will fail with exit code 15'; sleep 5; exit 15"     
In the example, the echo, sleep, and exit commands are executed along with the
container’s main process as soon as the container is created. Rather than run a com-
mand like this, you’d typically run a shell script or a binary executable file stored in
the container image. 
 Sadly, if the process started by the hook logs to the standard output, you can’t see
the output anywhere. This makes debugging lifecycle hooks painful. If the hook fails,
Listing 17.3
A pod with a post-start lifecycle hook: post-start-hook.yaml
The hook is executed as 
the container starts.
It executes the
postStart.sh
script in the /bin
directory inside
the container.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="519">
  <data key="d0">Page_519</data>
  <data key="d5">Page_519</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_430">
  <data key="d0">487
Understanding the pod’s lifecycle
you’ll only see a FailedPostStartHook warning among the pod’s events (you can see
them using kubectl describe pod). A while later, you’ll see more information on why
the hook failed, as shown in the following listing.
FailedSync   Error syncing pod, skipping: failed to "StartContainer" for 
             "kubia" with PostStart handler: command 'sh -c echo 'hook 
             will fail with exit code 15'; sleep 5 ; exit 15' exited 
             with 15: : "PostStart Hook Failed" 
The number 15 in the last line is the exit code of the command. When using an HTTP
GET hook handler, the reason may look like the following listing (you can try this by
deploying the post-start-hook-httpget.yaml file from the book’s code archive).
FailedSync   Error syncing pod, skipping: failed to "StartContainer" for 
             "kubia" with PostStart handler: Get 
             http://10.32.0.2:9090/postStart: dial tcp 10.32.0.2:9090: 
             getsockopt: connection refused: "PostStart Hook Failed" 
NOTE
The post-start hook is intentionally misconfigured to use port 9090
instead of the correct port 8080, to show what happens when the hook fails.
The standard and error outputs of command-based post-start hooks aren’t logged any-
where, so you may want to have the process the hook invokes log to a file in the con-
tainer’s filesystem, which will allow you to examine the contents of the file with
something like this:
$ kubectl exec my-pod cat logfile.txt 
If the container gets restarted for whatever reason (including because the hook failed),
the file may be gone before you can examine it. You can work around that by mount-
ing an emptyDir volume into the container and having the hook write to it.
USING A PRE-STOP CONTAINER LIFECYCLE HOOK
A pre-stop hook is executed immediately before a container is terminated. When a
container needs to be terminated, the Kubelet will run the pre-stop hook, if config-
ured, and only then send a SIGTERM to the process (and later kill the process if it
doesn’t terminate gracefully). 
 A pre-stop hook can be used to initiate a graceful shutdown of the container, if it
doesn’t shut down gracefully upon receipt of a SIGTERM signal. They can also be used
to perform arbitrary operations before shutdown without having to implement those
operations in the application itself (this is useful when you’re running a third-party
app, whose source code you don’t have access to and/or can’t modify). 
 Configuring a pre-stop hook in a pod manifest isn’t very different from adding a
post-start hook. The previous example showed a post-start hook that executes a com-
Listing 17.4
Pod’s events showing the exit code of the failed command-based hook
Listing 17.5
Pod’s events showing the reason why an HTTP GET hook failed
 
</data>
  <data key="d5">487
Understanding the pod’s lifecycle
you’ll only see a FailedPostStartHook warning among the pod’s events (you can see
them using kubectl describe pod). A while later, you’ll see more information on why
the hook failed, as shown in the following listing.
FailedSync   Error syncing pod, skipping: failed to "StartContainer" for 
             "kubia" with PostStart handler: command 'sh -c echo 'hook 
             will fail with exit code 15'; sleep 5 ; exit 15' exited 
             with 15: : "PostStart Hook Failed" 
The number 15 in the last line is the exit code of the command. When using an HTTP
GET hook handler, the reason may look like the following listing (you can try this by
deploying the post-start-hook-httpget.yaml file from the book’s code archive).
FailedSync   Error syncing pod, skipping: failed to "StartContainer" for 
             "kubia" with PostStart handler: Get 
             http://10.32.0.2:9090/postStart: dial tcp 10.32.0.2:9090: 
             getsockopt: connection refused: "PostStart Hook Failed" 
NOTE
The post-start hook is intentionally misconfigured to use port 9090
instead of the correct port 8080, to show what happens when the hook fails.
The standard and error outputs of command-based post-start hooks aren’t logged any-
where, so you may want to have the process the hook invokes log to a file in the con-
tainer’s filesystem, which will allow you to examine the contents of the file with
something like this:
$ kubectl exec my-pod cat logfile.txt 
If the container gets restarted for whatever reason (including because the hook failed),
the file may be gone before you can examine it. You can work around that by mount-
ing an emptyDir volume into the container and having the hook write to it.
USING A PRE-STOP CONTAINER LIFECYCLE HOOK
A pre-stop hook is executed immediately before a container is terminated. When a
container needs to be terminated, the Kubelet will run the pre-stop hook, if config-
ured, and only then send a SIGTERM to the process (and later kill the process if it
doesn’t terminate gracefully). 
 A pre-stop hook can be used to initiate a graceful shutdown of the container, if it
doesn’t shut down gracefully upon receipt of a SIGTERM signal. They can also be used
to perform arbitrary operations before shutdown without having to implement those
operations in the application itself (this is useful when you’re running a third-party
app, whose source code you don’t have access to and/or can’t modify). 
 Configuring a pre-stop hook in a pod manifest isn’t very different from adding a
post-start hook. The previous example showed a post-start hook that executes a com-
Listing 17.4
Pod’s events showing the exit code of the failed command-based hook
Listing 17.5
Pod’s events showing the reason why an HTTP GET hook failed
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="520">
  <data key="d0">Page_520</data>
  <data key="d5">Page_520</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_431">
  <data key="d0">488
CHAPTER 17
Best practices for developing apps
mand, so we’ll look at a pre-stop hook that performs an HTTP GET request now. The
following listing shows how to define a pre-stop HTTP GET hook in a pod.
    lifecycle:
      preStop:            
        httpGet:          
          port: 8080          
          path: shutdown      
The pre-stop hook defined in this listing performs an HTTP GET request to http:/
/
POD_IP:8080/shutdown as soon as the Kubelet starts terminating the container.
Apart from the port and path shown in the listing, you can also set the fields scheme
(HTTP or HTTPS) and host, as well as httpHeaders that should be sent in the
request. The host field defaults to the pod IP. Be sure not to set it to localhost,
because localhost would refer to the node, not the pod.
 In contrast to the post-start hook, the container will be terminated regardless of
the result of the hook—an error HTTP response code or a non-zero exit code when
using a command-based hook will not prevent the container from being terminated.
If the pre-stop hook fails, you’ll see a FailedPreStopHook warning event among the
pod’s events, but because the pod is deleted soon afterward (after all, the pod’s dele-
tion is what triggered the pre-stop hook in the first place), you may not even notice
that the pre-stop hook failed to run properly. 
TIP
If the successful completion of the pre-stop hook is critical to the proper
operation of your system, verify whether it’s being executed at all. I’ve wit-
nessed situations where the pre-stop hook didn’t run and the developer
wasn’t even aware of that.
USING A PRE-STOP HOOK BECAUSE YOUR APP DOESN’T RECEIVE THE SIGTERM SIGNAL
Many developers make the mistake of defining a pre-stop hook solely to send a SIGTERM
signal to their apps in the pre-stop hook. They do this because they don’t see their appli-
cation receive the SIGTERM signal sent by the Kubelet. The reason why the signal isn’t
received by the application isn’t because Kubernetes isn’t sending it, but because the sig-
nal isn’t being passed to the app process inside the container itself. If your container
image is configured to run a shell, which in turn runs the app process, the signal may be
eaten up by the shell itself, instead of being passed down to the child process.
 In such cases, instead of adding a pre-stop hook to send the signal directly to your
app, the proper fix is to make sure the shell passes the signal to the app. This can be
achieved by handling the signal in the shell script running as the main container pro-
cess and then passing it on to the app. Or you could not configure the container image
to run a shell at all and instead run the application binary directly. You do this by using
the exec form of ENTRYPOINT or CMD in the Dockerfile: ENTRYPOINT ["/mybinary"]
instead of ENTRYPOINT /mybinary.
Listing 17.6
A pre-stop hook YAML snippet: pre-stop-hook-httpget.yaml
This is a pre-stop hook that 
performs an HTTP GET request.
The request is sent to 
http://POD_IP:8080/shutdown.
 
</data>
  <data key="d5">488
CHAPTER 17
Best practices for developing apps
mand, so we’ll look at a pre-stop hook that performs an HTTP GET request now. The
following listing shows how to define a pre-stop HTTP GET hook in a pod.
    lifecycle:
      preStop:            
        httpGet:          
          port: 8080          
          path: shutdown      
The pre-stop hook defined in this listing performs an HTTP GET request to http:/
/
POD_IP:8080/shutdown as soon as the Kubelet starts terminating the container.
Apart from the port and path shown in the listing, you can also set the fields scheme
(HTTP or HTTPS) and host, as well as httpHeaders that should be sent in the
request. The host field defaults to the pod IP. Be sure not to set it to localhost,
because localhost would refer to the node, not the pod.
 In contrast to the post-start hook, the container will be terminated regardless of
the result of the hook—an error HTTP response code or a non-zero exit code when
using a command-based hook will not prevent the container from being terminated.
If the pre-stop hook fails, you’ll see a FailedPreStopHook warning event among the
pod’s events, but because the pod is deleted soon afterward (after all, the pod’s dele-
tion is what triggered the pre-stop hook in the first place), you may not even notice
that the pre-stop hook failed to run properly. 
TIP
If the successful completion of the pre-stop hook is critical to the proper
operation of your system, verify whether it’s being executed at all. I’ve wit-
nessed situations where the pre-stop hook didn’t run and the developer
wasn’t even aware of that.
USING A PRE-STOP HOOK BECAUSE YOUR APP DOESN’T RECEIVE THE SIGTERM SIGNAL
Many developers make the mistake of defining a pre-stop hook solely to send a SIGTERM
signal to their apps in the pre-stop hook. They do this because they don’t see their appli-
cation receive the SIGTERM signal sent by the Kubelet. The reason why the signal isn’t
received by the application isn’t because Kubernetes isn’t sending it, but because the sig-
nal isn’t being passed to the app process inside the container itself. If your container
image is configured to run a shell, which in turn runs the app process, the signal may be
eaten up by the shell itself, instead of being passed down to the child process.
 In such cases, instead of adding a pre-stop hook to send the signal directly to your
app, the proper fix is to make sure the shell passes the signal to the app. This can be
achieved by handling the signal in the shell script running as the main container pro-
cess and then passing it on to the app. Or you could not configure the container image
to run a shell at all and instead run the application binary directly. You do this by using
the exec form of ENTRYPOINT or CMD in the Dockerfile: ENTRYPOINT ["/mybinary"]
instead of ENTRYPOINT /mybinary.
Listing 17.6
A pre-stop hook YAML snippet: pre-stop-hook-httpget.yaml
This is a pre-stop hook that 
performs an HTTP GET request.
The request is sent to 
http://POD_IP:8080/shutdown.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="521">
  <data key="d0">Page_521</data>
  <data key="d5">Page_521</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_432">
  <data key="d0">489
Understanding the pod’s lifecycle
 A container using the first form runs the mybinary executable as its main process,
whereas the second form runs a shell as the main process with the mybinary process
executed as a child of the shell process.
UNDERSTANDING THAT LIFECYCLE HOOKS TARGET CONTAINERS, NOT PODS
As a final thought on post-start and pre-stop hooks, let me emphasize that these lifecy-
cle hooks relate to containers, not pods. You shouldn’t use a pre-stop hook for run-
ning actions that need to be performed when the pod is terminating. The reason is
that the pre-stop hook gets called when the container is being terminated (most likely
because of a failed liveness probe). This may happen multiple times in the pod’s life-
time, not only when the pod is in the process of being shut down. 
17.2.5 Understanding pod shutdown
We’ve touched on the subject of pod termination, so let’s explore this subject in more
detail and go over exactly what happens during pod shutdown. This is important for
understanding how to cleanly shut down an application running in a pod.
 Let’s start at the beginning. A pod’s shut-down is triggered by the deletion of the
Pod object through the API server. Upon receiving an HTTP DELETE request, the
API server doesn’t delete the object yet, but only sets a deletionTimestamp field in it.
Pods that have the deletionTimestamp field set are terminating. 
 Once the Kubelet notices the pod needs to be terminated, it starts terminating
each of the pod’s containers. It gives each container time to shut down gracefully, but
the time is limited. That time is called the termination grace period and is configu-
rable per pod. The timer starts as soon as the termination process starts. Then the fol-
lowing sequence of events is performed:
1
Run the pre-stop hook, if one is configured, and wait for it to finish.
2
Send the SIGTERM signal to the main process of the container.
3
Wait until the container shuts down cleanly or until the termination grace
period runs out.
4
Forcibly kill the process with SIGKILL, if it hasn’t terminated gracefully yet.
The sequence of events is illustrated in figure 17.5.
Pre-stop hook process
Termination grace period
Main container process
Container shutdown
initiated
Container killed
if still running
Time
SIGTERM
SIGKILL
Figure 17.5
The container termination sequence
 
</data>
  <data key="d5">489
Understanding the pod’s lifecycle
 A container using the first form runs the mybinary executable as its main process,
whereas the second form runs a shell as the main process with the mybinary process
executed as a child of the shell process.
UNDERSTANDING THAT LIFECYCLE HOOKS TARGET CONTAINERS, NOT PODS
As a final thought on post-start and pre-stop hooks, let me emphasize that these lifecy-
cle hooks relate to containers, not pods. You shouldn’t use a pre-stop hook for run-
ning actions that need to be performed when the pod is terminating. The reason is
that the pre-stop hook gets called when the container is being terminated (most likely
because of a failed liveness probe). This may happen multiple times in the pod’s life-
time, not only when the pod is in the process of being shut down. 
17.2.5 Understanding pod shutdown
We’ve touched on the subject of pod termination, so let’s explore this subject in more
detail and go over exactly what happens during pod shutdown. This is important for
understanding how to cleanly shut down an application running in a pod.
 Let’s start at the beginning. A pod’s shut-down is triggered by the deletion of the
Pod object through the API server. Upon receiving an HTTP DELETE request, the
API server doesn’t delete the object yet, but only sets a deletionTimestamp field in it.
Pods that have the deletionTimestamp field set are terminating. 
 Once the Kubelet notices the pod needs to be terminated, it starts terminating
each of the pod’s containers. It gives each container time to shut down gracefully, but
the time is limited. That time is called the termination grace period and is configu-
rable per pod. The timer starts as soon as the termination process starts. Then the fol-
lowing sequence of events is performed:
1
Run the pre-stop hook, if one is configured, and wait for it to finish.
2
Send the SIGTERM signal to the main process of the container.
3
Wait until the container shuts down cleanly or until the termination grace
period runs out.
4
Forcibly kill the process with SIGKILL, if it hasn’t terminated gracefully yet.
The sequence of events is illustrated in figure 17.5.
Pre-stop hook process
Termination grace period
Main container process
Container shutdown
initiated
Container killed
if still running
Time
SIGTERM
SIGKILL
Figure 17.5
The container termination sequence
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="522">
  <data key="d0">Page_522</data>
  <data key="d5">Page_522</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_433">
  <data key="d0">490
CHAPTER 17
Best practices for developing apps
SPECIFYING THE TERMINATION GRACE PERIOD
The termination grace period can be configured in the pod spec by setting the spec.
terminationGracePeriodSeconds field. It defaults to 30, which means the pod’s con-
tainers will be given 30 seconds to terminate gracefully before they’re killed forcibly. 
TIP
You should set the grace period to long enough so your process can fin-
ish cleaning up in that time. 
The grace period specified in the pod spec can also be overridden when deleting the
pod like this:
$ kubectl delete po mypod --grace-period=5
This will make the Kubelet wait five seconds for the pod to shut down cleanly. When
all the pod’s containers stop, the Kubelet notifies the API server and the Pod resource
is finally deleted. You can force the API server to delete the resource immediately,
without waiting for confirmation, by setting the grace period to zero and adding the
--force option like this:
$ kubectl delete po mypod --grace-period=0 --force
Be careful when using this option, especially with pods of a StatefulSet. The Stateful-
Set controller takes great care to never run two instances of the same pod at the same
time (two pods with the same ordinal index and name and attached to the same
PersistentVolume). By force-deleting a pod, you’ll cause the controller to create a
replacement pod without waiting for the containers of the deleted pod to shut
down. In other words, two instances of the same pod might be running at the same
time, which may cause your stateful cluster to malfunction. Only delete stateful pods
forcibly when you’re absolutely sure the pod isn’t running anymore or can’t talk to
the other members of the cluster (you can be sure of this when you confirm that the
node that hosted the pod has failed or has been disconnected from the network and
can’t reconnect). 
 Now that you understand how containers are shut down, let’s look at it from the
application’s perspective and go over how applications should handle the shutdown
procedure.
IMPLEMENTING THE PROPER SHUTDOWN HANDLER IN YOUR APPLICATION
Applications should react to a SIGTERM signal by starting their shut-down procedure
and terminating when it finishes. Instead of handling the SIGTERM signal, the applica-
tion can be notified to shut down through a pre-stop hook. In both cases, the app
then only has a fixed amount of time to terminate cleanly. 
 But what if you can’t predict how long the app will take to shut down cleanly? For
example, imagine your app is a distributed data store. On scale-down, one of the pod
instances will be deleted and therefore shut down. In the shut-down procedure, the
 
</data>
  <data key="d5">490
CHAPTER 17
Best practices for developing apps
SPECIFYING THE TERMINATION GRACE PERIOD
The termination grace period can be configured in the pod spec by setting the spec.
terminationGracePeriodSeconds field. It defaults to 30, which means the pod’s con-
tainers will be given 30 seconds to terminate gracefully before they’re killed forcibly. 
TIP
You should set the grace period to long enough so your process can fin-
ish cleaning up in that time. 
The grace period specified in the pod spec can also be overridden when deleting the
pod like this:
$ kubectl delete po mypod --grace-period=5
This will make the Kubelet wait five seconds for the pod to shut down cleanly. When
all the pod’s containers stop, the Kubelet notifies the API server and the Pod resource
is finally deleted. You can force the API server to delete the resource immediately,
without waiting for confirmation, by setting the grace period to zero and adding the
--force option like this:
$ kubectl delete po mypod --grace-period=0 --force
Be careful when using this option, especially with pods of a StatefulSet. The Stateful-
Set controller takes great care to never run two instances of the same pod at the same
time (two pods with the same ordinal index and name and attached to the same
PersistentVolume). By force-deleting a pod, you’ll cause the controller to create a
replacement pod without waiting for the containers of the deleted pod to shut
down. In other words, two instances of the same pod might be running at the same
time, which may cause your stateful cluster to malfunction. Only delete stateful pods
forcibly when you’re absolutely sure the pod isn’t running anymore or can’t talk to
the other members of the cluster (you can be sure of this when you confirm that the
node that hosted the pod has failed or has been disconnected from the network and
can’t reconnect). 
 Now that you understand how containers are shut down, let’s look at it from the
application’s perspective and go over how applications should handle the shutdown
procedure.
IMPLEMENTING THE PROPER SHUTDOWN HANDLER IN YOUR APPLICATION
Applications should react to a SIGTERM signal by starting their shut-down procedure
and terminating when it finishes. Instead of handling the SIGTERM signal, the applica-
tion can be notified to shut down through a pre-stop hook. In both cases, the app
then only has a fixed amount of time to terminate cleanly. 
 But what if you can’t predict how long the app will take to shut down cleanly? For
example, imagine your app is a distributed data store. On scale-down, one of the pod
instances will be deleted and therefore shut down. In the shut-down procedure, the
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="523">
  <data key="d0">Page_523</data>
  <data key="d5">Page_523</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_434">
  <data key="d0">491
Understanding the pod’s lifecycle
pod needs to migrate all its data to the remaining pods to make sure it’s not lost.
Should the pod start migrating the data upon receiving a termination signal (through
either the SIGTERM signal or through a pre-stop hook)? 
 Absolutely not! This is not recommended for at least the following two reasons:
A container terminating doesn’t necessarily mean the whole pod is being
terminated.
You have no guarantee the shut-down procedure will finish before the process
is killed.
This second scenario doesn’t happen only when the grace period runs out before the
application has finished shutting down gracefully, but also when the node running
the pod fails in the middle of the container shut-down sequence. Even if the node
then starts up again, the Kubelet will not restart the shut-down procedure (it won’t
even start up the container again). There are absolutely no guarantees that the pod
will be allowed to complete its whole shut-down procedure.
REPLACING CRITICAL SHUT-DOWN PROCEDURES WITH DEDICATED SHUT-DOWN PROCEDURE PODS
How do you ensure that a critical shut-down procedure that absolutely must run to
completion does run to completion (for example, to ensure that a pod’s data is
migrated to other pods)?
 One solution is for the app (upon receipt of a termination signal) to create a new
Job resource that would run a new pod, whose sole job is to migrate the deleted pod’s
data to the remaining pods. But if you’ve been paying attention, you’ll know that you
have no guarantee the app will indeed manage to create the Job object every single
time. What if the node fails exactly when the app tries to do that? 
 The proper way to handle this problem is by having a dedicated, constantly run-
ning pod that keeps checking for the existence of orphaned data. When this pod finds
the orphaned data, it can migrate it to the remaining pods. Rather than a constantly
running pod, you can also use a CronJob resource and run the pod periodically. 
 You may think StatefulSets could help here, but they don’t. As you’ll remember,
scaling down a StatefulSet leaves PersistentVolumeClaims orphaned, leaving the data
stored on the PersistentVolume stranded. Yes, upon a subsequent scale-up, the Persistent-
Volume will be reattached to the new pod instance, but what if that scale-up never
happens (or happens after a long time)? For this reason, you may want to run a
data-migrating pod also when using StatefulSets (this scenario is shown in figure 17.6).
To prevent the migration from occurring during an application upgrade, the data-
migrating pod could be configured to wait a while to give the stateful pod time to
come up again before performing the migration.
 
 
 
</data>
  <data key="d5">491
Understanding the pod’s lifecycle
pod needs to migrate all its data to the remaining pods to make sure it’s not lost.
Should the pod start migrating the data upon receiving a termination signal (through
either the SIGTERM signal or through a pre-stop hook)? 
 Absolutely not! This is not recommended for at least the following two reasons:
A container terminating doesn’t necessarily mean the whole pod is being
terminated.
You have no guarantee the shut-down procedure will finish before the process
is killed.
This second scenario doesn’t happen only when the grace period runs out before the
application has finished shutting down gracefully, but also when the node running
the pod fails in the middle of the container shut-down sequence. Even if the node
then starts up again, the Kubelet will not restart the shut-down procedure (it won’t
even start up the container again). There are absolutely no guarantees that the pod
will be allowed to complete its whole shut-down procedure.
REPLACING CRITICAL SHUT-DOWN PROCEDURES WITH DEDICATED SHUT-DOWN PROCEDURE PODS
How do you ensure that a critical shut-down procedure that absolutely must run to
completion does run to completion (for example, to ensure that a pod’s data is
migrated to other pods)?
 One solution is for the app (upon receipt of a termination signal) to create a new
Job resource that would run a new pod, whose sole job is to migrate the deleted pod’s
data to the remaining pods. But if you’ve been paying attention, you’ll know that you
have no guarantee the app will indeed manage to create the Job object every single
time. What if the node fails exactly when the app tries to do that? 
 The proper way to handle this problem is by having a dedicated, constantly run-
ning pod that keeps checking for the existence of orphaned data. When this pod finds
the orphaned data, it can migrate it to the remaining pods. Rather than a constantly
running pod, you can also use a CronJob resource and run the pod periodically. 
 You may think StatefulSets could help here, but they don’t. As you’ll remember,
scaling down a StatefulSet leaves PersistentVolumeClaims orphaned, leaving the data
stored on the PersistentVolume stranded. Yes, upon a subsequent scale-up, the Persistent-
Volume will be reattached to the new pod instance, but what if that scale-up never
happens (or happens after a long time)? For this reason, you may want to run a
data-migrating pod also when using StatefulSets (this scenario is shown in figure 17.6).
To prevent the migration from occurring during an application upgrade, the data-
migrating pod could be configured to wait a while to give the stateful pod time to
come up again before performing the migration.
 
 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="524">
  <data key="d0">Page_524</data>
  <data key="d5">Page_524</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_435">
  <data key="d0">492
CHAPTER 17
Best practices for developing apps
17.3
Ensuring all client requests are handled properly
You now have a good sense of how to make pods shut down cleanly. Now, we’ll look at
the pod’s lifecycle from the perspective of the pod’s clients (clients consuming the ser-
vice the pod is providing). This is important to understand if you don’t want clients to
run into problems when you scale pods up or down.
 It goes without saying that you want all client requests to be handled properly. You
obviously don’t want to see broken connections when pods are starting up or shutting
down. By itself, Kubernetes doesn’t prevent this from happening. Your app needs to
follow a few rules to prevent broken connections. First, let’s focus on making sure all
connections are handled properly when the pod starts up.
17.3.1 Preventing broken client connections when a pod is starting up
Ensuring each connection is handled properly at pod startup is simple if you under-
stand how Services and service Endpoints work. When a pod is started, it’s added as an
endpoint to all the Services, whose label selector matches the pod’s labels. As you may
remember from chapter 5, the pod also needs to signal to Kubernetes that it’s ready.
Until it is, it won’t become a service endpoint and therefore won’t receive any requests
from clients. 
 If you don’t specify a readiness probe in your pod spec, the pod is always considered
ready. It will start receiving requests almost immediately—as soon as the first kube-proxy
updates the iptables rules on its node and the first client pod tries to connect to the
service. If your app isn’t ready to accept connections by then, clients will see “connec-
tion refused” types of errors.
 All you need to do is make sure that your readiness probe returns success only
when your app is ready to properly handle incoming requests. A good first step is to
add an HTTP GET readiness probe and point it to the base URL of your app. In many
Pod
A-0
Pod
A-1
StatefulSet A
Replicas: 2
Scale
down
PVC
A-0
PV
PVC
A-1
PV
Pod
A-0
StatefulSet A
Replicas: 1
Transfers data to
remaining pod(s)
Connects to
orphaned PVC
Data-migrating
Pod
Job
PVC
A-0
PV
PVC
A-1
PV
Figure 17.6
Using a dedicated pod to migrate data 
 
</data>
  <data key="d5">492
CHAPTER 17
Best practices for developing apps
17.3
Ensuring all client requests are handled properly
You now have a good sense of how to make pods shut down cleanly. Now, we’ll look at
the pod’s lifecycle from the perspective of the pod’s clients (clients consuming the ser-
vice the pod is providing). This is important to understand if you don’t want clients to
run into problems when you scale pods up or down.
 It goes without saying that you want all client requests to be handled properly. You
obviously don’t want to see broken connections when pods are starting up or shutting
down. By itself, Kubernetes doesn’t prevent this from happening. Your app needs to
follow a few rules to prevent broken connections. First, let’s focus on making sure all
connections are handled properly when the pod starts up.
17.3.1 Preventing broken client connections when a pod is starting up
Ensuring each connection is handled properly at pod startup is simple if you under-
stand how Services and service Endpoints work. When a pod is started, it’s added as an
endpoint to all the Services, whose label selector matches the pod’s labels. As you may
remember from chapter 5, the pod also needs to signal to Kubernetes that it’s ready.
Until it is, it won’t become a service endpoint and therefore won’t receive any requests
from clients. 
 If you don’t specify a readiness probe in your pod spec, the pod is always considered
ready. It will start receiving requests almost immediately—as soon as the first kube-proxy
updates the iptables rules on its node and the first client pod tries to connect to the
service. If your app isn’t ready to accept connections by then, clients will see “connec-
tion refused” types of errors.
 All you need to do is make sure that your readiness probe returns success only
when your app is ready to properly handle incoming requests. A good first step is to
add an HTTP GET readiness probe and point it to the base URL of your app. In many
Pod
A-0
Pod
A-1
StatefulSet A
Replicas: 2
Scale
down
PVC
A-0
PV
PVC
A-1
PV
Pod
A-0
StatefulSet A
Replicas: 1
Transfers data to
remaining pod(s)
Connects to
orphaned PVC
Data-migrating
Pod
Job
PVC
A-0
PV
PVC
A-1
PV
Figure 17.6
Using a dedicated pod to migrate data 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="525">
  <data key="d0">Page_525</data>
  <data key="d5">Page_525</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_436">
  <data key="d0">493
Ensuring all client requests are handled properly
cases that gets you far enough and saves you from having to implement a special read-
iness endpoint in your app. 
17.3.2 Preventing broken connections during pod shut-down
Now let’s see what happens at the other end of a pod’s life—when the pod is deleted and
its containers are terminated. We’ve already talked about how the pod’s containers
should start shutting down cleanly as soon they receive the SIGTERM signal (or when its
pre-stop hook is executed). But does that ensure all client requests are handled properly? 
 How should the app behave when it receives a termination signal? Should it con-
tinue to accept requests? What about requests that have already been received but
haven’t completed yet? What about persistent HTTP connections, which may be in
between requests, but are open (when no active request exists on the connection)?
Before we can answer those questions, we need to take a detailed look at the chain of
events that unfolds across the cluster when a Pod is deleted. 
UNDERSTANDING THE SEQUENCE OF EVENTS OCCURRING AT POD DELETION
In chapter 11 we took an in-depth look at what components make up a Kubernetes clus-
ter. You need to always keep in mind that those components run as separate processes on
multiple machines. They aren’t all part of a single big monolithic process. It takes time
for all the components to be on the same page regarding the state of the cluster. Let’s
explore this fact by looking at what happens across the cluster when a Pod is deleted.
 When a request for a pod deletion is received by the API server, it first modifies the
state in etcd and then notifies its watchers of the deletion. Among those watchers are
the Kubelet and the Endpoints controller. The two sequences of events, which happen
in parallel (marked with either A or B), are shown in figure 17.7.
A2. Stop
containers
API server
kube-proxy
Kubelet
Worker node
Endpoints
controller
kube-proxy
Pod
(containers)
Client
Delete
pod
B1. Pod deletion
notiﬁcation
B2. Remove pod
as endpoint
A1. Pod deletion
notiﬁcation
B3. Endpoint
modiﬁcation
notiﬁcation
B4. Remove pod
from iptables
B4. Remove pod
from iptables
iptables
iptables
Worker node
Figure 17.7
Sequence of events that occurs when a Pod is deleted
 
</data>
  <data key="d5">493
Ensuring all client requests are handled properly
cases that gets you far enough and saves you from having to implement a special read-
iness endpoint in your app. 
17.3.2 Preventing broken connections during pod shut-down
Now let’s see what happens at the other end of a pod’s life—when the pod is deleted and
its containers are terminated. We’ve already talked about how the pod’s containers
should start shutting down cleanly as soon they receive the SIGTERM signal (or when its
pre-stop hook is executed). But does that ensure all client requests are handled properly? 
 How should the app behave when it receives a termination signal? Should it con-
tinue to accept requests? What about requests that have already been received but
haven’t completed yet? What about persistent HTTP connections, which may be in
between requests, but are open (when no active request exists on the connection)?
Before we can answer those questions, we need to take a detailed look at the chain of
events that unfolds across the cluster when a Pod is deleted. 
UNDERSTANDING THE SEQUENCE OF EVENTS OCCURRING AT POD DELETION
In chapter 11 we took an in-depth look at what components make up a Kubernetes clus-
ter. You need to always keep in mind that those components run as separate processes on
multiple machines. They aren’t all part of a single big monolithic process. It takes time
for all the components to be on the same page regarding the state of the cluster. Let’s
explore this fact by looking at what happens across the cluster when a Pod is deleted.
 When a request for a pod deletion is received by the API server, it first modifies the
state in etcd and then notifies its watchers of the deletion. Among those watchers are
the Kubelet and the Endpoints controller. The two sequences of events, which happen
in parallel (marked with either A or B), are shown in figure 17.7.
A2. Stop
containers
API server
kube-proxy
Kubelet
Worker node
Endpoints
controller
kube-proxy
Pod
(containers)
Client
Delete
pod
B1. Pod deletion
notiﬁcation
B2. Remove pod
as endpoint
A1. Pod deletion
notiﬁcation
B3. Endpoint
modiﬁcation
notiﬁcation
B4. Remove pod
from iptables
B4. Remove pod
from iptables
iptables
iptables
Worker node
Figure 17.7
Sequence of events that occurs when a Pod is deleted
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="526">
  <data key="d0">Page_526</data>
  <data key="d5">Page_526</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_437">
  <data key="d0">494
CHAPTER 17
Best practices for developing apps
In the A sequence of events, you’ll see that as soon as the Kubelet receives the notifica-
tion that the pod should be terminated, it initiates the shutdown sequence as explained
in section 17.2.5 (run the pre-stop hook, send SIGTERM, wait for a period of time, and
then forcibly kill the container if it hasn’t yet terminated on its own). If the app
responds to the SIGTERM by immediately ceasing to receive client requests, any client
trying to connect to it will receive a Connection Refused error. The time it takes for
this to happen from the time the pod is deleted is relatively short because of the direct
path from the API server to the Kubelet.
 Now, let’s look at what happens in the other sequence of events—the one leading
up to the pod being removed from the iptables rules (sequence B in the figure).
When the Endpoints controller (which runs in the Controller Manager in the Kuber-
netes Control Plane) receives the notification of the Pod being deleted, it removes
the pod as an endpoint in all services that the pod is a part of. It does this by modify-
ing the Endpoints API object by sending a REST request to the API server. The API
server then notifies all clients watching the Endpoints object. Among those watchers
are all the kube-proxies running on the worker nodes. Each of these proxies then
updates the iptables rules on its node, which is what prevents new connections
from being forwarded to the terminating pod. An important detail here is that
removing the iptables rules has no effect on existing connections—clients who are
already connected to the pod will still send additional requests to the pod through
those existing connections.
 Both of these sequences of events happen in parallel. Most likely, the time it takes
to shut down the app’s process in the pod is slightly shorter than the time required for
the iptables rules to be updated. The chain of events that leads to iptables rules
being updated is considerably longer (see figure 17.8), because the event must first
reach the Endpoints controller, which then sends a new request to the API server, and
A2. Send
SIGTERM
API server
API server
Kubelet
Endpoints
controller
Container(s)
A1. Watch
notiﬁcation
(pod modiﬁed)
B1. Watch
notiﬁcation
(pod modiﬁed)
B2. Remove pod’s IP
from endpoints
kube-proxy
B4. Update
iptables
rules
iptables
kube-proxy
iptables
Time
B3. Watch notiﬁcation
(endpoints changed)
Figure 17.8
Timeline of events when pod is deleted
 
</data>
  <data key="d5">494
CHAPTER 17
Best practices for developing apps
In the A sequence of events, you’ll see that as soon as the Kubelet receives the notifica-
tion that the pod should be terminated, it initiates the shutdown sequence as explained
in section 17.2.5 (run the pre-stop hook, send SIGTERM, wait for a period of time, and
then forcibly kill the container if it hasn’t yet terminated on its own). If the app
responds to the SIGTERM by immediately ceasing to receive client requests, any client
trying to connect to it will receive a Connection Refused error. The time it takes for
this to happen from the time the pod is deleted is relatively short because of the direct
path from the API server to the Kubelet.
 Now, let’s look at what happens in the other sequence of events—the one leading
up to the pod being removed from the iptables rules (sequence B in the figure).
When the Endpoints controller (which runs in the Controller Manager in the Kuber-
netes Control Plane) receives the notification of the Pod being deleted, it removes
the pod as an endpoint in all services that the pod is a part of. It does this by modify-
ing the Endpoints API object by sending a REST request to the API server. The API
server then notifies all clients watching the Endpoints object. Among those watchers
are all the kube-proxies running on the worker nodes. Each of these proxies then
updates the iptables rules on its node, which is what prevents new connections
from being forwarded to the terminating pod. An important detail here is that
removing the iptables rules has no effect on existing connections—clients who are
already connected to the pod will still send additional requests to the pod through
those existing connections.
 Both of these sequences of events happen in parallel. Most likely, the time it takes
to shut down the app’s process in the pod is slightly shorter than the time required for
the iptables rules to be updated. The chain of events that leads to iptables rules
being updated is considerably longer (see figure 17.8), because the event must first
reach the Endpoints controller, which then sends a new request to the API server, and
A2. Send
SIGTERM
API server
API server
Kubelet
Endpoints
controller
Container(s)
A1. Watch
notiﬁcation
(pod modiﬁed)
B1. Watch
notiﬁcation
(pod modiﬁed)
B2. Remove pod’s IP
from endpoints
kube-proxy
B4. Update
iptables
rules
iptables
kube-proxy
iptables
Time
B3. Watch notiﬁcation
(endpoints changed)
Figure 17.8
Timeline of events when pod is deleted
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="527">
  <data key="d0">Page_527</data>
  <data key="d5">Page_527</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_438">
  <data key="d0">495
Ensuring all client requests are handled properly
then the API server must notify the kube-proxy before the proxy finally modifies the
iptables rules. A high probability exists that the SIGTERM signal will be sent well
before the iptables rules are updated on all nodes.
 The end result is that the pod may still receive client requests after it was sent the
termination signal. If the app closes the server socket and stops accepting connections
immediately, this will cause clients to receive “Connection Refused” types of errors
(similar to what happens at pod startup if your app isn’t capable of accepting connec-
tions immediately and you don’t define a readiness probe for it). 
SOLVING THE PROBLEM
Googling solutions to this problem makes it seem as though adding a readiness probe
to your pod will solve the problem. Supposedly, all you need to do is make the readi-
ness probe start failing as soon as the pod receives the SIGTERM. This is supposed to
cause the pod to be removed as the endpoint of the service. But the removal would
happen only after the readiness probe fails for a few consecutive times (this is configu-
rable in the readiness probe spec). And, obviously, the removal then still needs to
reach the kube-proxy before the pod is removed from iptables rules. 
 In reality, the readiness probe has absolutely no bearing on the whole process at
all. The Endpoints controller removes the pod from the service Endpoints as soon as
it receives notice of the pod being deleted (when the deletionTimestamp field in the
pod’s spec is no longer null). From that point on, the result of the readiness probe
is irrelevant.
 What’s the proper solution to the problem? How can you make sure all requests
are handled fully?
 It’s clear the pod needs to keep accepting connections even after it receives the ter-
mination signal up until all the kube-proxies have finished updating the iptables
rules. Well, it’s not only the kube-proxies. There may also be Ingress controllers or
load balancers forwarding connections to the pod directly, without going through the
Service (iptables). This also includes clients using client-side load-balancing. To
ensure none of the clients experience broken connections, you’d have to wait until all
of them somehow notify you they’ll no longer forward connections to the pod. 
 That’s impossible, because all those components are distributed across many dif-
ferent computers. Even if you knew the location of every one of them and could wait
until all of them say it’s okay to shut down the pod, what do you do if one of them
doesn’t respond? How long do you wait for the response? Remember, during that
time, you’re holding up the shut-down process. 
 The only reasonable thing you can do is wait for a long-enough time to ensure all
the proxies have done their job. But how long is long enough? A few seconds should
be enough in most situations, but there’s no guarantee it will suffice every time. When
the API server or the Endpoints controller is overloaded, it may take longer for the
notification to reach the kube-proxy. It’s important to understand that you can’t solve
the problem perfectly, but even adding a 5- or 10-second delay should improve the
user experience considerably. You can use a longer delay, but don’t go overboard,
 
</data>
  <data key="d5">495
Ensuring all client requests are handled properly
then the API server must notify the kube-proxy before the proxy finally modifies the
iptables rules. A high probability exists that the SIGTERM signal will be sent well
before the iptables rules are updated on all nodes.
 The end result is that the pod may still receive client requests after it was sent the
termination signal. If the app closes the server socket and stops accepting connections
immediately, this will cause clients to receive “Connection Refused” types of errors
(similar to what happens at pod startup if your app isn’t capable of accepting connec-
tions immediately and you don’t define a readiness probe for it). 
SOLVING THE PROBLEM
Googling solutions to this problem makes it seem as though adding a readiness probe
to your pod will solve the problem. Supposedly, all you need to do is make the readi-
ness probe start failing as soon as the pod receives the SIGTERM. This is supposed to
cause the pod to be removed as the endpoint of the service. But the removal would
happen only after the readiness probe fails for a few consecutive times (this is configu-
rable in the readiness probe spec). And, obviously, the removal then still needs to
reach the kube-proxy before the pod is removed from iptables rules. 
 In reality, the readiness probe has absolutely no bearing on the whole process at
all. The Endpoints controller removes the pod from the service Endpoints as soon as
it receives notice of the pod being deleted (when the deletionTimestamp field in the
pod’s spec is no longer null). From that point on, the result of the readiness probe
is irrelevant.
 What’s the proper solution to the problem? How can you make sure all requests
are handled fully?
 It’s clear the pod needs to keep accepting connections even after it receives the ter-
mination signal up until all the kube-proxies have finished updating the iptables
rules. Well, it’s not only the kube-proxies. There may also be Ingress controllers or
load balancers forwarding connections to the pod directly, without going through the
Service (iptables). This also includes clients using client-side load-balancing. To
ensure none of the clients experience broken connections, you’d have to wait until all
of them somehow notify you they’ll no longer forward connections to the pod. 
 That’s impossible, because all those components are distributed across many dif-
ferent computers. Even if you knew the location of every one of them and could wait
until all of them say it’s okay to shut down the pod, what do you do if one of them
doesn’t respond? How long do you wait for the response? Remember, during that
time, you’re holding up the shut-down process. 
 The only reasonable thing you can do is wait for a long-enough time to ensure all
the proxies have done their job. But how long is long enough? A few seconds should
be enough in most situations, but there’s no guarantee it will suffice every time. When
the API server or the Endpoints controller is overloaded, it may take longer for the
notification to reach the kube-proxy. It’s important to understand that you can’t solve
the problem perfectly, but even adding a 5- or 10-second delay should improve the
user experience considerably. You can use a longer delay, but don’t go overboard,
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="528">
  <data key="d0">Page_528</data>
  <data key="d5">Page_528</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_439">
  <data key="d0">496
CHAPTER 17
Best practices for developing apps
because the delay will prevent the container from shutting down promptly and will
cause the pod to be shown in lists long after it has been deleted, which is always frus-
trating to the user deleting the pod.
WRAPPING UP THIS SECTION
To recap—properly shutting down an application includes these steps:
Wait for a few seconds, then stop accepting new connections. 
Close all keep-alive connections not in the middle of a request.
Wait for all active requests to finish.
Then shut down completely.
To understand what’s happening with the connections and requests during this pro-
cess, examine figure 17.9 carefully.
Not as simple as exiting the process immediately upon receiving the termination sig-
nal, right? Is it worth going through all this? That’s for you to decide. But the least you
can do is add a pre-stop hook that waits a few seconds, like the one in the following
listing, perhaps.
    lifecycle:                    
      preStop:                    
        exec:                     
          command:                
          - sh
          - -c
          - "sleep 5"
Listing 17.7
A pre-stop hook for preventing broken connections
Delay (few seconds)
Key:
Connection
Request
iptables rules
updated on all nodes
(no new connections
after this point)
Stop
accepting new
connections
Close inactive
keep-alive
connections
and wait for
active requests
to ﬁnish
When last
active request
completes,
shut down
completely
Time
SIGTERM
Figure 17.9
Properly handling existing and new connections after receiving a termination signal
 
</data>
  <data key="d5">496
CHAPTER 17
Best practices for developing apps
because the delay will prevent the container from shutting down promptly and will
cause the pod to be shown in lists long after it has been deleted, which is always frus-
trating to the user deleting the pod.
WRAPPING UP THIS SECTION
To recap—properly shutting down an application includes these steps:
Wait for a few seconds, then stop accepting new connections. 
Close all keep-alive connections not in the middle of a request.
Wait for all active requests to finish.
Then shut down completely.
To understand what’s happening with the connections and requests during this pro-
cess, examine figure 17.9 carefully.
Not as simple as exiting the process immediately upon receiving the termination sig-
nal, right? Is it worth going through all this? That’s for you to decide. But the least you
can do is add a pre-stop hook that waits a few seconds, like the one in the following
listing, perhaps.
    lifecycle:                    
      preStop:                    
        exec:                     
          command:                
          - sh
          - -c
          - "sleep 5"
Listing 17.7
A pre-stop hook for preventing broken connections
Delay (few seconds)
Key:
Connection
Request
iptables rules
updated on all nodes
(no new connections
after this point)
Stop
accepting new
connections
Close inactive
keep-alive
connections
and wait for
active requests
to ﬁnish
When last
active request
completes,
shut down
completely
Time
SIGTERM
Figure 17.9
Properly handling existing and new connections after receiving a termination signal
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="529">
  <data key="d0">Page_529</data>
  <data key="d5">Page_529</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_440">
  <data key="d0">497
Making your apps easy to run and manage in Kubernetes
This way, you don’t need to modify the code of your app at all. If your app already
ensures all in-flight requests are processed completely, this pre-stop delay may be all
you need.
17.4
Making your apps easy to run and manage in Kubernetes
I hope you now have a better sense of how to make your apps handle clients nicely.
Now we’ll look at other aspects of how an app should be built to make it easier to man-
age in Kubernetes.
17.4.1 Making manageable container images
When you package your app into an image, you can choose to include the app’s
binary executable and any additional libraries it needs, or you can package up a whole
OS filesystem along with the app. Way too many people do this, even though it’s usu-
ally unnecessary.
 Do you need every single file from an OS distribution in your image? Probably not.
Most of the files will never be used and will make your image larger than it needs to
be. Sure, the layering of images makes sure each individual layer is downloaded only
once, but even having to wait longer than necessary the first time a pod is scheduled
to a node is undesirable.
 Deploying new pods and scaling them should be fast. This demands having small
images without unnecessary cruft. If you’re building apps using the Go language, your
images don’t need to include anything else apart from the app’s single binary execut-
able file. This makes Go-based container images extremely small and perfect for
Kubernetes.
TIP
Use the FROM scratch directive in the Dockerfile for these images.
But in practice, you’ll soon see these minimal images are extremely difficult to debug.
The first time you need to run a tool such as ping, dig, curl, or something similar
inside the container, you’ll realize how important it is for container images to also
include at least a limited set of these tools. I can’t tell you what to include and what
not to include in your images, because it depends on how you do things, so you’ll
need to find the sweet spot yourself.
17.4.2 Properly tagging your images and using imagePullPolicy wisely
You’ll also soon learn that referring to the latest image tag in your pod manifests will
cause problems, because you can’t tell which version of the image each individual pod
replica is running. Even if initially all your pod replicas run the same image version, if
you push a new version of the image under the latest tag, and then pods are resched-
uled (or you scale up your Deployment), the new pods will run the new version,
whereas the old ones will still be running the old one. Also, using the latest tag
makes it impossible to roll back to a previous version (unless you push the old version
of the image again).
 
</data>
  <data key="d5">497
Making your apps easy to run and manage in Kubernetes
This way, you don’t need to modify the code of your app at all. If your app already
ensures all in-flight requests are processed completely, this pre-stop delay may be all
you need.
17.4
Making your apps easy to run and manage in Kubernetes
I hope you now have a better sense of how to make your apps handle clients nicely.
Now we’ll look at other aspects of how an app should be built to make it easier to man-
age in Kubernetes.
17.4.1 Making manageable container images
When you package your app into an image, you can choose to include the app’s
binary executable and any additional libraries it needs, or you can package up a whole
OS filesystem along with the app. Way too many people do this, even though it’s usu-
ally unnecessary.
 Do you need every single file from an OS distribution in your image? Probably not.
Most of the files will never be used and will make your image larger than it needs to
be. Sure, the layering of images makes sure each individual layer is downloaded only
once, but even having to wait longer than necessary the first time a pod is scheduled
to a node is undesirable.
 Deploying new pods and scaling them should be fast. This demands having small
images without unnecessary cruft. If you’re building apps using the Go language, your
images don’t need to include anything else apart from the app’s single binary execut-
able file. This makes Go-based container images extremely small and perfect for
Kubernetes.
TIP
Use the FROM scratch directive in the Dockerfile for these images.
But in practice, you’ll soon see these minimal images are extremely difficult to debug.
The first time you need to run a tool such as ping, dig, curl, or something similar
inside the container, you’ll realize how important it is for container images to also
include at least a limited set of these tools. I can’t tell you what to include and what
not to include in your images, because it depends on how you do things, so you’ll
need to find the sweet spot yourself.
17.4.2 Properly tagging your images and using imagePullPolicy wisely
You’ll also soon learn that referring to the latest image tag in your pod manifests will
cause problems, because you can’t tell which version of the image each individual pod
replica is running. Even if initially all your pod replicas run the same image version, if
you push a new version of the image under the latest tag, and then pods are resched-
uled (or you scale up your Deployment), the new pods will run the new version,
whereas the old ones will still be running the old one. Also, using the latest tag
makes it impossible to roll back to a previous version (unless you push the old version
of the image again).
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="530">
  <data key="d0">Page_530</data>
  <data key="d5">Page_530</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_441">
  <data key="d0">498
CHAPTER 17
Best practices for developing apps
 It’s almost mandatory to use tags containing a proper version designator instead
of latest, except maybe in development. Keep in mind that if you use mutable tags
(you push changes to the same tag), you’ll need to set the imagePullPolicy field in
the pod spec to Always. But if you use that in production pods, be aware of the big
caveat associated with it. If the image pull policy is set to Always, the container run-
time will contact the image registry every time a new pod is deployed. This slows
down pod startup a bit, because the node needs to check if the image has been mod-
ified. Worse yet, this policy prevents the pod from starting up when the registry can-
not be contacted.
17.4.3 Using multi-dimensional instead of single-dimensional labels
Don’t forget to label all your resources, not only Pods. Make sure you add multiple
labels to each resource, so they can be selected across each individual dimension. You
(or the ops team) will be grateful you did it when the number of resources increases.
 Labels may include things like
The name of the application (or perhaps microservice) the resource belongs to
Application tier (front-end, back-end, and so on)
Environment (development, QA, staging, production, and so on)
Version
Type of release (stable, canary, green or blue for green/blue deployments, and
so on)
Tenant (if you’re running separate pods for each tenant instead of using name-
spaces)
Shard for sharded systems
This will allow you to manage resources in groups instead of individually and make it
easy to see where each resource belongs.
17.4.4 Describing each resource through annotations
To add additional information to your resources use annotations. At the least,
resources should contain an annotation describing the resource and an annotation
with contact information of the person responsible for it. 
 In a microservices architecture, pods could contain an annotation that lists the
names of the other services the pod is using. This makes it possible to show dependen-
cies between pods. Other annotations could include build and version information
and metadata used by tooling or graphical user interfaces (icon names, and so on).
 Both labels and annotations make managing running applications much easier, but
nothing is worse than when an application starts crashing and you don’t know why.
17.4.5 Providing information on why the process terminated
Nothing is more frustrating than having to figure out why a container terminated
(or is even terminating continuously), especially if it happens at the worst possible
 
</data>
  <data key="d5">498
CHAPTER 17
Best practices for developing apps
 It’s almost mandatory to use tags containing a proper version designator instead
of latest, except maybe in development. Keep in mind that if you use mutable tags
(you push changes to the same tag), you’ll need to set the imagePullPolicy field in
the pod spec to Always. But if you use that in production pods, be aware of the big
caveat associated with it. If the image pull policy is set to Always, the container run-
time will contact the image registry every time a new pod is deployed. This slows
down pod startup a bit, because the node needs to check if the image has been mod-
ified. Worse yet, this policy prevents the pod from starting up when the registry can-
not be contacted.
17.4.3 Using multi-dimensional instead of single-dimensional labels
Don’t forget to label all your resources, not only Pods. Make sure you add multiple
labels to each resource, so they can be selected across each individual dimension. You
(or the ops team) will be grateful you did it when the number of resources increases.
 Labels may include things like
The name of the application (or perhaps microservice) the resource belongs to
Application tier (front-end, back-end, and so on)
Environment (development, QA, staging, production, and so on)
Version
Type of release (stable, canary, green or blue for green/blue deployments, and
so on)
Tenant (if you’re running separate pods for each tenant instead of using name-
spaces)
Shard for sharded systems
This will allow you to manage resources in groups instead of individually and make it
easy to see where each resource belongs.
17.4.4 Describing each resource through annotations
To add additional information to your resources use annotations. At the least,
resources should contain an annotation describing the resource and an annotation
with contact information of the person responsible for it. 
 In a microservices architecture, pods could contain an annotation that lists the
names of the other services the pod is using. This makes it possible to show dependen-
cies between pods. Other annotations could include build and version information
and metadata used by tooling or graphical user interfaces (icon names, and so on).
 Both labels and annotations make managing running applications much easier, but
nothing is worse than when an application starts crashing and you don’t know why.
17.4.5 Providing information on why the process terminated
Nothing is more frustrating than having to figure out why a container terminated
(or is even terminating continuously), especially if it happens at the worst possible
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="531">
  <data key="d0">Page_531</data>
  <data key="d5">Page_531</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_442">
  <data key="d0">499
Making your apps easy to run and manage in Kubernetes
moment. Be nice to the ops people and make their lives easier by including all the
necessary debug information in your log files. 
 But to make triage even easier, you can use one other Kubernetes feature that
makes it possible to show the reason why a container terminated in the pod’s status.
You do this by having the process write a termination message to a specific file in the
container’s filesystem. The contents of this file are read by the Kubelet when the con-
tainer terminates and are shown in the output of kubectl describe pod. If an applica-
tion uses this mechanism, an operator can quickly see why the app terminated without
even having to look at the container logs. 
 The default file the process needs to write the message to is /dev/termination-log,
but it can be changed by setting the terminationMessagePath field in the container
definition in the pod spec. 
 You can see this in action by running a pod whose container dies immediately, as
shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-termination-message
spec:
  containers:
  - image: busybox
    name: main
    terminationMessagePath: /var/termination-reason         
    command:
    - sh
    - -c
    - 'echo "I''ve had enough" &gt; /var/termination-reason ; exit 1'   
When running this pod, you’ll soon see the pod’s status shown as CrashLoopBackOff.
If you then use kubectl describe, you can see why the container died, without having
to dig down into its logs, as shown in the following listing.
$ kubectl describe po
Name:           pod-with-termination-message
...
Containers:
...
    State:      Waiting
      Reason:   CrashLoopBackOff
    Last State: Terminated
      Reason:   Error
      Message:  I've had enough          
      Exit Code:        1
      Started:          Tue, 21 Feb 2017 21:38:31 +0100
      Finished:         Tue, 21 Feb 2017 21:38:31 +0100
Listing 17.8
Pod writing a termination message: termination-message.yaml
Listing 17.9
Seeing the container’s termination message with kubectl describe
You’re overriding the 
default path of the 
termination message file.
The container
will write the
message to
the file just
before exiting.
You can see the reason 
why the container died 
without having to 
inspect its logs.
 
</data>
  <data key="d5">499
Making your apps easy to run and manage in Kubernetes
moment. Be nice to the ops people and make their lives easier by including all the
necessary debug information in your log files. 
 But to make triage even easier, you can use one other Kubernetes feature that
makes it possible to show the reason why a container terminated in the pod’s status.
You do this by having the process write a termination message to a specific file in the
container’s filesystem. The contents of this file are read by the Kubelet when the con-
tainer terminates and are shown in the output of kubectl describe pod. If an applica-
tion uses this mechanism, an operator can quickly see why the app terminated without
even having to look at the container logs. 
 The default file the process needs to write the message to is /dev/termination-log,
but it can be changed by setting the terminationMessagePath field in the container
definition in the pod spec. 
 You can see this in action by running a pod whose container dies immediately, as
shown in the following listing.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-termination-message
spec:
  containers:
  - image: busybox
    name: main
    terminationMessagePath: /var/termination-reason         
    command:
    - sh
    - -c
    - 'echo "I''ve had enough" &gt; /var/termination-reason ; exit 1'   
When running this pod, you’ll soon see the pod’s status shown as CrashLoopBackOff.
If you then use kubectl describe, you can see why the container died, without having
to dig down into its logs, as shown in the following listing.
$ kubectl describe po
Name:           pod-with-termination-message
...
Containers:
...
    State:      Waiting
      Reason:   CrashLoopBackOff
    Last State: Terminated
      Reason:   Error
      Message:  I've had enough          
      Exit Code:        1
      Started:          Tue, 21 Feb 2017 21:38:31 +0100
      Finished:         Tue, 21 Feb 2017 21:38:31 +0100
Listing 17.8
Pod writing a termination message: termination-message.yaml
Listing 17.9
Seeing the container’s termination message with kubectl describe
You’re overriding the 
default path of the 
termination message file.
The container
will write the
message to
the file just
before exiting.
You can see the reason 
why the container died 
without having to 
inspect its logs.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="532">
  <data key="d0">Page_532</data>
  <data key="d5">Page_532</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_443">
  <data key="d0">500
CHAPTER 17
Best practices for developing apps
    Ready:              False
    Restart Count:      6
As you can see, the “I’ve had enough” message the process wrote to the file /var/ter-
mination-reason is shown in the container’s Last State section. Note that this mecha-
nism isn’t limited only to containers that crash. It can also be used in pods that run a
completable task and terminate successfully (you’ll find an example in the file termi-
nation-message-success.yaml). 
 This mechanism is great for terminated containers, but you’ll probably agree that
a similar mechanism would also be useful for showing app-specific status messages of
running, not only terminated, containers. Kubernetes currently doesn’t provide any
such functionality and I’m not aware of any plans to introduce it.
NOTE
If the container doesn’t write the message to any file, you can set the
terminationMessagePolicy field to FallbackToLogsOnError. In that case,
the last few lines of the container’s log are used as its termination message
(but only when the container terminates unsuccessfully).
17.4.6 Handling application logs
While we’re on the subject of application logging, let’s reiterate that apps should write
to the standard output instead of files. This makes it easy to view logs with the kubectl
logs command. 
TIP
If a container crashes and is replaced with a new one, you’ll see the new
container’s log. To see the previous container’s logs, use the --previous
option with kubectl logs.
If the application logs to a file instead of the standard output, you can display the log
file using an alternative approach: 
$ kubectl exec &lt;pod&gt; cat &lt;logfile&gt;
This executes the cat command inside the container and streams the logs back to
kubectl, which prints them out in your terminal. 
COPYING LOG AND OTHER FILES TO AND FROM A CONTAINER
You can also copy the log file to your local machine using the kubectl cp command,
which we haven’t looked at yet. It allows you to copy files from and into a container. For
example, if a pod called foo-pod and its single container contains a file at /var/log/
foo.log, you can transfer it to your local machine with the following command:
$ kubectl cp foo-pod:/var/log/foo.log foo.log
To copy a file from your local machine into the pod, specify the pod’s name in the sec-
ond argument:
$ kubectl cp localfile foo-pod:/etc/remotefile
 
</data>
  <data key="d5">500
CHAPTER 17
Best practices for developing apps
    Ready:              False
    Restart Count:      6
As you can see, the “I’ve had enough” message the process wrote to the file /var/ter-
mination-reason is shown in the container’s Last State section. Note that this mecha-
nism isn’t limited only to containers that crash. It can also be used in pods that run a
completable task and terminate successfully (you’ll find an example in the file termi-
nation-message-success.yaml). 
 This mechanism is great for terminated containers, but you’ll probably agree that
a similar mechanism would also be useful for showing app-specific status messages of
running, not only terminated, containers. Kubernetes currently doesn’t provide any
such functionality and I’m not aware of any plans to introduce it.
NOTE
If the container doesn’t write the message to any file, you can set the
terminationMessagePolicy field to FallbackToLogsOnError. In that case,
the last few lines of the container’s log are used as its termination message
(but only when the container terminates unsuccessfully).
17.4.6 Handling application logs
While we’re on the subject of application logging, let’s reiterate that apps should write
to the standard output instead of files. This makes it easy to view logs with the kubectl
logs command. 
TIP
If a container crashes and is replaced with a new one, you’ll see the new
container’s log. To see the previous container’s logs, use the --previous
option with kubectl logs.
If the application logs to a file instead of the standard output, you can display the log
file using an alternative approach: 
$ kubectl exec &lt;pod&gt; cat &lt;logfile&gt;
This executes the cat command inside the container and streams the logs back to
kubectl, which prints them out in your terminal. 
COPYING LOG AND OTHER FILES TO AND FROM A CONTAINER
You can also copy the log file to your local machine using the kubectl cp command,
which we haven’t looked at yet. It allows you to copy files from and into a container. For
example, if a pod called foo-pod and its single container contains a file at /var/log/
foo.log, you can transfer it to your local machine with the following command:
$ kubectl cp foo-pod:/var/log/foo.log foo.log
To copy a file from your local machine into the pod, specify the pod’s name in the sec-
ond argument:
$ kubectl cp localfile foo-pod:/etc/remotefile
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="533">
  <data key="d0">Page_533</data>
  <data key="d5">Page_533</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_444">
  <data key="d0">501
Making your apps easy to run and manage in Kubernetes
This copies the file localfile to /etc/remotefile inside the pod’s container. If the pod has
more than one container, you specify the container using the -c containerName option.
USING CENTRALIZED LOGGING
In a production system, you’ll want to use a centralized, cluster-wide logging solution,
so all your logs are collected and (permanently) stored in a central location. This
allows you to examine historical logs and analyze trends. Without such a system, a
pod’s logs are only available while the pod exists. As soon as it’s deleted, its logs are
deleted also. 
 Kubernetes by itself doesn’t provide any kind of centralized logging. The compo-
nents necessary for providing a centralized storage and analysis of all the container
logs must be provided by additional components, which usually run as regular pods in
the cluster. 
 Deploying centralized logging solutions is easy. All you need to do is deploy a few
YAML/JSON manifests and you’re good to go. On Google Kubernetes Engine, it’s
even easier. Check the Enable Stackdriver Logging checkbox when setting up the clus-
ter. Setting up centralized logging on an on-premises Kubernetes cluster is beyond the
scope of this book, but I’ll give you a quick overview of how it’s usually done.
 You may have already heard of the ELK stack composed of ElasticSearch, Logstash,
and Kibana. A slightly modified variation is the EFK stack, where Logstash is replaced
with FluentD. 
 When using the EFK stack for centralized logging, each Kubernetes cluster node
runs a FluentD agent (usually as a pod deployed through a DaemonSet), which is
responsible for gathering the logs from the containers, tagging them with pod-specific
information, and delivering them to ElasticSearch, which stores them persistently.
ElasticSearch is also deployed as a pod somewhere in the cluster. The logs can then be
viewed and analyzed in a web browser through Kibana, which is a web tool for visualiz-
ing ElasticSearch data. It also usually runs as a pod and is exposed through a Service.
The three components of the EFK stack are shown in the following figure.
NOTE
In the next chapter, you’ll learn about Helm charts. You can use charts
created by the Kubernetes community to deploy the EFK stack instead of cre-
ating your own YAML manifests. 
Node 1
Container logs
Kibana
Web
browser
FluentD
Node 2
Container logs
FluentD
Node 3
Container logs
FluentD
ElasticSearch
Figure 17.10
Centralized logging with FluentD, ElasticSearch, and Kibana
 
</data>
  <data key="d5">501
Making your apps easy to run and manage in Kubernetes
This copies the file localfile to /etc/remotefile inside the pod’s container. If the pod has
more than one container, you specify the container using the -c containerName option.
USING CENTRALIZED LOGGING
In a production system, you’ll want to use a centralized, cluster-wide logging solution,
so all your logs are collected and (permanently) stored in a central location. This
allows you to examine historical logs and analyze trends. Without such a system, a
pod’s logs are only available while the pod exists. As soon as it’s deleted, its logs are
deleted also. 
 Kubernetes by itself doesn’t provide any kind of centralized logging. The compo-
nents necessary for providing a centralized storage and analysis of all the container
logs must be provided by additional components, which usually run as regular pods in
the cluster. 
 Deploying centralized logging solutions is easy. All you need to do is deploy a few
YAML/JSON manifests and you’re good to go. On Google Kubernetes Engine, it’s
even easier. Check the Enable Stackdriver Logging checkbox when setting up the clus-
ter. Setting up centralized logging on an on-premises Kubernetes cluster is beyond the
scope of this book, but I’ll give you a quick overview of how it’s usually done.
 You may have already heard of the ELK stack composed of ElasticSearch, Logstash,
and Kibana. A slightly modified variation is the EFK stack, where Logstash is replaced
with FluentD. 
 When using the EFK stack for centralized logging, each Kubernetes cluster node
runs a FluentD agent (usually as a pod deployed through a DaemonSet), which is
responsible for gathering the logs from the containers, tagging them with pod-specific
information, and delivering them to ElasticSearch, which stores them persistently.
ElasticSearch is also deployed as a pod somewhere in the cluster. The logs can then be
viewed and analyzed in a web browser through Kibana, which is a web tool for visualiz-
ing ElasticSearch data. It also usually runs as a pod and is exposed through a Service.
The three components of the EFK stack are shown in the following figure.
NOTE
In the next chapter, you’ll learn about Helm charts. You can use charts
created by the Kubernetes community to deploy the EFK stack instead of cre-
ating your own YAML manifests. 
Node 1
Container logs
Kibana
Web
browser
FluentD
Node 2
Container logs
FluentD
Node 3
Container logs
FluentD
ElasticSearch
Figure 17.10
Centralized logging with FluentD, ElasticSearch, and Kibana
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="534">
  <data key="d0">Page_534</data>
  <data key="d5">Page_534</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_445">
  <data key="d0">502
CHAPTER 17
Best practices for developing apps
HANDLING MULTI-LINE LOG STATEMENTS
The FluentD agent stores each line of the log file as an entry in the ElasticSearch
data store. There’s one problem with that. Log statements spanning multiple lines,
such as exception stack traces in Java, appear as separate entries in the centralized
logging system. 
 To solve this problem, you can have the apps output JSON instead of plain text.
This way, a multiline log statement can be stored and shown in Kibana as a single
entry. But that makes viewing logs with kubectl logs much less human-friendly. 
 The solution may be to keep outputting human-readable logs to standard output,
while writing JSON logs to a file and having them processed by FluentD. This requires
configuring the node-level FluentD agent appropriately or adding a logging sidecar
container to every pod. 
17.5
Best practices for development and testing
We’ve talked about what to be mindful of when developing apps, but we haven’t
talked about the development and testing workflows that will help you streamline
those processes. I don’t want to go into too much detail here, because everyone needs
to find what works best for them, but here are a few starting points.
17.5.1 Running apps outside of Kubernetes during development
When you’re developing an app that will run in a production Kubernetes cluster, does
that mean you also need to run it in Kubernetes during development? Not really. Hav-
ing to build the app after each minor change, then build the container image, push it
to a registry, and then re-deploy the pods would make development slow and painful.
Luckily, you don’t need to go through all that trouble.
 You can always develop and run apps on your local machine, the way you’re used
to. After all, an app running in Kubernetes is a regular (although isolated) process
running on one of the cluster nodes. If the app depends on certain features the
Kubernetes environment provides, you can easily replicate that environment on your
development machine.
 I’m not even talking about running the app in a container. Most of the time, you
don’t need that—you can usually run the app directly from your IDE. 
CONNECTING TO BACKEND SERVICES
In production, if the app connects to a backend Service and uses the BACKEND_SERVICE
_HOST and BACKEND_SERVICE_PORT environment variables to find the Service’s coordi-
nates, you can obviously set those environment variables on your local machine manu-
ally and point them to the backend Service, regardless of if it’s running outside or
inside a Kubernetes cluster. If it’s running inside Kubernetes, you can always (at least
temporarily) make the Service accessible externally by changing it to a NodePort or a
LoadBalancer-type Service. 
 
</data>
  <data key="d5">502
CHAPTER 17
Best practices for developing apps
HANDLING MULTI-LINE LOG STATEMENTS
The FluentD agent stores each line of the log file as an entry in the ElasticSearch
data store. There’s one problem with that. Log statements spanning multiple lines,
such as exception stack traces in Java, appear as separate entries in the centralized
logging system. 
 To solve this problem, you can have the apps output JSON instead of plain text.
This way, a multiline log statement can be stored and shown in Kibana as a single
entry. But that makes viewing logs with kubectl logs much less human-friendly. 
 The solution may be to keep outputting human-readable logs to standard output,
while writing JSON logs to a file and having them processed by FluentD. This requires
configuring the node-level FluentD agent appropriately or adding a logging sidecar
container to every pod. 
17.5
Best practices for development and testing
We’ve talked about what to be mindful of when developing apps, but we haven’t
talked about the development and testing workflows that will help you streamline
those processes. I don’t want to go into too much detail here, because everyone needs
to find what works best for them, but here are a few starting points.
17.5.1 Running apps outside of Kubernetes during development
When you’re developing an app that will run in a production Kubernetes cluster, does
that mean you also need to run it in Kubernetes during development? Not really. Hav-
ing to build the app after each minor change, then build the container image, push it
to a registry, and then re-deploy the pods would make development slow and painful.
Luckily, you don’t need to go through all that trouble.
 You can always develop and run apps on your local machine, the way you’re used
to. After all, an app running in Kubernetes is a regular (although isolated) process
running on one of the cluster nodes. If the app depends on certain features the
Kubernetes environment provides, you can easily replicate that environment on your
development machine.
 I’m not even talking about running the app in a container. Most of the time, you
don’t need that—you can usually run the app directly from your IDE. 
CONNECTING TO BACKEND SERVICES
In production, if the app connects to a backend Service and uses the BACKEND_SERVICE
_HOST and BACKEND_SERVICE_PORT environment variables to find the Service’s coordi-
nates, you can obviously set those environment variables on your local machine manu-
ally and point them to the backend Service, regardless of if it’s running outside or
inside a Kubernetes cluster. If it’s running inside Kubernetes, you can always (at least
temporarily) make the Service accessible externally by changing it to a NodePort or a
LoadBalancer-type Service. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="535">
  <data key="d0">Page_535</data>
  <data key="d5">Page_535</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_446">
  <data key="d0">503
Best practices for development and testing
CONNECTING TO THE API SERVER
Similarly, if your app requires access to the Kubernetes API server when running
inside a Kubernetes cluster, it can easily talk to the API server from outside the cluster
during development. If it uses the ServiceAccount’s token to authenticate itself, you
can always copy the ServiceAccount’s Secret’s files to your local machine with kubectl
cp. The API server doesn’t care if the client accessing it is inside or outside the cluster. 
 If the app uses an ambassador container like the one described in chapter 8, you
don’t even need those Secret files. Run kubectl proxy on your local machine, run
your app locally, and it should be ready to talk to your local kubectl proxy (as long as
it and the ambassador container bind the proxy to the same port).
 In this case, you’ll need to make sure the user account your local kubectl is using
has the same privileges as the ServiceAccount the app will run under.
RUNNING INSIDE A CONTAINER EVEN DURING DEVELOPMENT
When during development you absolutely have to run the app in a container for what-
ever reason, there is a way of avoiding having to build the container image every time.
Instead of baking the binaries into the image, you can always mount your local filesys-
tem into the container through Docker volumes, for example. This way, after you
build a new version of the app’s binaries, all you need to do is restart the container (or
not even that, if hot-redeploy is supported). No need to rebuild the image.
17.5.2 Using Minikube in development
As you can see, nothing forces you to run your app inside Kubernetes during develop-
ment. But you may do that anyway to see how the app behaves in a true Kubernetes
environment.
 You may have used Minikube to run examples in this book. Although a Minikube
cluster runs only a single worker node, it’s nevertheless a valuable method of trying
out your app in Kubernetes (and, of course, developing all the resource manifests that
make up your complete application). Minikube doesn’t offer everything that a proper
multi-node Kubernetes cluster usually provides, but in most cases, that doesn’t matter.
MOUNTING LOCAL FILES INTO THE MINIKUBE VM AND THEN INTO YOUR CONTAINERS
When you’re developing with Minikube and you’d like to try out every change to your
app in your Kubernetes cluster, you can mount your local filesystem into the Minikube
VM using the minikube mount command and then mount it into your containers
through a hostPath volume. You’ll find additional instructions on how to do that
in the Minikube documentation at https:/
/github.com/kubernetes/minikube/tree/
master/docs.
USING THE DOCKER DAEMON INSIDE THE MINIKUBE VM TO BUILD YOUR IMAGES
If you’re developing your app with Minikube and planning to build the container
image after every change, you can use the Docker daemon inside the Minikube VM to
do the building, instead of having to build the image through your local Docker dae-
mon, push it to a registry, and then have it pulled by the daemon in the VM. To use
 
</data>
  <data key="d5">503
Best practices for development and testing
CONNECTING TO THE API SERVER
Similarly, if your app requires access to the Kubernetes API server when running
inside a Kubernetes cluster, it can easily talk to the API server from outside the cluster
during development. If it uses the ServiceAccount’s token to authenticate itself, you
can always copy the ServiceAccount’s Secret’s files to your local machine with kubectl
cp. The API server doesn’t care if the client accessing it is inside or outside the cluster. 
 If the app uses an ambassador container like the one described in chapter 8, you
don’t even need those Secret files. Run kubectl proxy on your local machine, run
your app locally, and it should be ready to talk to your local kubectl proxy (as long as
it and the ambassador container bind the proxy to the same port).
 In this case, you’ll need to make sure the user account your local kubectl is using
has the same privileges as the ServiceAccount the app will run under.
RUNNING INSIDE A CONTAINER EVEN DURING DEVELOPMENT
When during development you absolutely have to run the app in a container for what-
ever reason, there is a way of avoiding having to build the container image every time.
Instead of baking the binaries into the image, you can always mount your local filesys-
tem into the container through Docker volumes, for example. This way, after you
build a new version of the app’s binaries, all you need to do is restart the container (or
not even that, if hot-redeploy is supported). No need to rebuild the image.
17.5.2 Using Minikube in development
As you can see, nothing forces you to run your app inside Kubernetes during develop-
ment. But you may do that anyway to see how the app behaves in a true Kubernetes
environment.
 You may have used Minikube to run examples in this book. Although a Minikube
cluster runs only a single worker node, it’s nevertheless a valuable method of trying
out your app in Kubernetes (and, of course, developing all the resource manifests that
make up your complete application). Minikube doesn’t offer everything that a proper
multi-node Kubernetes cluster usually provides, but in most cases, that doesn’t matter.
MOUNTING LOCAL FILES INTO THE MINIKUBE VM AND THEN INTO YOUR CONTAINERS
When you’re developing with Minikube and you’d like to try out every change to your
app in your Kubernetes cluster, you can mount your local filesystem into the Minikube
VM using the minikube mount command and then mount it into your containers
through a hostPath volume. You’ll find additional instructions on how to do that
in the Minikube documentation at https:/
/github.com/kubernetes/minikube/tree/
master/docs.
USING THE DOCKER DAEMON INSIDE THE MINIKUBE VM TO BUILD YOUR IMAGES
If you’re developing your app with Minikube and planning to build the container
image after every change, you can use the Docker daemon inside the Minikube VM to
do the building, instead of having to build the image through your local Docker dae-
mon, push it to a registry, and then have it pulled by the daemon in the VM. To use
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="536">
  <data key="d0">Page_536</data>
  <data key="d5">Page_536</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_447">
  <data key="d0">504
CHAPTER 17
Best practices for developing apps
Minikube’s Docker daemon, all you need to do is point your DOCKER_HOST environ-
ment variable to it. Luckily, this is much easier than it sounds. All you need to do is
run the following command on your local machine:
$ eval $(minikube docker-env)
This will set all the required environment variables for you. You then build your
images the same way as if the Docker daemon was running on your local machine.
After you build the image, you don’t need to push it anywhere, because it’s already
stored locally on the Minikube VM, which means new pods can use the image immedi-
ately. If your pods are already running, you either need to delete them or kill their
containers so they’re restarted.
BUILDING IMAGES LOCALLY AND COPYING THEM OVER TO THE MINIKUBE VM DIRECTLY
If you can’t use the daemon inside the VM to build the images, you still have a way to
avoid having to push the image to a registry and have the Kubelet running in the
Minikube VM pull it. If you build the image on your local machine, you can copy it
over to the Minikube VM with the following command:
$ docker save &lt;image&gt; | (eval $(minikube docker-env) &amp;&amp; docker load)
As before, the image is immediately ready to be used in a pod. But make sure the
imagePullPolicy in your pod spec isn’t set to Always, because that would cause the
image to be pulled from the external registry again and you’d lose the changes you’ve
copied over.
COMBINING MINIKUBE WITH A PROPER KUBERNETES CLUSTER
You have virtually no limit when developing apps with Minikube. You can even com-
bine a Minikube cluster with a proper Kubernetes cluster. I sometimes run my devel-
opment workloads in my local Minikube cluster and have them talk to my other
workloads that are deployed in a remote multi-node Kubernetes cluster thousands of
miles away. 
 Once I’m finished with development, I can move my local workloads to the remote
cluster with no modifications and with absolutely no problems thanks to how Kuber-
netes abstracts away the underlying infrastructure from the app.
17.5.3 Versioning and auto-deploying resource manifests
Because Kubernetes uses a declarative model, you never have to figure out the current
state of your deployed resources and issue imperative commands to bring that state to
what you desire. All you need to do is tell Kubernetes your desired state and it will take
all the necessary actions to reconcile the cluster state with the desired state.
 You can store your collection of resource manifests in a Version Control System,
enabling you to perform code reviews, keep an audit trail, and roll back changes
whenever necessary. After each commit, you can run the kubectl apply command to
have your changes reflected in your deployed resources. 
 
</data>
  <data key="d5">504
CHAPTER 17
Best practices for developing apps
Minikube’s Docker daemon, all you need to do is point your DOCKER_HOST environ-
ment variable to it. Luckily, this is much easier than it sounds. All you need to do is
run the following command on your local machine:
$ eval $(minikube docker-env)
This will set all the required environment variables for you. You then build your
images the same way as if the Docker daemon was running on your local machine.
After you build the image, you don’t need to push it anywhere, because it’s already
stored locally on the Minikube VM, which means new pods can use the image immedi-
ately. If your pods are already running, you either need to delete them or kill their
containers so they’re restarted.
BUILDING IMAGES LOCALLY AND COPYING THEM OVER TO THE MINIKUBE VM DIRECTLY
If you can’t use the daemon inside the VM to build the images, you still have a way to
avoid having to push the image to a registry and have the Kubelet running in the
Minikube VM pull it. If you build the image on your local machine, you can copy it
over to the Minikube VM with the following command:
$ docker save &lt;image&gt; | (eval $(minikube docker-env) &amp;&amp; docker load)
As before, the image is immediately ready to be used in a pod. But make sure the
imagePullPolicy in your pod spec isn’t set to Always, because that would cause the
image to be pulled from the external registry again and you’d lose the changes you’ve
copied over.
COMBINING MINIKUBE WITH A PROPER KUBERNETES CLUSTER
You have virtually no limit when developing apps with Minikube. You can even com-
bine a Minikube cluster with a proper Kubernetes cluster. I sometimes run my devel-
opment workloads in my local Minikube cluster and have them talk to my other
workloads that are deployed in a remote multi-node Kubernetes cluster thousands of
miles away. 
 Once I’m finished with development, I can move my local workloads to the remote
cluster with no modifications and with absolutely no problems thanks to how Kuber-
netes abstracts away the underlying infrastructure from the app.
17.5.3 Versioning and auto-deploying resource manifests
Because Kubernetes uses a declarative model, you never have to figure out the current
state of your deployed resources and issue imperative commands to bring that state to
what you desire. All you need to do is tell Kubernetes your desired state and it will take
all the necessary actions to reconcile the cluster state with the desired state.
 You can store your collection of resource manifests in a Version Control System,
enabling you to perform code reviews, keep an audit trail, and roll back changes
whenever necessary. After each commit, you can run the kubectl apply command to
have your changes reflected in your deployed resources. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="537">
  <data key="d0">Page_537</data>
  <data key="d5">Page_537</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_448">
  <data key="d0">505
Best practices for development and testing
 If you run an agent that periodically (or when it detects a new commit) checks out
your manifests from the Version Control System (VCS), and then runs the apply com-
mand, you can manage your running apps simply by committing changes to the VCS
without having to manually talk to the Kubernetes API server. Luckily, the people at
Box (which coincidently was used to host this book’s manuscript and other materials)
developed and released a tool called kube-applier, which does exactly what I described.
You’ll find the tool’s source code at https:/
/github.com/box/kube-applier.
 You can use multiple branches to deploy the manifests to a development, QA, stag-
ing, and production cluster (or in different namespaces in the same cluster).
17.5.4 Introducing Ksonnet as an alternative to writing YAML/JSON 
manifests
We’ve seen a number of YAML manifests throughout the book. I don’t see writing
YAML as too big of a problem, especially once you learn how to use kubectl explain
to see the available options, but some people do. 
 Just as I was finalizing the manuscript for this book, a new tool called Ksonnet was
announced. It’s a library built on top of Jsonnet, which is a data templating language
for building JSON data structures. Instead of writing the complete JSON by hand, it
lets you define parameterized JSON fragments, give them a name, and then build a
full JSON manifest by referencing those fragments by name, instead of repeating the
same JSON code in multiple locations—much like you use functions or methods in a
programming language. 
 Ksonnet defines the fragments you’d find in Kubernetes resource manifests, allow-
ing you to quickly build a complete Kubernetes resource JSON manifest with much
less code. The following listing shows an example.
local k = import "../ksonnet-lib/ksonnet.beta.1/k.libsonnet";
local container = k.core.v1.container;
local deployment = k.apps.v1beta1.deployment;
local kubiaContainer =                              
  container.default("kubia", "luksa/kubia:v1") +    
  container.helpers.namedPort("http", 8080);        
deployment.default("kubia", kubiaContainer) +    
deployment.mixin.spec.replicas(3)                
The kubia.ksonnet file shown in the listing is converted to a full JSON Deployment
manifest when you run the following command:
$ jsonnet kubia.ksonnet
Listing 17.10
The kubia Deployment written with Ksonnet: kubia.ksonnet
This defines a container called kubia, 
which uses the luksa/kubia:v1 image 
and includes a port called http.
This will be expanded into a full 
Deployment resource. The kubiaContainer 
defined here will be included in the 
Deployment’s pod template.
 
</data>
  <data key="d5">505
Best practices for development and testing
 If you run an agent that periodically (or when it detects a new commit) checks out
your manifests from the Version Control System (VCS), and then runs the apply com-
mand, you can manage your running apps simply by committing changes to the VCS
without having to manually talk to the Kubernetes API server. Luckily, the people at
Box (which coincidently was used to host this book’s manuscript and other materials)
developed and released a tool called kube-applier, which does exactly what I described.
You’ll find the tool’s source code at https:/
/github.com/box/kube-applier.
 You can use multiple branches to deploy the manifests to a development, QA, stag-
ing, and production cluster (or in different namespaces in the same cluster).
17.5.4 Introducing Ksonnet as an alternative to writing YAML/JSON 
manifests
We’ve seen a number of YAML manifests throughout the book. I don’t see writing
YAML as too big of a problem, especially once you learn how to use kubectl explain
to see the available options, but some people do. 
 Just as I was finalizing the manuscript for this book, a new tool called Ksonnet was
announced. It’s a library built on top of Jsonnet, which is a data templating language
for building JSON data structures. Instead of writing the complete JSON by hand, it
lets you define parameterized JSON fragments, give them a name, and then build a
full JSON manifest by referencing those fragments by name, instead of repeating the
same JSON code in multiple locations—much like you use functions or methods in a
programming language. 
 Ksonnet defines the fragments you’d find in Kubernetes resource manifests, allow-
ing you to quickly build a complete Kubernetes resource JSON manifest with much
less code. The following listing shows an example.
local k = import "../ksonnet-lib/ksonnet.beta.1/k.libsonnet";
local container = k.core.v1.container;
local deployment = k.apps.v1beta1.deployment;
local kubiaContainer =                              
  container.default("kubia", "luksa/kubia:v1") +    
  container.helpers.namedPort("http", 8080);        
deployment.default("kubia", kubiaContainer) +    
deployment.mixin.spec.replicas(3)                
The kubia.ksonnet file shown in the listing is converted to a full JSON Deployment
manifest when you run the following command:
$ jsonnet kubia.ksonnet
Listing 17.10
The kubia Deployment written with Ksonnet: kubia.ksonnet
This defines a container called kubia, 
which uses the luksa/kubia:v1 image 
and includes a port called http.
This will be expanded into a full 
Deployment resource. The kubiaContainer 
defined here will be included in the 
Deployment’s pod template.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="538">
  <data key="d0">Page_538</data>
  <data key="d5">Page_538</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_449">
  <data key="d0">506
CHAPTER 17
Best practices for developing apps
The power of Ksonnet and Jsonnet becomes apparent when you realize you can define
your own higher-level fragments and make all your manifests consistent and duplica-
tion-free. You’ll find more information on using and installing Ksonnet and Jsonnet at
https:/
/github.com/ksonnet/ksonnet-lib.
17.5.5 Employing Continuous Integration and Continuous Delivery 
(CI/CD)
We’ve touched on automating the deployment of Kubernetes resources two sections
back, but you may want to set up a complete CI/CD pipeline for building your appli-
cation binaries, container images, and resource manifests and then deploying them in
one or more Kubernetes clusters.
 You’ll find many online resources talking about this subject. Here, I’d like to point
you specifically to the Fabric8 project (http:/
/fabric8.io), which is an integrated
development platform for Kubernetes. It includes Jenkins, the well-known, open-
source automation system, and various other tools to deliver a full CI/CD pipeline
for DevOps-style development, deployment, and management of microservices on
Kubernetes.
 If you’d like to build your own solution, I also suggest looking at one of the Google
Cloud Platform’s online labs that talks about this subject. It’s available at https:/
/
github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.
17.6
Summary
Hopefully, the information in this chapter has given you an even deeper insight into
how Kubernetes works and will help you build apps that feel right at home when
deployed to a Kubernetes cluster. The aim of this chapter was to
Show you how all the resources covered in this book come together to repre-
sent a typical application running in Kubernetes.
Make you think about the difference between apps that are rarely moved
between machines and apps running as pods, which are relocated much more
frequently.
Help you understand that your multi-component apps (or microservices, if you
will) shouldn’t rely on a specific start-up order.
Introduce init containers, which can be used to initialize a pod or delay the start
of the pod’s main containers until a precondition is met.
Teach you about container lifecycle hooks and when to use them.
Gain a deeper insight into the consequences of the distributed nature of
Kubernetes components and its eventual consistency model.
Learn how to make your apps shut down properly without breaking client
connections.
 
</data>
  <data key="d5">506
CHAPTER 17
Best practices for developing apps
The power of Ksonnet and Jsonnet becomes apparent when you realize you can define
your own higher-level fragments and make all your manifests consistent and duplica-
tion-free. You’ll find more information on using and installing Ksonnet and Jsonnet at
https:/
/github.com/ksonnet/ksonnet-lib.
17.5.5 Employing Continuous Integration and Continuous Delivery 
(CI/CD)
We’ve touched on automating the deployment of Kubernetes resources two sections
back, but you may want to set up a complete CI/CD pipeline for building your appli-
cation binaries, container images, and resource manifests and then deploying them in
one or more Kubernetes clusters.
 You’ll find many online resources talking about this subject. Here, I’d like to point
you specifically to the Fabric8 project (http:/
/fabric8.io), which is an integrated
development platform for Kubernetes. It includes Jenkins, the well-known, open-
source automation system, and various other tools to deliver a full CI/CD pipeline
for DevOps-style development, deployment, and management of microservices on
Kubernetes.
 If you’d like to build your own solution, I also suggest looking at one of the Google
Cloud Platform’s online labs that talks about this subject. It’s available at https:/
/
github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.
17.6
Summary
Hopefully, the information in this chapter has given you an even deeper insight into
how Kubernetes works and will help you build apps that feel right at home when
deployed to a Kubernetes cluster. The aim of this chapter was to
Show you how all the resources covered in this book come together to repre-
sent a typical application running in Kubernetes.
Make you think about the difference between apps that are rarely moved
between machines and apps running as pods, which are relocated much more
frequently.
Help you understand that your multi-component apps (or microservices, if you
will) shouldn’t rely on a specific start-up order.
Introduce init containers, which can be used to initialize a pod or delay the start
of the pod’s main containers until a precondition is met.
Teach you about container lifecycle hooks and when to use them.
Gain a deeper insight into the consequences of the distributed nature of
Kubernetes components and its eventual consistency model.
Learn how to make your apps shut down properly without breaking client
connections.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="539">
  <data key="d0">Page_539</data>
  <data key="d5">Page_539</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_450">
  <data key="d0">507
Summary
Give you a few small tips on how to make your apps easier to manage by keep-
ing image sizes small, adding annotations and multi-dimensional labels to all
your resources, and making it easier to see why an application terminated.
Teach you how to develop Kubernetes apps and run them locally or in Mini-
kube before deploying them on a proper multi-node cluster.
In the next and final chapter, we’ll learn how you can extend Kubernetes with your
own custom API objects and controllers and how others have done it to create com-
plete Platform-as-a-Service solutions on top of Kubernetes.
 
</data>
  <data key="d5">507
Summary
Give you a few small tips on how to make your apps easier to manage by keep-
ing image sizes small, adding annotations and multi-dimensional labels to all
your resources, and making it easier to see why an application terminated.
Teach you how to develop Kubernetes apps and run them locally or in Mini-
kube before deploying them on a proper multi-node cluster.
In the next and final chapter, we’ll learn how you can extend Kubernetes with your
own custom API objects and controllers and how others have done it to create com-
plete Platform-as-a-Service solutions on top of Kubernetes.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="540">
  <data key="d0">Page_540</data>
  <data key="d5">Page_540</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_451">
  <data key="d0">508
Extending Kubernetes
You’re almost done. To wrap up, we’ll look at how you can define your own API
objects and create controllers for those objects. We’ll also look at how others have
extended Kubernetes and built Platform-as-a-Service solutions on top of it.
18.1
Defining custom API objects
Throughout the book, you’ve learned about the API objects that Kubernetes pro-
vides and how they’re used to build application systems. Currently, Kubernetes
users mostly use only these objects even though they represent relatively low-level,
generic concepts. 
This chapter covers
Adding custom objects to Kubernetes
Creating a controller for the custom object
Adding custom API servers
Self-provisioning of services with the Kubernetes 
Service Catalog
Red Hat’s OpenShift Container Platform
Deis Workflow and Helm
 
</data>
  <data key="d5">508
Extending Kubernetes
You’re almost done. To wrap up, we’ll look at how you can define your own API
objects and create controllers for those objects. We’ll also look at how others have
extended Kubernetes and built Platform-as-a-Service solutions on top of it.
18.1
Defining custom API objects
Throughout the book, you’ve learned about the API objects that Kubernetes pro-
vides and how they’re used to build application systems. Currently, Kubernetes
users mostly use only these objects even though they represent relatively low-level,
generic concepts. 
This chapter covers
Adding custom objects to Kubernetes
Creating a controller for the custom object
Adding custom API servers
Self-provisioning of services with the Kubernetes 
Service Catalog
Red Hat’s OpenShift Container Platform
Deis Workflow and Helm
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="541">
  <data key="d0">Page_541</data>
  <data key="d5">Page_541</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_452">
  <data key="d0">509
Defining custom API objects
 As the Kubernetes ecosystem evolves, you’ll see more and more high-level objects,
which will be much more specialized than the resources Kubernetes supports today.
Instead of dealing with Deployments, Services, ConfigMaps, and the like, you’ll create
and manage objects that represent whole applications or software services. A custom
controller will observe those high-level objects and create low-level objects based on
them. For example, to run a messaging broker inside a Kubernetes cluster, all you’ll
need to do is create an instance of a Queue resource and all the necessary Secrets,
Deployments, and Services will be created by a custom Queue controller. Kubernetes
already provides ways of adding custom resources like this. 
18.1.1 Introducing CustomResourceDefinitions
To define a new resource type, all you need to do is post a CustomResourceDefinition
object (CRD) to the Kubernetes API server. The CustomResourceDefinition object is
the description of the custom resource type. Once the CRD is posted, users can then
create instances of the custom resource by posting JSON or YAML manifests to the
API server, the same as with any other Kubernetes resource.
NOTE
Prior to Kubernetes 1.7, custom resources were defined through Third-
PartyResource objects, which were similar to CustomResourceDefinitions, but
were removed in version 1.8.
Creating a CRD so that users can create objects of the new type isn’t a useful feature if
those objects don’t make something tangible happen in the cluster. Each CRD will
usually also have an associated controller (an active component doing something
based on the custom objects), the same way that all the core Kubernetes resources
have an associated controller, as was explained in chapter 11. For this reason, to prop-
erly show what CustomResourceDefinitions allow you to do other than adding
instances of a custom object, a controller must be deployed as well. You’ll do that in
the next example.
INTRODUCING THE EXAMPLE CUSTOMRESOURCEDEFINITION
Let’s imagine you want to allow users of your Kubernetes cluster to run static websites
as easily as possible, without having to deal with Pods, Services, and other Kubernetes
resources. What you want to achieve is for users to create objects of type Website that
contain nothing more than the website’s name and the source from which the web-
site’s files (HTML, CSS, PNG, and others) should be obtained. You’ll use a Git reposi-
tory as the source of those files. When a user creates an instance of the Website
resource, you want Kubernetes to spin up a new web server pod and expose it through
a Service, as shown in figure 18.1.
 To create the Website resource, you want users to post manifests along the lines of
the one shown in the following listing.
 
 
 
 
</data>
  <data key="d5">509
Defining custom API objects
 As the Kubernetes ecosystem evolves, you’ll see more and more high-level objects,
which will be much more specialized than the resources Kubernetes supports today.
Instead of dealing with Deployments, Services, ConfigMaps, and the like, you’ll create
and manage objects that represent whole applications or software services. A custom
controller will observe those high-level objects and create low-level objects based on
them. For example, to run a messaging broker inside a Kubernetes cluster, all you’ll
need to do is create an instance of a Queue resource and all the necessary Secrets,
Deployments, and Services will be created by a custom Queue controller. Kubernetes
already provides ways of adding custom resources like this. 
18.1.1 Introducing CustomResourceDefinitions
To define a new resource type, all you need to do is post a CustomResourceDefinition
object (CRD) to the Kubernetes API server. The CustomResourceDefinition object is
the description of the custom resource type. Once the CRD is posted, users can then
create instances of the custom resource by posting JSON or YAML manifests to the
API server, the same as with any other Kubernetes resource.
NOTE
Prior to Kubernetes 1.7, custom resources were defined through Third-
PartyResource objects, which were similar to CustomResourceDefinitions, but
were removed in version 1.8.
Creating a CRD so that users can create objects of the new type isn’t a useful feature if
those objects don’t make something tangible happen in the cluster. Each CRD will
usually also have an associated controller (an active component doing something
based on the custom objects), the same way that all the core Kubernetes resources
have an associated controller, as was explained in chapter 11. For this reason, to prop-
erly show what CustomResourceDefinitions allow you to do other than adding
instances of a custom object, a controller must be deployed as well. You’ll do that in
the next example.
INTRODUCING THE EXAMPLE CUSTOMRESOURCEDEFINITION
Let’s imagine you want to allow users of your Kubernetes cluster to run static websites
as easily as possible, without having to deal with Pods, Services, and other Kubernetes
resources. What you want to achieve is for users to create objects of type Website that
contain nothing more than the website’s name and the source from which the web-
site’s files (HTML, CSS, PNG, and others) should be obtained. You’ll use a Git reposi-
tory as the source of those files. When a user creates an instance of the Website
resource, you want Kubernetes to spin up a new web server pod and expose it through
a Service, as shown in figure 18.1.
 To create the Website resource, you want users to post manifests along the lines of
the one shown in the following listing.
 
 
 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="542">
  <data key="d0">Page_542</data>
  <data key="d5">Page_542</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_453">
  <data key="d0">510
CHAPTER 18
Extending Kubernetes
kind: Website        
metadata:
  name: kubia             
spec:
  gitRepo: https://github.com/luksa/kubia-website-example.git   
Like all other resources, your resource contains a kind and a metadata.name field,
and like most resources, it also contains a spec section. It contains a single field called
gitRepo (you can choose the name)—it specifies the Git repository containing the
website’s files. You’ll also need to include an apiVersion field, but you don’t know yet
what its value must be for custom resources.
 If you try posting this resource to Kubernetes, you’ll receive an error because
Kubernetes doesn’t know what a Website object is yet:
$ kubectl create -f imaginary-kubia-website.yaml
error: unable to recognize "imaginary-kubia-website.yaml": no matches for 
➥ /, Kind=Website
Before you can create instances of your custom object, you need to make Kubernetes
recognize them.
CREATING A CUSTOMRESOURCEDEFINITION OBJECT
To make Kubernetes accept your custom Website resource instances, you need to post
the CustomResourceDefinition shown in the following listing to the API server.
apiVersion: apiextensions.k8s.io/v1beta1       
kind: CustomResourceDefinition                 
metadata:
  name: websites.extensions.example.com      
spec:
  scope: Namespaced                          
Listing 18.1
An imaginary custom resource: imaginary-kubia-website.yaml
Listing 18.2
A CustomResourceDefinition manifest: website-crd.yaml
Website
kind: Website
metadata:
name: kubia
spec:
gitRepo:
github.com/.../kubia.git
Pod:
kubia-website
Service:
kubia-website
Figure 18.1
Each Website object should result in the creation of a Service and an HTTP 
server Pod.
A custom 
object kind
The name of the website 
(used for naming the 
resulting Service and Pod)
The Git 
repository 
holding the 
website’s files
CustomResourceDefinitions belong 
to this API group and version.
The full
name of
your
custom
object
You want Website resources 
to be namespaced.
 
</data>
  <data key="d5">510
CHAPTER 18
Extending Kubernetes
kind: Website        
metadata:
  name: kubia             
spec:
  gitRepo: https://github.com/luksa/kubia-website-example.git   
Like all other resources, your resource contains a kind and a metadata.name field,
and like most resources, it also contains a spec section. It contains a single field called
gitRepo (you can choose the name)—it specifies the Git repository containing the
website’s files. You’ll also need to include an apiVersion field, but you don’t know yet
what its value must be for custom resources.
 If you try posting this resource to Kubernetes, you’ll receive an error because
Kubernetes doesn’t know what a Website object is yet:
$ kubectl create -f imaginary-kubia-website.yaml
error: unable to recognize "imaginary-kubia-website.yaml": no matches for 
➥ /, Kind=Website
Before you can create instances of your custom object, you need to make Kubernetes
recognize them.
CREATING A CUSTOMRESOURCEDEFINITION OBJECT
To make Kubernetes accept your custom Website resource instances, you need to post
the CustomResourceDefinition shown in the following listing to the API server.
apiVersion: apiextensions.k8s.io/v1beta1       
kind: CustomResourceDefinition                 
metadata:
  name: websites.extensions.example.com      
spec:
  scope: Namespaced                          
Listing 18.1
An imaginary custom resource: imaginary-kubia-website.yaml
Listing 18.2
A CustomResourceDefinition manifest: website-crd.yaml
Website
kind: Website
metadata:
name: kubia
spec:
gitRepo:
github.com/.../kubia.git
Pod:
kubia-website
Service:
kubia-website
Figure 18.1
Each Website object should result in the creation of a Service and an HTTP 
server Pod.
A custom 
object kind
The name of the website 
(used for naming the 
resulting Service and Pod)
The Git 
repository 
holding the 
website’s files
CustomResourceDefinitions belong 
to this API group and version.
The full
name of
your
custom
object
You want Website resources 
to be namespaced.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="543">
  <data key="d0">Page_543</data>
  <data key="d5">Page_543</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_454">
  <data key="d0">511
Defining custom API objects
  group: extensions.example.com                
  version: v1                                  
  names:                                    
    kind: Website                           
    singular: website                       
    plural: websites                        
After you post the descriptor to Kubernetes, it will allow you to create any number of
instances of the custom Website resource. 
 You can create the CRD from the website-crd.yaml file available in the code archive:
$ kubectl create -f website-crd-definition.yaml
customresourcedefinition "websites.extensions.example.com" created
I’m sure you’re wondering about the long name of the CRD. Why not call it Website?
The reason is to prevent name clashes. By adding a suffix to the name of the CRD
(which will usually include the name of the organization that created the CRD), you
keep CRD names unique. Luckily, the long name doesn’t mean you’ll need to create
your Website resources with kind: websites.extensions.example.com, but as kind:
Website, as specified in the names.kind property of the CRD. The extensions.exam-
ple.com part is the API group of your resource. 
 You’ve seen how creating Deployment objects requires you to set apiVersion to
apps/v1beta1 instead of v1. The part before the slash is the API group (Deployments
belong to the apps API group), and the part after it is the version name (v1beta1 in
the case of Deployments). When creating instances of the custom Website resource,
the apiVersion property will need to be set to extensions.example.com/v1.
CREATING AN INSTANCE OF A CUSTOM RESOURCE
Considering what you learned, you’ll now create a proper YAML for your Website
resource instance. The YAML manifest is shown in the following listing.
apiVersion: extensions.example.com/v1       
kind: Website                               
metadata:
  name: kubia                                
spec:
  gitRepo: https://github.com/luksa/kubia-website-example.git
The kind of your resource is Website, and the apiVersion is composed of the API
group and the version number you defined in the CustomResourceDefinition.
 Create your Website object now:
$ kubectl create -f kubia-website.yaml
website "kubia" created
Listing 18.3
A custom Website resource: kubia-website.yaml
Define an API group and version 
of the Website resource.
You need to specify the various 
forms of the custom object’s name.
Your custom API
group and version
This manifest 
describes a Website 
resource instance.
The name of the 
Website instance
 
</data>
  <data key="d5">511
Defining custom API objects
  group: extensions.example.com                
  version: v1                                  
  names:                                    
    kind: Website                           
    singular: website                       
    plural: websites                        
After you post the descriptor to Kubernetes, it will allow you to create any number of
instances of the custom Website resource. 
 You can create the CRD from the website-crd.yaml file available in the code archive:
$ kubectl create -f website-crd-definition.yaml
customresourcedefinition "websites.extensions.example.com" created
I’m sure you’re wondering about the long name of the CRD. Why not call it Website?
The reason is to prevent name clashes. By adding a suffix to the name of the CRD
(which will usually include the name of the organization that created the CRD), you
keep CRD names unique. Luckily, the long name doesn’t mean you’ll need to create
your Website resources with kind: websites.extensions.example.com, but as kind:
Website, as specified in the names.kind property of the CRD. The extensions.exam-
ple.com part is the API group of your resource. 
 You’ve seen how creating Deployment objects requires you to set apiVersion to
apps/v1beta1 instead of v1. The part before the slash is the API group (Deployments
belong to the apps API group), and the part after it is the version name (v1beta1 in
the case of Deployments). When creating instances of the custom Website resource,
the apiVersion property will need to be set to extensions.example.com/v1.
CREATING AN INSTANCE OF A CUSTOM RESOURCE
Considering what you learned, you’ll now create a proper YAML for your Website
resource instance. The YAML manifest is shown in the following listing.
apiVersion: extensions.example.com/v1       
kind: Website                               
metadata:
  name: kubia                                
spec:
  gitRepo: https://github.com/luksa/kubia-website-example.git
The kind of your resource is Website, and the apiVersion is composed of the API
group and the version number you defined in the CustomResourceDefinition.
 Create your Website object now:
$ kubectl create -f kubia-website.yaml
website "kubia" created
Listing 18.3
A custom Website resource: kubia-website.yaml
Define an API group and version 
of the Website resource.
You need to specify the various 
forms of the custom object’s name.
Your custom API
group and version
This manifest 
describes a Website 
resource instance.
The name of the 
Website instance
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="544">
  <data key="d0">Page_544</data>
  <data key="d5">Page_544</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_455">
  <data key="d0">512
CHAPTER 18
Extending Kubernetes
The response tells you that the API server has accepted and stored your custom
Website object. Let’s see if you can now retrieve it. 
RETRIEVING INSTANCES OF A CUSTOM RESOURCE
List all the websites in your cluster:
$ kubectl get websites
NAME      KIND
kubia     Website.v1.extensions.example.com
As with existing Kubernetes resources, you can create and then list instances of cus-
tom resources. You can also use kubectl describe to see the details of your custom
object, or retrieve the whole YAML with kubectl get, as in the following listing.
$ kubectl get website kubia -o yaml
apiVersion: extensions.example.com/v1
kind: Website
metadata:
  creationTimestamp: 2017-02-26T15:53:21Z
  name: kubia
  namespace: default
  resourceVersion: "57047"
  selfLink: /apis/extensions.example.com/v1/.../default/websites/kubia
  uid: b2eb6d99-fc3b-11e6-bd71-0800270a1c50
spec:
  gitRepo: https://github.com/luksa/kubia-website-example.git
Note that the resource includes everything that was in the original YAML definition,
and that Kubernetes has initialized additional metadata fields the way it does with all
other resources. 
DELETING AN INSTANCE OF A CUSTOM OBJECT
Obviously, in addition to creating and retrieving custom object instances, you can also
delete them:
$ kubectl delete website kubia
website "kubia" deleted
NOTE
You’re deleting an instance of a Website, not the Website CRD
resource. You could also delete the CRD object itself, but let’s hold off on that
for a while, because you’ll be creating additional Website instances in the
next section. 
Let’s go over everything you’ve done. By creating a CustomResourceDefinition object,
you can now store, retrieve, and delete custom objects through the Kubernetes API
server. These objects don’t do anything yet. You’ll need to create a controller to make
them do something. 
Listing 18.4
Full Website resource definition retrieved from the API server
 
</data>
  <data key="d5">512
CHAPTER 18
Extending Kubernetes
The response tells you that the API server has accepted and stored your custom
Website object. Let’s see if you can now retrieve it. 
RETRIEVING INSTANCES OF A CUSTOM RESOURCE
List all the websites in your cluster:
$ kubectl get websites
NAME      KIND
kubia     Website.v1.extensions.example.com
As with existing Kubernetes resources, you can create and then list instances of cus-
tom resources. You can also use kubectl describe to see the details of your custom
object, or retrieve the whole YAML with kubectl get, as in the following listing.
$ kubectl get website kubia -o yaml
apiVersion: extensions.example.com/v1
kind: Website
metadata:
  creationTimestamp: 2017-02-26T15:53:21Z
  name: kubia
  namespace: default
  resourceVersion: "57047"
  selfLink: /apis/extensions.example.com/v1/.../default/websites/kubia
  uid: b2eb6d99-fc3b-11e6-bd71-0800270a1c50
spec:
  gitRepo: https://github.com/luksa/kubia-website-example.git
Note that the resource includes everything that was in the original YAML definition,
and that Kubernetes has initialized additional metadata fields the way it does with all
other resources. 
DELETING AN INSTANCE OF A CUSTOM OBJECT
Obviously, in addition to creating and retrieving custom object instances, you can also
delete them:
$ kubectl delete website kubia
website "kubia" deleted
NOTE
You’re deleting an instance of a Website, not the Website CRD
resource. You could also delete the CRD object itself, but let’s hold off on that
for a while, because you’ll be creating additional Website instances in the
next section. 
Let’s go over everything you’ve done. By creating a CustomResourceDefinition object,
you can now store, retrieve, and delete custom objects through the Kubernetes API
server. These objects don’t do anything yet. You’ll need to create a controller to make
them do something. 
Listing 18.4
Full Website resource definition retrieved from the API server
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="545">
  <data key="d0">Page_545</data>
  <data key="d5">Page_545</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_456">
  <data key="d0">513
Defining custom API objects
 In general, the point of creating custom objects like this isn’t always to make some-
thing happen when the object is created. Certain custom objects are used to store data
instead of using a more generic mechanism such as a ConfigMap. Applications run-
ning inside pods can query the API server for those objects and read whatever is
stored in them. 
 But in this case, we said you wanted the existence of a Website object to result in
the spinning up of a web server serving the contents of the Git repository referenced
in the object. We’ll look at how to do that next.
18.1.2 Automating custom resources with custom controllers
To make your Website objects run a web server pod exposed through a Service, you’ll
need to build and deploy a Website controller, which will watch the API server for the
creation of Website objects and then create the Service and the web server Pod for
each of them. 
 To make sure the Pod is managed and survives node failures, the controller will
create a Deployment resource instead of an unmanaged Pod directly. The controller’s
operation is summarized in figure 18.2.
I’ve written a simple initial version of the controller, which works well enough to
show CRDs and the controller in action, but it’s far from being production-ready,
because it’s overly simplified. The container image is available at docker.io/luksa/
website-controller:latest, and the source code is at https:/
/github.com/luksa/k8s-
website-controller. Instead of going through its source code, I’ll explain what the con-
troller does.
API server
Websites
Website:
kubia
Deployments
Deployment:
kubia-website
Services
Service:
kubia-website
Website
controller
Watches
Creates
Figure 18.2
The Website controller 
watches for Website objects and 
creates a Deployment and a Service.
 
</data>
  <data key="d5">513
Defining custom API objects
 In general, the point of creating custom objects like this isn’t always to make some-
thing happen when the object is created. Certain custom objects are used to store data
instead of using a more generic mechanism such as a ConfigMap. Applications run-
ning inside pods can query the API server for those objects and read whatever is
stored in them. 
 But in this case, we said you wanted the existence of a Website object to result in
the spinning up of a web server serving the contents of the Git repository referenced
in the object. We’ll look at how to do that next.
18.1.2 Automating custom resources with custom controllers
To make your Website objects run a web server pod exposed through a Service, you’ll
need to build and deploy a Website controller, which will watch the API server for the
creation of Website objects and then create the Service and the web server Pod for
each of them. 
 To make sure the Pod is managed and survives node failures, the controller will
create a Deployment resource instead of an unmanaged Pod directly. The controller’s
operation is summarized in figure 18.2.
I’ve written a simple initial version of the controller, which works well enough to
show CRDs and the controller in action, but it’s far from being production-ready,
because it’s overly simplified. The container image is available at docker.io/luksa/
website-controller:latest, and the source code is at https:/
/github.com/luksa/k8s-
website-controller. Instead of going through its source code, I’ll explain what the con-
troller does.
API server
Websites
Website:
kubia
Deployments
Deployment:
kubia-website
Services
Service:
kubia-website
Website
controller
Watches
Creates
Figure 18.2
The Website controller 
watches for Website objects and 
creates a Deployment and a Service.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="546">
  <data key="d0">Page_546</data>
  <data key="d5">Page_546</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_457">
  <data key="d0">514
CHAPTER 18
Extending Kubernetes
UNDERSTANDING WHAT THE WEBSITE CONTROLLER DOES
Immediately upon startup, the controller starts to watch Website objects by requesting
the following URL:
http://localhost:8001/apis/extensions.example.com/v1/websites?watch=true
You may recognize the hostname and port—the controller isn’t connecting to the
API server directly, but is instead connecting to the kubectl proxy process, which
runs in a sidecar container in the same pod and acts as the ambassador to the API
server (we examined the ambassador pattern in chapter 8). The proxy forwards the
request to the API server, taking care of both TLS encryption and authentication
(see figure 18.3).
Through the connection opened by this HTTP GET request, the API server will send
watch events for every change to any Website object.
 The API server sends the ADDED watch event every time a new Website object is cre-
ated. When the controller receives such an event, it extracts the Website’s name and
the URL of the Git repository from the Website object it received in the watch event
and creates a Deployment and a Service object by posting their JSON manifests to the
API server. 
 The Deployment resource contains a template for a pod with two containers
(shown in figure 18.4): one running an nginx server and another one running a git-
sync process, which keeps a local directory synced with the contents of a Git repo.
The local directory is shared with the nginx container through an emptyDir volume
(you did something similar to that in chapter 6, but instead of keeping the local
directory synced with a Git repo, you used a gitRepo volume to download the Git
repo’s contents at pod startup; the volume’s contents weren’t kept in sync with the
Git repo afterward). The Service is a NodePort Service, which exposes your web
server pod through a random port on each node (the same port is used on all
nodes). When a pod is created by the Deployment object, clients can access the web-
site through the node port.
Pod: website-controller
Container: main
Website controller
GET http://localhost:8001/apis/extensions.
example.com/v1/websites?watch=true
GET https://kubernetes:443/apis/extensions.
example.com/v1/websites?watch=true
Authorization: Bearer &lt;token&gt;
Container: proxy
kubectl proxy
API server
Figure 18.3
The Website controller talks to the API server through a proxy (in the ambassador container).
 
</data>
  <data key="d5">514
CHAPTER 18
Extending Kubernetes
UNDERSTANDING WHAT THE WEBSITE CONTROLLER DOES
Immediately upon startup, the controller starts to watch Website objects by requesting
the following URL:
http://localhost:8001/apis/extensions.example.com/v1/websites?watch=true
You may recognize the hostname and port—the controller isn’t connecting to the
API server directly, but is instead connecting to the kubectl proxy process, which
runs in a sidecar container in the same pod and acts as the ambassador to the API
server (we examined the ambassador pattern in chapter 8). The proxy forwards the
request to the API server, taking care of both TLS encryption and authentication
(see figure 18.3).
Through the connection opened by this HTTP GET request, the API server will send
watch events for every change to any Website object.
 The API server sends the ADDED watch event every time a new Website object is cre-
ated. When the controller receives such an event, it extracts the Website’s name and
the URL of the Git repository from the Website object it received in the watch event
and creates a Deployment and a Service object by posting their JSON manifests to the
API server. 
 The Deployment resource contains a template for a pod with two containers
(shown in figure 18.4): one running an nginx server and another one running a git-
sync process, which keeps a local directory synced with the contents of a Git repo.
The local directory is shared with the nginx container through an emptyDir volume
(you did something similar to that in chapter 6, but instead of keeping the local
directory synced with a Git repo, you used a gitRepo volume to download the Git
repo’s contents at pod startup; the volume’s contents weren’t kept in sync with the
Git repo afterward). The Service is a NodePort Service, which exposes your web
server pod through a random port on each node (the same port is used on all
nodes). When a pod is created by the Deployment object, clients can access the web-
site through the node port.
Pod: website-controller
Container: main
Website controller
GET http://localhost:8001/apis/extensions.
example.com/v1/websites?watch=true
GET https://kubernetes:443/apis/extensions.
example.com/v1/websites?watch=true
Authorization: Bearer &lt;token&gt;
Container: proxy
kubectl proxy
API server
Figure 18.3
The Website controller talks to the API server through a proxy (in the ambassador container).
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="547">
  <data key="d0">Page_547</data>
  <data key="d5">Page_547</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_458">
  <data key="d0">515
Defining custom API objects
The API server also sends a DELETED watch event when a Website resource instance is
deleted. Upon receiving the event, the controller deletes the Deployment and the Ser-
vice resources it created earlier. As soon as a user deletes the Website instance, the
controller will shut down and remove the web server serving that website.
NOTE
My oversimplified controller isn’t implemented properly. The way it
watches the API objects doesn’t guarantee it won’t miss individual watch
events. The proper way to watch objects through the API server is to not only
watch them, but also periodically re-list all objects in case any watch events
were missed. 
RUNNING THE CONTROLLER AS A POD
During development, I ran the controller on my local development laptop and used a
locally running kubectl proxy process (not running as a pod) as the ambassador to
the Kubernetes API server. This allowed me to develop quickly, because I didn’t need
to build a container image after every change to the source code and then run it
inside Kubernetes. 
 When I’m ready to deploy the controller into production, the best way is to run the
controller inside Kubernetes itself, the way you do with all the other core controllers.
To run the controller in Kubernetes, you can deploy it through a Deployment resource.
The following listing shows an example of such a Deployment.
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: website-controller
spec:
  replicas: 1                      
  template:
Listing 18.5
A Website controller Deployment: website-controller.yaml
Pod
Webserver
container
Web client
git-sync
container
Serves website to
web client through
a random port
Clones Git repo
into volume and
keeps it synced
emptyDir
volume
Figure 18.4
The pod serving 
the website specified in the 
Website object
You’ll run a single 
replica of the 
controller.
 
</data>
  <data key="d5">515
Defining custom API objects
The API server also sends a DELETED watch event when a Website resource instance is
deleted. Upon receiving the event, the controller deletes the Deployment and the Ser-
vice resources it created earlier. As soon as a user deletes the Website instance, the
controller will shut down and remove the web server serving that website.
NOTE
My oversimplified controller isn’t implemented properly. The way it
watches the API objects doesn’t guarantee it won’t miss individual watch
events. The proper way to watch objects through the API server is to not only
watch them, but also periodically re-list all objects in case any watch events
were missed. 
RUNNING THE CONTROLLER AS A POD
During development, I ran the controller on my local development laptop and used a
locally running kubectl proxy process (not running as a pod) as the ambassador to
the Kubernetes API server. This allowed me to develop quickly, because I didn’t need
to build a container image after every change to the source code and then run it
inside Kubernetes. 
 When I’m ready to deploy the controller into production, the best way is to run the
controller inside Kubernetes itself, the way you do with all the other core controllers.
To run the controller in Kubernetes, you can deploy it through a Deployment resource.
The following listing shows an example of such a Deployment.
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: website-controller
spec:
  replicas: 1                      
  template:
Listing 18.5
A Website controller Deployment: website-controller.yaml
Pod
Webserver
container
Web client
git-sync
container
Serves website to
web client through
a random port
Clones Git repo
into volume and
keeps it synced
emptyDir
volume
Figure 18.4
The pod serving 
the website specified in the 
Website object
You’ll run a single 
replica of the 
controller.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="548">
  <data key="d0">Page_548</data>
  <data key="d5">Page_548</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_459">
  <data key="d0">516
CHAPTER 18
Extending Kubernetes
    metadata:
      name: website-controller
      labels:
        app: website-controller
    spec:
      serviceAccountName: website-controller    
      containers:                                    
      - name: main                                   
        image: luksa/website-controller              
      - name: proxy                                  
        image: luksa/kubectl-proxy:1.6.2             
As you can see, the Deployment deploys a single replica of a two-container pod. One
container runs your controller, whereas the other one is the ambassador container
used for simpler communication with the API server. The pod runs under its own spe-
cial ServiceAccount, so you’ll need to create it before you deploy the controller:
$ kubectl create serviceaccount website-controller
serviceaccount "website-controller" created
If Role Based Access Control (RBAC) is enabled in your cluster, Kubernetes will not
allow the controller to watch Website resources or create Deployments or Services. To
allow it to do that, you’ll need to bind the website-controller ServiceAccount to the
cluster-admin ClusterRole, by creating a ClusterRoleBinding like this:
$ kubectl create clusterrolebinding website-controller 
➥ --clusterrole=cluster-admin 
➥ --serviceaccount=default:website-controller
clusterrolebinding "website-controller" created
Once you have the ServiceAccount and ClusterRoleBinding in place, you can deploy
the controller’s Deployment. 
SEEING THE CONTROLLER IN ACTION
With the controller now running, create the kubia Website resource again:
$ kubectl create -f kubia-website.yaml
website "kubia" created
Now, let’s check the controller’s logs (shown in the following listing) to see if it has
received the watch event.
$ kubectl logs website-controller-2429717411-q43zs -c main
2017/02/26 16:54:41 website-controller started.
2017/02/26 16:54:47 Received watch event: ADDED: kubia: https://github.c...
2017/02/26 16:54:47 Creating services with name kubia-website in namespa... 
2017/02/26 16:54:47 Response status: 201 Created
2017/02/26 16:54:47 Creating deployments with name kubia-website in name... 
2017/02/26 16:54:47 Response status: 201 Created
Listing 18.6
Displaying logs of the Website controller
It will run 
under a special 
ServiceAccount.
Two containers: the 
main container and 
the proxy sidecar
 
</data>
  <data key="d5">516
CHAPTER 18
Extending Kubernetes
    metadata:
      name: website-controller
      labels:
        app: website-controller
    spec:
      serviceAccountName: website-controller    
      containers:                                    
      - name: main                                   
        image: luksa/website-controller              
      - name: proxy                                  
        image: luksa/kubectl-proxy:1.6.2             
As you can see, the Deployment deploys a single replica of a two-container pod. One
container runs your controller, whereas the other one is the ambassador container
used for simpler communication with the API server. The pod runs under its own spe-
cial ServiceAccount, so you’ll need to create it before you deploy the controller:
$ kubectl create serviceaccount website-controller
serviceaccount "website-controller" created
If Role Based Access Control (RBAC) is enabled in your cluster, Kubernetes will not
allow the controller to watch Website resources or create Deployments or Services. To
allow it to do that, you’ll need to bind the website-controller ServiceAccount to the
cluster-admin ClusterRole, by creating a ClusterRoleBinding like this:
$ kubectl create clusterrolebinding website-controller 
➥ --clusterrole=cluster-admin 
➥ --serviceaccount=default:website-controller
clusterrolebinding "website-controller" created
Once you have the ServiceAccount and ClusterRoleBinding in place, you can deploy
the controller’s Deployment. 
SEEING THE CONTROLLER IN ACTION
With the controller now running, create the kubia Website resource again:
$ kubectl create -f kubia-website.yaml
website "kubia" created
Now, let’s check the controller’s logs (shown in the following listing) to see if it has
received the watch event.
$ kubectl logs website-controller-2429717411-q43zs -c main
2017/02/26 16:54:41 website-controller started.
2017/02/26 16:54:47 Received watch event: ADDED: kubia: https://github.c...
2017/02/26 16:54:47 Creating services with name kubia-website in namespa... 
2017/02/26 16:54:47 Response status: 201 Created
2017/02/26 16:54:47 Creating deployments with name kubia-website in name... 
2017/02/26 16:54:47 Response status: 201 Created
Listing 18.6
Displaying logs of the Website controller
It will run 
under a special 
ServiceAccount.
Two containers: the 
main container and 
the proxy sidecar
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="549">
  <data key="d0">Page_549</data>
  <data key="d5">Page_549</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_460">
  <data key="d0">517
Defining custom API objects
The logs show that the controller received the ADDED event and that it created a Service
and a Deployment for the kubia-website Website. The API server responded with a
201 Created response, which means the two resources should now exist. Let’s verify
that the Deployment, Service and the resulting Pod were created. The following list-
ing lists all Deployments, Services and Pods.
$ kubectl get deploy,svc,po
NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE  AGE
deploy/kubia-website        1         1         1            1          4s
deploy/website-controller   1         1         1            1          5m
NAME                CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
svc/kubernetes      10.96.0.1      &lt;none&gt;        443/TCP        38d
svc/kubia-website   10.101.48.23   &lt;nodes&gt;       80:32589/TCP   4s
NAME                                     READY     STATUS    RESTARTS   AGE
po/kubia-website-1029415133-rs715        2/2       Running   0          4s
po/website-controller-1571685839-qzmg6   2/2       Running   1          5m
There they are. The kubia-website Service, through which you can access your web-
site, is available on port 32589 on all cluster nodes. You can access it with your browser.
Awesome, right? 
 Users of your Kubernetes cluster can now deploy static websites in seconds, with-
out knowing anything about Pods, Services, or any other Kubernetes resources, except
your custom Website resource. 
 Obviously, you still have room for improvement. The controller could, for exam-
ple, watch for Service objects and as soon as the node port is assigned, write the URL
the website is accessible at into the status section of the Website resource instance
itself. Or it could also create an Ingress object for each website. I’ll leave the imple-
mentation of these additional features to you as an exercise.
18.1.3 Validating custom objects
You may have noticed that you didn’t specify any kind of validation schema in the Web-
site CustomResourceDefinition. Users can include any field they want in the YAML of
their Website object. The API server doesn’t validate the contents of the YAML (except
the usual fields like apiVersion, kind, and metadata), so users can create invalid
Website objects (without a gitRepo field, for example). 
 Is it possible to add validation to the controller and prevent invalid objects from
being accepted by the API server? It isn’t, because the API server first stores the object,
then returns a success response to the client (kubectl), and only then notifies all the
watchers (the controller is one of them). All the controller can really do is validate
the object when it receives it in a watch event, and if the object is invalid, write the
error message to the Website object (by updating the object through a new request to
the API server). The user wouldn’t be notified of the error automatically. They’d have
Listing 18.7
The Deployment, Service, and Pod created for the kubia-website
 
</data>
  <data key="d5">517
Defining custom API objects
The logs show that the controller received the ADDED event and that it created a Service
and a Deployment for the kubia-website Website. The API server responded with a
201 Created response, which means the two resources should now exist. Let’s verify
that the Deployment, Service and the resulting Pod were created. The following list-
ing lists all Deployments, Services and Pods.
$ kubectl get deploy,svc,po
NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE  AGE
deploy/kubia-website        1         1         1            1          4s
deploy/website-controller   1         1         1            1          5m
NAME                CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
svc/kubernetes      10.96.0.1      &lt;none&gt;        443/TCP        38d
svc/kubia-website   10.101.48.23   &lt;nodes&gt;       80:32589/TCP   4s
NAME                                     READY     STATUS    RESTARTS   AGE
po/kubia-website-1029415133-rs715        2/2       Running   0          4s
po/website-controller-1571685839-qzmg6   2/2       Running   1          5m
There they are. The kubia-website Service, through which you can access your web-
site, is available on port 32589 on all cluster nodes. You can access it with your browser.
Awesome, right? 
 Users of your Kubernetes cluster can now deploy static websites in seconds, with-
out knowing anything about Pods, Services, or any other Kubernetes resources, except
your custom Website resource. 
 Obviously, you still have room for improvement. The controller could, for exam-
ple, watch for Service objects and as soon as the node port is assigned, write the URL
the website is accessible at into the status section of the Website resource instance
itself. Or it could also create an Ingress object for each website. I’ll leave the imple-
mentation of these additional features to you as an exercise.
18.1.3 Validating custom objects
You may have noticed that you didn’t specify any kind of validation schema in the Web-
site CustomResourceDefinition. Users can include any field they want in the YAML of
their Website object. The API server doesn’t validate the contents of the YAML (except
the usual fields like apiVersion, kind, and metadata), so users can create invalid
Website objects (without a gitRepo field, for example). 
 Is it possible to add validation to the controller and prevent invalid objects from
being accepted by the API server? It isn’t, because the API server first stores the object,
then returns a success response to the client (kubectl), and only then notifies all the
watchers (the controller is one of them). All the controller can really do is validate
the object when it receives it in a watch event, and if the object is invalid, write the
error message to the Website object (by updating the object through a new request to
the API server). The user wouldn’t be notified of the error automatically. They’d have
Listing 18.7
The Deployment, Service, and Pod created for the kubia-website
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="550">
  <data key="d0">Page_550</data>
  <data key="d5">Page_550</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_461">
  <data key="d0">518
CHAPTER 18
Extending Kubernetes
to notice the error message by querying the API server for the Website object. Unless
the user does this, they have no way of knowing whether the object is valid or not.
 This obviously isn’t ideal. You’d want the API server to validate the object and
reject invalid objects immediately. Validation of custom objects was introduced in
Kubernetes version 1.8 as an alpha feature. To have the API server validate your cus-
tom objects, you need to enable the CustomResourceValidation feature gate in the
API server and specify a JSON schema in the CRD.
18.1.4 Providing a custom API server for your custom objects
A better way of adding support for custom objects in Kubernetes is to implement your
own API server and have the clients talk directly to it. 
INTRODUCING API SERVER AGGREGATION
In Kubernetes version 1.7, you can integrate your custom API server with the main
Kubernetes API server, through API server aggregation. Initially, the Kubernetes API
server was a single monolithic component. From Kubernetes version 1.7, multiple
aggregated API servers will be exposed at a single location. Clients can connect to the
aggregated API and have their requests transparently forwarded to the appropriate
API server. This way, the client wouldn’t even be aware that multiple API servers han-
dle different objects behind the scenes. Even the core Kubernetes API server may
eventually end up being split into multiple smaller API servers and exposed as a single
server through the aggregator, as shown in figure 18.5.
In your case, you could create an API server responsible for handling your Website
objects. It could validate those objects the way the core Kubernetes API server validates
them. You’d no longer need to create a CRD to represent those objects, because you’d
implement the Website object type into the custom API server directly. 
 Generally, each API server is responsible for storing their own resources. As shown
in figure 18.5, it can either run its own instance of etcd (or a whole etcd cluster), or it
Main
API server
Custom
API server Y
Custom
API server X
kubectl
Uses its own etcd instance
for storing its resources
Uses CustomResourceDeﬁnitions
in main API server as storage
mechanism
etcd
etcd
Figure 18.5
API server aggregation
 
</data>
  <data key="d5">518
CHAPTER 18
Extending Kubernetes
to notice the error message by querying the API server for the Website object. Unless
the user does this, they have no way of knowing whether the object is valid or not.
 This obviously isn’t ideal. You’d want the API server to validate the object and
reject invalid objects immediately. Validation of custom objects was introduced in
Kubernetes version 1.8 as an alpha feature. To have the API server validate your cus-
tom objects, you need to enable the CustomResourceValidation feature gate in the
API server and specify a JSON schema in the CRD.
18.1.4 Providing a custom API server for your custom objects
A better way of adding support for custom objects in Kubernetes is to implement your
own API server and have the clients talk directly to it. 
INTRODUCING API SERVER AGGREGATION
In Kubernetes version 1.7, you can integrate your custom API server with the main
Kubernetes API server, through API server aggregation. Initially, the Kubernetes API
server was a single monolithic component. From Kubernetes version 1.7, multiple
aggregated API servers will be exposed at a single location. Clients can connect to the
aggregated API and have their requests transparently forwarded to the appropriate
API server. This way, the client wouldn’t even be aware that multiple API servers han-
dle different objects behind the scenes. Even the core Kubernetes API server may
eventually end up being split into multiple smaller API servers and exposed as a single
server through the aggregator, as shown in figure 18.5.
In your case, you could create an API server responsible for handling your Website
objects. It could validate those objects the way the core Kubernetes API server validates
them. You’d no longer need to create a CRD to represent those objects, because you’d
implement the Website object type into the custom API server directly. 
 Generally, each API server is responsible for storing their own resources. As shown
in figure 18.5, it can either run its own instance of etcd (or a whole etcd cluster), or it
Main
API server
Custom
API server Y
Custom
API server X
kubectl
Uses its own etcd instance
for storing its resources
Uses CustomResourceDeﬁnitions
in main API server as storage
mechanism
etcd
etcd
Figure 18.5
API server aggregation
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="551">
  <data key="d0">Page_551</data>
  <data key="d5">Page_551</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_462">
  <data key="d0">519
Extending Kubernetes with the Kubernetes Service Catalog
can store its resources in the core API server’s etcd store by creating CRD instances in
the core API server. In that case, it needs to create a CRD object first, before creating
instances of the CRD, the way you did in the example.
REGISTERING A CUSTOM API SERVER
To add a custom API server to your cluster, you’d deploy it as a pod and expose it
through a Service. Then, to integrate it into the main API server, you’d deploy a YAML
manifest describing an APIService resource like the one in the following listing.
apiVersion: apiregistration.k8s.io/v1beta1   
kind: APIService                             
metadata:
  name: v1alpha1.extensions.example.com
spec:
  group: extensions.example.com           
  version: v1alpha1                      
  priority: 150
  service:                    
    name: website-api         
    namespace: default        
After creating the APIService resource from the previous listing, client requests sent
to the main API server that contain any resource from the extensions.example.com
API group and version v1alpha1 would be forwarded to the custom API server pod(s)
exposed through the website-api Service. 
CREATING CUSTOM CLIENTS
While you can create custom resources from YAML files using the regular kubectl cli-
ent, to make deployment of custom objects even easier, in addition to providing a cus-
tom API server, you can also build a custom CLI tool. This will allow you to add
dedicated commands for manipulating those objects, similar to how kubectl allows
creating Secrets, Deployments, and other resources through resource-specific com-
mands like kubectl create secret or kubectl create deployment.
 As I’ve already mentioned, custom API servers, API server aggregation, and other
features related to extending Kubernetes are currently being worked on intensively, so
they may change after the book is published. To get up-to-date information on the
subject, refer to the Kubernetes GitHub repos at http:/
/github.com/kubernetes.
18.2
Extending Kubernetes with the Kubernetes Service 
Catalog
One of the first additional API servers that will be added to Kubernetes through API
server aggregation is the Service Catalog API server. The Service Catalog is a hot topic
in the Kubernetes community, so you may want to know about it. 
 Currently, for a pod to consume a service (here I use the term generally, not in
relation to Service resources; for example, a database service includes everything
Listing 18.8
An APIService YAML definition 
This is an APIService 
resource.
The API group this API 
server is responsible for
The supported API version
The Service the custom API 
server is exposed through
 
</data>
  <data key="d5">519
Extending Kubernetes with the Kubernetes Service Catalog
can store its resources in the core API server’s etcd store by creating CRD instances in
the core API server. In that case, it needs to create a CRD object first, before creating
instances of the CRD, the way you did in the example.
REGISTERING A CUSTOM API SERVER
To add a custom API server to your cluster, you’d deploy it as a pod and expose it
through a Service. Then, to integrate it into the main API server, you’d deploy a YAML
manifest describing an APIService resource like the one in the following listing.
apiVersion: apiregistration.k8s.io/v1beta1   
kind: APIService                             
metadata:
  name: v1alpha1.extensions.example.com
spec:
  group: extensions.example.com           
  version: v1alpha1                      
  priority: 150
  service:                    
    name: website-api         
    namespace: default        
After creating the APIService resource from the previous listing, client requests sent
to the main API server that contain any resource from the extensions.example.com
API group and version v1alpha1 would be forwarded to the custom API server pod(s)
exposed through the website-api Service. 
CREATING CUSTOM CLIENTS
While you can create custom resources from YAML files using the regular kubectl cli-
ent, to make deployment of custom objects even easier, in addition to providing a cus-
tom API server, you can also build a custom CLI tool. This will allow you to add
dedicated commands for manipulating those objects, similar to how kubectl allows
creating Secrets, Deployments, and other resources through resource-specific com-
mands like kubectl create secret or kubectl create deployment.
 As I’ve already mentioned, custom API servers, API server aggregation, and other
features related to extending Kubernetes are currently being worked on intensively, so
they may change after the book is published. To get up-to-date information on the
subject, refer to the Kubernetes GitHub repos at http:/
/github.com/kubernetes.
18.2
Extending Kubernetes with the Kubernetes Service 
Catalog
One of the first additional API servers that will be added to Kubernetes through API
server aggregation is the Service Catalog API server. The Service Catalog is a hot topic
in the Kubernetes community, so you may want to know about it. 
 Currently, for a pod to consume a service (here I use the term generally, not in
relation to Service resources; for example, a database service includes everything
Listing 18.8
An APIService YAML definition 
This is an APIService 
resource.
The API group this API 
server is responsible for
The supported API version
The Service the custom API 
server is exposed through
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="552">
  <data key="d0">Page_552</data>
  <data key="d5">Page_552</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_463">
  <data key="d0">520
CHAPTER 18
Extending Kubernetes
required to allow users to use a database in their app), someone needs to deploy the
pods providing the service, a Service resource, and possibly a Secret so the client pod
can use it to authenticate with the service. That someone is usually the same user
deploying the client pod or, if a team is dedicated to deploying these types of general
services, the user needs to file a ticket and wait for the team to provision the service.
This means the user needs to either create the manifests for all the components of the
service, know where to find an existing set of manifests, know how to configure it
properly, and deploy it manually, or wait for the other team to do it. 
 But Kubernetes is supposed to be an easy-to-use, self-service system. Ideally, users
whose apps require a certain service (for example, a web application requiring a back-
end database), should be able to say to Kubernetes. “Hey, I need a PostgreSQL data-
base. Please provision one and tell me where and how I can connect to it.” This will
soon be possible through the Kubernetes Service Catalog. 
18.2.1 Introducing the Service Catalog
As the name suggests, the Service Catalog is a catalog of services. Users can browse
through the catalog and provision instances of the services listed in the catalog by
themselves without having to deal with Pods, Services, ConfigMaps, and other resources
required for the service to run. You’ll recognize that this is similar to what you did
with the Website custom resource.
 Instead of adding custom resources to the API server for each type of service, the
Service Catalog introduces the following four generic API resources:
A ClusterServiceBroker, which describes an (external) system that can provision
services
A ClusterServiceClass, which describes a type of service that can be provisioned
A ServiceInstance, which is one instance of a service that has been provisioned
A ServiceBinding, which represents a binding between a set of clients (pods)
and a ServiceInstance
The relationships between those four resources are shown in the figure 18.6 and
explained in the following paragraphs.
In a nutshell, a cluster admin creates a ClusterServiceBroker resource for each service
broker whose services they’d like to make available in the cluster. Kubernetes then asks
the broker for a list of services that it can provide and creates a ClusterServiceClass
resource for each of them. When a user requires a service to be provisioned, they create
an ServiceInstance resource and then a ServiceBinding to bind that ServiceInstance to
Client pods
ServiceBinding
ServiceInstance
ClusterServiceClass(es)
ClusterServiceBroker
Figure 18.6
The relationships between Service Catalog API resources. 
 
</data>
  <data key="d5">520
CHAPTER 18
Extending Kubernetes
required to allow users to use a database in their app), someone needs to deploy the
pods providing the service, a Service resource, and possibly a Secret so the client pod
can use it to authenticate with the service. That someone is usually the same user
deploying the client pod or, if a team is dedicated to deploying these types of general
services, the user needs to file a ticket and wait for the team to provision the service.
This means the user needs to either create the manifests for all the components of the
service, know where to find an existing set of manifests, know how to configure it
properly, and deploy it manually, or wait for the other team to do it. 
 But Kubernetes is supposed to be an easy-to-use, self-service system. Ideally, users
whose apps require a certain service (for example, a web application requiring a back-
end database), should be able to say to Kubernetes. “Hey, I need a PostgreSQL data-
base. Please provision one and tell me where and how I can connect to it.” This will
soon be possible through the Kubernetes Service Catalog. 
18.2.1 Introducing the Service Catalog
As the name suggests, the Service Catalog is a catalog of services. Users can browse
through the catalog and provision instances of the services listed in the catalog by
themselves without having to deal with Pods, Services, ConfigMaps, and other resources
required for the service to run. You’ll recognize that this is similar to what you did
with the Website custom resource.
 Instead of adding custom resources to the API server for each type of service, the
Service Catalog introduces the following four generic API resources:
A ClusterServiceBroker, which describes an (external) system that can provision
services
A ClusterServiceClass, which describes a type of service that can be provisioned
A ServiceInstance, which is one instance of a service that has been provisioned
A ServiceBinding, which represents a binding between a set of clients (pods)
and a ServiceInstance
The relationships between those four resources are shown in the figure 18.6 and
explained in the following paragraphs.
In a nutshell, a cluster admin creates a ClusterServiceBroker resource for each service
broker whose services they’d like to make available in the cluster. Kubernetes then asks
the broker for a list of services that it can provide and creates a ClusterServiceClass
resource for each of them. When a user requires a service to be provisioned, they create
an ServiceInstance resource and then a ServiceBinding to bind that ServiceInstance to
Client pods
ServiceBinding
ServiceInstance
ClusterServiceClass(es)
ClusterServiceBroker
Figure 18.6
The relationships between Service Catalog API resources. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="553">
  <data key="d0">Page_553</data>
  <data key="d5">Page_553</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_464">
  <data key="d0">521
Extending Kubernetes with the Kubernetes Service Catalog
their pods. Those pods are then injected with a Secret that holds all the necessary cre-
dentials and other data required to connect to the provisioned ServiceInstance.
 The Service Catalog system architecture is shown in figure 18.7.
The components shown in the figure are explained in the following sections.
18.2.2 Introducing the Service Catalog API server and Controller 
Manager
Similar to core Kubernetes, the Service Catalog is a distributed system composed of
three components:
Service Catalog API Server
etcd as the storage
Controller Manager, where all the controllers run
The four Service Catalog–related resources we introduced earlier are created by post-
ing YAML/JSON manifests to the API server. It then stores them into its own etcd
instance or uses CustomResourceDefinitions in the main API server as an alternative
storage mechanism (in that case, no additional etcd instance is required). 
 The controllers running in the Controller Manager are the ones doing some-
thing with those resources. They obviously talk to the Service Catalog API server, the
way other core Kubernetes controllers talk to the core API server. Those controllers
don’t provision the requested services themselves. They leave that up to external
service brokers, which are registered by creating ServiceBroker resources in the Ser-
vice Catalog API.
Kubernetes cluster
External system(s)
Kubernetes Service Catalog
Client pods
Provisioned
services
Broker A
Broker B
etcd
Service
Catalog
API server
Controller
Manager
kubectl
Provisioned
services
Client pods use the
provisioned services
Figure 18.7
The architecture of the Service Catalog
 
</data>
  <data key="d5">521
Extending Kubernetes with the Kubernetes Service Catalog
their pods. Those pods are then injected with a Secret that holds all the necessary cre-
dentials and other data required to connect to the provisioned ServiceInstance.
 The Service Catalog system architecture is shown in figure 18.7.
The components shown in the figure are explained in the following sections.
18.2.2 Introducing the Service Catalog API server and Controller 
Manager
Similar to core Kubernetes, the Service Catalog is a distributed system composed of
three components:
Service Catalog API Server
etcd as the storage
Controller Manager, where all the controllers run
The four Service Catalog–related resources we introduced earlier are created by post-
ing YAML/JSON manifests to the API server. It then stores them into its own etcd
instance or uses CustomResourceDefinitions in the main API server as an alternative
storage mechanism (in that case, no additional etcd instance is required). 
 The controllers running in the Controller Manager are the ones doing some-
thing with those resources. They obviously talk to the Service Catalog API server, the
way other core Kubernetes controllers talk to the core API server. Those controllers
don’t provision the requested services themselves. They leave that up to external
service brokers, which are registered by creating ServiceBroker resources in the Ser-
vice Catalog API.
Kubernetes cluster
External system(s)
Kubernetes Service Catalog
Client pods
Provisioned
services
Broker A
Broker B
etcd
Service
Catalog
API server
Controller
Manager
kubectl
Provisioned
services
Client pods use the
provisioned services
Figure 18.7
The architecture of the Service Catalog
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="554">
  <data key="d0">Page_554</data>
  <data key="d5">Page_554</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_465">
  <data key="d0">522
CHAPTER 18
Extending Kubernetes
18.2.3 Introducing Service Brokers and the OpenServiceBroker API
A cluster administrator can register one or more external ServiceBrokers in the Ser-
vice Catalog. Every broker must implement the OpenServiceBroker API.
INTRODUCING THE OPENSERVICEBROKER API
The Service Catalog talks to the broker through that API. The API is relatively simple.
It’s a REST API providing the following operations:
Retrieving the list of services with GET /v2/catalog
Provisioning a service instance (PUT /v2/service_instances/:id)
Updating a service instance (PATCH /v2/service_instances/:id)
Binding a service instance (PUT /v2/service_instances/:id/service_bind-
ings/:binding_id)
Unbinding an instance (DELETE /v2/service_instances/:id/service_bind-
ings/:binding_id)
Deprovisioning a service instance (DELETE /v2/service_instances/:id)
You’ll find the OpenServiceBroker API spec at https:/
/github.com/openservicebro-
kerapi/servicebroker.
REGISTERING BROKERS IN THE SERVICE CATALOG
The cluster administrator registers a broker by posting a ServiceBroker resource man-
ifest to the Service Catalog API, like the one shown in the following listing.
apiVersion: servicecatalog.k8s.io/v1alpha1    
kind: ClusterServiceBroker                                  
metadata:
  name: database-broker                          
spec:
  url: http://database-osbapi.myorganization.org  
The listing describes an imaginary broker that can provision databases of different
types. After the administrator creates the ClusterServiceBroker resource, a controller
in the Service Catalog Controller Manager connects to the URL specified in the
resource to retrieve the list of services this broker can provision.
 After the Service Catalog retrieves the list of services, it creates a ClusterService-
Class resource for each of them. Each ClusterServiceClass resource describes a sin-
gle type of service that can be provisioned (an example of a ClusterServiceClass is
“PostgreSQL database”). Each ClusterServiceClass has one or more service plans asso-
ciated with it. These allow the user to choose the level of service they need (for exam-
ple, a database ClusterServiceClass could provide a “Free” plan, where the size of the
Listing 18.9
A ClusterServiceBroker manifest: database-broker.yaml
The resource kind and 
the API group and version
The name of this broker
Where the Service Catalog
can contact the broker
(its OpenServiceBroker [OSB] API URL)
 
</data>
  <data key="d5">522
CHAPTER 18
Extending Kubernetes
18.2.3 Introducing Service Brokers and the OpenServiceBroker API
A cluster administrator can register one or more external ServiceBrokers in the Ser-
vice Catalog. Every broker must implement the OpenServiceBroker API.
INTRODUCING THE OPENSERVICEBROKER API
The Service Catalog talks to the broker through that API. The API is relatively simple.
It’s a REST API providing the following operations:
Retrieving the list of services with GET /v2/catalog
Provisioning a service instance (PUT /v2/service_instances/:id)
Updating a service instance (PATCH /v2/service_instances/:id)
Binding a service instance (PUT /v2/service_instances/:id/service_bind-
ings/:binding_id)
Unbinding an instance (DELETE /v2/service_instances/:id/service_bind-
ings/:binding_id)
Deprovisioning a service instance (DELETE /v2/service_instances/:id)
You’ll find the OpenServiceBroker API spec at https:/
/github.com/openservicebro-
kerapi/servicebroker.
REGISTERING BROKERS IN THE SERVICE CATALOG
The cluster administrator registers a broker by posting a ServiceBroker resource man-
ifest to the Service Catalog API, like the one shown in the following listing.
apiVersion: servicecatalog.k8s.io/v1alpha1    
kind: ClusterServiceBroker                                  
metadata:
  name: database-broker                          
spec:
  url: http://database-osbapi.myorganization.org  
The listing describes an imaginary broker that can provision databases of different
types. After the administrator creates the ClusterServiceBroker resource, a controller
in the Service Catalog Controller Manager connects to the URL specified in the
resource to retrieve the list of services this broker can provision.
 After the Service Catalog retrieves the list of services, it creates a ClusterService-
Class resource for each of them. Each ClusterServiceClass resource describes a sin-
gle type of service that can be provisioned (an example of a ClusterServiceClass is
“PostgreSQL database”). Each ClusterServiceClass has one or more service plans asso-
ciated with it. These allow the user to choose the level of service they need (for exam-
ple, a database ClusterServiceClass could provide a “Free” plan, where the size of the
Listing 18.9
A ClusterServiceBroker manifest: database-broker.yaml
The resource kind and 
the API group and version
The name of this broker
Where the Service Catalog
can contact the broker
(its OpenServiceBroker [OSB] API URL)
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="555">
  <data key="d0">Page_555</data>
  <data key="d5">Page_555</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_466">
  <data key="d0">523
Extending Kubernetes with the Kubernetes Service Catalog
database is limited and the underlying storage is a spinning disk, and a “Premium”
plan, with unlimited size and SSD storage). 
LISTING THE AVAILABLE SERVICES IN A CLUSTER
Users of the Kubernetes cluster can retrieve a list of all services that can be provi-
sioned in the cluster with kubectl get serviceclasses, as shown in the following
listing.
$ kubectl get clusterserviceclasses
NAME                KIND
postgres-database   ClusterServiceClass.v1alpha1.servicecatalog.k8s.io
mysql-database      ServiceClass.v1alpha1.servicecatalog.k8s.io
mongodb-database    ServiceClass.v1alpha1.servicecatalog.k8s.io
The listing shows ClusterServiceClasses for services that your imaginary database bro-
ker could provide. You can compare ClusterServiceClasses to StorageClasses, which we
discussed in chapter 6. StorageClasses allow you to select the type of storage you’d like
to use in your pods, while ClusterServiceClasses allow you to select the type of service.
 You can see details of one of the ClusterServiceClasses by retrieving its YAML. An
example is shown in the following listing.
$ kubectl get serviceclass postgres-database -o yaml
apiVersion: servicecatalog.k8s.io/v1alpha1
bindable: true
brokerName: database-broker                     
description: A PostgreSQL database
kind: ClusterServiceClass
metadata:
  name: postgres-database
  ...
planUpdatable: false
plans:
- description: A free (but slow) PostgreSQL instance        
  name: free                                                
  osbFree: true                                             
  ...
- description: A paid (very fast) PostgreSQL instance      
  name: premium                                            
  osbFree: false                                           
  ...
The ClusterServiceClass in the listing contains two plans—a free plan, and a premium
plan. You can see that this ClusterServiceClass is provided by the database-broker
broker.
Listing 18.10
List of ClusterServiceClasses in a cluster
Listing 18.11
A ClusterServiceClass definition
This ClusterServiceClass 
is provided by the 
database-broker.
A free plan for 
this service
A paid plan
 
</data>
  <data key="d5">523
Extending Kubernetes with the Kubernetes Service Catalog
database is limited and the underlying storage is a spinning disk, and a “Premium”
plan, with unlimited size and SSD storage). 
LISTING THE AVAILABLE SERVICES IN A CLUSTER
Users of the Kubernetes cluster can retrieve a list of all services that can be provi-
sioned in the cluster with kubectl get serviceclasses, as shown in the following
listing.
$ kubectl get clusterserviceclasses
NAME                KIND
postgres-database   ClusterServiceClass.v1alpha1.servicecatalog.k8s.io
mysql-database      ServiceClass.v1alpha1.servicecatalog.k8s.io
mongodb-database    ServiceClass.v1alpha1.servicecatalog.k8s.io
The listing shows ClusterServiceClasses for services that your imaginary database bro-
ker could provide. You can compare ClusterServiceClasses to StorageClasses, which we
discussed in chapter 6. StorageClasses allow you to select the type of storage you’d like
to use in your pods, while ClusterServiceClasses allow you to select the type of service.
 You can see details of one of the ClusterServiceClasses by retrieving its YAML. An
example is shown in the following listing.
$ kubectl get serviceclass postgres-database -o yaml
apiVersion: servicecatalog.k8s.io/v1alpha1
bindable: true
brokerName: database-broker                     
description: A PostgreSQL database
kind: ClusterServiceClass
metadata:
  name: postgres-database
  ...
planUpdatable: false
plans:
- description: A free (but slow) PostgreSQL instance        
  name: free                                                
  osbFree: true                                             
  ...
- description: A paid (very fast) PostgreSQL instance      
  name: premium                                            
  osbFree: false                                           
  ...
The ClusterServiceClass in the listing contains two plans—a free plan, and a premium
plan. You can see that this ClusterServiceClass is provided by the database-broker
broker.
Listing 18.10
List of ClusterServiceClasses in a cluster
Listing 18.11
A ClusterServiceClass definition
This ClusterServiceClass 
is provided by the 
database-broker.
A free plan for 
this service
A paid plan
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="556">
  <data key="d0">Page_556</data>
  <data key="d5">Page_556</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_467">
  <data key="d0">524
CHAPTER 18
Extending Kubernetes
18.2.4 Provisioning and using a service
Let’s imagine the pods you’re deploying need to use a database. You’ve inspected the
list of available ClusterServiceClasses and have chosen to use the free plan of the
postgres-database ClusterServiceClass. 
PROVISIONING A SERVICEINSTANCE
To have the database provisioned for you, all you need to do is create a Service-
Instance resource, as shown in the following listing.
apiVersion: servicecatalog.k8s.io/v1alpha1
kind: ServiceInstance
metadata:
  name: my-postgres-db                     
spec:
  clusterServiceClassName: postgres-database        
  clusterServicePlanName: free                             
  parameters:
    init-db-args: --data-checksums         
You created a ServiceInstance called my-postgres-db (that will be the name of the
resource you’re deploying) and specified the ClusterServiceClass and the chosen
plan. You’re also specifying a parameter, which is specific for each broker and Cluster-
ServiceClass. Let’s imagine you looked up the possible parameters in the broker’s doc-
umentation.
 As soon as you create this resource, the Service Catalog will contact the broker the
ClusterServiceClass belongs to and ask it to provision the service. It will pass on the
chosen ClusterServiceClass and plan names, as well as all the parameters you specified.
 It’s then completely up to the broker to know what to do with this information. In
your case, your database broker will probably spin up a new instance of a PostgreSQL
database somewhere—not necessarily in the same Kubernetes cluster or even in
Kubernetes at all. It could run a Virtual Machine and run the database in there. The
Service Catalog doesn’t care, and neither does the user requesting the service. 
 You can check if the service has been provisioned successfully by inspecting the
status section of the my-postgres-db ServiceInstance you created, as shown in the
following listing.
$ kubectl get instance my-postgres-db -o yaml
apiVersion: servicecatalog.k8s.io/v1alpha1
kind: ServiceInstance
...
status:
  asyncOpInProgress: false
  conditions:
Listing 18.12
A ServiceInstance manifest: database-instance.yaml
Listing 18.13
Inspecting the status of a ServiceInstance
You’re giving this 
Instance a name.
The ServiceClass 
and Plan you want
Additional parameters 
passed to the broker
 
</data>
  <data key="d5">524
CHAPTER 18
Extending Kubernetes
18.2.4 Provisioning and using a service
Let’s imagine the pods you’re deploying need to use a database. You’ve inspected the
list of available ClusterServiceClasses and have chosen to use the free plan of the
postgres-database ClusterServiceClass. 
PROVISIONING A SERVICEINSTANCE
To have the database provisioned for you, all you need to do is create a Service-
Instance resource, as shown in the following listing.
apiVersion: servicecatalog.k8s.io/v1alpha1
kind: ServiceInstance
metadata:
  name: my-postgres-db                     
spec:
  clusterServiceClassName: postgres-database        
  clusterServicePlanName: free                             
  parameters:
    init-db-args: --data-checksums         
You created a ServiceInstance called my-postgres-db (that will be the name of the
resource you’re deploying) and specified the ClusterServiceClass and the chosen
plan. You’re also specifying a parameter, which is specific for each broker and Cluster-
ServiceClass. Let’s imagine you looked up the possible parameters in the broker’s doc-
umentation.
 As soon as you create this resource, the Service Catalog will contact the broker the
ClusterServiceClass belongs to and ask it to provision the service. It will pass on the
chosen ClusterServiceClass and plan names, as well as all the parameters you specified.
 It’s then completely up to the broker to know what to do with this information. In
your case, your database broker will probably spin up a new instance of a PostgreSQL
database somewhere—not necessarily in the same Kubernetes cluster or even in
Kubernetes at all. It could run a Virtual Machine and run the database in there. The
Service Catalog doesn’t care, and neither does the user requesting the service. 
 You can check if the service has been provisioned successfully by inspecting the
status section of the my-postgres-db ServiceInstance you created, as shown in the
following listing.
$ kubectl get instance my-postgres-db -o yaml
apiVersion: servicecatalog.k8s.io/v1alpha1
kind: ServiceInstance
...
status:
  asyncOpInProgress: false
  conditions:
Listing 18.12
A ServiceInstance manifest: database-instance.yaml
Listing 18.13
Inspecting the status of a ServiceInstance
You’re giving this 
Instance a name.
The ServiceClass 
and Plan you want
Additional parameters 
passed to the broker
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="557">
  <data key="d0">Page_557</data>
  <data key="d5">Page_557</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_468">
  <data key="d0">525
Extending Kubernetes with the Kubernetes Service Catalog
  - lastTransitionTime: 2017-05-17T13:57:22Z
    message: The instance was provisioned successfully    
    reason: ProvisionedSuccessfully                       
    status: "True"
    type: Ready                   
A database instance is now running somewhere, but how do you use it in your pods?
To do that, you need to bind it.
BINDING A SERVICEINSTANCE
To use a provisioned ServiceInstance in your pods, you create a ServiceBinding
resource, as shown in the following listing.
apiVersion: servicecatalog.k8s.io/v1alpha1
kind: ServiceBinding
metadata:
  name: my-postgres-db-binding
spec:
  instanceRef:                          
    name: my-postgres-db                
  secretName: postgres-secret           
The listing shows that you’re defining a ServiceBinding resource called my-postgres-
db-binding, in which you’re referencing the my-postgres-db service instance you
created earlier. You’re also specifying a name of a Secret. You want the Service Catalog
to put all the necessary credentials for accessing the service instance into a Secret
called postgres-secret. But where are you binding the ServiceInstance to your pods?
Nowhere, actually.
 Currently, the Service Catalog doesn’t yet make it possible to inject pods with the
ServiceInstance’s credentials. This will be possible when a new Kubernetes feature
called PodPresets is available. Until then, you can choose a name for the Secret
where you want the credentials to be stored in and mount that Secret into your pods
manually.
 When you submit the ServiceBinding resource from the previous listing to the Ser-
vice Catalog API server, the controller will contact the Database broker once again
and create a binding for the ServiceInstance you provisioned earlier. The broker
responds with a list of credentials and other data necessary for connecting to the data-
base. The Service Catalog creates a new Secret with the name you specified in the
ServiceBinding resource and stores all that data in the Secret. 
USING THE NEWLY CREATED SECRET IN CLIENT PODS
The Secret created by the Service Catalog system can be mounted into pods, so they
can read its contents and use them to connect to the provisioned service instance (a
PostgreSQL database in the example). The Secret could look like the one in the fol-
lowing listing.
Listing 18.14
A ServiceBinding: my-postgres-db-binding.yaml
The database was 
provisioned successfully.
It’s ready to be used.
You’re referencing the 
instance you created 
earlier.
You’d like the credentials 
for accessing the service 
stored in this Secret.
 
</data>
  <data key="d5">525
Extending Kubernetes with the Kubernetes Service Catalog
  - lastTransitionTime: 2017-05-17T13:57:22Z
    message: The instance was provisioned successfully    
    reason: ProvisionedSuccessfully                       
    status: "True"
    type: Ready                   
A database instance is now running somewhere, but how do you use it in your pods?
To do that, you need to bind it.
BINDING A SERVICEINSTANCE
To use a provisioned ServiceInstance in your pods, you create a ServiceBinding
resource, as shown in the following listing.
apiVersion: servicecatalog.k8s.io/v1alpha1
kind: ServiceBinding
metadata:
  name: my-postgres-db-binding
spec:
  instanceRef:                          
    name: my-postgres-db                
  secretName: postgres-secret           
The listing shows that you’re defining a ServiceBinding resource called my-postgres-
db-binding, in which you’re referencing the my-postgres-db service instance you
created earlier. You’re also specifying a name of a Secret. You want the Service Catalog
to put all the necessary credentials for accessing the service instance into a Secret
called postgres-secret. But where are you binding the ServiceInstance to your pods?
Nowhere, actually.
 Currently, the Service Catalog doesn’t yet make it possible to inject pods with the
ServiceInstance’s credentials. This will be possible when a new Kubernetes feature
called PodPresets is available. Until then, you can choose a name for the Secret
where you want the credentials to be stored in and mount that Secret into your pods
manually.
 When you submit the ServiceBinding resource from the previous listing to the Ser-
vice Catalog API server, the controller will contact the Database broker once again
and create a binding for the ServiceInstance you provisioned earlier. The broker
responds with a list of credentials and other data necessary for connecting to the data-
base. The Service Catalog creates a new Secret with the name you specified in the
ServiceBinding resource and stores all that data in the Secret. 
USING THE NEWLY CREATED SECRET IN CLIENT PODS
The Secret created by the Service Catalog system can be mounted into pods, so they
can read its contents and use them to connect to the provisioned service instance (a
PostgreSQL database in the example). The Secret could look like the one in the fol-
lowing listing.
Listing 18.14
A ServiceBinding: my-postgres-db-binding.yaml
The database was 
provisioned successfully.
It’s ready to be used.
You’re referencing the 
instance you created 
earlier.
You’d like the credentials 
for accessing the service 
stored in this Secret.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="558">
  <data key="d0">Page_558</data>
  <data key="d5">Page_558</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_469">
  <data key="d0">526
CHAPTER 18
Extending Kubernetes
$ kubectl get secret postgres-secret -o yaml
apiVersion: v1
data:
  host: &lt;base64-encoded hostname of the database&gt;     
  username: &lt;base64-encoded username&gt;                 
  password: &lt;base64-encoded password&gt;                 
kind: Secret
metadata:
  name: postgres-secret
  namespace: default
  ...
type: Opaque
Because you can choose the name of the Secret yourself, you can deploy pods before
provisioning or binding the service. As you learned in chapter 7, the pods won’t be
started until such a Secret exists. 
 If necessary, multiple bindings can be created for different pods. The service bro-
ker can choose to use the same set of credentials in every binding, but it’s better to
create a new set of credentials for every binding instance. This way, pods can be pre-
vented from using the service by deleting the ServiceBinding resource.
18.2.5 Unbinding and deprovisioning
Once you no longer need a ServiceBinding, you can delete it the way you delete other
resources:
$ kubectl delete servicebinding my-postgres-db-binding
servicebinding "my-postgres-db-binding" deleted
When you do this, the Service Catalog controller will delete the Secret and call the bro-
ker to perform an unbinding operation. The service instance (in your case a PostgreSQL
database) is still running. You can therefore create a new ServiceBinding if you want.
 But if you don’t need the database instance anymore, you should delete the Service-
Instance resource also:
$ kubectl delete serviceinstance my-postgres-db
serviceinstance "my-postgres-db " deleted
Deleting the ServiceInstance resource causes the Service Catalog to perform a depro-
visioning operation on the service broker. Again, exactly what that means is up to the
service broker, but in your case, the broker should shut down the PostgreSQL data-
base instance that it created when we provisioned the service instance.
18.2.6 Understanding what the Service Catalog brings
As you’ve learned, the Service Catalog enables service providers make it possible to
expose those services in any Kubernetes cluster by registering the broker in that cluster.
Listing 18.15
A Secret holding the credentials for connecting to the service instance
This is what the pod 
should use to connect to 
the database service.
 
</data>
  <data key="d5">526
CHAPTER 18
Extending Kubernetes
$ kubectl get secret postgres-secret -o yaml
apiVersion: v1
data:
  host: &lt;base64-encoded hostname of the database&gt;     
  username: &lt;base64-encoded username&gt;                 
  password: &lt;base64-encoded password&gt;                 
kind: Secret
metadata:
  name: postgres-secret
  namespace: default
  ...
type: Opaque
Because you can choose the name of the Secret yourself, you can deploy pods before
provisioning or binding the service. As you learned in chapter 7, the pods won’t be
started until such a Secret exists. 
 If necessary, multiple bindings can be created for different pods. The service bro-
ker can choose to use the same set of credentials in every binding, but it’s better to
create a new set of credentials for every binding instance. This way, pods can be pre-
vented from using the service by deleting the ServiceBinding resource.
18.2.5 Unbinding and deprovisioning
Once you no longer need a ServiceBinding, you can delete it the way you delete other
resources:
$ kubectl delete servicebinding my-postgres-db-binding
servicebinding "my-postgres-db-binding" deleted
When you do this, the Service Catalog controller will delete the Secret and call the bro-
ker to perform an unbinding operation. The service instance (in your case a PostgreSQL
database) is still running. You can therefore create a new ServiceBinding if you want.
 But if you don’t need the database instance anymore, you should delete the Service-
Instance resource also:
$ kubectl delete serviceinstance my-postgres-db
serviceinstance "my-postgres-db " deleted
Deleting the ServiceInstance resource causes the Service Catalog to perform a depro-
visioning operation on the service broker. Again, exactly what that means is up to the
service broker, but in your case, the broker should shut down the PostgreSQL data-
base instance that it created when we provisioned the service instance.
18.2.6 Understanding what the Service Catalog brings
As you’ve learned, the Service Catalog enables service providers make it possible to
expose those services in any Kubernetes cluster by registering the broker in that cluster.
Listing 18.15
A Secret holding the credentials for connecting to the service instance
This is what the pod 
should use to connect to 
the database service.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="559">
  <data key="d0">Page_559</data>
  <data key="d5">Page_559</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_470">
  <data key="d0">527
Platforms built on top of Kubernetes
For example, I’ve been involved with the Service Catalog since early on and have
implemented a broker, which makes it trivial to provision messaging systems and
expose them to pods in a Kubernetes cluster. Another team has implemented a broker
that makes it easy to provision Amazon Web Services. 
 In general, service brokers allow easy provisioning and exposing of services in
Kubernetes and will make Kubernetes an even more awesome platform for deploying
your applications. 
18.3
Platforms built on top of Kubernetes
I’m sure you’ll agree that Kubernetes is a great system by itself. Given that it’s easily
extensible across all its components, it’s no wonder companies that had previously
developed their own custom platforms are now re-implementing them on top of
Kubernetes. Kubernetes is, in fact, becoming a widely accepted foundation for the
new generation of Platform-as-a-Service offerings.
 Among the best-known PaaS systems built on Kubernetes are Deis Workflow and
Red Hat’s OpenShift. We’ll do a quick overview of both systems to give you a sense of
what they offer on top of all the awesome stuff Kubernetes already offers.
18.3.1 Red Hat OpenShift Container Platform
Red Hat OpenShift is a Platform-as-a-Service and as such, it has a strong focus on
developer experience. Among its goals are enabling rapid development of applica-
tions, as well as easy deployment, scaling, and long-term maintenance of those apps.
OpenShift has been around much longer than Kubernetes. Versions 1 and 2 were
built from the ground up and had nothing to do with Kubernetes, but when Kuberne-
tes was announced, Red Hat decided to rebuild OpenShift version 3 from scratch—
this time on top of Kubernetes. When a company such as Red Hat decides to throw
away an old version of their software and build a new one on top of an existing tech-
nology like Kubernetes, it should be clear to everyone how great Kubernetes is.
 Kubernetes automates rollouts and application scaling, whereas OpenShift also auto-
mates the actual building of application images and their automatic deployment with-
out requiring you to integrate a Continuous Integration solution into your cluster. 
 OpenShift also provides user and group management, which allows you to run a
properly secured multi-tenant Kubernetes cluster, where individual users are only
allowed to access their own Kubernetes namespaces and the apps running in those
namespaces are also fully network-isolated from each other by default. 
INTRODUCING ADDITIONAL RESOURCES AVAILABLE IN OPENSHIFT
OpenShift provides some additional API objects in addition to all those available in
Kubernetes. We’ll explain them in the next few paragraphs to give you a good over-
view of what OpenShift does and what it provides.
 The additional resources include
Users &amp; Groups
Projects
 
</data>
  <data key="d5">527
Platforms built on top of Kubernetes
For example, I’ve been involved with the Service Catalog since early on and have
implemented a broker, which makes it trivial to provision messaging systems and
expose them to pods in a Kubernetes cluster. Another team has implemented a broker
that makes it easy to provision Amazon Web Services. 
 In general, service brokers allow easy provisioning and exposing of services in
Kubernetes and will make Kubernetes an even more awesome platform for deploying
your applications. 
18.3
Platforms built on top of Kubernetes
I’m sure you’ll agree that Kubernetes is a great system by itself. Given that it’s easily
extensible across all its components, it’s no wonder companies that had previously
developed their own custom platforms are now re-implementing them on top of
Kubernetes. Kubernetes is, in fact, becoming a widely accepted foundation for the
new generation of Platform-as-a-Service offerings.
 Among the best-known PaaS systems built on Kubernetes are Deis Workflow and
Red Hat’s OpenShift. We’ll do a quick overview of both systems to give you a sense of
what they offer on top of all the awesome stuff Kubernetes already offers.
18.3.1 Red Hat OpenShift Container Platform
Red Hat OpenShift is a Platform-as-a-Service and as such, it has a strong focus on
developer experience. Among its goals are enabling rapid development of applica-
tions, as well as easy deployment, scaling, and long-term maintenance of those apps.
OpenShift has been around much longer than Kubernetes. Versions 1 and 2 were
built from the ground up and had nothing to do with Kubernetes, but when Kuberne-
tes was announced, Red Hat decided to rebuild OpenShift version 3 from scratch—
this time on top of Kubernetes. When a company such as Red Hat decides to throw
away an old version of their software and build a new one on top of an existing tech-
nology like Kubernetes, it should be clear to everyone how great Kubernetes is.
 Kubernetes automates rollouts and application scaling, whereas OpenShift also auto-
mates the actual building of application images and their automatic deployment with-
out requiring you to integrate a Continuous Integration solution into your cluster. 
 OpenShift also provides user and group management, which allows you to run a
properly secured multi-tenant Kubernetes cluster, where individual users are only
allowed to access their own Kubernetes namespaces and the apps running in those
namespaces are also fully network-isolated from each other by default. 
INTRODUCING ADDITIONAL RESOURCES AVAILABLE IN OPENSHIFT
OpenShift provides some additional API objects in addition to all those available in
Kubernetes. We’ll explain them in the next few paragraphs to give you a good over-
view of what OpenShift does and what it provides.
 The additional resources include
Users &amp; Groups
Projects
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="560">
  <data key="d0">Page_560</data>
  <data key="d5">Page_560</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_471">
  <data key="d0">528
CHAPTER 18
Extending Kubernetes
Templates
BuildConfigs
DeploymentConfigs
ImageStreams
Routes
And others
UNDERSTANDING USERS, GROUPS, AND PROJECTS
We’ve said that OpenShift provides a proper multi-tenant environment to its users.
Unlike Kubernetes, which doesn’t have an API object for representing an individual
user of the cluster (but does have ServiceAccounts that represent services running in
it), OpenShift provides powerful user management features, which make it possible to
specify what each user can do and what they cannot. These features pre-date the Role-
Based Access Control, which is now the standard in vanilla Kubernetes.
 Each user has access to certain Projects, which are nothing more than Kubernetes
Namespaces with additional annotations. Users can only act on resources that reside
in the projects the user has access to. Access to the project is granted by a cluster
administrator. 
INTRODUCING APPLICATION TEMPLATES
Kubernetes makes it possible to deploy a set of resources through a single JSON or
YAML manifest. OpenShift takes this a step further by allowing that manifest to be
parameterizable. A parameterizable list in OpenShift is called a Template; it’s a list of
objects whose definitions can include placeholders that get replaced with parameter
values when you process and then instantiate a template (see figure 18.8).
The template itself is a JSON or YAML file containing a list of parameters that are ref-
erenced in resources defined in that same JSON/YAML. The template can be stored
in the API server like any other object. Before a template can be instantiated, it needs
Template
Parameters
APP_NAME="kubia"
VOL_CAPACITY="5 Gi"
...
Pod
name: $(APP_NAME)
Service
name: $(APP_NAME)
Template
Pod
name: kubia
Service
name: kubia
Pod
name: kubia
Service
name: kubia
Process
Create
Figure 18.8
OpenShift templates
 
</data>
  <data key="d5">528
CHAPTER 18
Extending Kubernetes
Templates
BuildConfigs
DeploymentConfigs
ImageStreams
Routes
And others
UNDERSTANDING USERS, GROUPS, AND PROJECTS
We’ve said that OpenShift provides a proper multi-tenant environment to its users.
Unlike Kubernetes, which doesn’t have an API object for representing an individual
user of the cluster (but does have ServiceAccounts that represent services running in
it), OpenShift provides powerful user management features, which make it possible to
specify what each user can do and what they cannot. These features pre-date the Role-
Based Access Control, which is now the standard in vanilla Kubernetes.
 Each user has access to certain Projects, which are nothing more than Kubernetes
Namespaces with additional annotations. Users can only act on resources that reside
in the projects the user has access to. Access to the project is granted by a cluster
administrator. 
INTRODUCING APPLICATION TEMPLATES
Kubernetes makes it possible to deploy a set of resources through a single JSON or
YAML manifest. OpenShift takes this a step further by allowing that manifest to be
parameterizable. A parameterizable list in OpenShift is called a Template; it’s a list of
objects whose definitions can include placeholders that get replaced with parameter
values when you process and then instantiate a template (see figure 18.8).
The template itself is a JSON or YAML file containing a list of parameters that are ref-
erenced in resources defined in that same JSON/YAML. The template can be stored
in the API server like any other object. Before a template can be instantiated, it needs
Template
Parameters
APP_NAME="kubia"
VOL_CAPACITY="5 Gi"
...
Pod
name: $(APP_NAME)
Service
name: $(APP_NAME)
Template
Pod
name: kubia
Service
name: kubia
Pod
name: kubia
Service
name: kubia
Process
Create
Figure 18.8
OpenShift templates
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="561">
  <data key="d0">Page_561</data>
  <data key="d5">Page_561</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_472">
  <data key="d0">529
Platforms built on top of Kubernetes
to be processed. To process a template, you supply the values for the template’s
parameters and then OpenShift replaces the references to the parameters with those
values. The result is a processed template, which is exactly like a Kubernetes resource
list that can then be created with a single POST request.
 OpenShift provides a long list of pre-fabricated templates that allow users to
quickly run complex applications by specifying a few arguments (or none at all, if the
template provides good defaults for those arguments). For example, a template can
enable the creation of all the Kubernetes resources necessary to run a Java EE appli-
cation inside an Application Server, which connects to a back-end database, also
deployed as part of that same template. All those components can be deployed with a
single command.
BUILDING IMAGES FROM SOURCE USING BUILDCONFIGS
One of the best features of OpenShift is the ability to have OpenShift build and imme-
diately deploy an application in the OpenShift cluster by pointing it to a Git repository
holding the application’s source code. You don’t need to build the container image at
all—OpenShift does that for you. This is done by creating a resource called Build-
Config, which can be configured to trigger builds of container images immediately
after a change is committed to the source Git repository. 
 Although OpenShift doesn’t monitor the Git repository itself, a hook in the repos-
itory can notify OpenShift of the new commit. OpenShift will then pull the changes
from the Git repository and start the build process. A build mechanism called Source
To Image can detect what type of application is in the Git repository and run the
proper build procedure for it. For example, if it detects a pom.xml file, which is used
in Java Maven-formatted projects, it runs a Maven build. The resulting artifacts are
packaged into an appropriate container image, and are then pushed to an internal
container registry (provided by OpenShift). From there, they can be pulled and run
in the cluster immediately. 
 By creating a BuildConfig object, developers can thus point to a Git repo and not
worry about building container images. Developers have almost no need to know
anything about containers. Once the ops team deploys an OpenShift cluster and
gives developers access to it, those developers can develop their code, commit, and
push it to a Git repo, the same way they used to before we started packaging apps into
containers. Then OpenShift takes care of building, deploying, and managing apps
from that code.
AUTOMATICALLY DEPLOYING NEWLY BUILT IMAGES WITH DEPLOYMENTCONFIGS
Once a new container image is built, it can also automatically be deployed in the clus-
ter. This is enabled by creating a DeploymentConfig object and pointing it to an
ImageStream. As the name suggests, an ImageStream is a stream of images. When an
image is built, it’s added to the ImageStream. This enables the DeploymentConfig to
notice the newly built image and allows it to take action and initiate a rollout of the
new image (see figure 18.9).
 
</data>
  <data key="d5">529
Platforms built on top of Kubernetes
to be processed. To process a template, you supply the values for the template’s
parameters and then OpenShift replaces the references to the parameters with those
values. The result is a processed template, which is exactly like a Kubernetes resource
list that can then be created with a single POST request.
 OpenShift provides a long list of pre-fabricated templates that allow users to
quickly run complex applications by specifying a few arguments (or none at all, if the
template provides good defaults for those arguments). For example, a template can
enable the creation of all the Kubernetes resources necessary to run a Java EE appli-
cation inside an Application Server, which connects to a back-end database, also
deployed as part of that same template. All those components can be deployed with a
single command.
BUILDING IMAGES FROM SOURCE USING BUILDCONFIGS
One of the best features of OpenShift is the ability to have OpenShift build and imme-
diately deploy an application in the OpenShift cluster by pointing it to a Git repository
holding the application’s source code. You don’t need to build the container image at
all—OpenShift does that for you. This is done by creating a resource called Build-
Config, which can be configured to trigger builds of container images immediately
after a change is committed to the source Git repository. 
 Although OpenShift doesn’t monitor the Git repository itself, a hook in the repos-
itory can notify OpenShift of the new commit. OpenShift will then pull the changes
from the Git repository and start the build process. A build mechanism called Source
To Image can detect what type of application is in the Git repository and run the
proper build procedure for it. For example, if it detects a pom.xml file, which is used
in Java Maven-formatted projects, it runs a Maven build. The resulting artifacts are
packaged into an appropriate container image, and are then pushed to an internal
container registry (provided by OpenShift). From there, they can be pulled and run
in the cluster immediately. 
 By creating a BuildConfig object, developers can thus point to a Git repo and not
worry about building container images. Developers have almost no need to know
anything about containers. Once the ops team deploys an OpenShift cluster and
gives developers access to it, those developers can develop their code, commit, and
push it to a Git repo, the same way they used to before we started packaging apps into
containers. Then OpenShift takes care of building, deploying, and managing apps
from that code.
AUTOMATICALLY DEPLOYING NEWLY BUILT IMAGES WITH DEPLOYMENTCONFIGS
Once a new container image is built, it can also automatically be deployed in the clus-
ter. This is enabled by creating a DeploymentConfig object and pointing it to an
ImageStream. As the name suggests, an ImageStream is a stream of images. When an
image is built, it’s added to the ImageStream. This enables the DeploymentConfig to
notice the newly built image and allows it to take action and initiate a rollout of the
new image (see figure 18.9).
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="562">
  <data key="d0">Page_562</data>
  <data key="d5">Page_562</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_473">
  <data key="d0">530
CHAPTER 18
Extending Kubernetes
A DeploymentConfig is almost identical to the Deployment object in Kubernetes, but
it pre-dates it. Like a Deployment object, it has a configurable strategy for transition-
ing between Deployments. It contains a pod template used to create the actual pods,
but it also allows you to configure pre- and post-deployment hooks. In contrast to a
Kubernetes Deployment, it creates ReplicationControllers instead of ReplicaSets and
provides a few additional features.
EXPOSING SERVICES EXTERNALLY USING ROUTES
Early on, Kubernetes didn’t provide Ingress objects. To expose Services to the outside
world, you needed to use NodePort or LoadBalancer-type Services. But at that time,
OpenShift already provided a better option through a Route resource. A Route is sim-
ilar to an Ingress, but it provides additional configuration related to TLS termination
and traffic splitting. 
 Similar to an Ingress controller, a Route needs a Router, which is a controller that
provides the load balancer or proxy. In contrast to Kubernetes, the Router is available
out of the box in OpenShift. 
TRYING OUT OPENSHIFT
If you’re interested in trying out OpenShift, you can start by using Minishift, which is
the OpenShift equivalent of Minikube, or you can try OpenShift Online Starter at
https:/
/manage.openshift.com, which is a free multi-tenant, hosted solution provided
to get you started with OpenShift. 
18.3.2 Deis Workflow and Helm
A company called Deis, which has recently been acquired by Microsoft, also provides a
PaaS called Workflow, which is also built on top of Kubernetes. Besides Workflow,
Pods
Builder pod
Replication
Controller
BuildConﬁg
Git repo
DeploymentConﬁg
ImageStream
Build trigger
Clones Git repo, builds new
image from source, and adds
it to the ImageStream
Watches for new images in ImageStream
and rolls out new version (similarly to a
Deployment)
Figure 18.9
BuildConfigs and DeploymentConfigs in OpenShift
 
</data>
  <data key="d5">530
CHAPTER 18
Extending Kubernetes
A DeploymentConfig is almost identical to the Deployment object in Kubernetes, but
it pre-dates it. Like a Deployment object, it has a configurable strategy for transition-
ing between Deployments. It contains a pod template used to create the actual pods,
but it also allows you to configure pre- and post-deployment hooks. In contrast to a
Kubernetes Deployment, it creates ReplicationControllers instead of ReplicaSets and
provides a few additional features.
EXPOSING SERVICES EXTERNALLY USING ROUTES
Early on, Kubernetes didn’t provide Ingress objects. To expose Services to the outside
world, you needed to use NodePort or LoadBalancer-type Services. But at that time,
OpenShift already provided a better option through a Route resource. A Route is sim-
ilar to an Ingress, but it provides additional configuration related to TLS termination
and traffic splitting. 
 Similar to an Ingress controller, a Route needs a Router, which is a controller that
provides the load balancer or proxy. In contrast to Kubernetes, the Router is available
out of the box in OpenShift. 
TRYING OUT OPENSHIFT
If you’re interested in trying out OpenShift, you can start by using Minishift, which is
the OpenShift equivalent of Minikube, or you can try OpenShift Online Starter at
https:/
/manage.openshift.com, which is a free multi-tenant, hosted solution provided
to get you started with OpenShift. 
18.3.2 Deis Workflow and Helm
A company called Deis, which has recently been acquired by Microsoft, also provides a
PaaS called Workflow, which is also built on top of Kubernetes. Besides Workflow,
Pods
Builder pod
Replication
Controller
BuildConﬁg
Git repo
DeploymentConﬁg
ImageStream
Build trigger
Clones Git repo, builds new
image from source, and adds
it to the ImageStream
Watches for new images in ImageStream
and rolls out new version (similarly to a
Deployment)
Figure 18.9
BuildConfigs and DeploymentConfigs in OpenShift
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="563">
  <data key="d0">Page_563</data>
  <data key="d5">Page_563</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_474">
  <data key="d0">531
Platforms built on top of Kubernetes
they’ve also developed a tool called Helm, which is gaining traction in the Kubernetes
community as a standard way of deploying existing apps in Kubernetes. We’ll take a
brief look at both.
INTRODUCING DEIS WORKFLOW
You can deploy Deis Workflow to any existing Kubernetes cluster (unlike OpenShift,
which is a complete cluster with a modified API server and other Kubernetes compo-
nents). When you run Workflow, it creates a set of Services and ReplicationControllers,
which then provide developers with a simple, developer-friendly environment. 
 Deploying new versions of your app is triggered by pushing your changes with git
push deis master and letting Workflow take care of the rest. Similar to OpenShift,
Workflow also provides a source to image mechanism, application rollouts and roll-
backs, edge routing, and also log aggregation, metrics, and alerting, which aren’t
available in core Kubernetes. 
 To run Workflow in your Kubernetes cluster, you first need to install the Deis Work-
flow and Helm CLI tools and then install Workflow into your cluster. We won’t go into
how to do that here, but if you’d like to learn more, visit the website at https:/
/deis
.com/workflow. What we’ll explore here is the Helm tool, which can be used without
Workflow and has gained popularity in the community.
DEPLOYING RESOURCES THROUGH HELM
Helm is a package manager for Kubernetes (similar to OS package managers like yum
or apt in Linux or homebrew in MacOS). 
 Helm is comprised of two things:
A helm CLI tool (the client).
Tiller, a server component running as a Pod inside the Kubernetes cluster.
Those two components are used to deploy and manage application packages in a
Kubernetes cluster. Helm application packages are called Charts. They’re combined
with a Config, which contains configuration information and is merged into a Chart
to create a Release, which is a running instance of an application (a combined Chart
and Config). You deploy and manage Releases using the helm CLI tool, which talks to
the Tiller server, which is the component that creates all the necessary Kubernetes
resources defined in the Chart, as shown in figure 18.10.
 You can create charts yourself and keep them on your local disk, or you can use
any existing chart, which is available in the growing list of helm charts maintained by
the community at https:/
/github.com/kubernetes/charts. The list includes charts for
applications such as PostgreSQL, MySQL, MariaDB, Magento, Memcached, MongoDB,
OpenVPN, PHPBB, RabbitMQ, Redis, WordPress, and others.
 Similar to how you don’t build and install apps developed by other people to your
Linux system manually, you probably don’t want to build and manage your own
Kubernetes manifests for such applications, right? That’s why you’ll want to use Helm
and the charts available in the GitHub repository I mentioned. 
 
</data>
  <data key="d5">531
Platforms built on top of Kubernetes
they’ve also developed a tool called Helm, which is gaining traction in the Kubernetes
community as a standard way of deploying existing apps in Kubernetes. We’ll take a
brief look at both.
INTRODUCING DEIS WORKFLOW
You can deploy Deis Workflow to any existing Kubernetes cluster (unlike OpenShift,
which is a complete cluster with a modified API server and other Kubernetes compo-
nents). When you run Workflow, it creates a set of Services and ReplicationControllers,
which then provide developers with a simple, developer-friendly environment. 
 Deploying new versions of your app is triggered by pushing your changes with git
push deis master and letting Workflow take care of the rest. Similar to OpenShift,
Workflow also provides a source to image mechanism, application rollouts and roll-
backs, edge routing, and also log aggregation, metrics, and alerting, which aren’t
available in core Kubernetes. 
 To run Workflow in your Kubernetes cluster, you first need to install the Deis Work-
flow and Helm CLI tools and then install Workflow into your cluster. We won’t go into
how to do that here, but if you’d like to learn more, visit the website at https:/
/deis
.com/workflow. What we’ll explore here is the Helm tool, which can be used without
Workflow and has gained popularity in the community.
DEPLOYING RESOURCES THROUGH HELM
Helm is a package manager for Kubernetes (similar to OS package managers like yum
or apt in Linux or homebrew in MacOS). 
 Helm is comprised of two things:
A helm CLI tool (the client).
Tiller, a server component running as a Pod inside the Kubernetes cluster.
Those two components are used to deploy and manage application packages in a
Kubernetes cluster. Helm application packages are called Charts. They’re combined
with a Config, which contains configuration information and is merged into a Chart
to create a Release, which is a running instance of an application (a combined Chart
and Config). You deploy and manage Releases using the helm CLI tool, which talks to
the Tiller server, which is the component that creates all the necessary Kubernetes
resources defined in the Chart, as shown in figure 18.10.
 You can create charts yourself and keep them on your local disk, or you can use
any existing chart, which is available in the growing list of helm charts maintained by
the community at https:/
/github.com/kubernetes/charts. The list includes charts for
applications such as PostgreSQL, MySQL, MariaDB, Magento, Memcached, MongoDB,
OpenVPN, PHPBB, RabbitMQ, Redis, WordPress, and others.
 Similar to how you don’t build and install apps developed by other people to your
Linux system manually, you probably don’t want to build and manage your own
Kubernetes manifests for such applications, right? That’s why you’ll want to use Helm
and the charts available in the GitHub repository I mentioned. 
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="564">
  <data key="d0">Page_564</data>
  <data key="d5">Page_564</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_475">
  <data key="d0">532
CHAPTER 18
Extending Kubernetes
When you want to run a PostgreSQL or a MySQL database in your Kubernetes cluster,
don’t start writing manifests for them. Instead, check if someone else has already gone
through the trouble and prepared a Helm chart for it. 
 Once someone prepares a Helm chart for a specific application and adds it to the
Helm chart GitHub repo, installing the whole application takes a single one-line com-
mand. For example, to run MySQL in your Kubernetes cluster, all you need to do is
clone the charts Git repo to your local machine and run the following command (pro-
vided you have Helm’s CLI tool and Tiller running in your cluster):
$ helm install --name my-database stable/mysql
This will create all the necessary Deployments, Services, Secrets, and PersistentVolu-
meClaims needed to run MySQL in your cluster. You don’t need to concern yourself
with what components you need and how to configure them to run MySQL properly.
I’m sure you’ll agree this is awesome.
TIP
One of the most interesting charts available in the repo is an OpenVPN
chart, which runs an OpenVPN server inside your Kubernetes cluster and
allows you to enter the pod network through VPN and access Services as if
your local machine was a pod in the cluster. This is useful when you’re devel-
oping apps and running them locally.
These were several examples of how Kubernetes can be extended and how companies
like Red Hat and Deis (now Microsoft) have extended it. Now go and start riding the
Kubernetes wave yourself!
Kubernetes cluster
Chart
and
Conﬁg
Helm
Charts
(ﬁles on
local disk)
Tiller
(pod)
Deployments,
Services, and
other objects
helm
CLI tool
Manages
charts
Combines Chart and
Conﬁg into a Release
Creates Kubernetes objects
deﬁned in the Release
Figure 18.10
Overview of Helm
 
</data>
  <data key="d5">532
CHAPTER 18
Extending Kubernetes
When you want to run a PostgreSQL or a MySQL database in your Kubernetes cluster,
don’t start writing manifests for them. Instead, check if someone else has already gone
through the trouble and prepared a Helm chart for it. 
 Once someone prepares a Helm chart for a specific application and adds it to the
Helm chart GitHub repo, installing the whole application takes a single one-line com-
mand. For example, to run MySQL in your Kubernetes cluster, all you need to do is
clone the charts Git repo to your local machine and run the following command (pro-
vided you have Helm’s CLI tool and Tiller running in your cluster):
$ helm install --name my-database stable/mysql
This will create all the necessary Deployments, Services, Secrets, and PersistentVolu-
meClaims needed to run MySQL in your cluster. You don’t need to concern yourself
with what components you need and how to configure them to run MySQL properly.
I’m sure you’ll agree this is awesome.
TIP
One of the most interesting charts available in the repo is an OpenVPN
chart, which runs an OpenVPN server inside your Kubernetes cluster and
allows you to enter the pod network through VPN and access Services as if
your local machine was a pod in the cluster. This is useful when you’re devel-
oping apps and running them locally.
These were several examples of how Kubernetes can be extended and how companies
like Red Hat and Deis (now Microsoft) have extended it. Now go and start riding the
Kubernetes wave yourself!
Kubernetes cluster
Chart
and
Conﬁg
Helm
Charts
(ﬁles on
local disk)
Tiller
(pod)
Deployments,
Services, and
other objects
helm
CLI tool
Manages
charts
Combines Chart and
Conﬁg into a Release
Creates Kubernetes objects
deﬁned in the Release
Figure 18.10
Overview of Helm
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="565">
  <data key="d0">Page_565</data>
  <data key="d5">Page_565</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_476">
  <data key="d0">533
Summary
18.4
Summary
This final chapter has shown you how you can go beyond the existing functionalities
Kubernetes provides and how companies like Dies and Red Hat have done it. You’ve
learned how
Custom resources can be registered in the API server by creating a Custom-
ResourceDefinition object.
Instances of custom objects can be stored, retrieved, updated, and deleted with-
out having to change the API server code.
A custom controller can be implemented to bring those objects to life.
Kubernetes can be extended with custom API servers through API aggregation.
Kubernetes Service Catalog makes it possible to self-provision external services
and expose them to pods running in the Kubernetes cluster.
Platforms-as-a-Service built on top of Kubernetes make it easy to build contain-
erized applications inside the same Kubernetes cluster that then runs them. 
A package manager called Helm makes deploying existing apps without requir-
ing you to build resource manifests for them.
Thank you for taking the time to read through this long book. I hope you’ve learned
as much from reading it as I have from writing it.
 
</data>
  <data key="d5">533
Summary
18.4
Summary
This final chapter has shown you how you can go beyond the existing functionalities
Kubernetes provides and how companies like Dies and Red Hat have done it. You’ve
learned how
Custom resources can be registered in the API server by creating a Custom-
ResourceDefinition object.
Instances of custom objects can be stored, retrieved, updated, and deleted with-
out having to change the API server code.
A custom controller can be implemented to bring those objects to life.
Kubernetes can be extended with custom API servers through API aggregation.
Kubernetes Service Catalog makes it possible to self-provision external services
and expose them to pods running in the Kubernetes cluster.
Platforms-as-a-Service built on top of Kubernetes make it easy to build contain-
erized applications inside the same Kubernetes cluster that then runs them. 
A package manager called Helm makes deploying existing apps without requir-
ing you to build resource manifests for them.
Thank you for taking the time to read through this long book. I hope you’ve learned
as much from reading it as I have from writing it.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="566">
  <data key="d0">Page_566</data>
  <data key="d5">Page_566</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_477">
  <data key="d0">534
appendix A
Using kubectl
with multiple clusters
A.1
Switching between Minikube and Google Kubernetes 
Engine
The examples in this book can either be run in a cluster created with Minikube, or
one created with Google Kubernetes Engine (GKE). If you plan on using both, you
need to know how to switch between them. A detailed explanation of how to use
kubectl with multiple clusters is described in the next section. Here we look at how
to switch between Minikube and GKE.
SWITCHING TO MINIKUBE
Luckily, every time you start up a Minikube cluster with minikube start, it also
reconfigures kubectl to use it:
$ minikube start
Starting local Kubernetes cluster...
...
Setting up kubeconfig...                            
Kubectl is now configured to use the cluster.       
After switching from Minikube to GKE, you can switch back by stopping Minikube
and starting it up again. kubectl will then be re-configured to use the Minikube clus-
ter again.
SWITCHING TO GKE
To switch to using the GKE cluster, you can use the following command:
$ gcloud container clusters get-credentials my-gke-cluster
This will configure kubectl to use the GKE cluster called my-gke-cluster.
Minikube sets up kubectl every 
time you start the cluster.
 
</data>
  <data key="d5">534
appendix A
Using kubectl
with multiple clusters
A.1
Switching between Minikube and Google Kubernetes 
Engine
The examples in this book can either be run in a cluster created with Minikube, or
one created with Google Kubernetes Engine (GKE). If you plan on using both, you
need to know how to switch between them. A detailed explanation of how to use
kubectl with multiple clusters is described in the next section. Here we look at how
to switch between Minikube and GKE.
SWITCHING TO MINIKUBE
Luckily, every time you start up a Minikube cluster with minikube start, it also
reconfigures kubectl to use it:
$ minikube start
Starting local Kubernetes cluster...
...
Setting up kubeconfig...                            
Kubectl is now configured to use the cluster.       
After switching from Minikube to GKE, you can switch back by stopping Minikube
and starting it up again. kubectl will then be re-configured to use the Minikube clus-
ter again.
SWITCHING TO GKE
To switch to using the GKE cluster, you can use the following command:
$ gcloud container clusters get-credentials my-gke-cluster
This will configure kubectl to use the GKE cluster called my-gke-cluster.
Minikube sets up kubectl every 
time you start the cluster.
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="567">
  <data key="d0">Page_567</data>
  <data key="d5">Page_567</data>
  <data key="d1">blue</data>
  <data key="d4">10</data>
</node>
<node id="text_478">
  <data key="d0">535
Using kubectl with multiple clusters or namespaces
GOING FURTHER
These two methods should be enough to get you started quickly, but to understand
the complete picture of using kubectl with multiple clusters, study the next section. 
A.2
Using kubectl with multiple clusters or namespaces
If you need to switch between different Kubernetes clusters, or if you want to work in a
different namespace than the default and don’t want to specify the --namespace
option every time you run kubectl, here’s how to do it.
A.2.1
Configuring the location of the kubeconfig file
The config used by kubectl is usually stored in the ~/.kube/config file. If it’s stored
somewhere else, the KUBECONFIG environment variable needs to point to its location. 
NOTE
You can use multiple config files and have kubectl use them all at
once by specifying all of them in the KUBECONFIG environment variable (sepa-
rate them with a colon).
A.2.2
Understanding the contents of the kubeconfig file
An example config file is shown in the following listing.
apiVersion: v1
clusters:
- cluster:                                                 
    certificate-authority: /home/luksa/.minikube/ca.crt    
    server: https://192.168.99.100:8443                    
  name: minikube                                           
contexts:
- context:                          
    cluster: minikube               
    user: minikube                  
    namespace: default              
  name: minikube                    
current-context: minikube             
kind: Config
preferences: {}
users:
- name: minikube                                             
  user:                                                      
    client-certificate: /home/luksa/.minikube/apiserver.crt  
    client-key: /home/luksa/.minikube/apiserver.key          
The kubeconfig file consists of four sections:
■
A list of clusters
■
A list of users
■
A list of contexts
■
The name of the current context
Listing A.1
Example kubeconfig file
Contains 
information about a 
Kubernetes cluster
Defines a 
kubectl 
context
The current context 
kubectl uses
Contains 
a user’s 
credentials
 
</data>
  <data key="d5">535
Using kubectl with multiple clusters or namespaces
GOING FURTHER
These two methods should be enough to get you started quickly, but to understand
the complete picture of using kubectl with multiple clusters, study the next section. 
A.2
Using kubectl with multiple clusters or namespaces
If you need to switch between different Kubernetes clusters, or if you want to work in a
different namespace than the default and don’t want to specify the --namespace
option every time you run kubectl, here’s how to do it.
A.2.1
Configuring the location of the kubeconfig file
The config used by kubectl is usually stored in the ~/.kube/config file. If it’s stored
somewhere else, the KUBECONFIG environment variable needs to point to its location. 
NOTE
You can use multiple config files and have kubectl use them all at
once by specifying all of them in the KUBECONFIG environment variable (sepa-
rate them with a colon).
A.2.2
Understanding the contents of the kubeconfig file
An example config file is shown in the following listing.
apiVersion: v1
clusters:
- cluster:                                                 
    certificate-authority: /home/luksa/.minikube/ca.crt    
    server: https://192.168.99.100:8443                    
  name: minikube                                           
contexts:
- context:                          
    cluster: minikube               
    user: minikube                  
    namespace: default              
  name: minikube                    
current-context: minikube             
kind: Config
preferences: {}
users:
- name: minikube                                             
  user:                                                      
    client-certificate: /home/luksa/.minikube/apiserver.crt  
    client-key: /home/luksa/.minikube/apiserver.key          
The kubeconfig file consists of four sections:
■
A list of clusters
■
A list of users
■
A list of contexts
■
The name of the current context
Listing A.1
Example kubeconfig file
Contains 
information about a 
Kubernetes cluster
Defines a 
kubectl 
context
The current context 
kubectl uses
Contains 
a user’s 
credentials
 
</data>
  <data key="d1">green</data>
  <data key="d4">10</data>
</node>
<node id="UTS namespace">
  <data key="d5">UTS namespace</data>
  <data key="d2">Shared hostname and network interfaces among containers in a pod</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Port space">
  <data key="d5">Port space</data>
  <data key="d2">Shared port numbers among containers in a pod</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Flat inter-pod network">
  <data key="d5">Flat inter-pod network</data>
  <data key="d2">single flat, shared network-address space for all pods</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NAT (Network Address Translation)">
  <data key="d5">NAT (Network Address Translation)</data>
  <data key="d2">gateway between pods that translates IP addresses</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Worker nodes">
  <data key="d5">Worker nodes</data>
  <data key="d2">physical or virtual machines that run pods</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="LAN (Local Area Network)">
  <data key="d5">LAN (Local Area Network)</data>
  <data key="d2">network for computers on the same physical location</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Software-defined network">
  <data key="d5">Software-defined network</data>
  <data key="d2">additional network layered on top of actual network</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="VM (Virtual Machine)">
  <data key="d5">VM (Virtual Machine)</data>
  <data key="d2">virtualized environment for running operating systems</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="App">
  <data key="d5">App</data>
  <data key="d2">application or service that runs in a pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container 1">
  <data key="d5">Container 1</data>
  <data key="d2">instance of a container running an application</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container 2">
  <data key="d5">Container 2</data>
  <data key="d2">instance of a container running an application</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Log rotator">
  <data key="d5">Log rotator</data>
  <data key="d2">a type of sidecar container that rotates and manages logs for a container</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Collector">
  <data key="d5">Collector</data>
  <data key="d2">a type of sidecar container that collects data from a container or other sources</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Data processor">
  <data key="d5">Data processor</data>
  <data key="d2">a type of sidecar container that processes and transforms data for a container</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Communication adapter">
  <data key="d5">Communication adapter</data>
  <data key="d2">a type of sidecar container that enables communication between containers or other systems</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Frontend process">
  <data key="d5">Frontend process</data>
  <data key="d2">the main process running in the frontend pod, responsible for serving files and handling user requests</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Backend process">
  <data key="d5">Backend process</data>
  <data key="d2">the main process running in the backend pod, responsible for processing data and handling requests</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Frontend container">
  <data key="d5">Frontend container</data>
  <data key="d2">the container running the frontend process in a pod</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Backend container">
  <data key="d5">Backend container</data>
  <data key="d2">the container running the backend process in a pod</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="REST API">
  <data key="d5">REST API</data>
  <data key="d2">Web service for interacting with Kubernetes resources</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API object definitions">
  <data key="d5">API object definitions</data>
  <data key="d2">Definitions of Kubernetes API objects, such as Pods, Services, etc.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes API reference documentation">
  <data key="d5">Kubernetes API reference documentation</data>
  <data key="d2">Official documentation for the Kubernetes API</data>
  <data key="d3">documentation</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod metadata">
  <data key="d5">Pod metadata</data>
  <data key="d2">Information about a Pod, including its name, labels, annotations, etc.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod specification">
  <data key="d5">Pod specification</data>
  <data key="d2">Contents of a Pod, including its containers, volumes, and so on</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dnsPolicy">
  <data key="d5">dnsPolicy</data>
  <data key="d2">ClusterFirst</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nodeName">
  <data key="d5">nodeName</data>
  <data key="d2">gke-kubia-e8fe08b8-node-txje</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="serviceAccount">
  <data key="d5">serviceAccount</data>
  <data key="d2">default</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="terminationGracePeriodSeconds">
  <data key="d5">terminationGracePeriodSeconds</data>
  <data key="d2">30</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="containerID">
  <data key="d5">containerID</data>
  <data key="d2">docker://f0276994322d247ba...</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="lastState">
  <data key="d5">lastState</data>
  <data key="d2">{}</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ready">
  <data key="d5">ready</data>
  <data key="d2">true</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="restartCount">
  <data key="d5">restartCount</data>
  <data key="d2">0</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="state">
  <data key="d5">state</data>
  <data key="d2">running: startedAt: 2016-03-18T12:46:05Z</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="hostIP">
  <data key="d5">hostIP</data>
  <data key="d2">10.132.0.4</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="phase">
  <data key="d5">phase</data>
  <data key="d2">Running</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="podIP">
  <data key="d5">podIP</data>
  <data key="d2">10.0.2.3</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="startTime">
  <data key="d5">startTime</data>
  <data key="d2">2016-03-18T12:44:32Z</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes API version">
  <data key="d5">Kubernetes API version</data>
  <data key="d2">used in the YAML and the type of resource the YAML is describing</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Spec">
  <data key="d5">Spec</data>
  <data key="d2">contains the actual description of the pod’s contents</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod specification/contents">
  <data key="d5">Pod specification/contents</data>
  <data key="d2">list of pod’s containers, volumes, and so on</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Detailed status of the pod and its containers">
  <data key="d5">Detailed status of the pod and its containers</data>
  <data key="d2">status: conditions: lastProbeTime: null lastTransitionTime: null status: True type: Ready</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="protocol">
  <data key="d5">protocol</data>
  <data key="d2">The protocol being used for communication over a port, such as TCP or UDP.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="status">
  <data key="d5">status</data>
  <data key="d2">The current state of a pod, including its running containers and any errors.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="po kubia-manual">
  <data key="d5">po kubia-manual</data>
  <data key="d2">Pod object used to retrieve its YAML definition</data>
  <data key="d3">object</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-o yaml">
  <data key="d5">-o yaml</data>
  <data key="d2">Option to specify output format as YAML</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-o json">
  <data key="d5">-o json</data>
  <data key="d2">Option to specify output format as JSON</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="process's standard output">
  <data key="d5">process's standard output</data>
  <data key="d2">Output stream where the application logs are written</data>
  <data key="d3">output stream</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="standard error stream">
  <data key="d5">standard error stream</data>
  <data key="d2">Error output stream where the application logs are written</data>
  <data key="d3">output stream</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ docker logs &lt;container id&gt;">
  <data key="d5">$ docker logs &lt;container id&gt;</data>
  <data key="d2">command to get container's log</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl logs kubia-manual">
  <data key="d5">kubectl logs kubia-manual</data>
  <data key="d2">command to see pod's log (more precisely, container's log)</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubia server starting...">
  <data key="d5">Kubia server starting...</data>
  <data key="d2">log statement about the server starting up</data>
  <data key="d3">log message</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-c &lt;container name&gt;">
  <data key="d5">-c &lt;container name&gt;</data>
  <data key="d2">option to specify container name when getting logs of a multi-container pod</data>
  <data key="d3">flag</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ kubectl logs kubia-manual -c kubia">
  <data key="d5">$ kubectl logs kubia-manual -c kubia</data>
  <data key="d2">command to get logs of a specific container in a multi-container pod</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="container name">
  <data key="d5">container name</data>
  <data key="d2">name of the container for which logs are being retrieved</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="port forwarding">
  <data key="d5">port forwarding</data>
  <data key="d2">method to connect to a pod for testing and debugging purposes</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="localhost:8888">
  <data key="d5">localhost:8888</data>
  <data key="d2">The local machine's port being used to connect to the pod.</data>
  <data key="d3">port</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="8080">
  <data key="d5">8080</data>
  <data key="d2">The port number on the pod being accessed.</data>
  <data key="d3">port</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Resource">
  <data key="d5">Resource</data>
  <data key="d2">any object in Kubernetes that can be labeled</data>
  <data key="d3">object</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Microservice">
  <data key="d5">Microservice</data>
  <data key="d2">a small, independent service that can be deployed independently</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="UI Pod">
  <data key="d5">UI Pod</data>
  <data key="d2">a pod running the user interface for an application</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="canary release">
  <data key="d5">canary release</data>
  <data key="d2">A deployment strategy where a new version of an application is deployed next to the stable version</data>
  <data key="d3">deployment strategy</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="stable release">
  <data key="d5">stable release</data>
  <data key="d2">A deployment strategy where a new version of an application is deployed as the primary version</data>
  <data key="d3">deployment strategy</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="beta release">
  <data key="d5">beta release</data>
  <data key="d2">A deployment strategy where a new version of an application is deployed for testing and feedback</data>
  <data key="d3">deployment strategy</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-manual-with-labels.yaml">
  <data key="d5">kubia-manual-with-labels.yaml</data>
  <data key="d2">A YAML file that specifies labels when creating a pod</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ui pod">
  <data key="d5">ui pod</data>
  <data key="d2">A pod with a label specifying it as a UI component</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="account service">
  <data key="d5">account service</data>
  <data key="d2">A pod with a label specifying it as an account service component</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="product catalog">
  <data key="d5">product catalog</data>
  <data key="d2">A pod with a label specifying it as a product catalog component</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="shopping cart">
  <data key="d5">shopping cart</data>
  <data key="d2">A pod with a label specifying it as a shopping cart component</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="order service">
  <data key="d5">order service</data>
  <data key="d2">A pod with a label specifying it as an order service component</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--show-labels">
  <data key="d5">--show-labels</data>
  <data key="d2">switch to show labels in kubectl get pods output</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-L">
  <data key="d5">-L</data>
  <data key="d2">switch to specify labels to display in kubectl get pods output</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-manual-v2">
  <data key="d5">kubia-manual-v2</data>
  <data key="d2">pod name</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="manual">
  <data key="d5">manual</data>
  <data key="d2">Value of the creation_method label for manually-created pods</data>
  <data key="d3">label value</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="none">
  <data key="d5">none</data>
  <data key="d2">Value of the creation_method and env labels for certain pods</data>
  <data key="d3">label value</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="!env">
  <data key="d5">!env</data>
  <data key="d2">Bash shell variable</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="creation_method!=manual">
  <data key="d5">creation_method!=manual</data>
  <data key="d2">Label selector for pods created manually</data>
  <data key="d3">label_selector</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="env in (prod,devel)">
  <data key="d5">env in (prod,devel)</data>
  <data key="d2">Label selector for pods with env label set to prod or devel</data>
  <data key="d3">label_selector</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="env notin (prod,devel)">
  <data key="d5">env notin (prod,devel)</data>
  <data key="d2">Label selector for pods with env label not set to prod or devel</data>
  <data key="d3">label_selector</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="app=pc">
  <data key="d5">app=pc</data>
  <data key="d2">Label selector for product catalog microservice pods</data>
  <data key="d3">label_selector</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="rel=beta">
  <data key="d5">rel=beta</data>
  <data key="d2">Label selector for beta release of product catalog microservice</data>
  <data key="d3">label_selector</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="app=ui">
  <data key="d5">app=ui</data>
  <data key="d2">Label selector for UI pod</data>
  <data key="d3">label_selector</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="rel=stable">
  <data key="d5">rel=stable</data>
  <data key="d2">Label selector for stable release of various services</data>
  <data key="d3">label_selector</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="app=sc">
  <data key="d5">app=sc</data>
  <data key="d2">Label selector for Shopping Cart pod</data>
  <data key="d3">label_selector</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="rel=canary">
  <data key="d5">rel=canary</data>
  <data key="d2">Label selector for canary release of various services</data>
  <data key="d3">label_selector</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="selectors">
  <data key="d5">selectors</data>
  <data key="d2">used to constrain pod scheduling based on node labels</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SSDs">
  <data key="d5">SSDs</data>
  <data key="d2">solid-state drives used in some worker nodes</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="spinning hard drives">
  <data key="d5">spinning hard drives</data>
  <data key="d2">a type of storage device used in some worker nodes</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GPU acceleration">
  <data key="d5">GPU acceleration</data>
  <data key="d2">the use of graphics processing units to accelerate computation</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node requirements">
  <data key="d5">node requirements</data>
  <data key="d2">a description of the resources required by a pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="app: ui">
  <data key="d5">app: ui</data>
  <data key="d2">a label used to identify the UI pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="app: as">
  <data key="d5">app: as</data>
  <data key="d2">a label used to identify the Account Service pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GPU">
  <data key="d5">GPU</data>
  <data key="d2">general-purpose GPU computing</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gpu">
  <data key="d5">gpu</data>
  <data key="d2">label that indicates a node has a GPU</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubernetes.io/hostname">
  <data key="d5">kubernetes.io/hostname</data>
  <data key="d2">a unique label on each node with its hostname</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="chapter 16">
  <data key="d5">chapter 16</data>
  <data key="d2">a reference to additional information on scheduling pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubernetes.io/created-by">
  <data key="d5">kubernetes.io/created-by</data>
  <data key="d2">annotation key used to store information about the creator of a pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl annotate">
  <data key="d5">kubectl annotate</data>
  <data key="d2">command for adding or modifying annotations on existing pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="mycompany.com/someannotation">
  <data key="d5">mycompany.com/someannotation</data>
  <data key="d2">example annotation key used to store additional data about a pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="names">
  <data key="d5">names</data>
  <data key="d2">only need to be unique within a namespace</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-public namespace">
  <data key="d5">kube-public namespace</data>
  <data key="d2">a namespace used by Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fluentd-cloud-kubia-e8fe-node-txje pod">
  <data key="d5">fluentd-cloud-kubia-e8fe-node-txje pod</data>
  <data key="d2">a pod in the kube-system namespace</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="heapster-v11-fz1ge pod">
  <data key="d5">heapster-v11-fz1ge pod</data>
  <data key="d2">a pod in the kube-system namespace</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-dns-v9-p8a4t pod">
  <data key="d5">kube-dns-v9-p8a4t pod</data>
  <data key="d2">a pod in the kube-system namespace</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-ui-v4-kdlai pod">
  <data key="d5">kube-ui-v4-kdlai pod</data>
  <data key="d2">a pod in the kube-system namespace</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="l7-lb-controller-v0.5.2-bue96 pod">
  <data key="d5">l7-lb-controller-v0.5.2-bue96 pod</data>
  <data key="d2">a pod in the kube-system namespace</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="metadata section">
  <data key="d5">metadata section</data>
  <data key="d2">A section in a YAML manifest that specifies the metadata for a resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--namespace flag">
  <data key="d5">--namespace flag</data>
  <data key="d2">A flag used with the `kubectl` command to specify a namespace for an action</data>
  <data key="d3">flag</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl config commands">
  <data key="d5">kubectl config commands</data>
  <data key="d2">Commands used to manage Kubernetes contexts and namespaces</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="alias kcd">
  <data key="d5">alias kcd</data>
  <data key="d2">An alias created to quickly switch between namespaces using the `kubectl` command</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="networking solution">
  <data key="d5">networking solution</data>
  <data key="d2">A solution that provides network isolation between namespaces in Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="inter-namespace network isolation">
  <data key="d5">inter-namespace network isolation</data>
  <data key="d2">The ability of a networking solution to isolate pods across different namespaces</data>
  <data key="d3">feature</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="creation_method=manual label">
  <data key="d5">creation_method=manual label</data>
  <data key="d2">A label attached to pods created manually</data>
  <data key="d3">label</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="rel=canary label">
  <data key="d5">rel=canary label</data>
  <data key="d2">A label attached to canary pods</data>
  <data key="d3">label</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ns">
  <data key="d5">ns</data>
  <data key="d2">namespace in Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ui">
  <data key="d5">ui</data>
  <data key="d2">example application name</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="stable">
  <data key="d5">stable</data>
  <data key="d2">example release version</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="as">
  <data key="d5">as</data>
  <data key="d2">example application name</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pc">
  <data key="d5">pc</data>
  <data key="d2">example application name</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="canary">
  <data key="d5">canary</data>
  <data key="d2">release version label selector</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-09as0">
  <data key="d5">kubia-09as0</data>
  <data key="d2">pod name</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-something">
  <data key="d5">kubia-something</data>
  <data key="d2">pod name</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-http Service">
  <data key="d5">kubia-http Service</data>
  <data key="d2">service that exposes the kubia application to the network</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Label Selectors">
  <data key="d5">Label Selectors</data>
  <data key="d2">should be used to organize pods and easily perform operations on multiple pods at once.</data>
  <data key="d3">metadata</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Selectors">
  <data key="d5">Selectors</data>
  <data key="d2">can be used to schedule pods only to nodes that have certain features.</data>
  <data key="d3">node</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Annotations">
  <data key="d5">Annotations</data>
  <data key="d2">allow attaching larger blobs of data to pods either by people or tools and libraries.</data>
  <data key="d3">metadata</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl explain command">
  <data key="d5">kubectl explain command</data>
  <data key="d2">can be used to quickly look up the information on any Kubernetes resource.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cluster node">
  <data key="d5">cluster node</data>
  <data key="d2">physical or virtual machine running Kubernetes</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Replication and other controllers">
  <data key="d5">Replication and other controllers</data>
  <data key="d2">managed resources that create and manage pods</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="horizontal scaling">
  <data key="d5">horizontal scaling</data>
  <data key="d2">process of increasing the number of pods to meet demand</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system-level pods">
  <data key="d5">system-level pods</data>
  <data key="d2">pods that run on each cluster node and provide system services</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="batch jobs">
  <data key="d5">batch jobs</data>
  <data key="d2">short-lived, non-interactive processes that perform a specific task</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="scheduling jobs">
  <data key="d5">scheduling jobs</data>
  <data key="d2">process of running batch jobs periodically or once in the future</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="OutOfMemoryErrors">
  <data key="d5">OutOfMemoryErrors</data>
  <data key="d2">Error thrown when a process consumes excessive memory</data>
  <data key="d3">error</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Java app">
  <data key="d5">Java app</data>
  <data key="d2">Example application with a memory leak</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TCP Socket">
  <data key="d5">TCP Socket</data>
  <data key="d2">A type of probe that tries to open a TCP connection to the specified port of the container.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HTTP GET liveness probe">
  <data key="d5">HTTP GET liveness probe</data>
  <data key="d2">A type of probe that performs an HTTP GET request to check whether a web server is serving requests.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="livenessProbe">
  <data key="d5">livenessProbe</data>
  <data key="d2">A field in the pod specification that defines a liveness probe for the container.</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="httpGet liveness probe">
  <data key="d5">httpGet liveness probe</data>
  <data key="d2">a mechanism to periodically perform HTTP GET requests on a path to determine if a container is still healthy</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="RESTARTS column">
  <data key="d5">RESTARTS column</data>
  <data key="d2">a column in the output of kubectl get that shows how many times a pod's container has been restarted</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-liveness">
  <data key="d5">kubia-liveness</data>
  <data key="d2">the name of a pod in the example</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--previous option">
  <data key="d5">--previous option</data>
  <data key="d2">an option for kubectl logs that allows retrieving the log output from a previous instance of a container</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="myped">
  <data key="d5">myped</data>
  <data key="d2">the name of a pod in the example</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="http-get">
  <data key="d5">http-get</data>
  <data key="d2">HTTP request method used in liveness probe</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="delay=0s">
  <data key="d5">delay=0s</data>
  <data key="d2">Initial delay before starting the liveness probe</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="timeout=1s">
  <data key="d5">timeout=1s</data>
  <data key="d2">Timeout period for the liveness probe</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="period=10s">
  <data key="d5">period=10s</data>
  <data key="d2">Period between consecutive liveness probes</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="#success=1">
  <data key="d5">#success=1</data>
  <data key="d2">Number of successful liveness probe attempts</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="#failure=3">
  <data key="d5">#failure=3</data>
  <data key="d2">Number of failed liveness probe attempts before restart</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="initialDelaySeconds">
  <data key="d5">initialDelaySeconds</data>
  <data key="d2">delay before executing a probe</data>
  <data key="d3">process/thread</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="probe">
  <data key="d5">probe</data>
  <data key="d2">mechanism to check the health of a container</data>
  <data key="d3">application/process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="exit code">
  <data key="d5">exit code</data>
  <data key="d2">numeric value indicating the reason for a process termination</data>
  <data key="d3">process/thread</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="health check">
  <data key="d5">health check</data>
  <data key="d2">mechanism to verify the internal status of an application</data>
  <data key="d3">application/process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/health">
  <data key="d5">/health</data>
  <data key="d2">HTTP endpoint for health checks</data>
  <data key="d3">hardware/network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Exec Probe">
  <data key="d5">Exec Probe</data>
  <data key="d2">A type of liveness probe that executes a command to check if a container is running correctly</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HTTP GET Liveness Probe">
  <data key="d5">HTTP GET Liveness Probe</data>
  <data key="d2">A type of liveness probe that uses an HTTP GET request to check if a container is running correctly</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="RC">
  <data key="d5">RC</data>
  <data key="d2">Abbreviation for ReplicationController.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod B2">
  <data key="d5">pod B2</data>
  <data key="d2">A new pod instance created by the ReplicationController to replace pod B.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Replica count">
  <data key="d5">Replica count</data>
  <data key="d2">The desired number of pods that should be running at any given time.</data>
  <data key="d3">count</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod selector">
  <data key="d5">Pod selector</data>
  <data key="d2">A way to select pods based on labels attached to them.</data>
  <data key="d3">selector</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="JSON or YAML descriptor">
  <data key="d5">JSON or YAML descriptor</data>
  <data key="d2">A file format used to define and describe Kubernetes resources, such as ReplicationControllers.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ kubectl delete pod">
  <data key="d5">$ kubectl delete pod</data>
  <data key="d2">The command used to manually delete a pod from the cluster.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicationController spins up a new pod">
  <data key="d5">ReplicationController spins up a new pod</data>
  <data key="d2">The process by which a ReplicationController creates a new pod when one is deleted or fails.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ kubectl get rc">
  <data key="d5">$ kubectl get rc</data>
  <data key="d2">The command used to list all running ReplicationControllers in the cluster.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Ready">
  <data key="d5">Ready</data>
  <data key="d2">A status indicator that shows how many pods are currently running and ready to receive traffic.</data>
  <data key="d3">status</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="api server">
  <data key="d5">api server</data>
  <data key="d2">The central component that manages access to the Kubernetes cluster.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="containercreating">
  <data key="d5">containercreating</data>
  <data key="d2">An event indicating that a container is being created.</data>
  <data key="d3">event</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="terminating">
  <data key="d5">terminating</data>
  <data key="d2">An event indicating that a pod is terminating.</data>
  <data key="d3">event</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gcloud compute ssh command">
  <data key="d5">gcloud compute ssh command</data>
  <data key="d2">A command used to SSH into a Google Kubernetes Engine node.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ifconfig eth0 down">
  <data key="d5">ifconfig eth0 down</data>
  <data key="d2">A command used to shut down the network interface on a node.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl get node command">
  <data key="d5">kubectl get node command</data>
  <data key="d2">A command used to list the nodes in a Kubernetes cluster.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NotReady status">
  <data key="d5">NotReady status</data>
  <data key="d2">The status of a node that is not ready due to network issues or other problems.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Unknown status">
  <data key="d5">Unknown status</data>
  <data key="d2">The status of a pod that is scheduled to an unreachable node.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="instances">
  <data key="d5">instances</data>
  <data key="d2">compute instances in GKE cluster</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ownerReferences">
  <data key="d5">ownerReferences</data>
  <data key="d2">field in pod metadata referencing ReplicationController</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod creation">
  <data key="d5">pod creation</data>
  <data key="d2">event creating a new pod instance</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-L app">
  <data key="d5">-L app</data>
  <data key="d2">An option for the kubectl get command to display the app label in a column.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-2qneh">
  <data key="d5">kubia-2qneh</data>
  <data key="d2">A pod name</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-dmdck">
  <data key="d5">kubia-dmdck</data>
  <data key="d2">A pod name</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-oini2">
  <data key="d5">kubia-oini2</data>
  <data key="d2">A pod name</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-k0xz6">
  <data key="d5">kubia-k0xz6</data>
  <data key="d2">A pod name</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="rc kubia">
  <data key="d5">rc kubia</data>
  <data key="d2">The name of a ReplicationController.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="YAML definition">
  <data key="d5">YAML definition</data>
  <data key="d2">A human-readable format for representing data, used to define Kubernetes resources.</data>
  <data key="d3">format</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="chapter 9">
  <data key="d5">chapter 9</data>
  <data key="d2">A reference to a future chapter in the book, which will cover a better way of upgrading pods.</data>
  <data key="d3">book</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="scale">
  <data key="d5">scale</data>
  <data key="d2">a command used to change the number of replicas in a ReplicationController</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicationController resource">
  <data key="d5">ReplicationController resource</data>
  <data key="d2">a Kubernetes resource that defines the configuration for a set of pods</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="text editor">
  <data key="d5">text editor</data>
  <data key="d2">an application used for editing text files</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="KUBE_EDITOR">
  <data key="d5">KUBE_EDITOR</data>
  <data key="d2">an environment variable that specifies the default text editor to use with kubectl edit</data>
  <data key="d3">environment variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="EDITOR">
  <data key="d5">EDITOR</data>
  <data key="d2">an environment variable that specifies the default text editor to use for editing files</data>
  <data key="d3">environment variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="selector.app">
  <data key="d5">selector.app</data>
  <data key="d2">The field in the ReplicationController's definition that specifies the label selector for the pods managed by the controller.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicationController's definition">
  <data key="d5">ReplicationController's definition</data>
  <data key="d2">The configuration file that defines the ReplicationController's desired state and behavior.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="desired state">
  <data key="d5">desired state</data>
  <data key="d2">The specified number of replicas or other settings that define how a Kubernetes resource should behave.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="horizontal pod auto-scaling">
  <data key="d5">horizontal pod auto-scaling</data>
  <data key="d2">A feature in Kubernetes that automatically scales the number of replicas based on demand or other conditions.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--cascade=false">
  <data key="d5">--cascade=false</data>
  <data key="d2">An option passed to the kubectl delete command to keep pods running when deleting a ReplicationController.</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod selectors">
  <data key="d5">pod selectors</data>
  <data key="d2">Selectors used to match pods in a ReplicaSet or ReplicationController</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="metadata.labels">
  <data key="d5">metadata.labels</data>
  <data key="d2">Labels attached to the pod, used for matching and selection</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="spec.containers">
  <data key="d5">spec.containers</data>
  <data key="d2">A list of containers in the pod template</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="containers.name">
  <data key="d5">containers.name</data>
  <data key="d2">The name of a container in the pod template</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="selector.matchLabels">
  <data key="d5">selector.matchLabels</data>
  <data key="d2">A way to define label selectors for a ReplicaSet, specifying the labels directly under the selector property.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pods Status">
  <data key="d5">Pods Status</data>
  <data key="d2">A field that displays the status of pods managed by a ReplicaSet, including running, waiting, succeeded, and failed states.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="apiGroup">
  <data key="d5">apiGroup</data>
  <data key="d2">The group name for a Kubernetes API resource.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="v1beta2">
  <data key="d5">v1beta2</data>
  <data key="d2">A specific version of the apps API group.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="core API group">
  <data key="d5">core API group</data>
  <data key="d2">A category of Kubernetes resources that do not require an explicit API group in their apiVersion field.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="In">
  <data key="d5">In</data>
  <data key="d2">An operator that matches if the label's value is one of the specified values.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NotIn">
  <data key="d5">NotIn</data>
  <data key="d2">An operator that matches if the label's value is not any of the specified values.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Exists">
  <data key="d5">Exists</data>
  <data key="d2">An operator that matches if a pod has a label with the specified key, regardless of its value.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DoesNotExist">
  <data key="d5">DoesNotExist</data>
  <data key="d2">An operator that matches if a pod does not have a label with the specified key.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ssd-monitor">
  <data key="d5">ssd-monitor</data>
  <data key="d2">The name of the DaemonSet and pod.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ssd">
  <data key="d5">ssd</data>
  <data key="d2">The value of the disk label.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="luksa/ssd-monitor">
  <data key="d5">luksa/ssd-monitor</data>
  <data key="d2">The name of the Docker image.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ssd-monitor-daemonset.yaml">
  <data key="d5">ssd-monitor-daemonset.yaml</data>
  <data key="d2">A YAML file containing the configuration for the DaemonSet.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="disk=ssd">
  <data key="d5">disk=ssd</data>
  <data key="d2">A label that identifies nodes with solid-state drives (SSDs).</data>
  <data key="d3">label</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="disk=hdd">
  <data key="d5">disk=hdd</data>
  <data key="d2">A label that identifies nodes with hard disk drives (HDDs).</data>
  <data key="d3">label</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="batch API group">
  <data key="d5">batch API group</data>
  <data key="d2">A group of APIs related to batch processing</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="v1 API version">
  <data key="d5">v1 API version</data>
  <data key="d2">The version of the Kubernetes API used for batch processing</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="OnFailure">
  <data key="d5">OnFailure</data>
  <data key="d2">A value of the restart policy field that indicates a pod should be restarted only if it fails</data>
  <data key="d3">value</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="luksa/batch-job">
  <data key="d5">luksa/batch-job</data>
  <data key="d2">The name of an image used to create a container</data>
  <data key="d3">image</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod spec property">
  <data key="d5">pod spec property</data>
  <data key="d2">A property that defaults to Always</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Job pods">
  <data key="d5">Job pods</data>
  <data key="d2">Pods managed by a Job resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="restart policy">
  <data key="d5">restart policy</data>
  <data key="d2">Setting that prevents the container from being restarted when it finishes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl create command">
  <data key="d5">kubectl create command</data>
  <data key="d2">Command used to create a Job resource</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod list">
  <data key="d5">pod list</data>
  <data key="d2">List of running pods</data>
  <data key="d3">output</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--show-all switch">
  <data key="d5">--show-all switch</data>
  <data key="d2">Switch used to show completed pods in the pod list</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="completions property">
  <data key="d5">completions property</data>
  <data key="d2">Property used to set the number of pod instances in a Job</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="parallelism property">
  <data key="d5">parallelism property</data>
  <data key="d2">Property used to set the number of pod instances running in parallel</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="completions">
  <data key="d5">completions</data>
  <data key="d2">A Job property that specifies how many times the Job's pod should run.</data>
  <data key="d3">property</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="parallelism">
  <data key="d5">parallelism</data>
  <data key="d2">A Job property that specifies how many pods are allowed to run in parallel.</data>
  <data key="d3">property</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl scale command">
  <data key="d5">kubectl scale command</data>
  <data key="d2">A command used to scale a Job or ReplicationController by changing its parallelism property.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="spec.backoffLimit field">
  <data key="d5">spec.backoffLimit field</data>
  <data key="d2">A field in the Job manifest that configures how many times a Job can be retried before it is marked as failed.</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cron format">
  <data key="d5">cron format</data>
  <data key="d2">A well-known format for specifying a schedule for running a job.</data>
  <data key="d3">format</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Job template">
  <data key="d5">Job template</data>
  <data key="d2">A template used to create a Job resource according to the CronJob object.</data>
  <data key="d3">template</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod replicas">
  <data key="d5">pod replicas</data>
  <data key="d2">One or more pod replicas created and started according to the Job's pod template.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="jobTemplate">
  <data key="d5">jobTemplate</data>
  <data key="d2">A template from which Job objects will be created by the CronJob resource.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="minute">
  <data key="d5">minute</data>
  <data key="d2">A unit of time in the cron schedule format, representing minutes.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="hour">
  <data key="d5">hour</data>
  <data key="d2">A unit of time in the cron schedule format, representing hours.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="day of month">
  <data key="d5">day of month</data>
  <data key="d2">A unit of time in the cron schedule format, representing days of the month.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="month">
  <data key="d5">month</data>
  <data key="d2">A unit of time in the cron schedule format, representing months.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="day of week">
  <data key="d5">day of week</data>
  <data key="d2">A unit of time in the cron schedule format, representing days of the week.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="startingDeadlineSeconds">
  <data key="d5">startingDeadlineSeconds</data>
  <data key="d2">A field in the CronJob specification that specifies the deadline for starting a job.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="desired replica count">
  <data key="d5">desired replica count</data>
  <data key="d2">The desired number of replicas for a ReplicationController.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="sysadmin">
  <data key="d5">sysadmin</data>
  <data key="d2">system administrator configuring non-Kubernetes environment</data>
  <data key="d3">personnel</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="external clients">
  <data key="d5">external clients</data>
  <data key="d2">clients outside the cluster requesting services from pods</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="external services">
  <data key="d5">external services</data>
  <data key="d2">services provided by entities outside the cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod readiness">
  <data key="d5">pod readiness</data>
  <data key="d2">controlling whether a pod is ready to be part of a service or not</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes Service">
  <data key="d5">Kubernetes Service</data>
  <data key="d2">Resource that makes a single, constant point of entry to a group of pods providing the same service</data>
  <data key="d3">framework</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="IP address and port">
  <data key="d5">IP address and port</data>
  <data key="d2">Constant point of entry for a service</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Frontend web server">
  <data key="d5">Frontend web server</data>
  <data key="d2">Multiple pods providing the same service</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Backend database server">
  <data key="d5">Backend database server</data>
  <data key="d2">Single pod providing a service</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Service for frontend pods">
  <data key="d5">Service for frontend pods</data>
  <data key="d2">Stable address for external clients to connect to the pods</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Service for backend pod">
  <data key="d5">Service for backend pod</data>
  <data key="d2">Stable address for the backend pod</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Node.js app">
  <data key="d5">Node.js app</data>
  <data key="d2">web application framework</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-svc.yaml">
  <data key="d5">kubia-svc.yaml</data>
  <data key="d2">a YAML file that defines a service called kubia</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="get svc">
  <data key="d5">get svc</data>
  <data key="d2">command</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="service IP">
  <data key="d5">service IP</data>
  <data key="d2">IP address of the service</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node.js">
  <data key="d5">node.js</data>
  <data key="d2">JavaScript runtime environment for building scalable server-side applications</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HTTP response">
  <data key="d5">HTTP response</data>
  <data key="d2">Response sent from server to client using the HTTP protocol</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-s option">
  <data key="d5">-s option</data>
  <data key="d2">Option used with kubectl exec to specify an alternate API server</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--">
  <data key="d5">--</data>
  <data key="d2">Double dash character used to signal the end of command options for kubectl</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="sessionAffinity property">
  <data key="d5">sessionAffinity property</data>
  <data key="d2">sets the service's session affinity to ClientIP or None</data>
  <data key="d3">property</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ClientIP">
  <data key="d5">ClientIP</data>
  <data key="d2">a value for the sessionAffinity property that redirects all requests from the same client IP to the same pod</data>
  <data key="d3">value</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes services">
  <data key="d5">Kubernetes services</data>
  <data key="d2">don't operate at the HTTP level and don't care about the payload they carry</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TCP packets">
  <data key="d5">TCP packets</data>
  <data key="d2">and UDP packets are handled by Kubernetes services</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HTTP protocol">
  <data key="d5">HTTP protocol</data>
  <data key="d2">cookies are a construct of the HTTP protocol and aren't known to Kubernetes services</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cookies">
  <data key="d5">cookies</data>
  <data key="d2">a construct of the HTTP protocol that can't be used for session affinity in Kubernetes services</data>
  <data key="d3">construct</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="multi-port service">
  <data key="d5">multi-port service</data>
  <data key="d2">a service that exposes multiple ports, including HTTP and HTTPS</data>
  <data key="d3">service</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ports 80 and 443">
  <data key="d5">ports 80 and 443</data>
  <data key="d2">can be forwarded to the pod's ports 8080 and 8443 using a single multi-port service</data>
  <data key="d3">port</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod's ports 8080 and 8443">
  <data key="d5">pod's ports 8080 and 8443</data>
  <data key="d2">can be exposed through a single cluster IP using a multi-port service</data>
  <data key="d3">port</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="https">
  <data key="d5">https</data>
  <data key="d2">The HTTPS port, which maps to pod's port 8443</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="env command">
  <data key="d5">env command</data>
  <data key="d2">Lists environment variables inside a container</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Environment Variables">
  <data key="d5">Environment Variables</data>
  <data key="d2">Variables set by Kubernetes to point to services</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Port numbers">
  <data key="d5">Port numbers</data>
  <data key="d2">Numbers assigned to ports for communication between containers</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="IP addresses">
  <data key="d5">IP addresses</data>
  <data key="d2">Addresses used for communication between containers</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Service spec">
  <data key="d5">Service spec</data>
  <data key="d2">Configuration file for a service</data>
  <data key="d3">configuration</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="KUBIA_SERVICE_HOST">
  <data key="d5">KUBIA_SERVICE_HOST</data>
  <data key="d2">environment variable</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="KUBIA_SERVICE_PORT">
  <data key="d5">KUBIA_SERVICE_PORT</data>
  <data key="d2">environment variable</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="BACKEND_DATABASE_SERVICE_HOST">
  <data key="d5">BACKEND_DATABASE_SERVICE_HOST</data>
  <data key="d2">environment variable</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="BACKEND_DATABASE_SERVICE_PORT">
  <data key="d5">BACKEND_DATABASE_SERVICE_PORT</data>
  <data key="d2">environment variable</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS server">
  <data key="d5">DNS server</data>
  <data key="d2">service</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="backend-database">
  <data key="d5">backend-database</data>
  <data key="d2">service name</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="svc.cluster.local">
  <data key="d5">svc.cluster.local</data>
  <data key="d2">configurable cluster domain suffix used in all cluster local service names</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="client">
  <data key="d5">client</data>
  <data key="d2">must know the service's port number</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="http://kubia.default.svc.cluster.local">
  <data key="d5">http://kubia.default.svc.cluster.local</data>
  <data key="d2">FQDN of the kubia service</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="http://kubia.default">
  <data key="d5">http://kubia.default</data>
  <data key="d2">simplified FQDN of the kubia service</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="http://kubia">
  <data key="d5">http://kubia</data>
  <data key="d2">the URL of the Kubia service</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-8awf3">
  <data key="d5">kubia-8awf3</data>
  <data key="d2">the hostname of the Kubia service</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="service's name">
  <data key="d5">service's name</data>
  <data key="d2">the name of the service used as the hostname in the requested URL</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/etc/resolv.conf">
  <data key="d5">/etc/resolv.conf</data>
  <data key="d2">a file in the container that configures the DNS resolver</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS resolver">
  <data key="d5">DNS resolver</data>
  <data key="d2">a component that resolves domain names to IP addresses</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia.default.svc.cluster.local">
  <data key="d5">kubia.default.svc.cluster.local</data>
  <data key="d2">the fully qualified domain name (FQDN) of the Kubia service</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="service endpoints">
  <data key="d5">service endpoints</data>
  <data key="d2">a resource that sits between services and pods, providing load balancing and discovery</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="svc kubia">
  <data key="d5">svc kubia</data>
  <data key="d2">service name and selector</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="80/TCP">
  <data key="d5">80/TCP</data>
  <data key="d2">service port and protocol</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl get endpoints kubia">
  <data key="d5">kubectl get endpoints kubia</data>
  <data key="d2">command to display Endpoints resource information</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NAME ENDPOINTS AGE">
  <data key="d5">NAME ENDPOINTS AGE</data>
  <data key="d2">format of Endpoints resource output</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080">
  <data key="d5">10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080</data>
  <data key="d2">list of IP addresses and ports exposing a service</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="external-service">
  <data key="d5">external-service</data>
  <data key="d2">service name and selector</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="endpoints resource">
  <data key="d5">endpoints resource</data>
  <data key="d2">A separate resource and not an attribute of a service.</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="yaml manifest">
  <data key="d5">yaml manifest</data>
  <data key="d2">The YAML manifest for the Endpoints object.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="subsets">
  <data key="d5">subsets</data>
  <data key="d2">A list of subsets for the Endpoints resource.</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="addresses">
  <data key="d5">addresses</data>
  <data key="d2">A list of IP addresses for the Endpoints resource.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ip">
  <data key="d5">ip</data>
  <data key="d2">An individual IP address for the Endpoints resource.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="connections">
  <data key="d5">connections</data>
  <data key="d2">Connections to the service's IP:port pair will be load balanced between the service’s endpoints.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="service name">
  <data key="d5">service name</data>
  <data key="d2">The name of the service, which must match the name of the Endpoints object.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="endpoints IP">
  <data key="d5">endpoints IP</data>
  <data key="d2">The IPs of the endpoints that the service will forward connections to.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ExternalName">
  <data key="d5">ExternalName</data>
  <data key="d2">Type of service that serves as an alias for an external service</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="externalName">
  <data key="d5">externalName</data>
  <data key="d2">Field in Service resource definition, used to point to an external service</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS level">
  <data key="d5">DNS level</data>
  <data key="d2">Implementation level of ExternalName services</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="External client">
  <data key="d5">External client</data>
  <data key="d2">A client outside the Kubernetes cluster that can access services exposed to it through NodePort, LoadBalancer, or Ingress resources.</data>
  <data key="d3">hardware/network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nodePort">
  <data key="d5">nodePort</data>
  <data key="d2">Node port number for the Service resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="svc">
  <data key="d5">svc</data>
  <data key="d2">Short form for Service resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CLUSTER-IP">
  <data key="d5">CLUSTER-IP</data>
  <data key="d2">Cluster IP address for the Service resource</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="&lt;nodes&gt;">
  <data key="d5">&lt;nodes&gt;</data>
  <data key="d2">Placeholder for node IP addresses</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="80:30123/TCP">
  <data key="d5">80:30123/TCP</data>
  <data key="d2">Port number and protocol for the Service resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="10.111.254.223">
  <data key="d5">10.111.254.223</data>
  <data key="d2">Cluster IP address for the Service resource</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="&lt;1st node's IP&gt;">
  <data key="d5">&lt;1st node's IP&gt;</data>
  <data key="d2">Placeholder for first node's IP address</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="30123">
  <data key="d5">30123</data>
  <data key="d2">Node port number for the Service resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Firewall Rules">
  <data key="d5">Firewall Rules</data>
  <data key="d2">rules that control incoming and outgoing network traffic based on source and destination IP addresses, ports, and protocols</data>
  <data key="d3">hardware/network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gcloud compute firewall-rules create">
  <data key="d5">gcloud compute firewall-rules create</data>
  <data key="d2">a command to create a new firewall rule in Google Cloud Platform</data>
  <data key="d3">software/command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-svc-rule">
  <data key="d5">kubia-svc-rule</data>
  <data key="d2">a specific firewall rule created for the Kubia service</data>
  <data key="d3">hardware/network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tcp:30123">
  <data key="d5">tcp:30123</data>
  <data key="d2">a protocol and port number combination that allows incoming TCP connections on port 30123</data>
  <data key="d3">software/protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Port 30123">
  <data key="d5">Port 30123</data>
  <data key="d2">a specific port number used by the NodePort service</data>
  <data key="d3">hardware/computer/port</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="JSONPath">
  <data key="d5">JSONPath</data>
  <data key="d2">Query language for JSON data</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ExternalIP">
  <data key="d5">ExternalIP</data>
  <data key="d2">Type of IP address assigned to a node</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="APIVersion">
  <data key="d5">APIVersion</data>
  <data key="d2">a field in the YAML manifest that specifies the version of the Kubernetes API being used</data>
  <data key="d3">software,hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Port">
  <data key="d5">Port</data>
  <data key="d2">a field in the YAML manifest that specifies a port number for the service</data>
  <data key="d3">software,application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TargetPort">
  <data key="d5">TargetPort</data>
  <data key="d2">a field in the YAML manifest that specifies the port on which the pod is listening</data>
  <data key="d3">software,application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl explain">
  <data key="d5">kubectl explain</data>
  <data key="d2">command to check service's session affinity</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="keep-alive connections">
  <data key="d5">keep-alive connections</data>
  <data key="d2">browser requests sent through single connection</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="session affinity">
  <data key="d5">session affinity</data>
  <data key="d2">service setting to determine pod selection</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="external client">
  <data key="d5">external client</data>
  <data key="d2">client connecting to service through load balancer</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="IP: 130.211.53.173:80">
  <data key="d5">IP: 130.211.53.173:80</data>
  <data key="d2">load balancer IP address and port</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="IP: 130.211.99.206">
  <data key="d5">IP: 130.211.99.206</data>
  <data key="d2">node IP address</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="IP: 130.211.97.55">
  <data key="d5">IP: 130.211.97.55</data>
  <data key="d2">node IP address</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Port 32143">
  <data key="d5">Port 32143</data>
  <data key="d2">port used by service</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="externalTrafficPolicy">
  <data key="d5">externalTrafficPolicy</data>
  <data key="d2">a field in the service's spec section that determines how external traffic is handled</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Local">
  <data key="d5">Local</data>
  <data key="d2">a value for the externalTrafficPolicy field that causes the service proxy to choose a locally running pod</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="client's IP">
  <data key="d5">client's IP</data>
  <data key="d2">The client's IP address that is not preserved when connecting to a service through a node port.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Source Network Address Translation (SNAT)">
  <data key="d5">Source Network Address Translation (SNAT)</data>
  <data key="d2">A process that changes the source IP of packets received through a node port.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Local external traffic policy">
  <data key="d5">Local external traffic policy</data>
  <data key="d2">A policy that affects the preservation of the client's IP when connecting to a service through a node port.</data>
  <data key="d3">policy</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="LoadBalancer service">
  <data key="d5">LoadBalancer service</data>
  <data key="d2">A type of service that requires its own load balancer with its own public IP address.</data>
  <data key="d3">service</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="public IP address">
  <data key="d5">public IP address</data>
  <data key="d2">An IP address that is accessible from outside the cluster.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="host and path in the request">
  <data key="d5">host and path in the request</data>
  <data key="d2">The components of an HTTP request that determine which service to forward the request to.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Service using Local external traffic policy">
  <data key="d5">Service using Local external traffic policy</data>
  <data key="d2">A type of service that may lead to uneven load distribution across pods.</data>
  <data key="d3">service</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Ingress add-on">
  <data key="d5">Ingress add-on</data>
  <data key="d2">An optional feature in Minikube that enables the Ingress functionality.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kube-dns">
  <data key="d5">Kube-dns</data>
  <data key="d2">A service that provides DNS resolution for pods in a Kubernetes cluster.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NAMESPACE">
  <data key="d5">NAMESPACE</data>
  <data key="d2">column header for the namespace column</data>
  <data key="d3">key</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="default-http-backend-5wb0h">
  <data key="d5">default-http-backend-5wb0h</data>
  <data key="d2">name of the Ingress controller pod</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-addon-manager-minikube">
  <data key="d5">kube-addon-manager-minikube</data>
  <data key="d2">name of the Kubernetes addon manager process</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-dns-v20-101vq">
  <data key="d5">kube-dns-v20-101vq</data>
  <data key="d2">name of the Kubernetes DNS service pod</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubernetes-dashboard-jxd9l">
  <data key="d5">kubernetes-dashboard-jxd9l</data>
  <data key="d2">name of the Kubernetes dashboard process</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nginx-ingress-controller-gdts0">
  <data key="d5">nginx-ingress-controller-gdts0</data>
  <data key="d2">name of the Nginx Ingress controller process</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="get ingresses">
  <data key="d5">get ingresses</data>
  <data key="d2">A command used to list Ingress resources in a Kubernetes cluster.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ingresses">
  <data key="d5">ingresses</data>
  <data key="d2">A resource that exposes services externally through HTTP requests.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS servers">
  <data key="d5">DNS servers</data>
  <data key="d2">Servers that resolve domain names to IP addresses.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/etc/hosts">
  <data key="d5">/etc/hosts</data>
  <data key="d2">A file on Unix-like systems that maps hostnames to IP addresses.</data>
  <data key="d3">file system</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="End-points object">
  <data key="d5">End-points object</data>
  <data key="d2">An object that stores information about the IP addresses of pods associated with a service.</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Host header">
  <data key="d5">Host header</data>
  <data key="d2">A header in an HTTP request that specifies the hostname or IP address of the server being requested.</data>
  <data key="d3">header</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TLS">
  <data key="d5">TLS</data>
  <data key="d2">A protocol for encrypting communication between clients and servers.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="openssl">
  <data key="d5">openssl</data>
  <data key="d2">A command-line tool for generating and managing SSL/TLS certificates and private keys.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="genrsa">
  <data key="d5">genrsa</data>
  <data key="d2">A command used to generate a new RSA key pair.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="req">
  <data key="d5">req</data>
  <data key="d2">A command used to create a certificate signing request (CSR).</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="x509">
  <data key="d5">x509</data>
  <data key="d2">A command used to create a self-signed X.509 certificate.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tls-secret">
  <data key="d5">tls-secret</data>
  <data key="d2">name of the Secret resource</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CertificateSigningRequest">
  <data key="d5">CertificateSigningRequest</data>
  <data key="d2">resource to request a signed certificate</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="csr">
  <data key="d5">csr</data>
  <data key="d2">short form for CertificateSigningRequest</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-ingress-tls.yaml">
  <data key="d5">kubia-ingress-tls.yaml</data>
  <data key="d2">file containing the Ingress configuration</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tls">
  <data key="d5">tls</data>
  <data key="d2">attribute of the Ingress resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia.example.com">
  <data key="d5">kubia.example.com</data>
  <data key="d2">hostname for which TLS connections will be accepted</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="liveness probes">
  <data key="d5">liveness probes</data>
  <data key="d2">A mechanism to periodically check if a container is running and responding correctly.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GET / request">
  <data key="d5">GET / request</data>
  <data key="d2">HTTP request method</data>
  <data key="d3">network,application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TCP Socket probe">
  <data key="d5">TCP Socket probe</data>
  <data key="d2">Type of readiness probe that opens a TCP connection</data>
  <data key="d3">process,container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TCP connection">
  <data key="d5">TCP connection</data>
  <data key="d2">Network communication protocol</data>
  <data key="d3">network,application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="container specification">
  <data key="d5">container specification</data>
  <data key="d2">Definition of a container in the pod template</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="readiness probe definition">
  <data key="d5">readiness probe definition</data>
  <data key="d2">Periodic command to check if a file exists inside the container</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ls command">
  <data key="d5">ls command</data>
  <data key="d2">Command to list files and directories inside the container</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="exit code zero">
  <data key="d5">exit code zero</data>
  <data key="d2">Return value indicating success or failure of the readiness probe</data>
  <data key="d3">error</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="file /var/ready">
  <data key="d5">file /var/ready</data>
  <data key="d2">File used by the readiness probe to determine pod readiness</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="READY column">
  <data key="d5">READY column</data>
  <data key="d2">column in kubectl output showing pod readiness</data>
  <data key="d3">output</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="touch command">
  <data key="d5">touch command</data>
  <data key="d2">command to create a new file</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-loadbalancer">
  <data key="d5">kubia-loadbalancer</data>
  <data key="d2">service name</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod labels">
  <data key="d5">pod labels</data>
  <data key="d2">Metadata associated with a pod that can be used to identify or filter pods</data>
  <data key="d3">metadata</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS lookups">
  <data key="d5">DNS lookups</data>
  <data key="d2">a way to resolve service names to pod IPs</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Cluster IP">
  <data key="d5">Cluster IP</data>
  <data key="d2">a virtual IP address assigned to a service by Kubernetes</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Service specification">
  <data key="d5">Service specification</data>
  <data key="d2">the configuration file that defines a service</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="clusterIP field">
  <data key="d5">clusterIP field</data>
  <data key="d2">a field in the service specification that determines whether a cluster IP is assigned</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dig binary">
  <data key="d5">dig binary</data>
  <data key="d2">a binary used for DNS-related actions</data>
  <data key="d3">binary</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tutum/dnsutils container image">
  <data key="d5">tutum/dnsutils container image</data>
  <data key="d2">an image containing the binaries needed for DNS-related actions</data>
  <data key="d3">image</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--generator=run-pod/v1 option">
  <data key="d5">--generator=run-pod/v1 option</data>
  <data key="d2">an option used to create a pod directly without any kind of ReplicationController or similar behind it</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS A records">
  <data key="d5">DNS A records</data>
  <data key="d2">the IP addresses returned by a DNS server for a headless service</data>
  <data key="d3">object</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-headless.default.svc.cluster.local FQDN">
  <data key="d5">kubia-headless.default.svc.cluster.local FQDN</data>
  <data key="d2">a fully qualified domain name used to perform DNS lookups</data>
  <data key="d3">domain name</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl get pods command">
  <data key="d5">kubectl get pods command</data>
  <data key="d2">a command to list running pods with their IPs</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="headless services">
  <data key="d5">headless services</data>
  <data key="d2">services that don't have an IP address</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS round-robin">
  <data key="d5">DNS round-robin</data>
  <data key="d2">mechanism for distributing traffic across multiple pods</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="service.alpha.kubernetes.io/tolerate-unready-endpoints">
  <data key="d5">service.alpha.kubernetes.io/tolerate-unready-endpoints</data>
  <data key="d2">annotation for tolerating unready endpoints</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="publishNotReadyAddresses">
  <data key="d5">publishNotReadyAddresses</data>
  <data key="d2">field for publishing not ready addresses</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes version 1.9.0">
  <data key="d5">Kubernetes version 1.9.0</data>
  <data key="d2">version of Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="port exposed by the service">
  <data key="d5">port exposed by the service</data>
  <data key="d2">The port number that a service exposes to clients.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes Service resources">
  <data key="d5">Kubernetes Service resources</data>
  <data key="d2">A type of resource in Kubernetes that exposes services available in an application.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ExternalName service type">
  <data key="d5">ExternalName service type</data>
  <data key="d2">A type of service in Kubernetes that provides a DNS CNAME alias for external services.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod container">
  <data key="d5">pod container</data>
  <data key="d2">a pod's readiness probe is used to determine whether a pod should or shouldn't be included as a service endpoint</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="bash shell">
  <data key="d5">bash shell</data>
  <data key="d2">running a bash shell in an existing pod's container</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubernetes resources">
  <data key="d5">kubernetes resources</data>
  <data key="d2">managed by kubectl apply command</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod IPs">
  <data key="d5">pod IPs</data>
  <data key="d2">discovered through DNS when creating a headless service</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Volumes">
  <data key="d5">Volumes</data>
  <data key="d2">attaching disk storage to containers</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Disk storage">
  <data key="d5">Disk storage</data>
  <data key="d2">persistent storage for containers</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Persistent storage">
  <data key="d5">Persistent storage</data>
  <data key="d2">shared disk storage between containers</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/var/logs">
  <data key="d5">/var/logs</data>
  <data key="d2">A directory in the filesystem used by containers to store logs</data>
  <data key="d3">filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/var/html">
  <data key="d5">/var/html</data>
  <data key="d2">A directory in the filesystem used by containers to create and store HTML files</data>
  <data key="d3">filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="web server">
  <data key="d5">web server</data>
  <data key="d2">A software application that serves HTTP requests and returns web pages</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="agent">
  <data key="d5">agent</data>
  <data key="d2">A software component that performs a specific task or function</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="log rotator">
  <data key="d5">log rotator</data>
  <data key="d2">A software component that rotates, compresses, and analyzes logs</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container: WebServer">
  <data key="d5">Container: WebServer</data>
  <data key="d2">A container running a web server process</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Webserver">
  <data key="d5">Webserver</data>
  <data key="d2">A process running a web server application</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/">
  <data key="d5">/</data>
  <data key="d2">The root directory of the filesystem</data>
  <data key="d3">filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/htdocs/">
  <data key="d5">/htdocs/</data>
  <data key="d2">A subdirectory of the filesystem for storing web server files</data>
  <data key="d3">filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container: ContentAgent">
  <data key="d5">Container: ContentAgent</data>
  <data key="d2">A container running a content agent process</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ContentAgent">
  <data key="d5">ContentAgent</data>
  <data key="d2">A process running a content agent application</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/html/">
  <data key="d5">/html/</data>
  <data key="d2">A subdirectory of the filesystem for storing HTML files</data>
  <data key="d3">filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container: LogRotator">
  <data key="d5">Container: LogRotator</data>
  <data key="d2">A container running a log rotator process</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="LogRotator">
  <data key="d5">LogRotator</data>
  <data key="d2">A process running a log rotator application</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Volume: publicHtml">
  <data key="d5">Volume: publicHtml</data>
  <data key="d2">A shared storage volume for web server files</data>
  <data key="d3">storage</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Volume: logVol">
  <data key="d5">Volume: logVol</data>
  <data key="d2">A shared storage volume for log files</data>
  <data key="d3">storage</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Linux">
  <data key="d5">Linux</data>
  <data key="d2">Operating System</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Web-Server container">
  <data key="d5">Web-Server container</data>
  <data key="d2">A container running a web server</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ContentAgent container">
  <data key="d5">ContentAgent container</data>
  <data key="d2">A container generating content for the web server</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="LogRotator container">
  <data key="d5">LogRotator container</data>
  <data key="d2">A container rotating logs for the web server</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="publicHtml volume">
  <data key="d5">publicHtml volume</data>
  <data key="d2">A shared directory between containers serving HTML files</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="logVol volume">
  <data key="d5">logVol volume</data>
  <data key="d2">A shared directory for storing logs</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="awsElastic-BlockStore">
  <data key="d5">awsElastic-BlockStore</data>
  <data key="d2">A type of volume for Amazon Web Services Elastic Block Store Volume storage</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="quobyte">
  <data key="d5">quobyte</data>
  <data key="d2">Used for mounting other types of network storage.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="vsphere-Volume">
  <data key="d5">vsphere-Volume</data>
  <data key="d2">Used for mounting other types of network storage.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="photonPersistentDisk">
  <data key="d5">photonPersistentDisk</data>
  <data key="d2">Used for mounting other types of network storage.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="scaleIO">
  <data key="d5">scaleIO</data>
  <data key="d2">Used for mounting other types of network storage.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="UNIX fortune command">
  <data key="d5">UNIX fortune command</data>
  <data key="d2">A command that generates a random quote every time it is run.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune image">
  <data key="d5">fortune image</data>
  <data key="d2">A Docker image that generates fortunes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="apt-get">
  <data key="d5">apt-get</data>
  <data key="d2">A package manager for Debian-based systems</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune binary">
  <data key="d5">fortune binary</data>
  <data key="d2">A program that generates fortunes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="disk storage">
  <data key="d5">disk storage</data>
  <data key="d2">A type of persistent storage for containers</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="luksa/fortune">
  <data key="d5">luksa/fortune</data>
  <data key="d2">the Docker image used by html-generator container</data>
  <data key="d3">image</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nginx:alpine">
  <data key="d5">nginx:alpine</data>
  <data key="d2">the Docker image used by web-server container</data>
  <data key="d3">image</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="read-only">
  <data key="d5">read-only</data>
  <data key="d2">the mode of the volume mounted in web-server container</data>
  <data key="d3">docker</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TCP">
  <data key="d5">TCP</data>
  <data key="d2">the protocol used for communication between containers and the host machine</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune">
  <data key="d5">fortune</data>
  <data key="d2">the name of the pod that contains two containers and a shared volume</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tmpfs filesystem">
  <data key="d5">tmpfs filesystem</data>
  <data key="d2">a filesystem that stores data in memory instead of on disk</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="repository">
  <data key="d5">repository</data>
  <data key="d2">the URL of the Git repository to be cloned</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="directory">
  <data key="d5">directory</data>
  <data key="d2">the directory within the volume where the repository will be cloned</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Git">
  <data key="d5">Git</data>
  <data key="d2">a version control system</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GitHub">
  <data key="d5">GitHub</data>
  <data key="d2">a web-based platform for version control and collaboration</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="sidecar container">
  <data key="d5">sidecar container</data>
  <data key="d2">a secondary container that augments the operation of a main container</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Git sync process">
  <data key="d5">Git sync process</data>
  <data key="d2">a process that keeps a local directory synchronized with a Git repository</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SSH protocol">
  <data key="d5">SSH protocol</data>
  <data key="d2">a secure communication protocol for secure shell access</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="database pod">
  <data key="d5">database pod</data>
  <data key="d2">a pod that stores data in a hostPath volume</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fluentd-kubia-4ebc2f1e-9a3e pod">
  <data key="d5">fluentd-kubia-4ebc2f1e-9a3e pod</data>
  <data key="d2">a pod that uses hostPath volumes to access node's data</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="varlog volume">
  <data key="d5">varlog volume</data>
  <data key="d2">a hostPath volume that accesses the node's /var/log directory</data>
  <data key="d3">volume</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="varlibdockercontainers volume">
  <data key="d5">varlibdockercontainers volume</data>
  <data key="d2">a hostPath volume that accesses the node's /var/lib/docker/containers directory</data>
  <data key="d3">volume</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Minikube pod">
  <data key="d5">Minikube pod</data>
  <data key="d2">a pod used for testing persistent storage in single-node clusters</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CA certificates">
  <data key="d5">CA certificates</data>
  <data key="d2">the node's CA certificates accessed by hostPath volumes</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl get pod command">
  <data key="d5">kubectl get pod command</data>
  <data key="d2">a command used to list pods in a namespace</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl describe po command">
  <data key="d5">kubectl describe po command</data>
  <data key="d2">a command used to describe a pod's details</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="hostPath volumes">
  <data key="d5">hostPath volumes</data>
  <data key="d2">A type of volume that allows reading or writing system files on a node.</data>
  <data key="d3">volume</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gcloud command">
  <data key="d5">gcloud command</data>
  <data key="d2">A command-line tool for managing Google Cloud resources.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod volume">
  <data key="d5">pod volume</data>
  <data key="d2">A type of storage that allows data to be persisted within a pod.</data>
  <data key="d3">volume</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gce persistent disk">
  <data key="d5">gce persistent disk</data>
  <data key="d2">a type of persistent storage used by Google Cloud Engine</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="mongodb">
  <data key="d5">mongodb</data>
  <data key="d2">a database management system</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gce persistent disk volume">
  <data key="d5">gce persistent disk volume</data>
  <data key="d2">a type of persistent storage used by Google Cloud Engine, attached to a container</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/data/db">
  <data key="d5">/data/db</data>
  <data key="d2">the path where MongoDB stores its data</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="mongodb-pod-hostpath.yaml">
  <data key="d5">mongodb-pod-hostpath.yaml</data>
  <data key="d2">the YAML file for a pod using a hostPath volume</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gcepd.yaml">
  <data key="d5">gcepd.yaml</data>
  <data key="d2">the YAML file for a pod using a GCE Persistent Disk volume</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="JSON document">
  <data key="d5">JSON document</data>
  <data key="d2">A type of data that can be stored in a MongoDB database.</data>
  <data key="d3">data format</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="find() command">
  <data key="d5">find() command</data>
  <data key="d2">A MongoDB command used to retrieve documents from a collection.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="insert() command">
  <data key="d5">insert() command</data>
  <data key="d2">A MongoDB command used to insert a new document into a collection.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GCE persistent disk">
  <data key="d5">GCE persistent disk</data>
  <data key="d2">A type of storage that persists data even after a pod is deleted or recreated, provided by Google Cloud Engine.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="mongodb://127.0.0.1:27017">
  <data key="d5">mongodb://127.0.0.1:27017</data>
  <data key="d2">MongoDB connection string</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ObjectId">
  <data key="d5">ObjectId</data>
  <data key="d2">Data type in MongoDB for object IDs</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="azureFile">
  <data key="d5">azureFile</data>
  <data key="d2">Type of volume used for persistent storage on Microsoft Azure</data>
  <data key="d3">volume</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="volumeId">
  <data key="d5">volumeId</data>
  <data key="d2">unique identifier for a volume</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NFS volume">
  <data key="d5">NFS volume</data>
  <data key="d2">network file system storage option</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fc">
  <data key="d5">fc</data>
  <data key="d2">storage option for Fibre Channel connections</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Persistent Volume">
  <data key="d5">Persistent Volume</data>
  <data key="d2">Provisioned storage resource</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NFS export">
  <data key="d5">NFS export</data>
  <data key="d2">Network File System export</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentVolumeClaim (PVC)">
  <data key="d5">PersistentVolumeClaim (PVC)</data>
  <data key="d2">Request for persistent storage</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentVolume (PV)">
  <data key="d5">PersistentVolume (PV)</data>
  <data key="d2">Provisioned storage resource</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Cluster administrator">
  <data key="d5">Cluster administrator</data>
  <data key="d2">User responsible for setting up storage resources</data>
  <data key="d3">role</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Application developer">
  <data key="d5">Application developer</data>
  <data key="d2">User responsible for using persistent storage</data>
  <data key="d3">role</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentVolumeReclaimPolicy">
  <data key="d5">PersistentVolumeReclaimPolicy</data>
  <data key="d2">Policy for reclaiming a PersistentVolume</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReadOnlyMany">
  <data key="d5">ReadOnlyMany</data>
  <data key="d2">Access mode for a PersistentVolume</data>
  <data key="d3">access mode</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentVolumeClaim manifest">
  <data key="d5">PersistentVolumeClaim manifest</data>
  <data key="d2">A YAML file defining a PersistentVolumeClaim</data>
  <data key="d3">configuration</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cluster Nodes">
  <data key="d5">cluster Nodes</data>
  <data key="d2">The nodes of a Kubernetes cluster</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Namespace A">
  <data key="d5">Namespace A</data>
  <data key="d2">A namespace in a Kubernetes cluster</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="User A">
  <data key="d5">User A</data>
  <data key="d2">A user in a Kubernetes cluster</data>
  <data key="d3">user</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="RWO">
  <data key="d5">RWO</data>
  <data key="d2">ReadWriteOnce access mode (only one node can read and write)</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ROX">
  <data key="d5">ROX</data>
  <data key="d2">ReadOnlyMany access mode (multiple nodes can read)</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="RWX">
  <data key="d5">RWX</data>
  <data key="d2">ReadWriteMany access mode (multiple nodes can read and write)</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="claims">
  <data key="d5">claims</data>
  <data key="d2">requests for persistent storage</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GCE Persistent Disks">
  <data key="d5">GCE Persistent Disks</data>
  <data key="d2">type of storage resource provided by Google Cloud Platform</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Delete">
  <data key="d5">Delete</data>
  <data key="d2">persistentVolumeReclaimPolicy to delete the underlying storage</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Retain policy">
  <data key="d5">Retain policy</data>
  <data key="d2">a reclaim policy for PersistentVolumes that retains the data on deletion</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Delete policy">
  <data key="d5">Delete policy</data>
  <data key="d2">a reclaim policy for PersistentVolumes that deletes the data on deletion</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentVolume provisioner">
  <data key="d5">PersistentVolume provisioner</data>
  <data key="d2">a component that provisions PersistentVolumes dynamically</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentVolumeClaim 1">
  <data key="d5">PersistentVolumeClaim 1</data>
  <data key="d2">a specific PersistentVolumeClaim resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod 2">
  <data key="d5">Pod 2</data>
  <data key="d2">a specific Pod resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentVolumeClaim 2">
  <data key="d5">PersistentVolumeClaim 2</data>
  <data key="d2">a specific PersistentVolumeClaim resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod 3">
  <data key="d5">Pod 3</data>
  <data key="d2">a specific Pod resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Dynamic Provisioning">
  <data key="d5">Dynamic Provisioning</data>
  <data key="d2">Provisioning of PersistentVolumes on demand</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentDisk (PD)">
  <data key="d5">PersistentDisk (PD)</data>
  <data key="d2">Storage type provided by Google Compute Engine</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GCE">
  <data key="d5">GCE</data>
  <data key="d2">Google Cloud Platform service</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PVC definition">
  <data key="d5">PVC definition</data>
  <data key="d2">Definition of a PersistentVolumeClaim</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="storageclass-fast-gcepd.yaml">
  <data key="d5">storageclass-fast-gcepd.yaml</data>
  <data key="d2">File containing the StorageClass definition</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="mongodb-pvc-dp.yaml">
  <data key="d5">mongodb-pvc-dp.yaml</data>
  <data key="d2">Updated PVC definition with dynamic provisioning</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fast StorageClass">
  <data key="d5">fast StorageClass</data>
  <data key="d2">A custom storage class used in the example</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl get pvc">
  <data key="d5">kubectl get pvc</data>
  <data key="d2">A command to list Persistent Volume Claims</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gcloud compute disks list">
  <data key="d5">gcloud compute disks list</data>
  <data key="d2">A command to list Compute Engine disks</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubernetes.io/gce-pd provisioner">
  <data key="d5">kubernetes.io/gce-pd provisioner</data>
  <data key="d2">The provisioner used by the fast StorageClass</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="storage class">
  <data key="d5">storage class</data>
  <data key="d2">A storage class with different performance or other characteristics</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Persistent disk">
  <data key="d5">Persistent disk</data>
  <data key="d2">A persistent storage device attached to a pod</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SSD">
  <data key="d5">SSD</data>
  <data key="d2">Solid-State Drive, a type of fast storage device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fast storage class">
  <data key="d5">fast storage class</data>
  <data key="d2">A custom storage class created by the user</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="standard storage class">
  <data key="d5">standard storage class</data>
  <data key="d2">The default storage class provided by GKE or Minikube</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="standard">
  <data key="d5">standard</data>
  <data key="d2">default storage class in GKE cluster</data>
  <data key="d3">storage class</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="storageclass.beta.kubernetes.io/is-default-class">
  <data key="d5">storageclass.beta.kubernetes.io/is-default-class</data>
  <data key="d2">annotation to mark a storage class as default</data>
  <data key="d3">annotation</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pd-standard">
  <data key="d5">pd-standard</data>
  <data key="d2">type of GCE PD to create</data>
  <data key="d3">storage class parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="100Mi">
  <data key="d5">100Mi</data>
  <data key="d2">amount of storage requested by the PVC</data>
  <data key="d3">storage request</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="compute disks list">
  <data key="d5">compute disks list</data>
  <data key="d2">A command used with gcloud to retrieve information about compute disks</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dynamic volume provisioner">
  <data key="d5">dynamic volume provisioner</data>
  <data key="d2">A component that provisions PersistentVolumes dynamically</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="command-line options">
  <data key="d5">command-line options</data>
  <data key="d2">arguments passed to an application when it is run from the command line</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="config file">
  <data key="d5">config file</data>
  <data key="d2">file containing configuration options</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Docker containers">
  <data key="d5">Docker containers</data>
  <data key="d2">containerized applications</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes resources">
  <data key="d5">Kubernetes resources</data>
  <data key="d2">top-level Kubernetes resource for storing configuration data</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="docker exec">
  <data key="d5">docker exec</data>
  <data key="d2">command used to execute a command inside a running container</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ps x">
  <data key="d5">ps x</data>
  <data key="d2">Unix command used to list processes running in the system</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="docker exec -it">
  <data key="d5">docker exec -it</data>
  <data key="d2">command used to execute a command inside a running container with interactive shell</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ENTRYPOINT instruction">
  <data key="d5">ENTRYPOINT instruction</data>
  <data key="d2">a command in a Dockerfile that specifies the default command to run when the container is started</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CMD instruction">
  <data key="d5">CMD instruction</data>
  <data key="d2">a command in a Dockerfile that specifies the default arguments for the ENTRYPOINT command</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="apt-get update">
  <data key="d5">apt-get update</data>
  <data key="d2">a command used to update the package index in Ubuntu</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="apt-get -y install fortune">
  <data key="d5">apt-get -y install fortune</data>
  <data key="d2">a command used to install the fortune program in Ubuntu</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="interval">
  <data key="d5">interval</data>
  <data key="d2">a variable that specifies the delay interval between fortunes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PID 1">
  <data key="d5">PID 1</data>
  <data key="d2">the process ID of the shell process</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PID 7">
  <data key="d5">PID 7</data>
  <data key="d2">the process ID of the node process</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="shell">
  <data key="d5">shell</data>
  <data key="d2">a command-line interface for interacting with the operating system</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="exec form">
  <data key="d5">exec form</data>
  <data key="d2">a way to specify a default command in a Dockerfile using the ENTRYPOINT instruction</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="docker build">
  <data key="d5">docker build</data>
  <data key="d2">a command used to build a Docker image from a Dockerfile</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="docker push">
  <data key="d5">docker push</data>
  <data key="d2">a command used to push a Docker image to a registry, such as Docker Hub</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Control+C">
  <data key="d5">Control+C</data>
  <data key="d2">Keyboard shortcut to stop the script</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="docker.io/luksa/fortune:args">
  <data key="d5">docker.io/luksa/fortune:args</data>
  <data key="d2">Docker image with custom argument handling</data>
  <data key="d3">image</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/bin/command">
  <data key="d5">/bin/command</data>
  <data key="d2">Custom command to override the ENTRYPOINT</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="arg1, arg2, arg3">
  <data key="d5">arg1, arg2, arg3</data>
  <data key="d2">Custom arguments to pass to the container</data>
  <data key="d3">arguments</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune-pod-args.yaml">
  <data key="d5">fortune-pod-args.yaml</data>
  <data key="d2">Modified Kubernetes pod definition file with custom arguments</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune:args">
  <data key="d5">fortune:args</data>
  <data key="d2">a specific Docker image that generates a new fortune every two seconds</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune:latest">
  <data key="d5">fortune:latest</data>
  <data key="d2">a specific Docker image, but not specified what it does</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="FOO=BAR">
  <data key="d5">FOO=BAR</data>
  <data key="d2">an environment variable with key FOO and value BAR</data>
  <data key="d3">environment variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ABC=123">
  <data key="d5">ABC=123</data>
  <data key="d2">an environment variable with key ABC and value 123</data>
  <data key="d3">environment variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container A">
  <data key="d5">Container A</data>
  <data key="d2">the first container in the pod, with its own set of environment variables</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container B">
  <data key="d5">Container B</data>
  <data key="d2">the second container in the pod, with its own set of environment variables</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="trap">
  <data key="d5">trap</data>
  <data key="d2">Bash command used to catch and handle signals (e.g. SIGINT)</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SIGINT">
  <data key="d5">SIGINT</data>
  <data key="d2">Signal sent when the user presses Ctrl+C</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="mkdir">
  <data key="d5">mkdir</data>
  <data key="d2">Bash command used to create a new directory</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="while">
  <data key="d5">while</data>
  <data key="d2">Bash command used to create a loop that runs indefinitely</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/usr/games/fortune">
  <data key="d5">/usr/games/fortune</data>
  <data key="d2">Command used to generate a new fortune</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="System.getenv">
  <data key="d5">System.getenv</data>
  <data key="d2">Java method used to retrieve an environment variable</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="process.env">
  <data key="d5">process.env</data>
  <data key="d2">Node.js property used to access environment variables</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="os.environ">
  <data key="d5">os.environ</data>
  <data key="d2">Python dictionary used to store environment variables</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="container definition">
  <data key="d5">container definition</data>
  <data key="d2">YAML file used to specify the configuration for a container</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ConfigMap resource">
  <data key="d5">ConfigMap resource</data>
  <data key="d2">a source for environment variable values using the valueFrom instead of the value field</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$(VAR) syntax">
  <data key="d5">$(VAR) syntax</data>
  <data key="d2">referencing previously defined environment variables or any other existing variables</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="FIRST_VAR">
  <data key="d5">FIRST_VAR</data>
  <data key="d2">an environment variable with value 'foo'</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SECOND_VAR">
  <data key="d5">SECOND_VAR</data>
  <data key="d2">an environment variable with value '$(FIRST_VAR)bar'</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes REST API endpoint">
  <data key="d5">Kubernetes REST API endpoint</data>
  <data key="d2">an API endpoint for accessing ConfigMap data</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="configMap volume">
  <data key="d5">configMap volume</data>
  <data key="d2">a volume that provides access to a ConfigMap's data</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="key1=value1">
  <data key="d5">key1=value1</data>
  <data key="d2">an example key-value pair within a ConfigMap</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="key2=value2">
  <data key="d5">key2=value2</data>
  <data key="d2">another example key-value pair within a ConfigMap</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="app-config">
  <data key="d5">app-config</data>
  <data key="d2">a ConfigMap with a specific name and namespace</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="development values">
  <data key="d5">development values</data>
  <data key="d2">example configuration data for the development environment</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="production values">
  <data key="d5">production values</data>
  <data key="d2">example configuration data for the production environment</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="create configmap command">
  <data key="d5">create configmap command</data>
  <data key="d2">a special command for creating ConfigMaps</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ConfigMap keys">
  <data key="d5">ConfigMap keys</data>
  <data key="d2">must be valid DNS subdomains and can contain alphanumeric characters, dashes, underscores, and dots</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS subdomain">
  <data key="d5">DNS subdomain</data>
  <data key="d2">a subset of a domain name that is used to identify a specific resource or service</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="literal">
  <data key="d5">literal</data>
  <data key="d2">a value that is specified directly in a command or configuration file</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ConfigMap name">
  <data key="d5">ConfigMap name</data>
  <data key="d2">the unique identifier for a ConfigMap, used to reference it in other commands and configurations</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ kubectl create -f fortune-config.yaml">
  <data key="d5">$ kubectl create -f fortune-config.yaml</data>
  <data key="d2">A command to create a ConfigMap from a YAML file</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--from-file=config-file.conf">
  <data key="d5">--from-file=config-file.conf</data>
  <data key="d2">An option to create a ConfigMap from a file</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--from-literal=some=thing">
  <data key="d5">--from-literal=some=thing</data>
  <data key="d2">An option to create a ConfigMap from a literal value</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/path/to/dir">
  <data key="d5">/path/to/dir</data>
  <data key="d2">A directory path used to import files into a ConfigMap</data>
  <data key="d3">directory</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="bar=foobar.conf">
  <data key="d5">bar=foobar.conf</data>
  <data key="d2">A file stored under a custom key in a ConfigMap</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="config-opts/">
  <data key="d5">config-opts/</data>
  <data key="d2">A directory used to import files into a ConfigMap</data>
  <data key="d3">directory</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="some=thing">
  <data key="d5">some=thing</data>
  <data key="d2">A literal value used to create a ConfigMap</data>
  <data key="d3">value</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="my-conﬁg">
  <data key="d5">my-conﬁg</data>
  <data key="d2">the name of a ConfigMap</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="abc">
  <data key="d5">abc</data>
  <data key="d2">a value stored in the ConfigMap</data>
  <data key="d3">string</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="true">
  <data key="d5">true</data>
  <data key="d2">the value of a boolean variable stored in the ConfigMap</data>
  <data key="d3">boolean</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="repeat">
  <data key="d5">repeat</data>
  <data key="d2">a key-value pair stored in the ConfigMap</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="some thing">
  <data key="d5">some thing</data>
  <data key="d2">a key-value pair stored in the ConfigMap</data>
  <data key="d3">string</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="foobar.conf">
  <data key="d5">foobar.conf</data>
  <data key="d2">a file containing configuration data in JSON format</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ConfigMapKeyRef">
  <data key="d5">ConfigMapKeyRef</data>
  <data key="d2">A reference to a ConfigMap key, used to initialize environment variables in containers.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="envFrom">
  <data key="d5">envFrom</data>
  <data key="d2">An attribute used to expose all entries of a ConfigMap as environment variables.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="prefix">
  <data key="d5">prefix</data>
  <data key="d2">An optional attribute used to specify a prefix for environment variables.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CONFIG_FOO">
  <data key="d5">CONFIG_FOO</data>
  <data key="d2">An example environment variable created by using the envFrom attribute.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CONFIG_BAR">
  <data key="d5">CONFIG_BAR</data>
  <data key="d2">Another example environment variable created by using the envFrom attribute.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="FOO-BAR">
  <data key="d5">FOO-BAR</data>
  <data key="d2">A ConfigMap key that is not converted to a valid environment variable name.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="my-config-map">
  <data key="d5">my-config-map</data>
  <data key="d2">An example ConfigMap used in the listing.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="some-image">
  <data key="d5">some-image</data>
  <data key="d2">The image used in the container.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="configMapVolume">
  <data key="d5">configMapVolume</data>
  <data key="d2">a special volume type that exposes ConfigMap entries as files</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="luksa/fortune:args">
  <data key="d5">luksa/fortune:args</data>
  <data key="d2">the Docker image that takes an integer argument</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Nginx web server">
  <data key="d5">Nginx web server</data>
  <data key="d2">web server software used in the example</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortuneloop.sh script">
  <data key="d5">fortuneloop.sh script</data>
  <data key="d2">script used to configure the Nginx web server</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Config file">
  <data key="d5">Config file</data>
  <data key="d2">file containing configuration data for an application</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="server_name">
  <data key="d5">server_name</data>
  <data key="d2">directive in Nginx config file specifying the server name</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="listen directive">
  <data key="d5">listen directive</data>
  <data key="d2">directive in Nginx config file specifying the port number to listen on</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gzip_types">
  <data key="d5">gzip_types</data>
  <data key="d2">directive in Nginx config file specifying the types of files to compress</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="location directive">
  <data key="d5">location directive</data>
  <data key="d2">directive in Nginx config file specifying the location of a resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="root directive">
  <data key="d5">root directive</data>
  <data key="d2">directive in Nginx config file specifying the root directory for a resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="index directive">
  <data key="d5">index directive</data>
  <data key="d2">directive in Nginx config file specifying the index files to serve</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="configmap-files">
  <data key="d5">configmap-files</data>
  <data key="d2">directory containing configuration files used to create a ConfigMap</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ConfigMap's entries">
  <data key="d5">ConfigMap's entries</data>
  <data key="d2">The key-value pairs stored in a ConfigMap.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/etc/nginx/nginx.conf">
  <data key="d5">/etc/nginx/nginx.conf</data>
  <data key="d2">The default Nginx configuration file.</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/etc/nginx/conf.d/">
  <data key="d5">/etc/nginx/conf.d/</data>
  <data key="d2">A directory where Nginx automatically includes .conf files.</data>
  <data key="d3">directory</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="port-forwarding">
  <data key="d5">port-forwarding</data>
  <data key="d2">the process of forwarding traffic from a local port to a remote port in the pod</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune-configmap-volume">
  <data key="d5">fortune-configmap-volume</data>
  <data key="d2">a pod with a ConfigMap volume mounted to it</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="items attribute">
  <data key="d5">items attribute</data>
  <data key="d2">a configuration option for specifying which ConfigMap entries should be exposed as files in a volume</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gzip.conf">
  <data key="d5">gzip.conf</data>
  <data key="d2">a file containing compressed configuration data</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gzip.conf file">
  <data key="d5">gzip.conf file</data>
  <data key="d2">a configuration file for Nginx that enables gzip compression</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="subPath property">
  <data key="d5">subPath property</data>
  <data key="d2">an additional property on the volumeMount that allows you to mount a single file or directory from a ConfigMap</data>
  <data key="d3">property</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/etc directory">
  <data key="d5">/etc directory</data>
  <data key="d2">a system directory in Linux that contains important configuration files</data>
  <data key="d3">directory</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="myconfig.conf file">
  <data key="d5">myconfig.conf file</data>
  <data key="d2">a configuration file stored in a ConfigMap volume</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="subPath">
  <data key="d5">subPath</data>
  <data key="d2">A property used to mount part of a volume instead of the whole volume.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="file permissions">
  <data key="d5">file permissions</data>
  <data key="d2">The default permissions set for files in a ConfigMap volume (644 or -rw-r—r--).</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune-config Config-Map">
  <data key="d5">fortune-config Config-Map</data>
  <data key="d2">A specific ConfigMap used as an example in the chapter.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Nginx config file">
  <data key="d5">Nginx config file</data>
  <data key="d2">A configuration file used by Nginx to determine how to handle requests.</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nginx -s reload">
  <data key="d5">nginx -s reload</data>
  <data key="d2">A command used to signal Nginx to reload its configuration files.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="config file changes">
  <data key="d5">config file changes</data>
  <data key="d2">The ability of an app to detect and respond to changes in its configuration files.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="symbolic links">
  <data key="d5">symbolic links</data>
  <data key="d2">A feature used by Kubernetes to update all files in a configMap volume atomically.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="files">
  <data key="d5">files</data>
  <data key="d2">mounted into existing directories don't get updated</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="directories">
  <data key="d5">directories</data>
  <data key="d2">symbolic links pointing to files in the ..data dir</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="symlinks">
  <data key="d5">symlinks</data>
  <data key="d2">pointing to files in the ..data dir</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ConfigMap-backed volumes">
  <data key="d5">ConfigMap-backed volumes</data>
  <data key="d2">a way to pass configuration data to containers</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="processes">
  <data key="d5">processes</data>
  <data key="d2">new process will also see the new config</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="encryption keys">
  <data key="d5">encryption keys</data>
  <data key="d2">sensitive information that needs to be kept secure</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="credentials">
  <data key="d5">credentials</data>
  <data key="d2">sensitive information that needs to be kept secure</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="files in a volume">
  <data key="d5">files in a volume</data>
  <data key="d2">exposing Secret entries as files in a volume</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="default-token-cfee9">
  <data key="d5">default-token-cfee9</data>
  <data key="d2">Name of a Secret object in the default namespace.</data>
  <data key="d3">object</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="automountServiceAccountToken">
  <data key="d5">automountServiceAccountToken</data>
  <data key="d2">Field in the pod spec that controls whether the default token Secret is mounted into containers.</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="create secret">
  <data key="d5">create secret</data>
  <data key="d2">a command used to create a Secret in a Kubernetes cluster</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ConfigMaps and Secrets">
  <data key="d5">ConfigMaps and Secrets</data>
  <data key="d2">a chapter title in the document</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="create secret tls">
  <data key="d5">create secret tls</data>
  <data key="d2">a command for creating a TLS Secret</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Base64 encoding">
  <data key="d5">Base64 encoding</data>
  <data key="d2">a method for encoding binary data as plain-text</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="binary values">
  <data key="d5">binary values</data>
  <data key="d2">data that is not in plain-text format</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="1MB">
  <data key="d5">1MB</data>
  <data key="d2">the maximum size limit of a Secret</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="stringData field">
  <data key="d5">stringData field</data>
  <data key="d2">a write-only field in a Secret that allows setting values through plain text</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Base64-encoded">
  <data key="d5">Base64-encoded</data>
  <data key="d2">a way to encode binary data for storage</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune-https Secret">
  <data key="d5">fortune-https Secret</data>
  <data key="d2">a Secret containing certificates and keys for HTTPS</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune-config ConfigMap">
  <data key="d5">fortune-config ConfigMap</data>
  <data key="d2">a specific ConfigMap used in the example</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ssl_certificate">
  <data key="d5">ssl_certificate</data>
  <data key="d2">SSL certificate file</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="certs/https.cert">
  <data key="d5">certs/https.cert</data>
  <data key="d2">Path to SSL certificate file</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ssl_certificate_key">
  <data key="d5">ssl_certificate_key</data>
  <data key="d2">SSL key file</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="certs/https.key">
  <data key="d5">certs/https.key</data>
  <data key="d2">Path to SSL key file</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ssl_protocols">
  <data key="d5">ssl_protocols</data>
  <data key="d2">Supported SSL protocols</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TLSv1 TLSv1.1 TLSv1.2">
  <data key="d5">TLSv1 TLSv1.1 TLSv1.2</data>
  <data key="d2">List of supported SSL protocols</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ssl_ciphers">
  <data key="d5">ssl_ciphers</data>
  <data key="d2">Supported SSL ciphers</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HIGH:!aNULL:!MD5">
  <data key="d5">HIGH:!aNULL:!MD5</data>
  <data key="d2">List of supported SSL ciphers</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="location /">
  <data key="d5">location /</data>
  <data key="d2">Nginx configuration directive</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="index.html index.htm">
  <data key="d5">index.html index.htm</data>
  <data key="d2">List of index files</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/etc/nginx/certs">
  <data key="d5">/etc/nginx/certs</data>
  <data key="d2">Path to secret volume holding certificate and key files</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="apiVersion: v1">
  <data key="d5">apiVersion: v1</data>
  <data key="d2">Kubernetes API version</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kind: Pod">
  <data key="d5">kind: Pod</data>
  <data key="d2">Type of Kubernetes resource</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="name: fortune-https">
  <data key="d5">name: fortune-https</data>
  <data key="d2">Name of the pod</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="image: luksa/fortune:env">
  <data key="d5">image: luksa/fortune:env</data>
  <data key="d2">Image for the html-generator container</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="readOnly: true">
  <data key="d5">readOnly: true</data>
  <data key="d2">Flag indicating whether the volume is read-only</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="containerPort: 80">
  <data key="d5">containerPort: 80</data>
  <data key="d2">Exposed port for the web-server container</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="items">
  <data key="d5">items</data>
  <data key="d2">a list of key-value pairs for a configMap</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="mount">
  <data key="d5">mount</data>
  <data key="d2">the process of attaching a volume to a container</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384">
  <data key="d5">TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384</data>
  <data key="d2">encryption algorithm</data>
  <data key="d3">algorithm</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tmpfs">
  <data key="d5">tmpfs</data>
  <data key="d2">in-memory filesystem</data>
  <data key="d3">filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Secret volume">
  <data key="d5">Secret volume</data>
  <data key="d2">mounting a Secret in a container's directory tree</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="FOO_SECRET">
  <data key="d5">FOO_SECRET</data>
  <data key="d2">environment variable exposing the foo key from a Secret</data>
  <data key="d3">variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="secretKeyRef">
  <data key="d5">secretKeyRef</data>
  <data key="d2">a way to refer to a Secret in Kubernetes</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="image registries">
  <data key="d5">image registries</data>
  <data key="d2">repositories that store Docker images</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="private image registry">
  <data key="d5">private image registry</data>
  <data key="d2">an image registry that requires credentials to access</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="entry">
  <data key="d5">entry</data>
  <data key="d2">a single piece of data stored in a Secret</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="docker-registry">
  <data key="d5">docker-registry</data>
  <data key="d2">type of Secret for authenticating with a Docker registry</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id=".dockercfg">
  <data key="d5">.dockercfg</data>
  <data key="d2">file containing Docker Hub credentials</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="username/private:tag">
  <data key="d5">username/private:tag</data>
  <data key="d2">image name and tag for a private Docker Hub repository</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="docker-registry Secret">
  <data key="d5">docker-registry Secret</data>
  <data key="d2">a way to store sensitive data and use it to pull images from a private image registry</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="token Secret">
  <data key="d5">token Secret</data>
  <data key="d2">used to talk to the API server from within a pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes client libraries">
  <data key="d5">Kubernetes client libraries</data>
  <data key="d2">libraries that provide access to the Kubernetes API server</data>
  <data key="d3">library</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod manifest">
  <data key="d5">Pod manifest</data>
  <data key="d2">A YAML or JSON file that defines a pod's configuration, including its metadata and status.</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DownwardAPI volume">
  <data key="d5">DownwardAPI volume</data>
  <data key="d2">A Kubernetes resource that exposes metadata from the pod to the processes running inside it.</data>
  <data key="d3">volume</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="memory requests">
  <data key="d5">memory requests</data>
  <data key="d2">The CPU and memory requests for each container</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU limits">
  <data key="d5">CPU limits</data>
  <data key="d2">The CPU and memory limits for each container</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="memory limits">
  <data key="d5">memory limits</data>
  <data key="d2">The CPU and memory limits for each container</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="status.podIP">
  <data key="d5">status.podIP</data>
  <data key="d2">the IP address of the pod</data>
  <data key="d3">kubernetes</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="spec.nodeName">
  <data key="d5">spec.nodeName</data>
  <data key="d2">the name of the node on which the container is running</data>
  <data key="d3">kubernetes</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="divisor">
  <data key="d5">divisor</data>
  <data key="d2">a divisor used to calculate the value of a resource field</data>
  <data key="d3">kubernetes</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU limits and requests">
  <data key="d5">CPU limits and requests</data>
  <data key="d2">Resource constraints for container execution.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Memory limits/requests">
  <data key="d5">Memory limits/requests</data>
  <data key="d2">Resource constraints for container memory usage.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="POD_NAME">
  <data key="d5">POD_NAME</data>
  <data key="d2">An environment variable that specifies the name of the pod.</data>
  <data key="d3">environmental variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/etc/downward/labels">
  <data key="d5">/etc/downward/labels</data>
  <data key="d2">A file within the container that contains the pod's labels.</data>
  <data key="d3">Filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/etc/downward/annotations">
  <data key="d5">/etc/downward/annotations</data>
  <data key="d2">A file within the container that contains the pod's annotations.</data>
  <data key="d3">Filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/podName">
  <data key="d5">/podName</data>
  <data key="d2">A placeholder for the name of the pod.</data>
  <data key="d3">Kubernetes</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/podNamespace">
  <data key="d5">/podNamespace</data>
  <data key="d2">A placeholder for the namespace of the pod.</data>
  <data key="d3">Kubernetes</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/etc/downward/">
  <data key="d5">/etc/downward/</data>
  <data key="d2">mounted volume directory</data>
  <data key="d3">filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="podName">
  <data key="d5">podName</data>
  <data key="d2">file containing pod name</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="podNamespace">
  <data key="d5">podNamespace</data>
  <data key="d2">file containing pod namespace</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="labels and annotations">
  <data key="d5">labels and annotations</data>
  <data key="d2">metadata fields exposed through downwardAPI volume</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="key=value format">
  <data key="d5">key=value format</data>
  <data key="d2">format of labels and annotations in files</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="&#10;">
  <data key="d5">
</data>
  <data key="d2">newline character used to denote multi-line values</data>
  <data key="d3">character</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="container-level metadata">
  <data key="d5">container-level metadata</data>
  <data key="d2">Information about a container, such as resource limits or requests.</data>
  <data key="d3">metadata</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="containerName">
  <data key="d5">containerName</data>
  <data key="d2">The name of the container whose resource field is being referenced.</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="REST endpoints">
  <data key="d5">REST endpoints</data>
  <data key="d2">Kubernetes API server's REST endpoints</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl cluster-info">
  <data key="d5">kubectl cluster-info</data>
  <data key="d2">Command to get Kubernetes master URL</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="app process">
  <data key="d5">app process</data>
  <data key="d2">Application process running inside a pod</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="proxy">
  <data key="d5">proxy</data>
  <data key="d2">Kubernetes API server proxy</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="authorization token">
  <data key="d5">authorization token</data>
  <data key="d2">token used for authorization with the API server</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/api/v1">
  <data key="d5">/api/v1</data>
  <data key="d2">API path for Kubernetes v1 API group</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/apis/apps">
  <data key="d5">/apis/apps</data>
  <data key="d2">API path for Kubernetes apps API group</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/apis/apps/v1beta1">
  <data key="d5">/apis/apps/v1beta1</data>
  <data key="d2">API path for Kubernetes apps v1beta1 API version</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/apis/batch">
  <data key="d5">/apis/batch</data>
  <data key="d2">API path for Kubernetes batch API group</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/apis/batch/v1">
  <data key="d5">/apis/batch/v1</data>
  <data key="d2">API path for Kubernetes batch v1 API version</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/apis/batch/v2alpha1">
  <data key="d5">/apis/batch/v2alpha1</data>
  <data key="d2">API path for Kubernetes batch v2alpha1 API version</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API groups">
  <data key="d5">API groups</data>
  <data key="d2">Grouping of API resources</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Batch API group">
  <data key="d5">Batch API group</data>
  <data key="d2">API group for batch jobs</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="APIResourceList">
  <data key="d5">APIResourceList</data>
  <data key="d2">API resource list object</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="APIResource">
  <data key="d5">APIResource</data>
  <data key="d2">API resource object</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="v2alpha1">
  <data key="d5">v2alpha1</data>
  <data key="d2">Deprecated version of the batch API group</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="update">
  <data key="d5">update</data>
  <data key="d2">action to update a resource fully</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="jobs/status">
  <data key="d5">jobs/status</data>
  <data key="d2">special REST endpoint for modifying the status of a Job</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="JobList">
  <data key="d5">JobList</data>
  <data key="d2">resource type that represents a list of Jobs</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="REST API server">
  <data key="d5">REST API server</data>
  <data key="d2">The interface through which applications interact with the Kubernetes cluster.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="my-job">
  <data key="d5">my-job</data>
  <data key="d2">The name of a specific Job instance.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tutum/curl image">
  <data key="d5">tutum/curl image</data>
  <data key="d2">A container image containing the curl binary</data>
  <data key="d3">container image</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="port 443">
  <data key="d5">port 443</data>
  <data key="d2">Default port for HTTPS</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SSL certificate">
  <data key="d5">SSL certificate</data>
  <data key="d2">Secure Sockets Layer certificate</data>
  <data key="d3">security</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-k option">
  <data key="d5">-k option</data>
  <data key="d2">Insecure option for curl</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="default-token-xyz">
  <data key="d5">default-token-xyz</data>
  <data key="d2">Automatically created Secret</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--cacert option">
  <data key="d5">--cacert option</data>
  <data key="d2">Option to specify CA certificate</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CURL_CA_BUNDLE">
  <data key="d5">CURL_CA_BUNDLE</data>
  <data key="d2">An environment variable that specifies the CA bundle to use</data>
  <data key="d3">environment variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/var/run/secrets/kubernetes.io/">
  <data key="d5">/var/run/secrets/kubernetes.io/</data>
  <data key="d2">A directory containing secrets and certificates for Kubernetes</data>
  <data key="d3">directory</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="serviceaccount/ca.crt">
  <data key="d5">serviceaccount/ca.crt</data>
  <data key="d2">A certificate used to verify the identity of the API server</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TOKEN environment variable">
  <data key="d5">TOKEN environment variable</data>
  <data key="d2">An environment variable that stores the authentication token</data>
  <data key="d3">environment variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/ui/">
  <data key="d5">/ui/</data>
  <data key="d2">A path in the Kubernetes API server for accessing the UI</data>
  <data key="d3">path</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Authorization HTTP header">
  <data key="d5">Authorization HTTP header</data>
  <data key="d2">A header used to pass a token for authentication</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NS environment variable">
  <data key="d5">NS environment variable</data>
  <data key="d2">An environment variable used to store the namespace of a pod</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TOKEN">
  <data key="d5">TOKEN</data>
  <data key="d2">A token used for authentication with the API server</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PUT or PATCH requests">
  <data key="d5">PUT or PATCH requests</data>
  <data key="d2">HTTP requests used to update API objects</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:serviceaccounts">
  <data key="d5">system:serviceaccounts</data>
  <data key="d2">A group used to grant privileges to service accounts</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API server's certificate">
  <data key="d5">API server's certificate</data>
  <data key="d2">A digital certificate used to authenticate the API server.</data>
  <data key="d3">certificate</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Certificate Authority (CA)">
  <data key="d5">Certificate Authority (CA)</data>
  <data key="d2">An entity responsible for issuing and managing digital certificates.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ca.crt file">
  <data key="d5">ca.crt file</data>
  <data key="d2">A file containing the CA's public key, used to verify the API server's certificate.</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Bearer token">
  <data key="d5">Bearer token</data>
  <data key="d2">An authentication token sent in the Authorization header to authenticate with the API server.</data>
  <data key="d3">token</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Namespace file">
  <data key="d5">Namespace file</data>
  <data key="d2">A file containing the namespace, used to pass it to the API server when performing CRUD operations.</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CRUD (Create, Read, Update, Delete)">
  <data key="d5">CRUD (Create, Read, Update, Delete)</data>
  <data key="d2">A set of HTTP methods used to perform operations on API objects.</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="POST method">
  <data key="d5">POST method</data>
  <data key="d2">An HTTP method used to create a new resource.</data>
  <data key="d3">http-method</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GET method">
  <data key="d5">GET method</data>
  <data key="d2">An HTTP method used to read an existing resource.</data>
  <data key="d3">http-method</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PATCH/PUT method">
  <data key="d5">PATCH/PUT method</data>
  <data key="d2">An HTTP method used to update or replace a resource.</data>
  <data key="d3">http-method</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DELETE method">
  <data key="d5">DELETE method</data>
  <data key="d2">An HTTP method used to delete a resource.</data>
  <data key="d3">http-method</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Ambassador containers">
  <data key="d5">Ambassador containers</data>
  <data key="d2">Containers that simplify API server communication by handling HTTPS, certificates, and authentication tokens.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Server certificate">
  <data key="d5">Server certificate</data>
  <data key="d2">A digital certificate used to authenticate the API server.</data>
  <data key="d3">certificate</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Default token secret volume">
  <data key="d5">Default token secret volume</data>
  <data key="d2">A file containing the default token, used to authenticate with the API server.</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl proxy command">
  <data key="d5">kubectl proxy command</data>
  <data key="d2">a command used to access the API server through a proxy</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ambassador container pattern">
  <data key="d5">ambassador container pattern</data>
  <data key="d2">a design pattern that uses an ambassador container to proxy requests to the API server</data>
  <data key="d3">pattern</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl-proxy container image">
  <data key="d5">kubectl-proxy container image</data>
  <data key="d2">a general-purpose container image used as an ambassador container</data>
  <data key="d3">image</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HTTP">
  <data key="d5">HTTP</data>
  <data key="d2">the protocol used for communication between containers in a pod</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API server proxy">
  <data key="d5">API server proxy</data>
  <data key="d2">Component of Kubernetes that provides a proxy to the API server</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="port 8001">
  <data key="d5">port 8001</data>
  <data key="d2">Default port used by kubectl proxy</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes API client libraries">
  <data key="d5">Kubernetes API client libraries</data>
  <data key="d2">Existing Kubernetes API client libraries supported by the API Machinery special interest group (SIG)</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Golang client">
  <data key="d5">Golang client</data>
  <data key="d2">Kubernetes API client library for Golang</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Python client">
  <data key="d5">Python client</data>
  <data key="d2">Kubernetes API client library for Python</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Java client by Fabric8">
  <data key="d5">Java client by Fabric8</data>
  <data key="d2">User-contributed Kubernetes API client library for Java by Fabric8</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Java client by Amdatu">
  <data key="d5">Java client by Amdatu</data>
  <data key="d2">User-contributed Kubernetes API client library for Java by Amdatu</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Node.js client by tenxcloud">
  <data key="d5">Node.js client by tenxcloud</data>
  <data key="d2">User-contributed Kubernetes API client library for Node.js by tenxcloud</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Node.js client by GoDaddy">
  <data key="d5">Node.js client by GoDaddy</data>
  <data key="d2">User-contributed Kubernetes API client library for Node.js by GoDaddy</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PHP client">
  <data key="d5">PHP client</data>
  <data key="d2">User-contributed Kubernetes API client library for PHP</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Another PHP client">
  <data key="d5">Another PHP client</data>
  <data key="d2">User-contributed Kubernetes API client library for PHP</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Ruby">
  <data key="d5">Ruby</data>
  <data key="d2">A programming language</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="https://github.com/Ch00k/kubr">
  <data key="d5">https://github.com/Ch00k/kubr</data>
  <data key="d2">A Ruby client library for Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Another Ruby client">
  <data key="d5">Another Ruby client</data>
  <data key="d2">A Ruby client library for Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="https://github.com/abonas/kubeclient">
  <data key="d5">https://github.com/abonas/kubeclient</data>
  <data key="d2">A Ruby client library for Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Clojure">
  <data key="d5">Clojure</data>
  <data key="d2">A programming language</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="https://github.com/yanatan16/clj-kubernetes-api">
  <data key="d5">https://github.com/yanatan16/clj-kubernetes-api</data>
  <data key="d2">A Clojure client library for Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Scala">
  <data key="d5">Scala</data>
  <data key="d2">A programming language</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="https://github.com/doriordan/skuber">
  <data key="d5">https://github.com/doriordan/skuber</data>
  <data key="d2">A Scala client library for Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Perl">
  <data key="d5">Perl</data>
  <data key="d2">A programming language</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="https://metacpan.org/pod/Net::Kubernetes">
  <data key="d5">https://metacpan.org/pod/Net::Kubernetes</data>
  <data key="d2">A Perl client library for Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Fabric8 Java Client">
  <data key="d5">Fabric8 Java Client</data>
  <data key="d2">A Java client library for Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="io.fabric8.kubernetes.api.model.Pod">
  <data key="d5">io.fabric8.kubernetes.api.model.Pod</data>
  <data key="d2">A Kubernetes Pod object</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="io.fabric8.kubernetes.api.model.PodList">
  <data key="d5">io.fabric8.kubernetes.api.model.PodList</data>
  <data key="d2">A list of Kubernetes Pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DefaultKubernetesClient">
  <data key="d5">DefaultKubernetesClient</data>
  <data key="d2">A default Kubernetes client</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="KubernetesClient">
  <data key="d5">KubernetesClient</data>
  <data key="d2">A Kubernetes client interface</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="inNamespace">
  <data key="d5">inNamespace</data>
  <data key="d2">A method to get pods in a specific namespace</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="getItems">
  <data key="d5">getItems</data>
  <data key="d2">A method to get the items in a pod list</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="stream">
  <data key="d5">stream</data>
  <data key="d2">A method to stream the results of an operation</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="forEach">
  <data key="d5">forEach</data>
  <data key="d2">A method to iterate over a collection</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="getMetadata">
  <data key="d5">getMetadata</data>
  <data key="d2">A method to get the metadata of an object</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="getName">
  <data key="d5">getName</data>
  <data key="d2">A method to get the name of an object</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="createNew">
  <data key="d5">createNew</data>
  <data key="d2">A method to create a new pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="addToLabels">
  <data key="d5">addToLabels</data>
  <data key="d2">A method to add labels to a pod</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="endMetadata">
  <data key="d5">endMetadata</data>
  <data key="d2">A method to end metadata for a pod</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="done">
  <data key="d5">done</data>
  <data key="d2">A method to indicate the completion of an operation</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="System.out.println">
  <data key="d5">System.out.println</data>
  <data key="d2">A statement to print output to the console</data>
  <data key="d3">statement</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Thread.sleep">
  <data key="d5">Thread.sleep</data>
  <data key="d2">A method to pause execution for a specified time</data>
  <data key="d3">method</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="client.pods().inNamespace">
  <data key="d5">client.pods().inNamespace</data>
  <data key="d2">A method to access pods in a specific namespace</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Swagger API">
  <data key="d5">Swagger API</data>
  <data key="d2">An API framework for generating client libraries and documentation</data>
  <data key="d3">framework</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="OpenAPI spec">
  <data key="d5">OpenAPI spec</data>
  <data key="d2">A specification for defining APIs in a machine-readable format</data>
  <data key="d3">specification</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Swagger UI">
  <data key="d5">Swagger UI</data>
  <data key="d2">A web-based interface for exploring and interacting with REST APIs</data>
  <data key="d3">interface</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU requests and limits">
  <data key="d5">CPU requests and limits</data>
  <data key="d2">CPU requests and limits refer to the amount of CPU resources allocated to a pod.</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="memory requests and limits">
  <data key="d5">memory requests and limits</data>
  <data key="d2">Memory requests and limits refer to the amount of memory resources allocated to a pod.</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="client libraries">
  <data key="d5">client libraries</data>
  <data key="d2">Client libraries are pre-built libraries that provide a simple interface for interacting with Kubernetes.</data>
  <data key="d3">library</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="config data">
  <data key="d5">config data</data>
  <data key="d2">secret and non-secret configuration data passed to pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="microservices">
  <data key="d5">microservices</data>
  <data key="d2">smaller components that make up a full-fledged system</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Deployment resources">
  <data key="d5">Deployment resources</data>
  <data key="d2">Kubernetes resources used by Deployments to manage applications</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="rolling updates">
  <data key="d5">rolling updates</data>
  <data key="d2">the process of updating pods with newer versions while minimizing downtime</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="rollouts">
  <data key="d5">rollouts</data>
  <data key="d2">the process of deploying new versions of an application</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="reverting">
  <data key="d5">reverting</data>
  <data key="d2">the process of reverting pods to a previous version</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Downtime">
  <data key="d5">Downtime</data>
  <data key="d2">Short period of time when the application is not available due to updates</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="set selector">
  <data key="d5">set selector</data>
  <data key="d2">A command used to update the pod selector of a Service resource.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NodeJS">
  <data key="d5">NodeJS</data>
  <data key="d2">programming language and runtime environment</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="get svc kubia">
  <data key="d5">get svc kubia</data>
  <data key="d2">kubectl command to retrieve information about a service named 'kubia'</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cluster-ip">
  <data key="d5">cluster-ip</data>
  <data key="d2">unique IP address assigned to a Service within a Kubernetes cluster</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="external-ip">
  <data key="d5">external-ip</data>
  <data key="d2">publicly accessible IP address of a Service, used for accessing the application from outside the cluster</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="IfNotPresent">
  <data key="d5">IfNotPresent</data>
  <data key="d2">imagePullPolicy value that only pulls an image if it's not already present in the local cache</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="response.end()">
  <data key="d5">response.end()</data>
  <data key="d2">method for sending a response back to the client, used in Node.js applications</data>
  <data key="d3">programming language</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-v2">
  <data key="d5">kubia-v2</data>
  <data key="d2">The image name for the kubia app version 2.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--v option">
  <data key="d5">--v option</data>
  <data key="d2">A kubectl flag that enables verbose logging</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes master">
  <data key="d5">Kubernetes master</data>
  <data key="d2">the central component of a Kubernetes cluster</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="verbose logging option">
  <data key="d5">verbose logging option</data>
  <data key="d2">an option for running kubectl commands with detailed logging</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-deployment-v1.yaml">
  <data key="d5">kubia-deployment-v1.yaml</data>
  <data key="d2">A YAML manifest that defines a Deployment resource.</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia Service">
  <data key="d5">kubia Service</data>
  <data key="d2">A service that provides access to the kubia app</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--record">
  <data key="d5">--record</data>
  <data key="d2">An option used to record the command in the revision history</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl get deployment">
  <data key="d5">kubectl get deployment</data>
  <data key="d2">A command used to see details of the Deployment</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl rollout status deployment kubia">
  <data key="d5">kubectl rollout status deployment kubia</data>
  <data key="d2">A command used to check the status of a Deployment's rollout</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-1506449474-otnnh">
  <data key="d5">kubia-1506449474-otnnh</data>
  <data key="d2">A pod created by the Deployment</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-1506449474-vmn7s">
  <data key="d5">kubia-1506449474-vmn7s</data>
  <data key="d2">A pod created by the Deployment</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-1506449474-xis6m">
  <data key="d5">kubia-1506449474-xis6m</data>
  <data key="d2">A pod created by the Deployment</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Recreate strategy">
  <data key="d5">Recreate strategy</data>
  <data key="d2">A deployment strategy that deletes all old pods at once and then creates new ones, similar to modifying a ReplicationController's pod template.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="RollingUpdate strategy">
  <data key="d5">RollingUpdate strategy</data>
  <data key="d2">The default deployment strategy that performs a rolling update by replacing one or more replicas with new ones while keeping the overall number of replicas constant.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl patch command">
  <data key="d5">kubectl patch command</data>
  <data key="d2">A command for modifying a single property or limited number of properties of a resource without editing its definition in a text editor</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="minReadySeconds attribute">
  <data key="d5">minReadySeconds attribute</data>
  <data key="d2">An attribute that can be set on a Deployment to slow down the update process</data>
  <data key="d3">attribute</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Deployment spec">
  <data key="d5">Deployment spec</data>
  <data key="d2">The specification of a Deployment, which includes properties like desired replica count and deployment strategy</data>
  <data key="d3">specification</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl set image command">
  <data key="d5">kubectl set image command</data>
  <data key="d2">A command for changing the image used in a container (like Deployments)</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nodejs container">
  <data key="d5">nodejs container</data>
  <data key="d2">A container that runs the Node.js application</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Image registry">
  <data key="d5">Image registry</data>
  <data key="d2">A place where images are stored</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl patch">
  <data key="d5">kubectl patch</data>
  <data key="d2">A command to modify individual properties of an object</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl replace">
  <data key="d5">kubectl replace</data>
  <data key="d2">A command to replace an object with a new one from a YAML/JSON file</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-deployment-v2.yaml">
  <data key="d5">kubia-deployment-v2.yaml</data>
  <data key="d2">A YAML file containing the full definition of a resource</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="v1 pods">
  <data key="d5">v1 pods</data>
  <data key="d2">pods running the v1 version of an app</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="v2 pods">
  <data key="d5">v2 pods</data>
  <data key="d2">pods running the v2 version of an app</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicaSet v1">
  <data key="d5">ReplicaSet v1</data>
  <data key="d2">an instance of ReplicaSet running the v1 version</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicaSet v2">
  <data key="d5">ReplicaSet v2</data>
  <data key="d2">an instance of ReplicaSet running the v2 version</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="requestCount">
  <data key="d5">requestCount</data>
  <data key="d2">A variable that keeps track of the number of requests</data>
  <data key="d3">variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="500 error">
  <data key="d5">500 error</data>
  <data key="d2">An internal server error with an HTTP status code of 500</data>
  <data key="d3">error</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-1914148340-lalmx">
  <data key="d5">kubia-1914148340-lalmx</data>
  <data key="d2">A specific pod name.</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="internal error">
  <data key="d5">internal error</data>
  <data key="d2">An error that occurs within the application.</data>
  <data key="d3">error</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl rollout undo">
  <data key="d5">kubectl rollout undo</data>
  <data key="d2">The command to roll back a Deployment.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="rollout history">
  <data key="d5">rollout history</data>
  <data key="d2">displaying a deployment's rollout history using kubectl command</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--record command-line option">
  <data key="d5">--record command-line option</data>
  <data key="d2">option for recording the cause of changes in Deployments</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CHANGE-CAUSE column">
  <data key="d5">CHANGE-CAUSE column</data>
  <data key="d2">column in the revision history that shows the reason for each change</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="revisionHistoryLimit property">
  <data key="d5">revisionHistoryLimit property</data>
  <data key="d2">property on the Deployment resource that limits the length of the revision history</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicaSet list">
  <data key="d5">ReplicaSet list</data>
  <data key="d2">list of ReplicaSets associated with a Deployment</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="revision history">
  <data key="d5">revision history</data>
  <data key="d2">history of changes to a Deployment, stored in ReplicaSets</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--to-revision option">
  <data key="d5">--to-revision option</data>
  <data key="d2">option for specifying the revision to roll back to</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="undo command">
  <data key="d5">undo command</data>
  <data key="d2">command for rolling back a Deployment to a previous revision</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="revisionHistoryLimit">
  <data key="d5">revisionHistoryLimit</data>
  <data key="d2">A property that controls the number of revisions kept by a Deployment</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="apps/v1beta2">
  <data key="d5">apps/v1beta2</data>
  <data key="d2">A version of Deployments</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="v1beta1">
  <data key="d5">v1beta1</data>
  <data key="d2">version of Deployments</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="extensions">
  <data key="d5">extensions</data>
  <data key="d2">API group in Kubernetes</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Desired replica count">
  <data key="d5">Desired replica count</data>
  <data key="d2">number of desired replicas for a deployment</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="resume">
  <data key="d5">resume</data>
  <data key="d2">A command used to resume a paused Deployment and start the rollout process.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl apply command">
  <data key="d5">kubectl apply command</data>
  <data key="d2">A command used to update the deployment with a readiness probe</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="readinessProbe">
  <data key="d5">readinessProbe</data>
  <data key="d2">A mechanism to check if a pod is ready to serve traffic</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-deployment-v3-with-readinesscheck.yaml">
  <data key="d5">kubia-deployment-v3-with-readinesscheck.yaml</data>
  <data key="d2">A YAML file containing the deployment configuration</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="periodSeconds">
  <data key="d5">periodSeconds</data>
  <data key="d2">The interval at which a readiness probe is executed</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ProgressDeadlineExceeded">
  <data key="d5">ProgressDeadlineExceeded</data>
  <data key="d2">A condition that is displayed when the rollout can't make progress within the specified deadline.</data>
  <data key="d3">condition</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="progressDeadlineSeconds">
  <data key="d5">progressDeadlineSeconds</data>
  <data key="d2">A property in the Deployment spec that sets the time after which a Deployment is considered failed if it can't make progress.</data>
  <data key="d3">property</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl rollout undo deployment">
  <data key="d5">kubectl rollout undo deployment</data>
  <data key="d2">A command used to abort a bad rollout by rolling back the Deployment.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS SRV records">
  <data key="d5">DNS SRV records</data>
  <data key="d2">discovering peers through DNS SRV records</data>
  <data key="d3">network,process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Storage volume">
  <data key="d5">Storage volume</data>
  <data key="d2">Persistent storage resource for a pod</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicaSet A1">
  <data key="d5">ReplicaSet A1</data>
  <data key="d2">a ReplicaSet with 1 instance</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod A1-xyz">
  <data key="d5">Pod A1-xyz</data>
  <data key="d2">a pod with a unique identifier xyz</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PVC A2">
  <data key="d5">PVC A2</data>
  <data key="d2">Persistent Volume Claim for replica set A2</data>
  <data key="d3">storage</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PV A2">
  <data key="d5">PV A2</data>
  <data key="d2">Persistent Volume for replica set A2</data>
  <data key="d3">storage</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicaSet A2">
  <data key="d5">ReplicaSet A2</data>
  <data key="d2">a ReplicaSet with 1 instance</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod A2-xzy">
  <data key="d5">Pod A2-xzy</data>
  <data key="d2">a pod with a unique identifier xzy</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PVC A3">
  <data key="d5">PVC A3</data>
  <data key="d2">Persistent Volume Claim for replica set A3</data>
  <data key="d3">storage</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PV A3">
  <data key="d5">PV A3</data>
  <data key="d2">Persistent Volume for replica set A3</data>
  <data key="d3">storage</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicaSet A3">
  <data key="d5">ReplicaSet A3</data>
  <data key="d2">a ReplicaSet with 1 instance</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod A3-zyx">
  <data key="d5">Pod A3-zyx</data>
  <data key="d2">a pod with a unique identifier zyx</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Stateful pods">
  <data key="d5">Stateful pods</data>
  <data key="d2">a type of pod that requires a stable name and state</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pets vs. Cattle analogy">
  <data key="d5">Pets vs. Cattle analogy</data>
  <data key="d2">a metaphor for treating app instances as either pets or cattle</data>
  <data key="d3">concept</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Stateless apps">
  <data key="d5">Stateless apps</data>
  <data key="d2">applications where instances can be replaced without affecting the overall application</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Stateful apps">
  <data key="d5">Stateful apps</data>
  <data key="d2">applications where instances have a stable name and state, requiring special treatment when an instance dies</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="headless Service">
  <data key="d5">headless Service</data>
  <data key="d2">A Service that provides network identity without creating a load balancer.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS entry">
  <data key="d5">DNS entry</data>
  <data key="d2">A record in the DNS system that maps a hostname to an IP address.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="foo.default.svc.cluster.local">
  <data key="d5">foo.default.svc.cluster.local</data>
  <data key="d2">domain for StatefulSet's pods' names</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="a-0.foo.default.svc.cluster.local">
  <data key="d5">a-0.foo.default.svc.cluster.local</data>
  <data key="d2">fully qualified domain name of Pod A-0</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="StatefulSets scale down">
  <data key="d5">StatefulSets scale down</data>
  <data key="d2">The process of scaling down a StatefulSet</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="distributed data store">
  <data key="d5">distributed data store</data>
  <data key="d2">A type of database that stores data across multiple nodes</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="data entry">
  <data key="d5">data entry</data>
  <data key="d2">A single unit of data stored in a database</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentVolume-Claims">
  <data key="d5">PersistentVolume-Claims</data>
  <data key="d2">allow persistent storage to be attached to a pod by referencing the PersistentVolumeClaim in the pod by name</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="volume claim templates">
  <data key="d5">volume claim templates</data>
  <data key="d2">enable a StatefulSet to stamp out PersistentVolumeClaims along with each pod instance</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="replicas field">
  <data key="d5">replicas field</data>
  <data key="d2">field in a StatefulSet that determines how many replicas to create</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PVC A-0">
  <data key="d5">PVC A-0</data>
  <data key="d2">PersistentVolumeClaim for pod A-0</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PVC A-1">
  <data key="d5">PVC A-1</data>
  <data key="d2">PersistentVolumeClaim for pod A-1</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod A-1">
  <data key="d5">Pod A-1</data>
  <data key="d2">stateful pod that runs a stateful application, associated with PVC A-1 and PV</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PVC A-2">
  <data key="d5">PVC A-2</data>
  <data key="d2">PersistentVolumeClaim for pod A-2</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod A-2">
  <data key="d5">Pod A-2</data>
  <data key="d2">stateful pod that runs a stateful application, associated with PVC A-2 and PV</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="scale-up">
  <data key="d5">scale-up</data>
  <data key="d2">The process of increasing the number of replicas in a Kubernetes resource.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Data store">
  <data key="d5">Data store</data>
  <data key="d2">a simple clustered data store used as an example in the chapter</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubia app">
  <data key="d5">Kubia app</data>
  <data key="d2">the starting point for building a stateful application</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fs.createWriteStream()">
  <data key="d5">fs.createWriteStream()</data>
  <data key="d2">a function used to create a write stream for writing data to a file</data>
  <data key="d3">programming language</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="request.pipe()">
  <data key="d5">request.pipe()</data>
  <data key="d2">a method used to pipe the request body into a write stream</data>
  <data key="d3">programming language</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fs.readFileSync()">
  <data key="d5">fs.readFileSync()</data>
  <data key="d2">a function used to read data from a file synchronously</data>
  <data key="d3">programming language</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fileExists()">
  <data key="d5">fileExists()</data>
  <data key="d2">a function used to check if a file exists</data>
  <data key="d3">programming language</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node:7">
  <data key="d5">node:7</data>
  <data key="d2">A Docker base image based on Node.js version 7.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="http.createServer(handler)">
  <data key="d5">http.createServer(handler)</data>
  <data key="d2">A Node.js function that creates an HTTP server and sets up a handler function.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="listen(8080)">
  <data key="d5">listen(8080)</data>
  <data key="d2">A method call to start the HTTP server listening on port 8080.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gcloud compute disks create">
  <data key="d5">gcloud compute disks create</data>
  <data key="d2">A command to create a Google Compute Engine persistent disk.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="List">
  <data key="d5">List</data>
  <data key="d2">object and listing the resources as items of the object</data>
  <data key="d3">data structure</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NFS (Network File System)">
  <data key="d5">NFS (Network File System)</data>
  <data key="d2">volume type</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pv-a, pv-b, and pv-c">
  <data key="d5">pv-a, pv-b, and pv-c</data>
  <data key="d2">PersistentVolumes names</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="1 Mi">
  <data key="d5">1 Mi</data>
  <data key="d2">capacity of each persistent volume</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="volumeClaimTemplates">
  <data key="d5">volumeClaimTemplates</data>
  <data key="d2">a list of templates used to create PersistentVolumeClaims for pods in a StatefulSet</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ kubectl create -f kubia-statefulset.yaml ">
  <data key="d5">$ kubectl create -f kubia-statefulset.yaml </data>
  <data key="d2">command to create a StatefulSet</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-statefulset.yaml">
  <data key="d5">kubia-statefulset.yaml</data>
  <data key="d2">configuration file for the StatefulSet</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ kubectl get po kubia-0 -o yaml">
  <data key="d5">$ kubectl get po kubia-0 -o yaml</data>
  <data key="d2">command to view the pod spec in YAML format</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/var/data">
  <data key="d5">/var/data</data>
  <data key="d2">The mount path for the data volume.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="default-token-r2m41">
  <data key="d5">default-token-r2m41</data>
  <data key="d2">A secret that provides authentication tokens for the StatefulSet.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="get pvc">
  <data key="d5">get pvc</data>
  <data key="d2">A command used to list PersistentVolumeClaims.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="data-kubia-0">
  <data key="d5">data-kubia-0</data>
  <data key="d2">A PersistentVolumeClaim created by the StatefulSet.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pv-c">
  <data key="d5">pv-c</data>
  <data key="d2">A Persistent Volume object associated with the data-kubia-0 PVC.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pv-a">
  <data key="d5">pv-a</data>
  <data key="d2">A Persistent Volume object associated with another PVC.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/api/v1/namespaces/default/pods/kubia-0/proxy/&lt;path&gt;">
  <data key="d5">/api/v1/namespaces/default/pods/kubia-0/proxy/&lt;path&gt;</data>
  <data key="d2">A URL used to proxy connections directly to a pod through the API server.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-0 pod">
  <data key="d5">kubia-0 pod</data>
  <data key="d2">a container running a simple web application</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API server host and port">
  <data key="d5">API server host and port</data>
  <data key="d2">the address of the API server</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-L option">
  <data key="d5">-L option</data>
  <data key="d2">an option for curl to follow redirects</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl proxy and API server proxy">
  <data key="d5">kubectl proxy and API server proxy</data>
  <data key="d2">two different proxies used to access the pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/">
  <data key="d5">localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/</data>
  <data key="d2">The URL for accessing the kubia-0 pod's proxy service.</data>
  <data key="d3">url</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DELETING A STATEFUL POD">
  <data key="d5">DELETING A STATEFUL POD</data>
  <data key="d2">A process of deleting a StatefulSet pod and verifying its behavior.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="reschedule">
  <data key="d5">reschedule</data>
  <data key="d2">The process of scheduling a new pod to replace an existing one.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="persistent data">
  <data key="d5">persistent data</data>
  <data key="d2">data stored on a persistent volume</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ordinal number">
  <data key="d5">ordinal number</data>
  <data key="d2">the order of a pod in a StatefulSet</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ClusterIP Service">
  <data key="d5">ClusterIP Service</data>
  <data key="d2">a type of Service that is only accessible from within the cluster</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="StatefulSet peer discovery">
  <data key="d5">StatefulSet peer discovery</data>
  <data key="d2">the ability to find other members of a StatefulSet within the cluster</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SRV record">
  <data key="d5">SRV record</data>
  <data key="d2">Used to point to hostnames and ports of servers providing a specific service.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="A record">
  <data key="d5">A record</data>
  <data key="d2">Used to map a hostname to an IP address.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="MX record">
  <data key="d5">MX record</data>
  <data key="d2">Used to specify the mail server responsible for a domain.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dig DNS lookup tool">
  <data key="d5">dig DNS lookup tool</data>
  <data key="d2">A command-line tool used to perform DNS lookups.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tutum/dnsutils image">
  <data key="d5">tutum/dnsutils image</data>
  <data key="d2">A Docker image containing the DNS utilities.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dns.resolveSrv() function">
  <data key="d5">dns.resolveSrv() function</data>
  <data key="d2">A Node.js function used to perform an SRV DNS lookup.</data>
  <data key="d3">function</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Stone Age data store">
  <data key="d5">Stone Age data store</data>
  <data key="d2">An unclustered, independent data storage system</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-public Service">
  <data key="d5">kubia-public Service</data>
  <data key="d2">A Kubernetes service that exposes the kubia application to clients</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dns.resolveSrv">
  <data key="d5">dns.resolveSrv</data>
  <data key="d2">A function in Node.js that performs a DNS SRV record lookup</data>
  <data key="d3">library</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fs.readFileSync">
  <data key="d5">fs.readFileSync</data>
  <data key="d2">A function in Node.js that reads a file synchronously</data>
  <data key="d3">library</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="os.hostname">
  <data key="d5">os.hostname</data>
  <data key="d2">A property of the os module in Node.js that returns the hostname of the machine</data>
  <data key="d3">library</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="request">
  <data key="d5">request</data>
  <data key="d2">An object representing an HTTP request</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="response">
  <data key="d5">response</data>
  <data key="d2">An object representing an HTTP response</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fileExists">
  <data key="d5">fileExists</data>
  <data key="d2">A function in the kubia-pet-peers-image/app.js file that checks if a file exists</data>
  <data key="d3">library</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="addresses.forEach">
  <data key="d5">addresses.forEach</data>
  <data key="d2">iterating over a list of addresses</data>
  <data key="d3">javascript</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="requestOptions">
  <data key="d5">requestOptions</data>
  <data key="d2">object containing options for the HTTP request</data>
  <data key="d3">javascript</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="returnedData">
  <data key="d5">returnedData</data>
  <data key="d2">data returned from the HTTP GET request</data>
  <data key="d3">javascript</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="numResponses">
  <data key="d5">numResponses</data>
  <data key="d2">counter for tracking the number of responses received</data>
  <data key="d3">javascript</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="response.write">
  <data key="d5">response.write</data>
  <data key="d2">method for writing output to a response object</data>
  <data key="d3">javascript</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="docker.io/luksa/kubia-pet-peers">
  <data key="d5">docker.io/luksa/kubia-pet-peers</data>
  <data key="d2">container image URL</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GET /data">
  <data key="d5">GET /data</data>
  <data key="d2">HTTP request method and path</data>
  <data key="d3">http</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="spec.template.spec.containers.image">
  <data key="d5">spec.template.spec.containers.image</data>
  <data key="d2">A field in the StatefulSet definition that specifies the image to use for each replica.</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="StatefulSet updateStrategy">
  <data key="d5">StatefulSet updateStrategy</data>
  <data key="d2">A field in the StatefulSet definition that specifies how to update the set of replicas.</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="POST">
  <data key="d5">POST</data>
  <data key="d2">An HTTP method used to create or update resources on the server.</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/">
  <data key="d5">localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/</data>
  <data key="d2">A URL that specifies the endpoint for sending requests to a service in a cluster.</data>
  <data key="d3">url</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod kubia-0">
  <data key="d5">pod kubia-0</data>
  <data key="d2">A specific pod instance running on a node in the cluster.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod kubia-1">
  <data key="d5">pod kubia-1</data>
  <data key="d2">A specific pod instance running on a node in the cluster.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/kubia-public/proxy/">
  <data key="d5">/kubia-public/proxy/</data>
  <data key="d2">service endpoint</data>
  <data key="d3">endpoint</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-0.kubia.default.svc.cluster.local">
  <data key="d5">kubia-0.kubia.default.svc.cluster.local</data>
  <data key="d2">pod name</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-1.kubia.default.svc.cluster.local">
  <data key="d5">kubia-1.kubia.default.svc.cluster.local</data>
  <data key="d2">pod name</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-2.kubia.default.svc.cluster.local">
  <data key="d5">kubia-2.kubia.default.svc.cluster.local</data>
  <data key="d2">pod name</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node failures">
  <data key="d5">node failures</data>
  <data key="d2">An event where a node in the Kubernetes cluster fails or becomes unavailable.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ sudo ifconfig eth0 down">
  <data key="d5">$ sudo ifconfig eth0 down</data>
  <data key="d2">A Linux command that shuts down a network interface.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NotReady">
  <data key="d5">NotReady</data>
  <data key="d2">A status that the control plane assigns to a node when it is no longer receiving updates from the node.</data>
  <data key="d3">status</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ kubectl get node">
  <data key="d5">$ kubectl get node</data>
  <data key="d2">A Kubernetes command that lists all nodes in the cluster.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Unknown">
  <data key="d5">Unknown</data>
  <data key="d2">A status that the control plane assigns to a pod when it is no longer receiving updates from the node.</data>
  <data key="d3">status</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="evicted">
  <data key="d5">evicted</data>
  <data key="d2">An event where a pod is automatically deleted by the master due to its unknown status for an extended period.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gke-kubia-default-pool-32a2cac8-m0g1">
  <data key="d5">gke-kubia-default-pool-32a2cac8-m0g1</data>
  <data key="d2">name of the node being examined</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NodeLost">
  <data key="d5">NodeLost</data>
  <data key="d2">reason for pod termination due to unresponsive node</data>
  <data key="d3">error</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Terminating">
  <data key="d5">Terminating</data>
  <data key="d2">status of the pod being deleted</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod &quot;kubia-0&quot; deleted">
  <data key="d5">pod "kubia-0" deleted</data>
  <data key="d2">output of kubectl delete command indicating successful deletion</data>
  <data key="d3">output</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--force">
  <data key="d5">--force</data>
  <data key="d2">The --force option tells kubectl to delete the pod immediately, without waiting for confirmation from the Kubelet.</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--grace-period">
  <data key="d5">--grace-period</data>
  <data key="d2">The --grace-period option specifies how long to wait before deleting a pod.</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GCE web console">
  <data key="d5">GCE web console</data>
  <data key="d2">The GCE (Google Compute Engine) web console is a graphical interface for managing Google Cloud resources.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="host names">
  <data key="d5">host names</data>
  <data key="d2">connect to other members through their host names</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Deployment object">
  <data key="d5">Deployment object</data>
  <data key="d2">Resource that represents a set of replicas</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Running pod">
  <data key="d5">Running pod</data>
  <data key="d2">Pod that is currently executing</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Network between pods">
  <data key="d5">Network between pods</data>
  <data key="d2">Communication mechanism between pods</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes Services">
  <data key="d5">Kubernetes Services</data>
  <data key="d2">Resource that provides a network identity and load balancing for accessing applications</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="High-availability">
  <data key="d5">High-availability</data>
  <data key="d2">Capability to ensure continuous operation of an application or system</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="etcd distributed persistent storage">
  <data key="d5">etcd distributed persistent storage</data>
  <data key="d2">A component that stores and manages the state of the cluster.</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes Service Proxy (kube-proxy)">
  <data key="d5">Kubernetes Service Proxy (kube-proxy)</data>
  <data key="d2">A component that provides network connectivity to services running in the cluster.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container Runtime">
  <data key="d5">Container Runtime</data>
  <data key="d2">A component that runs containers on worker nodes, such as Docker or rkt.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes DNS server">
  <data key="d5">Kubernetes DNS server</data>
  <data key="d2">A component that provides DNS resolution for services running in the cluster.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container Network Interface network plugin">
  <data key="d5">Container Network Interface network plugin</data>
  <data key="d2">A component that provides networking capabilities for containers running in the cluster.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="attach">
  <data key="d5">attach</data>
  <data key="d2">kubectl command to attach to a running container</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="componentstatuses">
  <data key="d5">componentstatuses</data>
  <data key="d2">API resource that shows the health status of each Control Plane component</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Worker node(s)">
  <data key="d5">Worker node(s)</data>
  <data key="d2">Nodes that run Kubelet and other components</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="etcd instance">
  <data key="d5">etcd instance</data>
  <data key="d2">can be run multiple times for high availability and better performance</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="optimistic locking system">
  <data key="d5">optimistic locking system</data>
  <data key="d2">a method where data includes a version number to prevent concurrent updates</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="metadata.resourceVersion field">
  <data key="d5">metadata.resourceVersion field</data>
  <data key="d2">a field that clients need to pass back to the API server when updating an object</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/registry">
  <data key="d5">/registry</data>
  <data key="d2">the top-level directory where Kubernetes stores all its data in etcd</data>
  <data key="d3">directory</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="daemonsets">
  <data key="d5">daemonsets</data>
  <data key="d2">a type of resource stored in etcd under /registry/daemonsets</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="deployments">
  <data key="d5">deployments</data>
  <data key="d2">a type of resource stored in etcd under /registry/deployments</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/registry/pods">
  <data key="d5">/registry/pods</data>
  <data key="d2">Directory in etcd that stores pod information</data>
  <data key="d3">directory</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/registry/pods/default">
  <data key="d5">/registry/pods/default</data>
  <data key="d2">Subdirectory of /registry/pods that stores default namespace pods</data>
  <data key="d3">directory</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-159041347-wt6ga">
  <data key="d5">kubia-159041347-wt6ga</data>
  <data key="d2">Pod name and identifier</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="RAFT consensus algorithm">
  <data key="d5">RAFT consensus algorithm</data>
  <data key="d2">Consensus protocol used by etcd to ensure consistency across nodes</data>
  <data key="d3">algorithm</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Control Plane components">
  <data key="d5">Control Plane components</data>
  <data key="d2">Components that manage the Kubernetes cluster, including the API server and etcd</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="quorum">
  <data key="d5">quorum</data>
  <data key="d2">Majority of nodes required for a state change in an etcd cluster</data>
  <data key="d3">concept</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="state changes">
  <data key="d5">state changes</data>
  <data key="d2">Updates made to the cluster state by clients or other nodes</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="RESTful API">
  <data key="d5">RESTful API</data>
  <data key="d2">an architectural style for designing networked applications, emphasizing simplicity and scalability</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="majority">
  <data key="d5">majority</data>
  <data key="d2">the minimum number of etcd instances required for a quorum, ensuring the cluster can transition to a new state</data>
  <data key="d3">concept</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CRUD interface">
  <data key="d5">CRUD interface</data>
  <data key="d2">a set of operations (Create, Read, Update, Delete) provided by the Kubernetes API server for querying and modifying cluster state</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="optimistic locking">
  <data key="d5">optimistic locking</data>
  <data key="d2">a mechanism used by the Kubernetes API server to prevent changes to an object from being overridden by other clients in concurrent updates</data>
  <data key="d3">concept</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="resource validation">
  <data key="d5">resource validation</data>
  <data key="d2">the process of verifying that a resource meets certain criteria before it is stored in etcd</data>
  <data key="d3">concept</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HTTP POST request">
  <data key="d5">HTTP POST request</data>
  <data key="d2">a type of HTTP request used by kubectl to create resources through the Kubernetes API server</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Authentication Plugins">
  <data key="d5">Authentication Plugins</data>
  <data key="d2">Plugins configured in the API server to authenticate clients sending requests.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HTTP Request">
  <data key="d5">HTTP Request</data>
  <data key="d2">The request sent by the client to the API server.</data>
  <data key="d3">Network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Client Certificate">
  <data key="d5">Client Certificate</data>
  <data key="d2">A certificate used by clients to authenticate themselves.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HTTP Header">
  <data key="d5">HTTP Header</data>
  <data key="d2">A header in the HTTP request that contains authentication information.</data>
  <data key="d3">Network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Authorization Plugins">
  <data key="d5">Authorization Plugins</data>
  <data key="d2">Plugins configured in the API server to determine whether authenticated users can perform actions on requested resources.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Admission Control Plugins">
  <data key="d5">Admission Control Plugins</data>
  <data key="d2">Plugins that validate and/or modify resources in requests.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="AlwaysPullImages">
  <data key="d5">AlwaysPullImages</data>
  <data key="d2">An Admission Control plugin that overrides the pod's imagePullPolicy to Always.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Admission Control plugins">
  <data key="d5">Admission Control plugins</data>
  <data key="d2">Plugins that validate incoming requests before they are processed by the API server.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl tool">
  <data key="d5">kubectl tool</data>
  <data key="d2">A client used to interact with the API server and watch resources for changes.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HTTP connection">
  <data key="d5">HTTP connection</data>
  <data key="d2">The protocol used by clients to connect to the API server and receive notifications of resource changes.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="stream of modifications">
  <data key="d5">stream of modifications</data>
  <data key="d2">A continuous flow of updates sent by the API server to connected clients watching resources.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="watched objects">
  <data key="d5">watched objects</data>
  <data key="d2">Resources being observed by clients for changes, such as pods or services.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--watch flag">
  <data key="d5">--watch flag</data>
  <data key="d2">A command-line option used with kubectl to enable watching resources for changes.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GET /.../pods?watch=true">
  <data key="d5">GET /.../pods?watch=true</data>
  <data key="d2">An HTTP request used by clients to watch pods for changes.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="POST /.../pods/pod-xyz">
  <data key="d5">POST /.../pods/pod-xyz</data>
  <data key="d2">An HTTP request used by clients to create a new pod resource.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Send updated object to all watchers">
  <data key="d5">Send updated object to all watchers</data>
  <data key="d2">The process by which the API server notifies connected clients of changes to watched resources.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-159041347-14j3i">
  <data key="d5">kubia-159041347-14j3i</data>
  <data key="d2">pod name</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="watch mechanism">
  <data key="d5">watch mechanism</data>
  <data key="d2">API server feature used by the Scheduler and Kubelet</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl get pods -o yaml --watch">
  <data key="d5">kubectl get pods -o yaml --watch</data>
  <data key="d2">command used to print pod definitions with watch mechanism</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="round-robin">
  <data key="d5">round-robin</data>
  <data key="d2">algorithm used to prioritize nodes when multiple have the highest score</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Predicate functions">
  <data key="d5">Predicate functions</data>
  <data key="d2">Functions used to check node eligibility for pod scheduling</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Hardware resources">
  <data key="d5">Hardware resources</data>
  <data key="d2">Resources such as CPU, memory, and disk space required by a pod</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Memory pressure condition">
  <data key="d5">Memory pressure condition</data>
  <data key="d2">Condition where a node is running low on memory</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Disk pressure condition">
  <data key="d5">Disk pressure condition</data>
  <data key="d2">Condition where a node is running low on disk space</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Node selector">
  <data key="d5">Node selector</data>
  <data key="d2">Label used to select specific nodes for pod scheduling</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Host port">
  <data key="d5">Host port</data>
  <data key="d2">Port used by a pod to communicate with the host machine</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Taints and tolerations">
  <data key="d5">Taints and tolerations</data>
  <data key="d2">Mechanism for node affinity and anti-affinity rules</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Affinity or anti-affinity rules">
  <data key="d5">Affinity or anti-affinity rules</data>
  <data key="d2">Rules used to select specific nodes for pod scheduling</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicationManager">
  <data key="d5">ReplicationManager</data>
  <data key="d2">Controller for ReplicationController resources</data>
  <data key="d3">controller</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Watch mechanism">
  <data key="d5">Watch mechanism</data>
  <data key="d2">Mechanism used by controllers to be notified of changes to resources</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Re-list operation">
  <data key="d5">Re-list operation</data>
  <data key="d2">Periodic operation performed by controllers to ensure they haven't missed any events</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Constructor">
  <data key="d5">Constructor</data>
  <data key="d2">Method used by controllers to create an Informer and initialize themselves</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod resources">
  <data key="d5">Pod resources</data>
  <data key="d2">resources that represent running containers in the cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="syncHandler">
  <data key="d5">syncHandler</data>
  <data key="d2">function that gets invoked each time the controller needs to do something</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicationController resources">
  <data key="d5">ReplicationController resources</data>
  <data key="d2">resources that represent the desired replica count and actual replica count</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Worker() method">
  <data key="d5">Worker() method</data>
  <data key="d2">method that gets invoked each time the controller needs to do something</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="EndPoints controller">
  <data key="d5">EndPoints controller</data>
  <data key="d2">The active component that keeps the endpoint list constantly updated with the IPs and ports of pods matching the label selector.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes Service Proxies">
  <data key="d5">Kubernetes Service Proxies</data>
  <data key="d2">components that perform load balancing across pods</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="rkt">
  <data key="d5">rkt</data>
  <data key="d2">container runtime used by CoreOS</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubelet manifest directory">
  <data key="d5">Kubelet manifest directory</data>
  <data key="d2">local directory where pod manifests are stored</data>
  <data key="d3">file system</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod manifests">
  <data key="d5">pod manifests</data>
  <data key="d2">files that contain pod configuration information</data>
  <data key="d3">file system</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes Service Proxy">
  <data key="d5">Kubernetes Service Proxy</data>
  <data key="d2">Ensures connections to service IP and port end up at one of the pods backing that service</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Worker node">
  <data key="d5">Worker node</data>
  <data key="d2">A machine that runs the Kubelet and manages containers</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="userspace proxy mode">
  <data key="d5">userspace proxy mode</data>
  <data key="d2">a mode of operation for kube-proxy where packets are handled in user space</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="in kernel space">
  <data key="d5">in kernel space</data>
  <data key="d2">a mode of operation for kube-proxy where packets are handled by the Kernel</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="round-robin fashion">
  <data key="d5">round-robin fashion</data>
  <data key="d2">a method of balancing connections across pods</data>
  <data key="d3">algorithm</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes add-ons">
  <data key="d5">Kubernetes add-ons</data>
  <data key="d2">components that enable additional features in a Kubernetes cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dashboard add-ons">
  <data key="d5">dashboard add-ons</data>
  <data key="d2">components that provide a web interface for managing a Kubernetes cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="default-http-backend">
  <data key="d5">default-http-backend</data>
  <data key="d2">deployment for default HTTP backend</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubernetes-dashboard">
  <data key="d5">kubernetes-dashboard</data>
  <data key="d2">deployment for Kubernetes dashboard</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nginx-ingress-controller">
  <data key="d5">nginx-ingress-controller</data>
  <data key="d2">deployment for Nginx ingress controller</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DNS add-on">
  <data key="d5">DNS add-on</data>
  <data key="d2">add-on for DNS service</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cluster's internal DNS server">
  <data key="d5">cluster's internal DNS server</data>
  <data key="d2">DNS service within the cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-dns service">
  <data key="d5">kube-dns service</data>
  <data key="d2">service for Kubernetes DNS server</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nameserver">
  <data key="d5">nameserver</data>
  <data key="d2">DNS resolver in /etc/resolv.conf file</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API server's watch mechanism">
  <data key="d5">API server's watch mechanism</data>
  <data key="d2">mechanism to observe changes to resources</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Services and Endpoints">
  <data key="d5">Services and Endpoints</data>
  <data key="d2">resources for services and endpoints</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="reverse proxy server">
  <data key="d5">reverse proxy server</data>
  <data key="d2">server to forward traffic to pods</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicaSet A">
  <data key="d5">ReplicaSet A</data>
  <data key="d2">example ReplicaSet resource</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Node X">
  <data key="d5">Node X</data>
  <data key="d2">example node in a Kubernetes cluster</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicaSet Controller">
  <data key="d5">ReplicaSet Controller</data>
  <data key="d2">A specific controller that manages ReplicaSets</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Event">
  <data key="d5">Event</data>
  <data key="d2">A Kubernetes resource that represents a notification about a specific action or change</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="KIND">
  <data key="d5">KIND</data>
  <data key="d2">column in the events list that shows the type of resource being acted on</data>
  <data key="d3">column</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="REASON">
  <data key="d5">REASON</data>
  <data key="d2">column in the events list that shows why a controller action was taken</data>
  <data key="d3">column</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SOURCE">
  <data key="d5">SOURCE</data>
  <data key="d2">column in the events list that shows which controller performed an action</data>
  <data key="d3">column</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="deployment-controller">
  <data key="d5">deployment-controller</data>
  <data key="d2">controller responsible for managing deployments in Kubernetes</data>
  <data key="d3">controller</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="replicaset-controller">
  <data key="d5">replicaset-controller</data>
  <data key="d2">controller responsible for managing replica sets in Kubernetes</data>
  <data key="d3">controller</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="default-scheduler">
  <data key="d5">default-scheduler</data>
  <data key="d2">scheduler that assigns pods to nodes by default in Kubernetes</data>
  <data key="d3">scheduler</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="docker">
  <data key="d5">docker</data>
  <data key="d2">a containerization platform</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="minikubeVM">
  <data key="d5">minikubeVM</data>
  <data key="d2">the virtual machine running minikube</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CONTAINER ID">
  <data key="d5">CONTAINER ID</data>
  <data key="d2">a unique identifier for a Docker container</data>
  <data key="d3">attribute</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="IMAGE">
  <data key="d5">IMAGE</data>
  <data key="d2">the image used to create a Docker container</data>
  <data key="d3">attribute</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="COMMAND">
  <data key="d5">COMMAND</data>
  <data key="d2">the command run by a Docker container</data>
  <data key="d3">attribute</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod infrastructure container">
  <data key="d5">pod infrastructure container</data>
  <data key="d2">a container that holds all the namespaces for a pod</data>
  <data key="d3">container type</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Inter-pod networking">
  <data key="d5">Inter-pod networking</data>
  <data key="d2">communication between pods in Kubernetes</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NAT">
  <data key="d5">NAT</data>
  <data key="d2">Network Address Translation</data>
  <data key="d3">networking</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="IP: 10.1.1.1">
  <data key="d5">IP: 10.1.1.1</data>
  <data key="d2">IP address of Pod A</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="srcIP: 10.1.1.1">
  <data key="d5">srcIP: 10.1.1.1</data>
  <data key="d2">source IP address of packet sent by Pod A</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dstIP: 10.1.2.1">
  <data key="d5">dstIP: 10.1.2.1</data>
  <data key="d2">destination IP address of packet sent by Pod A</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod X">
  <data key="d5">Pod X</data>
  <data key="d2">Client pod providing notification service</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod Y">
  <data key="d5">Pod Y</data>
  <data key="d2">Notification service pod</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NAT-less communication">
  <data key="d5">NAT-less communication</data>
  <data key="d2">Communication between pods without network address translation</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pause container">
  <data key="d5">pause container</data>
  <data key="d2">Infrastructure container holding pod's IP address and network namespace</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node-to-pod communication">
  <data key="d5">node-to-pod communication</data>
  <data key="d2">Communication between a node and a pod</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod-to-node communication">
  <data key="d5">pod-to-node communication</data>
  <data key="d2">Communication between a pod and a node</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="outbound packets">
  <data key="d5">outbound packets</data>
  <data key="d2">Packets sent from a pod to the internet</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="interface">
  <data key="d5">interface</data>
  <data key="d2">The interface in the host's network namespace is attached to a network bridge.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="overlay networks">
  <data key="d5">overlay networks</data>
  <data key="d2">This can be done with overlay or underlay networks or by regular layer 3 routing, which we'll look at next.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="underlay networks">
  <data key="d5">underlay networks</data>
  <data key="d2">This can be done with overlay or underlay networks or by regular layer 3 routing, which we'll look at next.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="layer 3 routing">
  <data key="d5">layer 3 routing</data>
  <data key="d2">The node's physical network interface needs to be connected to the bridge as well.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="routing tables">
  <data key="d5">routing tables</data>
  <data key="d2">Routing tables on node A need to be configured so all packets destined for 10.1.2.0/24 are routed to node B, whereas node B's routing tables need to be configured so packets sent to 10.1.1.0/24 are routed to node A.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod C">
  <data key="d5">pod C</data>
  <data key="d2">Pod C is running on Node B and has an IP address of 10.1.2.1.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="physical adapter">
  <data key="d5">physical adapter</data>
  <data key="d2">Hardware component that connects a computer to a network</data>
  <data key="d3">hardware/network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Software Defined Network (SDN)">
  <data key="d5">Software Defined Network (SDN)</data>
  <data key="d2">Network architecture that uses software to manage network behavior</data>
  <data key="d3">hardware/network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container Network Interface (CNI)">
  <data key="d5">Container Network Interface (CNI)</data>
  <data key="d2">Project that allows Kubernetes to use any CNI plugin</data>
  <data key="d3">software/container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Calico">
  <data key="d5">Calico</data>
  <data key="d2">CNI plugin for container networking</data>
  <data key="d3">software/container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Romana">
  <data key="d5">Romana</data>
  <data key="d2">CNI plugin for container networking</data>
  <data key="d3">software/container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Weave Net">
  <data key="d5">Weave Net</data>
  <data key="d2">CNI plugin for container networking</data>
  <data key="d3">software/container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Endpoints objects">
  <data key="d5">Endpoints objects</data>
  <data key="d2">holds the IP/port pairs of all the pods that back the service</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="network interfaces">
  <data key="d5">network interfaces</data>
  <data key="d2">not assigned to the service IP address and port</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-proxy agents">
  <data key="d5">kube-proxy agents</data>
  <data key="d2">running on worker nodes, notified by API server when a new Service is created</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kernel">
  <data key="d5">kernel</data>
  <data key="d2">the core part of an operating system that manages hardware resources</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node A">
  <data key="d5">node A</data>
  <data key="d2">a Kubernetes node running on machine A</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node B">
  <data key="d5">node B</data>
  <data key="d2">a Kubernetes node running on machine B</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod B1">
  <data key="d5">Pod B1</data>
  <data key="d2">a containerized application running on a Kubernetes pod</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod B2">
  <data key="d5">Pod B2</data>
  <data key="d2">a containerized application running on a Kubernetes pod</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod B3">
  <data key="d5">Pod B3</data>
  <data key="d2">a containerized application running on a Kubernetes pod</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Packet X">
  <data key="d5">Packet X</data>
  <data key="d2">an example network packet being routed through the service</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Service B">
  <data key="d5">Service B</data>
  <data key="d2">a Kubernetes service providing access to multiple backend pods</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Endpoints B">
  <data key="d5">Endpoints B</data>
  <data key="d2">a collection of IP addresses and ports associated with a Kubernetes service</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Leader-election mechanism">
  <data key="d5">Leader-election mechanism</data>
  <data key="d2">Method for multiple app instances to agree on a leader</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes Control Plane components">
  <data key="d5">Kubernetes Control Plane components</data>
  <data key="d2">Components responsible for managing Kubernetes cluster state</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Load balancer">
  <data key="d5">Load balancer</data>
  <data key="d2">Component that distributes incoming traffic across multiple nodes</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Master node">
  <data key="d5">Master node</data>
  <data key="d2">Node that runs the Control Plane components</data>
  <data key="d3">node</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--leader-elect option">
  <data key="d5">--leader-elect option</data>
  <data key="d2">an option that controls which instance is active at a time</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="leader election mechanism">
  <data key="d5">leader election mechanism</data>
  <data key="d2">a way to elect a leader in control plane components</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-scheduler">
  <data key="d5">kube-scheduler</data>
  <data key="d2">an instance of the Scheduler component</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="get endpoints">
  <data key="d5">get endpoints</data>
  <data key="d2">a command used to retrieve information about an Endpoints resource</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="leaderTransitions">
  <data key="d5">leaderTransitions</data>
  <data key="d2">a field in the leader election annotation</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="leaseDurationSeconds">
  <data key="d5">leaseDurationSeconds</data>
  <data key="d2">a field in the leader election annotation</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Infrastructure Container">
  <data key="d5">Infrastructure Container</data>
  <data key="d2">binds together all containers of a pod</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Network Bridge">
  <data key="d5">Network Bridge</data>
  <data key="d2">connects pods running on different nodes</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ServiceAccount token">
  <data key="d5">ServiceAccount token</data>
  <data key="d2">A token used to authenticate with the Kubernetes API server.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="default roles and bindings">
  <data key="d5">default roles and bindings</data>
  <data key="d2">Pre-configured Roles and RoleBindings provided by Kubernetes.</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API server core">
  <data key="d5">API server core</data>
  <data key="d2">the central part of the API server that receives requests from authentication plugins</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="client certificate">
  <data key="d5">client certificate</data>
  <data key="d2">a method used by authentication plugins to obtain the identity of the client</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="authentication token">
  <data key="d5">authentication token</data>
  <data key="d2">a method used by authentication plugins to obtain the identity of the client through an HTTP header</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Basic HTTP authentication">
  <data key="d5">Basic HTTP authentication</data>
  <data key="d2">a method used by authentication plugins to obtain the identity of the client</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="service accounts">
  <data key="d5">service accounts</data>
  <data key="d2">a mechanism used by pods to authenticate themselves</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ServiceAccount resources">
  <data key="d5">ServiceAccount resources</data>
  <data key="d2">resources created and stored in the cluster for service accounts</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:unauthenticated group">
  <data key="d5">system:unauthenticated group</data>
  <data key="d2">Used for requests where none of the authentication plugins could authenticate the client.</data>
  <data key="d3">group</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:authenticated group">
  <data key="d5">system:authenticated group</data>
  <data key="d2">Automatically assigned to a user who was authenticated successfully.</data>
  <data key="d3">group</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:serviceaccounts group">
  <data key="d5">system:serviceaccounts group</data>
  <data key="d2">Encompasses all ServiceAccounts in the system.</data>
  <data key="d3">group</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ServiceAccount usernames">
  <data key="d5">ServiceAccount usernames</data>
  <data key="d2">Formatted like system:serviceaccount:&lt;namespace&gt;:&lt;service account name&gt;, and are passed to the API server core for authorization.</data>
  <data key="d3">string</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ServiceAccounts resource">
  <data key="d5">ServiceAccounts resource</data>
  <data key="d2">A resource just like Pods, Secrets, ConfigMaps, and so on, scoped to individual namespaces.</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ kubectl get sa">
  <data key="d5">$ kubectl get sa</data>
  <data key="d2">A command used to list ServiceAccounts in a namespace.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cluster administrator">
  <data key="d5">cluster administrator</data>
  <data key="d2">The cluster administrator is responsible for configuring and managing the Kubernetes cluster, including setting up authorization plugins.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="sa">
  <data key="d5">sa</data>
  <data key="d2">abbreviation for ServiceAccount</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="foo-token-qzq7j">
  <data key="d5">foo-token-qzq7j</data>
  <data key="d2">token Secret associated with the foo ServiceAccount</data>
  <data key="d3">secret</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="JSON Web Tokens (JWT)">
  <data key="d5">JSON Web Tokens (JWT)</data>
  <data key="d2">a type of authentication token used in ServiceAccounts</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Mountable secrets">
  <data key="d5">Mountable secrets</data>
  <data key="d2">list of Secrets that can be mounted inside a pod using a ServiceAccount</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="image pull Secrets">
  <data key="d5">image pull Secrets</data>
  <data key="d2">Secrets that hold the credentials for pulling container images from a private image repository.</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="mountable Secrets">
  <data key="d5">mountable Secrets</data>
  <data key="d2">Secrets that can be mounted by a pod, allowing it to access sensitive information.</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="foo ServiceAccount">
  <data key="d5">foo ServiceAccount</data>
  <data key="d2">Custom ServiceAccount used in the example</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/var/run/secrets/kubernetes.io/serviceaccount/token">
  <data key="d5">/var/run/secrets/kubernetes.io/serviceaccount/token</data>
  <data key="d2">Path to the ServiceAccount token file</data>
  <data key="d3">path</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="localhost:8001/api/v1/pods">
  <data key="d5">localhost:8001/api/v1/pods</data>
  <data key="d2">API endpoint for listing pods</data>
  <data key="d3">url</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Role-Based Access Control (RBAC)">
  <data key="d5">Role-Based Access Control (RBAC)</data>
  <data key="d2">A plugin used to secure the cluster by preventing unauthorized users from viewing or modifying the cluster state.</data>
  <data key="d3">authorization</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="RBAC authorization plugin">
  <data key="d5">RBAC authorization plugin</data>
  <data key="d2">A plugin that enables role-based access control in a Kubernetes cluster, preventing unauthorized users from viewing or modifying the cluster state.</data>
  <data key="d3">authorization</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Attribute-Based Access Control (ABAC)">
  <data key="d5">Attribute-Based Access Control (ABAC)</data>
  <data key="d2">An authorization plugin that allows access to be controlled based on attributes of users and resources.</data>
  <data key="d3">authorization</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Web-Hook plugin">
  <data key="d5">Web-Hook plugin</data>
  <data key="d2">An authorization plugin that uses web hooks to control access to a Kubernetes cluster.</data>
  <data key="d3">authorization</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="custom plugin implementations">
  <data key="d5">custom plugin implementations</data>
  <data key="d2">Custom plugins implemented by users to control access to a Kubernetes cluster.</data>
  <data key="d3">authorization</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="REST resources">
  <data key="d5">REST resources</data>
  <data key="d2">Resources in a Kubernetes cluster that can be accessed using HTTP requests, such as Pods, Services, Secrets, etc.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PUT request">
  <data key="d5">PUT request</data>
  <data key="d2">A type of HTTP request used to update existing resources in a Kubernetes cluster.</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DELETE request">
  <data key="d5">DELETE request</data>
  <data key="d2">A type of HTTP request used to delete resources from a Kubernetes cluster.</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="roles">
  <data key="d5">roles</data>
  <data key="d2">User-defined roles that define permissions for users or ServiceAccounts</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="clusterrole">
  <data key="d5">clusterrole</data>
  <data key="d2">Resource that defines permissions for a cluster-wide role</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Role resource">
  <data key="d5">Role resource</data>
  <data key="d2">allows users to get and list Services in the foo namespace</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="YAML file">
  <data key="d5">YAML file</data>
  <data key="d2">file format for defining Kubernetes resources</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cluster-admin rights">
  <data key="d5">cluster-admin rights</data>
  <data key="d2">permissions required to manage a Kubernetes cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--verb=get">
  <data key="d5">--verb=get</data>
  <data key="d2">command-line option for specifying the actions allowed by a Role</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--verb=list">
  <data key="d5">--verb=list</data>
  <data key="d2">command-line option for specifying the actions allowed by a Role</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--resource=services">
  <data key="d5">--resource=services</data>
  <data key="d2">command-line option for specifying the resources affected by a Role</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--namespace=foo">
  <data key="d5">--namespace=foo</data>
  <data key="d2">command-line option for specifying the namespace in which to create a resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--clusterrolebinding">
  <data key="d5">--clusterrolebinding</data>
  <data key="d2">command-line option for creating a clusterrolebinding resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ServiceList">
  <data key="d5">ServiceList</data>
  <data key="d2">A Kubernetes object that represents a list of Services.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="localhost:8001/api/v1/namespaces/foo/services">
  <data key="d5">localhost:8001/api/v1/namespaces/foo/services</data>
  <data key="d2">The URL of the Kubernetes API server to list Services in the foo namespace.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="rolebinding">
  <data key="d5">rolebinding</data>
  <data key="d2">a resource that binds a role to a set of subjects, such as users or service accounts</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="role">
  <data key="d5">role</data>
  <data key="d2">a resource that defines a set of permissions within a Kubernetes namespace</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="test">
  <data key="d5">test</data>
  <data key="d2">a role binding name</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Service-Accounts">
  <data key="d5">Service-Accounts</data>
  <data key="d2">An identity for a service running in a Kubernetes cluster.</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="persistentvolumes">
  <data key="d5">persistentvolumes</data>
  <data key="d2">a resource type in Kubernetes that represents a persistent volume</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentVolumeList">
  <data key="d5">PersistentVolumeList</data>
  <data key="d2">A list of PersistentVolumes in the cluster.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/api/*">
  <data key="d5">/api/*</data>
  <data key="d2">A non-resource URL pattern exposed by the API server.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/apis/*">
  <data key="d5">/apis/*</data>
  <data key="d2">A non-resource URL pattern exposed by the API server.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/healthz">
  <data key="d5">/healthz</data>
  <data key="d2">A non-resource URL exposed by the API server.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/swaggerapi">
  <data key="d5">/swaggerapi</data>
  <data key="d2">A non-resource URL exposed by the API server.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/swaggerapi/*">
  <data key="d5">/swaggerapi/*</data>
  <data key="d2">A non-resource URL pattern exposed by the API server.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/version">
  <data key="d5">/version</data>
  <data key="d2">A non-resource URL exposed by the API server.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="get clusterrole system:discovery -o yaml">
  <data key="d5">get clusterrole system:discovery -o yaml</data>
  <data key="d2">A command used to retrieve the system:discovery ClusterRole in YAML format.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:discovery">
  <data key="d5">system:discovery</data>
  <data key="d2">A specific ClusterRole and ClusterRoleBinding used for discovery purposes.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:authenticated">
  <data key="d5">system:authenticated</data>
  <data key="d2">A group that represents all authenticated users in a Kubernetes cluster.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:unauthenticated">
  <data key="d5">system:unauthenticated</data>
  <data key="d2">A group that represents all unauthenticated users in a Kubernetes cluster.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="authentication plugin">
  <data key="d5">authentication plugin</data>
  <data key="d2">A component of the Kubernetes API server responsible for authenticating incoming requests.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="APIVersions">
  <data key="d5">APIVersions</data>
  <data key="d2">a Kubernetes concept for managing API versions</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="view">
  <data key="d5">view</data>
  <data key="d2">a predefined ClusterRole that allows reading namespaced resources</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="replicationcontrollers">
  <data key="d5">replicationcontrollers</data>
  <data key="d2">a type of Kubernetes resource for managing replication and scaling</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/api/v1/pods">
  <data key="d5">/api/v1/pods</data>
  <data key="d2">An API endpoint that lists pods across all namespaces.</data>
  <data key="d3">API Endpoint</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/api/v1/namespaces/foo/pods">
  <data key="d5">/api/v1/namespaces/foo/pods</data>
  <data key="d2">An API endpoint that lists pods in a specific namespace (foo).</data>
  <data key="d3">API Endpoint</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="localhost:8001/api/v1/namespaces/bar/pods">
  <data key="d5">localhost:8001/api/v1/namespaces/bar/pods</data>
  <data key="d2">A URL endpoint for listing pods in the 'bar' namespace.</data>
  <data key="d3">url</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:serviceaccount:foo:default">
  <data key="d5">system:serviceaccount:foo:default</data>
  <data key="d2">A service account user with permissions to list pods in the 'foo' namespace.</data>
  <data key="d3">user</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:auth-delegator">
  <data key="d5">system:auth-delegator</data>
  <data key="d2">a ClusterRole that grants authentication delegation permissions in a Kubernetes cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:basic-user">
  <data key="d5">system:basic-user</data>
  <data key="d2">a ClusterRole that grants basic user permissions in a Kubernetes cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system:controller:attachdetach-controller">
  <data key="d5">system:controller:attachdetach-controller</data>
  <data key="d2">a ClusterRole that grants attach/detach controller permissions in a Kubernetes cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="view ClusterRole">
  <data key="d5">view ClusterRole</data>
  <data key="d2">It allows read-ing most resources in a namespace, except for Roles, RoleBindings, and Secrets.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="edit ClusterRole">
  <data key="d5">edit ClusterRole</data>
  <data key="d2">It allows you to modify resources in a namespace, but also allows both reading and modifying Secrets.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="admin ClusterRole">
  <data key="d5">admin ClusterRole</data>
  <data key="d2">Complete control of the resources in a namespace is granted in the admin Cluster-Role.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cluster-admin ClusterRole">
  <data key="d5">cluster-admin ClusterRole</data>
  <data key="d2">Subjects with this ClusterRole can read and modify any resource in the namespace, except ResourceQuotas (we’ll learn what those are in chapter 14) and the Namespace resource itself.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ResourceQuotas">
  <data key="d5">ResourceQuotas</data>
  <data key="d2">They’re used to limit the resources that a namespace can use.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ClusterAdmin">
  <data key="d5">ClusterAdmin</data>
  <data key="d2">The highest level of permissions in the cluster</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ServiceAccountName">
  <data key="d5">ServiceAccountName</data>
  <data key="d2">A property that specifies the service account to use for a pod</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ServiceAccountToken">
  <data key="d5">ServiceAccountToken</data>
  <data key="d2">An authentication token used by a ServiceAccount to authenticate with the API server</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="policies">
  <data key="d5">policies</data>
  <data key="d2">rules that govern what pods can do</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="IP and port space">
  <data key="d5">IP and port space</data>
  <data key="d2">Each pod gets its own IP and port space</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Inter-Process Communication (IPC)">
  <data key="d5">Inter-Process Communication (IPC)</data>
  <data key="d2">Mechanism for processes in the same pod to communicate</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="hostNetwork property">
  <data key="d5">hostNetwork property</data>
  <data key="d2">Allow pods to use node's network interfaces instead of their own</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node's network adapters">
  <data key="d5">node's network adapters</data>
  <data key="d2">Pods can use node's network adapters instead of virtual network adapters</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="virtual network adapters">
  <data key="d5">virtual network adapters</data>
  <data key="d2">Pods have their own virtual network adapters</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="eth1">
  <data key="d5">eth1</data>
  <data key="d2">Node's network interface</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ifconfig">
  <data key="d5">ifconfig</data>
  <data key="d2">a command to display network interface information</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="veth1178d4f">
  <data key="d5">veth1178d4f</data>
  <data key="d2">a virtual Ethernet interface for a pod</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="port 9000">
  <data key="d5">port 9000</data>
  <data key="d2">A port on the node that is bound to the pod's port 8080.</data>
  <data key="d3">hardware/network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="firewall-rules">
  <data key="d5">firewall-rules</data>
  <data key="d2">Rules used to configure the firewall in GKE.</data>
  <data key="d3">hardware/network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod-with-host-pid-and-ipc.yaml">
  <data key="d5">pod-with-host-pid-and-ipc.yaml</data>
  <data key="d2">A YAML file used to define a pod that uses the host's PID and IPC namespaces.</data>
  <data key="d3">software/configuration</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia-hostport.yaml">
  <data key="d5">kubia-hostport.yaml</data>
  <data key="d2">A YAML file used to define a pod that binds to a port on the node.</data>
  <data key="d3">software/configuration</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ps aux">
  <data key="d5">ps aux</data>
  <data key="d2">Unix command to list processes</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Inter-Process Communication">
  <data key="d5">Inter-Process Communication</data>
  <data key="d2">mechanism for processes to communicate with each other</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="user ID">
  <data key="d5">user ID</data>
  <data key="d2">ID of the user under which a process will run</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="privileged mode">
  <data key="d5">privileged mode</data>
  <data key="d2">mode that gives full access to the node's kernel</data>
  <data key="d3">property</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SELinux">
  <data key="d5">SELinux</data>
  <data key="d2">Security Enhanced Linux options to lock down a container</data>
  <data key="d3">property</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="hostPID: true">
  <data key="d5">hostPID: true</data>
  <data key="d2">property to enable processes in the pod's containers to communicate with host processes</data>
  <data key="d3">property</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="bin">
  <data key="d5">bin</data>
  <data key="d2">system binary directory</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="daemon">
  <data key="d5">daemon</data>
  <data key="d2">system daemon process</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="sys">
  <data key="d5">sys</data>
  <data key="d2">system group</data>
  <data key="d3">group</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="adm">
  <data key="d5">adm</data>
  <data key="d2">administrators group</data>
  <data key="d3">group</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="wheel">
  <data key="d5">wheel</data>
  <data key="d2">superuser group</data>
  <data key="d3">group</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="floppy">
  <data key="d5">floppy</data>
  <data key="d2">floppy disk device group</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dialout">
  <data key="d5">dialout</data>
  <data key="d2">dial-out modem group</data>
  <data key="d3">group</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tape">
  <data key="d5">tape</data>
  <data key="d2">tape drive device group</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="video">
  <data key="d5">video</data>
  <data key="d2">video display device group</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="runAsNonRoot">
  <data key="d5">runAsNonRoot</data>
  <data key="d2">A boolean flag that specifies whether a container should be allowed to run as root or not.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/dev">
  <data key="d5">/dev</data>
  <data key="d2">A special file directory on Linux systems containing device files for all devices on the system.</data>
  <data key="d3">directory</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="device files">
  <data key="d5">device files</data>
  <data key="d2">Special files used to communicate with devices on a Linux system.</data>
  <data key="d3">files</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/dev/null">
  <data key="d5">/dev/null</data>
  <data key="d2">A special file in Linux systems used to discard output or redirect input.</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/dev/zero">
  <data key="d5">/dev/zero</data>
  <data key="d2">A special file in Linux systems that generates a stream of zeros.</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/dev/random">
  <data key="d5">/dev/random</data>
  <data key="d2">A special file in Linux systems that generates a stream of random numbers.</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="btrfs-control">
  <data key="d5">btrfs-control</data>
  <data key="d2">device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="stderr">
  <data key="d5">stderr</data>
  <data key="d2">file descriptor</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tty48">
  <data key="d5">tty48</data>
  <data key="d2">terminal device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="core">
  <data key="d5">core</data>
  <data key="d2">system core file</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="stdin">
  <data key="d5">stdin</data>
  <data key="d2">standard input stream</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tty49">
  <data key="d5">tty49</data>
  <data key="d2">terminal device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="stdout">
  <data key="d5">stdout</data>
  <data key="d2">standard output stream</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tty5">
  <data key="d5">tty5</data>
  <data key="d2">terminal device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cpu_dma_latency">
  <data key="d5">cpu_dma_latency</data>
  <data key="d2">CPU DMA latency file</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="termination-log">
  <data key="d5">termination-log</data>
  <data key="d2">system log file</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tty50">
  <data key="d5">tty50</data>
  <data key="d2">terminal device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fd">
  <data key="d5">fd</data>
  <data key="d2">file descriptor</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tty">
  <data key="d5">tty</data>
  <data key="d2">terminal device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tty51">
  <data key="d5">tty51</data>
  <data key="d2">terminal device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="full">
  <data key="d5">full</data>
  <data key="d2">device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tty0">
  <data key="d5">tty0</data>
  <data key="d2">terminal device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fuse">
  <data key="d5">fuse</data>
  <data key="d2">file system</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tty1">
  <data key="d5">tty1</data>
  <data key="d2">terminal device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="hpet">
  <data key="d5">hpet</data>
  <data key="d2">high precision event timer</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tty10">
  <data key="d5">tty10</data>
  <data key="d2">terminal device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="hwrng">
  <data key="d5">hwrng</data>
  <data key="d2">hardware random number generator</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tty11">
  <data key="d5">tty11</data>
  <data key="d2">terminal device</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="privileged container">
  <data key="d5">privileged container</data>
  <data key="d2">container with elevated privileges</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CAP_SYS_TIME">
  <data key="d5">CAP_SYS_TIME</data>
  <data key="d2">kernel capability to set system time</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Linux kernel capabilities">
  <data key="d5">Linux kernel capabilities</data>
  <data key="d2">Capabilities that can be used by a container to perform specific actions.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CAP_">
  <data key="d5">CAP_</data>
  <data key="d2">Prefix for Linux kernel capabilities.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod-add-settime-capability">
  <data key="d5">pod-add-settime-capability</data>
  <data key="d2">Name of a pod that has been created with the SYS_TIME capability.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/tmp">
  <data key="d5">/tmp</data>
  <data key="d2">Directory where temporary files are stored.</data>
  <data key="d3">filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="securityContext.capabilities.drop">
  <data key="d5">securityContext.capabilities.drop</data>
  <data key="d2">property in Kubernetes Pod configuration to drop specific capabilities</data>
  <data key="d3">configuration</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="chown">
  <data key="d5">chown</data>
  <data key="d2">Unix command to change file ownership</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="securityContext.readOnlyRootFilesystem">
  <data key="d5">securityContext.readOnlyRootFilesystem</data>
  <data key="d2">property in Kubernetes Pod configuration to set the root filesystem as read-only</data>
  <data key="d3">configuration</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod-with-readonly-filesystem">
  <data key="d5">Pod-with-readonly-filesystem</data>
  <data key="d2">Kubernetes resource representing a Pod with a read-only root filesystem</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="touch">
  <data key="d5">touch</data>
  <data key="d2">A command that creates a new file or updates the timestamp of an existing file.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="EmptyDir">
  <data key="d5">EmptyDir</data>
  <data key="d2">In-memory volume that is deleted when the pod is terminated</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Security Context">
  <data key="d5">Security Context</data>
  <data key="d2">Configuration to specify security settings for a pod or container</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fsGroup ID">
  <data key="d5">fsGroup ID</data>
  <data key="d2">User ID used by containers to access shared files</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Supplemental Group IDs">
  <data key="d5">Supplemental Group IDs</data>
  <data key="d2">Additional group IDs used by containers to access shared files</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="id command">
  <data key="d5">id command</data>
  <data key="d2">shows the container is running with user ID 1111</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="privileged pod">
  <data key="d5">privileged pod</data>
  <data key="d2">pod that has full access to the cluster node</data>
  <data key="d3">pod type</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/volume/foo">
  <data key="d5">/volume/foo</data>
  <data key="d2">file created in the mounted volume's directory</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/tmp/foo">
  <data key="d5">/tmp/foo</data>
  <data key="d2">file created in the container's filesystem</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fsGroup security context property">
  <data key="d5">fsGroup security context property</data>
  <data key="d2">used when creating files in a volume</data>
  <data key="d3">property</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="host ports">
  <data key="d5">host ports</data>
  <data key="d2">Ports on the host machine that can be bound by pods</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="user IDs">
  <data key="d5">user IDs</data>
  <data key="d2">IDs under which containers can run</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PodSecurityPolicy admission control plugin">
  <data key="d5">PodSecurityPolicy admission control plugin</data>
  <data key="d2">A plugin that validates pod definitions against configured PodSecurityPolicies</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="apiserver.GenericServerRunOptions.AdmissionControl">
  <data key="d5">apiserver.GenericServerRunOptions.AdmissionControl</data>
  <data key="d2">An option that enables the PodSecurityPolicy admission control plugin</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="LimitRanger">
  <data key="d5">LimitRanger</data>
  <data key="d2">A feature that enforces resource limits on pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PersistentVolumeLabel">
  <data key="d5">PersistentVolumeLabel</data>
  <data key="d2">A feature that manages persistent volume labels</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DefaultStorageClass">
  <data key="d5">DefaultStorageClass</data>
  <data key="d2">A feature that sets a default storage class for pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DefaultTolerationSeconds">
  <data key="d5">DefaultTolerationSeconds</data>
  <data key="d2">A feature that sets a default toleration period for pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="BasicAuthFile">
  <data key="d5">BasicAuthFile</data>
  <data key="d2">A file that contains basic authentication credentials</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="password file">
  <data key="d5">password file</data>
  <data key="d2">A file that contains password and user ID information</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="hostPorts">
  <data key="d5">hostPorts</data>
  <data key="d2">allowing pods to bind to specific host ports</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="MustRunAs rule">
  <data key="d5">MustRunAs rule</data>
  <data key="d2">rule in PodSecurityPolicy that constrains user or group IDs</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ranges">
  <data key="d5">ranges</data>
  <data key="d2">field in MustRunAs rule that specifies allowed ID ranges</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="2-10">
  <data key="d5">2-10</data>
  <data key="d2">ID range allowed for user ID, filesystem group, and supplemental groups</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="20-30">
  <data key="d5">20-30</data>
  <data key="d2">ID range allowed for user ID, filesystem group, and supplemental groups</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="USER directive">
  <data key="d5">USER directive</data>
  <data key="d2">A command in a Dockerfile that sets the user ID for a container</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="MustRunAsNonRoot">
  <data key="d5">MustRunAsNonRoot</data>
  <data key="d2">A rule that prevents users from deploying containers that run as root.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="allowedCapabilities">
  <data key="d5">allowedCapabilities</data>
  <data key="d2">A field in the PodSecurityPolicy resource that specifies which capabilities containers can add.</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="defaultAddCapabilities">
  <data key="d5">defaultAddCapabilities</data>
  <data key="d2">A field in the PodSecurityPolicy resource that specifies which capabilities are automatically added to every container.</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SYS_ADMIN">
  <data key="d5">SYS_ADMIN</data>
  <data key="d2">A Linux kernel capability that allows a range of administrative operations.</data>
  <data key="d3">capability</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SYS_MODULE">
  <data key="d5">SYS_MODULE</data>
  <data key="d2">A Linux kernel capability that allows loading and unloading of Linux kernel modules.</data>
  <data key="d3">capability</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="security-related features">
  <data key="d5">security-related features</data>
  <data key="d2">features related to security in pods</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CAPABILITIES">
  <data key="d5">CAPABILITIES</data>
  <data key="d2">capabilities added to every deployed pod's containers</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PodSecurity-Policy Admission Control plugin">
  <data key="d5">PodSecurity-Policy Admission Control plugin</data>
  <data key="d2">plugin adding capabilities to every container's security-Context.capabilities.drop field</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod-add-sysadmin-capability.yaml">
  <data key="d5">pod-add-sysadmin-capability.yaml</data>
  <data key="d2">file containing YAML configuration for creating a pod with added capability</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="persistentVolume-Claim">
  <data key="d5">persistentVolume-Claim</data>
  <data key="d2">volume type allowing users to add persistent volume claims to their pods</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PodSecurityPolicy Admission Control plugin">
  <data key="d5">PodSecurityPolicy Admission Control plugin</data>
  <data key="d2">A plugin that enforces PodSecurityPolicies when creating pods in Kubernetes.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PodSecurityPolicy (psp)">
  <data key="d5">PodSecurityPolicy (psp)</data>
  <data key="d2">A shorthand for PodSecurityPolicy in Kubernetes.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="default policy">
  <data key="d5">default policy</data>
  <data key="d2">The default PodSecurityPolicy resource that allows running non-privileged containers.</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="privileged policy">
  <data key="d5">privileged policy</data>
  <data key="d2">A privileged PodSecurityPolicy resource that allows running privileged containers.</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Alice">
  <data key="d5">Alice</data>
  <data key="d2">A user who can only deploy restricted (non-privileged) pods.</data>
  <data key="d3">user</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Bob">
  <data key="d5">Bob</data>
  <data key="d2">A user who can deploy both non-privileged and privileged pods.</data>
  <data key="d3">user</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PodSecurityPolicy resource">
  <data key="d5">PodSecurityPolicy resource</data>
  <data key="d2">A PodSecurityPolicy resource defines a set of privileges that can be used by pods.</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="psp-default">
  <data key="d5">psp-default</data>
  <data key="d2">A ClusterRole that allows the use of the default PodSecurityPolicy resource.</data>
  <data key="d3">role</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="psp-all-users">
  <data key="d5">psp-all-users</data>
  <data key="d2">A ClusterRoleBinding that binds the psp-default ClusterRole to all authenticated users.</data>
  <data key="d3">binding</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="bob">
  <data key="d5">bob</data>
  <data key="d2">user account in Kubernetes</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="alice">
  <data key="d5">alice</data>
  <data key="d2">user account in Kubernetes</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--user">
  <data key="d5">--user</data>
  <data key="d2">option for specifying user credentials in kubectl commands</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="podSecurityPolicy">
  <data key="d5">podSecurityPolicy</data>
  <data key="d2">resource that defines a set of permissions for pods</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="context">
  <data key="d5">context</data>
  <data key="d2">configuration object that specifies user credentials and cluster information</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="appendix A">
  <data key="d5">appendix A</data>
  <data key="d2">section in the book that explains how to use kubectl with multiple clusters and contexts</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Admission Control">
  <data key="d5">Admission Control</data>
  <data key="d2">plugin for validating pod configurations</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ingress rules">
  <data key="d5">ingress rules</data>
  <data key="d2">rules for specifying which sources can access matched pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="egress rules">
  <data key="d5">egress rules</data>
  <data key="d2">rules for specifying which destinations can be accessed from matched pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="namespace selector">
  <data key="d5">namespace selector</data>
  <data key="d2">selector for matching namespaces based on labels</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="default-deny NetworkPolicy">
  <data key="d5">default-deny NetworkPolicy</data>
  <data key="d2">NetworkPolicy that prevents all clients from connecting to any pod in a namespace</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="5432">
  <data key="d5">5432</data>
  <data key="d2">PostgreSQL database port number</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes namespaces">
  <data key="d5">Kubernetes namespaces</data>
  <data key="d2">a way to group resources in a Kubernetes cluster</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tenant">
  <data key="d5">tenant</data>
  <data key="d2">a label for identifying a tenant in a Kubernetes cluster</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="manning">
  <data key="d5">manning</data>
  <data key="d2">the name of a tenant</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="shopping-cart">
  <data key="d5">shopping-cart</data>
  <data key="d2">the name of a microservice</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="microservice">
  <data key="d5">microservice</data>
  <data key="d2">a label for identifying a microservice in a Kubernetes cluster</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="shopping-cart-netpolicy">
  <data key="d5">shopping-cart-netpolicy</data>
  <data key="d2">a NetworkPolicy resource for allowing access to the shopping cart microservice</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ingress rule">
  <data key="d5">ingress rule</data>
  <data key="d2">A NetworkPolicy rule that allows traffic from a specific IP block or pod selector.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="shopping-cart microservice">
  <data key="d5">shopping-cart microservice</data>
  <data key="d2">A Kubernetes application that provides a shopping cart service.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="egress">
  <data key="d5">egress</data>
  <data key="d2">An egress rule in a NetworkPolicy that limits outbound traffic</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="to">
  <data key="d5">to</data>
  <data key="d2">A keyword indicating the destination of an egress rule</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="devices">
  <data key="d5">devices</data>
  <data key="d2">Hardware components that can be accessed by containers in privileged mode</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="if=/dev/zero">
  <data key="d5">if=/dev/zero</data>
  <data key="d2">A file descriptor pointing to the /dev/zero device, which generates zeros on demand</data>
  <data key="d3">filedescriptor</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="of=/dev/null">
  <data key="d5">of=/dev/null</data>
  <data key="d2">A file descriptor pointing to /dev/null, which discards any data written to it</data>
  <data key="d3">filedescriptor</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Mem">
  <data key="d5">Mem</data>
  <data key="d2">memory usage metric</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="top">
  <data key="d5">top</data>
  <data key="d2">command to display system information</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Minikube VM">
  <data key="d5">Minikube VM</data>
  <data key="d2">virtual machine used for testing</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="LeastRequestedPriority">
  <data key="d5">LeastRequestedPriority</data>
  <data key="d2">A prioritization function that prefers nodes with fewer requested resources.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="MostRequestedPriority">
  <data key="d5">MostRequestedPriority</data>
  <data key="d2">A prioritization function that prefers nodes with the most requested resources.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Capacity">
  <data key="d5">Capacity</data>
  <data key="d2">total resources available on a node, including CPU and memory</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PodScheduled">
  <data key="d5">PodScheduled</data>
  <data key="d2">A condition indicating whether a pod has been scheduled on a node.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="FailedScheduling">
  <data key="d5">FailedScheduling</data>
  <data key="d2">An event indicating that scheduling of a pod has failed.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Insufficient cpu">
  <data key="d5">Insufficient cpu</data>
  <data key="d2">A reason for failing to schedule a pod due to insufficient CPU resources on a node.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cores">
  <data key="d5">cores</data>
  <data key="d2">A measure of a computer's processing power, equivalent to the number of CPUs available.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dflt-http-b...">
  <data key="d5">dflt-http-b...</data>
  <data key="d2">pod name</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-addon-...">
  <data key="d5">kube-addon-...</data>
  <data key="d2">pod name</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-dns-26...">
  <data key="d5">kube-dns-26...</data>
  <data key="d2">pod name</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubernetes-...">
  <data key="d5">kubernetes-...</data>
  <data key="d2">pod name</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nginx-ingre...">
  <data key="d5">nginx-ingre...</data>
  <data key="d2">pod name</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU Requests">
  <data key="d5">CPU Requests</data>
  <data key="d2">resource type</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU Limits">
  <data key="d5">CPU Limits</data>
  <data key="d2">resource type</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Memory Requests">
  <data key="d5">Memory Requests</data>
  <data key="d2">resource type</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Memory Limits">
  <data key="d5">Memory Limits</data>
  <data key="d2">resource type</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="custom resources">
  <data key="d5">custom resources</data>
  <data key="d2">Resources added by the user to a node, such as Opaque Integer Resources or Extended Resources.</data>
  <data key="d3">resource_management</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Extended Resources">
  <data key="d5">Extended Resources</data>
  <data key="d2">A type of custom resource in Kubernetes that provides additional resources to pods and containers.</data>
  <data key="d3">resource_management</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Opaque Integer Resources">
  <data key="d5">Opaque Integer Resources</data>
  <data key="d2">An older type of custom resource in Kubernetes that was replaced by Extended Resources.</data>
  <data key="d3">resource_management</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU time sharing">
  <data key="d5">CPU time sharing</data>
  <data key="d2">The process of distributing unused CPU time among pods and containers based on their CPU requests.</data>
  <data key="d3">resource_management</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PATCH HTTP request">
  <data key="d5">PATCH HTTP request</data>
  <data key="d2">HTTP request method to update a resource</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="capacity field">
  <data key="d5">capacity field</data>
  <data key="d2">Field in the Node object to specify custom resources</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="example.org/my-resource">
  <data key="d5">example.org/my-resource</data>
  <data key="d2">Custom resource name</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="quantity">
  <data key="d5">quantity</data>
  <data key="d2">Integer value for custom resource allocation</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="resources.requests field">
  <data key="d5">resources.requests field</data>
  <data key="d2">Field in the container spec to specify requested resources</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GPU units">
  <data key="d5">GPU units</data>
  <data key="d2">Custom resource representing available GPU units</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="resources.requests">
  <data key="d5">resources.requests</data>
  <data key="d2">Parameter to specify requested resources for a pod</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="overcommitting">
  <data key="d5">overcommitting</data>
  <data key="d2">The ability to allocate more resources than available on the node</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="container restart policy">
  <data key="d5">container restart policy</data>
  <data key="d2">the policy for restarting a container when it crashes</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU limit">
  <data key="d5">CPU limit</data>
  <data key="d2">the maximum amount of CPU time available to a container</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="top command">
  <data key="d5">top command</data>
  <data key="d2">system monitoring command that displays memory and CPU usage statistics</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dd command">
  <data key="d5">dd command</data>
  <data key="d2">data duplication command used to generate load on the system</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Java Virtual Machine">
  <data key="d5">Java Virtual Machine</data>
  <data key="d2">JVM that runs Java apps</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-Xmx option">
  <data key="d5">-Xmx option</data>
  <data key="d2">option to set maximum heap size for JVM</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="OOMKill">
  <data key="d5">OOMKill</data>
  <data key="d2">out-of-memory kill signal sent by the kernel</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cgroups system">
  <data key="d5">cgroups system</data>
  <data key="d2">system that controls resource usage for containers</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/sys/fs/cgroup/cpu/cpu.cfs_quota_us">
  <data key="d5">/sys/fs/cgroup/cpu/cpu.cfs_quota_us</data>
  <data key="d2">file that stores the configured CPU limit</data>
  <data key="d3">filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/sys/fs/cgroup/cpu/cpu.cfs_period_us">
  <data key="d5">/sys/fs/cgroup/cpu/cpu.cfs_period_us</data>
  <data key="d2">file that stores the period of time over which the CPU limit is enforced</data>
  <data key="d3">filesystem</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="BestEffort class">
  <data key="d5">BestEffort class</data>
  <data key="d2">Lowest priority QoS class, assigned to pods with no resource requests or limits set</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Burstable class">
  <data key="d5">Burstable class</data>
  <data key="d2">QoS class for pods that can burst beyond their requested resources</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Guaranteed class">
  <data key="d5">Guaranteed class</data>
  <data key="d2">Highest priority QoS class, assigned to pods with equal requests and limits set for all resources</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="YAML/JSON manifest">
  <data key="d5">YAML/JSON manifest</data>
  <data key="d2">File format for storing configuration data in Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="status.qosClass field">
  <data key="d5">status.qosClass field</data>
  <data key="d2">Field in the pod's YAML/JSON manifest that shows its QoS class</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Table 14.1">
  <data key="d5">Table 14.1</data>
  <data key="d2">Table showing QoS classes for single-container pods based on resource requests and limits</data>
  <data key="d3">table</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Table 14.2">
  <data key="d5">Table 14.2</data>
  <data key="d2">Table showing a Pod’s QoS class derived from the classes of its containers</data>
  <data key="d3">table</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Validation">
  <data key="d5">Validation</data>
  <data key="d2">The process of checking the validity of resources in a cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Namespace XYZ">
  <data key="d5">Namespace XYZ</data>
  <data key="d2">A specific namespace with a LimitRange resource</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="LimitRanger Admission Control plugin">
  <data key="d5">LimitRanger Admission Control plugin</data>
  <data key="d2">A Kubernetes plugin that validates pod specs against LimitRange resources.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="LimitRange object">
  <data key="d5">LimitRange object</data>
  <data key="d2">A Kubernetes resource that specifies limits for pods and containers.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cpu usage">
  <data key="d5">cpu usage</data>
  <data key="d2">Maximum CPU usage per pod or container</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="memory usage">
  <data key="d5">memory usage</data>
  <data key="d2">Maximum memory usage per pod or container</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="LimitRanger plugin">
  <data key="d5">LimitRanger plugin</data>
  <data key="d2">an Admission Control plugin that enforces policies configured in LimitRange resources</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ResourceQuota Admission Control plugin">
  <data key="d5">ResourceQuota Admission Control plugin</data>
  <data key="d2">an Admission Control plugin that checks whether a pod being created would cause the configured ResourceQuota to be exceeded</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="hard">
  <data key="d5">hard</data>
  <data key="d2">a field in the ResourceQuota object that defines the maximum amount of resources allowed</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl describe command">
  <data key="d5">kubectl describe command</data>
  <data key="d2">command used to see how much of the quota is already used up</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cpu-and-mem">
  <data key="d5">cpu-and-mem</data>
  <data key="d2">ResourceQuota object name</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="limits.cpu 200m">
  <data key="d5">limits.cpu 200m</data>
  <data key="d2">maximum total CPU limits in the namespace</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="limits.memory 100Mi">
  <data key="d5">limits.memory 100Mi</data>
  <data key="d2">maximum total limits for memory resources</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="requests.cpu 100m">
  <data key="d5">requests.cpu 100m</data>
  <data key="d2">maximum amount of CPU pods can request</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="requests.memory 10Mi">
  <data key="d5">requests.memory 10Mi</data>
  <data key="d2">maximum total requests for memory resources</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Storage">
  <data key="d5">Storage</data>
  <data key="d2">A resource that can be limited by ResourceQuota</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="loadbalancers">
  <data key="d5">loadbalancers</data>
  <data key="d2">A Service that distributes incoming traffic across multiple Pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nodeports">
  <data key="d5">nodeports</data>
  <data key="d2">A Service that exposes a single port on the node's IP address</data>
  <data key="d3">networking</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="quota scopes">
  <data key="d5">quota scopes</data>
  <data key="d2">Four quota scopes available: BestEffort, NotBestEffort, Terminating, and NotTerminating.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="BestEffort scope">
  <data key="d5">BestEffort scope</data>
  <data key="d2">Applies to pods with the BestEffort QoS class or with one of the other two classes (Burstable and Guaranteed).</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NotBestEffort scope">
  <data key="d5">NotBestEffort scope</data>
  <data key="d2">Determines whether the quota applies to pods with the NotBestEffort QoS class.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Terminating scope">
  <data key="d5">Terminating scope</data>
  <data key="d2">Applies to pods that have the activeDeadlineSeconds set, marking them as Failed and then terminated.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NotTerminating scope">
  <data key="d5">NotTerminating scope</data>
  <data key="d2">Applies to pods that don't have an active deadline set.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU/memory requests">
  <data key="d5">CPU/memory requests</data>
  <data key="d2">Resource limits for pods, which can be limited by a ResourceQuota.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU/memory limits">
  <data key="d5">CPU/memory limits</data>
  <data key="d2">Resource limits for pods, which can be limited by a ResourceQuota.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl top command">
  <data key="d5">kubectl top command</data>
  <data key="d2">A command used to display resource usages for nodes and individual pods.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="minikube addons enable heapster command">
  <data key="d5">minikube addons enable heapster command</data>
  <data key="d2">A command used to enable Heapster as an add-on in Minikube.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl top node command">
  <data key="d5">kubectl top node command</data>
  <data key="d2">A command used to display actual CPU and memory usage of all pods running on a node.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl top pod command">
  <data key="d5">kubectl top pod command</data>
  <data key="d2">A command used to display actual CPU and memory usage of individual pods.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="top_pod.go">
  <data key="d5">top_pod.go</data>
  <data key="d2">file containing top pod command implementation</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Google Cloud Monitoring">
  <data key="d5">Google Cloud Monitoring</data>
  <data key="d2">service for monitoring Google Kubernetes Engine clusters</data>
  <data key="d3">cloud service</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="resource consumption statistics">
  <data key="d5">resource consumption statistics</data>
  <data key="d2">data about how much CPU or memory pods consumed throughout a period</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cluster-info">
  <data key="d5">cluster-info</data>
  <data key="d2">A command to display information about the Kubernetes cluster</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="network">
  <data key="d5">network</data>
  <data key="d2">A type of computational resource that measures data transfer rate</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Resource limits">
  <data key="d5">Resource limits</data>
  <data key="d2">The maximum amount of resources a pod can use</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Memory usage">
  <data key="d5">Memory usage</data>
  <data key="d2">The amount of memory used by a container</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="QoS classes">
  <data key="d5">QoS classes</data>
  <data key="d2">A way to prioritize pods based on their resource needs</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU request">
  <data key="d5">CPU request</data>
  <data key="d2">The amount of processing power requested by a pod</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Memory request">
  <data key="d5">Memory request</data>
  <data key="d2">The amount of memory requested by a pod</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Replicas field">
  <data key="d5">Replicas field</data>
  <data key="d2">A field in the ReplicationController, ReplicaSet, and Deployment resources that determines how many replicas of a pod to run.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="custom metrics">
  <data key="d5">custom metrics</data>
  <data key="d2">User-defined metrics that can be used to determine the load on a system, used for automatic horizontal scaling.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="autoscaling">
  <data key="d5">autoscaling</data>
  <data key="d2">Feature in Kubernetes that scales pods and nodes</data>
  <data key="d3">feature</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="REST calls">
  <data key="d5">REST calls</data>
  <data key="d2">Method used by HPA to query Heapster for metrics</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod metrics">
  <data key="d5">Pod metrics</data>
  <data key="d2">Metrics collected from pods to determine the required number of replicas.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Resource metrics API">
  <data key="d5">Resource metrics API</data>
  <data key="d2">An API that provides metrics for resources in a cluster.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Metrics collector">
  <data key="d5">Metrics collector</data>
  <data key="d2">A component that collects metrics from pods and other sources.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Autoscaling operation">
  <data key="d5">Autoscaling operation</data>
  <data key="d2">The final step of an autoscaling operation</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="desired replica count field">
  <data key="d5">desired replica count field</data>
  <data key="d2">A field on the scaled resource object</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Autoscaler controller">
  <data key="d5">Autoscaler controller</data>
  <data key="d2">A controller that modifies the replicas field through the Scale sub-resource</data>
  <data key="d3">controller</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Scale sub-resource">
  <data key="d5">Scale sub-resource</data>
  <data key="d2">A sub-resource of the API server that exposes the scaleable resource</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Target CPU utilization">
  <data key="d5">Target CPU utilization</data>
  <data key="d2">A target value for CPU utilization</data>
  <data key="d3">target</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Target QPS">
  <data key="d5">Target QPS</data>
  <data key="d2">A target value for QPS</data>
  <data key="d3">target</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="replicas: 3">
  <data key="d5">replicas: 3</data>
  <data key="d2">The initial number of replicas for the Deployment.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="metadata: name: kubia">
  <data key="d5">metadata: name: kubia</data>
  <data key="d2">Information about the pod being created, including its name.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="labels: app: kubia">
  <data key="d5">labels: app: kubia</data>
  <data key="d2">Labels applied to the pod for identification and organization.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="image: luksa/kubia:v1">
  <data key="d5">image: luksa/kubia:v1</data>
  <data key="d2">The image being used to create the container.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="name: nodejs">
  <data key="d5">name: nodejs</data>
  <data key="d2">The name of the container being created.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl autoscale command">
  <data key="d5">kubectl autoscale command</data>
  <data key="d2">A command used to create a HorizontalPodAutoscaler (HPA) object and enable horizontal autoscaling.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="autoscaling/v2beta1">
  <data key="d5">autoscaling/v2beta1</data>
  <data key="d2">API version for Horizontal Pod Autoscaler</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Metrics">
  <data key="d5">Metrics</data>
  <data key="d2">Data collected by Heapster to monitor the cluster</data>
  <data key="d3">data</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Target">
  <data key="d5">Target</data>
  <data key="d2">Desired value for a metric, in this case, CPU utilization</data>
  <data key="d3">data</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="MinPods">
  <data key="d5">MinPods</data>
  <data key="d2">Minimum number of pods allowed by the autoscaler</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="MaxPods">
  <data key="d5">MaxPods</data>
  <data key="d2">Maximum number of pods allowed by the autoscaler</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-it">
  <data key="d5">-it</data>
  <data key="d2">option to attach console to process</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--rm">
  <data key="d5">--rm</data>
  <data key="d2">option to delete pod after execution</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--restart=Never">
  <data key="d5">--restart=Never</data>
  <data key="d2">option to create unmanaged pod directly</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="targetAverageUtilization field">
  <data key="d5">targetAverageUtilization field</data>
  <data key="d2">field in HPA resource to set CPU utilization target</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="maxReplicas parameter">
  <data key="d5">maxReplicas parameter</data>
  <data key="d2">parameter to set maximum number of replicas</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="metrics section">
  <data key="d5">metrics section</data>
  <data key="d2">section in HPA resource to specify metrics for scaling</data>
  <data key="d3">section</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Resource type">
  <data key="d5">Resource type</data>
  <data key="d2">type of resource being scaled (e.g. CPU)</data>
  <data key="d3">resource type</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="autoscaler controller">
  <data key="d5">autoscaler controller</data>
  <data key="d2">controller that detects changes to HPA resource and scales pods accordingly</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Horizontal Autoscaler">
  <data key="d5">Horizontal Autoscaler</data>
  <data key="d2">Kubernetes component for scaling pods</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="memory consumption">
  <data key="d5">memory consumption</data>
  <data key="d2">Metric used to scale pods based on memory usage</data>
  <data key="d3">metric</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HPA (Horizontal Pod Autoscaler)">
  <data key="d5">HPA (Horizontal Pod Autoscaler)</data>
  <data key="d2">Kubernetes component for scaling pods based on CPU utilization</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="metrics field">
  <data key="d5">metrics field</data>
  <data key="d2">A field in the HPA object that defines the metric to use for scaling.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pods metric">
  <data key="d5">Pods metric</data>
  <data key="d2">A type of metric that is based on the pod itself, such as QPS or message queue size.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Object metric">
  <data key="d5">Object metric</data>
  <data key="d2">A type of metric that is not directly related to the pod, but rather to another cluster object, like an Ingress object.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="container's resource requests">
  <data key="d5">container's resource requests</data>
  <data key="d2">The resources requested by a container, such as CPU or memory.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Queries-Per-Second (QPS)">
  <data key="d5">Queries-Per-Second (QPS)</data>
  <data key="d2">A metric that measures the number of queries per second.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="message broker's queue">
  <data key="d5">message broker's queue</data>
  <data key="d2">A message queue used by a message broker, such as RabbitMQ or Apache Kafka.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Ingress object">
  <data key="d5">Ingress object</data>
  <data key="d2">A Kubernetes object that defines an ingress point for incoming HTTP requests.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Object">
  <data key="d5">Object</data>
  <data key="d2">A Kubernetes object, such as a pod or deployment</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="latencyMillis">
  <data key="d5">latencyMillis</data>
  <data key="d2">A metric measuring latency in milliseconds</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="targetValue">
  <data key="d5">targetValue</data>
  <data key="d2">The target value for a metric, used by the horizontal pod autoscaler</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="frontend">
  <data key="d5">frontend</data>
  <data key="d2">The name of a Kubernetes ingress object</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Vertical pod autoscaling">
  <data key="d5">Vertical pod autoscaling</data>
  <data key="d2">A feature that allows pods to be scaled down to zero and brought up when a new request comes in.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="InitialResources">
  <data key="d5">InitialResources</data>
  <data key="d2">An experimental feature that sets CPU and memory requests on newly created pods.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Tag">
  <data key="d5">Tag</data>
  <data key="d2">A label used to identify a specific version or build of a container image.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Historical resource usage data">
  <data key="d5">Historical resource usage data</data>
  <data key="d2">Data about the past resource usage of a pod's containers.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Vertical scaling">
  <data key="d5">Vertical scaling</data>
  <data key="d2">The process of increasing the resources (CPU and memory) available to a pod.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod Autoscaler">
  <data key="d5">Pod Autoscaler</data>
  <data key="d2">Automatically scales pod instances</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Node group">
  <data key="d5">Node group</data>
  <data key="d2">Group of same-sized nodes or nodes with the same features</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API call">
  <data key="d5">API call</data>
  <data key="d2">Request to cloud provider to add additional node</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CPU and memory requests">
  <data key="d5">CPU and memory requests</data>
  <data key="d2">the amount of CPU and memory resources required by pods running on a node</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="system pods">
  <data key="d5">system pods</data>
  <data key="d2">pods that run on every node in the cluster, such as those deployed by a DaemonSet</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="unmanaged pod">
  <data key="d5">unmanaged pod</data>
  <data key="d2">a pod that is not managed by a controller or ReplicaSet</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod with local storage">
  <data key="d5">pod with local storage</data>
  <data key="d2">a pod that requires access to local storage on a node</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node group">
  <data key="d5">node group</data>
  <data key="d2">a collection of nodes in a Kubernetes cluster that can be scaled up or down together</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Google Compute Engine (GCE)">
  <data key="d5">Google Compute Engine (GCE)</data>
  <data key="d2">a service for running virtual machines in the cloud, provided by Google Cloud Platform</data>
  <data key="d3">cloud platform</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Amazon Web Services (AWS)">
  <data key="d5">Amazon Web Services (AWS)</data>
  <data key="d2">a comprehensive cloud computing platform provided by Amazon</data>
  <data key="d3">cloud platform</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Microsoft Azure">
  <data key="d5">Microsoft Azure</data>
  <data key="d2">a cloud computing platform provided by Microsoft</data>
  <data key="d3">cloud platform</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cordon">
  <data key="d5">cordon</data>
  <data key="d2">a kubectl command that marks a node as unschedulable but doesn't evict pods</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="drain">
  <data key="d5">drain</data>
  <data key="d2">a kubectl command that marks a node as unschedulable and evicts all pods from the node</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="uncordon">
  <data key="d5">uncordon</data>
  <data key="d2">a kubectl command that unmarks a node as unschedulable, allowing new pods to be scheduled on it</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod-DisruptionBudget">
  <data key="d5">Pod-DisruptionBudget</data>
  <data key="d2">a Kubernetes resource that specifies the minimum number of pods that must always be available</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Drain command">
  <data key="d5">Drain command</data>
  <data key="d2">a kubectl command that evicts a node from the cluster, allowing it to be updated or replaced</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="MinAvailable">
  <data key="d5">MinAvailable</data>
  <data key="d2">a field in the Pod-DisruptionBudget resource that specifies the minimum number of pods that must always be available</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="MaxUnavailable">
  <data key="d5">MaxUnavailable</data>
  <data key="d2">a field in the Pod-DisruptionBudget resource that specifies the maximum number of pods that can be unavailable</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="minAvailable">
  <data key="d5">minAvailable</data>
  <data key="d2">Parameter for specifying the minimum available pods</data>
  <data key="d3">parameter</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CTRL+C">
  <data key="d5">CTRL+C</data>
  <data key="d2">Key combination for stopping and deleting a pod</data>
  <data key="d3">key combination</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node selector">
  <data key="d5">node selector</data>
  <data key="d2">Mechanism to specify a node for pod scheduling</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node affinity rules">
  <data key="d5">node affinity rules</data>
  <data key="d2">Alternative to node selectors for pod scheduling</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="describe node">
  <data key="d5">describe node</data>
  <data key="d2">A command used to display information about a node in a Kubernetes cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Control Plane pods">
  <data key="d5">Control Plane pods</data>
  <data key="d2">Pods that run the control plane components of a Kubernetes cluster</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="master node">
  <data key="d5">master node</data>
  <data key="d2">The master node in a Kubernetes cluster</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node-role.kubernetes.io/master">
  <data key="d5">node-role.kubernetes.io/master</data>
  <data key="d2">a taint applied to the master node</data>
  <data key="d3">kubernetes</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Exists:NoExecute">
  <data key="d5">Exists:NoExecute</data>
  <data key="d2">a toleration effect that prevents pods from running on a node if it is unreachable or not ready</data>
  <data key="d3">kubernetes</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Tolerations">
  <data key="d5">Tolerations</data>
  <data key="d2">Allow pods to run on tainted nodes</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Taints">
  <data key="d5">Taints</data>
  <data key="d2">Mark nodes with specific conditions</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl taint command">
  <data key="d5">kubectl taint command</data>
  <data key="d2">Command to add custom taints to a node</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NoSchedule effect">
  <data key="d5">NoSchedule effect</data>
  <data key="d2">Effect that prevents pod scheduling on a node</data>
  <data key="d3">effect</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod YAML">
  <data key="d5">pod YAML</data>
  <data key="d2">YAML snippet for adding tolerations to pods</data>
  <data key="d3">yaml</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="toleration">
  <data key="d5">toleration</data>
  <data key="d2">Mechanism to specify how long Kubernetes should wait before rescheduling a pod</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ kubectl get po prod-350605-1ph5h -o yaml">
  <data key="d5">$ kubectl get po prod-350605-1ph5h -o yaml</data>
  <data key="d2">Command to retrieve pod information in YAML format</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node.alpha.kubernetes.io/unreachable">
  <data key="d5">node.alpha.kubernetes.io/unreachable</data>
  <data key="d2">Label for a node that is unreachable</data>
  <data key="d3">label</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tolerationSeconds">
  <data key="d5">tolerationSeconds</data>
  <data key="d2">Field to specify how long Kubernetes should wait before rescheduling a pod</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--feature-gates=Taint-BasedEvictions=true">
  <data key="d5">--feature-gates=Taint-BasedEvictions=true</data>
  <data key="d2">Option to enable taint-based evictions in the Controller Manager</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Listing 16.6">
  <data key="d5">Listing 16.6</data>
  <data key="d2">Reference to a listing in the chapter</data>
  <data key="d3">reference</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Node Selectors">
  <data key="d5">Node Selectors</data>
  <data key="d2">A feature that allows you to specify which nodes a pod can be scheduled on.</data>
  <data key="d3">Software/Application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GKE (Google Kubernetes Engine)">
  <data key="d5">GKE (Google Kubernetes Engine)</data>
  <data key="d2">A managed container orchestration service provided by Google Cloud Platform.</data>
  <data key="d3">Cloud Service</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Node Pools">
  <data key="d5">Node Pools</data>
  <data key="d2">A group of nodes that can be used for scheduling pods.</data>
  <data key="d3">Software/Application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Region">
  <data key="d5">Region</data>
  <data key="d2">The geographical region where a node is located.</data>
  <data key="d3">Hardware/Location</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Zone">
  <data key="d5">Zone</data>
  <data key="d2">The availability zone where a node is located.</data>
  <data key="d3">Hardware/Location</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="requiredDuringSchedulingIgnoredDuringExecution">
  <data key="d5">requiredDuringSchedulingIgnoredDuringExecution</data>
  <data key="d2">a field in the nodeAffinity rule that specifies the labels a node must have for the pod to be scheduled to it, and ignores existing pods on the node</data>
  <data key="d3">software,application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="key: gpu">
  <data key="d5">key: gpu</data>
  <data key="d2">a key-value pair in the matchExpressions that specifies the label to match for the node selector</data>
  <data key="d3">hardware,label</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="operator: In">
  <data key="d5">operator: In</data>
  <data key="d2">an operator in the matchExpressions that specifies how to match the label for the node selector</data>
  <data key="d3">software,application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="values: - true">
  <data key="d5">values: - true</data>
  <data key="d2">a list of values in the matchExpressions that specifies the value to match for the node selector</data>
  <data key="d3">hardware,label</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gpu label">
  <data key="d5">gpu label</data>
  <data key="d2">a specific label on a node that indicates it has a GPU</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="availability zone">
  <data key="d5">availability zone</data>
  <data key="d2">a separate availability zone, such as a datacenter across different countries</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pref">
  <data key="d5">pref</data>
  <data key="d2">Name of a Deployment with preferred node affinity</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="preferred-deployment.yaml">
  <data key="d5">preferred-deployment.yaml</data>
  <data key="d2">Deployment manifest for specifying preferential node affinity rules</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="zone1">
  <data key="d5">zone1</data>
  <data key="d2">a specific availability zone</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dedicated">
  <data key="d5">dedicated</data>
  <data key="d2">a specific share type</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="-o wide">
  <data key="d5">-o wide</data>
  <data key="d2">option to display pod information in wide format</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pref-607515-1rnwv">
  <data key="d5">pref-607515-1rnwv</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pref-607515-27wp0">
  <data key="d5">pref-607515-27wp0</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pref-607515-5xd0z">
  <data key="d5">pref-607515-5xd0z</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pref-607515-jx9wt">
  <data key="d5">pref-607515-jx9wt</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pref-607515-mlgqm">
  <data key="d5">pref-607515-mlgqm</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Selector-SpreadPriority">
  <data key="d5">Selector-SpreadPriority</data>
  <data key="d2">Kubernetes scheduling function to spread pods across nodes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="frontend pod">
  <data key="d5">frontend pod</data>
  <data key="d2">type of pod for a frontend service</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod affinity configuration">
  <data key="d5">pod affinity configuration</data>
  <data key="d2">configuration to place pods near each other</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="get po -o wide">
  <data key="d5">get po -o wide</data>
  <data key="d2">a command for getting pod information in wide format</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="frontend-podaffinity-host.yaml">
  <data key="d5">frontend-podaffinity-host.yaml</data>
  <data key="d2">a YAML file defining a frontend pod with pod affinity rules</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="podAffinity configuration">
  <data key="d5">podAffinity configuration</data>
  <data key="d2">a set of rules defining how pods are scheduled based on affinity labels</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="frontend pods">
  <data key="d5">frontend pods</data>
  <data key="d2">a set of frontend pods that were scheduled to node2.k8s based on pod affinity rules</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="anti-affinity">
  <data key="d5">anti-affinity</data>
  <data key="d2">a scheduling strategy that prevents pods from being deployed on the same node</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="backend-qhqj6">
  <data key="d5">backend-qhqj6</data>
  <data key="d2">a pod name</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="InterPodAffinityPriority">
  <data key="d5">InterPodAffinityPriority</data>
  <data key="d2">a scheduling priority for inter-pod affinity</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="SelectorSpreadPriority">
  <data key="d5">SelectorSpreadPriority</data>
  <data key="d2">a scheduling priority for selector spread</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="NodeAffinityPriority">
  <data key="d5">NodeAffinityPriority</data>
  <data key="d2">a scheduling priority for node affinity</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="failure-domain.beta.kubernetes.io/zone">
  <data key="d5">failure-domain.beta.kubernetes.io/zone</data>
  <data key="d2">a label key for specifying an availability zone</data>
  <data key="d3">label</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="failure-domain.beta.kubernetes.io/region">
  <data key="d5">failure-domain.beta.kubernetes.io/region</data>
  <data key="d2">a label key for specifying a geographical region</data>
  <data key="d3">label</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="podAffinityTerm">
  <data key="d5">podAffinityTerm</data>
  <data key="d2">a specification for the pod affinity term</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="backend-257820-ssrgj">
  <data key="d5">backend-257820-ssrgj</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="frontend-941083-3mff9">
  <data key="d5">frontend-941083-3mff9</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="frontend-941083-7fp7d">
  <data key="d5">frontend-941083-7fp7d</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="frontend-941083-cq23b">
  <data key="d5">frontend-941083-cq23b</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="frontend-941083-m70sw">
  <data key="d5">frontend-941083-m70sw</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="frontend-941083-wsjv8">
  <data key="d5">frontend-941083-wsjv8</data>
  <data key="d2">pod name and ID</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="podAntiAffinity">
  <data key="d5">podAntiAffinity</data>
  <data key="d2">property for specifying pod anti-affinity preferences</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Topology key">
  <data key="d5">Topology key</data>
  <data key="d2">key used to specify the topology of a node</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod Affinity">
  <data key="d5">Pod Affinity</data>
  <data key="d2">A feature that allows you to schedule pods on specific nodes based on labels and selectors.</data>
  <data key="d3">Software/Application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pod AntiAffinity">
  <data key="d5">Pod AntiAffinity</data>
  <data key="d2">A feature that prevents pods from being scheduled on the same node based on labels and selectors.</data>
  <data key="d3">Software/Application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="LabelSelector">
  <data key="d5">LabelSelector</data>
  <data key="d2">A selector used to match pods based on labels attached to them.</data>
  <data key="d3">Software/Application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="TopologyKey">
  <data key="d5">TopologyKey</data>
  <data key="d2">A key used to identify the topology of a node, such as its hostname or IP address.</data>
  <data key="d3">Software/Framework</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="$ kubectl get po -l app=frontend -o wide">
  <data key="d5">$ kubectl get po -l app=frontend -o wide</data>
  <data key="d2">A command used to retrieve information about pods with a specific label.</data>
  <data key="d3">Software/Command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="taint">
  <data key="d5">taint</data>
  <data key="d2">a label that prevents pods from being scheduled on a node</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="affinity">
  <data key="d5">affinity</data>
  <data key="d2">a property of a pod that specifies which nodes it can be scheduled on</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="hard requirement">
  <data key="d5">hard requirement</data>
  <data key="d2">a type of node affinity that requires pods to be scheduled on specific nodes</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Resources">
  <data key="d5">Resources</data>
  <data key="d2">Components that make up a Kubernetes application</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Hooks">
  <data key="d5">Hooks</data>
  <data key="d2">Functions that run at specific points in the pod lifecycle</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Init containers">
  <data key="d5">Init containers</data>
  <data key="d2">Containers that run before the main container in a pod</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Volume(s)">
  <data key="d5">Volume(s)</data>
  <data key="d2">Provides persistent storage for pods</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ReplicaSet(s)">
  <data key="d5">ReplicaSet(s)</data>
  <data key="d2">Manages the number of replicas of a pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Health probes">
  <data key="d5">Health probes</data>
  <data key="d2">Checks the health of a container or pod</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Volume mounts">
  <data key="d5">Volume mounts</data>
  <data key="d2">Mounts a volume to a container's file system</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Resource reqs/limits">
  <data key="d5">Resource reqs/limits</data>
  <data key="d2">Defines resource constraints for pods and containers</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Secret(s)">
  <data key="d5">Secret(s)</data>
  <data key="d2">Stores sensitive data, such as passwords and API keys</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Persistent Volume Claim">
  <data key="d5">Persistent Volume Claim</data>
  <data key="d2">Requests storage resources from a cluster</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="clustered app">
  <data key="d5">clustered app</data>
  <data key="d2">Application running in multiple pods</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="OOMKiller">
  <data key="d5">OOMKiller</data>
  <data key="d2">Process that kills processes when the node runs out of memory</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod-scoped volume">
  <data key="d5">pod-scoped volume</data>
  <data key="d2">Volume that lives and dies together with the pod</data>
  <data key="d3">storage</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Writes to">
  <data key="d5">Writes to</data>
  <data key="d2">An operation that saves data to a file or filesystem.</data>
  <data key="d3">Process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Writable layer">
  <data key="d5">Writable layer</data>
  <data key="d2">A part of the container's filesystem where changes can be made.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Read-only layer">
  <data key="d5">Read-only layer</data>
  <data key="d2">A part of the container's filesystem that cannot be modified.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Image layers">
  <data key="d5">Image layers</data>
  <data key="d2">Pre-built components that make up a container image.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="New container">
  <data key="d5">New container</data>
  <data key="d2">A new instance of a container created to replace an existing one.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="New process">
  <data key="d5">New process</data>
  <data key="d2">A new program in execution within a container.</data>
  <data key="d3">Process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Filesystem volumeMount">
  <data key="d5">Filesystem volumeMount</data>
  <data key="d2">The ability to mount a filesystem as a directory within a container.</data>
  <data key="d3">Hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune-client pod">
  <data key="d5">fortune-client pod</data>
  <data key="d2">a pod that requires the fortune Service to be up before its main container starts</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="initContainers field">
  <data key="d5">initContainers field</data>
  <data key="d2">specifies init containers in a pod's YAML spec</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="while true loop">
  <data key="d5">while true loop</data>
  <data key="d2">a loop that runs until the fortune Service is up</data>
  <data key="d3">programming language</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="wget command">
  <data key="d5">wget command</data>
  <data key="d2">used to check if the fortune Service is responding to requests</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="init container">
  <data key="d5">init container</data>
  <data key="d2">An init container is a special type of container that runs before the main container(s) in a pod.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="fortune-server pod">
  <data key="d5">fortune-server pod</data>
  <data key="d2">A pod that runs the fortune server.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="lifecycle hooks">
  <data key="d5">lifecycle hooks</data>
  <data key="d2">Hooks that can be used to execute commands or perform HTTP requests when a container starts or stops.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="post-start hooks">
  <data key="d5">post-start hooks</data>
  <data key="d2">A type of lifecycle hook that is executed when a container starts.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pre-stop hooks">
  <data key="d5">pre-stop hooks</data>
  <data key="d2">A type of lifecycle hook that is executed before a container stops.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Post-Start Hook">
  <data key="d5">Post-Start Hook</data>
  <data key="d2">A hook executed immediately after the container's main process is started.</data>
  <data key="d3">Container Lifecycle</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Pending">
  <data key="d5">Pending</data>
  <data key="d2">The status of a pod while its containers are being created.</data>
  <data key="d3">Pod Status</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="postStart">
  <data key="d5">postStart</data>
  <data key="d2">A field in the lifecycle hook specification that specifies a post-start hook to be executed when the container starts.</data>
  <data key="d3">Lifecycle Hook</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="sh">
  <data key="d5">sh</data>
  <data key="d2">The shell used to execute a shell script or binary executable file stored in the container image.</data>
  <data key="d3">Shell</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="exit">
  <data key="d5">exit</data>
  <data key="d2">A command used to terminate the execution of a hook with a specified exit code.</data>
  <data key="d3">Hook Command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PostStart Hook">
  <data key="d5">PostStart Hook</data>
  <data key="d2">A hook executed immediately after a container starts.</data>
  <data key="d3">container lifecycle</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="FailedSync">
  <data key="d5">FailedSync</data>
  <data key="d2">An error syncing pod, skipping: failed to start container.</data>
  <data key="d3">pod events</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PostStart handler">
  <data key="d5">PostStart handler</data>
  <data key="d2">A command executed immediately after a container starts.</data>
  <data key="d3">container lifecycle</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Get http://10.32.0.2:9090/postStart">
  <data key="d5">Get http://10.32.0.2:9090/postStart</data>
  <data key="d2">An HTTP GET hook handler.</data>
  <data key="d3">container lifecycle</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dial tcp 10.32.0.2:9090">
  <data key="d5">dial tcp 10.32.0.2:9090</data>
  <data key="d2">A network error connecting to port 9090.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl exec my-pod cat logfile.txt">
  <data key="d5">kubectl exec my-pod cat logfile.txt</data>
  <data key="d2">A command to execute a process in a pod and display its output.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="scheme">
  <data key="d5">scheme</data>
  <data key="d2">A field used to specify the scheme (HTTP or HTTPS) for an HTTP GET request.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="httpHeaders">
  <data key="d5">httpHeaders</data>
  <data key="d2">A field used to specify headers that should be sent in an HTTP GET request.</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="shell script">
  <data key="d5">shell script</data>
  <data key="d2">A script that runs as the main container process and can handle signals and pass them on to child processes.</data>
  <data key="d3">script</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="deletionTimestamp">
  <data key="d5">deletionTimestamp</data>
  <data key="d2">A field in a Pod object that indicates when the pod is scheduled to be deleted.</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="spec.terminationGracePeriodSeconds">
  <data key="d5">spec.terminationGracePeriodSeconds</data>
  <data key="d2">A field in the pod spec that configures the termination grace period.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod instances">
  <data key="d5">pod instances</data>
  <data key="d2">The individual containers that make up a pod.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="data-migrating pod">
  <data key="d5">data-migrating pod</data>
  <data key="d2">A pod that runs a script to migrate data from one location to another.</data>
  <data key="d3">pod</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Service Endpoints">
  <data key="d5">Service Endpoints</data>
  <data key="d2">List of IP addresses of pods providing a service</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="HTTP GET">
  <data key="d5">HTTP GET</data>
  <data key="d2">Request method for retrieving data from a server</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Scale down">
  <data key="d5">Scale down</data>
  <data key="d2">Action to reduce the number of replicas</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="client requests">
  <data key="d5">client requests</data>
  <data key="d2">requests handled properly by Kubernetes</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="pod deletion notification">
  <data key="d5">pod deletion notification</data>
  <data key="d2">notification sent to components when a pod is deleted</data>
  <data key="d3">event</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Iptables rules">
  <data key="d5">Iptables rules</data>
  <data key="d2">Rules used to filter and manage incoming and outgoing network traffic in a Linux system.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Connection Refused error">
  <data key="d5">Connection Refused error</data>
  <data key="d2">An error that occurs when a client tries to connect to a terminated pod.</data>
  <data key="d3">error</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="REST request">
  <data key="d5">REST request</data>
  <data key="d2">A type of HTTP request used for interacting with web services.</data>
  <data key="d3">protocol</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Watcher(s)">
  <data key="d5">Watcher(s)</data>
  <data key="d2">Components in the Kubernetes control plane that monitor and respond to changes in cluster resources.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="deletionTimestamp field">
  <data key="d5">deletionTimestamp field</data>
  <data key="d2">A field in the pod's spec that indicates when it will be deleted</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kube-proxies">
  <data key="d5">kube-proxies</data>
  <data key="d2">Components that modify iptables rules</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="load balancers">
  <data key="d5">load balancers</data>
  <data key="d2">Components that distribute traffic across multiple servers</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="client-side load-balancing">
  <data key="d5">client-side load-balancing</data>
  <data key="d2">A technique used by clients to distribute traffic across multiple servers</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="termination signal">
  <data key="d5">termination signal</data>
  <data key="d2">A signal sent to a pod to terminate it</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="FROM scratch directive">
  <data key="d5">FROM scratch directive</data>
  <data key="d2">Instruction in a Dockerfile for creating an empty image</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="dig">
  <data key="d5">dig</data>
  <data key="d2">DNS lookup tool</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="latest tag">
  <data key="d5">latest tag</data>
  <data key="d2">Tag used to refer to the most recent image version</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="tags">
  <data key="d5">tags</data>
  <data key="d2">container tags with version designator</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="multi-dimensional labels">
  <data key="d5">multi-dimensional labels</data>
  <data key="d2">multiple labels per resource for selection across dimensions</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/dev/termination-log">
  <data key="d5">/dev/termination-log</data>
  <data key="d2">Default file for termination message</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Container definition">
  <data key="d5">Container definition</data>
  <data key="d2">Section in the pod spec that defines a container</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/var/termination-reason">
  <data key="d5">/var/termination-reason</data>
  <data key="d2">Custom file for termination message</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Error">
  <data key="d5">Error</data>
  <data key="d2">Reason for container termination</data>
  <data key="d3">error</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Exit Code">
  <data key="d5">Exit Code</data>
  <data key="d2">Code returned by the container when it exits</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="terminationMessagePolicy field">
  <data key="d5">terminationMessagePolicy field</data>
  <data key="d2">a field that determines how a termination message is handled</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="FallbackToLogsOnError policy">
  <data key="d5">FallbackToLogsOnError policy</data>
  <data key="d2">a policy that uses the last few lines of a container's log as its termination message when it terminates unsuccessfully</data>
  <data key="d3">policy</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="standard output">
  <data key="d5">standard output</data>
  <data key="d2">the default output stream for an application</data>
  <data key="d3">output</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubernetes logs command">
  <data key="d5">kubernetes logs command</data>
  <data key="d2">a command to view container logs</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cat command">
  <data key="d5">cat command</data>
  <data key="d2">a command to display the contents of a file</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubectl cp command">
  <data key="d5">kubectl cp command</data>
  <data key="d2">a command to copy files from and into a container</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="log file">
  <data key="d5">log file</data>
  <data key="d2">a file containing application logs</data>
  <data key="d3">data storage</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="termination message">
  <data key="d5">termination message</data>
  <data key="d2">a message written to a file by a container when it terminates</data>
  <data key="d3">message</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="application logging">
  <data key="d5">application logging</data>
  <data key="d2">the process of writing logs from an application</data>
  <data key="d3">logging</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="logging">
  <data key="d5">logging</data>
  <data key="d2">process of recording events and errors</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="centralized logging">
  <data key="d5">centralized logging</data>
  <data key="d2">system for collecting and storing logs in a central location</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cluster-wide logging">
  <data key="d5">cluster-wide logging</data>
  <data key="d2">system for collecting and storing logs across multiple nodes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ELK stack">
  <data key="d5">ELK stack</data>
  <data key="d2">collection of tools for logging and analytics</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="EFK stack">
  <data key="d5">EFK stack</data>
  <data key="d2">modified version of ELK stack using FluentD instead of Logstash</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Java">
  <data key="d5">Java</data>
  <data key="d2">A programming language where exception stack traces can span multiple lines.</data>
  <data key="d3">programming language</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="node-level FluentD agent">
  <data key="d5">node-level FluentD agent</data>
  <data key="d2">A configuration of the FluentD agent on a node to process JSON logs.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="logging sidecar container">
  <data key="d5">logging sidecar container</data>
  <data key="d2">A container that processes JSON logs and writes them to a file.</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="IDE">
  <data key="d5">IDE</data>
  <data key="d2">An Integrated Development Environment where developers can run apps directly without containers.</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="BACKEND_SERVICE_HOST">
  <data key="d5">BACKEND_SERVICE_HOST</data>
  <data key="d2">An environment variable used to find the coordinates of a backend Service.</data>
  <data key="d3">environment variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="BACKEND_SERVICE_PORT">
  <data key="d5">BACKEND_SERVICE_PORT</data>
  <data key="d2">An environment variable used to find the port number of a backend Service.</data>
  <data key="d3">environment variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Docker volumes">
  <data key="d5">Docker volumes</data>
  <data key="d2">Feature that allows mounting local filesystems into containers</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DOCKER_HOST">
  <data key="d5">DOCKER_HOST</data>
  <data key="d2">An environment variable that points to the Docker daemon.</data>
  <data key="d3">environment variable</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="minikube docker-env">
  <data key="d5">minikube docker-env</data>
  <data key="d2">A command that sets up the environment variables for Minikube.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Version Control System">
  <data key="d5">Version Control System</data>
  <data key="d2">A system for storing and managing code, such as Git or SVN.</data>
  <data key="d3">system</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kube-applier">
  <data key="d5">Kube-applier</data>
  <data key="d2">A tool that automates the deployment of Kubernetes manifests</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Version Control System (VCS)">
  <data key="d5">Version Control System (VCS)</data>
  <data key="d2">A system for managing and tracking changes to code</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes resource manifests">
  <data key="d5">Kubernetes resource manifests</data>
  <data key="d2">Files that define the configuration of Kubernetes resources</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CI/CD">
  <data key="d5">CI/CD</data>
  <data key="d2">Continuous Integration and Continuous Delivery pipeline</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Fabric8 project">
  <data key="d5">Fabric8 project</data>
  <data key="d2">An integrated development platform for Kubernetes</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Jenkins">
  <data key="d5">Jenkins</data>
  <data key="d2">An open-source automation system</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="container lifecycle hooks">
  <data key="d5">container lifecycle hooks</data>
  <data key="d2">Hooks that can be used to customize container startup and shutdown</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="manifests">
  <data key="d5">manifests</data>
  <data key="d2">Files that define the desired state of a Kubernetes resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="image sizes">
  <data key="d5">image sizes</data>
  <data key="d2">small image sizes for apps</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Mini-kube">
  <data key="d5">Mini-kube</data>
  <data key="d2">local Kubernetes cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Platform-as-a-Service">
  <data key="d5">Platform-as-a-Service</data>
  <data key="d2">PaaS solutions on top of Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Platform-as-a-Service (PaaS)">
  <data key="d5">Platform-as-a-Service (PaaS)</data>
  <data key="d2">cloud computing model for deploying applications</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API servers">
  <data key="d5">API servers</data>
  <data key="d2">components that handle requests to custom API objects</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="OpenShift Container Platform">
  <data key="d5">OpenShift Container Platform</data>
  <data key="d2">Red Hat's PaaS solution built on top of Kubernetes</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Deis Workflow and Helm">
  <data key="d5">Deis Workflow and Helm</data>
  <data key="d2">tools for deploying applications to Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CustomResourceDefinitions">
  <data key="d5">CustomResourceDefinitions</data>
  <data key="d2">Kubernetes feature for defining custom resources</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Queue resource">
  <data key="d5">Queue resource</data>
  <data key="d2">High-level object representing a messaging broker</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Third-PartyResource">
  <data key="d5">Third-PartyResource</data>
  <data key="d2">Deprecated Kubernetes feature for defining custom resources</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Website resource">
  <data key="d5">Website resource</data>
  <data key="d2">Custom resource type for running static websites in Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="scope">
  <data key="d5">scope</data>
  <data key="d2">Scope of the custom resource (namespaced or cluster-wide)</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Namespaced">
  <data key="d5">Namespaced</data>
  <data key="d2">Scope for the custom resource (namespaced)</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="website-crd.yaml">
  <data key="d5">website-crd.yaml</data>
  <data key="d2">File containing the CustomResourceDefinition manifest</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="imaginary-kubia-website.yaml">
  <data key="d5">imaginary-kubia-website.yaml</data>
  <data key="d2">File containing an imaginary custom resource definition</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Custom Resource Definition">
  <data key="d5">Custom Resource Definition</data>
  <data key="d2">A CustomResourceDefinition (CRD) is used to define custom API objects.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API Group">
  <data key="d5">API Group</data>
  <data key="d2">A way to group related API resources together.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Version">
  <data key="d5">Version</data>
  <data key="d2">The version number of an API resource.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Website">
  <data key="d5">Website</data>
  <data key="d2">A custom API object representing a website.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="apps/v1beta1">
  <data key="d5">apps/v1beta1</data>
  <data key="d2">The API group and version for Deployments.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="extensions.example.com">
  <data key="d5">extensions.example.com</data>
  <data key="d2">The API group for the custom Website resource.</data>
  <data key="d3">Software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="websites">
  <data key="d5">websites</data>
  <data key="d2">custom resource type for storing website configurations</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Website.v1.extensions.example.com">
  <data key="d5">Website.v1.extensions.example.com</data>
  <data key="d2">API group and version for the custom Website resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="extensions.example.com/v1">
  <data key="d5">extensions.example.com/v1</data>
  <data key="d2">API group and version for the custom Website resource</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="website">
  <data key="d5">website</data>
  <data key="d2">resource type for storing website configurations</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="nginx server">
  <data key="d5">nginx server</data>
  <data key="d2">Web server software</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="git-sync process">
  <data key="d5">git-sync process</data>
  <data key="d2">Process that keeps a local directory synced with a Git repo</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DELETED watch event">
  <data key="d5">DELETED watch event</data>
  <data key="d2">an event sent by the API server when a Website resource instance is deleted</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Deployment resource">
  <data key="d5">Deployment resource</data>
  <data key="d2">a Kubernetes resource that manages replicas of a pod</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="serviceAccountName">
  <data key="d5">serviceAccountName</data>
  <data key="d2">the name of a service account</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="luksa/website-controller">
  <data key="d5">luksa/website-controller</data>
  <data key="d2">the image name for the website controller</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Role Based Access Control (RBAC)">
  <data key="d5">Role Based Access Control (RBAC)</data>
  <data key="d2">a mechanism for controlling access to resources in a Kubernetes cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubia Website">
  <data key="d5">kubia Website</data>
  <data key="d2">a type of Kubernetes resource that represents the kubia website</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="website-controller">
  <data key="d5">website-controller</data>
  <data key="d2">the name of the website controller service account</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="proxy sidecar">
  <data key="d5">proxy sidecar</data>
  <data key="d2">a type of container that provides proxy functionality</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="get deploy,svc,po">
  <data key="d5">get deploy,svc,po</data>
  <data key="d2">The command used to list all Deployments, Services, and Pods.</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="cluster-IP">
  <data key="d5">cluster-IP</data>
  <data key="d2">The Cluster IP address of the Service.</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="validation schema">
  <data key="d5">validation schema</data>
  <data key="d2">A validation schema to validate the contents of the YAML.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gitRepo field">
  <data key="d5">gitRepo field</data>
  <data key="d2">A required field in the Website object.</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="CustomResourceValidation">
  <data key="d5">CustomResourceValidation</data>
  <data key="d2">Feature gate for validating custom objects</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="JSON schema">
  <data key="d5">JSON schema</data>
  <data key="d2">Schema for validating custom object definitions</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API server aggregation">
  <data key="d5">API server aggregation</data>
  <data key="d2">Feature for integrating multiple API servers into a single location</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Custom API server">
  <data key="d5">Custom API server</data>
  <data key="d2">API server responsible for handling custom objects</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Main API server">
  <data key="d5">Main API server</data>
  <data key="d2">Primary API server for Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="etcd store">
  <data key="d5">etcd store</data>
  <data key="d2">Key-value store for storing Kubernetes resources</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="APIService">
  <data key="d5">APIService</data>
  <data key="d2">Resource for registering a custom API server</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API group">
  <data key="d5">API group</data>
  <data key="d2">Namespace for APIs (e.g. extensions.example.com)</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="custom API server">
  <data key="d5">custom API server</data>
  <data key="d2">API server built on top of Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GitHub repos">
  <data key="d5">GitHub repos</data>
  <data key="d2">Kubernetes source code repository on GitHub</data>
  <data key="d3">software development</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Client pod">
  <data key="d5">Client pod</data>
  <data key="d2">Pod that uses a service to authenticate</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Team">
  <data key="d5">Team</data>
  <data key="d2">Group of users who are responsible for deploying general services</data>
  <data key="d3">group</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Ticket">
  <data key="d5">Ticket</data>
  <data key="d2">Request for a service to be provisioned by the team</data>
  <data key="d3">request</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Manifests">
  <data key="d5">Manifests</data>
  <data key="d2">Files that define the configuration and deployment of a service</data>
  <data key="d3">file</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ServiceBroker">
  <data key="d5">ServiceBroker</data>
  <data key="d2">Resource representing a service broker</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Client pods">
  <data key="d5">Client pods</data>
  <data key="d2">Pods that use provisioned services</data>
  <data key="d3">container</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Provisioned services">
  <data key="d5">Provisioned services</data>
  <data key="d2">Services provided by external systems</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Broker A and Broker B">
  <data key="d5">Broker A and Broker B</data>
  <data key="d2">External service brokers</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Service Brokers">
  <data key="d5">Service Brokers</data>
  <data key="d2">External service providers that implement the OpenServiceBroker API</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="OpenServiceBroker API">
  <data key="d5">OpenServiceBroker API</data>
  <data key="d2">REST API for interacting with Service Brokers</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="GET /v2/catalog">
  <data key="d5">GET /v2/catalog</data>
  <data key="d2">Operation to retrieve the list of services from a Service Broker</data>
  <data key="d3">operation</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PUT /v2/service_instances/:id">
  <data key="d5">PUT /v2/service_instances/:id</data>
  <data key="d2">Operation to provision a service instance from a Service Broker</data>
  <data key="d3">operation</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PATCH /v2/service_instances/:id">
  <data key="d5">PATCH /v2/service_instances/:id</data>
  <data key="d2">Operation to update a service instance from a Service Broker</data>
  <data key="d3">operation</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PUT /v2/service_instances/:id/service_bindings/:binding_id">
  <data key="d5">PUT /v2/service_instances/:id/service_bindings/:binding_id</data>
  <data key="d2">Operation to bind a service instance from a Service Broker</data>
  <data key="d3">operation</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DELETE /v2/service_instances/:id/service_bindings/:binding_id">
  <data key="d5">DELETE /v2/service_instances/:id/service_bindings/:binding_id</data>
  <data key="d2">Operation to unbind a service instance from a Service Broker</data>
  <data key="d3">operation</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DELETE /v2/service_instances/:id">
  <data key="d5">DELETE /v2/service_instances/:id</data>
  <data key="d2">Operation to deprovision a service instance from a Service Broker</data>
  <data key="d3">operation</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="url">
  <data key="d5">url</data>
  <data key="d2">Field in the ClusterServiceBroker resource manifest specifying the URL of the Service Broker's OpenServiceBroker API</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="service plans">
  <data key="d5">service plans</data>
  <data key="d2">Options for choosing the level of service needed for a particular type of service</data>
  <data key="d3">option</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="bindable">
  <data key="d5">bindable</data>
  <data key="d2">Flag indicating whether a service can be bound to a cluster</data>
  <data key="d3">flag</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="brokerName">
  <data key="d5">brokerName</data>
  <data key="d2">Name of the broker providing a service</data>
  <data key="d3">identifier</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="plans">
  <data key="d5">plans</data>
  <data key="d2">List of available plans for a ClusterServiceClass</data>
  <data key="d3">list</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="planUpdatable">
  <data key="d5">planUpdatable</data>
  <data key="d2">Flag indicating whether a plan can be updated</data>
  <data key="d3">flag</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="osbFree">
  <data key="d5">osbFree</data>
  <data key="d2">Flag indicating whether a plan is free or paid</data>
  <data key="d3">flag</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="postgres-database">
  <data key="d5">postgres-database</data>
  <data key="d2">ClusterServiceClass for PostgreSQL database</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="--data-checksums">
  <data key="d5">--data-checksums</data>
  <data key="d2">Parameter for initializing database</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="get instance">
  <data key="d5">get instance</data>
  <data key="d2">Command for retrieving service instance status</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="clusterServiceClassName">
  <data key="d5">clusterServiceClassName</data>
  <data key="d2">Field in ServiceInstance manifest</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="clusterServicePlanName">
  <data key="d5">clusterServicePlanName</data>
  <data key="d2">Field in ServiceInstance manifest</data>
  <data key="d3">field</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Kubernetes Service Catalog">
  <data key="d5">Kubernetes Service Catalog</data>
  <data key="d2">A Kubernetes feature that allows users to provision and use external services</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="instanceRef">
  <data key="d5">instanceRef</data>
  <data key="d2">A reference to the ServiceInstance being bound</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="secretName">
  <data key="d5">secretName</data>
  <data key="d2">The name of the Secret that will store the credentials for accessing the service</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Database broker">
  <data key="d5">Database broker</data>
  <data key="d2">A component that interacts with the Database service to provision and bind ServiceInstances</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PodPresets">
  <data key="d5">PodPresets</data>
  <data key="d2">A new Kubernetes feature that will allow pods to be pre-configured with credentials for accessing services</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PostgreSQL database">
  <data key="d5">PostgreSQL database</data>
  <data key="d2">An external service being provisioned and used by the Service Catalog</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="get secret">
  <data key="d5">get secret</data>
  <data key="d2">command to retrieve a Secret resource from a Kubernetes cluster</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="postgres-secret">
  <data key="d5">postgres-secret</data>
  <data key="d2">Secret resource holding database credentials</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="username">
  <data key="d5">username</data>
  <data key="d2">field containing username for database connection</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="password">
  <data key="d5">password</data>
  <data key="d2">field containing password for database connection</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="servicebroker">
  <data key="d5">servicebroker</data>
  <data key="d2">component responsible for managing service instances</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="servicebinding">
  <data key="d5">servicebinding</data>
  <data key="d2">resource representing a binding between a service and a pod</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="broker">
  <data key="d5">broker</data>
  <data key="d2">Software component for provisioning services</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Amazon Web Services">
  <data key="d5">Amazon Web Services</data>
  <data key="d2">Cloud computing platform</data>
  <data key="d3">cloud service</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Red Hat OpenShift">
  <data key="d5">Red Hat OpenShift</data>
  <data key="d2">Platform-as-a-Service and container application platform</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="developer experience">
  <data key="d5">developer experience</data>
  <data key="d2">Focus area for Red Hat OpenShift</data>
  <data key="d3">software feature</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="scaling">
  <data key="d5">scaling</data>
  <data key="d2">Process of adjusting application resources</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="maintenance">
  <data key="d5">maintenance</data>
  <data key="d2">Process of keeping applications up-to-date and secure</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="user management">
  <data key="d5">user management</data>
  <data key="d2">Feature for managing user access to Kubernetes resources</data>
  <data key="d3">software feature</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="group management">
  <data key="d5">group management</data>
  <data key="d2">Feature for managing group access to Kubernetes resources</data>
  <data key="d3">software feature</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="projects">
  <data key="d5">projects</data>
  <data key="d2">Additional resource in OpenShift</data>
  <data key="d3">resource</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Templates">
  <data key="d5">Templates</data>
  <data key="d2">A parameterizable list in OpenShift that allows a manifest to be instantiated with parameter values.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="BuildConfigs">
  <data key="d5">BuildConfigs</data>
  <data key="d2">A configuration for building images in OpenShift.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="DeploymentConfigs">
  <data key="d5">DeploymentConfigs</data>
  <data key="d2">A configuration for deploying applications in OpenShift.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="ImageStreams">
  <data key="d5">ImageStreams</data>
  <data key="d2">A stream of images in OpenShift that can be used to deploy applications.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Routes">
  <data key="d5">Routes</data>
  <data key="d2">A way to expose an application running in OpenShift to the outside world.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Projects">
  <data key="d5">Projects</data>
  <data key="d2">A concept in OpenShift that represents a set of resources, similar to a namespace in Kubernetes.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Users">
  <data key="d5">Users</data>
  <data key="d2">An entity in OpenShift that has access to certain projects and can perform actions on resources within those projects.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Groups">
  <data key="d5">Groups</data>
  <data key="d2">A collection of users in OpenShift that have shared permissions.</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Roles-Based Access Control (RBAC)">
  <data key="d5">Roles-Based Access Control (RBAC)</data>
  <data key="d2">A standard access control mechanism in Kubernetes that defines permissions for users and groups.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Templates (parameterizable)">
  <data key="d5">Templates (parameterizable)</data>
  <data key="d2">A way to define resources in OpenShift using placeholders that get replaced with parameter values when instantiated.</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="OpenShift">
  <data key="d5">OpenShift</data>
  <data key="d2">Cloud platform built on top of Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Java EE application">
  <data key="d5">Java EE application</data>
  <data key="d2">Type of application that can be run using OpenShift templates</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Application Server">
  <data key="d5">Application Server</data>
  <data key="d2">Component used to connect to a back-end database</data>
  <data key="d3">component</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Source To Image">
  <data key="d5">Source To Image</data>
  <data key="d2">Build mechanism that detects application type and runs proper build procedure</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Maven">
  <data key="d5">Maven</data>
  <data key="d2">Build tool used for Java projects</data>
  <data key="d3">tool</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Builder pod">
  <data key="d5">Builder pod</data>
  <data key="d2">OpenShift component for building images</data>
  <data key="d3">component</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Build trigger">
  <data key="d5">Build trigger</data>
  <data key="d2">OpenShift component for triggering builds</data>
  <data key="d3">component</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Git repo">
  <data key="d5">Git repo</data>
  <data key="d2">Version control system</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Minishift">
  <data key="d5">Minishift</data>
  <data key="d2">Tool for running OpenShift locally</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="OpenShift Online Starter">
  <data key="d5">OpenShift Online Starter</data>
  <data key="d2">Free, hosted version of OpenShift</data>
  <data key="d3">service</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="git">
  <data key="d5">git</data>
  <data key="d2">Version control system</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Charts">
  <data key="d5">Charts</data>
  <data key="d2">Helm packages containing application configuration and resources</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Config">
  <data key="d5">Config</data>
  <data key="d2">Configuration information merged into a Chart to create a Release</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Linux system">
  <data key="d5">Linux system</data>
  <data key="d2">Operating system</data>
  <data key="d3">hardware/software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="PostgreSQL">
  <data key="d5">PostgreSQL</data>
  <data key="d2">Database management system</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="MySQL">
  <data key="d5">MySQL</data>
  <data key="d2">Database management system</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Helm chart">
  <data key="d5">Helm chart</data>
  <data key="d2">Package for deploying applications to Kubernetes</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="OpenVPN">
  <data key="d5">OpenVPN</data>
  <data key="d2">Virtual private network software</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="helm CLI tool">
  <data key="d5">helm CLI tool</data>
  <data key="d2">Command-line interface for interacting with Helm</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Custom-ResourceDefinition">
  <data key="d5">Custom-ResourceDefinition</data>
  <data key="d2">API server object for registering custom resources</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="Resource manifests">
  <data key="d5">Resource manifests</data>
  <data key="d2">Files that define the resources needed by an application</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="API aggregation">
  <data key="d5">API aggregation</data>
  <data key="d2">Kubernetes feature that allows custom API servers to be added</data>
  <data key="d3">software</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="kubeconfig">
  <data key="d5">kubeconfig</data>
  <data key="d2">Configuration file for kubectl to interact with clusters</data>
  <data key="d3">configuration</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="minikube start">
  <data key="d5">minikube start</data>
  <data key="d2">Command to start a Minikube cluster and reconfigure kubectl</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="gcloud container clusters get-credentials">
  <data key="d5">gcloud container clusters get-credentials</data>
  <data key="d2">Command to configure kubectl to use a GKE cluster</data>
  <data key="d3">command</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="multiple clusters">
  <data key="d5">multiple clusters</data>
  <data key="d2">ability to manage multiple Kubernetes clusters simultaneously</data>
  <data key="d3">application</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="KUBECONFIG environment variable">
  <data key="d5">KUBECONFIG environment variable</data>
  <data key="d2">environment variable used to specify the location of kubeconfig files</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="clusters">
  <data key="d5">clusters</data>
  <data key="d2">list of Kubernetes clusters defined in the kubeconfig file</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="contexts">
  <data key="d5">contexts</data>
  <data key="d2">list of kubectl contexts that define a cluster, user, and namespace</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="current-context">
  <data key="d5">current-context</data>
  <data key="d2">name of the current kubectl context being used</data>
  <data key="d3">process</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="certificate-authority">
  <data key="d5">certificate-authority</data>
  <data key="d2">field in the cluster definition that specifies the certificate authority</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="/home/luksa/.minikube/ca.crt">
  <data key="d5">/home/luksa/.minikube/ca.crt</data>
  <data key="d2">path to the certificate authority file</data>
  <data key="d3">hardware</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="https://192.168.99.100:8443">
  <data key="d5">https://192.168.99.100:8443</data>
  <data key="d2">address of the Kubernetes server</data>
  <data key="d3">network</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="user">
  <data key="d5">user</data>
  <data key="d2">field in the context definition that specifies the user being used</data>
  <data key="d3">database</data>
  <data key="d1">cyan</data>
  <data key="d4">10</data>
</node>
<node id="0">
  <data key="d4">10</data>
</node>
<node id="1">
  <data key="d4">10</data>
</node>
<node id="3">
  <data key="d4">10</data>
</node>
<node id="4">
  <data key="d4">10</data>
</node>
<node id="5">
  <data key="d4">10</data>
</node>
<node id="6">
  <data key="d4">10</data>
</node>
<node id="7">
  <data key="d4">10</data>
</node>
<node id="8">
  <data key="d4">10</data>
</node>
<node id="9">
  <data key="d4">10</data>
</node>
<node id="10">
  <data key="d4">10</data>
</node>
<node id="11">
  <data key="d4">10</data>
</node>
<node id="12">
  <data key="d4">10</data>
</node>
<node id="13">
  <data key="d4">10</data>
</node>
<node id="14">
  <data key="d4">10</data>
</node>
<node id="15">
  <data key="d4">10</data>
</node>
<node id="16">
  <data key="d4">10</data>
</node>
<node id="17">
  <data key="d4">10</data>
</node>
<node id="18">
  <data key="d4">10</data>
</node>
<node id="19">
  <data key="d4">10</data>
</node>
<node id="20">
  <data key="d4">10</data>
</node>
<node id="22">
  <data key="d4">10</data>
</node>
<node id="23">
  <data key="d4">10</data>
</node>
<node id="24">
  <data key="d4">10</data>
</node>
<node id="25">
  <data key="d4">10</data>
</node>
<node id="26">
  <data key="d4">10</data>
</node>
<node id="27">
  <data key="d4">10</data>
</node>
<node id="28">
  <data key="d4">10</data>
</node>
<node id="29">
  <data key="d4">10</data>
</node>
<node id="30">
  <data key="d4">10</data>
</node>
<node id="31">
  <data key="d4">10</data>
</node>
<node id="32">
  <data key="d4">10</data>
</node>
<node id="33">
  <data key="d4">10</data>
</node>
<node id="34">
  <data key="d4">10</data>
</node>
<node id="35">
  <data key="d4">10</data>
</node>
<node id="36">
  <data key="d4">10</data>
</node>
<node id="38">
  <data key="d4">10</data>
</node>
<node id="39">
  <data key="d4">10</data>
</node>
<node id="40">
  <data key="d4">10</data>
</node>
<node id="41">
  <data key="d4">10</data>
</node>
<node id="42">
  <data key="d4">10</data>
</node>
<node id="43">
  <data key="d4">10</data>
</node>
<node id="44">
  <data key="d4">10</data>
</node>
<node id="45">
  <data key="d4">10</data>
</node>
<node id="46">
  <data key="d4">10</data>
</node>
<node id="47">
  <data key="d4">10</data>
</node>
<node id="48">
  <data key="d4">10</data>
</node>
<node id="49">
  <data key="d4">10</data>
</node>
<node id="50">
  <data key="d4">10</data>
</node>
<node id="53">
  <data key="d4">10</data>
</node>
<node id="54">
  <data key="d4">10</data>
</node>
<node id="56">
  <data key="d4">10</data>
</node>
<node id="57">
  <data key="d4">10</data>
</node>
<node id="58">
  <data key="d4">10</data>
</node>
<node id="59">
  <data key="d4">10</data>
</node>
<node id="60">
  <data key="d4">10</data>
</node>
<node id="61">
  <data key="d4">10</data>
</node>
<node id="63">
  <data key="d4">10</data>
</node>
<node id="64">
  <data key="d4">10</data>
</node>
<node id="65">
  <data key="d4">10</data>
</node>
<node id="66">
  <data key="d4">10</data>
</node>
<node id="67">
  <data key="d4">10</data>
</node>
<node id="68">
  <data key="d4">10</data>
</node>
<node id="69">
  <data key="d4">10</data>
</node>
<node id="70">
  <data key="d4">10</data>
</node>
<node id="71">
  <data key="d4">10</data>
</node>
<node id="72">
  <data key="d4">10</data>
</node>
<node id="73">
  <data key="d4">10</data>
</node>
<node id="74">
  <data key="d4">10</data>
</node>
<node id="75">
  <data key="d4">10</data>
</node>
<node id="76">
  <data key="d4">10</data>
</node>
<node id="77">
  <data key="d4">10</data>
</node>
<node id="78">
  <data key="d4">10</data>
</node>
<node id="79">
  <data key="d4">10</data>
</node>
<node id="80">
  <data key="d4">10</data>
</node>
<node id="81">
  <data key="d4">10</data>
</node>
<node id="82">
  <data key="d4">10</data>
</node>
<node id="83">
  <data key="d4">10</data>
</node>
<node id="84">
  <data key="d4">10</data>
</node>
<node id="85">
  <data key="d4">10</data>
</node>
<node id="86">
  <data key="d4">10</data>
</node>
<node id="87">
  <data key="d4">10</data>
</node>
<node id="88">
  <data key="d4">10</data>
</node>
<node id="90">
  <data key="d4">10</data>
</node>
<node id="91">
  <data key="d4">10</data>
</node>
<node id="92">
  <data key="d4">10</data>
</node>
<node id="93">
  <data key="d4">10</data>
</node>
<node id="94">
  <data key="d4">10</data>
</node>
<node id="95">
  <data key="d4">10</data>
</node>
<node id="96">
  <data key="d4">10</data>
</node>
<node id="97">
  <data key="d4">10</data>
</node>
<node id="98">
  <data key="d4">10</data>
</node>
<node id="99">
  <data key="d4">10</data>
</node>
<node id="100">
  <data key="d4">10</data>
</node>
<node id="101">
  <data key="d4">10</data>
</node>
<node id="102">
  <data key="d4">10</data>
</node>
<node id="103">
  <data key="d4">10</data>
</node>
<node id="104">
  <data key="d4">10</data>
</node>
<node id="105">
  <data key="d4">10</data>
</node>
<node id="106">
  <data key="d4">10</data>
</node>
<node id="107">
  <data key="d4">10</data>
</node>
<node id="108">
  <data key="d4">10</data>
</node>
<node id="109">
  <data key="d4">10</data>
</node>
<node id="110">
  <data key="d4">10</data>
</node>
<node id="111">
  <data key="d4">10</data>
</node>
<node id="113">
  <data key="d4">10</data>
</node>
<node id="114">
  <data key="d4">10</data>
</node>
<node id="115">
  <data key="d4">10</data>
</node>
<node id="116">
  <data key="d4">10</data>
</node>
<node id="117">
  <data key="d4">10</data>
</node>
<node id="118">
  <data key="d4">10</data>
</node>
<node id="119">
  <data key="d4">10</data>
</node>
<node id="120">
  <data key="d4">10</data>
</node>
<node id="122">
  <data key="d4">10</data>
</node>
<node id="123">
  <data key="d4">10</data>
</node>
<node id="125">
  <data key="d4">10</data>
</node>
<node id="126">
  <data key="d4">10</data>
</node>
<node id="127">
  <data key="d4">10</data>
</node>
<node id="128">
  <data key="d4">10</data>
</node>
<node id="129">
  <data key="d4">10</data>
</node>
<node id="130">
  <data key="d4">10</data>
</node>
<node id="131">
  <data key="d4">10</data>
</node>
<node id="132">
  <data key="d4">10</data>
</node>
<node id="134">
  <data key="d4">10</data>
</node>
<node id="135">
  <data key="d4">10</data>
</node>
<node id="136">
  <data key="d4">10</data>
</node>
<node id="137">
  <data key="d4">10</data>
</node>
<node id="138">
  <data key="d4">10</data>
</node>
<node id="139">
  <data key="d4">10</data>
</node>
<node id="140">
  <data key="d4">10</data>
</node>
<node id="141">
  <data key="d4">10</data>
</node>
<node id="142">
  <data key="d4">10</data>
</node>
<node id="143">
  <data key="d4">10</data>
</node>
<node id="144">
  <data key="d4">10</data>
</node>
<node id="145">
  <data key="d4">10</data>
</node>
<node id="146">
  <data key="d4">10</data>
</node>
<node id="147">
  <data key="d4">10</data>
</node>
<node id="148">
  <data key="d4">10</data>
</node>
<node id="149">
  <data key="d4">10</data>
</node>
<node id="150">
  <data key="d4">10</data>
</node>
<node id="151">
  <data key="d4">10</data>
</node>
<node id="152">
  <data key="d4">10</data>
</node>
<node id="153">
  <data key="d4">10</data>
</node>
<node id="154">
  <data key="d4">10</data>
</node>
<node id="155">
  <data key="d4">10</data>
</node>
<node id="156">
  <data key="d4">10</data>
</node>
<node id="157">
  <data key="d4">10</data>
</node>
<node id="158">
  <data key="d4">10</data>
</node>
<node id="159">
  <data key="d4">10</data>
</node>
<node id="160">
  <data key="d4">10</data>
</node>
<node id="161">
  <data key="d4">10</data>
</node>
<node id="162">
  <data key="d4">10</data>
</node>
<node id="163">
  <data key="d4">10</data>
</node>
<node id="164">
  <data key="d4">10</data>
</node>
<node id="165">
  <data key="d4">10</data>
</node>
<node id="166">
  <data key="d4">10</data>
</node>
<node id="167">
  <data key="d4">10</data>
</node>
<node id="168">
  <data key="d4">10</data>
</node>
<node id="169">
  <data key="d4">10</data>
</node>
<node id="170">
  <data key="d4">10</data>
</node>
<node id="171">
  <data key="d4">10</data>
</node>
<node id="172">
  <data key="d4">10</data>
</node>
<node id="174">
  <data key="d4">10</data>
</node>
<node id="175">
  <data key="d4">10</data>
</node>
<node id="176">
  <data key="d4">10</data>
</node>
<node id="177">
  <data key="d4">10</data>
</node>
<node id="178">
  <data key="d4">10</data>
</node>
<node id="179">
  <data key="d4">10</data>
</node>
<node id="180">
  <data key="d4">10</data>
</node>
<node id="181">
  <data key="d4">10</data>
</node>
<node id="182">
  <data key="d4">10</data>
</node>
<node id="183">
  <data key="d4">10</data>
</node>
<node id="184">
  <data key="d4">10</data>
</node>
<node id="185">
  <data key="d4">10</data>
</node>
<node id="186">
  <data key="d4">10</data>
</node>
<node id="187">
  <data key="d4">10</data>
</node>
<node id="188">
  <data key="d4">10</data>
</node>
<node id="189">
  <data key="d4">10</data>
</node>
<node id="190">
  <data key="d4">10</data>
</node>
<node id="191">
  <data key="d4">10</data>
</node>
<node id="192">
  <data key="d4">10</data>
</node>
<node id="193">
  <data key="d4">10</data>
</node>
<node id="195">
  <data key="d4">10</data>
</node>
<node id="196">
  <data key="d4">10</data>
</node>
<node id="197">
  <data key="d4">10</data>
</node>
<node id="199">
  <data key="d4">10</data>
</node>
<node id="200">
  <data key="d4">10</data>
</node>
<node id="203">
  <data key="d4">10</data>
</node>
<node id="204">
  <data key="d4">10</data>
</node>
<node id="205">
  <data key="d4">10</data>
</node>
<node id="206">
  <data key="d4">10</data>
</node>
<node id="207">
  <data key="d4">10</data>
</node>
<node id="208">
  <data key="d4">10</data>
</node>
<node id="209">
  <data key="d4">10</data>
</node>
<node id="210">
  <data key="d4">10</data>
</node>
<node id="211">
  <data key="d4">10</data>
</node>
<node id="212">
  <data key="d4">10</data>
</node>
<node id="213">
  <data key="d4">10</data>
</node>
<node id="214">
  <data key="d4">10</data>
</node>
<node id="215">
  <data key="d4">10</data>
</node>
<node id="217">
  <data key="d4">10</data>
</node>
<node id="218">
  <data key="d4">10</data>
</node>
<node id="219">
  <data key="d4">10</data>
</node>
<node id="221">
  <data key="d4">10</data>
</node>
<node id="223">
  <data key="d4">10</data>
</node>
<node id="224">
  <data key="d4">10</data>
</node>
<node id="225">
  <data key="d4">10</data>
</node>
<node id="227">
  <data key="d4">10</data>
</node>
<node id="228">
  <data key="d4">10</data>
</node>
<node id="229">
  <data key="d4">10</data>
</node>
<node id="230">
  <data key="d4">10</data>
</node>
<node id="231">
  <data key="d4">10</data>
</node>
<node id="232">
  <data key="d4">10</data>
</node>
<node id="233">
  <data key="d4">10</data>
</node>
<node id="234">
  <data key="d4">10</data>
</node>
<node id="235">
  <data key="d4">10</data>
</node>
<node id="236">
  <data key="d4">10</data>
</node>
<node id="237">
  <data key="d4">10</data>
</node>
<node id="238">
  <data key="d4">10</data>
</node>
<node id="239">
  <data key="d4">10</data>
</node>
<node id="240">
  <data key="d4">10</data>
</node>
<node id="241">
  <data key="d4">10</data>
</node>
<node id="242">
  <data key="d4">10</data>
</node>
<node id="243">
  <data key="d4">10</data>
</node>
<node id="244">
  <data key="d4">10</data>
</node>
<node id="245">
  <data key="d4">10</data>
</node>
<node id="246">
  <data key="d4">10</data>
</node>
<node id="247">
  <data key="d4">10</data>
</node>
<node id="248">
  <data key="d4">10</data>
</node>
<node id="249">
  <data key="d4">10</data>
</node>
<node id="250">
  <data key="d4">10</data>
</node>
<node id="251">
  <data key="d4">10</data>
</node>
<node id="252">
  <data key="d4">10</data>
</node>
<node id="253">
  <data key="d4">10</data>
</node>
<node id="254">
  <data key="d4">10</data>
</node>
<node id="256">
  <data key="d4">10</data>
</node>
<node id="257">
  <data key="d4">10</data>
</node>
<node id="258">
  <data key="d4">10</data>
</node>
<node id="259">
  <data key="d4">10</data>
</node>
<node id="260">
  <data key="d4">10</data>
</node>
<node id="261">
  <data key="d4">10</data>
</node>
<node id="262">
  <data key="d4">10</data>
</node>
<node id="263">
  <data key="d4">10</data>
</node>
<node id="264">
  <data key="d4">10</data>
</node>
<node id="265">
  <data key="d4">10</data>
</node>
<node id="266">
  <data key="d4">10</data>
</node>
<node id="268">
  <data key="d4">10</data>
</node>
<node id="269">
  <data key="d4">10</data>
</node>
<node id="270">
  <data key="d4">10</data>
</node>
<node id="271">
  <data key="d4">10</data>
</node>
<node id="272">
  <data key="d4">10</data>
</node>
<node id="274">
  <data key="d4">10</data>
</node>
<node id="275">
  <data key="d4">10</data>
</node>
<node id="276">
  <data key="d4">10</data>
</node>
<node id="277">
  <data key="d4">10</data>
</node>
<node id="278">
  <data key="d4">10</data>
</node>
<node id="279">
  <data key="d4">10</data>
</node>
<node id="280">
  <data key="d4">10</data>
</node>
<node id="281">
  <data key="d4">10</data>
</node>
<node id="282">
  <data key="d4">10</data>
</node>
<node id="283">
  <data key="d4">10</data>
</node>
<node id="284">
  <data key="d4">10</data>
</node>
<node id="285">
  <data key="d4">10</data>
</node>
<node id="286">
  <data key="d4">10</data>
</node>
<node id="287">
  <data key="d4">10</data>
</node>
<node id="288">
  <data key="d4">10</data>
</node>
<node id="289">
  <data key="d4">10</data>
</node>
<node id="290">
  <data key="d4">10</data>
</node>
<node id="291">
  <data key="d4">10</data>
</node>
<node id="292">
  <data key="d4">10</data>
</node>
<node id="293">
  <data key="d4">10</data>
</node>
<node id="294">
  <data key="d4">10</data>
</node>
<node id="295">
  <data key="d4">10</data>
</node>
<node id="296">
  <data key="d4">10</data>
</node>
<node id="297">
  <data key="d4">10</data>
</node>
<node id="299">
  <data key="d4">10</data>
</node>
<node id="301">
  <data key="d4">10</data>
</node>
<node id="302">
  <data key="d4">10</data>
</node>
<node id="303">
  <data key="d4">10</data>
</node>
<node id="304">
  <data key="d4">10</data>
</node>
<node id="305">
  <data key="d4">10</data>
</node>
<node id="306">
  <data key="d4">10</data>
</node>
<node id="307">
  <data key="d4">10</data>
</node>
<node id="308">
  <data key="d4">10</data>
</node>
<node id="309">
  <data key="d4">10</data>
</node>
<node id="310">
  <data key="d4">10</data>
</node>
<node id="311">
  <data key="d4">10</data>
</node>
<node id="313">
  <data key="d4">10</data>
</node>
<node id="314">
  <data key="d4">10</data>
</node>
<node id="315">
  <data key="d4">10</data>
</node>
<node id="316">
  <data key="d4">10</data>
</node>
<node id="318">
  <data key="d4">10</data>
</node>
<node id="319">
  <data key="d4">10</data>
</node>
<node id="320">
  <data key="d4">10</data>
</node>
<node id="322">
  <data key="d4">10</data>
</node>
<node id="323">
  <data key="d4">10</data>
</node>
<node id="324">
  <data key="d4">10</data>
</node>
<node id="325">
  <data key="d4">10</data>
</node>
<node id="326">
  <data key="d4">10</data>
</node>
<node id="327">
  <data key="d4">10</data>
</node>
<node id="328">
  <data key="d4">10</data>
</node>
<node id="329">
  <data key="d4">10</data>
</node>
<node id="330">
  <data key="d4">10</data>
</node>
<node id="331">
  <data key="d4">10</data>
</node>
<node id="332">
  <data key="d4">10</data>
</node>
<node id="333">
  <data key="d4">10</data>
</node>
<node id="334">
  <data key="d4">10</data>
</node>
<node id="335">
  <data key="d4">10</data>
</node>
<node id="336">
  <data key="d4">10</data>
</node>
<node id="337">
  <data key="d4">10</data>
</node>
<node id="338">
  <data key="d4">10</data>
</node>
<node id="339">
  <data key="d4">10</data>
</node>
<node id="340">
  <data key="d4">10</data>
</node>
<node id="341">
  <data key="d4">10</data>
</node>
<node id="342">
  <data key="d4">10</data>
</node>
<node id="343">
  <data key="d4">10</data>
</node>
<node id="344">
  <data key="d4">10</data>
</node>
<node id="345">
  <data key="d4">10</data>
</node>
<node id="346">
  <data key="d4">10</data>
</node>
<node id="348">
  <data key="d4">10</data>
</node>
<node id="349">
  <data key="d4">10</data>
</node>
<node id="350">
  <data key="d4">10</data>
</node>
<node id="351">
  <data key="d4">10</data>
</node>
<node id="352">
  <data key="d4">10</data>
</node>
<node id="353">
  <data key="d4">10</data>
</node>
<node id="354">
  <data key="d4">10</data>
</node>
<node id="355">
  <data key="d4">10</data>
</node>
<node id="356">
  <data key="d4">10</data>
</node>
<node id="357">
  <data key="d4">10</data>
</node>
<node id="358">
  <data key="d4">10</data>
</node>
<node id="359">
  <data key="d4">10</data>
</node>
<node id="360">
  <data key="d4">10</data>
</node>
<node id="362">
  <data key="d4">10</data>
</node>
<node id="364">
  <data key="d4">10</data>
</node>
<node id="365">
  <data key="d4">10</data>
</node>
<node id="367">
  <data key="d4">10</data>
</node>
<node id="368">
  <data key="d4">10</data>
</node>
<node id="369">
  <data key="d4">10</data>
</node>
<node id="370">
  <data key="d4">10</data>
</node>
<node id="371">
  <data key="d4">10</data>
</node>
<node id="372">
  <data key="d4">10</data>
</node>
<node id="374">
  <data key="d4">10</data>
</node>
<node id="375">
  <data key="d4">10</data>
</node>
<node id="376">
  <data key="d4">10</data>
</node>
<node id="377">
  <data key="d4">10</data>
</node>
<node id="378">
  <data key="d4">10</data>
</node>
<node id="380">
  <data key="d4">10</data>
</node>
<node id="381">
  <data key="d4">10</data>
</node>
<node id="382">
  <data key="d4">10</data>
</node>
<node id="383">
  <data key="d4">10</data>
</node>
<node id="385">
  <data key="d4">10</data>
</node>
<node id="386">
  <data key="d4">10</data>
</node>
<node id="387">
  <data key="d4">10</data>
</node>
<node id="389">
  <data key="d4">10</data>
</node>
<node id="390">
  <data key="d4">10</data>
</node>
<node id="391">
  <data key="d4">10</data>
</node>
<node id="392">
  <data key="d4">10</data>
</node>
<node id="393">
  <data key="d4">10</data>
</node>
<node id="394">
  <data key="d4">10</data>
</node>
<node id="395">
  <data key="d4">10</data>
</node>
<node id="396">
  <data key="d4">10</data>
</node>
<node id="397">
  <data key="d4">10</data>
</node>
<node id="398">
  <data key="d4">10</data>
</node>
<node id="399">
  <data key="d4">10</data>
</node>
<node id="400">
  <data key="d4">10</data>
</node>
<node id="401">
  <data key="d4">10</data>
</node>
<node id="402">
  <data key="d4">10</data>
</node>
<node id="403">
  <data key="d4">10</data>
</node>
<node id="405">
  <data key="d4">10</data>
</node>
<node id="406">
  <data key="d4">10</data>
</node>
<node id="407">
  <data key="d4">10</data>
</node>
<node id="408">
  <data key="d4">10</data>
</node>
<node id="409">
  <data key="d4">10</data>
</node>
<node id="410">
  <data key="d4">10</data>
</node>
<node id="411">
  <data key="d4">10</data>
</node>
<node id="413">
  <data key="d4">10</data>
</node>
<node id="414">
  <data key="d4">10</data>
</node>
<node id="416">
  <data key="d4">10</data>
</node>
<node id="417">
  <data key="d4">10</data>
</node>
<node id="418">
  <data key="d4">10</data>
</node>
<node id="419">
  <data key="d4">10</data>
</node>
<node id="420">
  <data key="d4">10</data>
</node>
<node id="421">
  <data key="d4">10</data>
</node>
<node id="423">
  <data key="d4">10</data>
</node>
<node id="424">
  <data key="d4">10</data>
</node>
<node id="427">
  <data key="d4">10</data>
</node>
<node id="428">
  <data key="d4">10</data>
</node>
<node id="429">
  <data key="d4">10</data>
</node>
<node id="430">
  <data key="d4">10</data>
</node>
<node id="431">
  <data key="d4">10</data>
</node>
<node id="432">
  <data key="d4">10</data>
</node>
<node id="433">
  <data key="d4">10</data>
</node>
<node id="434">
  <data key="d4">10</data>
</node>
<node id="435">
  <data key="d4">10</data>
</node>
<node id="436">
  <data key="d4">10</data>
</node>
<node id="437">
  <data key="d4">10</data>
</node>
<node id="438">
  <data key="d4">10</data>
</node>
<node id="440">
  <data key="d4">10</data>
</node>
<node id="441">
  <data key="d4">10</data>
</node>
<node id="442">
  <data key="d4">10</data>
</node>
<node id="443">
  <data key="d4">10</data>
</node>
<node id="444">
  <data key="d4">10</data>
</node>
<node id="445">
  <data key="d4">10</data>
</node>
<node id="446">
  <data key="d4">10</data>
</node>
<node id="447">
  <data key="d4">10</data>
</node>
<node id="448">
  <data key="d4">10</data>
</node>
<node id="449">
  <data key="d4">10</data>
</node>
<node id="450">
  <data key="d4">10</data>
</node>
<node id="451">
  <data key="d4">10</data>
</node>
<node id="452">
  <data key="d4">10</data>
</node>
<node id="453">
  <data key="d4">10</data>
</node>
<node id="454">
  <data key="d4">10</data>
</node>
<node id="455">
  <data key="d4">10</data>
</node>
<node id="457">
  <data key="d4">10</data>
</node>
<node id="458">
  <data key="d4">10</data>
</node>
<node id="459">
  <data key="d4">10</data>
</node>
<node id="460">
  <data key="d4">10</data>
</node>
<node id="461">
  <data key="d4">10</data>
</node>
<node id="462">
  <data key="d4">10</data>
</node>
<node id="463">
  <data key="d4">10</data>
</node>
<node id="464">
  <data key="d4">10</data>
</node>
<node id="465">
  <data key="d4">10</data>
</node>
<node id="466">
  <data key="d4">10</data>
</node>
<node id="467">
  <data key="d4">10</data>
</node>
<node id="468">
  <data key="d4">10</data>
</node>
<node id="469">
  <data key="d4">10</data>
</node>
<node id="470">
  <data key="d4">10</data>
</node>
<node id="471">
  <data key="d4">10</data>
</node>
<node id="472">
  <data key="d4">10</data>
</node>
<node id="473">
  <data key="d4">10</data>
</node>
<node id="474">
  <data key="d4">10</data>
</node>
<node id="475">
  <data key="d4">10</data>
</node>
<node id="476">
  <data key="d4">10</data>
</node>
<node id="477">
  <data key="d4">10</data>
</node>
<node id="478">
  <data key="d4">10</data>
</node>
<edge source="89" target="text_0">
  <data key="d6">1</data>
</edge>
<edge source="1" target="89">
  <data key="d6">1</data>
</edge>
<edge source="1" target="90">
  <data key="d6">1</data>
</edge>
<edge source="1" target="91">
  <data key="d6">1</data>
</edge>
<edge source="1" target="92">
  <data key="d6">1</data>
</edge>
<edge source="1" target="93">
  <data key="d6">1</data>
</edge>
<edge source="1" target="94">
  <data key="d6">1</data>
</edge>
<edge source="1" target="95">
  <data key="d6">1</data>
</edge>
<edge source="1" target="96">
  <data key="d6">1</data>
</edge>
<edge source="1" target="97">
  <data key="d6">1</data>
</edge>
<edge source="1" target="98">
  <data key="d6">1</data>
</edge>
<edge source="1" target="99">
  <data key="d6">1</data>
</edge>
<edge source="1" target="100">
  <data key="d6">1</data>
</edge>
<edge source="1" target="101">
  <data key="d6">1</data>
</edge>
<edge source="1" target="102">
  <data key="d6">1</data>
</edge>
<edge source="1" target="103">
  <data key="d6">1</data>
</edge>
<edge source="1" target="104">
  <data key="d6">1</data>
</edge>
<edge source="1" target="105">
  <data key="d6">1</data>
</edge>
<edge source="1" target="106">
  <data key="d6">1</data>
</edge>
<edge source="1" target="107">
  <data key="d6">1</data>
</edge>
<edge source="1" target="108">
  <data key="d6">1</data>
</edge>
<edge source="1" target="109">
  <data key="d6">1</data>
</edge>
<edge source="1" target="110">
  <data key="d6">1</data>
</edge>
<edge source="1" target="111">
  <data key="d6">1</data>
</edge>
<edge source="1" target="112">
  <data key="d6">1</data>
</edge>
<edge source="1" target="113">
  <data key="d6">1</data>
</edge>
<edge source="1" target="114">
  <data key="d6">1</data>
</edge>
<edge source="1" target="115">
  <data key="d6">1</data>
</edge>
<edge source="1" target="116">
  <data key="d6">1</data>
</edge>
<edge source="1" target="117">
  <data key="d6">1</data>
</edge>
<edge source="1" target="118">
  <data key="d6">1</data>
</edge>
<edge source="1" target="119">
  <data key="d6">1</data>
</edge>
<edge source="1" target="120">
  <data key="d6">1</data>
</edge>
<edge source="1" target="121">
  <data key="d6">1</data>
</edge>
<edge source="1" target="122">
  <data key="d6">1</data>
</edge>
<edge source="1" target="123">
  <data key="d6">1</data>
</edge>
<edge source="1" target="124">
  <data key="d6">1</data>
</edge>
<edge source="1" target="125">
  <data key="d6">1</data>
</edge>
<edge source="1" target="126">
  <data key="d6">1</data>
</edge>
<edge source="1" target="127">
  <data key="d6">1</data>
</edge>
<edge source="1" target="128">
  <data key="d6">1</data>
</edge>
<edge source="1" target="129">
  <data key="d6">1</data>
</edge>
<edge source="1" target="130">
  <data key="d6">1</data>
</edge>
<edge source="1" target="131">
  <data key="d6">1</data>
</edge>
<edge source="1" target="132">
  <data key="d6">1</data>
</edge>
<edge source="1" target="133">
  <data key="d6">1</data>
</edge>
<edge source="1" target="134">
  <data key="d6">1</data>
</edge>
<edge source="1" target="135">
  <data key="d6">1</data>
</edge>
<edge source="1" target="136">
  <data key="d6">1</data>
</edge>
<edge source="1" target="137">
  <data key="d6">1</data>
</edge>
<edge source="1" target="138">
  <data key="d6">1</data>
</edge>
<edge source="1" target="139">
  <data key="d6">1</data>
</edge>
<edge source="1" target="140">
  <data key="d6">1</data>
</edge>
<edge source="1" target="141">
  <data key="d6">1</data>
</edge>
<edge source="1" target="142">
  <data key="d6">1</data>
</edge>
<edge source="1" target="143">
  <data key="d6">1</data>
</edge>
<edge source="1" target="144">
  <data key="d6">1</data>
</edge>
<edge source="1" target="145">
  <data key="d6">1</data>
</edge>
<edge source="1" target="146">
  <data key="d6">1</data>
</edge>
<edge source="1" target="147">
  <data key="d6">1</data>
</edge>
<edge source="1" target="148">
  <data key="d6">1</data>
</edge>
<edge source="1" target="149">
  <data key="d6">1</data>
</edge>
<edge source="1" target="150">
  <data key="d6">1</data>
</edge>
<edge source="1" target="151">
  <data key="d6">1</data>
</edge>
<edge source="1" target="152">
  <data key="d6">1</data>
</edge>
<edge source="1" target="153">
  <data key="d6">1</data>
</edge>
<edge source="1" target="154">
  <data key="d6">1</data>
</edge>
<edge source="1" target="155">
  <data key="d6">1</data>
</edge>
<edge source="1" target="156">
  <data key="d6">1</data>
</edge>
<edge source="1" target="157">
  <data key="d6">1</data>
</edge>
<edge source="1" target="158">
  <data key="d6">1</data>
</edge>
<edge source="1" target="159">
  <data key="d6">1</data>
</edge>
<edge source="1" target="160">
  <data key="d6">1</data>
</edge>
<edge source="1" target="161">
  <data key="d6">1</data>
</edge>
<edge source="1" target="162">
  <data key="d6">1</data>
</edge>
<edge source="1" target="163">
  <data key="d6">1</data>
</edge>
<edge source="1" target="164">
  <data key="d6">1</data>
</edge>
<edge source="1" target="165">
  <data key="d6">1</data>
</edge>
<edge source="1" target="166">
  <data key="d6">1</data>
</edge>
<edge source="1" target="167">
  <data key="d6">1</data>
</edge>
<edge source="1" target="168">
  <data key="d6">1</data>
</edge>
<edge source="1" target="169">
  <data key="d6">1</data>
</edge>
<edge source="1" target="170">
  <data key="d6">1</data>
</edge>
<edge source="1" target="171">
  <data key="d6">1</data>
</edge>
<edge source="1" target="172">
  <data key="d6">1</data>
</edge>
<edge source="1" target="173">
  <data key="d6">1</data>
</edge>
<edge source="1" target="174">
  <data key="d6">1</data>
</edge>
<edge source="1" target="175">
  <data key="d6">1</data>
</edge>
<edge source="1" target="176">
  <data key="d6">1</data>
</edge>
<edge source="1" target="177">
  <data key="d6">1</data>
</edge>
<edge source="1" target="178">
  <data key="d6">1</data>
</edge>
<edge source="1" target="179">
  <data key="d6">1</data>
</edge>
<edge source="1" target="180">
  <data key="d6">1</data>
</edge>
<edge source="1" target="181">
  <data key="d6">1</data>
</edge>
<edge source="1" target="182">
  <data key="d6">1</data>
</edge>
<edge source="1" target="183">
  <data key="d6">1</data>
</edge>
<edge source="1" target="184">
  <data key="d6">1</data>
</edge>
<edge source="1" target="185">
  <data key="d6">1</data>
</edge>
<edge source="1" target="186">
  <data key="d6">1</data>
</edge>
<edge source="1" target="187">
  <data key="d6">1</data>
</edge>
<edge source="1" target="188">
  <data key="d6">1</data>
</edge>
<edge source="1" target="189">
  <data key="d6">1</data>
</edge>
<edge source="1" target="190">
  <data key="d6">1</data>
</edge>
<edge source="1" target="191">
  <data key="d6">1</data>
</edge>
<edge source="1" target="192">
  <data key="d6">1</data>
</edge>
<edge source="1" target="193">
  <data key="d6">1</data>
</edge>
<edge source="1" target="194">
  <data key="d6">1</data>
</edge>
<edge source="1" target="195">
  <data key="d6">1</data>
</edge>
<edge source="1" target="196">
  <data key="d6">1</data>
</edge>
<edge source="1" target="197">
  <data key="d6">1</data>
</edge>
<edge source="1" target="198">
  <data key="d6">1</data>
</edge>
<edge source="1" target="199">
  <data key="d6">1</data>
</edge>
<edge source="1" target="200">
  <data key="d6">1</data>
</edge>
<edge source="1" target="201">
  <data key="d6">1</data>
</edge>
<edge source="1" target="202">
  <data key="d6">1</data>
</edge>
<edge source="1" target="203">
  <data key="d6">1</data>
</edge>
<edge source="1" target="204">
  <data key="d6">1</data>
</edge>
<edge source="1" target="205">
  <data key="d6">1</data>
</edge>
<edge source="1" target="206">
  <data key="d6">1</data>
</edge>
<edge source="1" target="207">
  <data key="d6">1</data>
</edge>
<edge source="1" target="208">
  <data key="d6">1</data>
</edge>
<edge source="1" target="209">
  <data key="d6">1</data>
</edge>
<edge source="1" target="210">
  <data key="d6">1</data>
</edge>
<edge source="1" target="211">
  <data key="d6">1</data>
</edge>
<edge source="1" target="212">
  <data key="d6">1</data>
</edge>
<edge source="1" target="213">
  <data key="d6">1</data>
</edge>
<edge source="1" target="214">
  <data key="d6">1</data>
</edge>
<edge source="1" target="215">
  <data key="d6">1</data>
</edge>
<edge source="1" target="216">
  <data key="d6">1</data>
</edge>
<edge source="1" target="217">
  <data key="d6">1</data>
</edge>
<edge source="1" target="218">
  <data key="d6">1</data>
</edge>
<edge source="1" target="219">
  <data key="d6">1</data>
</edge>
<edge source="1" target="220">
  <data key="d6">1</data>
</edge>
<edge source="1" target="221">
  <data key="d6">1</data>
</edge>
<edge source="1" target="222">
  <data key="d6">1</data>
</edge>
<edge source="1" target="223">
  <data key="d6">1</data>
</edge>
<edge source="1" target="224">
  <data key="d6">1</data>
</edge>
<edge source="1" target="225">
  <data key="d6">1</data>
</edge>
<edge source="1" target="226">
  <data key="d6">1</data>
</edge>
<edge source="1" target="227">
  <data key="d6">1</data>
</edge>
<edge source="1" target="228">
  <data key="d6">1</data>
</edge>
<edge source="1" target="229">
  <data key="d6">1</data>
</edge>
<edge source="1" target="230">
  <data key="d6">1</data>
</edge>
<edge source="1" target="231">
  <data key="d6">1</data>
</edge>
<edge source="1" target="232">
  <data key="d6">1</data>
</edge>
<edge source="1" target="233">
  <data key="d6">1</data>
</edge>
<edge source="1" target="234">
  <data key="d6">1</data>
</edge>
<edge source="1" target="235">
  <data key="d6">1</data>
</edge>
<edge source="1" target="236">
  <data key="d6">1</data>
</edge>
<edge source="1" target="237">
  <data key="d6">1</data>
</edge>
<edge source="1" target="238">
  <data key="d6">1</data>
</edge>
<edge source="1" target="239">
  <data key="d6">1</data>
</edge>
<edge source="1" target="240">
  <data key="d6">1</data>
</edge>
<edge source="1" target="241">
  <data key="d6">1</data>
</edge>
<edge source="1" target="242">
  <data key="d6">1</data>
</edge>
<edge source="1" target="243">
  <data key="d6">1</data>
</edge>
<edge source="1" target="244">
  <data key="d6">1</data>
</edge>
<edge source="1" target="245">
  <data key="d6">1</data>
</edge>
<edge source="1" target="246">
  <data key="d6">1</data>
</edge>
<edge source="1" target="247">
  <data key="d6">1</data>
</edge>
<edge source="1" target="248">
  <data key="d6">1</data>
</edge>
<edge source="1" target="249">
  <data key="d6">1</data>
</edge>
<edge source="1" target="250">
  <data key="d6">1</data>
</edge>
<edge source="1" target="251">
  <data key="d6">1</data>
</edge>
<edge source="1" target="252">
  <data key="d6">1</data>
</edge>
<edge source="1" target="253">
  <data key="d6">1</data>
</edge>
<edge source="1" target="254">
  <data key="d6">1</data>
</edge>
<edge source="1" target="255">
  <data key="d6">1</data>
</edge>
<edge source="1" target="256">
  <data key="d6">1</data>
</edge>
<edge source="1" target="257">
  <data key="d6">1</data>
</edge>
<edge source="1" target="258">
  <data key="d6">1</data>
</edge>
<edge source="1" target="259">
  <data key="d6">1</data>
</edge>
<edge source="1" target="260">
  <data key="d6">1</data>
</edge>
<edge source="1" target="261">
  <data key="d6">1</data>
</edge>
<edge source="1" target="262">
  <data key="d6">1</data>
</edge>
<edge source="1" target="263">
  <data key="d6">1</data>
</edge>
<edge source="1" target="264">
  <data key="d6">1</data>
</edge>
<edge source="1" target="265">
  <data key="d6">1</data>
</edge>
<edge source="1" target="266">
  <data key="d6">1</data>
</edge>
<edge source="1" target="267">
  <data key="d6">1</data>
</edge>
<edge source="1" target="268">
  <data key="d6">1</data>
</edge>
<edge source="1" target="269">
  <data key="d6">1</data>
</edge>
<edge source="1" target="270">
  <data key="d6">1</data>
</edge>
<edge source="1" target="271">
  <data key="d6">1</data>
</edge>
<edge source="1" target="272">
  <data key="d6">1</data>
</edge>
<edge source="1" target="273">
  <data key="d6">1</data>
</edge>
<edge source="1" target="274">
  <data key="d6">1</data>
</edge>
<edge source="1" target="275">
  <data key="d6">1</data>
</edge>
<edge source="1" target="276">
  <data key="d6">1</data>
</edge>
<edge source="1" target="277">
  <data key="d6">1</data>
</edge>
<edge source="1" target="278">
  <data key="d6">1</data>
</edge>
<edge source="1" target="279">
  <data key="d6">1</data>
</edge>
<edge source="1" target="280">
  <data key="d6">1</data>
</edge>
<edge source="1" target="281">
  <data key="d6">1</data>
</edge>
<edge source="1" target="282">
  <data key="d6">1</data>
</edge>
<edge source="1" target="283">
  <data key="d6">1</data>
</edge>
<edge source="1" target="284">
  <data key="d6">1</data>
</edge>
<edge source="1" target="285">
  <data key="d6">1</data>
</edge>
<edge source="1" target="286">
  <data key="d6">1</data>
</edge>
<edge source="1" target="287">
  <data key="d6">1</data>
</edge>
<edge source="1" target="288">
  <data key="d6">1</data>
</edge>
<edge source="1" target="289">
  <data key="d6">1</data>
</edge>
<edge source="1" target="290">
  <data key="d6">1</data>
</edge>
<edge source="1" target="291">
  <data key="d6">1</data>
</edge>
<edge source="1" target="292">
  <data key="d6">1</data>
</edge>
<edge source="1" target="293">
  <data key="d6">1</data>
</edge>
<edge source="1" target="294">
  <data key="d6">1</data>
</edge>
<edge source="1" target="295">
  <data key="d6">1</data>
</edge>
<edge source="1" target="296">
  <data key="d6">1</data>
</edge>
<edge source="1" target="297">
  <data key="d6">1</data>
</edge>
<edge source="1" target="298">
  <data key="d6">1</data>
</edge>
<edge source="1" target="299">
  <data key="d6">1</data>
</edge>
<edge source="1" target="300">
  <data key="d6">1</data>
</edge>
<edge source="1" target="301">
  <data key="d6">1</data>
</edge>
<edge source="1" target="302">
  <data key="d6">1</data>
</edge>
<edge source="1" target="303">
  <data key="d6">1</data>
</edge>
<edge source="1" target="304">
  <data key="d6">1</data>
</edge>
<edge source="1" target="305">
  <data key="d6">1</data>
</edge>
<edge source="1" target="306">
  <data key="d6">1</data>
</edge>
<edge source="1" target="307">
  <data key="d6">1</data>
</edge>
<edge source="1" target="308">
  <data key="d6">1</data>
</edge>
<edge source="1" target="309">
  <data key="d6">1</data>
</edge>
<edge source="1" target="310">
  <data key="d6">1</data>
</edge>
<edge source="1" target="311">
  <data key="d6">1</data>
</edge>
<edge source="1" target="312">
  <data key="d6">1</data>
</edge>
<edge source="1" target="313">
  <data key="d6">1</data>
</edge>
<edge source="1" target="314">
  <data key="d6">1</data>
</edge>
<edge source="1" target="315">
  <data key="d6">1</data>
</edge>
<edge source="1" target="316">
  <data key="d6">1</data>
</edge>
<edge source="1" target="317">
  <data key="d6">1</data>
</edge>
<edge source="1" target="318">
  <data key="d6">1</data>
</edge>
<edge source="1" target="319">
  <data key="d6">1</data>
</edge>
<edge source="1" target="320">
  <data key="d6">1</data>
</edge>
<edge source="1" target="321">
  <data key="d6">1</data>
</edge>
<edge source="1" target="322">
  <data key="d6">1</data>
</edge>
<edge source="1" target="323">
  <data key="d6">1</data>
</edge>
<edge source="1" target="324">
  <data key="d6">1</data>
</edge>
<edge source="1" target="325">
  <data key="d6">1</data>
</edge>
<edge source="1" target="326">
  <data key="d6">1</data>
</edge>
<edge source="1" target="327">
  <data key="d6">1</data>
</edge>
<edge source="1" target="328">
  <data key="d6">1</data>
</edge>
<edge source="1" target="329">
  <data key="d6">1</data>
</edge>
<edge source="1" target="330">
  <data key="d6">1</data>
</edge>
<edge source="1" target="331">
  <data key="d6">1</data>
</edge>
<edge source="1" target="332">
  <data key="d6">1</data>
</edge>
<edge source="1" target="333">
  <data key="d6">1</data>
</edge>
<edge source="1" target="334">
  <data key="d6">1</data>
</edge>
<edge source="1" target="335">
  <data key="d6">1</data>
</edge>
<edge source="1" target="336">
  <data key="d6">1</data>
</edge>
<edge source="1" target="337">
  <data key="d6">1</data>
</edge>
<edge source="1" target="338">
  <data key="d6">1</data>
</edge>
<edge source="1" target="339">
  <data key="d6">1</data>
</edge>
<edge source="1" target="340">
  <data key="d6">1</data>
</edge>
<edge source="1" target="341">
  <data key="d6">1</data>
</edge>
<edge source="1" target="342">
  <data key="d6">1</data>
</edge>
<edge source="1" target="343">
  <data key="d6">1</data>
</edge>
<edge source="1" target="344">
  <data key="d6">1</data>
</edge>
<edge source="1" target="345">
  <data key="d6">1</data>
</edge>
<edge source="1" target="346">
  <data key="d6">1</data>
</edge>
<edge source="1" target="347">
  <data key="d6">1</data>
</edge>
<edge source="1" target="348">
  <data key="d6">1</data>
</edge>
<edge source="1" target="349">
  <data key="d6">1</data>
</edge>
<edge source="1" target="350">
  <data key="d6">1</data>
</edge>
<edge source="1" target="351">
  <data key="d6">1</data>
</edge>
<edge source="1" target="352">
  <data key="d6">1</data>
</edge>
<edge source="1" target="353">
  <data key="d6">1</data>
</edge>
<edge source="1" target="354">
  <data key="d6">1</data>
</edge>
<edge source="1" target="355">
  <data key="d6">1</data>
</edge>
<edge source="1" target="356">
  <data key="d6">1</data>
</edge>
<edge source="1" target="357">
  <data key="d6">1</data>
</edge>
<edge source="1" target="358">
  <data key="d6">1</data>
</edge>
<edge source="1" target="359">
  <data key="d6">1</data>
</edge>
<edge source="1" target="360">
  <data key="d6">1</data>
</edge>
<edge source="1" target="361">
  <data key="d6">1</data>
</edge>
<edge source="1" target="362">
  <data key="d6">1</data>
</edge>
<edge source="1" target="363">
  <data key="d6">1</data>
</edge>
<edge source="1" target="364">
  <data key="d6">1</data>
</edge>
<edge source="1" target="365">
  <data key="d6">1</data>
</edge>
<edge source="1" target="366">
  <data key="d6">1</data>
</edge>
<edge source="1" target="367">
  <data key="d6">1</data>
</edge>
<edge source="1" target="368">
  <data key="d6">1</data>
</edge>
<edge source="1" target="369">
  <data key="d6">1</data>
</edge>
<edge source="1" target="370">
  <data key="d6">1</data>
</edge>
<edge source="1" target="371">
  <data key="d6">1</data>
</edge>
<edge source="1" target="372">
  <data key="d6">1</data>
</edge>
<edge source="1" target="373">
  <data key="d6">1</data>
</edge>
<edge source="1" target="374">
  <data key="d6">1</data>
</edge>
<edge source="1" target="375">
  <data key="d6">1</data>
</edge>
<edge source="1" target="376">
  <data key="d6">1</data>
</edge>
<edge source="1" target="377">
  <data key="d6">1</data>
</edge>
<edge source="1" target="378">
  <data key="d6">1</data>
</edge>
<edge source="1" target="379">
  <data key="d6">1</data>
</edge>
<edge source="1" target="380">
  <data key="d6">1</data>
</edge>
<edge source="1" target="381">
  <data key="d6">1</data>
</edge>
<edge source="1" target="382">
  <data key="d6">1</data>
</edge>
<edge source="1" target="383">
  <data key="d6">1</data>
</edge>
<edge source="1" target="384">
  <data key="d6">1</data>
</edge>
<edge source="1" target="385">
  <data key="d6">1</data>
</edge>
<edge source="1" target="386">
  <data key="d6">1</data>
</edge>
<edge source="1" target="387">
  <data key="d6">1</data>
</edge>
<edge source="1" target="388">
  <data key="d6">1</data>
</edge>
<edge source="1" target="389">
  <data key="d6">1</data>
</edge>
<edge source="1" target="390">
  <data key="d6">1</data>
</edge>
<edge source="1" target="391">
  <data key="d6">1</data>
</edge>
<edge source="1" target="392">
  <data key="d6">1</data>
</edge>
<edge source="1" target="393">
  <data key="d6">1</data>
</edge>
<edge source="1" target="394">
  <data key="d6">1</data>
</edge>
<edge source="1" target="395">
  <data key="d6">1</data>
</edge>
<edge source="1" target="396">
  <data key="d6">1</data>
</edge>
<edge source="1" target="397">
  <data key="d6">1</data>
</edge>
<edge source="1" target="398">
  <data key="d6">1</data>
</edge>
<edge source="1" target="399">
  <data key="d6">1</data>
</edge>
<edge source="1" target="400">
  <data key="d6">1</data>
</edge>
<edge source="1" target="401">
  <data key="d6">1</data>
</edge>
<edge source="1" target="402">
  <data key="d6">1</data>
</edge>
<edge source="1" target="403">
  <data key="d6">1</data>
</edge>
<edge source="1" target="404">
  <data key="d6">1</data>
</edge>
<edge source="1" target="405">
  <data key="d6">1</data>
</edge>
<edge source="1" target="406">
  <data key="d6">1</data>
</edge>
<edge source="1" target="407">
  <data key="d6">1</data>
</edge>
<edge source="1" target="408">
  <data key="d6">1</data>
</edge>
<edge source="1" target="409">
  <data key="d6">1</data>
</edge>
<edge source="1" target="410">
  <data key="d6">1</data>
</edge>
<edge source="1" target="411">
  <data key="d6">1</data>
</edge>
<edge source="1" target="412">
  <data key="d6">1</data>
</edge>
<edge source="1" target="413">
  <data key="d6">1</data>
</edge>
<edge source="1" target="414">
  <data key="d6">1</data>
</edge>
<edge source="1" target="415">
  <data key="d6">1</data>
</edge>
<edge source="1" target="416">
  <data key="d6">1</data>
</edge>
<edge source="1" target="417">
  <data key="d6">1</data>
</edge>
<edge source="1" target="418">
  <data key="d6">1</data>
</edge>
<edge source="1" target="419">
  <data key="d6">1</data>
</edge>
<edge source="1" target="420">
  <data key="d6">1</data>
</edge>
<edge source="1" target="421">
  <data key="d6">1</data>
</edge>
<edge source="1" target="422">
  <data key="d6">1</data>
</edge>
<edge source="1" target="423">
  <data key="d6">1</data>
</edge>
<edge source="1" target="424">
  <data key="d6">1</data>
</edge>
<edge source="1" target="425">
  <data key="d6">1</data>
</edge>
<edge source="1" target="426">
  <data key="d6">1</data>
</edge>
<edge source="1" target="427">
  <data key="d6">1</data>
</edge>
<edge source="1" target="428">
  <data key="d6">1</data>
</edge>
<edge source="1" target="429">
  <data key="d6">1</data>
</edge>
<edge source="1" target="430">
  <data key="d6">1</data>
</edge>
<edge source="1" target="431">
  <data key="d6">1</data>
</edge>
<edge source="1" target="432">
  <data key="d6">1</data>
</edge>
<edge source="1" target="433">
  <data key="d6">1</data>
</edge>
<edge source="1" target="434">
  <data key="d6">1</data>
</edge>
<edge source="1" target="435">
  <data key="d6">1</data>
</edge>
<edge source="1" target="436">
  <data key="d6">1</data>
</edge>
<edge source="1" target="437">
  <data key="d6">1</data>
</edge>
<edge source="1" target="438">
  <data key="d6">1</data>
</edge>
<edge source="1" target="439">
  <data key="d6">1</data>
</edge>
<edge source="1" target="440">
  <data key="d6">1</data>
</edge>
<edge source="1" target="441">
  <data key="d6">1</data>
</edge>
<edge source="1" target="442">
  <data key="d6">1</data>
</edge>
<edge source="1" target="443">
  <data key="d6">1</data>
</edge>
<edge source="1" target="444">
  <data key="d6">1</data>
</edge>
<edge source="1" target="445">
  <data key="d6">1</data>
</edge>
<edge source="1" target="446">
  <data key="d6">1</data>
</edge>
<edge source="1" target="447">
  <data key="d6">1</data>
</edge>
<edge source="1" target="448">
  <data key="d6">1</data>
</edge>
<edge source="1" target="449">
  <data key="d6">1</data>
</edge>
<edge source="1" target="450">
  <data key="d6">1</data>
</edge>
<edge source="1" target="451">
  <data key="d6">1</data>
</edge>
<edge source="1" target="452">
  <data key="d6">1</data>
</edge>
<edge source="1" target="453">
  <data key="d6">1</data>
</edge>
<edge source="1" target="454">
  <data key="d6">1</data>
</edge>
<edge source="1" target="455">
  <data key="d6">1</data>
</edge>
<edge source="1" target="456">
  <data key="d6">1</data>
</edge>
<edge source="1" target="457">
  <data key="d6">1</data>
</edge>
<edge source="1" target="458">
  <data key="d6">1</data>
</edge>
<edge source="1" target="459">
  <data key="d6">1</data>
</edge>
<edge source="1" target="460">
  <data key="d6">1</data>
</edge>
<edge source="1" target="461">
  <data key="d6">1</data>
</edge>
<edge source="1" target="462">
  <data key="d6">1</data>
</edge>
<edge source="1" target="463">
  <data key="d6">1</data>
</edge>
<edge source="1" target="464">
  <data key="d6">1</data>
</edge>
<edge source="1" target="465">
  <data key="d6">1</data>
</edge>
<edge source="1" target="466">
  <data key="d6">1</data>
</edge>
<edge source="1" target="467">
  <data key="d6">1</data>
</edge>
<edge source="1" target="468">
  <data key="d6">1</data>
</edge>
<edge source="1" target="469">
  <data key="d6">1</data>
</edge>
<edge source="1" target="470">
  <data key="d6">1</data>
</edge>
<edge source="1" target="471">
  <data key="d6">1</data>
</edge>
<edge source="1" target="472">
  <data key="d6">1</data>
</edge>
<edge source="1" target="473">
  <data key="d6">1</data>
</edge>
<edge source="1" target="474">
  <data key="d6">1</data>
</edge>
<edge source="1" target="475">
  <data key="d6">1</data>
</edge>
<edge source="1" target="476">
  <data key="d6">1</data>
</edge>
<edge source="1" target="477">
  <data key="d6">1</data>
</edge>
<edge source="1" target="478">
  <data key="d6">1</data>
</edge>
<edge source="1" target="479">
  <data key="d6">1</data>
</edge>
<edge source="1" target="480">
  <data key="d6">1</data>
</edge>
<edge source="1" target="481">
  <data key="d6">1</data>
</edge>
<edge source="1" target="482">
  <data key="d6">1</data>
</edge>
<edge source="1" target="483">
  <data key="d6">1</data>
</edge>
<edge source="1" target="484">
  <data key="d6">1</data>
</edge>
<edge source="1" target="485">
  <data key="d6">1</data>
</edge>
<edge source="1" target="486">
  <data key="d6">1</data>
</edge>
<edge source="1" target="487">
  <data key="d6">1</data>
</edge>
<edge source="1" target="488">
  <data key="d6">1</data>
</edge>
<edge source="1" target="489">
  <data key="d6">1</data>
</edge>
<edge source="1" target="490">
  <data key="d6">1</data>
</edge>
<edge source="1" target="491">
  <data key="d6">1</data>
</edge>
<edge source="1" target="492">
  <data key="d6">1</data>
</edge>
<edge source="1" target="493">
  <data key="d6">1</data>
</edge>
<edge source="1" target="494">
  <data key="d6">1</data>
</edge>
<edge source="1" target="495">
  <data key="d6">1</data>
</edge>
<edge source="1" target="496">
  <data key="d6">1</data>
</edge>
<edge source="1" target="497">
  <data key="d6">1</data>
</edge>
<edge source="1" target="498">
  <data key="d6">1</data>
</edge>
<edge source="1" target="499">
  <data key="d6">1</data>
</edge>
<edge source="1" target="500">
  <data key="d6">1</data>
</edge>
<edge source="1" target="501">
  <data key="d6">1</data>
</edge>
<edge source="1" target="502">
  <data key="d6">1</data>
</edge>
<edge source="1" target="503">
  <data key="d6">1</data>
</edge>
<edge source="1" target="504">
  <data key="d6">1</data>
</edge>
<edge source="1" target="505">
  <data key="d6">1</data>
</edge>
<edge source="1" target="506">
  <data key="d6">1</data>
</edge>
<edge source="1" target="507">
  <data key="d6">1</data>
</edge>
<edge source="1" target="508">
  <data key="d6">1</data>
</edge>
<edge source="1" target="509">
  <data key="d6">1</data>
</edge>
<edge source="1" target="510">
  <data key="d6">1</data>
</edge>
<edge source="1" target="511">
  <data key="d6">1</data>
</edge>
<edge source="1" target="512">
  <data key="d6">1</data>
</edge>
<edge source="1" target="513">
  <data key="d6">1</data>
</edge>
<edge source="1" target="514">
  <data key="d6">1</data>
</edge>
<edge source="1" target="515">
  <data key="d6">1</data>
</edge>
<edge source="1" target="516">
  <data key="d6">1</data>
</edge>
<edge source="1" target="517">
  <data key="d6">1</data>
</edge>
<edge source="1" target="518">
  <data key="d6">1</data>
</edge>
<edge source="1" target="519">
  <data key="d6">1</data>
</edge>
<edge source="1" target="520">
  <data key="d6">1</data>
</edge>
<edge source="1" target="521">
  <data key="d6">1</data>
</edge>
<edge source="1" target="522">
  <data key="d6">1</data>
</edge>
<edge source="1" target="523">
  <data key="d6">1</data>
</edge>
<edge source="1" target="524">
  <data key="d6">1</data>
</edge>
<edge source="1" target="525">
  <data key="d6">1</data>
</edge>
<edge source="1" target="526">
  <data key="d6">1</data>
</edge>
<edge source="1" target="527">
  <data key="d6">1</data>
</edge>
<edge source="1" target="528">
  <data key="d6">1</data>
</edge>
<edge source="1" target="529">
  <data key="d6">1</data>
</edge>
<edge source="1" target="530">
  <data key="d6">1</data>
</edge>
<edge source="1" target="531">
  <data key="d6">1</data>
</edge>
<edge source="1" target="532">
  <data key="d6">1</data>
</edge>
<edge source="1" target="533">
  <data key="d6">1</data>
</edge>
<edge source="1" target="534">
  <data key="d6">1</data>
</edge>
<edge source="1" target="535">
  <data key="d6">1</data>
</edge>
<edge source="1" target="536">
  <data key="d6">1</data>
</edge>
<edge source="1" target="537">
  <data key="d6">1</data>
</edge>
<edge source="1" target="538">
  <data key="d6">1</data>
</edge>
<edge source="1" target="539">
  <data key="d6">1</data>
</edge>
<edge source="1" target="540">
  <data key="d6">1</data>
</edge>
<edge source="1" target="541">
  <data key="d6">1</data>
</edge>
<edge source="1" target="542">
  <data key="d6">1</data>
</edge>
<edge source="1" target="543">
  <data key="d6">1</data>
</edge>
<edge source="1" target="544">
  <data key="d6">1</data>
</edge>
<edge source="1" target="545">
  <data key="d6">1</data>
</edge>
<edge source="1" target="546">
  <data key="d6">1</data>
</edge>
<edge source="1" target="547">
  <data key="d6">1</data>
</edge>
<edge source="1" target="548">
  <data key="d6">1</data>
</edge>
<edge source="1" target="549">
  <data key="d6">1</data>
</edge>
<edge source="1" target="550">
  <data key="d6">1</data>
</edge>
<edge source="1" target="551">
  <data key="d6">1</data>
</edge>
<edge source="1" target="552">
  <data key="d6">1</data>
</edge>
<edge source="1" target="553">
  <data key="d6">1</data>
</edge>
<edge source="1" target="554">
  <data key="d6">1</data>
</edge>
<edge source="1" target="555">
  <data key="d6">1</data>
</edge>
<edge source="1" target="556">
  <data key="d6">1</data>
</edge>
<edge source="1" target="557">
  <data key="d6">1</data>
</edge>
<edge source="1" target="558">
  <data key="d6">1</data>
</edge>
<edge source="1" target="559">
  <data key="d6">1</data>
</edge>
<edge source="1" target="560">
  <data key="d6">1</data>
</edge>
<edge source="1" target="561">
  <data key="d6">1</data>
</edge>
<edge source="1" target="562">
  <data key="d6">1</data>
</edge>
<edge source="1" target="563">
  <data key="d6">1</data>
</edge>
<edge source="1" target="564">
  <data key="d6">1</data>
</edge>
<edge source="1" target="565">
  <data key="d6">1</data>
</edge>
<edge source="1" target="566">
  <data key="d6">1</data>
</edge>
<edge source="1" target="567">
  <data key="d6">1</data>
</edge>
<edge source="90" target="text_1">
  <data key="d6">1</data>
</edge>
<edge source="91" target="text_2">
  <data key="d6">1</data>
</edge>
<edge source="92" target="text_3">
  <data key="d6">1</data>
</edge>
<edge source="93" target="text_4">
  <data key="d6">1</data>
</edge>
<edge source="94" target="text_5">
  <data key="d6">1</data>
</edge>
<edge source="95" target="text_6">
  <data key="d6">1</data>
</edge>
<edge source="96" target="text_7">
  <data key="d6">1</data>
</edge>
<edge source="97" target="text_8">
  <data key="d6">1</data>
</edge>
<edge source="98" target="text_9">
  <data key="d6">1</data>
</edge>
<edge source="99" target="text_10">
  <data key="d6">1</data>
</edge>
<edge source="100" target="text_11">
  <data key="d6">1</data>
</edge>
<edge source="101" target="text_12">
  <data key="d6">1</data>
</edge>
<edge source="102" target="text_13">
  <data key="d6">1</data>
</edge>
<edge source="103" target="text_14">
  <data key="d6">1</data>
</edge>
<edge source="104" target="text_15">
  <data key="d6">1</data>
</edge>
<edge source="105" target="text_16">
  <data key="d6">1</data>
</edge>
<edge source="106" target="text_17">
  <data key="d6">1</data>
</edge>
<edge source="107" target="text_18">
  <data key="d6">1</data>
</edge>
<edge source="108" target="text_19">
  <data key="d6">1</data>
</edge>
<edge source="109" target="text_20">
  <data key="d6">1</data>
</edge>
<edge source="110" target="text_21">
  <data key="d6">1</data>
</edge>
<edge source="111" target="text_22">
  <data key="d6">1</data>
</edge>
<edge source="112" target="text_23">
  <data key="d6">1</data>
</edge>
<edge source="113" target="text_24">
  <data key="d6">1</data>
</edge>
<edge source="114" target="text_25">
  <data key="d6">1</data>
</edge>
<edge source="115" target="text_26">
  <data key="d6">1</data>
</edge>
<edge source="116" target="text_27">
  <data key="d6">1</data>
</edge>
<edge source="117" target="text_28">
  <data key="d6">1</data>
</edge>
<edge source="118" target="text_29">
  <data key="d6">1</data>
</edge>
<edge source="119" target="text_30">
  <data key="d6">1</data>
</edge>
<edge source="120" target="text_31">
  <data key="d6">1</data>
</edge>
<edge source="121" target="text_32">
  <data key="d6">1</data>
</edge>
<edge source="122" target="text_33">
  <data key="d6">1</data>
</edge>
<edge source="123" target="text_34">
  <data key="d6">1</data>
</edge>
<edge source="124" target="text_35">
  <data key="d6">1</data>
</edge>
<edge source="125" target="text_36">
  <data key="d6">1</data>
</edge>
<edge source="126" target="text_37">
  <data key="d6">1</data>
</edge>
<edge source="127" target="text_38">
  <data key="d6">1</data>
</edge>
<edge source="128" target="text_39">
  <data key="d6">1</data>
</edge>
<edge source="129" target="text_40">
  <data key="d6">1</data>
</edge>
<edge source="130" target="text_41">
  <data key="d6">1</data>
</edge>
<edge source="131" target="text_42">
  <data key="d6">1</data>
</edge>
<edge source="132" target="text_43">
  <data key="d6">1</data>
</edge>
<edge source="133" target="text_44">
  <data key="d6">1</data>
</edge>
<edge source="134" target="text_45">
  <data key="d6">1</data>
</edge>
<edge source="135" target="text_46">
  <data key="d6">1</data>
</edge>
<edge source="136" target="text_47">
  <data key="d6">1</data>
</edge>
<edge source="137" target="text_48">
  <data key="d6">1</data>
</edge>
<edge source="138" target="text_49">
  <data key="d6">1</data>
</edge>
<edge source="139" target="text_50">
  <data key="d6">1</data>
</edge>
<edge source="140" target="text_51">
  <data key="d6">1</data>
</edge>
<edge source="141" target="text_52">
  <data key="d6">1</data>
</edge>
<edge source="142" target="text_53">
  <data key="d6">1</data>
</edge>
<edge source="143" target="text_54">
  <data key="d6">1</data>
</edge>
<edge source="144" target="text_55">
  <data key="d6">1</data>
</edge>
<edge source="145" target="text_56">
  <data key="d6">1</data>
</edge>
<edge source="146" target="text_57">
  <data key="d6">1</data>
</edge>
<edge source="147" target="text_58">
  <data key="d6">1</data>
</edge>
<edge source="148" target="text_59">
  <data key="d6">1</data>
</edge>
<edge source="149" target="text_60">
  <data key="d6">1</data>
</edge>
<edge source="150" target="text_61">
  <data key="d6">1</data>
</edge>
<edge source="151" target="text_62">
  <data key="d6">1</data>
</edge>
<edge source="152" target="text_63">
  <data key="d6">1</data>
</edge>
<edge source="153" target="text_64">
  <data key="d6">1</data>
</edge>
<edge source="154" target="text_65">
  <data key="d6">1</data>
</edge>
<edge source="155" target="text_66">
  <data key="d6">1</data>
</edge>
<edge source="156" target="text_67">
  <data key="d6">1</data>
</edge>
<edge source="157" target="text_68">
  <data key="d6">1</data>
</edge>
<edge source="158" target="text_69">
  <data key="d6">1</data>
</edge>
<edge source="159" target="text_70">
  <data key="d6">1</data>
</edge>
<edge source="160" target="text_71">
  <data key="d6">1</data>
</edge>
<edge source="161" target="text_72">
  <data key="d6">1</data>
</edge>
<edge source="162" target="text_73">
  <data key="d6">1</data>
</edge>
<edge source="163" target="text_74">
  <data key="d6">1</data>
</edge>
<edge source="164" target="text_75">
  <data key="d6">1</data>
</edge>
<edge source="165" target="text_76">
  <data key="d6">1</data>
</edge>
<edge source="166" target="text_77">
  <data key="d6">1</data>
</edge>
<edge source="167" target="text_78">
  <data key="d6">1</data>
</edge>
<edge source="168" target="text_79">
  <data key="d6">1</data>
</edge>
<edge source="169" target="text_80">
  <data key="d6">1</data>
</edge>
<edge source="170" target="text_81">
  <data key="d6">1</data>
</edge>
<edge source="171" target="text_82">
  <data key="d6">1</data>
</edge>
<edge source="172" target="text_83">
  <data key="d6">1</data>
</edge>
<edge source="173" target="text_84">
  <data key="d6">1</data>
</edge>
<edge source="174" target="text_85">
  <data key="d6">1</data>
</edge>
<edge source="175" target="text_86">
  <data key="d6">1</data>
</edge>
<edge source="176" target="text_87">
  <data key="d6">1</data>
</edge>
<edge source="177" target="text_88">
  <data key="d6">1</data>
</edge>
<edge source="178" target="text_89">
  <data key="d6">1</data>
</edge>
<edge source="179" target="text_90">
  <data key="d6">1</data>
</edge>
<edge source="180" target="text_91">
  <data key="d6">1</data>
</edge>
<edge source="181" target="text_92">
  <data key="d6">1</data>
</edge>
<edge source="182" target="text_93">
  <data key="d6">1</data>
</edge>
<edge source="183" target="text_94">
  <data key="d6">1</data>
</edge>
<edge source="184" target="text_95">
  <data key="d6">1</data>
</edge>
<edge source="185" target="text_96">
  <data key="d6">1</data>
</edge>
<edge source="186" target="text_97">
  <data key="d6">1</data>
</edge>
<edge source="187" target="text_98">
  <data key="d6">1</data>
</edge>
<edge source="188" target="text_99">
  <data key="d6">1</data>
</edge>
<edge source="189" target="text_100">
  <data key="d6">1</data>
</edge>
<edge source="190" target="text_101">
  <data key="d6">1</data>
</edge>
<edge source="191" target="text_102">
  <data key="d6">1</data>
</edge>
<edge source="192" target="text_103">
  <data key="d6">1</data>
</edge>
<edge source="193" target="text_104">
  <data key="d6">1</data>
</edge>
<edge source="194" target="text_105">
  <data key="d6">1</data>
</edge>
<edge source="195" target="text_106">
  <data key="d6">1</data>
</edge>
<edge source="196" target="text_107">
  <data key="d6">1</data>
</edge>
<edge source="197" target="text_108">
  <data key="d6">1</data>
</edge>
<edge source="198" target="text_109">
  <data key="d6">1</data>
</edge>
<edge source="199" target="text_110">
  <data key="d6">1</data>
</edge>
<edge source="200" target="text_111">
  <data key="d6">1</data>
</edge>
<edge source="201" target="text_112">
  <data key="d6">1</data>
</edge>
<edge source="202" target="text_113">
  <data key="d6">1</data>
</edge>
<edge source="203" target="text_114">
  <data key="d6">1</data>
</edge>
<edge source="204" target="text_115">
  <data key="d6">1</data>
</edge>
<edge source="205" target="text_116">
  <data key="d6">1</data>
</edge>
<edge source="206" target="text_117">
  <data key="d6">1</data>
</edge>
<edge source="207" target="text_118">
  <data key="d6">1</data>
</edge>
<edge source="208" target="text_119">
  <data key="d6">1</data>
</edge>
<edge source="209" target="text_120">
  <data key="d6">1</data>
</edge>
<edge source="210" target="text_121">
  <data key="d6">1</data>
</edge>
<edge source="211" target="text_122">
  <data key="d6">1</data>
</edge>
<edge source="212" target="text_123">
  <data key="d6">1</data>
</edge>
<edge source="213" target="text_124">
  <data key="d6">1</data>
</edge>
<edge source="214" target="text_125">
  <data key="d6">1</data>
</edge>
<edge source="215" target="text_126">
  <data key="d6">1</data>
</edge>
<edge source="216" target="text_127">
  <data key="d6">1</data>
</edge>
<edge source="217" target="text_128">
  <data key="d6">1</data>
</edge>
<edge source="218" target="text_129">
  <data key="d6">1</data>
</edge>
<edge source="219" target="text_130">
  <data key="d6">1</data>
</edge>
<edge source="220" target="text_131">
  <data key="d6">1</data>
</edge>
<edge source="221" target="text_132">
  <data key="d6">1</data>
</edge>
<edge source="222" target="text_133">
  <data key="d6">1</data>
</edge>
<edge source="223" target="text_134">
  <data key="d6">1</data>
</edge>
<edge source="224" target="text_135">
  <data key="d6">1</data>
</edge>
<edge source="225" target="text_136">
  <data key="d6">1</data>
</edge>
<edge source="226" target="text_137">
  <data key="d6">1</data>
</edge>
<edge source="227" target="text_138">
  <data key="d6">1</data>
</edge>
<edge source="228" target="text_139">
  <data key="d6">1</data>
</edge>
<edge source="229" target="text_140">
  <data key="d6">1</data>
</edge>
<edge source="230" target="text_141">
  <data key="d6">1</data>
</edge>
<edge source="231" target="text_142">
  <data key="d6">1</data>
</edge>
<edge source="232" target="text_143">
  <data key="d6">1</data>
</edge>
<edge source="233" target="text_144">
  <data key="d6">1</data>
</edge>
<edge source="234" target="text_145">
  <data key="d6">1</data>
</edge>
<edge source="235" target="text_146">
  <data key="d6">1</data>
</edge>
<edge source="236" target="text_147">
  <data key="d6">1</data>
</edge>
<edge source="237" target="text_148">
  <data key="d6">1</data>
</edge>
<edge source="238" target="text_149">
  <data key="d6">1</data>
</edge>
<edge source="239" target="text_150">
  <data key="d6">1</data>
</edge>
<edge source="240" target="text_151">
  <data key="d6">1</data>
</edge>
<edge source="241" target="text_152">
  <data key="d6">1</data>
</edge>
<edge source="242" target="text_153">
  <data key="d6">1</data>
</edge>
<edge source="243" target="text_154">
  <data key="d6">1</data>
</edge>
<edge source="244" target="text_155">
  <data key="d6">1</data>
</edge>
<edge source="245" target="text_156">
  <data key="d6">1</data>
</edge>
<edge source="246" target="text_157">
  <data key="d6">1</data>
</edge>
<edge source="247" target="text_158">
  <data key="d6">1</data>
</edge>
<edge source="248" target="text_159">
  <data key="d6">1</data>
</edge>
<edge source="249" target="text_160">
  <data key="d6">1</data>
</edge>
<edge source="250" target="text_161">
  <data key="d6">1</data>
</edge>
<edge source="251" target="text_162">
  <data key="d6">1</data>
</edge>
<edge source="252" target="text_163">
  <data key="d6">1</data>
</edge>
<edge source="253" target="text_164">
  <data key="d6">1</data>
</edge>
<edge source="254" target="text_165">
  <data key="d6">1</data>
</edge>
<edge source="255" target="text_166">
  <data key="d6">1</data>
</edge>
<edge source="256" target="text_167">
  <data key="d6">1</data>
</edge>
<edge source="257" target="text_168">
  <data key="d6">1</data>
</edge>
<edge source="258" target="text_169">
  <data key="d6">1</data>
</edge>
<edge source="259" target="text_170">
  <data key="d6">1</data>
</edge>
<edge source="260" target="text_171">
  <data key="d6">1</data>
</edge>
<edge source="261" target="text_172">
  <data key="d6">1</data>
</edge>
<edge source="262" target="text_173">
  <data key="d6">1</data>
</edge>
<edge source="263" target="text_174">
  <data key="d6">1</data>
</edge>
<edge source="264" target="text_175">
  <data key="d6">1</data>
</edge>
<edge source="265" target="text_176">
  <data key="d6">1</data>
</edge>
<edge source="266" target="text_177">
  <data key="d6">1</data>
</edge>
<edge source="267" target="text_178">
  <data key="d6">1</data>
</edge>
<edge source="268" target="text_179">
  <data key="d6">1</data>
</edge>
<edge source="269" target="text_180">
  <data key="d6">1</data>
</edge>
<edge source="270" target="text_181">
  <data key="d6">1</data>
</edge>
<edge source="271" target="text_182">
  <data key="d6">1</data>
</edge>
<edge source="272" target="text_183">
  <data key="d6">1</data>
</edge>
<edge source="273" target="text_184">
  <data key="d6">1</data>
</edge>
<edge source="274" target="text_185">
  <data key="d6">1</data>
</edge>
<edge source="275" target="text_186">
  <data key="d6">1</data>
</edge>
<edge source="276" target="text_187">
  <data key="d6">1</data>
</edge>
<edge source="277" target="text_188">
  <data key="d6">1</data>
</edge>
<edge source="278" target="text_189">
  <data key="d6">1</data>
</edge>
<edge source="279" target="text_190">
  <data key="d6">1</data>
</edge>
<edge source="280" target="text_191">
  <data key="d6">1</data>
</edge>
<edge source="281" target="text_192">
  <data key="d6">1</data>
</edge>
<edge source="282" target="text_193">
  <data key="d6">1</data>
</edge>
<edge source="283" target="text_194">
  <data key="d6">1</data>
</edge>
<edge source="284" target="text_195">
  <data key="d6">1</data>
</edge>
<edge source="285" target="text_196">
  <data key="d6">1</data>
</edge>
<edge source="286" target="text_197">
  <data key="d6">1</data>
</edge>
<edge source="287" target="text_198">
  <data key="d6">1</data>
</edge>
<edge source="288" target="text_199">
  <data key="d6">1</data>
</edge>
<edge source="289" target="text_200">
  <data key="d6">1</data>
</edge>
<edge source="290" target="text_201">
  <data key="d6">1</data>
</edge>
<edge source="291" target="text_202">
  <data key="d6">1</data>
</edge>
<edge source="292" target="text_203">
  <data key="d6">1</data>
</edge>
<edge source="293" target="text_204">
  <data key="d6">1</data>
</edge>
<edge source="294" target="text_205">
  <data key="d6">1</data>
</edge>
<edge source="295" target="text_206">
  <data key="d6">1</data>
</edge>
<edge source="296" target="text_207">
  <data key="d6">1</data>
</edge>
<edge source="297" target="text_208">
  <data key="d6">1</data>
</edge>
<edge source="298" target="text_209">
  <data key="d6">1</data>
</edge>
<edge source="299" target="text_210">
  <data key="d6">1</data>
</edge>
<edge source="300" target="text_211">
  <data key="d6">1</data>
</edge>
<edge source="301" target="text_212">
  <data key="d6">1</data>
</edge>
<edge source="302" target="text_213">
  <data key="d6">1</data>
</edge>
<edge source="303" target="text_214">
  <data key="d6">1</data>
</edge>
<edge source="304" target="text_215">
  <data key="d6">1</data>
</edge>
<edge source="305" target="text_216">
  <data key="d6">1</data>
</edge>
<edge source="306" target="text_217">
  <data key="d6">1</data>
</edge>
<edge source="307" target="text_218">
  <data key="d6">1</data>
</edge>
<edge source="308" target="text_219">
  <data key="d6">1</data>
</edge>
<edge source="309" target="text_220">
  <data key="d6">1</data>
</edge>
<edge source="310" target="text_221">
  <data key="d6">1</data>
</edge>
<edge source="311" target="text_222">
  <data key="d6">1</data>
</edge>
<edge source="312" target="text_223">
  <data key="d6">1</data>
</edge>
<edge source="313" target="text_224">
  <data key="d6">1</data>
</edge>
<edge source="314" target="text_225">
  <data key="d6">1</data>
</edge>
<edge source="315" target="text_226">
  <data key="d6">1</data>
</edge>
<edge source="316" target="text_227">
  <data key="d6">1</data>
</edge>
<edge source="317" target="text_228">
  <data key="d6">1</data>
</edge>
<edge source="318" target="text_229">
  <data key="d6">1</data>
</edge>
<edge source="319" target="text_230">
  <data key="d6">1</data>
</edge>
<edge source="320" target="text_231">
  <data key="d6">1</data>
</edge>
<edge source="321" target="text_232">
  <data key="d6">1</data>
</edge>
<edge source="322" target="text_233">
  <data key="d6">1</data>
</edge>
<edge source="323" target="text_234">
  <data key="d6">1</data>
</edge>
<edge source="324" target="text_235">
  <data key="d6">1</data>
</edge>
<edge source="325" target="text_236">
  <data key="d6">1</data>
</edge>
<edge source="326" target="text_237">
  <data key="d6">1</data>
</edge>
<edge source="327" target="text_238">
  <data key="d6">1</data>
</edge>
<edge source="328" target="text_239">
  <data key="d6">1</data>
</edge>
<edge source="329" target="text_240">
  <data key="d6">1</data>
</edge>
<edge source="330" target="text_241">
  <data key="d6">1</data>
</edge>
<edge source="331" target="text_242">
  <data key="d6">1</data>
</edge>
<edge source="332" target="text_243">
  <data key="d6">1</data>
</edge>
<edge source="333" target="text_244">
  <data key="d6">1</data>
</edge>
<edge source="334" target="text_245">
  <data key="d6">1</data>
</edge>
<edge source="335" target="text_246">
  <data key="d6">1</data>
</edge>
<edge source="336" target="text_247">
  <data key="d6">1</data>
</edge>
<edge source="337" target="text_248">
  <data key="d6">1</data>
</edge>
<edge source="338" target="text_249">
  <data key="d6">1</data>
</edge>
<edge source="339" target="text_250">
  <data key="d6">1</data>
</edge>
<edge source="340" target="text_251">
  <data key="d6">1</data>
</edge>
<edge source="341" target="text_252">
  <data key="d6">1</data>
</edge>
<edge source="342" target="text_253">
  <data key="d6">1</data>
</edge>
<edge source="343" target="text_254">
  <data key="d6">1</data>
</edge>
<edge source="344" target="text_255">
  <data key="d6">1</data>
</edge>
<edge source="345" target="text_256">
  <data key="d6">1</data>
</edge>
<edge source="346" target="text_257">
  <data key="d6">1</data>
</edge>
<edge source="347" target="text_258">
  <data key="d6">1</data>
</edge>
<edge source="348" target="text_259">
  <data key="d6">1</data>
</edge>
<edge source="349" target="text_260">
  <data key="d6">1</data>
</edge>
<edge source="350" target="text_261">
  <data key="d6">1</data>
</edge>
<edge source="351" target="text_262">
  <data key="d6">1</data>
</edge>
<edge source="352" target="text_263">
  <data key="d6">1</data>
</edge>
<edge source="353" target="text_264">
  <data key="d6">1</data>
</edge>
<edge source="354" target="text_265">
  <data key="d6">1</data>
</edge>
<edge source="355" target="text_266">
  <data key="d6">1</data>
</edge>
<edge source="356" target="text_267">
  <data key="d6">1</data>
</edge>
<edge source="357" target="text_268">
  <data key="d6">1</data>
</edge>
<edge source="358" target="text_269">
  <data key="d6">1</data>
</edge>
<edge source="359" target="text_270">
  <data key="d6">1</data>
</edge>
<edge source="360" target="text_271">
  <data key="d6">1</data>
</edge>
<edge source="361" target="text_272">
  <data key="d6">1</data>
</edge>
<edge source="362" target="text_273">
  <data key="d6">1</data>
</edge>
<edge source="363" target="text_274">
  <data key="d6">1</data>
</edge>
<edge source="364" target="text_275">
  <data key="d6">1</data>
</edge>
<edge source="365" target="text_276">
  <data key="d6">1</data>
</edge>
<edge source="366" target="text_277">
  <data key="d6">1</data>
</edge>
<edge source="367" target="text_278">
  <data key="d6">1</data>
</edge>
<edge source="368" target="text_279">
  <data key="d6">1</data>
</edge>
<edge source="369" target="text_280">
  <data key="d6">1</data>
</edge>
<edge source="370" target="text_281">
  <data key="d6">1</data>
</edge>
<edge source="371" target="text_282">
  <data key="d6">1</data>
</edge>
<edge source="372" target="text_283">
  <data key="d6">1</data>
</edge>
<edge source="373" target="text_284">
  <data key="d6">1</data>
</edge>
<edge source="374" target="text_285">
  <data key="d6">1</data>
</edge>
<edge source="375" target="text_286">
  <data key="d6">1</data>
</edge>
<edge source="376" target="text_287">
  <data key="d6">1</data>
</edge>
<edge source="377" target="text_288">
  <data key="d6">1</data>
</edge>
<edge source="378" target="text_289">
  <data key="d6">1</data>
</edge>
<edge source="379" target="text_290">
  <data key="d6">1</data>
</edge>
<edge source="380" target="text_291">
  <data key="d6">1</data>
</edge>
<edge source="381" target="text_292">
  <data key="d6">1</data>
</edge>
<edge source="382" target="text_293">
  <data key="d6">1</data>
</edge>
<edge source="383" target="text_294">
  <data key="d6">1</data>
</edge>
<edge source="384" target="text_295">
  <data key="d6">1</data>
</edge>
<edge source="385" target="text_296">
  <data key="d6">1</data>
</edge>
<edge source="386" target="text_297">
  <data key="d6">1</data>
</edge>
<edge source="387" target="text_298">
  <data key="d6">1</data>
</edge>
<edge source="388" target="text_299">
  <data key="d6">1</data>
</edge>
<edge source="389" target="text_300">
  <data key="d6">1</data>
</edge>
<edge source="390" target="text_301">
  <data key="d6">1</data>
</edge>
<edge source="391" target="text_302">
  <data key="d6">1</data>
</edge>
<edge source="392" target="text_303">
  <data key="d6">1</data>
</edge>
<edge source="393" target="text_304">
  <data key="d6">1</data>
</edge>
<edge source="394" target="text_305">
  <data key="d6">1</data>
</edge>
<edge source="395" target="text_306">
  <data key="d6">1</data>
</edge>
<edge source="396" target="text_307">
  <data key="d6">1</data>
</edge>
<edge source="397" target="text_308">
  <data key="d6">1</data>
</edge>
<edge source="398" target="text_309">
  <data key="d6">1</data>
</edge>
<edge source="399" target="text_310">
  <data key="d6">1</data>
</edge>
<edge source="400" target="text_311">
  <data key="d6">1</data>
</edge>
<edge source="401" target="text_312">
  <data key="d6">1</data>
</edge>
<edge source="402" target="text_313">
  <data key="d6">1</data>
</edge>
<edge source="403" target="text_314">
  <data key="d6">1</data>
</edge>
<edge source="404" target="text_315">
  <data key="d6">1</data>
</edge>
<edge source="405" target="text_316">
  <data key="d6">1</data>
</edge>
<edge source="406" target="text_317">
  <data key="d6">1</data>
</edge>
<edge source="407" target="text_318">
  <data key="d6">1</data>
</edge>
<edge source="408" target="text_319">
  <data key="d6">1</data>
</edge>
<edge source="409" target="text_320">
  <data key="d6">1</data>
</edge>
<edge source="410" target="text_321">
  <data key="d6">1</data>
</edge>
<edge source="411" target="text_322">
  <data key="d6">1</data>
</edge>
<edge source="412" target="text_323">
  <data key="d6">1</data>
</edge>
<edge source="413" target="text_324">
  <data key="d6">1</data>
</edge>
<edge source="414" target="text_325">
  <data key="d6">1</data>
</edge>
<edge source="415" target="text_326">
  <data key="d6">1</data>
</edge>
<edge source="416" target="text_327">
  <data key="d6">1</data>
</edge>
<edge source="417" target="text_328">
  <data key="d6">1</data>
</edge>
<edge source="418" target="text_329">
  <data key="d6">1</data>
</edge>
<edge source="419" target="text_330">
  <data key="d6">1</data>
</edge>
<edge source="420" target="text_331">
  <data key="d6">1</data>
</edge>
<edge source="421" target="text_332">
  <data key="d6">1</data>
</edge>
<edge source="422" target="text_333">
  <data key="d6">1</data>
</edge>
<edge source="423" target="text_334">
  <data key="d6">1</data>
</edge>
<edge source="424" target="text_335">
  <data key="d6">1</data>
</edge>
<edge source="425" target="text_336">
  <data key="d6">1</data>
</edge>
<edge source="426" target="text_337">
  <data key="d6">1</data>
</edge>
<edge source="427" target="text_338">
  <data key="d6">1</data>
</edge>
<edge source="428" target="text_339">
  <data key="d6">1</data>
</edge>
<edge source="429" target="text_340">
  <data key="d6">1</data>
</edge>
<edge source="430" target="text_341">
  <data key="d6">1</data>
</edge>
<edge source="431" target="text_342">
  <data key="d6">1</data>
</edge>
<edge source="432" target="text_343">
  <data key="d6">1</data>
</edge>
<edge source="433" target="text_344">
  <data key="d6">1</data>
</edge>
<edge source="434" target="text_345">
  <data key="d6">1</data>
</edge>
<edge source="435" target="text_346">
  <data key="d6">1</data>
</edge>
<edge source="436" target="text_347">
  <data key="d6">1</data>
</edge>
<edge source="437" target="text_348">
  <data key="d6">1</data>
</edge>
<edge source="438" target="text_349">
  <data key="d6">1</data>
</edge>
<edge source="439" target="text_350">
  <data key="d6">1</data>
</edge>
<edge source="440" target="text_351">
  <data key="d6">1</data>
</edge>
<edge source="441" target="text_352">
  <data key="d6">1</data>
</edge>
<edge source="442" target="text_353">
  <data key="d6">1</data>
</edge>
<edge source="443" target="text_354">
  <data key="d6">1</data>
</edge>
<edge source="444" target="text_355">
  <data key="d6">1</data>
</edge>
<edge source="445" target="text_356">
  <data key="d6">1</data>
</edge>
<edge source="446" target="text_357">
  <data key="d6">1</data>
</edge>
<edge source="447" target="text_358">
  <data key="d6">1</data>
</edge>
<edge source="448" target="text_359">
  <data key="d6">1</data>
</edge>
<edge source="449" target="text_360">
  <data key="d6">1</data>
</edge>
<edge source="450" target="text_361">
  <data key="d6">1</data>
</edge>
<edge source="451" target="text_362">
  <data key="d6">1</data>
</edge>
<edge source="452" target="text_363">
  <data key="d6">1</data>
</edge>
<edge source="453" target="text_364">
  <data key="d6">1</data>
</edge>
<edge source="454" target="text_365">
  <data key="d6">1</data>
</edge>
<edge source="455" target="text_366">
  <data key="d6">1</data>
</edge>
<edge source="456" target="text_367">
  <data key="d6">1</data>
</edge>
<edge source="457" target="text_368">
  <data key="d6">1</data>
</edge>
<edge source="458" target="text_369">
  <data key="d6">1</data>
</edge>
<edge source="459" target="text_370">
  <data key="d6">1</data>
</edge>
<edge source="460" target="text_371">
  <data key="d6">1</data>
</edge>
<edge source="461" target="text_372">
  <data key="d6">1</data>
</edge>
<edge source="462" target="text_373">
  <data key="d6">1</data>
</edge>
<edge source="463" target="text_374">
  <data key="d6">1</data>
</edge>
<edge source="464" target="text_375">
  <data key="d6">1</data>
</edge>
<edge source="465" target="text_376">
  <data key="d6">1</data>
</edge>
<edge source="466" target="text_377">
  <data key="d6">1</data>
</edge>
<edge source="467" target="text_378">
  <data key="d6">1</data>
</edge>
<edge source="468" target="text_379">
  <data key="d6">1</data>
</edge>
<edge source="469" target="text_380">
  <data key="d6">1</data>
</edge>
<edge source="470" target="text_381">
  <data key="d6">1</data>
</edge>
<edge source="471" target="text_382">
  <data key="d6">1</data>
</edge>
<edge source="472" target="text_383">
  <data key="d6">1</data>
</edge>
<edge source="473" target="text_384">
  <data key="d6">1</data>
</edge>
<edge source="474" target="text_385">
  <data key="d6">1</data>
</edge>
<edge source="475" target="text_386">
  <data key="d6">1</data>
</edge>
<edge source="476" target="text_387">
  <data key="d6">1</data>
</edge>
<edge source="477" target="text_388">
  <data key="d6">1</data>
</edge>
<edge source="478" target="text_389">
  <data key="d6">1</data>
</edge>
<edge source="479" target="text_390">
  <data key="d6">1</data>
</edge>
<edge source="480" target="text_391">
  <data key="d6">1</data>
</edge>
<edge source="481" target="text_392">
  <data key="d6">1</data>
</edge>
<edge source="482" target="text_393">
  <data key="d6">1</data>
</edge>
<edge source="483" target="text_394">
  <data key="d6">1</data>
</edge>
<edge source="484" target="text_395">
  <data key="d6">1</data>
</edge>
<edge source="485" target="text_396">
  <data key="d6">1</data>
</edge>
<edge source="486" target="text_397">
  <data key="d6">1</data>
</edge>
<edge source="487" target="text_398">
  <data key="d6">1</data>
</edge>
<edge source="488" target="text_399">
  <data key="d6">1</data>
</edge>
<edge source="489" target="text_400">
  <data key="d6">1</data>
</edge>
<edge source="490" target="text_401">
  <data key="d6">1</data>
</edge>
<edge source="491" target="text_402">
  <data key="d6">1</data>
</edge>
<edge source="492" target="text_403">
  <data key="d6">1</data>
</edge>
<edge source="493" target="text_404">
  <data key="d6">1</data>
</edge>
<edge source="494" target="text_405">
  <data key="d6">1</data>
</edge>
<edge source="495" target="text_406">
  <data key="d6">1</data>
</edge>
<edge source="496" target="text_407">
  <data key="d6">1</data>
</edge>
<edge source="497" target="text_408">
  <data key="d6">1</data>
</edge>
<edge source="498" target="text_409">
  <data key="d6">1</data>
</edge>
<edge source="499" target="text_410">
  <data key="d6">1</data>
</edge>
<edge source="500" target="text_411">
  <data key="d6">1</data>
</edge>
<edge source="501" target="text_412">
  <data key="d6">1</data>
</edge>
<edge source="502" target="text_413">
  <data key="d6">1</data>
</edge>
<edge source="503" target="text_414">
  <data key="d6">1</data>
</edge>
<edge source="504" target="text_415">
  <data key="d6">1</data>
</edge>
<edge source="505" target="text_416">
  <data key="d6">1</data>
</edge>
<edge source="506" target="text_417">
  <data key="d6">1</data>
</edge>
<edge source="507" target="text_418">
  <data key="d6">1</data>
</edge>
<edge source="508" target="text_419">
  <data key="d6">1</data>
</edge>
<edge source="509" target="text_420">
  <data key="d6">1</data>
</edge>
<edge source="510" target="text_421">
  <data key="d6">1</data>
</edge>
<edge source="511" target="text_422">
  <data key="d6">1</data>
</edge>
<edge source="512" target="text_423">
  <data key="d6">1</data>
</edge>
<edge source="513" target="text_424">
  <data key="d6">1</data>
</edge>
<edge source="514" target="text_425">
  <data key="d6">1</data>
</edge>
<edge source="515" target="text_426">
  <data key="d6">1</data>
</edge>
<edge source="516" target="text_427">
  <data key="d6">1</data>
</edge>
<edge source="517" target="text_428">
  <data key="d6">1</data>
</edge>
<edge source="518" target="text_429">
  <data key="d6">1</data>
</edge>
<edge source="519" target="text_430">
  <data key="d6">1</data>
</edge>
<edge source="520" target="text_431">
  <data key="d6">1</data>
</edge>
<edge source="521" target="text_432">
  <data key="d6">1</data>
</edge>
<edge source="522" target="text_433">
  <data key="d6">1</data>
</edge>
<edge source="523" target="text_434">
  <data key="d6">1</data>
</edge>
<edge source="524" target="text_435">
  <data key="d6">1</data>
</edge>
<edge source="525" target="text_436">
  <data key="d6">1</data>
</edge>
<edge source="526" target="text_437">
  <data key="d6">1</data>
</edge>
<edge source="527" target="text_438">
  <data key="d6">1</data>
</edge>
<edge source="528" target="text_439">
  <data key="d6">1</data>
</edge>
<edge source="529" target="text_440">
  <data key="d6">1</data>
</edge>
<edge source="530" target="text_441">
  <data key="d6">1</data>
</edge>
<edge source="531" target="text_442">
  <data key="d6">1</data>
</edge>
<edge source="532" target="text_443">
  <data key="d6">1</data>
</edge>
<edge source="533" target="text_444">
  <data key="d6">1</data>
</edge>
<edge source="534" target="text_445">
  <data key="d6">1</data>
</edge>
<edge source="535" target="text_446">
  <data key="d6">1</data>
</edge>
<edge source="536" target="text_447">
  <data key="d6">1</data>
</edge>
<edge source="537" target="text_448">
  <data key="d6">1</data>
</edge>
<edge source="538" target="text_449">
  <data key="d6">1</data>
</edge>
<edge source="539" target="text_450">
  <data key="d6">1</data>
</edge>
<edge source="540" target="text_451">
  <data key="d6">1</data>
</edge>
<edge source="541" target="text_452">
  <data key="d6">1</data>
</edge>
<edge source="542" target="text_453">
  <data key="d6">1</data>
</edge>
<edge source="543" target="text_454">
  <data key="d6">1</data>
</edge>
<edge source="544" target="text_455">
  <data key="d6">1</data>
</edge>
<edge source="545" target="text_456">
  <data key="d6">1</data>
</edge>
<edge source="546" target="text_457">
  <data key="d6">1</data>
</edge>
<edge source="547" target="text_458">
  <data key="d6">1</data>
</edge>
<edge source="548" target="text_459">
  <data key="d6">1</data>
</edge>
<edge source="549" target="text_460">
  <data key="d6">1</data>
</edge>
<edge source="550" target="text_461">
  <data key="d6">1</data>
</edge>
<edge source="551" target="text_462">
  <data key="d6">1</data>
</edge>
<edge source="552" target="text_463">
  <data key="d6">1</data>
</edge>
<edge source="553" target="text_464">
  <data key="d6">1</data>
</edge>
<edge source="554" target="text_465">
  <data key="d6">1</data>
</edge>
<edge source="555" target="text_466">
  <data key="d6">1</data>
</edge>
<edge source="556" target="text_467">
  <data key="d6">1</data>
</edge>
<edge source="557" target="text_468">
  <data key="d6">1</data>
</edge>
<edge source="558" target="text_469">
  <data key="d6">1</data>
</edge>
<edge source="559" target="text_470">
  <data key="d6">1</data>
</edge>
<edge source="560" target="text_471">
  <data key="d6">1</data>
</edge>
<edge source="561" target="text_472">
  <data key="d6">1</data>
</edge>
<edge source="562" target="text_473">
  <data key="d6">1</data>
</edge>
<edge source="563" target="text_474">
  <data key="d6">1</data>
</edge>
<edge source="564" target="text_475">
  <data key="d6">1</data>
</edge>
<edge source="565" target="text_476">
  <data key="d6">1</data>
</edge>
<edge source="566" target="text_477">
  <data key="d6">1</data>
</edge>
<edge source="567" target="text_478">
  <data key="d6">1</data>
</edge>
<edge source="0" target="UTS namespace">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="0" target="Port space">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="1" target="Flat inter-pod network">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="1" target="NAT (Network Address Translation)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="1" target="Worker nodes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="1" target="LAN (Local Area Network)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="1" target="Software-defined network">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="1" target="VM (Virtual Machine)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="1" target="App">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="1" target="Container 1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="1" target="Container 2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="3" target="Log rotator">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="3" target="Collector">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="3" target="Data processor">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="3" target="Communication adapter">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="3" target="Frontend process">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="3" target="Backend process">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="3" target="Frontend container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="3" target="Backend container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="4" target="REST API">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="4" target="API object definitions">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="4" target="Kubernetes API reference documentation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="4" target="Pod metadata">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="4" target="Pod specification">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="dnsPolicy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="nodeName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="serviceAccount">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="terminationGracePeriodSeconds">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="containerID">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="lastState">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="ready">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="restartCount">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="state">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="hostIP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="phase">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="podIP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="startTime">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="Kubernetes API version">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="Spec">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="Pod specification/contents">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="5" target="Detailed status of the pod and its containers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="6" target="protocol">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="7" target="status">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="8" target="po kubia-manual">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="8" target="-o yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="8" target="-o json">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="8" target="process's standard output">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="8" target="standard error stream">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="9" target="$ docker logs &lt;container id&gt;">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="9" target="kubectl logs kubia-manual">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="9" target="Kubia server starting...">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="9" target="-c &lt;container name&gt;">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="9" target="$ kubectl logs kubia-manual -c kubia">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="9" target="container name">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="9" target="port forwarding">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="10" target="localhost:8888">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="10" target="8080">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="11" target="Resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="11" target="Microservice">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="11" target="UI Pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="12" target="canary release">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="12" target="stable release">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="12" target="beta release">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="12" target="kubia-manual-with-labels.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="12" target="ui pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="12" target="account service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="12" target="product catalog">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="12" target="shopping cart">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="12" target="order service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="13" target="--show-labels">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="13" target="-L">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="13" target="kubia-manual-v2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="14" target="manual">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="14" target="none">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="15" target="!env">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="15" target="creation_method!=manual">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="15" target="env in (prod,devel)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="15" target="env notin (prod,devel)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="15" target="app=pc">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="15" target="rel=beta">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="15" target="app=ui">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="15" target="rel=stable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="15" target="app=sc">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="15" target="rel=canary">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="16" target="selectors">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="16" target="SSDs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="16" target="spinning hard drives">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="16" target="GPU acceleration">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="16" target="node requirements">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="16" target="app: ui">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="16" target="app: as">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="17" target="GPU">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="17" target="gpu">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="18" target="kubernetes.io/hostname">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="18" target="chapter 16">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="19" target="kubernetes.io/created-by">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="19" target="kubectl annotate">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="19" target="mycompany.com/someannotation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="20" target="names">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="20" target="kube-public namespace">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="20" target="fluentd-cloud-kubia-e8fe-node-txje pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="20" target="heapster-v11-fz1ge pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="20" target="kube-dns-v9-p8a4t pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="20" target="kube-ui-v4-kdlai pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="20" target="l7-lb-controller-v0.5.2-bue96 pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="22" target="metadata section">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="22" target="--namespace flag">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="22" target="kubectl config commands">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="22" target="alias kcd">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="22" target="networking solution">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="22" target="inter-namespace network isolation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="23" target="creation_method=manual label">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="23" target="rel=canary label">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="24" target="ns">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="24" target="ui">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="24" target="stable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="24" target="as">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="24" target="pc">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="24" target="canary">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="25" target="kubia-09as0">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="25" target="kubia-something">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="25" target="kubia-http Service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="26" target="Label Selectors">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="26" target="Selectors">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="26" target="Annotations">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="26" target="kubectl explain command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="27" target="cluster node">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="27" target="Replication and other controllers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="27" target="horizontal scaling">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="27" target="system-level pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="27" target="batch jobs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="27" target="scheduling jobs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="28" target="OutOfMemoryErrors">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="28" target="Java app">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="29" target="TCP Socket">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="29" target="HTTP GET liveness probe">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="29" target="livenessProbe">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="30" target="httpGet liveness probe">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="30" target="RESTARTS column">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="30" target="kubia-liveness">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="30" target="--previous option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="30" target="myped">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="31" target="http-get">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="31" target="delay=0s">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="31" target="timeout=1s">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="31" target="period=10s">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="31" target="#success=1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="31" target="#failure=3">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="32" target="initialDelaySeconds">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="32" target="probe">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="32" target="exit code">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="32" target="health check">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="32" target="/health">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="33" target="Exec Probe">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="33" target="HTTP GET Liveness Probe">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="34" target="RC">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="34" target="pod B2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="35" target="Replica count">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="35" target="Pod selector">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="36" target="JSON or YAML descriptor">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="38" target="$ kubectl delete pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="38" target="ReplicationController spins up a new pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="38" target="$ kubectl get rc">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="38" target="Ready">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="39" target="api server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="39" target="containercreating">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="39" target="terminating">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="40" target="gcloud compute ssh command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="40" target="ifconfig eth0 down">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="40" target="kubectl get node command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="40" target="NotReady status">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="40" target="Unknown status">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="41" target="instances">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="41" target="ownerReferences">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="41" target="pod creation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="42" target="-L app">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="43" target="kubia-2qneh">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="43" target="kubia-dmdck">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="43" target="kubia-oini2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="43" target="kubia-k0xz6">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="44" target="rc kubia">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="44" target="YAML definition">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="44" target="chapter 9">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="45" target="scale">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="45" target="ReplicationController resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="45" target="text editor">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="45" target="KUBE_EDITOR">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="45" target="EDITOR">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="46" target="selector.app">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="46" target="ReplicationController's definition">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="46" target="desired state">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="46" target="horizontal pod auto-scaling">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="47" target="--cascade=false">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="48" target="pod selectors">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="48" target="metadata.labels">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="48" target="spec.containers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="48" target="containers.name">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="49" target="selector.matchLabels">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="49" target="Pods Status">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="49" target="apiGroup">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="49" target="v1beta2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="49" target="core API group">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="50" target="In">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="50" target="NotIn">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="50" target="Exists">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="50" target="DoesNotExist">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="53" target="ssd-monitor">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="53" target="ssd">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="53" target="luksa/ssd-monitor">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="54" target="ssd-monitor-daemonset.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="54" target="disk=ssd">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="54" target="disk=hdd">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="56" target="batch API group">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="56" target="v1 API version">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="56" target="OnFailure">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="56" target="luksa/batch-job">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="57" target="pod spec property">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="57" target="Job pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="57" target="restart policy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="57" target="kubectl create command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="57" target="pod list">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="57" target="--show-all switch">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="57" target="completions property">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="57" target="parallelism property">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="58" target="completions">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="58" target="parallelism">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="59" target="kubectl scale command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="59" target="spec.backoffLimit field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="59" target="cron format">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="59" target="Job template">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="59" target="pod replicas">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="60" target="jobTemplate">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="60" target="minute">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="60" target="hour">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="60" target="day of month">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="60" target="month">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="60" target="day of week">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="61" target="startingDeadlineSeconds">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="61" target="desired replica count">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="63" target="sysadmin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="63" target="external clients">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="63" target="external services">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="63" target="pod readiness">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="64" target="Kubernetes Service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="64" target="IP address and port">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="64" target="Frontend web server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="64" target="Backend database server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="64" target="Service for frontend pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="64" target="Service for backend pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="65" target="Node.js app">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="66" target="kubia-svc.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="67" target="get svc">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="67" target="service IP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="68" target="node.js">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="68" target="HTTP response">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="68" target="-s option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="68" target="--">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="69" target="sessionAffinity property">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="69" target="ClientIP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="69" target="Kubernetes services">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="69" target="TCP packets">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="69" target="HTTP protocol">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="69" target="cookies">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="69" target="multi-port service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="69" target="ports 80 and 443">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="69" target="pod's ports 8080 and 8443">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="70" target="https">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="71" target="env command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="71" target="Environment Variables">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="71" target="Port numbers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="71" target="IP addresses">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="71" target="Service spec">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="72" target="KUBIA_SERVICE_HOST">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="72" target="KUBIA_SERVICE_PORT">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="72" target="BACKEND_DATABASE_SERVICE_HOST">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="72" target="BACKEND_DATABASE_SERVICE_PORT">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="72" target="DNS server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="73" target="backend-database">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="73" target="svc.cluster.local">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="73" target="client">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="73" target="http://kubia.default.svc.cluster.local">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="73" target="http://kubia.default">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="74" target="http://kubia">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="74" target="kubia-8awf3">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="74" target="service's name">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="74" target="/etc/resolv.conf">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="74" target="DNS resolver">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="74" target="kubia.default.svc.cluster.local">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="74" target="service endpoints">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="75" target="svc kubia">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="75" target="80/TCP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="75" target="kubectl get endpoints kubia">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="75" target="NAME ENDPOINTS AGE">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="75" target="10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="75" target="external-service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="76" target="endpoints resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="76" target="yaml manifest">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="76" target="subsets">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="76" target="addresses">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="76" target="ip">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="76" target="connections">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="76" target="service name">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="76" target="endpoints IP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="77" target="ExternalName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="77" target="externalName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="77" target="DNS level">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="78" target="External client">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="79" target="nodePort">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="79" target="svc">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="79" target="CLUSTER-IP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="79" target="&lt;nodes&gt;">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="79" target="80:30123/TCP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="79" target="10.111.254.223">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="79" target="&lt;1st node's IP&gt;">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="79" target="30123">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="80" target="Firewall Rules">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="80" target="gcloud compute firewall-rules create">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="80" target="kubia-svc-rule">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="80" target="tcp:30123">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="80" target="Port 30123">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="81" target="JSONPath">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="81" target="ExternalIP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="82" target="APIVersion">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="82" target="Port">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="82" target="TargetPort">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="83" target="kubectl explain">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="83" target="keep-alive connections">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="83" target="session affinity">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="83" target="external client">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="83" target="IP: 130.211.53.173:80">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="83" target="IP: 130.211.99.206">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="83" target="IP: 130.211.97.55">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="83" target="Port 32143">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="84" target="externalTrafficPolicy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="84" target="Local">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="85" target="client's IP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="85" target="Source Network Address Translation (SNAT)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="85" target="Local external traffic policy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="85" target="LoadBalancer service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="85" target="public IP address">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="85" target="host and path in the request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="85" target="Service using Local external traffic policy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="86" target="Ingress add-on">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="86" target="Kube-dns">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="87" target="NAMESPACE">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="87" target="default-http-backend-5wb0h">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="87" target="kube-addon-manager-minikube">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="87" target="kube-dns-v20-101vq">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="87" target="kubernetes-dashboard-jxd9l">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="87" target="nginx-ingress-controller-gdts0">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="88" target="get ingresses">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="88" target="ingresses">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="88" target="DNS servers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="88" target="/etc/hosts">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="88" target="End-points object">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="88" target="Host header">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="90" target="TLS">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="90" target="openssl">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="90" target="genrsa">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="90" target="req">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="90" target="x509">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="91" target="tls-secret">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="91" target="CertificateSigningRequest">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="91" target="csr">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="91" target="kubia-ingress-tls.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="91" target="tls">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="91" target="kubia.example.com">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="92" target="liveness probes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="93" target="GET / request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="93" target="TCP Socket probe">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="93" target="TCP connection">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="94" target="container specification">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="94" target="readiness probe definition">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="94" target="ls command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="94" target="exit code zero">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="94" target="file /var/ready">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="95" target="READY column">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="95" target="touch command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="95" target="kubia-loadbalancer">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="96" target="pod labels">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="97" target="DNS lookups">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="97" target="Cluster IP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="97" target="Service specification">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="97" target="clusterIP field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="98" target="dig binary">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="98" target="tutum/dnsutils container image">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="98" target="--generator=run-pod/v1 option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="98" target="DNS A records">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="98" target="kubia-headless.default.svc.cluster.local FQDN">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="98" target="kubectl get pods command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="99" target="headless services">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="99" target="DNS round-robin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="99" target="service.alpha.kubernetes.io/tolerate-unready-endpoints">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="99" target="publishNotReadyAddresses">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="99" target="Kubernetes version 1.9.0">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="100" target="port exposed by the service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="100" target="Kubernetes Service resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="100" target="ExternalName service type">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="101" target="pod container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="101" target="bash shell">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="101" target="kubernetes resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="101" target="pod IPs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="102" target="Volumes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="102" target="Disk storage">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="102" target="Persistent storage">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="103" target="/var/logs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="103" target="/var/html">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="103" target="web server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="103" target="agent">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="103" target="log rotator">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="Container: WebServer">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="Webserver">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="/htdocs/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="Container: ContentAgent">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="ContentAgent">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="/html/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="Container: LogRotator">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="LogRotator">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="Volume: publicHtml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="104" target="Volume: logVol">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="105" target="Linux">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="105" target="Web-Server container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="105" target="ContentAgent container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="105" target="LogRotator container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="105" target="publicHtml volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="105" target="logVol volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="105" target="awsElastic-BlockStore">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="106" target="quobyte">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="106" target="vsphere-Volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="106" target="photonPersistentDisk">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="106" target="scaleIO">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="106" target="UNIX fortune command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="107" target="fortune image">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="107" target="apt-get">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="107" target="fortune binary">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="107" target="disk storage">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="108" target="luksa/fortune">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="108" target="nginx:alpine">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="108" target="read-only">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="108" target="TCP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="108" target="fortune">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="109" target="tmpfs filesystem">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="110" target="repository">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="110" target="directory">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="111" target="Git">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="111" target="GitHub">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="111" target="sidecar container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="111" target="Git sync process">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="111" target="SSH protocol">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="113" target="database pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="113" target="fluentd-kubia-4ebc2f1e-9a3e pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="113" target="varlog volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="113" target="varlibdockercontainers volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="113" target="Minikube pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="113" target="CA certificates">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="113" target="kubectl get pod command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="113" target="kubectl describe po command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="114" target="hostPath volumes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="114" target="gcloud command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="114" target="pod volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="115" target="gce persistent disk">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="115" target="mongodb">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="115" target="gce persistent disk volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="115" target="/data/db">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="115" target="mongodb-pod-hostpath.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="115" target="gcepd.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="116" target="JSON document">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="116" target="find() command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="116" target="insert() command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="116" target="GCE persistent disk">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="117" target="mongodb://127.0.0.1:27017">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="117" target="ObjectId">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="117" target="azureFile">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="118" target="volumeId">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="118" target="NFS volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="118" target="fc">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="119" target="Persistent Volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="119" target="NFS export">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="119" target="PersistentVolumeClaim (PVC)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="119" target="PersistentVolume (PV)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="120" target="Cluster administrator">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="120" target="Application developer">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="120" target="PersistentVolumeReclaimPolicy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="120" target="ReadOnlyMany">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="122" target="PersistentVolumeClaim manifest">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="122" target="cluster Nodes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="122" target="Namespace A">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="122" target="User A">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="123" target="RWO">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="123" target="ROX">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="123" target="RWX">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="125" target="claims">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="126" target="GCE Persistent Disks">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="126" target="Delete">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="127" target="Retain policy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="127" target="Delete policy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="127" target="PersistentVolume provisioner">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="127" target="PersistentVolumeClaim 1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="127" target="Pod 2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="127" target="PersistentVolumeClaim 2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="127" target="Pod 3">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="128" target="Dynamic Provisioning">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="128" target="PersistentDisk (PD)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="128" target="GCE">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="128" target="PVC definition">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="128" target="storageclass-fast-gcepd.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="128" target="mongodb-pvc-dp.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="129" target="fast StorageClass">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="129" target="kubectl get pvc">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="129" target="gcloud compute disks list">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="129" target="kubernetes.io/gce-pd provisioner">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="130" target="storage class">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="130" target="Persistent disk">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="130" target="SSD">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="130" target="fast storage class">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="130" target="standard storage class">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="131" target="standard">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="131" target="storageclass.beta.kubernetes.io/is-default-class">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="131" target="pd-standard">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="131" target="100Mi">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="132" target="compute disks list">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="132" target="dynamic volume provisioner">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="134" target="command-line options">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="135" target="config file">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="135" target="Docker containers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="135" target="Kubernetes resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="136" target="docker exec">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="136" target="ps x">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="136" target="docker exec -it">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="ENTRYPOINT instruction">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="CMD instruction">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="apt-get update">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="apt-get -y install fortune">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="interval">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="PID 1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="PID 7">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="shell">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="exec form">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="docker build">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="137" target="docker push">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="138" target="Control+C">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="138" target="docker.io/luksa/fortune:args">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="138" target="/bin/command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="138" target="arg1, arg2, arg3">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="138" target="fortune-pod-args.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="139" target="fortune:args">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="139" target="fortune:latest">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="139" target="FOO=BAR">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="139" target="ABC=123">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="139" target="Container A">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="139" target="Container B">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="140" target="trap">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="140" target="SIGINT">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="140" target="mkdir">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="140" target="while">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="140" target="/usr/games/fortune">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="140" target="System.getenv">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="140" target="process.env">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="140" target="os.environ">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="140" target="container definition">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="141" target="ConfigMap resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="141" target="$(VAR) syntax">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="141" target="FIRST_VAR">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="141" target="SECOND_VAR">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="142" target="Kubernetes REST API endpoint">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="142" target="configMap volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="142" target="key1=value1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="142" target="key2=value2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="142" target="app-config">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="142" target="development values">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="142" target="production values">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="143" target="create configmap command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="143" target="ConfigMap keys">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="143" target="DNS subdomain">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="143" target="literal">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="143" target="ConfigMap name">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="144" target="$ kubectl create -f fortune-config.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="144" target="--from-file=config-file.conf">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="144" target="--from-literal=some=thing">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="144" target="/path/to/dir">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="144" target="bar=foobar.conf">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="144" target="config-opts/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="144" target="some=thing">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="145" target="my-conﬁg">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="145" target="abc">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="145" target="true">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="145" target="repeat">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="145" target="100">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="145" target="some thing">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="145" target="foobar.conf">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="146" target="ConfigMapKeyRef">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="147" target="envFrom">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="147" target="prefix">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="147" target="CONFIG_FOO">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="147" target="CONFIG_BAR">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="147" target="FOO-BAR">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="147" target="my-config-map">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="147" target="some-image">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="148" target="configMapVolume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="148" target="luksa/fortune:args">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="149" target="Nginx web server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="149" target="fortuneloop.sh script">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="149" target="Config file">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="149" target="server_name">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="149" target="listen directive">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="149" target="gzip_types">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="149" target="location directive">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="149" target="root directive">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="149" target="index directive">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="149" target="configmap-files">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="150" target="ConfigMap's entries">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="150" target="/etc/nginx/nginx.conf">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="150" target="/etc/nginx/conf.d/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="151" target="port-forwarding">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="152" target="fortune-configmap-volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="152" target="items attribute">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="152" target="gzip.conf">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="153" target="gzip.conf file">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="153" target="subPath property">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="153" target="/etc directory">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="153" target="myconfig.conf file">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="154" target="subPath">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="154" target="file permissions">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="155" target="fortune-config Config-Map">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="155" target="Nginx config file">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="155" target="nginx -s reload">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="155" target="config file changes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="155" target="symbolic links">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="156" target="files">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="156" target="directories">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="156" target="symlinks">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="156" target="ConfigMap-backed volumes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="156" target="processes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="156" target="encryption keys">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="156" target="credentials">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="157" target="files in a volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="158" target="default-token-cfee9">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="158" target="automountServiceAccountToken">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="159" target="create secret">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="159" target="ConfigMaps and Secrets">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="160" target="create secret tls">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="160" target="Base64 encoding">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="160" target="binary values">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="160" target="1MB">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="161" target="stringData field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="161" target="Base64-encoded">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="161" target="fortune-https Secret">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="161" target="fortune-config ConfigMap">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="ssl_certificate">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="certs/https.cert">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="ssl_certificate_key">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="certs/https.key">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="ssl_protocols">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="TLSv1 TLSv1.1 TLSv1.2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="ssl_ciphers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="HIGH:!aNULL:!MD5">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="location /">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="index.html index.htm">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="/etc/nginx/certs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="apiVersion: v1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="kind: Pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="name: fortune-https">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="image: luksa/fortune:env">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="readOnly: true">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="162" target="containerPort: 80">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="163" target="items">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="163" target="mount">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="164" target="TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="164" target="tmpfs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="164" target="Secret volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="164" target="FOO_SECRET">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="165" target="secretKeyRef">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="165" target="image registries">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="165" target="private image registry">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="165" target="entry">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="166" target="docker-registry">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="166" target=".dockercfg">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="166" target="username/private:tag">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="167" target="docker-registry Secret">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="167" target="token Secret">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="168" target="Kubernetes client libraries">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="169" target="Pod manifest">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="169" target="DownwardAPI volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="170" target="memory requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="170" target="CPU limits">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="170" target="memory limits">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="171" target="status.podIP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="171" target="spec.nodeName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="171" target="divisor">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="172" target="CPU limits and requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="172" target="Memory limits/requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="172" target="POD_NAME">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="174" target="/etc/downward/labels">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="174" target="/etc/downward/annotations">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="174" target="/podName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="174" target="/podNamespace">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="175" target="/etc/downward/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="175" target="podName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="175" target="podNamespace">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="175" target="labels and annotations">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="175" target="key=value format">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="175" target="&#10;">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="176" target="container-level metadata">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="176" target="containerName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="177" target="REST endpoints">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="177" target="kubectl cluster-info">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="177" target="app process">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="178" target="proxy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="178" target="authorization token">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="178" target="/api/v1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="178" target="/apis/apps">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="178" target="/apis/apps/v1beta1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="178" target="/apis/batch">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="178" target="/apis/batch/v1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="178" target="/apis/batch/v2alpha1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="179" target="API groups">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="179" target="Batch API group">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="179" target="APIResourceList">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="179" target="APIResource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="179" target="v2alpha1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="180" target="update">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="180" target="jobs/status">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="180" target="JobList">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="181" target="REST API server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="181" target="my-job">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="182" target="tutum/curl image">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="183" target="port 443">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="183" target="SSL certificate">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="183" target="-k option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="183" target="default-token-xyz">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="183" target="--cacert option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="184" target="CURL_CA_BUNDLE">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="184" target="/var/run/secrets/kubernetes.io/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="184" target="serviceaccount/ca.crt">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="184" target="TOKEN environment variable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="184" target="/ui/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="185" target="Authorization HTTP header">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="185" target="NS environment variable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="185" target="TOKEN">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="185" target="PUT or PATCH requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="185" target="system:serviceaccounts">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="API server's certificate">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="Certificate Authority (CA)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="ca.crt file">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="Bearer token">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="Namespace file">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="CRUD (Create, Read, Update, Delete)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="POST method">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="GET method">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="PATCH/PUT method">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="DELETE method">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="Ambassador containers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="Server certificate">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="186" target="Default token secret volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="187" target="kubectl proxy command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="187" target="ambassador container pattern">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="187" target="kubectl-proxy container image">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="187" target="HTTP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="188" target="API server proxy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="188" target="port 8001">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="189" target="Kubernetes API client libraries">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="189" target="Golang client">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="189" target="Python client">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="189" target="Java client by Fabric8">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="189" target="Java client by Amdatu">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="189" target="Node.js client by tenxcloud">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="189" target="Node.js client by GoDaddy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="189" target="PHP client">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="189" target="Another PHP client">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="Ruby">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="https://github.com/Ch00k/kubr">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="Another Ruby client">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="https://github.com/abonas/kubeclient">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="Clojure">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="https://github.com/yanatan16/clj-kubernetes-api">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="Scala">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="https://github.com/doriordan/skuber">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="Perl">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="https://metacpan.org/pod/Net::Kubernetes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="Fabric8 Java Client">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="io.fabric8.kubernetes.api.model.Pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="io.fabric8.kubernetes.api.model.PodList">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="DefaultKubernetesClient">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="KubernetesClient">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="inNamespace">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="getItems">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="stream">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="forEach">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="getMetadata">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="getName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="190" target="createNew">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="191" target="addToLabels">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="191" target="endMetadata">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="191" target="done">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="191" target="System.out.println">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="191" target="Thread.sleep">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="191" target="client.pods().inNamespace">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="191" target="Swagger API">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="191" target="OpenAPI spec">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="191" target="Swagger UI">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="192" target="CPU requests and limits">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="192" target="memory requests and limits">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="192" target="client libraries">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="193" target="config data">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="193" target="microservices">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="193" target="Deployment resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="193" target="rolling updates">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="193" target="rollouts">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="193" target="reverting">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="195" target="Downtime">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="196" target="set selector">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="197" target="NodeJS">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="199" target="get svc kubia">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="199" target="cluster-ip">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="199" target="external-ip">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="199" target="IfNotPresent">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="199" target="response.end()">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="200" target="kubia-v2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="203" target="--v option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="204" target="Kubernetes master">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="204" target="verbose logging option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="205" target="kubia-deployment-v1.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="206" target="kubia Service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="206" target="--record">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="206" target="kubectl get deployment">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="206" target="kubectl rollout status deployment kubia">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="206" target="kubia-1506449474-otnnh">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="206" target="kubia-1506449474-vmn7s">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="206" target="kubia-1506449474-xis6m">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="207" target="Recreate strategy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="207" target="RollingUpdate strategy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="208" target="kubectl patch command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="208" target="minReadySeconds attribute">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="208" target="Deployment spec">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="208" target="kubectl set image command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="208" target="nodejs container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="209" target="Image registry">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="209" target="kubectl patch">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="209" target="kubectl replace">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="209" target="kubia-deployment-v2.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="210" target="v1 pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="210" target="v2 pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="210" target="ReplicaSet v1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="210" target="ReplicaSet v2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="211" target="requestCount">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="211" target="500 error">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="212" target="kubia-1914148340-lalmx">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="212" target="internal error">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="212" target="kubectl rollout undo">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="213" target="rollout history">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="213" target="--record command-line option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="213" target="CHANGE-CAUSE column">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="213" target="revisionHistoryLimit property">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="213" target="ReplicaSet list">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="213" target="revision history">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="213" target="--to-revision option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="213" target="undo command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="214" target="revisionHistoryLimit">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="214" target="apps/v1beta2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="215" target="v1beta1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="215" target="extensions">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="215" target="Desired replica count">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="217" target="resume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="218" target="kubectl apply command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="219" target="readinessProbe">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="219" target="kubia-deployment-v3-with-readinesscheck.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="219" target="periodSeconds">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="221" target="ProgressDeadlineExceeded">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="221" target="progressDeadlineSeconds">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="221" target="kubectl rollout undo deployment">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="223" target="DNS SRV records">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="224" target="Storage volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="225" target="ReplicaSet A1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="225" target="Pod A1-xyz">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="225" target="PVC A2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="225" target="PV A2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="225" target="ReplicaSet A2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="225" target="Pod A2-xzy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="225" target="PVC A3">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="225" target="PV A3">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="225" target="ReplicaSet A3">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="225" target="Pod A3-zyx">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="227" target="Stateful pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="227" target="Pets vs. Cattle analogy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="227" target="Stateless apps">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="227" target="Stateful apps">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="228" target="headless Service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="228" target="DNS entry">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="229" target="foo.default.svc.cluster.local">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="229" target="a-0.foo.default.svc.cluster.local">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="230" target="StatefulSets scale down">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="230" target="distributed data store">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="230" target="data entry">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="231" target="PersistentVolume-Claims">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="231" target="volume claim templates">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="231" target="replicas field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="231" target="PVC A-0">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="231" target="PVC A-1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="231" target="Pod A-1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="231" target="PVC A-2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="231" target="Pod A-2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="232" target="scale-up">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="233" target="Data store">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="233" target="Kubia app">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="233" target="fs.createWriteStream()">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="233" target="request.pipe()">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="233" target="fs.readFileSync()">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="233" target="fileExists()">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="234" target="node:7">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="234" target="http.createServer(handler)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="234" target="listen(8080)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="234" target="gcloud compute disks create">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="235" target="List">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="235" target="NFS (Network File System)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="235" target="pv-a, pv-b, and pv-c">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="235" target="1 Mi">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="236" target="volumeClaimTemplates">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="237" target="$ kubectl create -f kubia-statefulset.yaml ">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="237" target="kubia-statefulset.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="237" target="$ kubectl get po kubia-0 -o yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="238" target="/var/data">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="238" target="default-token-r2m41">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="238" target="get pvc">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="238" target="data-kubia-0">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="238" target="pv-c">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="238" target="pv-a">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="238" target="/api/v1/namespaces/default/pods/kubia-0/proxy/&lt;path&gt;">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="239" target="kubia-0 pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="239" target="API server host and port">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="239" target="-L option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="239" target="kubectl proxy and API server proxy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="240" target="localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="240" target="DELETING A STATEFUL POD">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="240" target="reschedule">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="241" target="persistent data">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="241" target="ordinal number">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="242" target="ClusterIP Service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="242" target="StatefulSet peer discovery">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="243" target="SRV record">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="243" target="A record">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="243" target="MX record">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="243" target="dig DNS lookup tool">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="243" target="tutum/dnsutils image">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="243" target="dns.resolveSrv() function">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="244" target="Stone Age data store">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="244" target="kubia-public Service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="244" target="dns.resolveSrv">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="244" target="fs.readFileSync">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="244" target="os.hostname">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="244" target="request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="244" target="response">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="244" target="fileExists">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="245" target="addresses.forEach">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="245" target="requestOptions">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="245" target="returnedData">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="245" target="numResponses">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="245" target="response.write">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="245" target="docker.io/luksa/kubia-pet-peers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="245" target="GET /data">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="246" target="spec.template.spec.containers.image">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="246" target="StatefulSet updateStrategy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="246" target="POST">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="246" target="localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="246" target="pod kubia-0">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="246" target="pod kubia-1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="247" target="/kubia-public/proxy/">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="247" target="kubia-0.kubia.default.svc.cluster.local">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="247" target="kubia-1.kubia.default.svc.cluster.local">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="247" target="kubia-2.kubia.default.svc.cluster.local">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="248" target="node failures">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="248" target="$ sudo ifconfig eth0 down">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="248" target="NotReady">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="248" target="$ kubectl get node">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="248" target="Unknown">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="248" target="evicted">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="249" target="gke-kubia-default-pool-32a2cac8-m0g1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="249" target="NodeLost">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="249" target="Terminating">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="249" target="pod &quot;kubia-0&quot; deleted">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="250" target="--force">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="250" target="--grace-period">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="250" target="GCE web console">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="251" target="host names">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="252" target="Deployment object">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="252" target="Running pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="252" target="Network between pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="252" target="Kubernetes Services">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="252" target="High-availability">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="253" target="etcd distributed persistent storage">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="253" target="Kubernetes Service Proxy (kube-proxy)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="253" target="Container Runtime">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="253" target="Kubernetes DNS server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="253" target="Container Network Interface network plugin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="254" target="attach">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="254" target="componentstatuses">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="254" target="Worker node(s)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="256" target="etcd instance">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="256" target="optimistic locking system">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="256" target="metadata.resourceVersion field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="256" target="/registry">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="256" target="daemonsets">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="256" target="deployments">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="257" target="/registry/pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="257" target="/registry/pods/default">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="257" target="kubia-159041347-wt6ga">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="258" target="RAFT consensus algorithm">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="258" target="Control Plane components">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="258" target="quorum">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="258" target="state changes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="259" target="RESTful API">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="259" target="majority">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="259" target="CRUD interface">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="259" target="optimistic locking">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="259" target="resource validation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="259" target="HTTP POST request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="260" target="Authentication Plugins">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="260" target="HTTP Request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="260" target="Client Certificate">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="260" target="HTTP Header">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="260" target="Authorization Plugins">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="260" target="Admission Control Plugins">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="260" target="AlwaysPullImages">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="261" target="Admission Control plugins">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="261" target="kubectl tool">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="261" target="HTTP connection">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="261" target="stream of modifications">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="261" target="watched objects">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="261" target="--watch flag">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="261" target="GET /.../pods?watch=true">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="261" target="POST /.../pods/pod-xyz">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="261" target="Send updated object to all watchers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="262" target="kubia-159041347-14j3i">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="262" target="watch mechanism">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="262" target="kubectl get pods -o yaml --watch">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="262" target="round-robin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="263" target="Predicate functions">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="263" target="Hardware resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="263" target="Memory pressure condition">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="263" target="Disk pressure condition">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="263" target="Node selector">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="263" target="Host port">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="263" target="Taints and tolerations">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="263" target="Affinity or anti-affinity rules">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="264" target="ReplicationManager">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="265" target="Watch mechanism">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="265" target="Re-list operation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="265" target="Constructor">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="266" target="Pod resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="266" target="syncHandler">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="266" target="ReplicationController resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="266" target="Worker() method">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="268" target="EndPoints controller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="269" target="Kubernetes Service Proxies">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="269" target="rkt">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="269" target="Kubelet manifest directory">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="269" target="pod manifests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="270" target="Kubernetes Service Proxy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="270" target="Worker node">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="271" target="userspace proxy mode">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="271" target="in kernel space">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="271" target="round-robin fashion">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="271" target="Kubernetes add-ons">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="271" target="dashboard add-ons">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="272" target="default-http-backend">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="272" target="kubernetes-dashboard">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="272" target="nginx-ingress-controller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="272" target="DNS add-on">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="272" target="cluster's internal DNS server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="272" target="kube-dns service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="272" target="nameserver">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="272" target="API server's watch mechanism">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="272" target="Services and Endpoints">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="272" target="reverse proxy server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="274" target="ReplicaSet A">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="274" target="Node X">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="275" target="ReplicaSet Controller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="275" target="Event">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="276" target="KIND">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="276" target="REASON">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="276" target="SOURCE">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="276" target="deployment-controller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="276" target="replicaset-controller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="276" target="default-scheduler">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="277" target="docker">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="277" target="minikubeVM">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="277" target="CONTAINER ID">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="277" target="IMAGE">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="277" target="COMMAND">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="277" target="pod infrastructure container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="278" target="Inter-pod networking">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="278" target="NAT">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="278" target="IP: 10.1.1.1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="278" target="srcIP: 10.1.1.1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="278" target="dstIP: 10.1.2.1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="279" target="Pod X">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="279" target="Pod Y">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="279" target="NAT-less communication">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="279" target="pause container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="279" target="node-to-pod communication">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="279" target="pod-to-node communication">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="279" target="outbound packets">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="280" target="interface">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="280" target="overlay networks">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="280" target="underlay networks">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="280" target="layer 3 routing">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="280" target="routing tables">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="280" target="pod C">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="281" target="physical adapter">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="281" target="Software Defined Network (SDN)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="281" target="Container Network Interface (CNI)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="281" target="Calico">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="281" target="Romana">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="281" target="Weave Net">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="282" target="Endpoints objects">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="282" target="network interfaces">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="282" target="kube-proxy agents">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="283" target="kernel">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="283" target="node A">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="283" target="node B">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="283" target="Pod B1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="283" target="Pod B2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="283" target="Pod B3">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="283" target="Packet X">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="283" target="Service B">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="283" target="Endpoints B">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="284" target="Leader-election mechanism">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="284" target="Kubernetes Control Plane components">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="285" target="Load balancer">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="285" target="Master node">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="286" target="--leader-elect option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="287" target="leader election mechanism">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="287" target="kube-scheduler">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="287" target="get endpoints">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="287" target="leaderTransitions">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="287" target="leaseDurationSeconds">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="288" target="Infrastructure Container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="288" target="Network Bridge">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="289" target="ServiceAccount token">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="289" target="default roles and bindings">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="290" target="API server core">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="290" target="client certificate">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="290" target="authentication token">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="290" target="Basic HTTP authentication">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="290" target="service accounts">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="290" target="ServiceAccount resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="291" target="system:unauthenticated group">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="291" target="system:authenticated group">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="291" target="system:serviceaccounts group">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="291" target="ServiceAccount usernames">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="291" target="ServiceAccounts resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="291" target="$ kubectl get sa">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="292" target="cluster administrator">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="293" target="sa">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="293" target="foo-token-qzq7j">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="293" target="JSON Web Tokens (JWT)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="293" target="Mountable secrets">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="294" target="image pull Secrets">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="294" target="mountable Secrets">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="295" target="foo ServiceAccount">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="295" target="/var/run/secrets/kubernetes.io/serviceaccount/token">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="295" target="localhost:8001/api/v1/pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="296" target="Role-Based Access Control (RBAC)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="296" target="RBAC authorization plugin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="296" target="Attribute-Based Access Control (ABAC)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="296" target="Web-Hook plugin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="296" target="custom plugin implementations">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="296" target="REST resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="296" target="PUT request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="296" target="DELETE request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="297" target="roles">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="299" target="clusterrole">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="301" target="Role resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="302" target="YAML file">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="302" target="cluster-admin rights">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="302" target="--verb=get">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="302" target="--verb=list">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="302" target="--resource=services">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="302" target="--namespace=foo">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="302" target="--clusterrolebinding">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="303" target="ServiceList">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="303" target="localhost:8001/api/v1/namespaces/foo/services">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="304" target="rolebinding">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="304" target="role">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="304" target="test">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="305" target="Service-Accounts">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="306" target="persistentvolumes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="307" target="PersistentVolumeList">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="308" target="/api/*">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="308" target="/apis/*">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="308" target="/healthz">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="308" target="/swaggerapi">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="308" target="/swaggerapi/*">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="308" target="/version">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="308" target="get clusterrole system:discovery -o yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="309" target="system:discovery">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="309" target="system:authenticated">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="309" target="system:unauthenticated">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="309" target="authentication plugin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="310" target="APIVersions">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="310" target="view">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="310" target="replicationcontrollers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="311" target="/api/v1/pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="311" target="/api/v1/namespaces/foo/pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="313" target="localhost:8001/api/v1/namespaces/bar/pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="313" target="system:serviceaccount:foo:default">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="314" target="system:auth-delegator">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="314" target="system:basic-user">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="314" target="system:controller:attachdetach-controller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="315" target="view ClusterRole">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="315" target="edit ClusterRole">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="315" target="admin ClusterRole">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="315" target="cluster-admin ClusterRole">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="315" target="ResourceQuotas">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="316" target="ClusterAdmin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="316" target="ServiceAccountName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="316" target="ServiceAccountToken">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="318" target="policies">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="319" target="IP and port space">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="319" target="Inter-Process Communication (IPC)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="319" target="hostNetwork property">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="319" target="node's network adapters">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="319" target="virtual network adapters">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="319" target="eth1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="320" target="ifconfig">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="320" target="veth1178d4f">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="322" target="port 9000">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="322" target="firewall-rules">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="322" target="pod-with-host-pid-and-ipc.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="322" target="kubia-hostport.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="323" target="ps aux">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="323" target="Inter-Process Communication">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="323" target="user ID">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="323" target="privileged mode">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="323" target="SELinux">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="323" target="hostPID: true">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="324" target="bin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="324" target="daemon">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="324" target="sys">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="324" target="adm">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="324" target="wheel">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="324" target="floppy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="324" target="dialout">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="324" target="tape">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="324" target="video">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="325" target="runAsNonRoot">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="326" target="/dev">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="326" target="device files">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="326" target="/dev/null">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="326" target="/dev/zero">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="326" target="/dev/random">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="btrfs-control">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="stderr">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="tty48">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="core">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="stdin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="tty49">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="stdout">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="tty5">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="cpu_dma_latency">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="termination-log">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="tty50">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="fd">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="tty">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="tty51">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="full">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="tty0">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="fuse">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="tty1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="hpet">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="tty10">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="hwrng">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="tty11">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="privileged container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="327" target="CAP_SYS_TIME">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="328" target="Linux kernel capabilities">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="328" target="CAP_">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="328" target="pod-add-settime-capability">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="328" target="/tmp">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="329" target="securityContext.capabilities.drop">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="329" target="chown">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="329" target="securityContext.readOnlyRootFilesystem">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="329" target="Pod-with-readonly-filesystem">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="330" target="touch">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="331" target="EmptyDir">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="331" target="Security Context">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="331" target="fsGroup ID">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="331" target="Supplemental Group IDs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="332" target="id command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="332" target="privileged pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="332" target="/volume/foo">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="332" target="/tmp/foo">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="332" target="fsGroup security context property">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="333" target="host ports">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="333" target="user IDs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="333" target="PodSecurityPolicy admission control plugin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="333" target="apiserver.GenericServerRunOptions.AdmissionControl">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="333" target="LimitRanger">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="333" target="PersistentVolumeLabel">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="333" target="DefaultStorageClass">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="333" target="DefaultTolerationSeconds">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="333" target="BasicAuthFile">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="333" target="password file">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="334" target="hostPorts">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="335" target="MustRunAs rule">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="335" target="ranges">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="335" target="2-10">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="335" target="20-30">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="336" target="USER directive">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="337" target="MustRunAsNonRoot">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="337" target="allowedCapabilities">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="337" target="defaultAddCapabilities">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="337" target="SYS_ADMIN">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="337" target="SYS_MODULE">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="338" target="security-related features">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="338" target="CAPABILITIES">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="338" target="PodSecurity-Policy Admission Control plugin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="338" target="pod-add-sysadmin-capability.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="338" target="persistentVolume-Claim">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="339" target="PodSecurityPolicy Admission Control plugin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="339" target="PodSecurityPolicy (psp)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="340" target="default policy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="340" target="privileged policy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="340" target="Alice">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="340" target="Bob">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="340" target="PodSecurityPolicy resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="340" target="psp-default">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="340" target="psp-all-users">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="341" target="bob">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="341" target="alice">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="341" target="--user">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="341" target="podSecurityPolicy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="341" target="context">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="341" target="appendix A">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="342" target="Admission Control">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="342" target="ingress rules">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="342" target="egress rules">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="342" target="namespace selector">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="342" target="default-deny NetworkPolicy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="343" target="5432">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="344" target="Kubernetes namespaces">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="344" target="tenant">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="344" target="manning">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="344" target="shopping-cart">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="344" target="microservice">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="344" target="shopping-cart-netpolicy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="345" target="ingress rule">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="345" target="shopping-cart microservice">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="346" target="egress">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="346" target="to">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="346" target="devices">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="348" target="if=/dev/zero">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="348" target="of=/dev/null">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="349" target="Mem">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="349" target="top">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="349" target="Minikube VM">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="350" target="LeastRequestedPriority">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="350" target="MostRequestedPriority">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="351" target="Capacity">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="352" target="PodScheduled">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="352" target="FailedScheduling">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="352" target="Insufficient cpu">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="352" target="cores">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="353" target="dflt-http-b...">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="353" target="kube-addon-...">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="353" target="kube-dns-26...">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="353" target="kubernetes-...">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="353" target="nginx-ingre...">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="353" target="CPU Requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="353" target="CPU Limits">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="353" target="Memory Requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="353" target="Memory Limits">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="354" target="custom resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="354" target="Extended Resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="354" target="Opaque Integer Resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="354" target="CPU time sharing">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="355" target="PATCH HTTP request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="355" target="capacity field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="355" target="example.org/my-resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="355" target="quantity">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="355" target="resources.requests field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="355" target="GPU units">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="355" target="resources.requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="356" target="overcommitting">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="357" target="container restart policy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="358" target="CPU limit">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="358" target="top command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="358" target="dd command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="359" target="Java Virtual Machine">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="359" target="-Xmx option">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="359" target="OOMKill">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="359" target="cgroups system">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="359" target="/sys/fs/cgroup/cpu/cpu.cfs_quota_us">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="359" target="/sys/fs/cgroup/cpu/cpu.cfs_period_us">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="360" target="BestEffort class">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="360" target="Burstable class">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="360" target="Guaranteed class">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="362" target="YAML/JSON manifest">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="362" target="status.qosClass field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="362" target="Table 14.1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="362" target="Table 14.2">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="364" target="Validation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="364" target="Namespace XYZ">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="365" target="LimitRanger Admission Control plugin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="365" target="LimitRange object">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="367" target="cpu usage">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="367" target="memory usage">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="368" target="LimitRanger plugin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="368" target="ResourceQuota Admission Control plugin">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="368" target="hard">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="369" target="kubectl describe command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="369" target="cpu-and-mem">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="369" target="limits.cpu 200m">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="369" target="limits.memory 100Mi">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="369" target="requests.cpu 100m">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="369" target="requests.memory 10Mi">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="370" target="Storage">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="371" target="loadbalancers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="371" target="nodeports">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="372" target="quota scopes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="372" target="BestEffort scope">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="372" target="NotBestEffort scope">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="372" target="Terminating scope">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="372" target="NotTerminating scope">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="372" target="CPU/memory requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="372" target="CPU/memory limits">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="374" target="kubectl top command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="374" target="minikube addons enable heapster command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="374" target="kubectl top node command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="374" target="kubectl top pod command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="375" target="top_pod.go">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="375" target="Google Cloud Monitoring">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="375" target="resource consumption statistics">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="376" target="cluster-info">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="377" target="network">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="378" target="Resource limits">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="378" target="Memory usage">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="378" target="QoS classes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="378" target="CPU request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="378" target="Memory request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="380" target="Replicas field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="380" target="custom metrics">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="381" target="autoscaling">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="381" target="REST calls">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="382" target="Pod metrics">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="382" target="Resource metrics API">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="382" target="Metrics collector">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="383" target="Autoscaling operation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="383" target="desired replica count field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="383" target="Autoscaler controller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="383" target="Scale sub-resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="383" target="Target CPU utilization">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="383" target="Target QPS">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="385" target="replicas: 3">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="385" target="metadata: name: kubia">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="385" target="labels: app: kubia">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="385" target="image: luksa/kubia:v1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="385" target="name: nodejs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="386" target="kubectl autoscale command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="387" target="autoscaling/v2beta1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="387" target="Metrics">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="387" target="Target">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="387" target="MinPods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="387" target="MaxPods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="389" target="-it">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="389" target="--rm">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="389" target="--restart=Never">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="390" target="targetAverageUtilization field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="390" target="maxReplicas parameter">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="390" target="metrics section">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="390" target="Resource type">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="390" target="autoscaler controller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="391" target="Horizontal Autoscaler">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="391" target="memory consumption">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="391" target="HPA (Horizontal Pod Autoscaler)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="392" target="metrics field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="392" target="Pods metric">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="392" target="Object metric">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="392" target="container's resource requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="392" target="Queries-Per-Second (QPS)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="392" target="message broker's queue">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="392" target="Ingress object">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="393" target="Object">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="393" target="latencyMillis">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="393" target="targetValue">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="393" target="frontend">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="394" target="Vertical pod autoscaling">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="394" target="InitialResources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="394" target="Tag">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="394" target="Historical resource usage data">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="394" target="Vertical scaling">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="395" target="Pod Autoscaler">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="395" target="Node group">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="395" target="API call">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="396" target="CPU and memory requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="396" target="system pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="396" target="unmanaged pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="396" target="pod with local storage">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="396" target="node group">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="397" target="Google Compute Engine (GCE)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="397" target="Amazon Web Services (AWS)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="397" target="Microsoft Azure">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="397" target="cordon">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="397" target="drain">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="397" target="uncordon">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="398" target="Pod-DisruptionBudget">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="398" target="Drain command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="398" target="MinAvailable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="398" target="MaxUnavailable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="399" target="minAvailable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="399" target="CTRL+C">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="400" target="node selector">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="400" target="node affinity rules">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="401" target="describe node">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="401" target="Control Plane pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="401" target="master node">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="402" target="node-role.kubernetes.io/master">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="402" target="Exists:NoExecute">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="403" target="Tolerations">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="403" target="Taints">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="403" target="kubectl taint command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="403" target="NoSchedule effect">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="403" target="pod YAML">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="405" target="toleration">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="405" target="$ kubectl get po prod-350605-1ph5h -o yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="405" target="node.alpha.kubernetes.io/unreachable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="405" target="tolerationSeconds">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="405" target="--feature-gates=Taint-BasedEvictions=true">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="405" target="Listing 16.6">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="406" target="Node Selectors">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="406" target="GKE (Google Kubernetes Engine)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="406" target="Node Pools">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="406" target="Region">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="406" target="Zone">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="407" target="requiredDuringSchedulingIgnoredDuringExecution">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="407" target="key: gpu">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="407" target="operator: In">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="407" target="values: - true">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="408" target="gpu label">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="408" target="availability zone">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="409" target="pref">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="409" target="preferred-deployment.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="410" target="zone1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="410" target="dedicated">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="411" target="-o wide">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="411" target="pref-607515-1rnwv">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="411" target="pref-607515-27wp0">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="411" target="pref-607515-5xd0z">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="411" target="pref-607515-jx9wt">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="411" target="pref-607515-mlgqm">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="411" target="Selector-SpreadPriority">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="411" target="frontend pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="411" target="pod affinity configuration">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="413" target="get po -o wide">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="413" target="frontend-podaffinity-host.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="413" target="podAffinity configuration">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="413" target="frontend pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="414" target="anti-affinity">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="414" target="backend-qhqj6">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="414" target="InterPodAffinityPriority">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="414" target="SelectorSpreadPriority">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="414" target="NodeAffinityPriority">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="414" target="failure-domain.beta.kubernetes.io/zone">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="414" target="failure-domain.beta.kubernetes.io/region">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="416" target="podAffinityTerm">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="417" target="backend-257820-ssrgj">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="417" target="frontend-941083-3mff9">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="417" target="frontend-941083-7fp7d">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="417" target="frontend-941083-cq23b">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="417" target="frontend-941083-m70sw">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="417" target="frontend-941083-wsjv8">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="417" target="podAntiAffinity">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="417" target="Topology key">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="418" target="Pod Affinity">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="418" target="Pod AntiAffinity">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="418" target="LabelSelector">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="418" target="TopologyKey">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="418" target="$ kubectl get po -l app=frontend -o wide">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="419" target="taint">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="419" target="affinity">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="419" target="hard requirement">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="420" target="Resources">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="420" target="Hooks">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="420" target="Init containers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="421" target="Volume(s)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="421" target="ReplicaSet(s)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="421" target="Health probes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="421" target="Volume mounts">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="421" target="Resource reqs/limits">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="421" target="Secret(s)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="421" target="Persistent Volume Claim">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="423" target="clustered app">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="423" target="OOMKiller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="423" target="pod-scoped volume">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="424" target="Writes to">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="424" target="Writable layer">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="424" target="Read-only layer">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="424" target="Image layers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="424" target="New container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="424" target="New process">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="424" target="Filesystem volumeMount">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="427" target="fortune-client pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="427" target="initContainers field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="427" target="while true loop">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="427" target="wget command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="428" target="init container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="428" target="fortune-server pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="428" target="lifecycle hooks">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="428" target="post-start hooks">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="428" target="pre-stop hooks">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="429" target="Post-Start Hook">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="429" target="Pending">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="429" target="postStart">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="429" target="sh">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="429" target="exit">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="430" target="PostStart Hook">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="430" target="FailedSync">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="430" target="PostStart handler">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="430" target="Get http://10.32.0.2:9090/postStart">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="430" target="dial tcp 10.32.0.2:9090">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="430" target="kubectl exec my-pod cat logfile.txt">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="431" target="scheme">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="431" target="httpHeaders">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="431" target="shell script">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="432" target="deletionTimestamp">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="433" target="spec.terminationGracePeriodSeconds">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="433" target="pod instances">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="434" target="data-migrating pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="435" target="Service Endpoints">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="435" target="HTTP GET">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="435" target="Scale down">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="436" target="client requests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="436" target="pod deletion notification">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="437" target="Iptables rules">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="437" target="Connection Refused error">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="437" target="REST request">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="437" target="Watcher(s)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="438" target="deletionTimestamp field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="438" target="kube-proxies">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="438" target="load balancers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="438" target="client-side load-balancing">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="438" target="termination signal">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="440" target="FROM scratch directive">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="440" target="dig">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="440" target="latest tag">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="441" target="tags">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="441" target="multi-dimensional labels">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="442" target="/dev/termination-log">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="442" target="Container definition">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="442" target="/var/termination-reason">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="442" target="Error">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="442" target="Exit Code">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="443" target="terminationMessagePolicy field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="443" target="FallbackToLogsOnError policy">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="443" target="standard output">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="443" target="kubernetes logs command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="443" target="cat command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="443" target="kubectl cp command">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="443" target="log file">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="443" target="termination message">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="443" target="application logging">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="444" target="logging">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="444" target="centralized logging">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="444" target="cluster-wide logging">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="444" target="ELK stack">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="444" target="EFK stack">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="445" target="Java">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="445" target="node-level FluentD agent">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="445" target="logging sidecar container">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="445" target="IDE">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="445" target="BACKEND_SERVICE_HOST">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="445" target="BACKEND_SERVICE_PORT">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="446" target="Docker volumes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="447" target="DOCKER_HOST">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="447" target="minikube docker-env">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="447" target="Version Control System">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="448" target="Kube-applier">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="448" target="Version Control System (VCS)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="448" target="Kubernetes resource manifests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="449" target="CI/CD">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="449" target="Fabric8 project">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="449" target="Jenkins">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="449" target="container lifecycle hooks">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="449" target="manifests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="450" target="image sizes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="450" target="Mini-kube">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="450" target="Platform-as-a-Service">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="451" target="Platform-as-a-Service (PaaS)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="451" target="API servers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="451" target="OpenShift Container Platform">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="451" target="Deis Workflow and Helm">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="452" target="CustomResourceDefinitions">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="452" target="Queue resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="452" target="Third-PartyResource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="452" target="Website resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="453" target="scope">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="453" target="Namespaced">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="453" target="website-crd.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="453" target="imaginary-kubia-website.yaml">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="454" target="Custom Resource Definition">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="454" target="API Group">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="454" target="Version">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="454" target="Website">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="454" target="apps/v1beta1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="454" target="extensions.example.com">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="455" target="websites">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="455" target="Website.v1.extensions.example.com">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="455" target="extensions.example.com/v1">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="455" target="website">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="457" target="nginx server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="457" target="git-sync process">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="458" target="DELETED watch event">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="458" target="Deployment resource">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="459" target="serviceAccountName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="459" target="luksa/website-controller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="459" target="Role Based Access Control (RBAC)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="459" target="kubia Website">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="459" target="website-controller">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="459" target="proxy sidecar">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="460" target="get deploy,svc,po">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="460" target="cluster-IP">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="460" target="validation schema">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="460" target="gitRepo field">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="461" target="CustomResourceValidation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="461" target="JSON schema">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="461" target="API server aggregation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="461" target="Custom API server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="461" target="Main API server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="462" target="etcd store">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="462" target="APIService">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="462" target="API group">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="462" target="custom API server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="462" target="GitHub repos">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="463" target="Client pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="463" target="Team">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="463" target="Ticket">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="463" target="Manifests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="464" target="ServiceBroker">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="464" target="Client pods">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="464" target="Provisioned services">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="464" target="Broker A and Broker B">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="465" target="Service Brokers">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="465" target="OpenServiceBroker API">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="465" target="GET /v2/catalog">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="465" target="PUT /v2/service_instances/:id">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="465" target="PATCH /v2/service_instances/:id">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="465" target="PUT /v2/service_instances/:id/service_bindings/:binding_id">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="465" target="DELETE /v2/service_instances/:id/service_bindings/:binding_id">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="465" target="DELETE /v2/service_instances/:id">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="465" target="url">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="465" target="service plans">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="466" target="bindable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="466" target="brokerName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="466" target="plans">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="466" target="planUpdatable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="466" target="osbFree">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="467" target="postgres-database">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="467" target="--data-checksums">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="467" target="get instance">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="467" target="clusterServiceClassName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="467" target="clusterServicePlanName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="468" target="Kubernetes Service Catalog">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="468" target="instanceRef">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="468" target="secretName">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="468" target="Database broker">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="468" target="PodPresets">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="468" target="PostgreSQL database">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="469" target="get secret">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="469" target="postgres-secret">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="469" target="username">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="469" target="password">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="469" target="servicebroker">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="469" target="servicebinding">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="470" target="broker">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="470" target="Amazon Web Services">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="470" target="Red Hat OpenShift">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="470" target="developer experience">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="470" target="scaling">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="470" target="maintenance">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="470" target="user management">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="470" target="group management">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="470" target="projects">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="471" target="Templates">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="471" target="BuildConfigs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="471" target="DeploymentConfigs">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="471" target="ImageStreams">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="471" target="Routes">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="471" target="Projects">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="471" target="Users">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="471" target="Groups">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="471" target="Roles-Based Access Control (RBAC)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="471" target="Templates (parameterizable)">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="472" target="OpenShift">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="472" target="Java EE application">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="472" target="Application Server">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="472" target="Source To Image">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="472" target="Maven">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="473" target="Builder pod">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="473" target="Build trigger">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="473" target="Git repo">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="473" target="Minishift">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="473" target="OpenShift Online Starter">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="474" target="git">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="474" target="Charts">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="474" target="Config">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="474" target="Linux system">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="475" target="PostgreSQL">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="475" target="MySQL">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="475" target="Helm chart">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="475" target="OpenVPN">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="475" target="helm CLI tool">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="476" target="Custom-ResourceDefinition">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="476" target="Resource manifests">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="476" target="API aggregation">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="477" target="kubeconfig">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="477" target="minikube start">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="477" target="gcloud container clusters get-credentials">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="478" target="multiple clusters">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="478" target="KUBECONFIG environment variable">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="478" target="clusters">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="478" target="contexts">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="478" target="current-context">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="478" target="certificate-authority">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="478" target="/home/luksa/.minikube/ca.crt">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="478" target="https://192.168.99.100:8443">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
<edge source="478" target="user">
  <data key="d7">contains</data>
  <data key="d6">1</data>
</edge>
</graph></graphml>