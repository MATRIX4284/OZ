{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python venv: search_agent_poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4375a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This module will take the serialized dictionary got out of PDF Parsing ann try to extract\n",
    "##Semantic Knowldge like: \n",
    "## 1.Identifying top highlights from each page text\n",
    "## 2.Convert these highlights pertaining to each page to a json object.\n",
    "## 3.Store these structered list of json that list of highlights per page into the main pdf dictionary\n",
    "\n",
    "#Reasons for highlight extraction:\n",
    "####This helps in adding semantic information from the page text and bring them our as highlights####  \n",
    "####This helps in separating diffrent semantic information from th text chunked where the information present###\n",
    "####In the embedding is ofeten an averaged out information and functions like cosine similarity is ###\n",
    "####calculated to identify the main chunks some chunks became top candidates or the closest candidates of###\n",
    "####Similarity functions.\n",
    "\n",
    "##Usecase 1\n",
    "#Due to the aveeaging effect in chunks even if a chunks has only 70 percent information###\n",
    "####related to the question it is selected as a whole to answer the Question, in case the\n",
    "#chunks has context information than 70 percent which resullts in hallucination###\n",
    "####as it also tries to use other 30 percent unrelated information in some way to generate.#######\n",
    "\n",
    "#Usecase 2:\n",
    "####There can be scenario where there are 10 chunks out which 6 chunks contains 3 percent context-related\n",
    "####information and rest 4 chunks contain 10,5,5,5 percent each.So now if the max chunks for the data rag\n",
    "#### is set to 4 then the 4 chunks with 10,5,5,4 will be selected which is too less in delivering the answer \n",
    "#### and owing to the less information and chunked RAG only 25 percent informtaion will be used to make\n",
    "##the answer.\n",
    "\n",
    "\n",
    "####In this method the page is first strcutured as triplet then as highlights so that the granularity of\n",
    "####the contextual information is made much finer than a bink chunk owing to which the during QA or summary\n",
    "####Every individual entity and relation information of the triplet is taken into account which acts as a\n",
    "####fine-grained,more to the point ,very much suitable to the context and less unrelated information due to th removal of\n",
    "####chunk based information which is a averaged information#####\n",
    "####This helps in separating diffrent semantic information from th text chunked where the information present###\n",
    "####In the embedding is ofeten an averaged out information and functions like cosine similarity is ###\n",
    "####calculated to identify the main chunks some chunks became top candidates or the closest candidates of###\n",
    "####Similarity functions.Due to the averaging effect in chunks even if a chunks has only 50 percent information###\n",
    "####related to the question it is selected as a whole to answer the Question which resullts in hallucination###\n",
    "####as it also tries to use other 50 percent unrelated information in some way.#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15191ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "#rag_test_input_path='/home/matrix4284/MY_GEN_AI_PROJECTS/RAG/GraphRAG/graphrag-local-ollama/ragtest/input/'+file_name\n",
    "import os\n",
    "# importing shutil module\n",
    "import shutil\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import XMLOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01aaca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"llama3.1\"\n",
    "book_text=''\n",
    "page_text=''\n",
    "file_name='Kubernetes_in_action_text_only'\n",
    "extension='.txt'\n",
    "start_page_idx=306\n",
    "end_page_index=479\n",
    "#full_filename=file_name+'_'+str(page_idx)+extension\n",
    "#full_filename\n",
    "pdf_enrichment_output_dir='./pdf_enriched_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbcded8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Model for Prompt Tuning\n",
    "llm = ChatOllama(base_url=\"http://192.168.50.100:11434\",model=model_name,temperature=0.15)\n",
    "\n",
    "#embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",model_kwargs = model_kwargs)\n",
    "\n",
    "##Define Vectorstore\n",
    "vectorstore = Chroma(embedding_function=embeddings, persist_directory=\"./chroma_kubernetes_in_action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a648bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialized data) Ininitial Load\n",
    "#with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_phase_4_pagewise_summary_478_final.pickle', 'rb') as handle:\n",
    "#    document_dict_deserialized = pickle.load(handle)\n",
    "\n",
    "#Intermediate Load based on the last succesfuly index that was fully done\n",
    "with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_phase5_extract_highligts_305.pickle', 'rb') as handle:\n",
    "    document_dict_deserialized = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44433479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_dict_deserialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "da4eb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dict_deserialized_stage2=document_dict_deserialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20db8986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_dict_deserialized_stage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2f9e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "def extract_summary_per_page(page_text):\n",
    "    \n",
    "    \n",
    "    parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker \\n\n",
    "            Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a document page and extract a small summary within 150 words that will\\n\n",
    "            be enough to represent all information for that page.\n",
    "            There is no need to mention any header statement before the summary.\n",
    "            Wrap the summary in a json with key named summary.\n",
    "            Output the json and nothing else no headers no footers.\n",
    "            Here is the document page: \\n\\n {context} \\n\\n\"\"\",\n",
    "            input_variables=[\"context\"],\n",
    "            ###Introduced by Kaustav while experimenting with XMLParsers\n",
    "            #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"context\": page_text,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    json_output = output\n",
    "    json_output=json_output.split('{')[1]\n",
    "    #page_output_json=json.loads(output)\n",
    "    \n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    json_output='{'+json_output\n",
    "    \n",
    "    json_output=json_output.split('}')[0]\n",
    "    \n",
    "    json_output=json_output+'}'\n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output\n",
    "    \n",
    "    #return page_output_json\n",
    "    #return output\n",
    "\n",
    "    \n",
    "# Function to reverse a string\n",
    "def reverse(string):\n",
    "    string = string[::-1]\n",
    "    return string\n",
    "    \n",
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "def extract_highlights_per_page(page_text):\n",
    "    \n",
    "    \n",
    "    parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker,\\n\n",
    "            Machine Learning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a document page and extract upto maximum of 5 most important and most \\n\n",
    "            informative highlights from the page content. \\n\n",
    "            content.Every highlight extracted but have some concrete information and all highlights together \\n\n",
    "            for the same page should make sense.\\n\n",
    "            Extract the highlights following the mentioned rules below:\\n\n",
    "            1.For each highlight wrap it up in json with the key named highlight.\\n\n",
    "            2.After all the highlights have been extracted collate them into a list of json.\\n\n",
    "            Output should only contain the list of json and no other words or character or sentences.\\n\n",
    "            Here is the document page: \\n\\n {context} \\n\\n\"\"\",\n",
    "            input_variables=[\"context\"],\n",
    "            ###Introduced by Kaustav while experimenting with XMLParsers\n",
    "            #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"context\": page_text,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"Raw Output\")\n",
    "    print(output)\n",
    "    \n",
    "    \n",
    "    json_output=output\n",
    "    \n",
    "    \n",
    "            \n",
    "    if '[' in json_output:\n",
    "        json_output=json_output.split('[')[1]\n",
    "        json_output='['+json_output\n",
    "    elif '{' in json_output:\n",
    "        json_output=json_output.split('{')[1]\n",
    "        json_output='{'+json_output\n",
    "        json_output='['+json_output\n",
    "            \n",
    "        #page_output_json=json.loads(output)\n",
    "        \n",
    "    \n",
    "        #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "        #except:\n",
    "        #    json_output=json_output\n",
    "        #    cotinue\n",
    "        \n",
    "        #try:\n",
    "    if ']' in json_output:\n",
    "        json_output=reverse(json_output)\n",
    "        #print('Reversed JSON OUTPUT:')\n",
    "        #print(json_output)\n",
    "        json_output=json_output.split(']')[1]\n",
    "        json_output=reverse(json_output)\n",
    "        #json_output=json_output.rsplit(']')[-1]\n",
    "        #page_output_json=json.loads(output)\n",
    "        #print('JSON OUTPUT:')\n",
    "        #print(json_output)\n",
    "        json_output= json_output + ']'\n",
    "        #print('JSON OUTPUT:')\n",
    "        #print(json_output) \n",
    "    elif '}' in json_output:\n",
    "        json_output=json_output.split('}')[0]\n",
    "        json_output=json_output+'}'\n",
    "        json_output=json_output+']'\n",
    "    \n",
    "    print('JSON OUTPUT:')\n",
    "    print(json_output)\n",
    "    \n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output\n",
    "    \n",
    "    #return page_output_json\n",
    "    #return output\n",
    "\n",
    "def entity_collector_per_page(entity_lst):\n",
    "\n",
    "    entities=[]\n",
    "    \n",
    "    for entity in entity_lst:\n",
    "        print(\"Entity:\")\n",
    "        print(entity)\n",
    "        entity_name=entity['entity']\n",
    "        entities.append(entity_name)\n",
    "    return list(set(entities))\n",
    "\n",
    "\n",
    "def enrich_page(page_idx):\n",
    "    \n",
    "    #print(\"Page Number\")\n",
    "    #print(page_idx)\n",
    "    \n",
    "    page_text=document_dict_deserialized_stage2[page_idx]['text']\n",
    "    \n",
    "    \n",
    "    ########################################Summary Extraction Enrichment###########################################\n",
    "    \n",
    "    ##Summary Extraction Enrichment\n",
    "    #page_summary_txt=extract_summary_per_page(page_text)\n",
    "    \n",
    "    #print(\"Page Summary Text\")\n",
    "    #print(page_summary_txt)\n",
    "    \n",
    "    ######Add Summary as a part of pdf structured dictionary list in order summarization enrichment to data######### \n",
    "    #document_dict_deserialized_stage2[page_idx]['summary']=json.loads(page_summary_txt.strip())[\"summary\"]\n",
    "    \n",
    "    ################################################################################################################\n",
    "    \n",
    "    ######################################Highlight Extraction######################################################\n",
    "    \n",
    "    ##Summary Extraction Enrichment\n",
    "    page_highlights_json_lst=extract_highlights_per_page('\"\"'+page_text+'\"\"')\n",
    "    \n",
    "    #print(\"Page Highlights\")\n",
    "    #print(page_highlights_json_lst)\n",
    "    try:\n",
    "        page_highlights_json_lst=json.loads(page_highlights_json_lst)\n",
    "        ######Add Summary as a part of pdf structured dictionary list in order summarization enrichment to data######### \n",
    "        document_dict_deserialized_stage2[page_idx]['highlights']=page_highlights_json_lst\n",
    "    except:\n",
    "        print(\"Highlights could not be parsed for the page:\"+str(page_idx))\n",
    "    \n",
    "    ################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5465a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here is the list of extracted highlights wrapped in JSON:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To secure a Kubernetes cluster with role-based access control rules, you need to bind a ClusterRole to your ServiceAccount.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The default ServiceAccount cannot list PersistentVolumes at the cluster scope and requires a RoleBinding to allow it to do so.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A RoleBinding referencing a ClusterRole has exactly the same rules as a regular Role.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can create a RoleBinding using the command `kubectl create rolebinding pv-test --clusterrole=pv-reader --serviceaccount=foo:default -n foo`\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The RoleBinding's YAML output should be checked to ensure it is correctly referencing the ClusterRole.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To secure a Kubernetes cluster with role-based access control rules, you need to bind a ClusterRole to your ServiceAccount.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The default ServiceAccount cannot list PersistentVolumes at the cluster scope and requires a RoleBinding to allow it to do so.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A RoleBinding referencing a ClusterRole has exactly the same rules as a regular Role.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can create a RoleBinding using the command `kubectl create rolebinding pv-test --clusterrole=pv-reader --serviceaccount=foo:default -n foo`\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The RoleBinding's YAML output should be checked to ensure it is correctly referencing the ClusterRole.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:306\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To grant access to cluster-level resources, you must always use a ClusterRoleBinding.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can't use RoleBinding for cluster-level (non-namespaced) resources; instead, use ClusterRoleBinding.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Delete the existing RoleBinding before creating a ClusterRoleBinding: $ kubectl delete rolebinding pv-test\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Create a ClusterRoleBinding by replacing 'rolebinding' with 'clusterrolebinding' in the command and omitting namespace: $ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=foo:default\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The default ServiceAccount in the foo namespace is unable to get and list PersistentVolumes due to lack of ClusterRoleBinding.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To grant access to cluster-level resources, you must always use a ClusterRoleBinding.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can't use RoleBinding for cluster-level (non-namespaced) resources; instead, use ClusterRoleBinding.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Delete the existing RoleBinding before creating a ClusterRoleBinding: $ kubectl delete rolebinding pv-test\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Create a ClusterRoleBinding by replacing 'rolebinding' with 'clusterrolebinding' in the command and omitting namespace: $ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=foo:default\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The default ServiceAccount in the foo namespace is unable to get and list PersistentVolumes due to lack of ClusterRoleBinding.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:307\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"You can use a ClusterRole and a ClusterRoleBinding when granting access to cluster-level resources.\"}]\n",
      "\n",
      "[{\"highlight\": \"A RoleBinding cannot grant access to cluster-level resources, even if it references a ClusterRoleBinding.\"}]\n",
      "\n",
      "[{\"highlight\": \"Access to non-resource URLs must be granted explicitly; otherwise the API server will reject the client's request.\"}]\n",
      "\n",
      "[{\"highlight\": \"The system:discovery ClusterRole and ClusterRoleBinding are used to grant access to non-resource URLs by default.\"}]\n",
      "\n",
      "[{\"highlight\": \"A ClusterRoleBinding is required to grant access to cluster-level resources, such as Persistent Volumes.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"You can use a ClusterRole and a ClusterRoleBinding when granting access to cluster-level resources.\"}]\n",
      "Done for page number:308\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page in JSON format:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The verbs field only allows the GET HTTP method to be used on these URLs.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"For non-resource URLs, plain HTTP verbs such as post, put, and patch are used instead of create or update.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The system:discovery ClusterRole has a corresponding system:discovery Cluster-RoleBinding, so let’s see what’s in it by examining the following listing.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"This means absolutely everyone can access the URLs listed in the ClusterRole.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Only the HTTP GET method is allowed for these URLs.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The verbs field only allows the GET HTTP method to be used on these URLs.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"For non-resource URLs, plain HTTP verbs such as post, put, and patch are used instead of create or update.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The system:discovery ClusterRole has a corresponding system:discovery Cluster-RoleBinding, so let’s see what’s in it by examining the following listing.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"This means absolutely everyone can access the URLs listed in the ClusterRole.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Only the HTTP GET method is allowed for these URLs.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:309\n",
      "Raw Output\n",
      "[{\"highlight\": \"You can use ClusterRoles with namespaced RoleBindings to grant access to namespaced resources in a specific namespace.\"}, {\"highlight\": \"The view ClusterRole allows getting, listing, and watching namespaced resources like ConfigMaps, Endpoints, Persistent-VolumeClaims, and so on.\"}, {\"highlight\": \"This ClusterRole has many rules that allow reading but not writing the listed resources.\"}, {\"highlight\": \"You can use curl to access the cluster API without authentication tokens, making you an unauthenticated user.\"}, {\"highlight\": \"ClusterRoles can be bound with regular, namespaced RoleBindings in addition to cluster-level ClusterRoleBindings.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"You can use ClusterRoles with namespaced RoleBindings to grant access to namespaced resources in a specific namespace.\"}, {\"highlight\": \"The view ClusterRole allows getting, listing, and watching namespaced resources like ConfigMaps, Endpoints, Persistent-VolumeClaims, and so on.\"}, {\"highlight\": \"This ClusterRole has many rules that allow reading but not writing the listed resources.\"}, {\"highlight\": \"You can use curl to access the cluster API without authentication tokens, making you an unauthenticated user.\"}, {\"highlight\": \"ClusterRoles can be bound with regular, namespaced RoleBindings in addition to cluster-level ClusterRoleBindings.\"}]\n",
      "Done for page number:310\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page, wrapped in JSON with the key named \"highlight\", and collated into a list of JSON:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"A ClusterRoleBinding allows subjects to view resources across all namespaces, while a RoleBinding only allows viewing resources in the namespace of the binding.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Creating a ClusterRoleBinding with the 'view' cluster role and referencing it in the binding allows a pod's ServiceAccount to list pods across all namespaces.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod can list pods in a different namespace if a ClusterRoleBinding is created, even if the pod is not in that namespace.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Kubernetes API server does not allow listing pods across all namespaces or in a specific namespace without a binding in place.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A RoleBinding only allows viewing resources in the namespace of the binding, whereas a ClusterRoleBinding applies across all namespaces.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"A ClusterRoleBinding allows subjects to view resources across all namespaces, while a RoleBinding only allows viewing resources in the namespace of the binding.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Creating a ClusterRoleBinding with the 'view' cluster role and referencing it in the binding allows a pod's ServiceAccount to list pods across all namespaces.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod can list pods in a different namespace if a ClusterRoleBinding is created, even if the pod is not in that namespace.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Kubernetes API server does not allow listing pods across all namespaces or in a specific namespace without a binding in place.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A RoleBinding only allows viewing resources in the namespace of the binding, whereas a ClusterRoleBinding applies across all namespaces.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[{\"highlight\": \"Combining a ClusterRoleBinding with a ClusterRole referring to namespaced resources allows the pod to access namespaced resources in any namespace.\"}, {\"highlight\": \"Replacing the ClusterRoleBinding with a RoleBinding restricts access to only the specified namespace.\"}, {\"highlight\": \"A RoleBinding is namespaced, requiring specification of the target namespace when created.\"}, {\"highlight\": \"The default ServiceAccount in a namespace has access to view pods in any namespace through a ClusterRole.\"}, {\"highlight\": \"A ClusterRole grants permission to resources such as Pods, Services, Endpoints, ConfigMaps, and more across all namespaces.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Combining a ClusterRoleBinding with a ClusterRole referring to namespaced resources allows the pod to access namespaced resources in any namespace.\"}, {\"highlight\": \"Replacing the ClusterRoleBinding with a RoleBinding restricts access to only the specified namespace.\"}, {\"highlight\": \"A RoleBinding is namespaced, requiring specification of the target namespace when created.\"}, {\"highlight\": \"The default ServiceAccount in a namespace has access to view pods in any namespace through a ClusterRole.\"}, {\"highlight\": \"A ClusterRole grants permission to resources such as Pods, Services, Endpoints, ConfigMaps, and more across all namespaces.\"}]\n",
      "Done for page number:312\n",
      "Raw Output\n",
      "[{\"highlight\": \"User \\\"system:serviceaccount:foo:default\\\" cannot list pods in the namespace \\\"bar\\\".\"}, {\"highlight\": \"User \\\"system:serviceaccount:foo:default\\\" cannot list pods at the cluster scope.\"}, {\"highlight\": \"Default ServiceAccount in foo namespace is only allowed to view pods in namespace foo, despite using a ClusterRole.\"}, {\"highlight\": \"A RoleBinding referring to a ClusterRole only grants access to resources inside the RoleBinding’s namespace.\"}, {\"highlight\": \"ClusterRole: view Allows getting, listing, watching\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"User \\\"system:serviceaccount:foo:default\\\" cannot list pods in the namespace \\\"bar\\\".\"}, {\"highlight\": \"User \\\"system:serviceaccount:foo:default\\\" cannot list pods at the cluster scope.\"}, {\"highlight\": \"Default ServiceAccount in foo namespace is only allowed to view pods in namespace foo, despite using a ClusterRole.\"}, {\"highlight\": \"A RoleBinding referring to a ClusterRole only grants access to resources inside the RoleBinding’s namespace.\"}, {\"highlight\": \"ClusterRole: view Allows getting, listing, watching\"}]\n",
      "Done for page number:313\n",
      "Raw Output\n",
      "[{\"highlight\": \"Kubernetes comes with a default set of ClusterRoles and ClusterRoleBindings, which are updated every time the API server starts.\"}, {\"highlight\": \"You can see the default cluster roles and bindings in the following listing.\"}, {\"highlight\": \"The default cluster roles and bindings include cluster-admin, system:basic-user, system:kube-controller-manager, system:kube-dns, system:kube-scheduler, system:node, and system:node-proxier.\"}, {\"highlight\": \"These default ClusterRoles and ClusterRoleBindings are recreated if you mistakenly delete them or if a newer version of Kubernetes uses a different configuration of cluster roles and bindings.\"}, {\"highlight\": \"$ kubectl get clusterrolebindings and $ kubectl get clusterroles commands can be used to view the default ClusterRoleBindings and ClusterRoles, respectively.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Kubernetes comes with a default set of ClusterRoles and ClusterRoleBindings, which are updated every time the API server starts.\"}, {\"highlight\": \"You can see the default cluster roles and bindings in the following listing.\"}, {\"highlight\": \"The default cluster roles and bindings include cluster-admin, system:basic-user, system:kube-controller-manager, system:kube-dns, system:kube-scheduler, system:node, and system:node-proxier.\"}, {\"highlight\": \"These default ClusterRoles and ClusterRoleBindings are recreated if you mistakenly delete them or if a newer version of Kubernetes uses a different configuration of cluster roles and bindings.\"}, {\"highlight\": \"$ kubectl get clusterrolebindings and $ kubectl get clusterroles commands can be used to view the default ClusterRoleBindings and ClusterRoles, respectively.\"}]\n",
      "Done for page number:314\n",
      "Raw Output\n",
      "[{\"highlight\": \"The most important roles are the view, edit, admin, and cluster-admin ClusterRoles.\"}, \n",
      " {\"highlight\": \"ALLOWING READ-ONLY ACCESS TO RESOURCES WITH THE VIEW CLUSTERROLE\"}, \n",
      " {\"highlight\": \"GRANTING FULL CONTROL OF A NAMESPACE WITH THE ADMIN CLUSTERROLE\"}, \n",
      " {\"highlight\": \"ALLOWING COMPLETE CONTROL WITH THE CLUSTER-ADMIN CLUSTERROLE\"}, \n",
      " {\"highlight\": \"The list of default ClusterRoles includes a large number of other ClusterRoles, which start with the system: prefix.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The most important roles are the view, edit, admin, and cluster-admin ClusterRoles.\"}, \n",
      " {\"highlight\": \"ALLOWING READ-ONLY ACCESS TO RESOURCES WITH THE VIEW CLUSTERROLE\"}, \n",
      " {\"highlight\": \"GRANTING FULL CONTROL OF A NAMESPACE WITH THE ADMIN CLUSTERROLE\"}, \n",
      " {\"highlight\": \"ALLOWING COMPLETE CONTROL WITH THE CLUSTER-ADMIN CLUSTERROLE\"}, \n",
      " {\"highlight\": \"The list of default ClusterRoles includes a large number of other ClusterRoles, which start with the system: prefix.\"}]\n",
      "Done for page number:315\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Each controller running inside the Controller Manager can use a separate ClusterRole and ClusterRoleBinding, which binds it to the user the system component authenticates as.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"By default, the default ServiceAccount in a namespace has no permissions other than those of an unauthenticated user, so pods can’t even view cluster state without explicit permission.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"It’s best to give everyone only the permissions they need to do their job and not a single permission more (principle of least privilege), rather than giving all ServiceAccounts the cluster-admin ClusterRole.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Creating specific ServiceAccounts for each pod and associating them with tailor-made Roles or ClusterRoles through RoleBindings is a good idea, especially when different pods have different permissions needs.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You should expect unwanted persons to eventually get hold of your cluster's ServiceAccount authentication token, so you should always constrain the ServiceAccount to prevent them from doing any real damage.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Each controller running inside the Controller Manager can use a separate ClusterRole and ClusterRoleBinding, which binds it to the user the system component authenticates as.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"By default, the default ServiceAccount in a namespace has no permissions other than those of an unauthenticated user, so pods can’t even view cluster state without explicit permission.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"It’s best to give everyone only the permissions they need to do their job and not a single permission more (principle of least privilege), rather than giving all ServiceAccounts the cluster-admin ClusterRole.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Creating specific ServiceAccounts for each pod and associating them with tailor-made Roles or ClusterRoles through RoleBindings is a good idea, especially when different pods have different permissions needs.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You should expect unwanted persons to eventually get hold of your cluster's ServiceAccount authentication token, so you should always constrain the ServiceAccount to prevent them from doing any real damage.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:316\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"By default, pods run under the default ServiceAccount, which is created for each namespace automatically.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"ServiceAccounts can be configured to allow mounting only a constrained list of Secrets in a given pod.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Roles and ClusterRoles define what actions can be performed on which resources.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"RoleBindings and ClusterRoleBindings bind Roles and ClusterRoles to users, groups, and ServiceAccounts.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Each cluster comes with default ClusterRoles and ClusterRoleBindings.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"By default, pods run under the default ServiceAccount, which is created for each namespace automatically.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"ServiceAccounts can be configured to allow mounting only a constrained list of Secrets in a given pod.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Roles and ClusterRoles define what actions can be performed on which resources.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"RoleBindings and ClusterRoleBindings bind Roles and ClusterRoles to users, groups, and ServiceAccounts.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Each cluster comes with default ClusterRoles and ClusterRoleBindings.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\"highlight\": \"Allowing pods to access node resources can be done by using the node's default Linux namespaces in pods.\"},\n",
      "    {\"highlight\": \"Running containers as different users can help configure the cluster so users aren't able to do whatever they want with their pods.\"},\n",
      "    {\"highlight\": \"Privileged containers can be run, but this requires careful consideration of security policies to limit what pods can do.\"},\n",
      "    {\"highlight\": \"Adding or dropping a container's kernel capabilities is another way to secure the pod network and prevent unauthorized access.\"},\n",
      "    {\"highlight\": \"Defining security policies is crucial to limit what pods can do and prevent attackers from running malicious code in containers.\"}\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\"highlight\": \"Allowing pods to access node resources can be done by using the node's default Linux namespaces in pods.\"},\n",
      "    {\"highlight\": \"Running containers as different users can help configure the cluster so users aren't able to do whatever they want with their pods.\"},\n",
      "    {\"highlight\": \"Privileged containers can be run, but this requires careful consideration of security policies to limit what pods can do.\"},\n",
      "    {\"highlight\": \"Adding or dropping a container's kernel capabilities is another way to secure the pod network and prevent unauthorized access.\"},\n",
      "    {\"highlight\": \"Defining security policies is crucial to limit what pods can do and prevent attackers from running malicious code in containers.\"}\n",
      "]\n",
      "Done for page number:318\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Containers in a pod usually run under separate Linux namespaces, which isolate their processes from processes running in other containers or in the node's default namespaces.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Certain pods (usually system pods) need to operate in the host's default namespaces, allowing them to see and manipulate node-level resources and devices.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"By setting the hostNetwork property in the pod spec to true, a pod gets to use the node's network interfaces instead of having its own set, without getting its own IP address.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod using the node's network namespace does not get its own IP address and if it runs a process that binds to a port, the process will be bound to the node's port.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The hostNetwork property in the pod spec can be set to true to allow a pod to use the node's network interfaces instead of its own virtual network adapters.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Containers in a pod usually run under separate Linux namespaces, which isolate their processes from processes running in other containers or in the node's default namespaces.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Certain pods (usually system pods) need to operate in the host's default namespaces, allowing them to see and manipulate node-level resources and devices.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"By setting the hostNetwork property in the pod spec to true, a pod gets to use the node's network interfaces instead of having its own set, without getting its own IP address.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod using the node's network namespace does not get its own IP address and if it runs a process that binds to a port, the process will be bound to the node's port.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The hostNetwork property in the pod spec can be set to true to allow a pod to use the node's network interfaces instead of its own virtual network adapters.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:319\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When you deploy your cluster with kubeadm, Kubernetes Control Plane components are deployed as pods and use the hostNetwork option, effectively making them behave as if they weren't running inside a pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods can bind to a port in the node's default namespace without using the host's network namespace by using the hostPort property in one of the container's ports defined in the spec.containers.ports field.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Using hostPort allows pods to directly connect to the node's port, whereas NodePort services forward connections to a randomly selected pod on any node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods using hostPort only bind the node's port on nodes that run such pods, whereas NodePort services bind the port on all nodes, even those that don't run the pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use the command kubectl exec pod-with-host-network ifconfig to see that a pod is indeed using the host's network namespace and sees all the host's network adapters.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When you deploy your cluster with kubeadm, Kubernetes Control Plane components are deployed as pods and use the hostNetwork option, effectively making them behave as if they weren't running inside a pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods can bind to a port in the node's default namespace without using the host's network namespace by using the hostPort property in one of the container's ports defined in the spec.containers.ports field.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Using hostPort allows pods to directly connect to the node's port, whereas NodePort services forward connections to a randomly selected pod on any node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods using hostPort only bind the node's port on nodes that run such pods, whereas NodePort services bind the port on all nodes, even those that don't run the pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use the command kubectl exec pod-with-host-network ifconfig to see that a pod is indeed using the host's network namespace and sees all the host's network adapters.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:320\n",
      "Raw Output\n",
      "[{\"highlight\": \"If a pod is using a specific host port, only one instance of the pod can be scheduled to each node.\"}, {\"highlight\": \"The Scheduler takes this into account when scheduling pods, so it doesn’t schedule multiple pods to the same node.\"}, {\"highlight\": \"If you have three nodes and want to deploy four pod replicas, only three will be scheduled (one pod will remain Pending).\"}, {\"highlight\": \"Pods using a hostPort can only be scheduled to one node per port, while pods behind a NodePort service can be scheduled to multiple nodes.\"}, {\"highlight\": \"Using a host port limits the number of pod instances that can be scheduled to each node.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"If a pod is using a specific host port, only one instance of the pod can be scheduled to each node.\"}, {\"highlight\": \"The Scheduler takes this into account when scheduling pods, so it doesn’t schedule multiple pods to the same node.\"}, {\"highlight\": \"If you have three nodes and want to deploy four pod replicas, only three will be scheduled (one pod will remain Pending).\"}, {\"highlight\": \"Pods using a hostPort can only be scheduled to one node per port, while pods behind a NodePort service can be scheduled to multiple nodes.\"}, {\"highlight\": \"Using a host port limits the number of pod instances that can be scheduled to each node.\"}]\n",
      "Done for page number:321\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To access a pod through its host node's port, you can define the hostPort in a pod's YAML definition.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The hostPort feature is primarily used for exposing system services deployed to every node using DaemonSets.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use the hostPID and hostIPC pod spec properties to allow processes running in containers to see all other processes on the node or communicate with them through IPC.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When you set hostPID and hostIPC to true, a pod's containers will use the node's PID and IPC namespaces.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can access a container on port 8080 of the pod's IP or on port 9000 of the node it's deployed on.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To access a pod through its host node's port, you can define the hostPort in a pod's YAML definition.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The hostPort feature is primarily used for exposing system services deployed to every node using DaemonSets.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use the hostPID and hostIPC pod spec properties to allow processes running in containers to see all other processes on the node or communicate with them through IPC.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When you set hostPID and hostIPC to true, a pod's containers will use the node's PID and IPC namespaces.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can access a container on port 8080 of the pod's IP or on port 9000 of the node it's deployed on.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"By setting the hostIPC property to true, processes in the pod’s containers can also communicate with all the other processes running on the node, through Inter-Process Communication.\"}]\n",
      "\n",
      "[{\"highlight\": \"Configuring the security context allows you to do various things: Specify the user (the user’s ID) under which the process in the container will run. Prevent the container from running as root. Run the container in privileged mode, giving it full access to the node’s kernel. Configure fine-grained privileges, by adding or dropping capabilities.\"}]\n",
      "\n",
      "[{\"highlight\": \"Processes in a pod with hostPID: true can see all processes running on the host node, not only the ones running in the container.\"}]\n",
      "\n",
      "[{\"highlight\": \"The security context properties can be specified under the pod spec directly and inside the spec of individual containers.\"}]\n",
      "\n",
      "[{\"highlight\": \"Configuring the security context allows you to prevent the process from writing to the container’s filesystem and set SELinux options to strongly lock down a container.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"By setting the hostIPC property to true, processes in the pod’s containers can also communicate with all the other processes running on the node, through Inter-Process Communication.\"}]\n",
      "Done for page number:323\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When running a pod without specifying a security context, the container runs as root with user ID (uid) 0 and group ID (gid) 0.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The user that the container runs as is specified in the container image using the USER directive in a Dockerfile, and defaults to root if omitted.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To run a pod under a different user ID than the one baked into the container image, set the pod's securityContext.runAsUser property with a specific user ID (e.g. 405 for the guest user).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The id command can be used to see the user and group ID that the container is running as, and which groups it belongs to.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A user ID must be specified in the securityContext.runAsUser property, not a username (e.g. 'guest' instead of 'user guest').\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When running a pod without specifying a security context, the container runs as root with user ID (uid) 0 and group ID (gid) 0.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The user that the container runs as is specified in the container image using the USER directive in a Dockerfile, and defaults to root if omitted.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To run a pod under a different user ID than the one baked into the container image, set the pod's securityContext.runAsUser property with a specific user ID (e.g. 405 for the guest user).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The id command can be used to see the user and group ID that the container is running as, and which groups it belongs to.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A user ID must be specified in the securityContext.runAsUser property, not a username (e.g. 'guest' instead of 'user guest').\"\n",
      "    }\n",
      "]\n",
      "Done for page number:324\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page, wrapped in JSON with the key \"highlight\":\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To prevent containers from running as root, you can specify that the pod's container needs to run as a non-root user by setting `runAsNonRoot` to true in the Pod's securityContext.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If an attacker gets access to your image registry and pushes a different image under the same tag, running containers as root allows them to execute malicious code with full host system privileges.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Running containers as non-root user prevents them from accessing mounted directories on the host system, even if they are mounted into the container.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To deploy a pod that runs its container as a non-root user, you can use the following YAML configuration: `apiVersion: v1`, `kind: Pod`, `spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsNonRoot: true`\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Running pods in privileged mode allows them to use protected system devices or other kernel features, but it also increases the risk of container escape and privilege escalation.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To prevent containers from running as root, you can specify that the pod's container needs to run as a non-root user by setting `runAsNonRoot` to true in the Pod's securityContext.\"\n",
      "    }]\n",
      "Done for page number:325\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page, wrapped in JSON with the key \"highlight\":\n",
      "\n",
      "[{\"highlight\": \"A pod can run in privileged mode by setting the privileged property in the container's security-Context property to true.\"}, {\"highlight\": \"In a non-privileged container, only a short list of devices is visible in the /dev directory, including core, null, stderr, stdin, stdout, urandom, fd, ptmx, and tty.\"}, {\"highlight\": \"A privileged pod can see all device files in the /dev directory, including autofs, bsg, sr0, snd, and tty46-47.\"}, {\"highlight\": \"The kube-proxy pod runs in privileged mode to modify node's iptables rules for services to work.\"}, {\"highlight\": \"Deploying a cluster with kubeadm results in every cluster node running a kube-proxy pod, which can be examined for its YAML specification.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"A pod can run in privileged mode by setting the privileged property in the container's security-Context property to true.\"}, {\"highlight\": \"In a non-privileged container, only a short list of devices is visible in the /dev directory, including core, null, stderr, stdin, stdout, urandom, fd, ptmx, and tty.\"}, {\"highlight\": \"A privileged pod can see all device files in the /dev directory, including autofs, bsg, sr0, snd, and tty46-47.\"}, {\"highlight\": \"The kube-proxy pod runs in privileged mode to modify node's iptables rules for services to work.\"}, {\"highlight\": \"Deploying a cluster with kubeadm results in every cluster node running a kube-proxy pod, which can be examined for its YAML specification.\"}]\n",
      "Done for page number:326\n",
      "Raw Output\n",
      "[{\"highlight\": \"The privileged container sees all the host node's devices, allowing it to use any device freely.\"}, \n",
      " {\"highlight\": \"Kubernetes allows you to add capabilities to each container or drop part of them, fine-tuning the container's permissions and limiting the impact of a potential intrusion by an attacker.\"}, \n",
      " {\"highlight\": \"A container usually isn't allowed to change the system time (the hardware clock's time), but can be granted access to do so by adding the CAP_SYS_TIME capability.\"}, \n",
      " {\"highlight\": \"The CAP_SYS_TIME capability allows a container to set the system time, as demonstrated in Listing 13.11.\"}, \n",
      " {\"highlight\": \"You can add individual kernel capabilities to a container using Kubernetes, providing a safer method than making it privileged and giving it unlimited permissions.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The privileged container sees all the host node's devices, allowing it to use any device freely.\"}, \n",
      " {\"highlight\": \"Kubernetes allows you to add capabilities to each container or drop part of them, fine-tuning the container's permissions and limiting the impact of a potential intrusion by an attacker.\"}, \n",
      " {\"highlight\": \"A container usually isn't allowed to change the system time (the hardware clock's time), but can be granted access to do so by adding the CAP_SYS_TIME capability.\"}, \n",
      " {\"highlight\": \"The CAP_SYS_TIME capability allows a container to set the system time, as demonstrated in Listing 13.11.\"}, \n",
      " {\"highlight\": \"You can add individual kernel capabilities to a container using Kubernetes, providing a safer method than making it privileged and giving it unlimited permissions.\"}]\n",
      "Done for page number:327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here is the list of extracted highlights in JSON format:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Linux kernel capabilities are usually prefixed with CAP_, but when specifying them in a pod spec, you must leave out the prefix.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"If you try to change the system time in a container, it may cause your worker node to become unusable and require a reboot.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Adding capabilities like SYS_TIME is a better way than giving a container full privileges with privileged: true.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Capabilities can be added or dropped under the securityContext property, allowing you to customize the capabilities available to a container.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The CAP_CHOWN capability allows processes to change the ownership of files in the filesystem and can be dropped from a container if not needed.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Linux kernel capabilities are usually prefixed with CAP_, but when specifying them in a pod spec, you must leave out the prefix.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"If you try to change the system time in a container, it may cause your worker node to become unusable and require a reboot.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Adding capabilities like SYS_TIME is a better way than giving a container full privileges with privileged: true.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Capabilities can be added or dropped under the securityContext property, allowing you to customize the capabilities available to a container.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The CAP_CHOWN capability allows processes to change the ownership of files in the filesystem and can be dropped from a container if not needed.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:328\n",
      "Raw Output\n",
      "[{\"highlight\": \"To prevent the container from doing that, you need to drop the capability by listing it under the container’s securityContext.capabilities.drop property.\"}, {\"highlight\": \"By dropping the CHOWN capability, you’re not allowed to change the owner of the /tmp directory in this pod:\"}, {\"highlight\": \"You may want to prevent the processes running in the container from writing to the container’s filesystem,\"}, {\"highlight\": \"This is done by setting the container’s securityContext.readOnlyRootFilesystem property to true,\"}, {\"highlight\": \"You’re not allowing this container to change file ownership.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"To prevent the container from doing that, you need to drop the capability by listing it under the container’s securityContext.capabilities.drop property.\"}, {\"highlight\": \"By dropping the CHOWN capability, you’re not allowed to change the owner of the /tmp directory in this pod:\"}, {\"highlight\": \"You may want to prevent the processes running in the container from writing to the container’s filesystem,\"}, {\"highlight\": \"This is done by setting the container’s securityContext.readOnlyRootFilesystem property to true,\"}, {\"highlight\": \"You’re not allowing this container to change file ownership.\"}]\n",
      "Done for page number:329\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"When you deploy this pod, the container is running as root, which has write permissions to the / directory, but trying to write a file there fails: touch /new-file\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"On the other hand, writing to the mounted volume is allowed: $ kubectl exec -it pod-with-readonly-filesystem touch /volume/newfile\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To increase security, when running pods in production, set their container’s readOnlyRootFilesystem property to true.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You may need to run the two containers as two different users (perhaps you’re using two third-party container images, where each one runs its process\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"When you make the container’s filesystem read-only, you’ll probably want to mount a volume in every directory the application writes to (for example, logs, on-disk caches, and so on).\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"When you deploy this pod, the container is running as root, which has write permissions to the / directory, but trying to write a file there fails: touch /new-file\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"On the other hand, writing to the mounted volume is allowed: $ kubectl exec -it pod-with-readonly-filesystem touch /volume/newfile\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To increase security, when running pods in production, set their container’s readOnlyRootFilesystem property to true.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You may need to run the two containers as two different users (perhaps you’re using two third-party container images, where each one runs its process\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"When you make the container’s filesystem read-only, you’ll probably want to mount a volume in every directory the application writes to (for example, logs, on-disk caches, and so on).\"\n",
      "  }\n",
      "]\n",
      "Done for page number:330\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes allows you to specify supplemental groups for all the pods running in the container, allowing them to share files, regardless of the user IDs they’re running as.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The fsGroup and supplementalGroups are defined in the security context at the pod level.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Both containers use the same volume, but may not necessarily be able to read or write files of one another due to different user IDs.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"fsGroup is used to specify a single group ID for all pods running in the container, while supplementalGroups allows multiple group IDs to be specified.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The first and second containers run as user IDs 1111 and 2222 respectively, but share the same volume with group IDs 555, 666, and 777.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes allows you to specify supplemental groups for all the pods running in the container, allowing them to share files, regardless of the user IDs they’re running as.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The fsGroup and supplementalGroups are defined in the security context at the pod level.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Both containers use the same volume, but may not necessarily be able to read or write files of one another due to different user IDs.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"fsGroup is used to specify a single group ID for all pods running in the container, while supplementalGroups allows multiple group IDs to be specified.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The first and second containers run as user IDs 1111 and 2222 respectively, but share the same volume with group IDs 555, 666, and 777.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:331\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"The fsGroup security context property is used when a process creates files in a volume, whereas the supplementalGroups property defines a list of additional group IDs the user is associated with.\"}]\n",
      "\n",
      "[{\"highlight\": \"PodSecurityPolicy resources can be created by cluster admins to restrict users from using certain security-related features in their pods.\"}]\n",
      "\n",
      "[{\"highlight\": \"The PodSecurityPolicy resource is a cluster-level, non-namespaced resource that defines what security-related features users can or cannot use in their pods.\"}]\n",
      "\n",
      "[{\"highlight\": \"Restricting the use of security-related features in pods can be achieved by creating one or more PodSecurityPolicy resources.\"}]\n",
      "\n",
      "[{\"highlight\": \"The id command shows a container running with user ID 1111, as specified in the pod definition, and effective group ID 0 (root), but also associated with group IDs 555, 666, and 777.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The fsGroup security context property is used when a process creates files in a volume, whereas the supplementalGroups property defines a list of additional group IDs the user is associated with.\"}]\n",
      "Done for page number:332\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The PodSecurityPolicy admission control plugin validates the pod definition against the configured PodSecurity-Policies, and if the pod conforms to the cluster's policies, it's accepted and stored into etcd; otherwise it's rejected immediately.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A PodSecurityPolicy resource defines things like whether a pod can use the host's IPC, PID, or Network namespaces, which host ports a pod can bind to, what user IDs a container can run as, and whether a pod with privileged containers can be created.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To enable RBAC and PodSecurityPolicy admission control in Minikube, you need to use the command: $ minikube start --extra-config apiserver.Authentication.PasswordFile=/etc/kubernetes/passwd --extra-config=apiserver.Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRunOptions.AdmissionControl=PodSecurityPolicy\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You need to create a password file by running the command: $ cat <<EOF | minikube ssh sudo tee /etc/kubernetes/passwd password,alice,1000,basic-user password,bob,2000,privileged-user EOF\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The PodSecurityPolicy admission control plugin may not be enabled in your cluster, so you need to ensure it's enabled before running the examples.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The PodSecurityPolicy admission control plugin validates the pod definition against the configured PodSecurity-Policies, and if the pod conforms to the cluster's policies, it's accepted and stored into etcd; otherwise it's rejected immediately.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A PodSecurityPolicy resource defines things like whether a pod can use the host's IPC, PID, or Network namespaces, which host ports a pod can bind to, what user IDs a container can run as, and whether a pod with privileged containers can be created.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To enable RBAC and PodSecurityPolicy admission control in Minikube, you need to use the command: $ minikube start --extra-config apiserver.Authentication.PasswordFile=/etc/kubernetes/passwd --extra-config=apiserver.Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRunOptions.AdmissionControl=PodSecurityPolicy\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You need to create a password file by running the command: $ cat <<EOF | minikube ssh sudo tee /etc/kubernetes/passwd password,alice,1000,basic-user password,bob,2000,privileged-user EOF\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The PodSecurityPolicy admission control plugin may not be enabled in your cluster, so you need to ensure it's enabled before running the examples.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here is the list of extracted highlights in JSON format:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"PodSecurityPolicy restricts the use of security-related features in pods, including kernel capabilities, SELinux labels, writable root filesystem, filesystem groups, volume types, and host ports.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A sample PodSecurityPolicy prevents pods from using the host's IPC, PID, and Network namespaces, and running privileged containers and most host ports (except ports 10000-11000 and 13000-14000).\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The policy allows containers to run as any user, group, or SELinux group, and use all volume types.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Containers are forced to run with a read-only root filesystem and cannot run in privileged mode.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Host ports 10000-11000 and 13000-14000 are allowed for container binding.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"PodSecurityPolicy restricts the use of security-related features in pods, including kernel capabilities, SELinux labels, writable root filesystem, filesystem groups, volume types, and host ports.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A sample PodSecurityPolicy prevents pods from using the host's IPC, PID, and Network namespaces, and running privileged containers and most host ports (except ports 10000-11000 and 13000-14000).\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The policy allows containers to run as any user, group, or SELinux group, and use all volume types.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Containers are forced to run with a read-only root filesystem and cannot run in privileged mode.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Host ports 10000-11000 and 13000-14000 are allowed for container binding.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:334\n",
      "Raw Output\n",
      "[{\"highlight\": \"The API server will no longer allow deploying privileged pods due to security policy restrictions.\"}, {\"highlight\": \"Pods with host's PID, IPC, or Network namespace are also restricted from deployment.\"}, {\"highlight\": \"Container filesystems in all pods will be read-only due to readOnlyRootFilesystem set to true in the policy.\"}, {\"highlight\": \"The RunAsAny rule allows containers to run as any user and group IDs without restrictions.\"}, {\"highlight\": \"Using the MustRunAs rule, you can specify a range of allowed user or group IDs for containers to run as.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The API server will no longer allow deploying privileged pods due to security policy restrictions.\"}, {\"highlight\": \"Pods with host's PID, IPC, or Network namespace are also restricted from deployment.\"}, {\"highlight\": \"Container filesystems in all pods will be read-only due to readOnlyRootFilesystem set to true in the policy.\"}, {\"highlight\": \"The RunAsAny rule allows containers to run as any user and group IDs without restrictions.\"}, {\"highlight\": \"Using the MustRunAs rule, you can specify a range of allowed user or group IDs for containers to run as.\"}]\n",
      "Done for page number:335\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"If the pod spec tries to set either of those fields (runAsUser or fsGroup) to a value outside of these ranges, the pod will not be accepted by the API server.\"}]\n",
      "\n",
      "[{\"highlight\": \"Changing the policy has no effect on existing pods, because PodSecurity-Policies are enforced only when creating or updating pods.\"}]\n",
      "\n",
      "[{\"highlight\": \"If you try deploying a pod with runAsUser set to 405, the API server rejects the pod due to unable to validate against any pod security policy.\"}]\n",
      "\n",
      "[{\"highlight\": \"Deploying a pod without setting the runAsUser property but with a container image configured to run as user ID 5 does not get rejected by the API server.\"}]\n",
      "\n",
      "[{\"highlight\": \"The PodSecurityPolicy can be used to override the user ID hardcoded into a container image, and containers will run with the specified ID even if it's different from the one in the image.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"If the pod spec tries to set either of those fields (runAsUser or fsGroup) to a value outside of these ranges, the pod will not be accepted by the API server.\"}]\n",
      "Done for page number:336\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page in JSON format:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The MustRunAsNonRoot rule can be used in the runAsUser field to prevent users from deploying containers that run as root.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Three fields influence which capabilities containers can or cannot use: allowedCapabilities, defaultAddCapabilities, and requiredDropCapabilities.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The allowedCapabilities field specifies which capabilities pod authors can add in the securityContext.capabilities field in the container spec.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The SYS_ADMIN capability allows a range of administrative operations, and the SYS_MODULE capability allows loading and unloading of Linux kernel modules.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The requiredDropCapabilities field requires containers to drop specific capabilities, such as SYS_ADMIN and SYS_MODULE.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The MustRunAsNonRoot rule can be used in the runAsUser field to prevent users from deploying containers that run as root.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Three fields influence which capabilities containers can or cannot use: allowedCapabilities, defaultAddCapabilities, and requiredDropCapabilities.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The allowedCapabilities field specifies which capabilities pod authors can add in the securityContext.capabilities field in the container spec.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The SYS_ADMIN capability allows a range of administrative operations, and the SYS_MODULE capability allows loading and unloading of Linux kernel modules.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The requiredDropCapabilities field requires containers to drop specific capabilities, such as SYS_ADMIN and SYS_MODULE.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:337\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page, wrapped in JSON with the key \"highlight\":\n",
      "\n",
      "[{\"highlight\": \"All capabilities listed under the defaultAddCapabilities field will be added to every deployed pod’s containers.\"}, {\"highlight\": \"The final field in this example is requiredDropCapabilities. Capabilities listed in this field are dropped automatically from every container.\"}, {\"highlight\": \"If a user tries to create a pod where they explicitly add one of the capabilities listed in the policy’s requiredDropCapabilities field, the pod is rejected.\"}, {\"highlight\": \"A PodSecurityPolicy resource can define which volume types users can add to their pods. At least emptyDir, configMap, secret, downwardAPI, and persistentVolume-Claim volumes should be allowed.\"}, {\"highlight\": \"If multiple PodSecurityPolicy resources are in place, pods can use any volume type defined in any of the policies (the union of all volumes lists is used).\" }]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"All capabilities listed under the defaultAddCapabilities field will be added to every deployed pod’s containers.\"}, {\"highlight\": \"The final field in this example is requiredDropCapabilities. Capabilities listed in this field are dropped automatically from every container.\"}, {\"highlight\": \"If a user tries to create a pod where they explicitly add one of the capabilities listed in the policy’s requiredDropCapabilities field, the pod is rejected.\"}, {\"highlight\": \"A PodSecurityPolicy resource can define which volume types users can add to their pods. At least emptyDir, configMap, secret, downwardAPI, and persistentVolume-Claim volumes should be allowed.\"}, {\"highlight\": \"If multiple PodSecurityPolicy resources are in place, pods can use any volume type defined in any of the policies (the union of all volumes lists is used).\" }]\n",
      "Done for page number:338\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page, wrapped in JSON with the key named \"highlight\", and collated into a list:\n",
      "\n",
      "[{\"highlight\": \"PodSecurityPolicies can be assigned to different users and groups through the RBAC mechanism, allowing for fine-grained access control.\"}, \n",
      " {\"highlight\": \"A PodSecurityPolicy is a cluster-level resource that cannot be stored in and applied to a specific namespace, but can be made available to individual users or groups by creating ClusterRole resources.\"}, \n",
      " {\"highlight\": \"The PodSecurityPolicy Admission Control plugin considers only the policies accessible to the user creating the pod when deciding whether to admit a pod definition or not.\"}, \n",
      " {\"highlight\": \"A special PodSecurityPolicy allowing privileged containers to be deployed can be created, which allows running privileged containers and has a name of 'privileged'.\"}, \n",
      " {\"highlight\": \"The shorthand for PodSecurityPolicy is 'psp', and the command 'kubectl get psp' can be used to list all available policies in the cluster.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"PodSecurityPolicies can be assigned to different users and groups through the RBAC mechanism, allowing for fine-grained access control.\"}, \n",
      " {\"highlight\": \"A PodSecurityPolicy is a cluster-level resource that cannot be stored in and applied to a specific namespace, but can be made available to individual users or groups by creating ClusterRole resources.\"}, \n",
      " {\"highlight\": \"The PodSecurityPolicy Admission Control plugin considers only the policies accessible to the user creating the pod when deciding whether to admit a pod definition or not.\"}, \n",
      " {\"highlight\": \"A special PodSecurityPolicy allowing privileged containers to be deployed can be created, which allows running privileged containers and has a name of 'privileged'.\"}, \n",
      " {\"highlight\": \"The shorthand for PodSecurityPolicy is 'psp', and the command 'kubectl get psp' can be used to list all available policies in the cluster.\"}]\n",
      "Done for page number:339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To restrict the use of security-related features in pods, you can create multiple PodSecurityPolicies (PSPs) and assign them to different users using Role-Based Access Control (RBAC).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can create a ClusterRole for each PSP, allowing specific users or groups to use that policy. For example, you can create a 'psp-default' role for the default PSP and a 'psp-privileged' role for the privileged PSP.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To bind a ClusterRole to a user or group, you need to use a ClusterRoleBinding instead of a RoleBinding. This is because ClusterRoles grant access to cluster-level resources like PodSecurityPolicies.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can bind the 'psp-default' role to all authenticated users by creating a ClusterRoleBinding with the 'system:authenticated' group, ensuring that no one can create pods without a policy in place.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"By using RBAC and PSPs, you can enforce different security policies for different users or groups within your Kubernetes cluster, improving overall security and compliance.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To restrict the use of security-related features in pods, you can create multiple PodSecurityPolicies (PSPs) and assign them to different users using Role-Based Access Control (RBAC).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can create a ClusterRole for each PSP, allowing specific users or groups to use that policy. For example, you can create a 'psp-default' role for the default PSP and a 'psp-privileged' role for the privileged PSP.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To bind a ClusterRole to a user or group, you need to use a ClusterRoleBinding instead of a RoleBinding. This is because ClusterRoles grant access to cluster-level resources like PodSecurityPolicies.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can bind the 'psp-default' role to all authenticated users by creating a ClusterRoleBinding with the 'system:authenticated' group, ensuring that no one can create pods without a policy in place.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"By using RBAC and PSPs, you can enforce different security policies for different users or groups within your Kubernetes cluster, improving overall security and compliance.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:340\n",
      "Raw Output\n",
      "Here is the list of extracted highlights in JSON format:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"You’ll bind the psp-privileged ClusterRole only to Bob: $ kubectl create clusterrolebinding psp-bob --clusterrole=psp-privileged --user=bob\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"$ kubectl config set-credentials alice --username=alice --password=password User \\\"alice\\\" set.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"$ kubectl config set-credentials bob --username=bob --password=password User \\\"bob\\\" set.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"$ kubectl --user alice create -f pod-privileged.yaml Error from server (Forbidden): error when creating \\\"pod-privileged.yaml\\\": pods \\\"pod-privileged\\\" is forbidden: unable to validate against any pod security policy: [spec.containers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed]\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"$ kubectl --user bob create -f pod-privileged.yaml pod \\\"pod-privileged\\\" created\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"You’ll bind the psp-privileged ClusterRole only to Bob: $ kubectl create clusterrolebinding psp-bob --clusterrole=psp-privileged --user=bob\"\n",
      "  }]\n",
      "Done for page number:341\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page, wrapped up in JSON with the key \"highlight\":\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Network isolation can be secured by limiting which pods can talk to which pods using NetworkPolicy resources.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A default-deny NetworkPolicy prevents all clients from connecting to any pod in a given namespace.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Ingress rules in a NetworkPolicy have nothing to do with the Ingress resource discussed in chapter 5.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"NetworkPolicies can match pods based on label selector, namespace selector, or network IP block using CIDR notation.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Creating a default-deny NetworkPolicy will prevent all clients from connecting to any pod in the same namespace.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Network isolation can be secured by limiting which pods can talk to which pods using NetworkPolicy resources.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A default-deny NetworkPolicy prevents all clients from connecting to any pod in a given namespace.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Ingress rules in a NetworkPolicy have nothing to do with the Ingress resource discussed in chapter 5.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"NetworkPolicies can match pods based on label selector, namespace selector, or network IP block using CIDR notation.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Creating a default-deny NetworkPolicy will prevent all clients from connecting to any pod in the same namespace.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:342\n",
      "Raw Output\n",
      "[{\"highlight\": \"The CNI plugin or other type of networking solution used in the cluster must support NetworkPolicy, or else there will be no effect on inter-pod connectivity.\"}, {\"highlight\": \"To let clients connect to the pods in the namespace, you must now explicitly say who can connect to the pods. By who I mean which pods.\"}, {\"highlight\": \"The example NetworkPolicy allows pods with the app=webserver label to connect to pods with the app=database label, and only on port 5432.\"}, {\"highlight\": \"Client pods usually connect to server pods through a Service instead of directly to the pod, but that doesn’t change anything.\"}, {\"highlight\": \"The NetworkPolicy is enforced when connecting through a Service, as well.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The CNI plugin or other type of networking solution used in the cluster must support NetworkPolicy, or else there will be no effect on inter-pod connectivity.\"}, {\"highlight\": \"To let clients connect to the pods in the namespace, you must now explicitly say who can connect to the pods. By who I mean which pods.\"}, {\"highlight\": \"The example NetworkPolicy allows pods with the app=webserver label to connect to pods with the app=database label, and only on port 5432.\"}, {\"highlight\": \"Client pods usually connect to server pods through a Service instead of directly to the pod, but that doesn’t change anything.\"}, {\"highlight\": \"The NetworkPolicy is enforced when connecting through a Service, as well.\"}]\n",
      "Done for page number:343\n",
      "Raw Output\n",
      "Here is the list of extracted highlights in JSON format:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"To secure their microservice, they create the NetworkPolicy resource shown in the following listing.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"This policy applies to pods labeled as microservice= shopping-cart.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Only pods running in namespaces labeled as tenant=manning are allowed to access the microservice.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: shoppingcart-netpolicy spec:\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The NetworkPolicy resource is used to isolate the network between Kubernetes namespaces and secure a microservice for a specific tenant.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"To secure their microservice, they create the NetworkPolicy resource shown in the following listing.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"This policy applies to pods labeled as microservice= shopping-cart.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Only pods running in namespaces labeled as tenant=manning are allowed to access the microservice.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: shoppingcart-netpolicy spec:\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The NetworkPolicy resource is used to isolate the network between Kubernetes namespaces and secure a microservice for a specific tenant.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:344\n",
      "Raw Output\n",
      "[{\"highlight\": \"NetworkPolicy ensures only pods running in namespaces labeled as tenant: manning can access their Shopping Cart microservice.\"}, {\"highlight\": \"Tenants usually can’t add labels (or annotations) to their namespaces themselves, which prevents them from circumventing namespaceSelector-based ingress rules.\"}, {\"highlight\": \"You can specify an IP block in CIDR notation instead of a pod- or namespace selector to define who can access the pods targeted in the NetworkPolicy.\"}, {\"highlight\": \"The ingress rule only allows traffic from clients in the 192.168.1.0/24 IP block.\"}, {\"highlight\": \"A NetworkPolicy only allows pods in namespaces matching a namespaceSelector to access a specific pod.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"NetworkPolicy ensures only pods running in namespaces labeled as tenant: manning can access their Shopping Cart microservice.\"}, {\"highlight\": \"Tenants usually can’t add labels (or annotations) to their namespaces themselves, which prevents them from circumventing namespaceSelector-based ingress rules.\"}, {\"highlight\": \"You can specify an IP block in CIDR notation instead of a pod- or namespace selector to define who can access the pods targeted in the NetworkPolicy.\"}, {\"highlight\": \"The ingress rule only allows traffic from clients in the 192.168.1.0/24 IP block.\"}, {\"highlight\": \"A NetworkPolicy only allows pods in namespaces matching a namespaceSelector to access a specific pod.\"}]\n",
      "Done for page number:345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[{\"highlight\": \"NetworkPolicy can limit outbound traffic of a set of pods through egress rules.\"}, {\"highlight\": \"Egress rules allow pods to access specific pods or IP addresses, but not others.\"}, {\"highlight\": \"Pods can be configured to run as different user and/or group than the one defined in the container image.\"}, {\"highlight\": \"Cluster-level PodSecurityPolicy resources can be created to prevent users from creating pods that could compromise a node.\"}, {\"highlight\": \"NetworkPolicy resources are used to limit a pod's inbound and/or outbound traffic.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"NetworkPolicy can limit outbound traffic of a set of pods through egress rules.\"}, {\"highlight\": \"Egress rules allow pods to access specific pods or IP addresses, but not others.\"}, {\"highlight\": \"Pods can be configured to run as different user and/or group than the one defined in the container image.\"}, {\"highlight\": \"Cluster-level PodSecurityPolicy resources can be created to prevent users from creating pods that could compromise a node.\"}, {\"highlight\": \"NetworkPolicy resources are used to limit a pod's inbound and/or outbound traffic.\"}]\n",
      "Done for page number:346\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Managing pods' computational resources is a vital part of any pod definition.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Setting parameters makes sure that a pod takes only its fair share of the resources provided by the Kubernetes cluster.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Requesting CPU, memory, and other computational resources for containers is covered in this chapter.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Understanding Quality of Service guarantees for pods is an important aspect of resource management.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Limiting the total amount of resources available in a namespace is crucial for efficient cluster usage.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Managing pods' computational resources is a vital part of any pod definition.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Setting parameters makes sure that a pod takes only its fair share of the resources provided by the Kubernetes cluster.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Requesting CPU, memory, and other computational resources for containers is covered in this chapter.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Understanding Quality of Service guarantees for pods is an important aspect of resource management.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Limiting the total amount of resources available in a namespace is crucial for efficient cluster usage.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:347\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When creating a pod, you can specify the amount of CPU and memory that a container needs (these are called requests) and a hard limit on what it may consume (known as limits).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The pod's resource requests and limits are the sum of the requests and limits of all its containers.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"In the pod manifest, your single container requires one-fifth of a CPU core (200 millicores) to run properly.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"By doing that, you're saying that you expect the processes running inside the container to use at most 10 mebibytes of RAM.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The container requests 200 millicores (that is, 1/5 of a single CPU core's time) and also requests 10 mebibytes of memory.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When creating a pod, you can specify the amount of CPU and memory that a container needs (these are called requests) and a hard limit on what it may consume (known as limits).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The pod's resource requests and limits are the sum of the requests and limits of all its containers.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"In the pod manifest, your single container requires one-fifth of a CPU core (200 millicores) to run properly.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"By doing that, you're saying that you expect the processes running inside the container to use at most 10 mebibytes of RAM.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The container requests 200 millicores (that is, 1/5 of a single CPU core's time) and also requests 10 mebibytes of memory.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:348\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The dd command you’re running in the container consumes as much CPU as it can, but it only runs a single thread so it can only use a single core.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Specifying resource requests in a pod specifies the minimum amount of resources your pod needs, which is what the Scheduler uses when scheduling the pod to a node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Scheduler doesn’t look at how much of each individual resource is being used at the exact time of scheduling but at the sum of resources requested by the existing pods deployed on the node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods are scheduled based on their resource requests, not actual consumption, to maintain guarantees given to already deployed pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To limit a container’s CPU usage, you need to specify a CPU limit in the pod specification, which is different from specifying a resource request.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The dd command you’re running in the container consumes as much CPU as it can, but it only runs a single thread so it can only use a single core.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Specifying resource requests in a pod specifies the minimum amount of resources your pod needs, which is what the Scheduler uses when scheduling the pod to a node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Scheduler doesn’t look at how much of each individual resource is being used at the exact time of scheduling but at the sum of resources requested by the existing pods deployed on the node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods are scheduled based on their resource requests, not actual consumption, to maintain guarantees given to already deployed pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To limit a container’s CPU usage, you need to specify a CPU limit in the pod specification, which is different from specifying a resource request.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:349\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The Scheduler first filters the list of nodes to exclude those that the pod can’t fit on and then prioritizes the remaining nodes per the configured prioritization functions.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The LeastRequestedPriority function prefers nodes with fewer requested resources (with a greater amount of unallocated resources), whereas the MostRequestedPriority function prefers nodes that have the most requested resources.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"By configuring the Scheduler to use the Most-RequestedPriority function, you guarantee that Kubernetes will use the smallest possible number of nodes while still providing each pod with the amount of CPU/memory it requests.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Kubelet reports a node's capacity (CPU and memory) to the API server, making it available through the Pod C interface.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Scheduler only cares about resource requests, not actual usage or consumption, when selecting the best node for a pod.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The Scheduler first filters the list of nodes to exclude those that the pod can’t fit on and then prioritizes the remaining nodes per the configured prioritization functions.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The LeastRequestedPriority function prefers nodes with fewer requested resources (with a greater amount of unallocated resources), whereas the MostRequestedPriority function prefers nodes that have the most requested resources.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"By configuring the Scheduler to use the Most-RequestedPriority function, you guarantee that Kubernetes will use the smallest possible number of nodes while still providing each pod with the amount of CPU/memory it requests.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Kubelet reports a node's capacity (CPU and memory) to the API server, making it available through the Pod C interface.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Scheduler only cares about resource requests, not actual usage or consumption, when selecting the best node for a pod.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[{\"highlight\": \"The Node resource can be seen by using the kubectl describe command.\"}, {\"highlight\": \"The Scheduler bases its decisions only on the allocatable resource amounts.\"}, {\"highlight\": \"A pod requesting 800 millicores should have no problem being scheduled.\"}, {\"highlight\": \"Two pods deployed together have requested a total of 1,000 millicores or exactly 1 core.\"}, {\"highlight\": \"Deploying another pod with a resource request of 1,000 millicores may not be possible due to limited resources.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The Node resource can be seen by using the kubectl describe command.\"}, {\"highlight\": \"The Scheduler bases its decisions only on the allocatable resource amounts.\"}, {\"highlight\": \"A pod requesting 800 millicores should have no problem being scheduled.\"}, {\"highlight\": \"Two pods deployed together have requested a total of 1,000 millicores or exactly 1 core.\"}, {\"highlight\": \"Deploying another pod with a resource request of 1,000 millicores may not be possible due to limited resources.\"}]\n",
      "Done for page number:351\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"The pod hasn't been scheduled because it can't fit on any node due to insufficient CPU on your single node.\"}]\n",
      "\n",
      "[{\"highlight\": \"The sum of the CPU requests of all three pods equals 2,000 millicores or exactly two cores, which is exactly what your node can provide.\"}]\n",
      "\n",
      "[{\"highlight\": \"You can figure out why the pod isn't being scheduled by inspecting the node resource using the kubectl describe node command.\"}]\n",
      "\n",
      "[{\"highlight\": \"The output of kubectl describe node shows that there are 7 non-terminated pods on the node, with a total CPU request of 200m (10%) and a total memory request of 10Mi (0%).\"}]\n",
      "\n",
      "[{\"highlight\": \"Scheduling has failed because of insufficient CPU, even though the sum of the CPU requests of all three pods equals exactly two cores.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The pod hasn't been scheduled because it can't fit on any node due to insufficient CPU on your single node.\"}]\n",
      "Done for page number:352\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\"highlight\": \"The total CPU requests by running pods have exceeded the initial request, indicating additional resources are being consumed.\"},\n",
      "    {\"highlight\": \"Three pods in the kube-system namespace have explicitly requested CPU resources, leaving only 725 millicores available for additional pods.\"},\n",
      "    {\"highlight\": \"Deleting one of the first two pods will free up enough CPU to schedule the third pod, as shown in Listing 14.6.\"},\n",
      "    {\"highlight\": \"Pods' CPU requests play a role elsewhere while the pod is running, unlike memory requests which only affect scheduling.\"},\n",
      "    {\"highlight\": \"The Scheduler will be notified of pod deletions through the watch mechanism and reschedule pods accordingly.\"}\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\"highlight\": \"The total CPU requests by running pods have exceeded the initial request, indicating additional resources are being consumed.\"},\n",
      "    {\"highlight\": \"Three pods in the kube-system namespace have explicitly requested CPU resources, leaving only 725 millicores available for additional pods.\"},\n",
      "    {\"highlight\": \"Deleting one of the first two pods will free up enough CPU to schedule the third pod, as shown in Listing 14.6.\"},\n",
      "    {\"highlight\": \"Pods' CPU requests play a role elsewhere while the pod is running, unlike memory requests which only affect scheduling.\"},\n",
      "    {\"highlight\": \"The Scheduler will be notified of pod deletions through the watch mechanism and reschedule pods accordingly.\"}\n",
      "]\n",
      "Done for page number:353\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes distinguishes between resource requests and limits, where requests affect scheduling and determine how unused CPU time is distributed between pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"CPU requests are in a 1 to 5 ratio, so if one pod consumes as much CPU as it can, the other pod will get 83.3% of the CPU time and the first pod will get 16.7%.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If one container is idle, the other container can use up to 100% of the available CPU time, but as soon as the idle container needs CPU time, it will be throttled back.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes allows you to add custom resources to a node and request them in the pod's resource requests, which were initially known as Opaque Integer Resources but replaced with Extended Resources in Kubernetes version 1.8.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Unused CPU time is distributed between containers based on their CPU requests, as shown in Figure 14.2, where Pod A gets 16.7% and Pod B gets 83.3% of the available CPU time.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes distinguishes between resource requests and limits, where requests affect scheduling and determine how unused CPU time is distributed between pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"CPU requests are in a 1 to 5 ratio, so if one pod consumes as much CPU as it can, the other pod will get 83.3% of the CPU time and the first pod will get 16.7%.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If one container is idle, the other container can use up to 100% of the available CPU time, but as soon as the idle container needs CPU time, it will be throttled back.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes allows you to add custom resources to a node and request them in the pod's resource requests, which were initially known as Opaque Integer Resources but replaced with Extended Resources in Kubernetes version 1.8.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Unused CPU time is distributed between containers based on their CPU requests, as shown in Figure 14.2, where Pod A gets 16.7% and Pod B gets 83.3% of the available CPU time.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:354\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To make Kubernetes aware of a custom resource, add it to the Node object's capacity field by performing a PATCH HTTP request.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When creating pods, specify the same resource name and requested quantity under resources.requests in the container spec or with --requests using kubectl run.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes allows you to specify resource limits for every container (along with resource requests) to prevent a malfunctioning or malicious pod from affecting other pods on the node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Resource limits can be set for CPU and memory, but memory is an incompressible resource that cannot be throttled without affecting the process running in the container.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A single malfunctioning or malicious pod can practically make a whole node unusable if not limited by resource limits, especially when it comes to memory usage.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To make Kubernetes aware of a custom resource, add it to the Node object's capacity field by performing a PATCH HTTP request.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When creating pods, specify the same resource name and requested quantity under resources.requests in the container spec or with --requests using kubectl run.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes allows you to specify resource limits for every container (along with resource requests) to prevent a malfunctioning or malicious pod from affecting other pods on the node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Resource limits can be set for CPU and memory, but memory is an incompressible resource that cannot be throttled without affecting the process running in the container.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A single malfunctioning or malicious pod can practically make a whole node unusable if not limited by resource limits, especially when it comes to memory usage.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:355\n",
      "Raw Output\n",
      "[{\"highlight\": \"This pod's container has resource limits configured for both CPU and memory.\"}, {\"highlight\": \"Resource limits aren't constrained by the node's allocatable resource amounts, allowing them to exceed 100% of the node's capacity.\"}, {\"highlight\": \"Certain containers will need to be killed when 100% of the node's resources are used up.\"}, {\"highlight\": \"Individual containers can be killed even if they try to use more than their resource limits specify.\"}, {\"highlight\": \"Resource requests are set to the same values as the resource limits by default.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"This pod's container has resource limits configured for both CPU and memory.\"}, {\"highlight\": \"Resource limits aren't constrained by the node's allocatable resource amounts, allowing them to exceed 100% of the node's capacity.\"}, {\"highlight\": \"Certain containers will need to be killed when 100% of the node's resources are used up.\"}, {\"highlight\": \"Individual containers can be killed even if they try to use more than their resource limits specify.\"}, {\"highlight\": \"Resource requests are set to the same values as the resource limits by default.\"}]\n",
      "Done for page number:356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When a process running in a container tries to use a greater amount of resources than it's allowed to, Kubernetes will kill the process (OOMKilled) if the pod's restart policy is set to Always or OnFailure.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If a pod's restart policy is set to Always or OnFailure and the process keeps going over the memory limit and getting killed, Kubernetes will begin restarting it with increasing delays between restarts, resulting in a CrashLoopBackOff status.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Kubelet increases the time period before restarting the container after each crash, starting from 10 seconds and exponentially increasing to 20, 40, 80, and 160 seconds, and finally limited to 300 seconds.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To examine why a container crashed, you can check the pod's log and/or use the kubectl describe pod command to get information about the terminated container, including its reason for termination (OOMKilled).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A CrashLoopBackOff status doesn't mean the Kubelet has given up; it means that after each crash, the Kubelet is increasing the time period before restarting the container.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When a process running in a container tries to use a greater amount of resources than it's allowed to, Kubernetes will kill the process (OOMKilled) if the pod's restart policy is set to Always or OnFailure.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If a pod's restart policy is set to Always or OnFailure and the process keeps going over the memory limit and getting killed, Kubernetes will begin restarting it with increasing delays between restarts, resulting in a CrashLoopBackOff status.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Kubelet increases the time period before restarting the container after each crash, starting from 10 seconds and exponentially increasing to 20, 40, 80, and 160 seconds, and finally limited to 300 seconds.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To examine why a container crashed, you can check the pod's log and/or use the kubectl describe pod command to get information about the terminated container, including its reason for termination (OOMKilled).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A CrashLoopBackOff status doesn't mean the Kubelet has given up; it means that after each crash, the Kubelet is increasing the time period before restarting the container.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:357\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The OOMKilled status tells you that the container was killed because it was out of memory.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Containers can get OOMKilled even if they aren't over their limit due to how apps in containers see limits.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When running the top command in a container, the output shows the node's memory usage, not the container's memory limit.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The top command shows the main process using only 50% of available CPU time despite being set to use one core.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Containers always see the node's memory, not their own memory limit, which can lead to unexpected behavior and OOMKilled status.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The OOMKilled status tells you that the container was killed because it was out of memory.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Containers can get OOMKilled even if they aren't over their limit due to how apps in containers see limits.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When running the top command in a container, the output shows the node's memory usage, not the container's memory limit.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The top command shows the main process using only 50% of available CPU time despite being set to use one core.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Containers always see the node's memory, not their own memory limit, which can lead to unexpected behavior and OOMKilled status.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:358\n",
      "Raw Output\n",
      "[{\"highlight\": \"When running Java apps, especially if you don’t specify the maximum heap size for the JVM with the -Xmx option, the JVM will set the maximum heap size based on the host’s total memory instead of the memory available to the container.\"}, {\"highlight\": \"A container with a one-core CPU limit running on a 64-core CPU will get 1/64th of the overall CPU time, and its processes will not run on only one core at different points in time.\"}, {\"highlight\": \"Certain applications look up the number of CPUs on the system to decide how many worker threads they should run, which can cause issues when deployed on a node with a much bigger number of cores.\"}, {\"highlight\": \"You may want to use the Downward API to pass the CPU limit to the container and use it instead of relying on the number of CPUs your app can see on the system.\"}, {\"highlight\": \"New versions of Java alleviate the problem by taking the configured container limits into account, constraining both heap size and off-heap memory.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"When running Java apps, especially if you don’t specify the maximum heap size for the JVM with the -Xmx option, the JVM will set the maximum heap size based on the host’s total memory instead of the memory available to the container.\"}, {\"highlight\": \"A container with a one-core CPU limit running on a 64-core CPU will get 1/64th of the overall CPU time, and its processes will not run on only one core at different points in time.\"}, {\"highlight\": \"Certain applications look up the number of CPUs on the system to decide how many worker threads they should run, which can cause issues when deployed on a node with a much bigger number of cores.\"}, {\"highlight\": \"You may want to use the Downward API to pass the CPU limit to the container and use it instead of relying on the number of CPUs your app can see on the system.\"}, {\"highlight\": \"New versions of Java alleviate the problem by taking the configured container limits into account, constraining both heap size and off-heap memory.\"}]\n",
      "Done for page number:359\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes categorizes pods into three Quality of Service (QoS) classes: BestEffort, Burstable, and Guaranteed.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The QoS class is derived from the combination of resource requests and limits for a pod's containers, not through a separate field in the manifest.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A pod with no resource requests or limits set is assigned to the BestEffort class, which has the lowest priority and may get almost no CPU time at all.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A Guaranteed QoS class is given to pods whose containers' requests are equal to the limits for all resources, requiring matching requests and limits for both CPU and memory.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes can't make a proper decision on its own when it comes to killing a pod due to resource constraints; instead, it relies on QoS classes to prioritize pods in such cases.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes categorizes pods into three Quality of Service (QoS) classes: BestEffort, Burstable, and Guaranteed.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The QoS class is derived from the combination of resource requests and limits for a pod's containers, not through a separate field in the manifest.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A pod with no resource requests or limits set is assigned to the BestEffort class, which has the lowest priority and may get almost no CPU time at all.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A Guaranteed QoS class is given to pods whose containers' requests are equal to the limits for all resources, requiring matching requests and limits for both CPU and memory.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes can't make a proper decision on its own when it comes to killing a pod due to resource constraints; instead, it relies on QoS classes to prioritize pods in such cases.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:360\n",
      "Raw Output\n",
      "[{\"highlight\": \"The pod to be Guaranteed gets the requested amount of resources, but cannot consume additional ones.\"}, {\"highlight\": \"Burstable pods get the amount of resources they request, but are allowed to use additional resources (up to the limit) if needed.\"}, {\"highlight\": \"All three QoS classes and their relationships with requests and limits are shown in figure 14.4.\"}, {\"highlight\": \"For single-container pods, the QoS class applies to the pod as well.\"}, {\"highlight\": \"Requests and limits are not set, Requests are below limits, Requests equal limits are the conditions for BestEffort, Burstable and Guaranteed QoS classes respectively.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The pod to be Guaranteed gets the requested amount of resources, but cannot consume additional ones.\"}, {\"highlight\": \"Burstable pods get the amount of resources they request, but are allowed to use additional resources (up to the limit) if needed.\"}, {\"highlight\": \"All three QoS classes and their relationships with requests and limits are shown in figure 14.4.\"}, {\"highlight\": \"For single-container pods, the QoS class applies to the pod as well.\"}, {\"highlight\": \"Requests and limits are not set, Requests are below limits, Requests equal limits are the conditions for BestEffort, Burstable and Guaranteed QoS classes respectively.\"}]\n",
      "Done for page number:361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[{\"highlight\": \"If only requests are set, but not limits, refer to the table rows where requests are less than the limits.\"}, {\"highlight\": \"For multi-container pods, if all the containers have the same QoS class, that’s also the pod’s QoS class.\"}, {\"highlight\": \"A pod’s QoS class is shown when running kubectl describe pod and in the pod’s YAML/JSON manifest in the status.qosClass field.\"}, {\"highlight\": \"The QoS class of a single-container pod based on resource requests and limits is as follows: CPU requests vs. limits, Memory requests vs. limits.\"}, {\"highlight\": \"A Pod’s QoS class derived from the classes of its containers can be BestEffort, Burstable, or Guaranteed.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"If only requests are set, but not limits, refer to the table rows where requests are less than the limits.\"}, {\"highlight\": \"For multi-container pods, if all the containers have the same QoS class, that’s also the pod’s QoS class.\"}, {\"highlight\": \"A pod’s QoS class is shown when running kubectl describe pod and in the pod’s YAML/JSON manifest in the status.qosClass field.\"}, {\"highlight\": \"The QoS class of a single-container pod based on resource requests and limits is as follows: CPU requests vs. limits, Memory requests vs. limits.\"}, {\"highlight\": \"A Pod’s QoS class derived from the classes of its containers can be BestEffort, Burstable, or Guaranteed.\"}]\n",
      "Done for page number:362\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When the system is overcommitted, the QoS classes determine which container gets killed first so the freed resources can be given to higher priority pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The process running in the BestEffort pod will always be killed before the one in the Burstable pod, and a Guaranteed pod's processes are only killed if system processes need memory.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Each running process has an OutOfMemory (OOM) score. The system selects the process to kill by comparing OOM scores of all the running processes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The system will kill the container using more of its requested memory when two single-container pods exist, both in the Burstable class.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Guaranteed QoS pod's processes are only killed if system processes need memory and BestEffort QoS pod's process will always be killed before any Guaranteed pods' processes are killed.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When the system is overcommitted, the QoS classes determine which container gets killed first so the freed resources can be given to higher priority pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The process running in the BestEffort pod will always be killed before the one in the Burstable pod, and a Guaranteed pod's processes are only killed if system processes need memory.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Each running process has an OutOfMemory (OOM) score. The system selects the process to kill by comparing OOM scores of all the running processes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The system will kill the container using more of its requested memory when two single-container pods exist, both in the Burstable class.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Guaranteed QoS pod's processes are only killed if system processes need memory and BestEffort QoS pod's process will always be killed before any Guaranteed pods' processes are killed.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:363\n",
      "Raw Output\n",
      "[{\"highlight\": \"Setting default requests and limits for pods per namespace can be done by creating a LimitRange resource.\"}, {\"highlight\": \"The LimitRange resource allows you to specify minimum and maximum limits, as well as default resource requests for containers that don't specify them explicitly.\"}, {\"highlight\": \"If no requests or limits are specified in a pod's manifest, the API server will reject it because they are outside the min/max values defined by the LimitRange.\"}, {\"highlight\": \"When a pod's manifest is missing requests and limits, the Defaulting process applies the default resource requests and limits set by the LimitRange.\"}, {\"highlight\": \"A LimitRange is used for validation and defaulting pods in a namespace.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Setting default requests and limits for pods per namespace can be done by creating a LimitRange resource.\"}, {\"highlight\": \"The LimitRange resource allows you to specify minimum and maximum limits, as well as default resource requests for containers that don't specify them explicitly.\"}, {\"highlight\": \"If no requests or limits are specified in a pod's manifest, the API server will reject it because they are outside the min/max values defined by the LimitRange.\"}, {\"highlight\": \"When a pod's manifest is missing requests and limits, the Defaulting process applies the default resource requests and limits set by the LimitRange.\"}, {\"highlight\": \"A LimitRange is used for validation and defaulting pods in a namespace.\"}]\n",
      "Done for page number:364\n",
      "Raw Output\n",
      "[{\"highlight\": \"LimitRange resources are used by the LimitRanger Admission Control plugin to validate pod specs.\"}, {\"highlight\": \"Limits specified in a LimitRange resource apply to individual pods/containers, not total resources across all pods in the namespace.\"}, {\"highlight\": \"ResourceQuota objects specify total amount of resources available across all pods in a namespace.\"}, {\"highlight\": \"A full example of a LimitRange object includes properties such as min and max CPU/memory limits, default requests, and maximum limit request ratio.\"}, {\"highlight\": \"The LimitRanger plugin rejects pod manifests if validation fails due to exceeding node resource limits.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"LimitRange resources are used by the LimitRanger Admission Control plugin to validate pod specs.\"}, {\"highlight\": \"Limits specified in a LimitRange resource apply to individual pods/containers, not total resources across all pods in the namespace.\"}, {\"highlight\": \"ResourceQuota objects specify total amount of resources available across all pods in a namespace.\"}, {\"highlight\": \"A full example of a LimitRange object includes properties such as min and max CPU/memory limits, default requests, and maximum limit request ratio.\"}, {\"highlight\": \"The LimitRanger plugin rejects pod manifests if validation fails due to exceeding node resource limits.\"}]\n",
      "Done for page number:365\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can configure minimum and maximum limits for a whole pod, which apply to the sum of all the pod’s containers’ requests and limits.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"At the container level, you can set default resource requests (defaultRequest) and default limits (default) that will be applied to each container that doesn’t specify them explicitly.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A LimitRange object allows you to limit the amount of storage a single PVC can request, similarly to how you limit CPU and memory for containers.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Limits from multiple LimitRange objects are consolidated when validating a pod or PVC, but existing pods and PVCs will not be revalidated if limits are modified afterwards.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A LimitRange can enforce limits by preventing the creation of pods that request more CPU than allowed, as shown in listing 14.11.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can configure minimum and maximum limits for a whole pod, which apply to the sum of all the pod’s containers’ requests and limits.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"At the container level, you can set default resource requests (defaultRequest) and default limits (default) that will be applied to each container that doesn’t specify them explicitly.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A LimitRange object allows you to limit the amount of storage a single PVC can request, similarly to how you limit CPU and memory for containers.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Limits from multiple LimitRange objects are consolidated when validating a pod or PVC, but existing pods and PVCs will not be revalidated if limits are modified afterwards.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A LimitRange can enforce limits by preventing the creation of pods that request more CPU than allowed, as shown in listing 14.11.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The pod's single container is requesting two CPUs, which is more than the maximum you set in the LimitRange earlier.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"maximum cpu usage per Pod is 1, but request is 2., maximum cpu usage per Container is 1, but request is 2.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"default resource requests and limits are set on containers that don’t specify them\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can confirm this by describing the kubia-manual pod, as shown in the following listing.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The container’s requests and limits match the ones you specified in the LimitRange object\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The pod's single container is requesting two CPUs, which is more than the maximum you set in the LimitRange earlier.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"maximum cpu usage per Pod is 1, but request is 2., maximum cpu usage per Container is 1, but request is 2.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"default resource requests and limits are set on containers that don’t specify them\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can confirm this by describing the kubia-manual pod, as shown in the following listing.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The container’s requests and limits match the ones you specified in the LimitRange object\"\n",
      "    }\n",
      "]\n",
      "Done for page number:367\n",
      "Raw Output\n",
      "Here are the extracted highlights in JSON format, collated into a list:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"LimitRanges only apply to individual pods, but cluster admins also need a way to limit the total amount of resources available in a namespace.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A ResourceQuota object limits the amount of computational resources the pods and the amount of storage PersistentVolumeClaims in a namespace can consume.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"ResourceQuotas are enforced at pod creation time, so creating one has no effect on existing pods.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A ResourceQuota object can limit the number of pods, claims, and other API objects users are allowed to create inside a namespace.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To specify quotas for CPU and memory, create a ResourceQuota object with 'requests.cpu', 'requests.memory', 'limits.cpu', and 'limits.memory' specifications.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"LimitRanges only apply to individual pods, but cluster admins also need a way to limit the total amount of resources available in a namespace.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A ResourceQuota object limits the amount of computational resources the pods and the amount of storage PersistentVolumeClaims in a namespace can consume.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"ResourceQuotas are enforced at pod creation time, so creating one has no effect on existing pods.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A ResourceQuota object can limit the number of pods, claims, and other API objects users are allowed to create inside a namespace.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To specify quotas for CPU and memory, create a ResourceQuota object with 'requests.cpu', 'requests.memory', 'limits.cpu', and 'limits.memory' specifications.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:368\n",
      "Raw Output\n",
      "[{\"highlight\": \"A ResourceQuota object applies to the namespace it's created in, like a LimitRange, but it applies to all the pods' resource requests and limits in total.\"}, {\"highlight\": \"The maximum total CPU limits in the namespace are set to 600 millicores, whereas the maximum total requests are set to 400 millicores for CPU and 200 MiB for memory.\"}, {\"highlight\": \"You can use the kubectl describe command to see how much of the quota is already used up.\"}, {\"highlight\": \"The Used column in the output matches the resource requests and limits of all running pods in the namespace.\"}, {\"highlight\": \"LimitRanges apply to individual pods, while ResourceQuotas apply to all pods in the namespace.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"A ResourceQuota object applies to the namespace it's created in, like a LimitRange, but it applies to all the pods' resource requests and limits in total.\"}, {\"highlight\": \"The maximum total CPU limits in the namespace are set to 600 millicores, whereas the maximum total requests are set to 400 millicores for CPU and 200 MiB for memory.\"}, {\"highlight\": \"You can use the kubectl describe command to see how much of the quota is already used up.\"}, {\"highlight\": \"The Used column in the output matches the resource requests and limits of all running pods in the namespace.\"}, {\"highlight\": \"LimitRanges apply to individual pods, while ResourceQuotas apply to all pods in the namespace.\"}]\n",
      "Done for page number:369\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When creating a ResourceQuota, you must also create a LimitRange object alongside it to specify resource requests or limits for pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A ResourceQuota can limit the amount of persistent storage that can be claimed in a namespace, with separate quotas for different StorageClasses.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"ResourceQuotas can also be configured to limit the number of Pods, Replication-Controllers, Services, and other objects inside a single namespace.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The amount of storage claimable by PersistentVolumeClaims in a namespace is limited to 500 GiB, with separate quotas for SSD (300 GiB) and HDD (1 TiB) StorageClasses.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes requires pods to have resource requests or limits set when a quota for a specific resource (CPU or memory) is configured in the namespace.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When creating a ResourceQuota, you must also create a LimitRange object alongside it to specify resource requests or limits for pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A ResourceQuota can limit the amount of persistent storage that can be claimed in a namespace, with separate quotas for different StorageClasses.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"ResourceQuotas can also be configured to limit the number of Pods, Replication-Controllers, Services, and other objects inside a single namespace.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The amount of storage claimable by PersistentVolumeClaims in a namespace is limited to 500 GiB, with separate quotas for SSD (300 GiB) and HDD (1 TiB) StorageClasses.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes requires pods to have resource requests or limits set when a quota for a specific resource (CPU or memory) is configured in the namespace.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:370\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"A ResourceQuota object that limits the number of objects may look like: apiVersion: v1, kind: ResourceQuota, metadata: name: objects, spec: hard: pods: 10, replicationcontrollers: 5, secrets: 10, configmaps: 10, persistentvolumeclaims: 4, services: 5, services.loadbalancers: 1, services.nodeports: 2\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The ResourceQuota in this listing allows users to create at most 10 Pods in the namespace, regardless if they’re created manually or by a ReplicationController, Replica-Set, DaemonSet, Job, and so on.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Object count quotas can currently be set for the following objects: Pods, ReplicationControllers, Secrets, ConfigMaps, PersistentVolumeClaims, Services (in general), and for two specific types of Services, such as Load-Balancer Services and NodePort Services\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Only 10 Pods, 5 ReplicationControllers, 10 Secrets, 10 ConfigMaps, and 4 PersistentVolumeClaims can be created in the namespace.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Five Services overall can be created, of which at most one can be a LoadBalancer Service and at most two can be NodePort Services.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"A ResourceQuota object that limits the number of objects may look like: apiVersion: v1, kind: ResourceQuota, metadata: name: objects, spec: hard: pods: 10, replicationcontrollers: 5, secrets: 10, configmaps: 10, persistentvolumeclaims: 4, services: 5, services.loadbalancers: 1, services.nodeports: 2\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The ResourceQuota in this listing allows users to create at most 10 Pods in the namespace, regardless if they’re created manually or by a ReplicationController, Replica-Set, DaemonSet, Job, and so on.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Object count quotas can currently be set for the following objects: Pods, ReplicationControllers, Secrets, ConfigMaps, PersistentVolumeClaims, Services (in general), and for two specific types of Services, such as Load-Balancer Services and NodePort Services\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Only 10 Pods, 5 ReplicationControllers, 10 Secrets, 10 ConfigMaps, and 4 PersistentVolumeClaims can be created in the namespace.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Five Services overall can be created, of which at most one can be a LoadBalancer Service and at most two can be NodePort Services.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"You can specify quotas for specific pod states and/or QoS classes, including BestEffort, NotBestEffort, Terminating, and NotTerminating scopes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The BestEffort and NotBestEffort scopes determine whether the quota applies to pods with the BestEffort QoS class or with one of the other two classes (that is, Burstable and Guaranteed).\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can specify how long each pod is allowed to run before it's terminated and marked as Failed by setting the activeDeadlineSeconds field in the pod spec.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A pod must match all the specified scopes for the quota to apply to it, and what a quota can limit depends on the quota's scope.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can create a ResourceQuota object that applies only to BestEffort, NotTerminating pods by specifying the corresponding scopes in the spec section.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"You can specify quotas for specific pod states and/or QoS classes, including BestEffort, NotBestEffort, Terminating, and NotTerminating scopes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The BestEffort and NotBestEffort scopes determine whether the quota applies to pods with the BestEffort QoS class or with one of the other two classes (that is, Burstable and Guaranteed).\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can specify how long each pod is allowed to run before it's terminated and marked as Failed by setting the activeDeadlineSeconds field in the pod spec.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A pod must match all the specified scopes for the quota to apply to it, and what a quota can limit depends on the quota's scope.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can create a ResourceQuota object that applies only to BestEffort, NotTerminating pods by specifying the corresponding scopes in the spec section.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:372\n",
      "Raw Output\n",
      "[{\"highlight\": \"Properly setting resource requests and limits is crucial for getting the most out of your Kubernetes cluster.\"}, \n",
      " {\"highlight\": \"Monitoring the actual resource usage of your containers under expected load levels helps find the sweet spot for requests and limits.\"}, \n",
      " {\"highlight\": \"cAdvisor, an agent in Kubelet, collects basic resource consumption data for individual containers and nodes.\"}, \n",
      " {\"highlight\": \"Heapster, a pod running on one node, collects metrics from all cAdvisors and exposes them in a single location.\"}, \n",
      " {\"highlight\": \"Monitoring apps running in Kubernetes is essential to adjust resource requests and limits as required.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Properly setting resource requests and limits is crucial for getting the most out of your Kubernetes cluster.\"}, \n",
      " {\"highlight\": \"Monitoring the actual resource usage of your containers under expected load levels helps find the sweet spot for requests and limits.\"}, \n",
      " {\"highlight\": \"cAdvisor, an agent in Kubelet, collects basic resource consumption data for individual containers and nodes.\"}, \n",
      " {\"highlight\": \"Heapster, a pod running on one node, collects metrics from all cAdvisors and exposes them in a single location.\"}, \n",
      " {\"highlight\": \"Monitoring apps running in Kubernetes is essential to adjust resource requests and limits as required.\"}]\n",
      "Done for page number:373\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Heapster is a component that connects to all cAdvisors, which collect container and node usage data without having to talk to the processes running inside the pods' containers.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To run Heapster manually in other types of Kubernetes clusters, you can refer to instructions located at https://github.com/kubernetes/heapster.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Running Heapster in your cluster makes it possible to obtain resource usages for nodes and individual pods through the kubectl top command.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The kubectl top node command shows the actual, current CPU and memory usage of all the pods running on the node, unlike the kubectl describe node command which shows the amount of CPU and memory requests and limits instead of actual runtime usage data.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To see how much each individual pod is using, you can use the kubectl top pod command, as shown in the following listing: $ kubectl top pod --all-namespaces\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Heapster is a component that connects to all cAdvisors, which collect container and node usage data without having to talk to the processes running inside the pods' containers.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To run Heapster manually in other types of Kubernetes clusters, you can refer to instructions located at https://github.com/kubernetes/heapster.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Running Heapster in your cluster makes it possible to obtain resource usages for nodes and individual pods through the kubectl top command.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The kubectl top node command shows the actual, current CPU and memory usage of all the pods running on the node, unlike the kubectl describe node command which shows the amount of CPU and memory requests and limits instead of actual runtime usage data.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To see how much each individual pod is using, you can use the kubectl top pod command, as shown in the following listing: $ kubectl top pod --all-namespaces\"\n",
      "  }\n",
      "]\n",
      "Done for page number:374\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The kubectl top command may refuse to show metrics for a pod, but waiting a few minutes and rerunning the command should resolve the issue.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To see resource usages across individual containers instead of pods, use the --containers option with the kubectl top command.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"InfluxDB is an open source time-series database ideal for storing application metrics and other monitoring data.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Grafana is an analytics and visualization suite that allows you to visualize data stored in InfluxDB and discover how resource usage behaves over time.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Google Cloud Monitoring can be used to monitor Kubernetes clusters on Google Kubernetes Engine, but InfluxDB and Grafana are commonly used for local clusters.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The kubectl top command may refuse to show metrics for a pod, but waiting a few minutes and rerunning the command should resolve the issue.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To see resource usages across individual containers instead of pods, use the --containers option with the kubectl top command.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"InfluxDB is an open source time-series database ideal for storing application metrics and other monitoring data.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Grafana is an analytics and visualization suite that allows you to visualize data stored in InfluxDB and discover how resource usage behaves over time.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Google Cloud Monitoring can be used to monitor Kubernetes clusters on Google Kubernetes Engine, but InfluxDB and Grafana are commonly used for local clusters.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:375\n",
      "Raw Output\n",
      "Here are the extracted highlights in JSON format, collated into a list:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Both InfluxDB and Grafana can run as pods and necessary manifests are available in the Heapster Git repository at http://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When using Minikube, you don’t even need to deploy them manually, because they’re deployed along with Heapster when you enable the Heapster add-on.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To discover how much of each resource your pod requires over time, open the Grafana web console and explore the predefined dashboards.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can find out the URL of Grafana’s web console with kubectl cluster-info: $ kubectl cluster-info...\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Grafana dashboard showing CPU usage across the cluster is available at https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Both InfluxDB and Grafana can run as pods and necessary manifests are available in the Heapster Git repository at http://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When using Minikube, you don’t even need to deploy them manually, because they’re deployed along with Heapster when you enable the Heapster add-on.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To discover how much of each resource your pod requires over time, open the Grafana web console and explore the predefined dashboards.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can find out the URL of Grafana’s web console with kubectl cluster-info: $ kubectl cluster-info...\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Grafana dashboard showing CPU usage across the cluster is available at https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana\"\n",
      "    }\n",
      "]\n",
      "Done for page number:376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"When using Minikube, Grafana's web console is exposed through a NodePort Service, so you can open it in your browser with the following command: $ minikube service monitoring-grafana -n kube-system\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To see the resource usage statistics of the nodes, open the Cluster dashboard. There you’ll see several charts showing the overall cluster usage, usage by node, and the individual usage for CPU, memory, network, and filesystem.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can quickly see if the resource requests or limits you’ve set for your pods need to be raised or whether they can be lowered to allow more pods to fit on your nodes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A pod is only guaranteed as much of a resource as it requests through resource requests. Your pod may be running fine now, but when other pods are deployed to the same node and start using the CPU, your pod’s CPU time may be throttled.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The unused memory is therefore wasted. You should decrease the pod's memory request to make the memory available to other pods running on the node.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"When using Minikube, Grafana's web console is exposed through a NodePort Service, so you can open it in your browser with the following command: $ minikube service monitoring-grafana -n kube-system\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To see the resource usage statistics of the nodes, open the Cluster dashboard. There you’ll see several charts showing the overall cluster usage, usage by node, and the individual usage for CPU, memory, network, and filesystem.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can quickly see if the resource requests or limits you’ve set for your pods need to be raised or whether they can be lowered to allow more pods to fit on your nodes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A pod is only guaranteed as much of a resource as it requests through resource requests. Your pod may be running fine now, but when other pods are deployed to the same node and start using the CPU, your pod’s CPU time may be throttled.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The unused memory is therefore wasted. You should decrease the pod's memory request to make the memory available to other pods running on the node.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:377\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Specifying resource requests helps Kubernetes schedule pods across the cluster.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Unused CPU time is allocated based on containers’ CPU requests.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You should increase the CPU request when actual CPU usage is higher than what was requested.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You’ve reserved too much memory for this app, wasting memory that won’t ever be used by this app or other apps. You should decrease the memory request.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Specifying resource limits keeps pods from starving other pods of resources and containers are killed if they try to use too much memory or CPU in an overcommitted system.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Specifying resource requests helps Kubernetes schedule pods across the cluster.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Unused CPU time is allocated based on containers’ CPU requests.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You should increase the CPU request when actual CPU usage is higher than what was requested.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You’ve reserved too much memory for this app, wasting memory that won’t ever be used by this app or other apps. You should decrease the memory request.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Specifying resource limits keeps pods from starving other pods of resources and containers are killed if they try to use too much memory or CPU in an overcommitted system.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:378\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page in JSON format:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can use LimitRange objects to define the minimum, maximum, and default resource requests and limits for individual pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use ResourceQuota objects to limit the amount of resources available to all the pods in a namespace.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To know how high to set a pod's resource requests and limits, you need to monitor how the pod uses resources over a long-enough time period.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"In the next chapter, you'll see how these metrics can be used by Kubernetes to automatically scale your pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use LimitRange and ResourceQuota objects together to manage pods' computational resources effectively.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can use LimitRange objects to define the minimum, maximum, and default resource requests and limits for individual pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use ResourceQuota objects to limit the amount of resources available to all the pods in a namespace.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To know how high to set a pod's resource requests and limits, you need to monitor how the pod uses resources over a long-enough time period.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"In the next chapter, you'll see how these metrics can be used by Kubernetes to automatically scale your pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use LimitRange and ResourceQuota objects together to manage pods' computational resources effectively.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:379\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Applications running in pods can be scaled out manually by increasing the replicas field in the ReplicationController, ReplicaSet, Deployment, or other scalable resource.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Pods can also be scaled vertically by increasing their container’s resource requests and limits (though this can currently only be done at pod creation time, not while the pod is running).\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"This chapter covers configuring automatic horizontal scaling of pods based on CPU utilization.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"This chapter also covers understanding why vertical scaling of pods isn’t possible yet.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"This chapter further covers understanding automatic horizontal scaling of cluster nodes.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Applications running in pods can be scaled out manually by increasing the replicas field in the ReplicationController, ReplicaSet, Deployment, or other scalable resource.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Pods can also be scaled vertically by increasing their container’s resource requests and limits (though this can currently only be done at pod creation time, not while the pod is running).\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"This chapter covers configuring automatic horizontal scaling of pods based on CPU utilization.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"This chapter also covers understanding why vertical scaling of pods isn’t possible yet.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"This chapter further covers understanding automatic horizontal scaling of cluster nodes.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:380\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes can monitor pods and scale them up automatically as soon as it detects an increase in CPU usage or other metric.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Horizontal pod autoscaling is the automatic scaling of the number of pod replicas managed by a controller, enabled and configured by creating a HorizontalPodAutoscaler (HPA) resource.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The autoscaling process involves obtaining metrics of all pods, calculating the number of pods required to meet the target metric value, and updating the replicas field of the scaled resource.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Heapster must be running in the cluster for autoscaling to work, as it collects pod and node metrics and provides them to the Horizontal Pod Autoscaler controller through REST calls.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes' autoscaling feature was completely rewritten between version 1.6 and 1.7, so outdated information may be found online.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes can monitor pods and scale them up automatically as soon as it detects an increase in CPU usage or other metric.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Horizontal pod autoscaling is the automatic scaling of the number of pod replicas managed by a controller, enabled and configured by creating a HorizontalPodAutoscaler (HPA) resource.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The autoscaling process involves obtaining metrics of all pods, calculating the number of pods required to meet the target metric value, and updating the replicas field of the scaled resource.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Heapster must be running in the cluster for autoscaling to work, as it collects pod and node metrics and provides them to the Horizontal Pod Autoscaler controller through REST calls.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes' autoscaling feature was completely rewritten between version 1.6 and 1.7, so outdated information may be found online.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Horizontal pod autoscaling should already be enabled in your cluster.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The Autoscaler calculates the replica count for each metric individually and then takes the highest value.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Prior to Kubernetes version 1.6, the HorizontalPodAutoscaler obtained the metrics from Heapster directly.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"From version 1.9, this behavior will be enabled by default for the Autoscaler to get metrics through an aggregated resource metrics API.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Selecting what metrics collector to use in their clusters will be up to cluster administrators.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Horizontal pod autoscaling should already be enabled in your cluster.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The Autoscaler calculates the replica count for each metric individually and then takes the highest value.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Prior to Kubernetes version 1.6, the HorizontalPodAutoscaler obtained the metrics from Heapster directly.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"From version 1.9, this behavior will be enabled by default for the Autoscaler to get metrics through an aggregated resource metrics API.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Selecting what metrics collector to use in their clusters will be up to cluster administrators.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:382\n",
      "Raw Output\n",
      "[{\"highlight\": \"The final step of an autoscaling operation is updating the desired replica count field on the scaled resource object (a ReplicaSet, for example) and then letting the Replica-Set controller take care of spinning up additional pods or deleting excess ones.\"}, {\"highlight\": \"Currently, it’s exposed for Deployments, ReplicaSets, ReplicationControllers, StatefulSets. These are currently the only objects you can attach an Autoscaler to.\"}, {\"highlight\": \"The Autoscaler controller modifies the replicas field of the scaled resource through the Scale sub-resource.\"}, {\"highlight\": \"Horizontal Pod Autoscaler modifies only on the Scale sub-resource.\"}, {\"highlight\": \"Autoscaler adjusts replicas (++ or --)\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The final step of an autoscaling operation is updating the desired replica count field on the scaled resource object (a ReplicaSet, for example) and then letting the Replica-Set controller take care of spinning up additional pods or deleting excess ones.\"}, {\"highlight\": \"Currently, it’s exposed for Deployments, ReplicaSets, ReplicationControllers, StatefulSets. These are currently the only objects you can attach an Autoscaler to.\"}, {\"highlight\": \"The Autoscaler controller modifies the replicas field of the scaled resource through the Scale sub-resource.\"}, {\"highlight\": \"Horizontal Pod Autoscaler modifies only on the Scale sub-resource.\"}, {\"highlight\": \"Autoscaler adjusts replicas (++ or --)\"}]\n",
      "Done for page number:383\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page in JSON format:\n",
      "\n",
      "[{\"highlight\": \"The autoscaling process involves three steps: scaling based on CPU utilization, scaling based on memory utilization, and scaling based on custom metrics.\"}, \n",
      " {\"highlight\": \"The Horizontal Pod Autoscaler (HPA) controller uses metrics data from cAdvisor, Heapster, and the Kubelet to determine when to scale replicas up or down.\"}, \n",
      " {\"highlight\": \"The autoscaling process is not immediate, as it takes time for metrics data to be propagated and a scaling action to be performed.\"}, \n",
      " {\"highlight\": \"CPU utilization is the most important metric for autoscaling, as it indicates whether pods can cope with demand and need to be scaled up or out.\"}, \n",
      " {\"highlight\": \"The HPA controller collects metrics from Heapster, which in turn collects metrics from all nodes, including cAdvisor, which collects metrics from all containers on a node.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The autoscaling process involves three steps: scaling based on CPU utilization, scaling based on memory utilization, and scaling based on custom metrics.\"}, \n",
      " {\"highlight\": \"The Horizontal Pod Autoscaler (HPA) controller uses metrics data from cAdvisor, Heapster, and the Kubelet to determine when to scale replicas up or down.\"}, \n",
      " {\"highlight\": \"The autoscaling process is not immediate, as it takes time for metrics data to be propagated and a scaling action to be performed.\"}, \n",
      " {\"highlight\": \"CPU utilization is the most important metric for autoscaling, as it indicates whether pods can cope with demand and need to be scaled up or out.\"}, \n",
      " {\"highlight\": \"The HPA controller collects metrics from Heapster, which in turn collects metrics from all nodes, including cAdvisor, which collects metrics from all containers on a node.\"}]\n",
      "Done for page number:384\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Always set the target CPU usage well below 100% (and definitely never above 90%) to leave enough room for handling sudden load spikes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Autoscaler compares the pod's actual CPU consumption and its CPU requests, which means the pods you're autoscaling need to have CPU requests set for the Autoscaler to determine the CPU utilization percentage.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You'll create a Deployment similar to the one in chapter 9, but as we've discussed, you'll need to make sure the pods created by the Deployment all have the CPU resource requests specified in order to make autoscaling possible.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Autoscaler is only concerned with the pod's guaranteed CPU amount (the CPU requests) when determining the CPU utilization of a pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You'll need to add a CPU resource request to the Deployment's pod template, as shown in Listing 15.1, for autoscaling to be possible.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Always set the target CPU usage well below 100% (and definitely never above 90%) to leave enough room for handling sudden load spikes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Autoscaler compares the pod's actual CPU consumption and its CPU requests, which means the pods you're autoscaling need to have CPU requests set for the Autoscaler to determine the CPU utilization percentage.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You'll create a Deployment similar to the one in chapter 9, but as we've discussed, you'll need to make sure the pods created by the Deployment all have the CPU resource requests specified in order to make autoscaling possible.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Autoscaler is only concerned with the pod's guaranteed CPU amount (the CPU requests) when determining the CPU utilization of a pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You'll need to add a CPU resource request to the Deployment's pod template, as shown in Listing 15.1, for autoscaling to be possible.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:385\n",
      "Raw Output\n",
      "Here are the extracted highlights in JSON format, collated into a list:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To enable horizontal autoscaling of its pods, you need to create a HorizontalPodAutoscaler (HPA) object and point it to the Deployment.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use the kubectl autoscale command: $ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Autoscaler will constantly keep adjusting the number of replicas to keep their CPU utilization around 30%, but it will never scale down to less than one or scale up to more than five replicas.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Always make sure to autoscale Deployments instead of the underlying ReplicaSets, to ensure the desired replica count is preserved across application updates.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The HorizontalPodAutoscaler resource has a name, minimum and maximum number of replicas, and a target resource which this Autoscaler will act upon.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To enable horizontal autoscaling of its pods, you need to create a HorizontalPodAutoscaler (HPA) object and point it to the Deployment.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use the kubectl autoscale command: $ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Autoscaler will constantly keep adjusting the number of replicas to keep their CPU utilization around 30%, but it will never scale down to less than one or scale up to more than five replicas.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Always make sure to autoscale Deployments instead of the underlying ReplicaSets, to ensure the desired replica count is preserved across application updates.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The HorizontalPodAutoscaler resource has a name, minimum and maximum number of replicas, and a target resource which this Autoscaler will act upon.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"Multiple versions of HPA resources exist: the new autoscaling/v2beta1 and the old autoscaling/v1. You’re requesting the new version here.\"}]\n",
      "\n",
      "[{\"highlight\": \"It takes a while for cAdvisor to get the CPU metrics and for Heapster to collect them before the Autoscaler can take action.\"}]\n",
      "\n",
      "[{\"highlight\": \"The autoscaler scales down the Deployment to a single replica, because even with a single pod, the CPU utilization will still be below the 30% target.\"}]\n",
      "\n",
      "[{\"highlight\": \"The autoscaler only adjusts the desired replica count on the Deployment. The Deployment controller then takes care of updating the desired replica count on the ReplicaSet object.\"}]\n",
      "\n",
      "[{\"highlight\": \"You can use kubectl describe to see more information on the HorizontalPod-Autoscaler and the operation of the underlying controller.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Multiple versions of HPA resources exist: the new autoscaling/v2beta1 and the old autoscaling/v1. You’re requesting the new version here.\"}]\n",
      "Done for page number:387\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The horizontal pod autoscaler has successfully rescaled to one replica, because all metrics were below target.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To trigger a scale-up, you need to expose the pods through a Service, so you can hit all of them through a single URL using kubectl expose: $ kubectl expose deployment kubia --port=80 --target-port=8080\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use the command 'kubectl get hpa,deployment' to keep an eye on what's happening with the HorizontalPodAutoscaler and the Deployment.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To list multiple resource types with kubectl get, you can delimit them with a comma: $ kubectl get hpa,deployment\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use 'kubectl run' to create a load-generating pod that sends requests to your pod, thereby increasing its CPU usage and triggering a scale-up.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The horizontal pod autoscaler has successfully rescaled to one replica, because all metrics were below target.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To trigger a scale-up, you need to expose the pods through a Service, so you can hit all of them through a single URL using kubectl expose: $ kubectl expose deployment kubia --port=80 --target-port=8080\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use the command 'kubectl get hpa,deployment' to keep an eye on what's happening with the HorizontalPodAutoscaler and the Deployment.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To list multiple resource types with kubectl get, you can delimit them with a comma: $ kubectl get hpa,deployment\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use 'kubectl run' to create a load-generating pod that sends requests to your pod, thereby increasing its CPU usage and triggering a scale-up.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:388\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The combination of options --rm and --restart=Never with kubectl run allows you to run commands inside the cluster without having to piggyback on an existing pod, and cleans up everything when the command terminates.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"As the load-generator pod runs, the autoscaler increases the number of replicas in response to CPU utilization exceeding 30%, with a maximum of four pods observed in this case.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A container's CPU utilization can exceed 100% because it is calculated as actual CPU usage divided by requested CPU, which defines the minimum available CPU.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The autoscaler concluded that four replicas were needed based on an initial average CPU utilization of 108%, which was rounded up from a division result of 3.6 (108/30).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can inspect autoscaler events with kubectl describe to see what the autoscaler has done, including SuccessfulRescale events triggered by CPU resource utilization exceeding target levels.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The combination of options --rm and --restart=Never with kubectl run allows you to run commands inside the cluster without having to piggyback on an existing pod, and cleans up everything when the command terminates.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"As the load-generator pod runs, the autoscaler increases the number of replicas in response to CPU utilization exceeding 30%, with a maximum of four pods observed in this case.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A container's CPU utilization can exceed 100% because it is calculated as actual CPU usage divided by requested CPU, which defines the minimum available CPU.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The autoscaler concluded that four replicas were needed based on an initial average CPU utilization of 108%, which was rounded up from a division result of 3.6 (108/30).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can inspect autoscaler events with kubectl describe to see what the autoscaler has done, including SuccessfulRescale events triggered by CPU resource utilization exceeding target levels.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:389\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The autoscaler scales up to four pods when the initial average CPU utilization is around 27%, which is close to the target value of 30%.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The autoscaler has a limit on how many replicas can be added in a single scale-up operation, at most doubling the number of replicas if more than two current replicas exist.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A scale-up event will occur only if no rescaling event occurred in the last three minutes, while a scale-down event is performed every five minutes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To modify the target metric value on an existing HPA object, you can edit the resource with kubectl edit command and change the targetAverageUtilization field to the desired value.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Deleting the HPA resource disables autoscaling of the target resource but leaves it at its current scale, and automatic scaling will resume after creating a new HPA resource for the Deployment.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The autoscaler scales up to four pods when the initial average CPU utilization is around 27%, which is close to the target value of 30%.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The autoscaler has a limit on how many replicas can be added in a single scale-up operation, at most doubling the number of replicas if more than two current replicas exist.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A scale-up event will occur only if no rescaling event occurred in the last three minutes, while a scale-down event is performed every five minutes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To modify the target metric value on an existing HPA object, you can edit the resource with kubectl edit command and change the targetAverageUtilization field to the desired value.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Deleting the HPA resource disables autoscaling of the target resource but leaves it at its current scale, and automatic scaling will resume after creating a new HPA resource for the Deployment.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:390\n",
      "Raw Output\n",
      "Here are the extracted highlights in JSON format:\n",
      "\n",
      "[{\"highlight\": \"Memory-based autoscaling is much more problematic than CPU-based autoscaling due to the need for apps to release memory when scaled up.\"}, {\"highlight\": \"Memory-based autoscaling was introduced in Kubernetes version 1.8 and is configured exactly like CPU-based autoscaling.\"}, {\"highlight\": \"Scaling based on custom, app-defined metrics was initially complicated but has been redesigned in newer versions of Kubernetes.\"}, {\"highlight\": \"The initial autoscaler design made it difficult to move beyond simple CPU-based scaling, prompting a complete redesign by the Kubernetes Autoscaling Special Interest Group (SIG).\"}, {\"highlight\": \"Newer versions of Kubernetes allow for easy configuration of autoscalers to use different metrics sources without the complications of earlier versions.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Memory-based autoscaling is much more problematic than CPU-based autoscaling due to the need for apps to release memory when scaled up.\"}, {\"highlight\": \"Memory-based autoscaling was introduced in Kubernetes version 1.8 and is configured exactly like CPU-based autoscaling.\"}, {\"highlight\": \"Scaling based on custom, app-defined metrics was initially complicated but has been redesigned in newer versions of Kubernetes.\"}, {\"highlight\": \"The initial autoscaler design made it difficult to move beyond simple CPU-based scaling, prompting a complete redesign by the Kubernetes Autoscaling Special Interest Group (SIG).\"}, {\"highlight\": \"Newer versions of Kubernetes allow for easy configuration of autoscalers to use different metrics sources without the complications of earlier versions.\"}]\n",
      "Done for page number:391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"There are three types of metrics you can use in an HPA object: Resource, Pods, and Object.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Resource type makes the autoscaler base its autoscaling decisions on a resource metric, like container's resource requests.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods type is used to refer to any other (including custom) metric related to the pod directly, such as Queries-Per-Second (QPS).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Object metric type is used when you want to make the autoscaler scale pods based on a metric that doesn’t pertain directly to those pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The autoscaler obtains a single metric from the single object when using an Object metric type, unlike in the previous case where it needed to obtain the metric for all targeted pods.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"There are three types of metrics you can use in an HPA object: Resource, Pods, and Object.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Resource type makes the autoscaler base its autoscaling decisions on a resource metric, like container's resource requests.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods type is used to refer to any other (including custom) metric related to the pod directly, such as Queries-Per-Second (QPS).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Object metric type is used when you want to make the autoscaler scale pods based on a metric that doesn’t pertain directly to those pods.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The autoscaler obtains a single metric from the single object when using an Object metric type, unlike in the previous case where it needed to obtain the metric for all targeted pods.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:392\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"To configure an HPA, you need to specify the target object and the target value.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Not all metrics are appropriate for use as the basis of autoscaling, such as pods' containers' memory consumption.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A good metric for autoscaling is Queries per Second (QPS), which reports the number of requests the application is receiving per second.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The horizontal pod autoscaler currently doesn't allow setting the minReplicas field to 0, so it will never scale down to zero replicas.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To use a metric of a different object in the HPA, you need to specify the name of the metric and the specific object whose metric the autoscaler should obtain.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"To configure an HPA, you need to specify the target object and the target value.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Not all metrics are appropriate for use as the basis of autoscaling, such as pods' containers' memory consumption.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A good metric for autoscaling is Queries per Second (QPS), which reports the number of requests the application is receiving per second.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The horizontal pod autoscaler currently doesn't allow setting the minReplicas field to 0, so it will never scale down to zero replicas.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To use a metric of a different object in the HPA, you need to specify the name of the metric and the specific object whose metric the autoscaler should obtain.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:393\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes currently doesn’t provide vertical pod autoscaling feature yet, but it will eventually.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Vertical pod autoscaling allows pods that provide a certain service to be scaled down to zero and brought up immediately when a new request comes in.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"An experimental feature called InitialResources sets the CPU and memory requests on newly created pods based on historical resource usage data of their containers.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes will eventually modify an existing pod’s resource requests using the same mechanism as InitialResources, allowing for vertical scaling while a pod is running.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Vertical pod autoscaling can dramatically increase hardware utilization by idling and un-idling pods that get infrequent client requests.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes currently doesn’t provide vertical pod autoscaling feature yet, but it will eventually.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Vertical pod autoscaling allows pods that provide a certain service to be scaled down to zero and brought up immediately when a new request comes in.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"An experimental feature called InitialResources sets the CPU and memory requests on newly created pods based on historical resource usage data of their containers.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes will eventually modify an existing pod’s resource requests using the same mechanism as InitialResources, allowing for vertical scaling while a pod is running.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Vertical pod autoscaling can dramatically increase hardware utilization by idling and un-idling pods that get infrequent client requests.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:394\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes includes the feature to automatically request additional nodes from the cloud provider as soon as it detects additional nodes are needed.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Cluster Autoscaler takes care of automatically provisioning additional nodes when it notices a pod that can’t be scheduled to existing nodes because of a lack of resources on those nodes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A new node will be provisioned if, after a new pod is created, the Scheduler can’t schedule it to any of the existing nodes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Cluster Autoscaler does this by examining the available node groups to see if at least one node type would be able to fit the unscheduled pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes includes the feature to automatically request additional nodes from the cloud provider as soon as it detects additional nodes are needed, performed by the Cluster Autoscaler.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes includes the feature to automatically request additional nodes from the cloud provider as soon as it detects additional nodes are needed.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Cluster Autoscaler takes care of automatically provisioning additional nodes when it notices a pod that can’t be scheduled to existing nodes because of a lack of resources on those nodes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A new node will be provisioned if, after a new pod is created, the Scheduler can’t schedule it to any of the existing nodes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Cluster Autoscaler does this by examining the available node groups to see if at least one node type would be able to fit the unscheduled pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes includes the feature to automatically request additional nodes from the cloud provider as soon as it detects additional nodes are needed, performed by the Cluster Autoscaler.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:395\n",
      "Raw Output\n",
      "[{\"highlight\": \"When the new node starts up, the Kubelet on that node contacts the API server and registers the node by creating a Node resource.\"}, {\"highlight\": \"The Cluster Autoscaler also needs to scale down the number of nodes when they aren’t being utilized enough.\"}, {\"highlight\": \"A node will only be returned to the cloud provider if the Cluster Autoscaler knows the pods running on the node will be rescheduled to other nodes.\"}, {\"highlight\": \"When a node is selected to be shut down, the node is first marked as unschedulable and then all the pods running on the node are evicted.\"}, {\"highlight\": \"The Cluster Autoscaler scales up when it finds a pod that can’t be scheduled to existing nodes.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"When the new node starts up, the Kubelet on that node contacts the API server and registers the node by creating a Node resource.\"}, {\"highlight\": \"The Cluster Autoscaler also needs to scale down the number of nodes when they aren’t being utilized enough.\"}, {\"highlight\": \"A node will only be returned to the cloud provider if the Cluster Autoscaler knows the pods running on the node will be rescheduled to other nodes.\"}, {\"highlight\": \"When a node is selected to be shut down, the node is first marked as unschedulable and then all the pods running on the node are evicted.\"}, {\"highlight\": \"The Cluster Autoscaler scales up when it finds a pod that can’t be scheduled to existing nodes.\"}]\n",
      "Done for page number:396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[{\"highlight\": \"Cluster autoscaling is currently available on Google Kubernetes Engine (GKE), Google Compute Engine (GCE), Amazon Web Services (AWS), and Microsoft Azure.\"}, {\"highlight\": \"To enable the Cluster Autoscaler on GKE, run $ gcloud container clusters update kubia --enable-autoscaling \\\\  --min-nodes=3 --max-nodes=5\"}, {\"highlight\": \"On GCE, set three environment variables: KUBE_ENABLE_CLUSTER_AUTOSCALER=true, KUBE_AUTOSCALER_MIN_NODES=3, and KUBE_AUTOSCALER_MAX_NODES=5 before running kube-up.sh\"}, {\"highlight\": \"The Cluster Autoscaler publishes its status to the cluster-autoscaler-status ConfigMap in the kube-system namespace.\"}, {\"highlight\": \"To limit service disruption during cluster scale-down, manually cordoning and draining nodes with kubectl commands can be used.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Cluster autoscaling is currently available on Google Kubernetes Engine (GKE), Google Compute Engine (GCE), Amazon Web Services (AWS), and Microsoft Azure.\"}, {\"highlight\": \"To enable the Cluster Autoscaler on GKE, run $ gcloud container clusters update kubia --enable-autoscaling \\\\  --min-nodes=3 --max-nodes=5\"}, {\"highlight\": \"On GCE, set three environment variables: KUBE_ENABLE_CLUSTER_AUTOSCALER=true, KUBE_AUTOSCALER_MIN_NODES=3, and KUBE_AUTOSCALER_MAX_NODES=5 before running kube-up.sh\"}, {\"highlight\": \"The Cluster Autoscaler publishes its status to the cluster-autoscaler-status ConfigMap in the kube-system namespace.\"}, {\"highlight\": \"To limit service disruption during cluster scale-down, manually cordoning and draining nodes with kubectl commands can be used.\"}]\n",
      "Done for page number:397\n",
      "Raw Output\n",
      "[{\"highlight\": \"Kubernetes provides a way to specify the minimum number of pods that need to keep running while performing operations, using a Pod-DisruptionBudget resource.\"}, {\"highlight\": \"A PodDisruptionBudget (PDB) resource contains only a pod label selector and a number specifying the minimum or maximum number of pods that must always be available.\"}, {\"highlight\": \"To ensure three instances of a kubia pod are always running, create a PodDisruptionBudget resource with kubectl create pdb and specify the min-available parameter as 3.\"}, {\"highlight\": \"Starting with Kubernetes 1.7, the PodDisruptionBudget resource also supports the maxUnavailable field, which can be used instead of minAvailable to block evictions when more than that many pods are unavailable.\"}, {\"highlight\": \"The Cluster Autoscaler and kubectl drain command will adhere to a PodDisruptionBudget resource and never evict a pod if it would bring the number of such pods below the specified minimum.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Kubernetes provides a way to specify the minimum number of pods that need to keep running while performing operations, using a Pod-DisruptionBudget resource.\"}, {\"highlight\": \"A PodDisruptionBudget (PDB) resource contains only a pod label selector and a number specifying the minimum or maximum number of pods that must always be available.\"}, {\"highlight\": \"To ensure three instances of a kubia pod are always running, create a PodDisruptionBudget resource with kubectl create pdb and specify the min-available parameter as 3.\"}, {\"highlight\": \"Starting with Kubernetes 1.7, the PodDisruptionBudget resource also supports the maxUnavailable field, which can be used instead of minAvailable to block evictions when more than that many pods are unavailable.\"}, {\"highlight\": \"The Cluster Autoscaler and kubectl drain command will adhere to a PodDisruptionBudget resource and never evict a pod if it would bring the number of such pods below the specified minimum.\"}]\n",
      "Done for page number:398\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Configuring automatic horizontal scaling of pods is easy with HorizontalPodAutoscaler object.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Horizontal Pod Autoscaler can scale based on custom metrics or other objects deployed in the cluster.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Vertical pod autoscaling is not possible yet.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Cluster nodes can be scaled automatically with supported cloud providers.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Pods can be run and deleted automatically with kubectl run -it --rm options.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Configuring automatic horizontal scaling of pods is easy with HorizontalPodAutoscaler object.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Horizontal Pod Autoscaler can scale based on custom metrics or other objects deployed in the cluster.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Vertical pod autoscaling is not possible yet.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Cluster nodes can be scaled automatically with supported cloud providers.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Pods can be run and deleted automatically with kubectl run -it --rm options.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:399\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes allows you to affect where pods are scheduled.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Using node taints and pod tolerations to keep pods away from certain nodes\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Defining node affinity rules as an alternative to node selectors\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Co-locating pods using pod affinity\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Keeping pods away from each other using pod anti-affinity\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes allows you to affect where pods are scheduled.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Using node taints and pod tolerations to keep pods away from certain nodes\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Defining node affinity rules as an alternative to node selectors\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Co-locating pods using pod affinity\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Keeping pods away from each other using pod anti-affinity\"\n",
      "  }\n",
      "]\n",
      "Done for page number:400\n",
      "Raw Output\n",
      "[{\"highlight\": \"A pod can only be scheduled to a node if it tolerates the node's taints.\"}, {\"highlight\": \"Node selectors and node affinity rules make it possible to select which nodes a pod can or can't be scheduled to by specifically adding that information to the pod, whereas taints allow rejecting deployment of pods to certain nodes by only adding taints to the node without having to modify existing pods.\"}, {\"highlight\": \"The master node in a cluster created with kubeadm tool is tainted, so only Control Plane pods can be deployed on it.\"}, {\"highlight\": \"Taints have a key, value, and an effect, and are represented as <key>=<value>:<effect>.\"}, {\"highlight\": \"This taint prevents pods from being scheduled to the master node, unless those pods tolerate this taint.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"A pod can only be scheduled to a node if it tolerates the node's taints.\"}, {\"highlight\": \"Node selectors and node affinity rules make it possible to select which nodes a pod can or can't be scheduled to by specifically adding that information to the pod, whereas taints allow rejecting deployment of pods to certain nodes by only adding taints to the node without having to modify existing pods.\"}, {\"highlight\": \"The master node in a cluster created with kubeadm tool is tainted, so only Control Plane pods can be deployed on it.\"}, {\"highlight\": \"Taints have a key, value, and an effect, and are represented as <key>=<value>:<effect>.\"}, {\"highlight\": \"This taint prevents pods from being scheduled to the master node, unless those pods tolerate this taint.\"}]\n",
      "Done for page number:401\n",
      "Raw Output\n",
      "Here are the extracted highlights in JSON format:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"In a cluster installed with kubeadm, the kube-proxy cluster component runs as a pod on every node, including the master node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod's tolerations define how long the pod is allowed to run on nodes that aren't ready or are unreachable.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods with no tolerations may only be scheduled to nodes without taints.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The first toleration matches the master node's taint, allowing this kube-proxy pod to be scheduled to the master node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod is only scheduled to a node if it tolerates the node's taints.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"In a cluster installed with kubeadm, the kube-proxy cluster component runs as a pod on every node, including the master node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod's tolerations define how long the pod is allowed to run on nodes that aren't ready or are unreachable.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods with no tolerations may only be scheduled to nodes without taints.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The first toleration matches the master node's taint, allowing this kube-proxy pod to be scheduled to the master node.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod is only scheduled to a node if it tolerates the node's taints.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Three possible effects exist for each taint: NoSchedule, PreferNoSchedule, and NoExecute.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Adding a NoExecute taint to a node will evict pods that are already running on the node and don't tolerate the taint.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To deploy production pods to production nodes, they need to tolerate the taint added to the nodes by including a YAML snippet in their manifests.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The kubectl taint command is used to add a taint to a node with a specific key, value, and effect.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Tolerations are required for pods to be scheduled on nodes that have been tainted with a NoSchedule or NoExecute effect.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Three possible effects exist for each taint: NoSchedule, PreferNoSchedule, and NoExecute.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Adding a NoExecute taint to a node will evict pods that are already running on the node and don't tolerate the taint.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To deploy production pods to production nodes, they need to tolerate the taint added to the nodes by including a YAML snippet in their manifests.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The kubectl taint command is used to add a taint to a node with a specific key, value, and effect.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Tolerations are required for pods to be scheduled on nodes that have been tainted with a NoSchedule or NoExecute effect.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:403\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can use taints and tolerations to prevent pods from being deployed to certain nodes in a Kubernetes cluster.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Tolerations can tolerate specific values for a taint key using the Equal operator, or any value for a specific taint key using the Exists operator.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Taints can be used to prevent scheduling of new pods (NoSchedule effect), define unpreferred nodes (PreferNoSchedule effect), and evict existing pods from a node (NoExecute).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can set up taints and tolerations to partition your cluster into multiple partitions, allowing different teams to schedule pods only to their respective nodes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Tolerations can be used in pods to allow them to be scheduled on specific types of nodes, such as production or non-production nodes.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can use taints and tolerations to prevent pods from being deployed to certain nodes in a Kubernetes cluster.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Tolerations can tolerate specific values for a taint key using the Equal operator, or any value for a specific taint key using the Exists operator.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Taints can be used to prevent scheduling of new pods (NoSchedule effect), define unpreferred nodes (PreferNoSchedule effect), and evict existing pods from a node (NoExecute).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can set up taints and tolerations to partition your cluster into multiple partitions, allowing different teams to schedule pods only to their respective nodes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Tolerations can be used in pods to allow them to be scheduled on specific types of nodes, such as production or non-production nodes.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:404\n",
      "Raw Output\n",
      "[{\"highlight\": \"You can use a toleration to specify how long Kubernetes should wait before rescheduling a pod to another node if the node the pod is running on becomes unready or unreachable.\"}, {\"highlight\": \"Tolerations say that this pod tolerates a node being notReady or unreachable for 300 seconds, and the Kubernetes Control Plane will wait for 300 seconds before deleting the pod and rescheduling it to another node.\"}, {\"highlight\": \"Node affinity allows you to tell Kubernetes to schedule pods only to specific subsets of nodes, which is more powerful than node selectors that require all labels specified in the field to be eligible to become the target for the pod.\"}, {\"highlight\": \"Taint-based evictions are currently an alpha feature and may change in future versions of Kubernetes, but can be enabled by running the Controller Manager with the --feature-gates=Taint-BasedEvictions=true option.\"}, {\"highlight\": \"The tolerationsSeconds value specifies how long a node must be notReady or unreachable before a pod is rescheduled to another node.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"You can use a toleration to specify how long Kubernetes should wait before rescheduling a pod to another node if the node the pod is running on becomes unready or unreachable.\"}, {\"highlight\": \"Tolerations say that this pod tolerates a node being notReady or unreachable for 300 seconds, and the Kubernetes Control Plane will wait for 300 seconds before deleting the pod and rescheduling it to another node.\"}, {\"highlight\": \"Node affinity allows you to tell Kubernetes to schedule pods only to specific subsets of nodes, which is more powerful than node selectors that require all labels specified in the field to be eligible to become the target for the pod.\"}, {\"highlight\": \"Taint-based evictions are currently an alpha feature and may change in future versions of Kubernetes, but can be enabled by running the Controller Manager with the --feature-gates=Taint-BasedEvictions=true option.\"}, {\"highlight\": \"The tolerationsSeconds value specifies how long a node must be notReady or unreachable before a pod is rescheduled to another node.\"}]\n",
      "Done for page number:405\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Node selectors will eventually be deprecated, so it’s important you understand the new node affinity rules.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can specify either hard requirements or preferences in node affinity rules, allowing Kubernetes to schedule pods accordingly.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The default node labels include beta.kubernetes.io/arch=amd64, beta.kubernetes.io/fluentd-ds-ready=true, and cloud.google.com/gke-nodepool=default-pool, among others.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Node affinity selects nodes based on their labels, the same way node selectors do, using the failure-domain.beta.kubernetes.io/region, failure-domain.beta.kubernetes.io/zone, and kubernetes.io/hostname labels as key criteria.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use custom labels in pod affinity rules, similar to how you used a custom label to deploy pods only to nodes with that label using node selectors in chapter 3.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Node selectors will eventually be deprecated, so it’s important you understand the new node affinity rules.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can specify either hard requirements or preferences in node affinity rules, allowing Kubernetes to schedule pods accordingly.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The default node labels include beta.kubernetes.io/arch=amd64, beta.kubernetes.io/fluentd-ds-ready=true, and cloud.google.com/gke-nodepool=default-pool, among others.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Node affinity selects nodes based on their labels, the same way node selectors do, using the failure-domain.beta.kubernetes.io/region, failure-domain.beta.kubernetes.io/zone, and kubernetes.io/hostname labels as key criteria.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use custom labels in pod affinity rules, similar to how you used a custom label to deploy pods only to nodes with that label using node selectors in chapter 3.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:406\n",
      "Raw Output\n",
      "[{\"highlight\": \"The nodeSelector field specifies that the pod should only be deployed on nodes that include the gpu=true label.\"}, {\"highlight\": \"If you replace the node selector with a node affinity rule, the pod definition will look like the following listing.\"}, {\"highlight\": \"requiredDuringSchedulingIgnoredDuringExecution means the rules defined under this field specify the labels the node must have for the pod to be scheduled to the node.\"}, {\"highlight\": \"IgnoredDuringExecution means the rules defined under the field don’t affect pods already executing on the node.\"}, {\"highlight\": \"That’s why all the rules right now always end with IgnoredDuringExecution.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The nodeSelector field specifies that the pod should only be deployed on nodes that include the gpu=true label.\"}, {\"highlight\": \"If you replace the node selector with a node affinity rule, the pod definition will look like the following listing.\"}, {\"highlight\": \"requiredDuringSchedulingIgnoredDuringExecution means the rules defined under this field specify the labels the node must have for the pod to be scheduled to the node.\"}, {\"highlight\": \"IgnoredDuringExecution means the rules defined under the field don’t affect pods already executing on the node.\"}, {\"highlight\": \"That’s why all the rules right now always end with IgnoredDuringExecution.\"}]\n",
      "Done for page number:407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[{\"highlight\": \"Node affinity allows pods to be scheduled to specific nodes based on labels.\"}, {\"highlight\": \"The nodeSelectorTerms field and matchExpressions define which expressions a node's labels must match for a pod to be scheduled.\"}, {\"highlight\": \"Prioritizing nodes during scheduling is done through the preferredDuringSchedulingIgnoredDuringExecution field.\"}, {\"highlight\": \"Node affinity can specify which nodes the Scheduler should prefer when scheduling a specific pod.\"}, {\"highlight\": \"The ability to prioritize nodes during scheduling is a benefit of the newly introduced node affinity feature.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Node affinity allows pods to be scheduled to specific nodes based on labels.\"}, {\"highlight\": \"The nodeSelectorTerms field and matchExpressions define which expressions a node's labels must match for a pod to be scheduled.\"}, {\"highlight\": \"Prioritizing nodes during scheduling is done through the preferredDuringSchedulingIgnoredDuringExecution field.\"}, {\"highlight\": \"Node affinity can specify which nodes the Scheduler should prefer when scheduling a specific pod.\"}, {\"highlight\": \"The ability to prioritize nodes during scheduling is a benefit of the newly introduced node affinity feature.\"}]\n",
      "Done for page number:408\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page, wrapped in JSON:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Node affinity allows you to schedule machines reserved for your company’s deployments to other zones if they don’t have enough room or due to other important reasons.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To set up node affinity, first label each node with a label designating the availability zone and a label marking it as dedicated or shared.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The command `kubectl label node <node_name> availability-zone=<zone_name>` is used to label nodes with their respective zones.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can create a Deployment that prefers dedicated nodes in a specific zone by specifying preferential node affinity rules in the Deployment manifest.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The `nodeAffinity` field in the Deployment manifest allows you to specify preferred node labels, such as availability-zone and share-type.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Node affinity allows you to schedule machines reserved for your company’s deployments to other zones if they don’t have enough room or due to other important reasons.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To set up node affinity, first label each node with a label designating the availability zone and a label marking it as dedicated or shared.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The command `kubectl label node <node_name> availability-zone=<zone_name>` is used to label nodes with their respective zones.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can create a Deployment that prefers dedicated nodes in a specific zone by specifying preferential node affinity rules in the Deployment manifest.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The `nodeAffinity` field in the Deployment manifest allows you to specify preferred node labels, such as availability-zone and share-type.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:409\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You're defining a node affinity preference, instead of a hard requirement.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The first preference rule is important by setting its weight to 80, whereas the second one is much less important (weight is set to 20).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Nodes whose availability-zone and share-type labels match the pod’s node affinity are ranked the highest.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You prefer the pod to be scheduled to zone1, which is your most important preference.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The weight of 80 for zone1 preference means it's four times more important than the dedicated nodes preference with a weight of 20.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You're defining a node affinity preference, instead of a hard requirement.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The first preference rule is important by setting its weight to 80, whereas the second one is much less important (weight is set to 20).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Nodes whose availability-zone and share-type labels match the pod’s node affinity are ranked the highest.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You prefer the pod to be scheduled to zone1, which is your most important preference.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The weight of 80 for zone1 preference means it's four times more important than the dedicated nodes preference with a weight of 20.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:410\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page, wrapped in JSON with the key named \"highlight\":\n",
      "\n",
      "[{\"highlight\": \"In a two-node cluster, most (if not all) of your pods deployed to node1 when creating a Deployment.\"}, \n",
      " {\"highlight\": \"The Scheduler uses other prioritization functions besides node affinity, such as Selector-SpreadPriority, to decide where to schedule a pod.\"}, \n",
      " {\"highlight\": \"Pods belonging to the same ReplicaSet or Service are spread around different nodes using the Selector-SpreadPriority function to prevent a node failure from bringing down the whole service.\"}, \n",
      " {\"highlight\": \"Using pod affinity can deploy pods close together, reducing latency and improving performance, without specifying exact node, rack, or datacenter locations.\"}, \n",
      " {\"highlight\": \"Pod affinity can be used to deploy multiple replicas of a pod on the same node as another pod, such as deploying frontend and backend pods together.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"In a two-node cluster, most (if not all) of your pods deployed to node1 when creating a Deployment.\"}, \n",
      " {\"highlight\": \"The Scheduler uses other prioritization functions besides node affinity, such as Selector-SpreadPriority, to decide where to schedule a pod.\"}, \n",
      " {\"highlight\": \"Pods belonging to the same ReplicaSet or Service are spread around different nodes using the Selector-SpreadPriority function to prevent a node failure from bringing down the whole service.\"}, \n",
      " {\"highlight\": \"Using pod affinity can deploy pods close together, reducing latency and improving performance, without specifying exact node, rack, or datacenter locations.\"}, \n",
      " {\"highlight\": \"Pod affinity can be used to deploy multiple replicas of a pod on the same node as another pod, such as deploying frontend and backend pods together.\"}]\n",
      "Done for page number:411\n",
      "Raw Output\n",
      "[{\"highlight\": \"This Deployment is not special in any way, but it uses podAffinity and anti-affinity to co-locate pods.\"}, {\"highlight\": \"The frontend pod's definition specifies a hard requirement to be deployed on the same node as pods with the app=backend label.\"}, {\"highlight\": \"Pod affinity allows scheduling pods to the node where other pods with a specific label are.\"}, {\"highlight\": \"All frontend pods will be scheduled only to the node the backend pod was scheduled to.\"}, {\"highlight\": \"The topologyKey field specifies that the pods must be deployed on the same node as pods that match the selector.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"This Deployment is not special in any way, but it uses podAffinity and anti-affinity to co-locate pods.\"}, {\"highlight\": \"The frontend pod's definition specifies a hard requirement to be deployed on the same node as pods with the app=backend label.\"}, {\"highlight\": \"Pod affinity allows scheduling pods to the node where other pods with a specific label are.\"}, {\"highlight\": \"All frontend pods will be scheduled only to the node the backend pod was scheduled to.\"}, {\"highlight\": \"The topologyKey field specifies that the pods must be deployed on the same node as pods that match the selector.\"}]\n",
      "Done for page number:412\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page in JSON format:\n",
      "\n",
      "[{\"highlight\": \"Instead of the simpler matchLabels field, you could also use the more expressive matchExpressions field.\"}, \n",
      " {\"highlight\": \"When scheduling the frontend pod, the Scheduler first found all the pods that match the labelSelector defined in the frontend pod’s podAffinity configuration and then scheduled the frontend pod to the same node.\"}, \n",
      " {\"highlight\": \"If you delete the backend pod, the Scheduler will schedule the pod to node2 even though it doesn’t define any pod affinity rules itself.\"}, \n",
      " {\"highlight\": \"The Scheduler takes other pods’ pod affinity rules into account when scheduling a new pod.\"}, \n",
      " {\"highlight\": \"You can confirm this by increasing the Scheduler’s logging level and checking its log, which will show why the backend pod is scheduled to node2.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Instead of the simpler matchLabels field, you could also use the more expressive matchExpressions field.\"}, \n",
      " {\"highlight\": \"When scheduling the frontend pod, the Scheduler first found all the pods that match the labelSelector defined in the frontend pod’s podAffinity configuration and then scheduled the frontend pod to the same node.\"}, \n",
      " {\"highlight\": \"If you delete the backend pod, the Scheduler will schedule the pod to node2 even though it doesn’t define any pod affinity rules itself.\"}, \n",
      " {\"highlight\": \"The Scheduler takes other pods’ pod affinity rules into account when scheduling a new pod.\"}, \n",
      " {\"highlight\": \"You can confirm this by increasing the Scheduler’s logging level and checking its log, which will show why the backend pod is scheduled to node2.\"}]\n",
      "Done for page number:413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\"highlight\": \"Pod affinity and anti-affinity can be used to co-locate pods on the same node or in the same availability zone, geographic region, or rack.\"},\n",
      "    {\"highlight\": \"The topologyKey property can be set to failure-domain.beta.kubernetes.io/zone or failure-domain.beta.kubernetes.io/region to schedule pods in the same availability zone or geographic region.\"},\n",
      "    {\"highlight\": \"To co-locate pods in the same rack, a custom topologyKey such as 'rack' can be used and nodes must be labeled with a matching key.\"},\n",
      "    {\"highlight\": \"The Scheduler checks the pod's pod-Affinity config to find matching pods and their corresponding nodes when deciding where to deploy a new pod.\"},\n",
      "    {\"highlight\": \"Pod affinity scores are calculated based on InterPodAffinityPriority, SelectorSpreadPriority, and NodeAffinityPriority, with higher scores indicating a better match for co-location.\"}\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\"highlight\": \"Pod affinity and anti-affinity can be used to co-locate pods on the same node or in the same availability zone, geographic region, or rack.\"},\n",
      "    {\"highlight\": \"The topologyKey property can be set to failure-domain.beta.kubernetes.io/zone or failure-domain.beta.kubernetes.io/region to schedule pods in the same availability zone or geographic region.\"},\n",
      "    {\"highlight\": \"To co-locate pods in the same rack, a custom topologyKey such as 'rack' can be used and nodes must be labeled with a matching key.\"},\n",
      "    {\"highlight\": \"The Scheduler checks the pod's pod-Affinity config to find matching pods and their corresponding nodes when deciding where to deploy a new pod.\"},\n",
      "    {\"highlight\": \"Pod affinity scores are calculated based on InterPodAffinityPriority, SelectorSpreadPriority, and NodeAffinityPriority, with higher scores indicating a better match for co-location.\"}\n",
      "]\n",
      "Done for page number:414\n",
      "Raw Output\n",
      "[{\"highlight\": \"By default, the label selector only matches pods in the same namespace as the pod that’s being scheduled.\"}, {\"highlight\": \"You can also select pods from other namespaces by adding a namespaces field at the same level as label-Selector.\"}, {\"highlight\": \"podAffinity can be used to specify node preferences, to instruct the Scheduler to schedule the pod to certain nodes, while allowing it to schedule it anywhere else if those nodes can’t fit the pod for any reason.\"}, {\"highlight\": \"The topologyKey in podAffinity determines the scope of where the pod should be scheduled to.\"}, {\"highlight\": \"Frontend pods will be scheduled to nodes in the same rack as the backend pod.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"By default, the label selector only matches pods in the same namespace as the pod that’s being scheduled.\"}, {\"highlight\": \"You can also select pods from other namespaces by adding a namespaces field at the same level as label-Selector.\"}, {\"highlight\": \"podAffinity can be used to specify node preferences, to instruct the Scheduler to schedule the pod to certain nodes, while allowing it to schedule it anywhere else if those nodes can’t fit the pod for any reason.\"}, {\"highlight\": \"The topologyKey in podAffinity determines the scope of where the pod should be scheduled to.\"}, {\"highlight\": \"Frontend pods will be scheduled to nodes in the same rack as the backend pod.\"}]\n",
      "Done for page number:415\n",
      "Raw Output\n",
      "[{\"highlight\": \"You need to define a weight for each pod affinity rule.\"}, {\"highlight\": \"Specify the topologyKey and labelSelector in pod affinity rules, similar to hard-requirement podAffinity rules.\"}, {\"highlight\": \"Pod affinity can be used to make the Scheduler prefer nodes where pods with a certain label are running.\"}, {\"highlight\": \"A weight and a podAffinity term is specified as in the previous example.\"}, {\"highlight\": \"The Scheduler will prefer Node 2 for frontend pods, but may schedule pods to Node 1 as well.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"You need to define a weight for each pod affinity rule.\"}, {\"highlight\": \"Specify the topologyKey and labelSelector in pod affinity rules, similar to hard-requirement podAffinity rules.\"}, {\"highlight\": \"Pod affinity can be used to make the Scheduler prefer nodes where pods with a certain label are running.\"}, {\"highlight\": \"A weight and a podAffinity term is specified as in the previous example.\"}, {\"highlight\": \"The Scheduler will prefer Node 2 for frontend pods, but may schedule pods to Node 1 as well.\"}]\n",
      "Done for page number:416\n",
      "Raw Output\n",
      "[{\"highlight\": \"You can use pod anti-affinity to schedule pods away from each other.\"}, {\"highlight\": \"Pod anti-affinity is specified the same way as pod affinity, but with the podAntiAffinity property instead of podAffinity.\"}, {\"highlight\": \"The Scheduler will never choose nodes where pods matching the podAntiAffinity's label selector are running.\"}, {\"highlight\": \"Pod anti-affinity can be used to prevent two sets of pods from interfering with each other's performance if they run on the same node.\"}, {\"highlight\": \"Pod anti-affinity can also be used to spread pods of the same group across different availability zones or regions for high availability.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"You can use pod anti-affinity to schedule pods away from each other.\"}, {\"highlight\": \"Pod anti-affinity is specified the same way as pod affinity, but with the podAntiAffinity property instead of podAffinity.\"}, {\"highlight\": \"The Scheduler will never choose nodes where pods matching the podAntiAffinity's label selector are running.\"}, {\"highlight\": \"Pod anti-affinity can be used to prevent two sets of pods from interfering with each other's performance if they run on the same node.\"}, {\"highlight\": \"Pod anti-affinity can also be used to spread pods of the same group across different availability zones or regions for high availability.\"}]\n",
      "Done for page number:417\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"To force frontend pods to be scheduled to different nodes, you can use pod anti-affinity and configure the pods' anti-affinity in the Deployment spec.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The `podAntiAffinity` property is used instead of `podAffinity`, and the labelSelector matches the same pods created by the Deployment.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"When creating a Deployment with pod anti-affinity, only two pods were scheduled to different nodes, while three remaining pods are Pending due to the Scheduler's restriction.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Using `preferredDuringSchedulingIgnoredDuringExecution` property is recommended instead of `requiredDuringSchedulingIgnoredDuringExecution` for scenarios where pod anti-affinity is not strictly required.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A frontend pod must not be scheduled to the same machine as a pod with app=frontend label, which is a hard requirement for pod anti-affinity.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"To force frontend pods to be scheduled to different nodes, you can use pod anti-affinity and configure the pods' anti-affinity in the Deployment spec.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The `podAntiAffinity` property is used instead of `podAffinity`, and the labelSelector matches the same pods created by the Deployment.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"When creating a Deployment with pod anti-affinity, only two pods were scheduled to different nodes, while three remaining pods are Pending due to the Scheduler's restriction.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Using `preferredDuringSchedulingIgnoredDuringExecution` property is recommended instead of `requiredDuringSchedulingIgnoredDuringExecution` for scenarios where pod anti-affinity is not strictly required.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A frontend pod must not be scheduled to the same machine as a pod with app=frontend label, which is a hard requirement for pod anti-affinity.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:418\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\"highlight\": \"The topologyKey property determines the scope of where the pod shouldn’t be deployed to, allowing you to ensure pods aren’t deployed to the same rack, availability zone, region, or any custom scope.\"},\n",
      "\n",
      "    {\"highlight\": \"Three types of taints exist: NoSchedule completely prevents scheduling, Prefer-NoSchedule isn’t as strict, and NoExecute even evicts existing pods from a node.\"},\n",
      "\n",
      "    {\"highlight\": \"Node affinity allows you to specify which nodes a pod should be scheduled to, and can be used to specify a hard requirement or to only express a node preference.\"},\n",
      "\n",
      "    {\"highlight\": \"Pod affinity’s topologyKey specifies how close the pod should be deployed to the other pod (onto the same node or onto a node in the same rack, availability zone, or availability region).\"},\n",
      "\n",
      "    {\"highlight\": \"Both pod affinity and anti-affinity can either specify hard requirements or preferences, allowing you to control where pods are scheduled.\"}\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\"highlight\": \"The topologyKey property determines the scope of where the pod shouldn’t be deployed to, allowing you to ensure pods aren’t deployed to the same rack, availability zone, region, or any custom scope.\"},\n",
      "\n",
      "    {\"highlight\": \"Three types of taints exist: NoSchedule completely prevents scheduling, Prefer-NoSchedule isn’t as strict, and NoExecute even evicts existing pods from a node.\"},\n",
      "\n",
      "    {\"highlight\": \"Node affinity allows you to specify which nodes a pod should be scheduled to, and can be used to specify a hard requirement or to only express a node preference.\"},\n",
      "\n",
      "    {\"highlight\": \"Pod affinity’s topologyKey specifies how close the pod should be deployed to the other pod (onto the same node or onto a node in the same rack, availability zone, or availability region).\"},\n",
      "\n",
      "    {\"highlight\": \"Both pod affinity and anti-affinity can either specify hard requirements or preferences, allowing you to control where pods are scheduled.\"}\n",
      "]\n",
      "Done for page number:419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"We’ve now covered most of what you need to know to run your apps in Kubernetes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"This chapter covers Understanding which Kubernetes resources appear in a typical application\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Properly terminating an app without breaking client requests\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Making apps easy to manage in Kubernetes\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Developing locally with Minikube\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"We’ve now covered most of what you need to know to run your apps in Kubernetes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"This chapter covers Understanding which Kubernetes resources appear in a typical application\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Properly terminating an app without breaking client requests\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Making apps easy to manage in Kubernetes\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Developing locally with Minikube\"\n",
      "  }\n",
      "]\n",
      "Done for page number:420\n",
      "Raw Output\n",
      "[{\"highlight\": \"A typical application manifest contains one or more Deployment and/or StatefulSet objects.\"}, \n",
      " {\"highlight\": \"Pods that provide services to others are exposed through one or more Services.\"}, \n",
      " {\"highlight\": \"The pod templates (and the pods created from them) usually reference two types of Secrets—those for pulling container images from private image registries and those used directly by the process running inside the pods.\"}, \n",
      " {\"highlight\": \"Secrets are usually assigned to ServiceAccounts, which are assigned to individual pods.\"}, \n",
      " {\"highlight\": \"A typical application consists of a pod template containing one or more containers, with a live-ness probe for each of them and a readiness probe for the service(s) the container provides (if any).\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"A typical application manifest contains one or more Deployment and/or StatefulSet objects.\"}, \n",
      " {\"highlight\": \"Pods that provide services to others are exposed through one or more Services.\"}, \n",
      " {\"highlight\": \"The pod templates (and the pods created from them) usually reference two types of Secrets—those for pulling container images from private image registries and those used directly by the process running inside the pods.\"}, \n",
      " {\"highlight\": \"Secrets are usually assigned to ServiceAccounts, which are assigned to individual pods.\"}, \n",
      " {\"highlight\": \"A typical application consists of a pod template containing one or more containers, with a live-ness probe for each of them and a readiness probe for the service(s) the container provides (if any).\"}]\n",
      "Done for page number:421\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The application contains one or more ConfigMaps, which are used to initialize environment variables or mounted as a configMap volume in the pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Persistent-VolumeClaims are part of the application manifest, whereas StorageClasses referenced by them are created by system administrators upfront.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"After the application is deployed, additional objects are created automatically by the various Kubernetes controllers, including service Endpoints objects and ReplicaSets.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Resources are often labeled with one or more labels to keep them organized, and most resources also contain annotations that describe each resource and provide additional metadata for management and other tools.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods can be killed any time by Kubernetes due to relocation or scale-down requests, which is a significant difference from apps running in VMs outside of Kubernetes.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The application contains one or more ConfigMaps, which are used to initialize environment variables or mounted as a configMap volume in the pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Persistent-VolumeClaims are part of the application manifest, whereas StorageClasses referenced by them are created by system administrators upfront.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"After the application is deployed, additional objects are created automatically by the various Kubernetes controllers, including service Endpoints objects and ReplicaSets.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Resources are often labeled with one or more labels to keep them organized, and most resources also contain annotations that describe each resource and provide additional metadata for management and other tools.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pods can be killed any time by Kubernetes due to relocation or scale-down requests, which is a significant difference from apps running in VMs outside of Kubernetes.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:422\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When a pod is killed and run elsewhere, it not only has a new IP address but also a new name and hostname.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Apps need to be prepared for their pod's IP to change when rescheduled, and should never base membership in a clustered app on the member's IP address.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Files written to disk by an app running in a pod may disappear even during the lifetime of a single pod, unless persistent storage is mounted at the location the app is writing to.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Individual containers may be restarted for several reasons, such as process crashes or node memory issues, resulting in lost data written to disk by previous container instances.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Using pod-scoped volumes can help preserve data across container restarts, allowing new containers to reuse data written to the volume by previous containers.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When a pod is killed and run elsewhere, it not only has a new IP address but also a new name and hostname.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Apps need to be prepared for their pod's IP to change when rescheduled, and should never base membership in a clustered app on the member's IP address.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Files written to disk by an app running in a pod may disappear even during the lifetime of a single pod, unless persistent storage is mounted at the location the app is writing to.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Individual containers may be restarted for several reasons, such as process crashes or node memory issues, resulting in lost data written to disk by previous container instances.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Using pod-scoped volumes can help preserve data across container restarts, allowing new containers to reuse data written to the volume by previous containers.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:423\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Files written to the container’s filesystem are lost when the container is restarted.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"New process can use data preserved in the volume\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Container crashes or is killed, Pod creates a new container with a new writable layer and read-only layers from image\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Using a volume to persist data across container restarts\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"New container starts with a new writable layer: all files are lost\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Files written to the container’s filesystem are lost when the container is restarted.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"New process can use data preserved in the volume\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Container crashes or is killed, Pod creates a new container with a new writable layer and read-only layers from image\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Using a volume to persist data across container restarts\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"New container starts with a new writable layer: all files are lost\"\n",
      "    }\n",
      "]\n",
      "Done for page number:424\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Using a volume to preserve files across container restarts is not always recommended, as it can lead to a continuous crash loop if the data gets corrupted.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes does not automatically remove and reschedule dead or partially dead pods, even if they're part of a ReplicaSet or similar controller.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod's container keeps crashing indefinitely until it reaches five minutes between restarts, after which the pod is essentially dead.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The ReplicaSet controller doesn't care if the pods are dead and will not delete and replace them with a new instance.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A ReplicaSet with a desired replica count of three can end up with only two properly running replicas due to a crashing container in one of the pods.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Using a volume to preserve files across container restarts is not always recommended, as it can lead to a continuous crash loop if the data gets corrupted.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes does not automatically remove and reschedule dead or partially dead pods, even if they're part of a ReplicaSet or similar controller.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod's container keeps crashing indefinitely until it reaches five minutes between restarts, after which the pod is essentially dead.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The ReplicaSet controller doesn't care if the pods are dead and will not delete and replace them with a new instance.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A ReplicaSet with a desired replica count of three can end up with only two properly running replicas due to a crashing container in one of the pods.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"When a pod crashes in Kubernetes, it will be restarted every five minutes in the hope that the underlying cause of the crash will be resolved.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes does not have a built-in way to start certain pods first and others only when the first pods are already up and ready to serve.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The pod's status shows the Kubelet is delaying the restart because the container keeps crashing, but no action is taken by the controller if current replicas match desired replicas.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A ReplicaSet in Kubernetes will keep creating new pods to replace ones that crash, until the desired replica count is met.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The kubectl get and describe commands can be used to inspect the status of pods and ReplicaSets in a Kubernetes cluster.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"When a pod crashes in Kubernetes, it will be restarted every five minutes in the hope that the underlying cause of the crash will be resolved.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes does not have a built-in way to start certain pods first and others only when the first pods are already up and ready to serve.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The pod's status shows the Kubelet is delaying the restart because the container keeps crashing, but no action is taken by the controller if current replicas match desired replicas.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A ReplicaSet in Kubernetes will keep creating new pods to replace ones that crash, until the desired replica count is met.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The kubectl get and describe commands can be used to inspect the status of pods and ReplicaSets in a Kubernetes cluster.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:426\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"A pod may have any number of init containers, which are executed sequentially and only after the last one completes are the pod's main containers started.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Init containers can be used to delay the start of a pod's main container(s) until a certain precondition is met, such as waiting for a service required by the pod's main container to be up and ready.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"An init container can check whether a Service is responding to requests and retry until it gets a response, then terminate and let the main container start.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Init containers are defined in the pod spec through the spec.initContainers field, like main containers but with their own set of fields.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The order in which init containers are listed in a YAML or JSON file does not guarantee the order in which they will be started; however, you can use an init container to delay the start of a pod's main container(s).\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"A pod may have any number of init containers, which are executed sequentially and only after the last one completes are the pod's main containers started.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Init containers can be used to delay the start of a pod's main container(s) until a certain precondition is met, such as waiting for a service required by the pod's main container to be up and ready.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"An init container can check whether a Service is responding to requests and retry until it gets a response, then terminate and let the main container start.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Init containers are defined in the pod spec through the spec.initContainers field, like main containers but with their own set of fields.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The order in which init containers are listed in a YAML or JSON file does not guarantee the order in which they will be started; however, you can use an init container to delay the start of a pod's main container(s).\"\n",
      "    }\n",
      "]\n",
      "Done for page number:427\n",
      "Raw Output\n",
      "Here is the list of extracted highlights in JSON format:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"When you deploy this pod, only its init container is started. This is shown in the pod’s status when you list pods with kubectl get:\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The main container won’t run until you deploy the fortune Service and the fortune-server pod.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You’ve seen how an init container can be used to delay starting the pod’s main container(s) until a precondition is met, but it’s much better to write apps that don’t require every service they rely on to be ready before the app starts up.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The application needs to handle internally the possibility that its dependencies aren’t ready. And don’t forget readiness probes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"pods also allow you to define two lifecycle hooks: Post-start hooks and Pre-stop hooks, which are specified per container and can execute a command or perform an HTTP GET request against a URL\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"When you deploy this pod, only its init container is started. This is shown in the pod’s status when you list pods with kubectl get:\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The main container won’t run until you deploy the fortune Service and the fortune-server pod.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You’ve seen how an init container can be used to delay starting the pod’s main container(s) until a precondition is met, but it’s much better to write apps that don’t require every service they rely on to be ready before the app starts up.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The application needs to handle internally the possibility that its dependencies aren’t ready. And don’t forget readiness probes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"pods also allow you to define two lifecycle hooks: Post-start hooks and Pre-stop hooks, which are specified per container and can execute a command or perform an HTTP GET request against a URL\"\n",
      "  }\n",
      "]\n",
      "Done for page number:428\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"A post-start hook is executed immediately after the container’s main process is started.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The hook runs asynchronously, but it does affect the container in two ways: until the hook completes, the container will stay in the Waiting state with the reason ContainerCreating.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod manifest containing a post-start hook looks like the following listing.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The echo, sleep, and exit commands are executed along with the container’s main process as soon as the container is created.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If the hook fails, the main container will be killed.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"A post-start hook is executed immediately after the container’s main process is started.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The hook runs asynchronously, but it does affect the container in two ways: until the hook completes, the container will stay in the Waiting state with the reason ContainerCreating.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod manifest containing a post-start hook looks like the following listing.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The echo, sleep, and exit commands are executed along with the container’s main process as soon as the container is created.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If the hook fails, the main container will be killed.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:429\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When a post-start hook fails, you'll see a FailedPostStartHook warning among the pod's events, followed by more information on why the hook failed.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The standard and error outputs of command-based post-start hooks aren't logged anywhere, so it's recommended to have the process invoked by the hook log to a file in the container's filesystem.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pre-stop hook is executed immediately before a container is terminated and can be used to initiate a graceful shutdown of the container or perform arbitrary operations before shutdown.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Configuring a pre-stop hook in a pod manifest is similar to adding a post-start hook, but with a different lifecycle event.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When using an HTTP GET hook handler, the reason for failure may look like 'Get http://<ip>:9090/postStart: dial tcp <ip>:9090: getsockopt: connection refused', indicating a connection refusal error.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When a post-start hook fails, you'll see a FailedPostStartHook warning among the pod's events, followed by more information on why the hook failed.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The standard and error outputs of command-based post-start hooks aren't logged anywhere, so it's recommended to have the process invoked by the hook log to a file in the container's filesystem.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pre-stop hook is executed immediately before a container is terminated and can be used to initiate a graceful shutdown of the container or perform arbitrary operations before shutdown.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Configuring a pre-stop hook in a pod manifest is similar to adding a post-start hook, but with a different lifecycle event.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When using an HTTP GET hook handler, the reason for failure may look like 'Get http://<ip>:9090/postStart: dial tcp <ip>:9090: getsockopt: connection refused', indicating a connection refusal error.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"A pre-stop hook can be defined in a pod's lifecycle section with an httpGet field to perform an HTTP GET request before termination.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The host field in a pre-stop hook defaults to the pod IP, and setting it to localhost will refer to the node instead of the pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If the pre-stop hook fails, a FailedPreStopHook warning event is logged among the pod's events, but may not be noticed due to the pod being deleted soon afterward.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Defining a pre-stop hook solely to send a SIGTERM signal to an app is incorrect and can be fixed by handling the signal in the shell script running as the main container process or by running the application binary directly without a shell.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pre-stop hook's successful completion is not guaranteed, and its execution should be verified if critical for system operation, especially when using a command-based hook.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"A pre-stop hook can be defined in a pod's lifecycle section with an httpGet field to perform an HTTP GET request before termination.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The host field in a pre-stop hook defaults to the pod IP, and setting it to localhost will refer to the node instead of the pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If the pre-stop hook fails, a FailedPreStopHook warning event is logged among the pod's events, but may not be noticed due to the pod being deleted soon afterward.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Defining a pre-stop hook solely to send a SIGTERM signal to an app is incorrect and can be fixed by handling the signal in the shell script running as the main container process or by running the application binary directly without a shell.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pre-stop hook's successful completion is not guaranteed, and its execution should be verified if critical for system operation, especially when using a command-based hook.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:431\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"A pod's shut-down is triggered by the deletion of the Pod object through the API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Kubelet notices the pod needs to be terminated, it starts terminating each of the pod's containers with a configurable termination grace period.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The sequence of events during pod shutdown is: run the pre-stop hook, send SIGTERM signal to the main process, wait for container shutdown or until the termination grace period runs out, and forcibly kill the process if it hasn't terminated gracefully yet.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pre-stop hooks target containers, not pods, and shouldn't be used for running actions that need to be performed when the pod is terminating.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod's termination involves setting a deletionTimestamp field in the Pod object, which triggers the Kubelet to terminate each container with a configurable termination grace period.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"A pod's shut-down is triggered by the deletion of the Pod object through the API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Kubelet notices the pod needs to be terminated, it starts terminating each of the pod's containers with a configurable termination grace period.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The sequence of events during pod shutdown is: run the pre-stop hook, send SIGTERM signal to the main process, wait for container shutdown or until the termination grace period runs out, and forcibly kill the process if it hasn't terminated gracefully yet.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Pre-stop hooks target containers, not pods, and shouldn't be used for running actions that need to be performed when the pod is terminating.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A pod's termination involves setting a deletionTimestamp field in the Pod object, which triggers the Kubelet to terminate each container with a configurable termination grace period.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:432\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page in JSON format:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The termination grace period can be configured in the pod spec by setting the spec.terminationGracePeriodSeconds field.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You should set the grace period to long enough so your process can finish cleaning up in that time.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When deleting a pod, you can override the specified grace period using the --grace-period option with kubectl delete command.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Applications should react to a SIGTERM signal by starting their shut-down procedure and terminating when it finishes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Be careful when force-deleting a pod, especially with pods of a StatefulSet, as it may cause the controller to create a replacement pod without waiting for the containers of the deleted pod to shut down.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The termination grace period can be configured in the pod spec by setting the spec.terminationGracePeriodSeconds field.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You should set the grace period to long enough so your process can finish cleaning up in that time.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When deleting a pod, you can override the specified grace period using the --grace-period option with kubectl delete command.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Applications should react to a SIGTERM signal by starting their shut-down procedure and terminating when it finishes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Be careful when force-deleting a pod, especially with pods of a StatefulSet, as it may cause the controller to create a replacement pod without waiting for the containers of the deleted pod to shut down.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:433\n",
      "Raw Output\n",
      "[{\"highlight\": \"A container terminating doesn’t necessarily mean the whole pod is being terminated.\"}, {\"highlight\": \"You have no guarantee the shut-down procedure will finish before the process is killed.\"}, {\"highlight\": \"The proper way to handle [pod data] migration is by having a dedicated, constantly running pod that keeps checking for the existence of orphaned data.\"}, {\"highlight\": \"StatefulSets don’t help with [pod data] migration because scaling down leaves PersistentVolumeClaims orphaned.\"}, {\"highlight\": \"A data-migrating pod can be configured to wait before performing the migration to prevent it from occurring during an application upgrade.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"A container terminating doesn’t necessarily mean the whole pod is being terminated.\"}]\n",
      "Done for page number:434\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To prevent broken client connections when a pod is starting up, ensure each connection is handled properly by adding an HTTP GET readiness probe and pointing it to the base URL of your app.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If you don't specify a readiness probe in your pod spec, the pod will be considered ready immediately after startup, which may cause clients to see 'connection refused' errors if your app isn't ready to accept connections.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes doesn't prevent broken client connections by itself; your app needs to follow rules to handle connections properly when pods are starting up or shutting down.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To ensure all client requests are handled properly, make sure your app is ready to accept connections before becoming a service endpoint and receiving requests from clients.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A dedicated pod can be used to migrate data when scaling down StatefulSets, which helps prevent data loss or corruption during the process.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To prevent broken client connections when a pod is starting up, ensure each connection is handled properly by adding an HTTP GET readiness probe and pointing it to the base URL of your app.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"If you don't specify a readiness probe in your pod spec, the pod will be considered ready immediately after startup, which may cause clients to see 'connection refused' errors if your app isn't ready to accept connections.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes doesn't prevent broken client connections by itself; your app needs to follow rules to handle connections properly when pods are starting up or shutting down.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To ensure all client requests are handled properly, make sure your app is ready to accept connections before becoming a service endpoint and receiving requests from clients.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A dedicated pod can be used to migrate data when scaling down StatefulSets, which helps prevent data loss or corruption during the process.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When a request for a pod deletion is received by the API server, it first modifies the state in etcd and then notifies its watchers of the deletion.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Kubelet and the Endpoints controller are among the watchers that receive notification of the pod deletion.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When a Pod is deleted, its containers should start shutting down cleanly as soon as they receive the SIGTERM signal or when their pre-stop hook is executed.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The sequence of events that unfolds across the cluster when a Pod is deleted includes modification of state in etcd, notification to watchers, and removal of the pod from iptables.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Preventing broken connections during pod shut-down involves handling client requests properly, continuing to accept requests, and completing pending requests before terminating.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When a request for a pod deletion is received by the API server, it first modifies the state in etcd and then notifies its watchers of the deletion.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Kubelet and the Endpoints controller are among the watchers that receive notification of the pod deletion.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When a Pod is deleted, its containers should start shutting down cleanly as soon as they receive the SIGTERM signal or when their pre-stop hook is executed.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The sequence of events that unfolds across the cluster when a Pod is deleted includes modification of state in etcd, notification to watchers, and removal of the pod from iptables.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Preventing broken connections during pod shut-down involves handling client requests properly, continuing to accept requests, and completing pending requests before terminating.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:436\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When a pod is deleted, the Kubelet initiates a shutdown sequence that includes running the pre-stop hook, sending SIGTERM, waiting for a period of time, and then forcibly killing the container if it hasn't yet terminated on its own.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Endpoints controller removes the pod as an endpoint in all services that the pod is a part of by modifying the Endpoints API object and notifying all clients watching the Endpoints object.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Removing iptables rules has no effect on existing connections, but prevents new connections from being forwarded to the terminating pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The time it takes for a pod to be shut down is relatively short compared to the time required for iptables rules to be updated, which involves multiple steps and notifications between components.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Existing clients connected to the pod will still send additional requests to the pod through existing connections even after the iptables rules have been updated.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"When a pod is deleted, the Kubelet initiates a shutdown sequence that includes running the pre-stop hook, sending SIGTERM, waiting for a period of time, and then forcibly killing the container if it hasn't yet terminated on its own.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Endpoints controller removes the pod as an endpoint in all services that the pod is a part of by modifying the Endpoints API object and notifying all clients watching the Endpoints object.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Removing iptables rules has no effect on existing connections, but prevents new connections from being forwarded to the terminating pod.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The time it takes for a pod to be shut down is relatively short compared to the time required for iptables rules to be updated, which involves multiple steps and notifications between components.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Existing clients connected to the pod will still send additional requests to the pod through existing connections even after the iptables rules have been updated.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:437\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The readiness probe has no bearing on the process of removing a pod from a service's Endpoints after receiving notice of deletion.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To ensure all requests are handled fully, the pod needs to keep accepting connections until all kube-proxies have finished updating iptables rules.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Waiting for all clients to notify that they'll no longer forward connections is impossible due to distributed components across many computers.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Adding a delay of 5-10 seconds before shutting down the pod can improve user experience, but there's no guarantee it will suffice every time.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Endpoints controller removes the pod from the service Endpoints as soon as it receives notice of deletion, regardless of readiness probe results.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The readiness probe has no bearing on the process of removing a pod from a service's Endpoints after receiving notice of deletion.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To ensure all requests are handled fully, the pod needs to keep accepting connections until all kube-proxies have finished updating iptables rules.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Waiting for all clients to notify that they'll no longer forward connections is impossible due to distributed components across many computers.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Adding a delay of 5-10 seconds before shutting down the pod can improve user experience, but there's no guarantee it will suffice every time.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Endpoints controller removes the pod from the service Endpoints as soon as it receives notice of deletion, regardless of readiness probe results.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:438\n",
      "Raw Output\n",
      "[{\"highlight\": \"To properly shut down an application, wait for a few seconds, then stop accepting new connections, close all keep-alive connections not in the middle of a request, wait for all active requests to finish, and then shut down completely.\"}, \n",
      " {\"highlight\": \"A pre-stop hook can be added to wait a few seconds before shutting down, such as the one in Listing 17.7 that waits 5 seconds.\"}, \n",
      " {\"highlight\": \"The process of shutting down an application involves stopping new connections, closing inactive keep-alive connections, and waiting for active requests to finish before shutting down completely.\"}, \n",
      " {\"highlight\": \"Properly handling existing and new connections after receiving a termination signal is crucial to prevent broken connections and ensure a smooth shutdown process.\"}, \n",
      " {\"highlight\": \"The use of a pre-stop hook can help prevent broken connections by delaying the shutdown process for a few seconds, allowing active requests to finish before shutting down completely.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"To properly shut down an application, wait for a few seconds, then stop accepting new connections, close all keep-alive connections not in the middle of a request, wait for all active requests to finish, and then shut down completely.\"}, \n",
      " {\"highlight\": \"A pre-stop hook can be added to wait a few seconds before shutting down, such as the one in Listing 17.7 that waits 5 seconds.\"}, \n",
      " {\"highlight\": \"The process of shutting down an application involves stopping new connections, closing inactive keep-alive connections, and waiting for active requests to finish before shutting down completely.\"}, \n",
      " {\"highlight\": \"Properly handling existing and new connections after receiving a termination signal is crucial to prevent broken connections and ensure a smooth shutdown process.\"}, \n",
      " {\"highlight\": \"The use of a pre-stop hook can help prevent broken connections by delaying the shutdown process for a few seconds, allowing active requests to finish before shutting down completely.\"}]\n",
      "Done for page number:439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"You don't need to modify the code of your app at all to make it easy to run and manage in Kubernetes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Including unnecessary files in container images can make them larger than necessary, which can slow down deploying new pods and scaling them.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Using the FROM scratch directive in the Dockerfile for minimal images can be difficult to debug because they don't include essential tools like ping or curl.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Referring to the latest image tag in pod manifests can cause problems because it's unclear which version of the image each individual pod replica is running.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Using a specific image tag instead of 'latest' and configuring imagePullPolicy wisely can help ensure that all pod replicas run the same version of the image.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"You don't need to modify the code of your app at all to make it easy to run and manage in Kubernetes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Including unnecessary files in container images can make them larger than necessary, which can slow down deploying new pods and scaling them.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Using the FROM scratch directive in the Dockerfile for minimal images can be difficult to debug because they don't include essential tools like ping or curl.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Referring to the latest image tag in pod manifests can cause problems because it's unclear which version of the image each individual pod replica is running.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Using a specific image tag instead of 'latest' and configuring imagePullPolicy wisely can help ensure that all pod replicas run the same version of the image.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:440\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Use tags containing a proper version designator instead of latest, except in development. Set imagePullPolicy field to Always for mutable tags.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Label all resources with multiple labels across each individual dimension, including application name, tier, environment, version, and type of release.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Use annotations to add additional information to resources, such as contact information, dependencies between pods, build and version info, and metadata for tooling or GUIs.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Set imagePullPolicy field to Always can slow down pod startup and prevent pod from starting up when registry cannot be contacted.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Provide information on why a container terminated through annotations or labels to make it easier to manage running applications.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Use tags containing a proper version designator instead of latest, except in development. Set imagePullPolicy field to Always for mutable tags.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Label all resources with multiple labels across each individual dimension, including application name, tier, environment, version, and type of release.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Use annotations to add additional information to resources, such as contact information, dependencies between pods, build and version info, and metadata for tooling or GUIs.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Set imagePullPolicy field to Always can slow down pod startup and prevent pod from starting up when registry cannot be contacted.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Provide information on why a container terminated through annotations or labels to make it easier to manage running applications.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:441\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"You can use one other Kubernetes feature that makes it possible to show the reason why a container terminated in the pod’s status.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The process needs to write the message to /dev/termination-log, but it can be changed by setting the terminationMessagePath field in the container definition in the pod spec.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"When running this pod, you’ll soon see the pod’s status shown as CrashLoopBackOff. If you then use kubectl describe, you can see why the container died,\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The container will write the message to the file just before exiting.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You’re overriding the default path of the termination message file.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"You can use one other Kubernetes feature that makes it possible to show the reason why a container terminated in the pod’s status.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The process needs to write the message to /dev/termination-log, but it can be changed by setting the terminationMessagePath field in the container definition in the pod spec.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"When running this pod, you’ll soon see the pod’s status shown as CrashLoopBackOff. If you then use kubectl describe, you can see why the container died,\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The container will write the message to the file just before exiting.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You’re overriding the default path of the termination message file.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:442\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes currently doesn’t provide any functionality for showing app-specific status messages of running containers.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"If the container doesn’t write the message to any file, you can set the terminationMessagePolicy field to FallbackToLogsOnError.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Apps should write to the standard output instead of files for easy log viewing with kubectl logs command.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can copy the log file to your local machine using the kubectl cp command, which allows you to copy files from and into a container.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To see the previous container’s logs, use the --previous option with kubectl logs command if a container crashes and is replaced with a new one.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes currently doesn’t provide any functionality for showing app-specific status messages of running containers.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"If the container doesn’t write the message to any file, you can set the terminationMessagePolicy field to FallbackToLogsOnError.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Apps should write to the standard output instead of files for easy log viewing with kubectl logs command.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can copy the log file to your local machine using the kubectl cp command, which allows you to copy files from and into a container.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To see the previous container’s logs, use the --previous option with kubectl logs command if a container crashes and is replaced with a new one.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:443\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes by itself doesn’t provide any kind of centralized logging.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Deploying centralized logging solutions is easy. All you need to do is deploy a few YAML/JSON manifests and you’re good to go.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When using the EFK stack for centralized logging, each Kubernetes cluster node runs a FluentD agent (usually as a pod deployed through a DaemonSet), which is responsible for gathering the logs from the containers,\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"ElasticSearch is also deployed as a pod somewhere in the cluster. The logs can then be viewed and analyzed in a web browser through Kibana, which is a web tool for visualizing ElasticSearch data.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You may have already heard of the ELK stack composed of ElasticSearch, Logstash, and Kibana. A slightly modified variation is the EFK stack, where Logstash is replaced with FluentD.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes by itself doesn’t provide any kind of centralized logging.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Deploying centralized logging solutions is easy. All you need to do is deploy a few YAML/JSON manifests and you’re good to go.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When using the EFK stack for centralized logging, each Kubernetes cluster node runs a FluentD agent (usually as a pod deployed through a DaemonSet), which is responsible for gathering the logs from the containers,\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"ElasticSearch is also deployed as a pod somewhere in the cluster. The logs can then be viewed and analyzed in a web browser through Kibana, which is a web tool for visualizing ElasticSearch data.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You may have already heard of the ELK stack composed of ElasticSearch, Logstash, and Kibana. A slightly modified variation is the EFK stack, where Logstash is replaced with FluentD.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"The FluentD agent stores each line of the log file as an entry in the ElasticSearch data store, but this can cause issues with multiline log statements.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Developing and running apps on a local machine during development is a viable option, allowing for faster iteration without the need to build and deploy containers.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Connecting to backend services in production can be achieved by setting environment variables on a local machine or temporarily exposing a Service externally via NodePort or LoadBalancer-type Service.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Outputting JSON logs instead of plain text can help solve the issue with multiline log statements, but this may make viewing logs with kubectl logs less human-friendly.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Configuring the node-level FluentD agent or adding a logging sidecar container to every pod is required to process JSON logs and keep outputting human-readable logs to standard output.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"The FluentD agent stores each line of the log file as an entry in the ElasticSearch data store, but this can cause issues with multiline log statements.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Developing and running apps on a local machine during development is a viable option, allowing for faster iteration without the need to build and deploy containers.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Connecting to backend services in production can be achieved by setting environment variables on a local machine or temporarily exposing a Service externally via NodePort or LoadBalancer-type Service.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Outputting JSON logs instead of plain text can help solve the issue with multiline log statements, but this may make viewing logs with kubectl logs less human-friendly.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Configuring the node-level FluentD agent or adding a logging sidecar container to every pod is required to process JSON logs and keep outputting human-readable logs to standard output.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:445\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can easily talk to the Kubernetes API server from outside the cluster during development using the ServiceAccount's token or an ambassador container.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When developing with Minikube, you can mount your local filesystem into the Minikube VM and then into your containers through a hostPath volume.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You don't need to rebuild the Docker image every time during development; instead, you can mount your local filesytem into the container using Docker volumes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Minikube is a valuable method of trying out your app in Kubernetes and developing resource manifests for your complete application.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use the Docker daemon inside the Minikube VM to build your images instead of building through your local Docker daemon and pushing to a registry.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can easily talk to the Kubernetes API server from outside the cluster during development using the ServiceAccount's token or an ambassador container.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When developing with Minikube, you can mount your local filesystem into the Minikube VM and then into your containers through a hostPath volume.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You don't need to rebuild the Docker image every time during development; instead, you can mount your local filesytem into the container using Docker volumes.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Minikube is a valuable method of trying out your app in Kubernetes and developing resource manifests for your complete application.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use the Docker daemon inside the Minikube VM to build your images instead of building through your local Docker daemon and pushing to a registry.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:446\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To use Minikube's Docker daemon, set the DOCKER_HOST environment variable by running $ eval $(minikube docker-env) on your local machine.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can build images locally and copy them over to the Minikube VM using the command $ docker save <image> | (eval $(minikube docker-env) && docker load).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When combining Minikube with a proper Kubernetes cluster, you can develop apps on your local Minikube cluster and deploy them to a remote multi-node cluster without modifications.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes uses a declarative model, so you only need to tell it your desired state, and it will take the necessary actions to reconcile the cluster state with the desired state using kubectl apply command.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can store resource manifests in a Version Control System, enabling code reviews, audit trails, and rolling back changes whenever necessary.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To use Minikube's Docker daemon, set the DOCKER_HOST environment variable by running $ eval $(minikube docker-env) on your local machine.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can build images locally and copy them over to the Minikube VM using the command $ docker save <image> | (eval $(minikube docker-env) && docker load).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"When combining Minikube with a proper Kubernetes cluster, you can develop apps on your local Minikube cluster and deploy them to a remote multi-node cluster without modifications.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes uses a declarative model, so you only need to tell it your desired state, and it will take the necessary actions to reconcile the cluster state with the desired state using kubectl apply command.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can store resource manifests in a Version Control System, enabling code reviews, audit trails, and rolling back changes whenever necessary.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:447\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can manage your running apps simply by committing changes to the VCS without having to manually talk to the Kubernetes API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Ksonnet is a library built on top of Jsonnet, which allows you to define parameterized JSON fragments and build a full JSON manifest by referencing those fragments by name.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Ksonnet defines the fragments you'd find in Kubernetes resource manifests, allowing you to quickly build a complete Kubernetes resource JSON manifest with much less code.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use multiple branches to deploy the manifests to a development, QA, staging, and production cluster (or in different namespaces in the same cluster).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Ksonnet allows you to define a container called kubia, which uses the luksa/kubia:v1 image and includes a port called http, and this will be expanded into a full Deployment resource.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can manage your running apps simply by committing changes to the VCS without having to manually talk to the Kubernetes API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Ksonnet is a library built on top of Jsonnet, which allows you to define parameterized JSON fragments and build a full JSON manifest by referencing those fragments by name.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Ksonnet defines the fragments you'd find in Kubernetes resource manifests, allowing you to quickly build a complete Kubernetes resource JSON manifest with much less code.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can use multiple branches to deploy the manifests to a development, QA, staging, and production cluster (or in different namespaces in the same cluster).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Ksonnet allows you to define a container called kubia, which uses the luksa/kubia:v1 image and includes a port called http, and this will be expanded into a full Deployment resource.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"The power of Ksonnet and Jsonnet becomes apparent when you can define your own higher-level fragments and make all your manifests consistent and duplication-free.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Employing Continuous Integration and Continuous Delivery (CI/CD) to build application binaries, container images, and resource manifests and then deploying them in one or more Kubernetes clusters.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The Fabric8 project is an integrated development platform for Kubernetes that includes Jenkins and various other tools to deliver a full CI/CD pipeline for DevOps-style development, deployment, and management of microservices on Kubernetes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Init containers can be used to initialize a pod or delay the start of the pod's main containers until a precondition is met.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Container lifecycle hooks can be used to make your apps shut down properly without breaking client connections, and understand the consequences of the distributed nature of Kubernetes components and its eventual consistency model.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"The power of Ksonnet and Jsonnet becomes apparent when you can define your own higher-level fragments and make all your manifests consistent and duplication-free.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Employing Continuous Integration and Continuous Delivery (CI/CD) to build application binaries, container images, and resource manifests and then deploying them in one or more Kubernetes clusters.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The Fabric8 project is an integrated development platform for Kubernetes that includes Jenkins and various other tools to deliver a full CI/CD pipeline for DevOps-style development, deployment, and management of microservices on Kubernetes.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Init containers can be used to initialize a pod or delay the start of the pod's main containers until a precondition is met.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Container lifecycle hooks can be used to make your apps shut down properly without breaking client connections, and understand the consequences of the distributed nature of Kubernetes components and its eventual consistency model.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:449\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"Keep image sizes small, add annotations and multi-dimensional labels to resources, and make it easier to see why an application terminated.\"}]\n",
      "\n",
      "[{\"highlight\": \"Develop Kubernetes apps and run them locally or in Minikube before deploying them on a proper multi-node cluster.\"}]\n",
      "\n",
      "[{\"highlight\": \"Extend Kubernetes with custom API objects and controllers to create complete Platform-as-a-Service solutions.\"}]\n",
      "\n",
      "[{\"highlight\": \"Learn how to manage apps by following small tips for easier management.\"}]\n",
      "\n",
      "[{\"highlight\": \"Develop and deploy Kubernetes apps locally or on a multi-node cluster.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Keep image sizes small, add annotations and multi-dimensional labels to resources, and make it easier to see why an application terminated.\"}]\n",
      "Done for page number:450\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"Defining custom API objects is a key aspect of extending Kubernetes, allowing users to create their own API objects and controllers.\"}]\n",
      "\n",
      "[{\"highlight\": \"Kubernetes users can extend the platform by adding custom objects, creating controllers for those objects, and adding custom API servers.\"}]\n",
      "\n",
      "[{\"highlight\": \"The Service Catalog in Kubernetes enables self-provisioning of services, making it easier to deploy applications on the platform.\"}]\n",
      "\n",
      "[{\"highlight\": \"Red Hat's OpenShift Container Platform and Deis Workflow are examples of Platform-as-a-Service solutions built on top of Kubernetes.\"}]\n",
      "\n",
      "[{\"highlight\": \"Helm is another example of a tool that has extended Kubernetes, providing a package manager for Kubernetes applications.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Defining custom API objects is a key aspect of extending Kubernetes, allowing users to create their own API objects and controllers.\"}]\n",
      "Done for page number:451\n",
      "Raw Output\n",
      "[{\"highlight\": \"Custom API objects can be defined in Kubernetes to represent whole applications or software services.\"}, \n",
      " {\"highlight\": \"A custom controller will observe those high-level objects and create low-level objects based on them, such as running a messaging broker inside a Kubernetes cluster.\"}, \n",
      " {\"highlight\": \"To define a new resource type, a CustomResourceDefinition object (CRD) must be posted to the Kubernetes API server.\"}, \n",
      " {\"highlight\": \"Each CRD will usually have an associated controller that makes something tangible happen in the cluster, such as spinning up a new web server pod and exposing it through a Service.\"}, \n",
      " {\"highlight\": \"Users can create instances of custom resources by posting JSON or YAML manifests to the API server, similar to creating any other Kubernetes resource.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Custom API objects can be defined in Kubernetes to represent whole applications or software services.\"}, \n",
      " {\"highlight\": \"A custom controller will observe those high-level objects and create low-level objects based on them, such as running a messaging broker inside a Kubernetes cluster.\"}, \n",
      " {\"highlight\": \"To define a new resource type, a CustomResourceDefinition object (CRD) must be posted to the Kubernetes API server.\"}, \n",
      " {\"highlight\": \"Each CRD will usually have an associated controller that makes something tangible happen in the cluster, such as spinning up a new web server pod and exposing it through a Service.\"}, \n",
      " {\"highlight\": \"Users can create instances of custom resources by posting JSON or YAML manifests to the API server, similar to creating any other Kubernetes resource.\"}]\n",
      "Done for page number:452\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"To make Kubernetes accept your custom Website resource instances, you need to post the CustomResourceDefinition shown in the following listing to the API server.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A custom object kind is required for each Website object, which should result in the creation of a Service and an HTTP server Pod.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The name of the website (used for naming the resulting Service and Pod) must be specified in the metadata.name field.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"CustomResourceDefinitions belong to this API group and version, which is apiextensions.k8s.io/v1beta1.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You want Website resources to be namespaced, so you need to specify scope: Namespaced in the CustomResourceDefinition manifest.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"To make Kubernetes accept your custom Website resource instances, you need to post the CustomResourceDefinition shown in the following listing to the API server.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A custom object kind is required for each Website object, which should result in the creation of a Service and an HTTP server Pod.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The name of the website (used for naming the resulting Service and Pod) must be specified in the metadata.name field.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"CustomResourceDefinitions belong to this API group and version, which is apiextensions.k8s.io/v1beta1.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You want Website resources to be namespaced, so you need to specify scope: Namespaced in the CustomResourceDefinition manifest.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:453\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"To create a custom API object, you need to define its group, version, and names in the CustomResourceDefinition (CRD).\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The CRD's long name is used to prevent name clashes, but you can use the shorter 'kind' property when creating instances of the custom resource.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"When creating an instance of a custom resource, you need to set the 'apiVersion' property to the API group and version number defined in the CRD.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can create a YAML manifest for your custom Website resource instance using the 'kind', 'metadata', and 'spec' properties.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To create an instance of the custom Website resource, you need to use the 'kubectl create -f' command with the YAML manifest file.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"To create a custom API object, you need to define its group, version, and names in the CustomResourceDefinition (CRD).\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The CRD's long name is used to prevent name clashes, but you can use the shorter 'kind' property when creating instances of the custom resource.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"When creating an instance of a custom resource, you need to set the 'apiVersion' property to the API group and version number defined in the CRD.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can create a YAML manifest for your custom Website resource instance using the 'kind', 'metadata', and 'spec' properties.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To create an instance of the custom Website resource, you need to use the 'kubectl create -f' command with the YAML manifest file.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"You can now retrieve your custom Website object using kubectl get websites.\"}]\n",
      "\n",
      "[{\"highlight\": \"Custom resources can be created, listed, described, and deleted using kubectl commands.\"}]\n",
      "\n",
      "[{\"highlight\": \"The resource includes everything that was in the original YAML definition, and Kubernetes has initialized additional metadata fields.\"}]\n",
      "\n",
      "[{\"highlight\": \"You're deleting an instance of a Website, not the Website CRD resource.\"}]\n",
      "\n",
      "[{\"highlight\": \"Custom objects don't do anything yet; you'll need to create a controller to make them do something.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"You can now retrieve your custom Website object using kubectl get websites.\"}]\n",
      "Done for page number:455\n",
      "Raw Output\n",
      "Here are the extracted highlights in JSON format, collated into a list:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Custom API objects can be used to store data instead of using a ConfigMap, allowing applications running inside pods to query the API server for those objects.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A custom Website controller is needed to automate the creation of a web server pod exposed through a Service for each Website object created in the API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Website controller creates a Deployment resource instead of an unmanaged Pod directly, ensuring the Pod is managed and survives node failures.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A simple initial version of the Website controller is available at docker.io/luksa/website-controller:latest, with source code at https://github.com/luksa/k8s-website-controller.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Website controller watches for Website objects and creates a Deployment and a Service for each one, as shown in Figure 18.2.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Custom API objects can be used to store data instead of using a ConfigMap, allowing applications running inside pods to query the API server for those objects.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A custom Website controller is needed to automate the creation of a web server pod exposed through a Service for each Website object created in the API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Website controller creates a Deployment resource instead of an unmanaged Pod directly, ensuring the Pod is managed and survives node failures.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A simple initial version of the Website controller is available at docker.io/luksa/website-controller:latest, with source code at https://github.com/luksa/k8s-website-controller.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Website controller watches for Website objects and creates a Deployment and a Service for each one, as shown in Figure 18.2.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:456\n",
      "Raw Output\n",
      "[{\"highlight\": \"The Kubernetes website controller starts watching Website objects immediately upon startup by requesting the URL http://localhost:8001/apis/extensions.example.com/v1/websites?watch=true.\"}, {\"highlight\": \"The API server sends watch events for every change to any Website object, including an ADDED event when a new Website object is created.\"}, {\"highlight\": \"The website controller extracts the Website's name and Git repository URL from the watch event and creates a Deployment and Service object by posting their JSON manifests to the API server.\"}, {\"highlight\": \"The Deployment resource contains a template for a pod with two containers: one running an nginx server and another running a git-sync process that keeps a local directory synced with a Git repo.\"}, {\"highlight\": \"A NodePort Service exposes the web server pod through a random port on each node, allowing clients to access the website through the node port.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The Kubernetes website controller starts watching Website objects immediately upon startup by requesting the URL http://localhost:8001/apis/extensions.example.com/v1/websites?watch=true.\"}, {\"highlight\": \"The API server sends watch events for every change to any Website object, including an ADDED event when a new Website object is created.\"}, {\"highlight\": \"The website controller extracts the Website's name and Git repository URL from the watch event and creates a Deployment and Service object by posting their JSON manifests to the API server.\"}, {\"highlight\": \"The Deployment resource contains a template for a pod with two containers: one running an nginx server and another running a git-sync process that keeps a local directory synced with a Git repo.\"}, {\"highlight\": \"A NodePort Service exposes the web server pod through a random port on each node, allowing clients to access the website through the node port.\"}]\n",
      "Done for page number:457\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\"highlight\": \"When a Website resource instance is deleted, the controller deletes the Deployment and Service resources it created earlier.\"},\n",
      "    {\"highlight\": \"The proper way to watch objects through the API server is to not only watch them, but also periodically re-list all objects in case any watch events were missed.\"},\n",
      "    {\"highlight\": \"To run the controller in Kubernetes, you can deploy it through a Deployment resource using an example like apiVersion: apps/v1beta1 kind: Deployment metadata: name: website-controller spec: replicas: 1 template: ...\"},\n",
      "    {\"highlight\": \"The controller will shut down and remove the web server serving that website as soon as a user deletes the Website instance.\"},\n",
      "    {\"highlight\": \"Running the controller inside Kubernetes itself is the best way to deploy it into production, rather than running it on a local development laptop.\"}\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\"highlight\": \"When a Website resource instance is deleted, the controller deletes the Deployment and Service resources it created earlier.\"},\n",
      "    {\"highlight\": \"The proper way to watch objects through the API server is to not only watch them, but also periodically re-list all objects in case any watch events were missed.\"},\n",
      "    {\"highlight\": \"To run the controller in Kubernetes, you can deploy it through a Deployment resource using an example like apiVersion: apps/v1beta1 kind: Deployment metadata: name: website-controller spec: replicas: 1 template: ...\"},\n",
      "    {\"highlight\": \"The controller will shut down and remove the web server serving that website as soon as a user deletes the Website instance.\"},\n",
      "    {\"highlight\": \"Running the controller inside Kubernetes itself is the best way to deploy it into production, rather than running it on a local development laptop.\"}\n",
      "]\n",
      "Done for page number:458\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The Deployment deploys a single replica of a two-container pod, one container running your controller and the other being the ambassador container for simpler communication with the API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To allow the controller to watch Website resources or create Deployments or Services, you need to bind the website-controller ServiceAccount to the cluster-admin ClusterRole using a ClusterRoleBinding.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The controller's Deployment can be deployed once the ServiceAccount and ClusterRoleBinding are in place.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To see the controller in action, create the kubia Website resource again after deploying the controller's Deployment.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The controller's logs show that it has received a watch event and created services and deployments with name kubia-website in namespace default.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The Deployment deploys a single replica of a two-container pod, one container running your controller and the other being the ambassador container for simpler communication with the API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To allow the controller to watch Website resources or create Deployments or Services, you need to bind the website-controller ServiceAccount to the cluster-admin ClusterRole using a ClusterRoleBinding.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The controller's Deployment can be deployed once the ServiceAccount and ClusterRoleBinding are in place.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To see the controller in action, create the kubia Website resource again after deploying the controller's Deployment.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The controller's logs show that it has received a watch event and created services and deployments with name kubia-website in namespace default.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\"highlight\": \"A custom API object was successfully created by the controller, including a Service and a Deployment for the kubia-website Website.\"},\n",
      "    {\"highlight\": \"The API server responded with a 201 Created response, indicating that the two resources should now exist.\"},\n",
      "    {\"highlight\": \"Users of your Kubernetes cluster can now deploy static websites in seconds without knowing anything about Pods, Services, or other Kubernetes resources.\"},\n",
      "    {\"highlight\": \"Validation schema was not specified in the Website CustomResourceDefinition, allowing users to create invalid Website objects.\"},\n",
      "    {\"highlight\": \"The controller can validate the object when it receives it in a watch event and update the object with an error message if it's invalid.\"}\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\"highlight\": \"A custom API object was successfully created by the controller, including a Service and a Deployment for the kubia-website Website.\"},\n",
      "    {\"highlight\": \"The API server responded with a 201 Created response, indicating that the two resources should now exist.\"},\n",
      "    {\"highlight\": \"Users of your Kubernetes cluster can now deploy static websites in seconds without knowing anything about Pods, Services, or other Kubernetes resources.\"},\n",
      "    {\"highlight\": \"Validation schema was not specified in the Website CustomResourceDefinition, allowing users to create invalid Website objects.\"},\n",
      "    {\"highlight\": \"The controller can validate the object when it receives it in a watch event and update the object with an error message if it's invalid.\"}\n",
      "]\n",
      "Done for page number:460\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page, wrapped in JSON with the key \"highlight\":\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes version 1.8 introduced validation of custom objects as an alpha feature.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To validate custom objects, you need to enable the CustomResourceValidation feature gate and specify a JSON schema in the CRD.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"API server aggregation was introduced in Kubernetes version 1.7, allowing multiple API servers to be exposed at a single location.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can create a custom API server responsible for handling your Website objects and validate them directly, eliminating the need for a CRD.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Each API server is responsible for storing its own resources, which can either run their own etcd instance or use CustomResourceDefinitions in the main API server as storage.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Kubernetes version 1.8 introduced validation of custom objects as an alpha feature.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"To validate custom objects, you need to enable the CustomResourceValidation feature gate and specify a JSON schema in the CRD.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"API server aggregation was introduced in Kubernetes version 1.7, allowing multiple API servers to be exposed at a single location.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"You can create a custom API server responsible for handling your Website objects and validate them directly, eliminating the need for a CRD.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Each API server is responsible for storing its own resources, which can either run their own etcd instance or use CustomResourceDefinitions in the main API server as storage.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:461\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page, wrapped in JSON with the key \"highlight\":\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To add a custom API server to your cluster, you’d deploy it as a pod and expose it through a Service. Then, to integrate it into the main API server, you’d deploy a YAML manifest describing an APIService resource.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Client requests sent to the main API server that contain any resource from the extensions.example.com API group and version v1alpha1 would be forwarded to the custom API server pod(s) exposed through the website-api Service.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can create custom resources from YAML files using the regular kubectl client, or build a custom CLI tool to add dedicated commands for manipulating those objects.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Service Catalog is a hot topic in the Kubernetes community, and one of the first additional API servers that will be added to Kubernetes through API server aggregation is the Service Catalog API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To create a custom API server, you need to create a CRD object first, before creating instances of the CRD, by creating a CRD instance in the core API server’s etcd store.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"To add a custom API server to your cluster, you’d deploy it as a pod and expose it through a Service. Then, to integrate it into the main API server, you’d deploy a YAML manifest describing an APIService resource.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Client requests sent to the main API server that contain any resource from the extensions.example.com API group and version v1alpha1 would be forwarded to the custom API server pod(s) exposed through the website-api Service.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can create custom resources from YAML files using the regular kubectl client, or build a custom CLI tool to add dedicated commands for manipulating those objects.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Service Catalog is a hot topic in the Kubernetes community, and one of the first additional API servers that will be added to Kubernetes through API server aggregation is the Service Catalog API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"To create a custom API server, you need to create a CRD object first, before creating instances of the CRD, by creating a CRD instance in the core API server’s etcd store.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:462\n",
      "Raw Output\n",
      "[{\"highlight\": \"Kubernetes is supposed to be an easy-to-use, self-service system where users can provision services like PostgreSQL databases without manual configuration and deployment.\"}, {\"highlight\": \"The Kubernetes Service Catalog introduces four generic API resources: ClusterServiceBroker, ClusterServiceClass, ServiceInstance, and ServiceBinding to enable users to browse and provision services without dealing with underlying resources.\"}, {\"highlight\": \"A cluster admin creates a ClusterServiceBroker resource for each service broker whose services they'd like to make available in the cluster.\"}, {\"highlight\": \"Users can create a ServiceInstance resource to request a specific service, such as PostgreSQL, and then bind it to client pods using a ServiceBinding resource.\"}, {\"highlight\": \"The relationships between Service Catalog API resources are defined by ClusterServiceBroker, ClusterServiceClass, ServiceInstance, and ServiceBinding, which enable users to provision services without manual configuration and deployment.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Kubernetes is supposed to be an easy-to-use, self-service system where users can provision services like PostgreSQL databases without manual configuration and deployment.\"}, {\"highlight\": \"The Kubernetes Service Catalog introduces four generic API resources: ClusterServiceBroker, ClusterServiceClass, ServiceInstance, and ServiceBinding to enable users to browse and provision services without dealing with underlying resources.\"}, {\"highlight\": \"A cluster admin creates a ClusterServiceBroker resource for each service broker whose services they'd like to make available in the cluster.\"}, {\"highlight\": \"Users can create a ServiceInstance resource to request a specific service, such as PostgreSQL, and then bind it to client pods using a ServiceBinding resource.\"}, {\"highlight\": \"The relationships between Service Catalog API resources are defined by ClusterServiceBroker, ClusterServiceClass, ServiceInstance, and ServiceBinding, which enable users to provision services without manual configuration and deployment.\"}]\n",
      "Done for page number:463\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The Kubernetes Service Catalog is a distributed system composed of three components: Service Catalog API Server, etcd as the storage, and Controller Manager.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Four Service Catalog-related resources (ServiceInstance, ClusterServiceClass, ServiceBroker, and Plans) are created by posting YAML/JSON manifests to the API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The controllers running in the Controller Manager provision services by talking to external service brokers registered through ServiceBroker resources.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Client pods use the provisioned services provided by external systems, which are managed by the Kubernetes Service Catalog.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Service Catalog API server stores resources in its own etcd instance or uses CustomResourceDefinitions in the main API server as an alternative storage mechanism.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"The Kubernetes Service Catalog is a distributed system composed of three components: Service Catalog API Server, etcd as the storage, and Controller Manager.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Four Service Catalog-related resources (ServiceInstance, ClusterServiceClass, ServiceBroker, and Plans) are created by posting YAML/JSON manifests to the API server.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The controllers running in the Controller Manager provision services by talking to external service brokers registered through ServiceBroker resources.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Client pods use the provisioned services provided by external systems, which are managed by the Kubernetes Service Catalog.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The Service Catalog API server stores resources in its own etcd instance or uses CustomResourceDefinitions in the main API server as an alternative storage mechanism.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[{\"highlight\": \"The Service Catalog talks to the broker through the OpenServiceBroker API, which is a REST API providing operations for retrieving services, provisioning service instances, updating instances, binding and unbinding instances, and deprovisioning instances.\"}, {\"highlight\": \"A cluster administrator can register one or more external ServiceBrokers in the Service Catalog by posting a ServiceBroker resource manifest to the Service Catalog API.\"}, {\"highlight\": \"Each ClusterServiceClass resource describes a single type of service that can be provisioned, with associated service plans allowing users to choose the level of service they need.\"}, {\"highlight\": \"The OpenServiceBroker API spec is available at https://github.com/openservicebrokerapi/servicebroker.\"}, {\"highlight\": \"The Service Catalog creates a ClusterServiceClass resource for each service this broker can provision after retrieving the list of services from the broker's URL.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"The Service Catalog talks to the broker through the OpenServiceBroker API, which is a REST API providing operations for retrieving services, provisioning service instances, updating instances, binding and unbinding instances, and deprovisioning instances.\"}, {\"highlight\": \"A cluster administrator can register one or more external ServiceBrokers in the Service Catalog by posting a ServiceBroker resource manifest to the Service Catalog API.\"}, {\"highlight\": \"Each ClusterServiceClass resource describes a single type of service that can be provisioned, with associated service plans allowing users to choose the level of service they need.\"}, {\"highlight\": \"The OpenServiceBroker API spec is available at https://github.com/openservicebrokerapi/servicebroker.\"}, {\"highlight\": \"The Service Catalog creates a ClusterServiceClass resource for each service this broker can provision after retrieving the list of services from the broker's URL.\"}]\n",
      "Done for page number:465\n",
      "Raw Output\n",
      "[{\"highlight\": \"Kubernetes Service Catalog allows users to retrieve a list of all services that can be provisioned in a cluster with kubectl get serviceclasses.\"}, {\"highlight\": \"ClusterServiceClasses are similar to StorageClasses, but allow users to select the type of service rather than storage.\"}, {\"highlight\": \"Each ClusterServiceClass has multiple plans, such as free and premium plans, which can be selected by users.\"}, {\"highlight\": \"Users can retrieve details of a ClusterServiceClass by running kubectl get serviceclass <serviceclass_name> -o yaml.\"}, {\"highlight\": \"The Kubernetes Service Catalog is provided by the database-broker broker in this example.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Kubernetes Service Catalog allows users to retrieve a list of all services that can be provisioned in a cluster with kubectl get serviceclasses.\"}, {\"highlight\": \"ClusterServiceClasses are similar to StorageClasses, but allow users to select the type of service rather than storage.\"}, {\"highlight\": \"Each ClusterServiceClass has multiple plans, such as free and premium plans, which can be selected by users.\"}, {\"highlight\": \"Users can retrieve details of a ClusterServiceClass by running kubectl get serviceclass <serviceclass_name> -o yaml.\"}, {\"highlight\": \"The Kubernetes Service Catalog is provided by the database-broker broker in this example.\"}]\n",
      "Done for page number:466\n",
      "Raw Output\n",
      "[{\"highlight\": \"To have the database provisioned for you, all you need to do is create a Service-Instance resource, as shown in the following listing.\"}, {\"highlight\": \"You created a ServiceInstance called my-postgres-db and specified the ClusterServiceClass and the chosen plan.\"}, {\"highlight\": \"The Service Catalog will contact the broker the ClusterServiceClass belongs to and ask it to provision the service.\"}, {\"highlight\": \"You can check if the service has been provisioned successfully by inspecting the status section of the my-postgres-db ServiceInstance you created.\"}, {\"highlight\": \"A ServiceInstance manifest: database-instance.yaml\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"To have the database provisioned for you, all you need to do is create a Service-Instance resource, as shown in the following listing.\"}, {\"highlight\": \"You created a ServiceInstance called my-postgres-db and specified the ClusterServiceClass and the chosen plan.\"}, {\"highlight\": \"The Service Catalog will contact the broker the ClusterServiceClass belongs to and ask it to provision the service.\"}, {\"highlight\": \"You can check if the service has been provisioned successfully by inspecting the status section of the my-postgres-db ServiceInstance you created.\"}, {\"highlight\": \"A ServiceInstance manifest: database-instance.yaml\"}]\n",
      "Done for page number:467\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"A ServiceBinding resource is created to use a provisioned ServiceInstance in pods, referencing the instance and specifying a name for a Secret where credentials will be stored.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The Service Catalog creates a new Secret with the specified name and stores all data necessary for connecting to the database in it.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The newly created Secret can be mounted into pods, allowing them to read its contents and connect to the provisioned service instance.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Currently, the Service Catalog does not make it possible to inject pods with the ServiceInstance's credentials, but this will be possible when PodPresets is available.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A Secret created by the Service Catalog system can store all data necessary for connecting to a provisioned service instance, such as database credentials.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"A ServiceBinding resource is created to use a provisioned ServiceInstance in pods, referencing the instance and specifying a name for a Secret where credentials will be stored.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The Service Catalog creates a new Secret with the specified name and stores all data necessary for connecting to the database in it.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The newly created Secret can be mounted into pods, allowing them to read its contents and connect to the provisioned service instance.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Currently, the Service Catalog does not make it possible to inject pods with the ServiceInstance's credentials, but this will be possible when PodPresets is available.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"A Secret created by the Service Catalog system can store all data necessary for connecting to a provisioned service instance, such as database credentials.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:468\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"You can deploy pods before provisioning or binding the service, as long as a Secret exists.\"}]\n",
      "\n",
      "[{\"highlight\": \"Multiple bindings can be created for different pods, with each binding having its own set of credentials to prevent pods from using the service by deleting the ServiceBinding resource.\"}]\n",
      "\n",
      "[{\"highlight\": \"When you delete a ServiceBinding, the Service Catalog controller will delete the Secret and call the broker to perform an unbinding operation, but the service instance (in this case, a PostgreSQL database) is still running.\"}]\n",
      "\n",
      "[{\"highlight\": \"To completely remove a service, you should also delete the ServiceInstance resource, which causes the Service Catalog to perform a deprovisioning operation on the service broker and shut down the service instance.\"}]\n",
      "\n",
      "[{\"highlight\": \"The Service Catalog enables service providers to expose services in any Kubernetes cluster by registering the broker in that cluster, making it possible for users to provision and use external services within their cluster.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"You can deploy pods before provisioning or binding the service, as long as a Secret exists.\"}]\n",
      "Done for page number:469\n",
      "Raw Output\n",
      "Here are the 5 most important highlights extracted from the document page:\n",
      "\n",
      "[{\"highlight\": \"Kubernetes is becoming a widely accepted foundation for the new generation of Platform-as-a-Service offerings.\"}]\n",
      "\n",
      "[{\"highlight\": \"Red Hat OpenShift is a Platform-as-a-Service that enables rapid development, easy deployment, scaling, and long-term maintenance of applications on top of Kubernetes.\"}]\n",
      "\n",
      "[{\"highlight\": \"OpenShift automates the building of application images and their automatic deployment without requiring a Continuous Integration solution into your cluster.\"}]\n",
      "\n",
      "[{\"highlight\": \"OpenShift provides user and group management for running a properly secured multi-tenant Kubernetes cluster with network-isolated apps by default.\"}]\n",
      "\n",
      "[{\"highlight\": \"OpenShift offers additional API objects, including Users & Groups and Projects, in addition to the resources available in Kubernetes.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Kubernetes is becoming a widely accepted foundation for the new generation of Platform-as-a-Service offerings.\"}]\n",
      "Done for page number:470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[{\"highlight\": \"Kubernetes doesn't have an API object for representing an individual user of the cluster, but OpenShift provides powerful user management features.\"}, {\"highlight\": \"Each user has access to certain Projects, which are nothing more than Kubernetes Namespaces with additional annotations.\"}, {\"highlight\": \"Users can only act on resources that reside in the projects the user has access to.\"}, {\"highlight\": \"OpenShift allows a parameterizable list in JSON or YAML manifest called Template; it's a list of objects whose definitions can include placeholders.\"}, {\"highlight\": \"A template itself is a JSON or YAML file containing a list of parameters referenced in resources defined in that same JSON/YAML.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"Kubernetes doesn't have an API object for representing an individual user of the cluster, but OpenShift provides powerful user management features.\"}, {\"highlight\": \"Each user has access to certain Projects, which are nothing more than Kubernetes Namespaces with additional annotations.\"}, {\"highlight\": \"Users can only act on resources that reside in the projects the user has access to.\"}, {\"highlight\": \"OpenShift allows a parameterizable list in JSON or YAML manifest called Template; it's a list of objects whose definitions can include placeholders.\"}, {\"highlight\": \"A template itself is a JSON or YAML file containing a list of parameters referenced in resources defined in that same JSON/YAML.\"}]\n",
      "Done for page number:471\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"OpenShift provides a long list of pre-fabricated templates that allow users to quickly run complex applications by specifying a few arguments (or none at all, if the template provides good defaults for those arguments).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You don’t need to build the container image at all—OpenShift does that for you. This is done by creating a resource called Build-Config, which can be configured to trigger builds of container images immediately after a change is committed to the source Git repository.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Once a new container image is built, it can also automatically be deployed in the cluster. This is enabled by creating a DeploymentConfig object and pointing it to an ImageStream.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"OpenShift takes care of building, deploying, and managing apps from that code, allowing developers to develop their code, commit, and push it to a Git repo without worrying about containers.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"An ImageStream is a stream of images. When an image is built, it’s added to the ImageStream, enabling the DeploymentConfig to notice the newly built image and initiate a rollout of the new image.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"OpenShift provides a long list of pre-fabricated templates that allow users to quickly run complex applications by specifying a few arguments (or none at all, if the template provides good defaults for those arguments).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You don’t need to build the container image at all—OpenShift does that for you. This is done by creating a resource called Build-Config, which can be configured to trigger builds of container images immediately after a change is committed to the source Git repository.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Once a new container image is built, it can also automatically be deployed in the cluster. This is enabled by creating a DeploymentConfig object and pointing it to an ImageStream.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"OpenShift takes care of building, deploying, and managing apps from that code, allowing developers to develop their code, commit, and push it to a Git repo without worrying about containers.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"An ImageStream is a stream of images. When an image is built, it’s added to the ImageStream, enabling the DeploymentConfig to notice the newly built image and initiate a rollout of the new image.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:472\n",
      "Raw Output\n",
      "Here are the extracted highlights wrapped in JSON:\n",
      "\n",
      "[{\"highlight\": \"A DeploymentConfig is almost identical to the Deployment object in Kubernetes, but it pre-dates it.\"}, \n",
      " {\"highlight\": \"To expose Services to the outside world, you needed to use NodePort or LoadBalancer-type Services, but OpenShift provided a better option through a Route resource.\"}, \n",
      " {\"highlight\": \"A Route needs a Router, which is a controller that provides the load balancer or proxy, and this Router is available out of the box in OpenShift.\"}, \n",
      " {\"highlight\": \"Deis Workflow and Helm provide a PaaS built on top of Kubernetes, with features like BuildConfigs and DeploymentConfigs.\"}, \n",
      " {\"highlight\": \"Minishift and OpenShift Online Starter are free multi-tenant, hosted solutions provided to get you started with OpenShift.\"}]\n",
      "JSON OUTPUT:\n",
      "[{\"highlight\": \"A DeploymentConfig is almost identical to the Deployment object in Kubernetes, but it pre-dates it.\"}, \n",
      " {\"highlight\": \"To expose Services to the outside world, you needed to use NodePort or LoadBalancer-type Services, but OpenShift provided a better option through a Route resource.\"}, \n",
      " {\"highlight\": \"A Route needs a Router, which is a controller that provides the load balancer or proxy, and this Router is available out of the box in OpenShift.\"}, \n",
      " {\"highlight\": \"Deis Workflow and Helm provide a PaaS built on top of Kubernetes, with features like BuildConfigs and DeploymentConfigs.\"}, \n",
      " {\"highlight\": \"Minishift and OpenShift Online Starter are free multi-tenant, hosted solutions provided to get you started with OpenShift.\"}]\n",
      "Done for page number:473\n",
      "Raw Output\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Deis Workflow is a tool that can be deployed on any existing Kubernetes cluster and provides developers with a simple, developer-friendly environment.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Helm is a package manager for Kubernetes that allows users to deploy and manage application packages in a Kubernetes cluster through the use of Charts, Configs, and Releases.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Charts are pre-built application packages in Helm that can be used to deploy applications in a Kubernetes cluster, with many charts available in the community at https://github.com/kubernetes/charts.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Tiller is a server component running as a Pod inside the Kubernetes cluster that creates all the necessary Kubernetes resources defined in a Chart when deploying an application through Helm.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The helm CLI tool can be used to deploy and manage Releases, which are running instances of applications created by combining Charts with Configs, without requiring users to build and install their own Kubernetes manifests for such applications.\"\n",
      "  }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "  {\n",
      "    \"highlight\": \"Deis Workflow is a tool that can be deployed on any existing Kubernetes cluster and provides developers with a simple, developer-friendly environment.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Helm is a package manager for Kubernetes that allows users to deploy and manage application packages in a Kubernetes cluster through the use of Charts, Configs, and Releases.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Charts are pre-built application packages in Helm that can be used to deploy applications in a Kubernetes cluster, with many charts available in the community at https://github.com/kubernetes/charts.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"Tiller is a server component running as a Pod inside the Kubernetes cluster that creates all the necessary Kubernetes resources defined in a Chart when deploying an application through Helm.\"\n",
      "  },\n",
      "  {\n",
      "    \"highlight\": \"The helm CLI tool can be used to deploy and manage Releases, which are running instances of applications created by combining Charts with Configs, without requiring users to build and install their own Kubernetes manifests for such applications.\"\n",
      "  }\n",
      "]\n",
      "Done for page number:474\n",
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can run a PostgreSQL or MySQL database in your Kubernetes cluster by using a pre-made Helm chart, which can be installed with a single one-line command.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Helm charts provide a convenient way to install and manage applications in a Kubernetes cluster, without requiring manual configuration of components.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The OpenVPN chart is an example of a Helm chart that allows you to run an OpenVPN server inside your Kubernetes cluster and access Services as if your local machine was a pod in the cluster.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Companies like Red Hat and Deis (now Microsoft) have extended Kubernetes by providing pre-made Helm charts for various applications, making it easier to use and extend the platform.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Helm is a package manager for Kubernetes that allows you to manage charts, which are files on local disk that contain configuration and other metadata for deploying applications in a cluster.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can run a PostgreSQL or MySQL database in your Kubernetes cluster by using a pre-made Helm chart, which can be installed with a single one-line command.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Helm charts provide a convenient way to install and manage applications in a Kubernetes cluster, without requiring manual configuration of components.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The OpenVPN chart is an example of a Helm chart that allows you to run an OpenVPN server inside your Kubernetes cluster and access Services as if your local machine was a pod in the cluster.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Companies like Red Hat and Deis (now Microsoft) have extended Kubernetes by providing pre-made Helm charts for various applications, making it easier to use and extend the platform.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Helm is a package manager for Kubernetes that allows you to manage charts, which are files on local disk that contain configuration and other metadata for deploying applications in a cluster.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Custom resources can be registered in the API server by creating a Custom-ResourceDefinition object.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Instances of custom objects can be stored, retrieved, updated, and deleted without having to change the API server code.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A custom controller can be implemented to bring those objects to life.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes Service Catalog makes it possible to self-provision external services and expose them to pods running in the Kubernetes cluster.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A package manager called Helm makes deploying existing apps without requiring you to build resource manifests for them.\"\n",
      "    }\n",
      "]\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"Custom resources can be registered in the API server by creating a Custom-ResourceDefinition object.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Instances of custom objects can be stored, retrieved, updated, and deleted without having to change the API server code.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A custom controller can be implemented to bring those objects to life.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"Kubernetes Service Catalog makes it possible to self-provision external services and expose them to pods running in the Kubernetes cluster.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"A package manager called Helm makes deploying existing apps without requiring you to build resource manifests for them.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:476\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\"highlight\": \"You can run examples in this book either in a Minikube cluster or a Google Kubernetes Engine (GKE) cluster.\"},\n",
      "    {\"highlight\": \"To switch between Minikube and GKE, you need to reconfigure kubectl using the 'minikube start' command for Minikube and the 'gcloud container clusters get-credentials' command for GKE.\"},\n",
      "    {\"highlight\": \"Minikube automatically sets up kubectl every time you start a new cluster.\"},\n",
      "    {\"highlight\": \"To switch from GKE to Minikube, stop Minikube and restart it; this will reconfigure kubectl to use the Minikube cluster again.\"},\n",
      "    {\"highlight\": \"The 'gcloud container clusters get-credentials' command is used to configure kubectl to use a specific GKE cluster.\"}\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\"highlight\": \"You can run examples in this book either in a Minikube cluster or a Google Kubernetes Engine (GKE) cluster.\"},\n",
      "    {\"highlight\": \"To switch between Minikube and GKE, you need to reconfigure kubectl using the 'minikube start' command for Minikube and the 'gcloud container clusters get-credentials' command for GKE.\"},\n",
      "    {\"highlight\": \"Minikube automatically sets up kubectl every time you start a new cluster.\"},\n",
      "    {\"highlight\": \"To switch from GKE to Minikube, stop Minikube and restart it; this will reconfigure kubectl to use the Minikube cluster again.\"},\n",
      "    {\"highlight\": \"The 'gcloud container clusters get-credentials' command is used to configure kubectl to use a specific GKE cluster.\"}\n",
      "]\n",
      "Done for page number:477\n",
      "Raw Output\n",
      "Here are the 5 most important and informative highlights extracted from the document page:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can use multiple config files and have kubectl use them all at once by specifying all of them in the KUBECONFIG environment variable (separate them with a colon).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The kubeconfig file consists of four sections: A list of clusters, A list of users, A list of contexts, and The name of the current context.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"An example config file is shown in the following listing, which includes information about a Kubernetes cluster, defines a kubectl context, contains a user's credentials, and specifies the current context used by kubectl.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The KUBECONFIG environment variable needs to point to the location of the kubeconfig file if it's stored somewhere else than the default ~/.kube/config file.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can switch between different Kubernetes clusters or work in a different namespace without specifying the --namespace option every time you run kubectl by configuring the kubeconfig file and using the KUBECONFIG environment variable.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "JSON OUTPUT:\n",
      "[\n",
      "    {\n",
      "        \"highlight\": \"You can use multiple config files and have kubectl use them all at once by specifying all of them in the KUBECONFIG environment variable (separate them with a colon).\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The kubeconfig file consists of four sections: A list of clusters, A list of users, A list of contexts, and The name of the current context.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"An example config file is shown in the following listing, which includes information about a Kubernetes cluster, defines a kubectl context, contains a user's credentials, and specifies the current context used by kubectl.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"The KUBECONFIG environment variable needs to point to the location of the kubeconfig file if it's stored somewhere else than the default ~/.kube/config file.\"\n",
      "    },\n",
      "    {\n",
      "        \"highlight\": \"You can switch between different Kubernetes clusters or work in a different namespace without specifying the --namespace option every time you run kubectl by configuring the kubeconfig file and using the KUBECONFIG environment variable.\"\n",
      "    }\n",
      "]\n",
      "Done for page number:478\n"
     ]
    }
   ],
   "source": [
    "for idx in range(start_page_idx,end_page_index):\n",
    "   \n",
    "        enrich_page(idx)\n",
    "        print(\"Done for page number:\"+str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c44d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Store data (serialize in a pickle) upto page 102\n",
    "with open('./pdf_enriched_output/pdf_enriched_content_dict_phase5_extract_highligts_478_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(document_dict_deserialized_stage2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbf33a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
