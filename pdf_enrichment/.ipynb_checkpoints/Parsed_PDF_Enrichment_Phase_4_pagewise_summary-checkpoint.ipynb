{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8447b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python venv: search_agent_poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4375a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This module will take the serialized dictionary got out of PDF Parsing ann try to extract\n",
    "##Semantic Knowldge like identifying \n",
    "## 1.Important Objects/Entities\n",
    "## 2.Deduplicate Entities\n",
    "## 3.Extracting Relations\n",
    "## 4.Extract the main Ideas/Topics around Each Page\n",
    "## 5.Link the different topics via diffrent entities/Objects\n",
    "## 6.Break down the document by pages instead of Chunks .\n",
    "## 7.If a page does not fit a chunk then chunk them extract information and then deduplicate the information across\n",
    "## the page.\n",
    "\n",
    "#Next Steps:\n",
    "## 5.Try to Seggregate the BigPDF on Sections.\n",
    "## 8.Try To Find Common Objects or ideas that link these sections.\n",
    "## 9.Try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15191ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "#rag_test_input_path='/home/matrix4284/MY_GEN_AI_PROJECTS/RAG/GraphRAG/graphrag-local-ollama/ragtest/input/'+file_name\n",
    "import os\n",
    "# importing shutil module\n",
    "import shutil\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import XMLOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01aaca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"llama3.1\"\n",
    "book_text=''\n",
    "page_text=''\n",
    "file_name='Kubernetes_in_action_text_only'\n",
    "extension='.txt'\n",
    "start_page_idx=132\n",
    "end_page_index=480\n",
    "#full_filename=file_name+'_'+str(page_idx)+extension\n",
    "#full_filename\n",
    "pdf_enrichment_output_dir='./pdf_enriched_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbcded8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matrix4284/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/matrix4284/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#LLM Model for Prompt Tuning\n",
    "llm = ChatOllama(base_url=\"http://192.168.50.100:11434\",model=model_name)\n",
    "\n",
    "#embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",model_kwargs = model_kwargs)\n",
    "\n",
    "##Define Vectorstore\n",
    "vectorstore = Chroma(embedding_function=embeddings, persist_directory=\"./chroma_kubernetes_in_action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a648bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Load data (deserialized data) from relationship_summary phase3:\n",
    "### pdf_enriched_content_dict_phase3_relationship_summary_exraction_478_final.pickle is outout of Phase2\n",
    "#with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_phase3_relationship_summary_exraction_478_final.pickle', 'rb') as handle:\n",
    "#    document_dict_deserialized = pickle.load(handle)\n",
    " \n",
    "##Incremental if load fails\n",
    "with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_phase4_pagewise_summary_131.pickle', 'rb') as handle:\n",
    "    document_dict_deserialized = pickle.load(handle)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44433479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_dict_deserialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da4eb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dict_deserialized_stage2=document_dict_deserialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20db8986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document_dict_deserialized_stage2[0]['summary_rel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f9e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "def extract_summary_per_page(page_text):\n",
    "    \n",
    "    \n",
    "    parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker \\n\n",
    "            Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a document page and extract a small summary within 150 words that will\\n\n",
    "            be enough to represent all information for that page.\n",
    "            There is no need to mention any header statement before the summary.\n",
    "            Wrap the summary in a json with key named summary.\n",
    "            Output the json and nothing else no headers no footers.\n",
    "            Here is the document page: \\n\\n {context} \\n\\n\"\"\",\n",
    "            input_variables=[\"context\"],\n",
    "            ###Introduced by Kaustav while experimenting with XMLParsers\n",
    "            #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"context\": page_text,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    json_output = output\n",
    "    json_output=json_output.split('{')[1]\n",
    "    #page_output_json=json.loads(output)\n",
    "    \n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    json_output='{'+json_output\n",
    "    \n",
    "    json_output=json_output.split('}')[0]\n",
    "    \n",
    "    json_output=json_output+'}'\n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output\n",
    "    \n",
    "    #return page_output_json\n",
    "    #return output\n",
    "\n",
    "    \n",
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "def extract_highlights_per_page(page_text):\n",
    "    \n",
    "    \n",
    "    parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker,\\n\n",
    "            Machine Learning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a document page and extract upto maximum of 10 highlights from the page \\n\n",
    "            content.\\n\n",
    "            Extract the highlights following the mentioned rules below:\\n\n",
    "            1.For each highlight wrap it up in json with the key named highlight.\\n\n",
    "            2.After all the highlights have been extracted collate them into a list of json.\\n\n",
    "            Out should only contain the list of json and no other words or character or sentences.\\n\n",
    "            Here is the document page: \\n\\n {context} \\n\\n\"\"\",\n",
    "            input_variables=[\"context\"],\n",
    "            ###Introduced by Kaustav while experimenting with XMLParsers\n",
    "            #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"context\": page_text,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    json_output=output.split('[')[1]\n",
    "    #page_output_json=json.loads(output)\n",
    "    \n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    json_output='['+json_output\n",
    "    \n",
    "    \n",
    "    json_output=reverse(json_output)\n",
    "    \n",
    "    print('Reversed JSON OUTPUT:')\n",
    "    print(json_output)\n",
    "    \n",
    "    json_output=json_output.split(']')[1]\n",
    "    \n",
    "    json_output=reverse(json_output)\n",
    "    #json_output=json_output.rsplit(']')[-1]\n",
    "    #page_output_json=json.loads(output)\n",
    "    \n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    json_output= json_output + ']'\n",
    "    \n",
    "    print('JSON OUTPUT:')\n",
    "    print(json_output)\n",
    "    \n",
    "    \n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output\n",
    "    \n",
    "    #return page_output_json\n",
    "    #return output\n",
    "\n",
    "def entity_collector_per_page(entity_lst):\n",
    "\n",
    "    entities=[]\n",
    "    \n",
    "    for entity in entity_lst:\n",
    "        print(\"Entity:\")\n",
    "        print(entity)\n",
    "        entity_name=entity['entity']\n",
    "        entities.append(entity_name)\n",
    "    return list(set(entities))\n",
    "\n",
    "\n",
    "def enrich_page(page_idx):\n",
    "    \n",
    "    #print(\"Page Number\")\n",
    "    #print(page_idx)\n",
    "    \n",
    "    page_text=document_dict_deserialized_stage2[page_idx]['text']\n",
    "    \n",
    "    ##Entity Extraction Enrichment\n",
    "    page_summary_txt=extract_summary_per_page(page_text)\n",
    "    \n",
    "    #print(\"Page Summary Text\")\n",
    "    print(page_summary_txt)\n",
    "    \n",
    "    #print(\"Page Summary Json\")\n",
    "    #print(page_entity_lst_dict)\n",
    "    \n",
    "    #page_highlights_lst_dict_json={}\n",
    "    #page_highlights_lst_dict_json['summary']=page_highlights_lst_dict\n",
    "    \n",
    "    #page_highlights_lst_dict_json=json.loads(page_highlights_lst_dict)\n",
    "    document_dict_deserialized_stage2[page_idx]['summary']=json.loads(page_summary_txt.strip())[\"summary\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5465a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"A pod of containers allows you to run closely related processes together, providing them with almost the same environment as a single container, while keeping them somewhat isolated. Containers within a pod share certain resources like network interfaces and IPC namespaces, but have fully isolated filesystems unless shared using a Kubernetes Volume concept.\"}\n",
      "Done for page number:0\n",
      "{\"summary\": \"Kubernetes pods are logical hosts that behave like physical hosts or VMs, with processes running in the same pod behaving like processes on the same machine. Each pod has its own IP address and can communicate directly with other pods through a flat network, without NAT gateways. Pods should be organized by app, with each containing tightly related components or processes, allowing for as many pods as needed without significant overhead.\"}\n",
      "Done for page number:1\n",
      "{\"summary\": \"A multi-tier application consisting of frontend and backend components should not be configured as a single pod but rather split into multiple pods to enable individual scaling and utilize computational resources on multiple nodes. This approach allows for separate scaling requirements for frontend and backend components, making it more efficient and suitable for applications with diverse resource needs.\"}\n",
      "Done for page number:2\n",
      "{\"summary\": \"Pods in Kubernetes are groups of containers that can be run together, like a web server and a sidecar container for downloading content. To decide when to use multiple containers in a pod, ask yourself: do they need to run together, represent a single whole, or must they be scaled together? Typically, containers should be run in separate pods unless a specific reason requires them to be part of the same pod.\"}\n",
      "Done for page number:3\n",
      "{\"summary\": \"You can create pods by posting a JSON or YAML manifest to the Kubernetes REST API endpoint. This method allows configuration of all properties, but requires knowledge of the Kubernetes API object definitions. Alternatively, you can use commands like kubectl run, but they limit the configurable properties. The YAML descriptor for an existing pod can be obtained using kubectl get with the -o yaml option, showing metadata and specification details.\"}\n",
      "Done for page number:4\n",
      "{\"summary\": \"A Kubernetes Pod is a logical host for one or more application containers. It consists of metadata (name, namespace, labels) and spec (containers, volumes), with optional detailed status information. Key elements include terminationMessagePath, dnsPolicy, restartPolicy, serviceAccount, and volumeMounts.\"}\n",
      "Done for page number:5\n",
      "{\n",
      "  \"summary\": \"A Kubernetes pod can be created using a YAML or JSON descriptor, which typically consists of three parts: metadata, spec, and status. The spec section defines the container's image, name, and ports, with specifying ports being informational only. A simple example is shown in kubia-manual.yaml, where a single container based on luksa/kubia image listens on port 8080.\"\n",
      "}\n",
      "Done for page number:6\n",
      "{\"summary\": \"A Kubernetes pod is a collection of containers that run on a host, and can be described using a manifest. The pod spec contains attributes such as hostname, IP addresses, ports, and volumes that can be mounted by containers. Using kubectl explain, one can discover possible API object fields and drill deeper to learn more about each attribute.\"}\n",
      "Done for page number:7\n",
      "{\"summary\": \"To create a pod from a YAML file, use kubectl create -f command. After creating the pod, you can retrieve its full YAML or JSON definition using kubectl get po <pod_name> -o yaml/json commands. You can also view application logs by tailing the container's standard output and error streams.\"}\n",
      "Done for page number:8\n",
      "{\n",
      "  \"summary\": \"This chapter discusses pods in Kubernetes, focusing on retrieving logs from containers running within a pod. The container runtime redirects streams to files, allowing users to view logs by running `docker logs <container id>`. However, Kubernetes provides an easier way using `kubectl logs`, which can be used to retrieve logs from a pod without the need for SSH access. Additionally, if a pod contains multiple containers, the user must specify the container name when retrieving logs. The chapter also touches on centralized logging and port forwarding as methods to connect to a pod for testing and debugging purposes.\"\n",
      "}\n",
      "Done for page number:9\n",
      "{\"summary\": \"Kubernetes allows port forwarding to a specific pod through the `kubectl port-forward` command, enabling direct access for debugging or testing purposes. This can be achieved by running `$ kubectl port-forward kubia-manual 8888:8080` and sending an HTTP request using `curl localhost:8888`. This method is effective for testing individual pods, especially in microservices architectures where many pods need to be categorized and managed.\"}\n",
      "Done for page number:10\n",
      "{\"summary\": \"Kubernetes allows running multiple copies of the same component and different versions concurrently, which can lead to hundreds of pods without organization. To manage this, labels are used to organize pods and other Kubernetes resources into smaller groups based on arbitrary criteria, allowing developers and administrators to easily identify and operate on specific pods or groups with a single action.\"}\n",
      "Done for page number:11\n",
      "{\"summary\": \"Adding labels to pods in a Kubernetes system allows for easy organization and understanding of the system's structure. Labels can specify which app or microservice a pod belongs to, as well as whether it's a stable, beta, or canary release. By using these labels, developers and ops personnel can easily see where each pod fits in, making it easier to manage complex microservices architectures.\"}\n",
      "Done for page number:12\n",
      "{\"summary\": \"This chapter is about pods in Kubernetes, specifically running containers and adding/removing labels. Labels can be added to or modified on existing pods using kubectl label command. The --overwrite option is required when changing existing labels. Examples of labeling a new pod, viewing labels with kubectl get po --show-labels, and modifying labels on an existing pod are shown.\"}\n",
      "Done for page number:13\n",
      "{\n",
      "  \"summary\": \"Label selectors allow selecting subsets of pods based on labels. A label selector can filter resources by key, value, or not equal to a specified value. Examples include listing pods with creation_method=manual, env label, or no env label.\"\n",
      "}\n",
      "Done for page number:14\n",
      "{\"summary\": \"This chapter discusses Kubernetes Pods, specifically focusing on running containers in a cluster. Label selectors are used to identify and select pods based on labels, with examples including creation_method!=manual, env in (prod,devel), and app=pc for selecting the product catalog microservice pods. Multiple conditions can be combined using comma-separated criteria, as shown in the selector app=pc,rel=beta. Label selectors are not only used for listing pods but also for performing actions on a subset of all pods.\"}\n",
      "Done for page number:15\n",
      "{\"summary\": \"In Kubernetes, using labels and selectors is a way to constrain pod scheduling without specifying exact node placement. This allows for flexible scheduling based on node requirements, such as hardware infrastructure or GPU acceleration. Labels can be applied to nodes, and selectors can be used to match those labels, ensuring that pods are scheduled to nodes that meet specific criteria, while maintaining the decoupling of applications from infrastructure.\"}\n",
      "Done for page number:16\n",
      "{\"summary\": \"Labels can be attached to Kubernetes objects like pods and nodes. Using labels, the ops team categorizes new nodes by hardware type or features like GPU availability. To schedule a pod that requires a GPU, create a YAML file with a node selector set to gpu=true and use kubectl create -f to deploy the pod.\"}\n",
      "Done for page number:17\n",
      "{\"summary\": \"Pods can be annotated with labels and annotations. Labels are key-value pairs used for identification and grouping, while annotations hold larger pieces of information primarily meant for tools. Annotations are automatically added by Kubernetes or manually by users and are useful for adding descriptions, specifying creator names, and introducing new features. The importance of label selectors will become evident in future chapters on Replication-Controllers and Services.\"}\n",
      "Done for page number:18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"Kubernetes pods can have labels and annotations, where labels are short and used for organization, while annotations can contain large blobs of data up to 256KB. Annotations like kubernetes.io/created-by were deprecated in version 1.8 and removed in 1.9. Annotations can be added or modified using the kubectl annotate command, and it's recommended to use unique prefixes to prevent key collisions.\"}\n",
      "Done for page number:19\n",
      "{\"summary\": \"Kubernetes groups objects into namespaces, which provide a scope for object names and allow for separate, non-overlapping groups. Namespaces enable operating within one group at a time and using the same resource names multiple times across different namespaces. They can be used to split complex systems, separate resources in multi-tenant environments, or divide resources into production, development, and QA environments.\"}\n",
      "Done for page number:20\n",
      "{\"summary\": \"Namespaces in Kubernetes enable separation of resources into non-overlapping groups, isolating them from other users' resources. They can be created by posting a YAML file to the Kubernetes API server using kubectl create -f or kubectl create namespace command.\"}\n",
      "Done for page number:21\n",
      "{\"summary\": \"Namespaces allow grouping resources and can be created using kubectl create command or by posting YAML manifest to API server. Resources in other namespaces can be managed by adding namespace entry to metadata section or specifying namespace with kubectl create command. Namespaces provide isolation for objects, but do not guarantee network isolation between pods across different namespaces.\"}\n",
      "Done for page number:22\n",
      "{\"summary\": \"Pods in Kubernetes can communicate with each other within a namespace. To stop and remove pods, use kubectl delete command. Pods can be deleted by name, label selector, or even deleting the whole namespace. When deleting a pod, Kubernetes sends a SIGTERM signal to shut down containers, and if they don't respond, a SIGKILL signal is sent. It's essential for processes to handle the SIGTERM signal properly.\"}\n",
      "Done for page number:23\n",
      "{\"summary\": \"To delete all pods in a namespace, use the command $ kubectl delete po --all. This will delete all running and terminating pods in the current namespace. Alternatively, you can delete a specific pod by its name or delete all pods with a certain label using the label selector.\"}\n",
      "Done for page number:24\n",
      "{\"summary\": \"Kubernetes pods run multiple containers as one entity, with kubectl commands like run and delete creating ReplicationControllers that manage pods. Deleting all resources in a namespace can be done with kubectl delete all --all, but note that some resources like Secrets are preserved and need to be deleted explicitly.\"}\n",
      "Done for page number:25\n",
      "{\"summary\": \"Pods can run multiple processes similar to physical hosts. They have YAML/JSON descriptors that define their specification and current state. Labels and selectors help organize and perform operations on multiple pods. Annotations attach data to pods, while namespaces allow different teams to use the same cluster as separate Kubernetes clusters. The kubectl explain command provides information on resources.\"}\n",
      "Done for page number:26\n",
      "{\"summary\": \"Kubernetes manages pods automatically, using resources like Replication-Controllers and Deployments to create and manage pods. This chapter focuses on keeping pods healthy, running multiple instances of the same pod, automating rescheduling after a node fails, scaling pods horizontally, running system-level pods, and batch jobs, as well as scheduling periodic or future tasks.\"}\n",
      "Done for page number:27\n",
      "{\"summary\": \"Kubernetes checks if a container is alive through liveness probes and restarts it if it fails. Liveness probes can be specified for each container in a pod's specification. Kubernetes periodically executes the probe and restarts the container if it fails. This ensures that applications are restarted even if they stop working without crashing, such as due to memory leaks or infinite loops.\"}\n",
      "Done for page number:28\n",
      "{\"summary\": \"A liveness probe checks if a container is running correctly. A successful probe returns a 2xx or 3xx HTTP response code, while a failed probe returns an error code or no response at all. The chapter demonstrates creating a new pod with an HTTP GET liveness probe for a Node.js app that intentionally fails after five requests.\"}\n",
      "Done for page number:29\n",
      "{\"summary\": \"The document explains how Kubernetes uses liveness probes to keep pods healthy. An httpGet liveness probe sends HTTP GET requests to a path on port 8080, and if the status code becomes 500, Kubernetes restarts the container. The document also demonstrates this by creating a pod with a liveness probe, showing it gets restarted after about a minute and a half, and describes how to obtain the application log of a crashed container using kubectl logs --previous.\"}\n",
      "Done for page number:30\n",
      "{\"summary\": \"This page discusses replication and other controllers in Kubernetes, specifically the liveness probe which checks if a container is running correctly. If the container fails the probe, it will be killed and re-created. The page also explains how to configure additional properties of the liveness probe, such as delay, timeout, period, and initial delay. An example YAML file is provided to demonstrate how to set an initial delay for the liveness probe.\"}\n",
      "Done for page number:31\n",
      "{\"summary\": \"To keep pods healthy, it's essential to set an initial delay for liveness probes. This prevents probes from failing as soon as the app starts, leading to unnecessary restarts. A liveness probe should check if the server is responding and ideally perform internal status checks on vital components. It's crucial to ensure the /health endpoint doesn't require authentication and only checks internals of the app, not external factors.\"}\n",
      "Done for page number:32\n",
      "{\"summary\": \"A ReplicationController in Kubernetes ensures its pods are always kept running by creating replacement pods if one disappears. Liveness probes shouldn't use too many computational resources and should be executed relatively often to keep containers running. Kubernetes will retry a probe several times before considering it a failed attempt, so implementing a retry loop is unnecessary.\"}\n",
      "Done for page number:33\n",
      "{\"summary\": \"A ReplicationController monitors running pods and ensures the desired number of replicas matches the actual number. If too few, it creates new replicas; if too many, it removes excess replicas. It recreates lost pods when a node fails, but not manually created or changed pods.\"}\n",
      "Done for page number:34\n",
      "{\"summary\": \"A ReplicationController's job is to maintain an exact number of pods that match its label selector by creating or deleting pods as needed, with three essential parts: a label selector, replica count, and pod template.\"}\n",
      "Done for page number:35\n",
      "{\"summary\": \"A ReplicationController's replica count, label selector, and pod template can be modified at any time, but only changes to the replica count affect existing pods. Changes to the label selector or pod template have no effect on existing pods and are used as a 'cookie cutter' for new pods created by this ReplicationController. The controller ensures a pod is always running, creates replacement replicas when a cluster node fails, and enables easy horizontal scaling of pods.\"}\n",
      "Done for page number:36\n",
      "{\"summary\": \"Kubernetes creates a Replication-Controller named kubia that ensures three pod instances match the label selector app=kubia. When there aren't enough pods, it creates new ones from the provided pod template. The API server verifies the ReplicationController definition and will not accept it if misconfigured. To prevent such scenarios, let Kubernetes extract the selector from the pod template.\"}\n",
      "Done for page number:37\n",
      "{\"summary\": \"A ReplicationController is introduced, managing three pods and automatically spinning up new ones if any are deleted. The kubectl get command shows information about ReplicationControllers, including desired and actual pod numbers. Additional details can be obtained with the kubectl describe command, displaying the ReplicationController's name, namespace, selector, labels, annotations, replicas, and pod status.\"}\n",
      "Done for page number:38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"A ReplicationController in Kubernetes creates a new pod to replace one that has been deleted when it detects an inadequate number of running pods, triggered by events such as pod deletion or termination.\"}\n",
      "Done for page number:39\n",
      "{\"summary\": \"A ReplicationController in Kubernetes automatically spins up new pods to replace those that are down when a node fails, as demonstrated by simulating a node failure on a three-node cluster. After shutting down the network interface of one node, the status is shown as NotReady, and the pods remain unchanged for several minutes before the ReplicationController creates a new pod to replace the downed ones.\"}\n",
      "Done for page number:40\n",
      "{\"summary\": \"ReplicationController automatically manages pods based on a label selector and can spin up new pods if one fails or is removed from scope. A pod's labels can be changed to move it in or out of the ReplicationController's scope, but changing its labels does not delete it. The replication controller will notice if a managed pod is missing and spin up a new one to replace it.\"}\n",
      "Done for page number:41\n",
      "{\"summary\": \"A ReplicationController doesn't care if labels are added to its managed pods. Changing a label on a managed pod makes it no longer match the controller's label selector, prompting the controller to start a new pod to bring the number back to three.\"}\n",
      "Done for page number:42\n",
      "{\"summary\": \"A ReplicationController can spin up new pods to bring the number back up after one is removed, allowing for independent pod management and changing label selectors to control which pods are included in the controller's scope.\"}\n",
      "Done for page number:43\n",
      "{\"summary\": \"ReplicationControllers can modify their pod template at any time, but changes only affect new pods created after the modification. To change an existing pod, it must be deleted and a new one will be created based on the updated template.\"}\n",
      "Done for page number:44\n",
      "{\"summary\": \"ReplicationControllers ensure a specific number of pod instances is always running. Scaling pods horizontally is trivial and can be done by changing the replicas field, either by using the command `kubectl scale` or by editing the ReplicationController's definition directly with `kubectl edit`. This allows scaling up or down with ease.\"}\n",
      "Done for page number:45\n",
      "{\"summary\": \"A ReplicationController is updated when scaled up or down, and it immediately scales the number of pods to the desired state. Scaling is a matter of stating the desired state, not telling Kubernetes how to do it. Declarative approach makes interacting with a Kubernetes cluster easy. When deleting a ReplicationController through kubectl delete, the pods are also deleted unless managed by another controller.\"}\n",
      "Done for page number:46\n",
      "{\"summary\": \"ReplicationControllers manage pods and keep them running without interruption, but can be deleted while keeping the pods running using the --cascade=false option. ReplicaSets are a newer resource that replaces ReplicationControllers completely and should be used instead. Deleting a ReplicationController leaves its pods unmanaged, but a new one can be created to manage them again.\"}\n",
      "Done for page number:47\n",
      "{\"summary\": \"ReplicaSets are used instead of ReplicationControllers to manage replicas. A ReplicaSet behaves exactly like a ReplicationController but with more expressive pod selectors, allowing matching pods based on label presence or absence, and not just specific values. This enables a single ReplicaSet to match multiple sets of pods and treat them as a single group. The process of creating a ReplicaSet involves defining its YAML configuration, including the API version, kind, metadata, selector, replicas, template, and containers, which can be used to adopt orphaned pods created by a ReplicationController.\"}\n",
      "Done for page number:48\n",
      "{\"summary\": \"ReplicaSets aren't part of the v1 API, so specify the proper apiVersion when creating a resource. To create a ReplicaSet, use kubectl create command with YAML file, then examine it with kubectl get and describe commands. The apiVersion property specifies the API group (apps) and actual API version (v1beta2), which categorizes Kubernetes resources into core and other groups.\"}\n",
      "Done for page number:49\n",
      "{\"summary\": \"ReplicaSets are similar to ReplicationControllers, but with more expressive label selectors. The matchExpressions property allows for complex matching rules, such as requiring a pod to have a specific label or not having a certain label. This provides flexibility in selecting pods, making ReplicaSets more powerful than ReplicationControllers.\"}\n",
      "Done for page number:50\n",
      "{\"summary\": \"ReplicaSets and ReplicationControllers are used for running a specific number of pods in a Kubernetes cluster, but DaemonSets are used to run one pod on each node, with exactly one instance per node, suitable for infrastructure-related pods like log collectors and resource monitors.\"}\n",
      "Done for page number:51\n",
      "{\n",
      "\"summary\": \"A DaemonSet is used to run a pod on every node in a Kubernetes cluster, or on a subset of nodes specified by a node selector. It ensures a desired number of pods exist and creates a new pod instance if a new node is added or an existing node is deleted. Unlike ReplicaSets, DaemonSets do not need a replica count and will deploy pods even to unschedulable nodes.\"\n",
      "}\n",
      "Done for page number:52\n",
      "{\"summary\": \"A DaemonSet is created for deploying managed pods. A YAML definition is written for the DaemonSet to run a mock ssd-monitor process on nodes with the 'disk=ssd' label. The DaemonSet will create an instance of the pod on each node that meets this condition.\"}\n",
      "Done for page number:53\n",
      "{\"summary\": \"Creating a DaemonSet to run one pod on each node requires labeling nodes with the required label. Initially, the DaemonSet appears not to deploy pods due to missing labels. Adding the label to one or more nodes triggers the DaemonSet to create pods for matching nodes.\"}\n",
      "Done for page number:54\n",
      "{\"summary\": \"A chapter about deploying managed pods using Replication and other controllers. It discusses running pods that perform a single completable task, which is different from continuous tasks like DaemonSets. The Job resource is introduced as a solution for this type of task, allowing pods to be rescheduled in case of node failure. An example is given of running a container image built on top of the busybox image, invoking the sleep command for two minutes.\"}\n",
      "Done for page number:55\n",
      "{\"summary\": \"A Job resource in Kubernetes runs a single completable task. A YAML definition of a Job resource is provided, which defines a pod that will run an image invoking a process for 120 seconds and then exit. The restartPolicy specifies what to do when the processes finish. Jobs are part of the batch API group, version v1, and cannot use the default restart policy. Pods managed by Jobs are rescheduled until they finish successfully.\"}\n",
      "Done for page number:56\n",
      "{\"summary\": \"A Job resource in Kubernetes can be used to deploy a single, managed pod with a restart policy of OnFailure or Never. Once created, the Job will run the pod until completion, at which point it will be marked as successful and the pod deleted. Jobs can also be configured to create multiple pods that run in parallel or sequentially by setting the completions and parallelism properties.\"}\n",
      "Done for page number:57\n",
      "{\"summary\": \"A Job can be configured to run multiple pods sequentially or in parallel. To run pods sequentially, set completions to the number of times you want the Job's pod to run. For example, setting completions to 5 will create one pod at a time until five pods complete successfully. To run pods in parallel, specify how many pods are allowed to run with the parallelism Job spec property. This allows up to that many pods to be created and running at the same time.\"}\n",
      "Done for page number:58\n",
      "{\"summary\": \"You can scale a Job's parallelism property while it's running using kubectl scale command. Additionally, you can limit a pod's time to complete by setting activeDeadlineSeconds in the pod spec. A Job can also be configured to retry failed pods up to 6 times before being marked as failed. Furthermore, Kubernetes supports scheduling Jobs periodically or once in the future through CronJobs, which create a Job resource at the specified time and run it according to the Job template.\"}\n",
      "Done for page number:59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"A CronJob resource creates Job objects based on a specified schedule. The schedule is set using the cron format (minute, hour, day of month, month, day of week), and can be configured to run jobs at specific intervals. The jobTemplate property defines the template for creating Job resources, which are created from the CronJob resource at approximately the scheduled time.\"}\n",
      "Done for page number:60\n",
      "{\"summary\": \"A CronJob creates a single Job for each execution configured in the schedule, but can create two Jobs if run concurrently or none at all. To combat this, jobs should be idempotent and next job runs should perform work missed by previous runs. A startingDeadlineSeconds field can also be specified to ensure pods start running within a certain timeframe.\"}\n",
      "Done for page number:61\n",
      "{\"summary\": \"ReplicationControllers are being replaced with ReplicaSets and Deployments which provide additional features, DaemonSets ensure every node runs a pod instance, Jobs schedule batch tasks while CronJobs handle future executions.\"}\n",
      "Done for page number:62\n",
      "{\"summary\": \"Services enable clients to discover and communicate with pods, allowing them to respond to external requests. This chapter covers creating Service resources to expose a group of pods at a single address, discovering services in the cluster, exposing services to external clients, connecting to external services from inside the cluster, controlling pod readiness for service participation, and troubleshooting services.\"}\n",
      "Done for page number:63\n",
      "{\"summary\": \"A Kubernetes Service is a resource that provides a single, constant point of entry to a group of pods providing the same service. Each service has an IP address and port that never change while the service exists, allowing clients to connect without needing to know individual pod locations. This enables external clients to connect to frontend pods without worrying about IP changes and allows frontend pods to connect to backend database services with a stable address.\"}\n",
      "Done for page number:64\n",
      "{\"summary\": \"A service enables clients to discover and talk to pods, even if the pod's IP address changes. Services are created using label selectors, which specify which pods belong to the same set. A service can be backed by more than one pod, with connections load-balanced across all backing pods.\"}\n",
      "Done for page number:65\n",
      "{\"summary\": \"A Kubernetes service called kubia is created manually by posting a YAML descriptor, which exposes all pods matching the app=kubia label selector on port 80 and routes connections to port 8080 of each pod. The service accepts connections on port 80 and forwards them to port 8080 of one of the matching pods, allowing clients to access the service through a single IP address and port.\"}\n",
      "Done for page number:66\n",
      "{\"summary\": \"The chapter explains services in Kubernetes, enabling clients to discover and talk to pods. A service is exposed through an internal cluster IP that's only accessible from inside the cluster. The primary purpose of services is exposing groups of pods to other pods in the cluster. To test a service, one can send requests to it from within the cluster using various methods such as creating a pod, ssh-ing into a node, or executing a command in an existing pod using kubectl exec.\"}\n",
      "Done for page number:67\n",
      "{\"summary\": \"When running curl inside a pod using kubectl exec, Kubernetes proxies the connection to a random available pod among those backing the service. The double dash (--), signals the end of command options for kubectl and everything after it is executed within the pod. Without the double dash, the -s option would be interpreted as an option for kubectl, resulting in misleading errors.\"}\n",
      "Done for page number:68\n",
      "{\"summary\": \"This chapter discusses services in Kubernetes, enabling clients to discover and talk to pods. Session affinity can be set to either None or ClientIP, redirecting requests from the same client IP to the same pod. Services can also support multiple ports, exposing all ports through a single cluster IP, with each port requiring a specified name.\"}\n",
      "Done for page number:69\n",
      "{\"summary\": \"A Kubernetes Service can be defined with named ports in both the pod and service specifications. The label selector applies to the whole service, not individual ports. Ports can be referred to by name or number in the service spec.\"}\n",
      "Done for page number:70\n",
      "{\"summary\": \"Services enable clients to discover and talk to pods through a single and stable IP address and port, which remains unchanged throughout its lifetime. Client pods can discover the service's IP and port through environment variables or by manually looking up its IP address.\"}\n",
      "Done for page number:71\n",
      "{\n",
      "  \"summary\": \"Services in Kubernetes are exposed through environment variables, but can also be discovered using DNS. Each service gets a DNS entry and client pods can access them through their fully qualified domain name (FQDN). This allows for a more flexible way of accessing services without relying on environment variables.\"\n",
      "}\n",
      "Done for page number:72\n",
      "{\"summary\": \"Services enable clients to discover and talk to pods. Clients can connect to a service by opening a connection to its FQDN, which includes the service name, namespace, and cluster domain suffix. If in the same namespace as the database pod, the client can refer to the service simply by its name. To access a service inside a pod's container, run bash using kubectl exec command with the -it option, and then use curl to access the service.\"}\n",
      "Done for page number:73\n",
      "{\"summary\": \"You can connect to services living outside the cluster by using its name as the hostname in the requested URL. Omitting namespace and svc.cluster.local suffix is also allowed due to how DNS resolver inside each pod's container is configured. However, trying to ping service IP will not work because it's a virtual IP that only has meaning when combined with the service port.\"}\n",
      "Done for page number:74\n",
      "{\"summary\": \"Kubernetes services enable clients to discover and talk to pods. The Endpoints resource is a list of IP addresses and ports exposing a service. The pod selector in the service spec is used to build a list of IPs and ports, which are stored in the Endpoints resource. Clients connect to a service, and the service proxy selects one of those IP and port pairs and redirects the incoming connection to the server listening at that location.\"}\n",
      "Done for page number:75\n",
      "{\"summary\": \"A Kubernetes service called `external-service` is created without a pod selector, requiring a separate Endpoints resource to be manually created with the same name and containing target IP addresses and ports for the service.\"}\n",
      "Done for page number:76\n",
      "{\"summary\": \"Kubernetes services enable clients to discover and talk to pods, and can be exposed externally using an ExternalName service which creates a DNS record pointing to a fully qualified domain name. This allows clients to connect directly to the external service without going through the service proxy, and does not require a cluster IP address. ExternalName services are implemented solely at the DNS level and can be modified by changing the externalName attribute or switching to a ClusterIP service with an Endpoints object.\"}\n",
      "Done for page number:77\n",
      "{\"summary\": \"A service can be made accessible externally by setting its type to NodePort, LoadBalancer, or creating an Ingress resource. A NodePort service makes a port on all nodes reserve and forward incoming connections to the pods that are part of the service, allowing access through any node's IP and reserved node port.\"}\n",
      "Done for page number:78\n",
      "{\"summary\": \"A Kubernetes Service named kubia-nodeport is created with type NodePort, specifying node port 30123 and exposing internal port 80. The service is accessible through the IP address of any cluster node on port 30123, redirecting incoming connections to a randomly selected pod.\"}\n",
      "Done for page number:79\n",
      "{\"summary\": \"To expose services to external clients, configure Google Cloud Platform's firewalls to allow connections on the desired port, e.g., $ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123. This enables access through one of the node's IPs on that port, which can be found in a separate step.\"}\n",
      "Done for page number:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"Services in Kubernetes allow clients to discover and talk to pods. With NodePort services, pods are accessible through port 30123 on any node. However, this can lead to issues if a node fails. A load balancer can be used to distribute traffic across healthy nodes, and Kubernetes clusters running on cloud providers often support automatic load balancer provisioning. Using JSONPath with kubectl allows for efficient retrieval of node IPs.\"}\n",
      "Done for page number:81\n",
      "{\"summary\": \"Creating a Kubernetes Service with a LoadBalancer allows external access through a unique, publicly accessible IP address. The service type is set to LoadBalancer, and ports are specified for external connection. Once created, the load balancer's IP address is listed in the Service object, enabling direct access via curl or other tools.\"}\n",
      "Done for page number:82\n",
      "{\"summary\": \"Services in Kubernetes allow clients to discover and talk to pods, using the load balancer to route HTTP requests to a random pod for each connection. Even with session affinity set to None, users will hit the same pod every time due to keep-alive connections from web browsers, whereas tools like curl open new connections each time.\"}\n",
      "Done for page number:83\n",
      "{\"summary\": \"Exposing services to external clients can be done through NodePort or LoadBalancer-type services. However, when using a node port, externally originating connections may not always go directly to the pod running on the same node, requiring an additional network hop. This can be prevented by configuring the service's externalTrafficPolicy field to 'Local', but this has its own drawbacks such as uneven distribution of connections across pods.\"}\n",
      "Done for page number:84\n",
      "{\"summary\": \"Services in Kubernetes allow clients to discover and communicate with pods, but can't preserve client IP when using node ports due to Source Network Address Translation (SNAT). The Local external traffic policy affects this, but creating an Ingress resource is another way to expose services externally, allowing multiple services to share one public IP address and load balancer, improving load distribution and scalability.\"}\n",
      "Done for page number:85\n",
      "{\"summary\": \"Ingresses in Kubernetes operate at the application layer, providing features like cookie-based session affinity. An Ingress controller is required to make Ingress resources work, and different environments use different implementations. To enable the Ingress add-on in Minikube, run $ minikube addons enable ingress, which allows exposing multiple services through a single Ingress.\"}\n",
      "Done for page number:86\n",
      "{\"summary\": \"Creating an Ingress resource in Kubernetes enables clients to discover and talk to pods. An example YAML manifest is provided, which defines an Ingress with a single rule sending all HTTP requests from the host kubia.example.com to the kubia-nodeport service on port 80. The Ingress controller pod can be listed using kubectl get po --all-namespaces.\"}\n",
      "Done for page number:87\n",
      "{\"summary\": \"Exposing services externally through an Ingress resource requires configuring DNS or /etc/hosts to point to the Ingress controller's IP address. The Ingress controller then selects a pod based on the Host header and forwards the request to it, allowing access to the service at http://kubia.example.com.\"}\n",
      "Done for page number:88\n",
      "{\"summary\": \"An Ingress can expose multiple services on the same host by mapping different paths to different services, allowing clients to reach two or more services through a single IP address. This is achieved by specifying multiple paths in the Ingress spec and mapping each path to a specific service, as shown in Listing 5.14. Requests are routed to the corresponding service based on the path in the requested URL.\"}\n",
      "Done for page number:89\n",
      "{\"summary\": \"An Ingress resource can map different services to different hosts based on the Host header in the request, and can also handle TLS traffic by attaching a certificate and private key to the Ingress as a Secret. This allows for secure communication between clients and the controller without requiring the application pod to support TLS.\"}\n",
      "Done for page number:90\n",
      "{\"summary\": \"Services allow clients to discover and communicate with pods. A Secret was created using two files, and an Ingress object was updated to accept HTTPS requests for kubia.example.com. Alternatively, 'kubectl apply' can be used to update the Ingress resource. CertificateSigningRequest resources enable certificates to be signed by a human operator or automated process, retrieving a signed certificate from the CSR's status field.\"}\n",
      "Done for page number:91\n",
      "{\"summary\": \"Kubernetes allows you to define a readiness probe for your pod, which periodically determines whether the pod should receive client requests or not. When a container's readiness probe returns success, it signals that the container is ready to accept requests, allowing traffic to be directed to it only when it's fully ready to serve.\"}\n",
      "Done for page number:92\n",
      "{\"summary\": \"Services in Kubernetes enable clients to discover and talk to pods through GET requests or specific URL paths. Readiness probes check if a container is ready to serve requests, with three types: Exec, HTTP GET, and TCP Socket. If a pod fails the readiness check, it's removed from the service until it becomes ready again. This ensures only healthy containers receive requests, distinguishing from liveness probes which keep pods running.\"}\n",
      "Done for page number:93\n",
      "{\"summary\": \"Readiness probes ensure clients only talk to healthy pods by signaling when a pod is ready to accept connections. A readiness probe can be added to a pod by modifying the ReplicationController's pod template using kubectl edit, adding the probe definition under spec.template.spec.containers. The probe periodically checks if a file exists, and if it does, the pod is considered ready.\"}\n",
      "Done for page number:94\n",
      "{\"summary\": \"Services: enabling clients to discover and talk to pods. ReplicationController's pod template changes have no effect on existing pods. Existing pods report not being ready until they're re-created by the Replication-Controller, which will fail the readiness check unless a /var/ready file is created in each of them.\"}\n",
      "Done for page number:95\n",
      "{\n",
      "  \"summary\": \"A readiness probe in Kubernetes determines if a pod is ready to accept connections. In real-world scenarios, it should return success or failure depending on whether the app can receive client requests. If no readiness probe is defined, pods become service endpoints immediately and clients may experience connection errors when the app takes too long to start listening for incoming connections.\"\n",
      "}\n",
      "Done for page number:96\n",
      "{\"summary\": \"Kubernetes allows clients to discover pod IPs through DNS lookups, enabling connection to all pods or individual pods using a headless service with clusterIP set to None. This method is ideal for Kubernetes-agnostic apps, providing a stable IP address for clients to connect to all backing pods.\"}\n",
      "Done for page number:97\n",
      "{\n",
      "  \"summary\": \"A headless service is used to discover individual pods based on a pod selector. The service will list only ready pods as endpoints. To confirm readiness, create the /var/ready file in each pod. A DNS lookup can be performed from inside a pod using the tutum/dnsutils container image or by running a new pod without writing a YAML manifest using kubectl run with the --generator=run-pod/v1 option. This allows understanding of how DNS A records are returned for a headless service, which returns IPs of all ready pods.\"\n",
      "}\n",
      "Done for page number:98\n",
      "{\"summary\": \"Headless services allow clients to connect directly to pods by DNS name. Kubernetes provides load balancing across pods through DNS round-robin mechanism instead of service proxy. To discover all pods, including unready ones, add annotation 'service.alpha.kubernetes.io/tolerate-unready-endpoints: true' or use the publishNotReadyAddresses field in service spec.\"}\n",
      "Done for page number:99\n",
      "{\"summary\": \"Make sure to access Kubernetes service from within the cluster and not outside. Check if readiness probe is succeeding, examine Endpoints object, and try accessing service using its cluster IP or FQDN. Ensure correct port is exposed and target port is not used. Connect directly to pod IP to confirm connections are being accepted. If still issues persist, check if app is binding only to localhost.\"}\n",
      "Done for page number:100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"Services enable clients to discover and talk to pods using a pod's readiness probe, enabling discovery of pod IPs through DNS for headless services. Additionally, troubleshooting and modifying firewall rules in Google Kubernetes/Compute Engine, executing commands in pod containers, running bash shells in existing pods, and modifying resources with kubectl apply can be performed.\"}\n",
      "Done for page number:101\n",
      "{\"summary\": \"This chapter explores how containers in a pod can access external disk storage and share storage between them. Containers have isolated file systems, but volumes allow sharing disk space. Topics include creating multi-container pods, using Git repositories inside pods, attaching persistent storage, and dynamic provisioning of persistent storage.\"}\n",
      "Done for page number:102\n",
      "{\"summary\": \"Kubernetes provides storage volumes that allow new containers to continue where the last one finished, preserving directories with actual data across container restarts. Volumes are defined in a pod's specification and must be mounted in each container that needs to access it, allowing multiple containers to share disk storage and enabling them to work together effectively.\"}\n",
      "Done for page number:103\n",
      "{\"summary\": \"The document introduces the concept of volumes in Kubernetes, where multiple containers within a pod can share storage without relying on shared filesystems. Three containers are used as examples: WebServer, ContentAgent, and LogRotator, each with its own filesystem but sharing two volumes, publicHtml and logVol, mounted at different paths to illustrate this concept.\"}\n",
      "Done for page number:104\n",
      "{\"summary\": \"Volumes in Kubernetes allow attaching disk storage to containers, enabling them to operate on the same files. A volume is bound to a pod's lifecycle and can be mounted at arbitrary locations within the file tree. Various types of volumes are available, including emptyDir, hostPath, gitRepo, nfs, gcePersistentDisk, awsElasticBlockStore, and azureDisk, each with its own purpose and use case. To access a volume from within a container, a VolumeMount must be defined in the container's spec.\"}\n",
      "Done for page number:105\n",
      "{\"summary\": \"Volumes in Kubernetes can be used to share data between containers or for exposing Kubernetes resources and cluster information. Special types of volumes like secret, downwardAPI, and configMap are used to expose metadata to apps running in a pod. A single pod can use multiple volumes of different types at the same time, with each container having the option to mount or not. An emptyDir volume is useful for sharing files between containers or for temporary data storage by a single container.\"}\n",
      "Done for page number:106\n",
      "{\"summary\": \"To create a pod that uses a shared volume, you need to build a Docker image with the required binary (fortune) and script (fortuneloop.sh). The image is based on ubuntu:latest, installs fortune, adds the script to /bin folder, and sets it as the ENTRYPOINT. You then create a pod manifest (fortune-pod.yaml) that specifies two containers sharing the same volume. Finally, you can run the pod using kubectl apply -f fortune-pod.yaml\"}\n",
      "Done for page number:107\n",
      "{\"summary\": \"A pod contains two containers and a shared volume between them. The html-generator container writes to the volume every 10 seconds, while the web-server container serves files from it. By forwarding port 80 on the local machine to the pod's port, users can access the Nginx server through localhost:8080 and receive a different fortune message with each request.\"}\n",
      "Done for page number:108\n",
      "{\"summary\": \"An emptyDir volume can be created on tmpfs filesystem for better performance, while a gitRepo volume clones and checks out a Git repository at pod startup. The files in a gitRepo volume are not kept in sync with the referenced repo, but are updated when a new pod is created. This type of volume is useful for storing static HTML files or serving the latest version of a website.\"}\n",
      "Done for page number:109\n",
      "{\"summary\": \"Using volumes in Kubernetes, specifically gitRepo volumes, allows sharing data between containers. This is demonstrated by running a web server pod serving files from a cloned Git repository, where the pod is created with a single Nginx container and a single gitRepo volume that clones the repository into the root directory of the volume.\"}\n",
      "Done for page number:110\n",
      "{\"summary\": \"To keep files in sync with a Git repository, you can create a sidecar container that runs a Git sync process. This process can be run in an existing container image from Docker Hub, such as 'git sync'. The sidecar container should mount the gitRepo volume and configure the Git sync process to keep the files in sync with the Git repo. This method is recommended instead of using a gitRepo volume for private Git repositories, which are not supported by Kubernetes.\"}\n",
      "Done for page number:111\n",
      "{\"summary\": \"A gitRepo volume is created for and used exclusively by a pod, but its contents can survive multiple pod instantiations if the volume type is different. hostPath volumes allow pods to access files on the node's filesystem, making it possible for system-level pods to read or use the node's devices through the filesystem.\"}\n",
      "Done for page number:112\n",
      "{\"summary\": \"HostPath volumes are not suitable for storing a database's data directory as they store contents on a specific node's filesystem, making it sensitive to scheduling. Instead, use them to access the node's log files, kubeconfig, or CA certificates. System-wide pods like fluentd-kubia use hostPath volumes to access node's data.\"}\n",
      "Done for page number:113\n",
      "{\"summary\": \"To persist data across pods, a network-attached storage (NAS) is needed. A GCE Persistent Disk can be used as underlying storage mechanism on Google Kubernetes Engine. The disk must be created in the same zone as the Kubernetes cluster and its size should be at least 200GB for optimal I/O performance.\"}\n",
      "Done for page number:114\n",
      "{\"summary\": \"This chapter explains how to attach disk storage to containers using Kubernetes volumes. It provides an example of creating a 1 GiB GCE persistent disk called 'mongodb' and configuring a pod to use it as a volume, mounting it at '/data/db'. The YAML for the pod is provided, specifying the gcePersistentDisk type, fsType as ext4, and mountPath as /data/db. A note is also given for using Minikube, where you can't use a GCE Persistent Disk, but instead deploy mongodb-pod-hostpath.yaml using a hostPath volume.\"}\n",
      "Done for page number:115\n",
      "{\"summary\": \"To use persistent storage, write data to the MongoDB database by running the MongoDB shell inside the container and inserting JSON documents. The data will be stored on a GCE persistent disk. After deleting and re-creating the pod, the new pod can read the persisted data from the previous pod, using the same GCE persistent disk.\"}\n",
      "Done for page number:116\n",
      "{\"summary\": \"You can attach disk storage to containers using Kubernetes volumes, such as GCE Persistent Disk, awsElasticBlockStore, azureFile, or azureDisk. These volumes provide persistent storage for pods, allowing data to be retained across pod instances. To use a different volume type, create the underlying storage and set properties in the volume definition.\"}\n",
      "Done for page number:117\n",
      "{\"summary\": \"Kubernetes supports various storage technologies, including NFS, ISCSI, GlusterFS, and others. However, it's recommended to use volumes in a way that decouples pod definitions from specific clusters, avoiding infrastructure-related details in pod specifications.\"}\n",
      "Done for page number:118\n",
      "{\"summary\": \"Kubernetes aims to hide infrastructure from developers, allowing them to request persistent storage without knowing specific details. Cluster admins configure the cluster to provide what apps request, using PersistentVolumes and PersistentVolumeClaims to decouple pods from underlying storage technology.\"}\n",
      "Done for page number:119\n",
      "{\"summary\": \"A cluster administrator creates a PersistentVolume resource through the Kubernetes API server, specifying its size and access modes. A user then creates a PersistentVolumeClaim manifest, specifying their required size and access mode, which is bound to an existing PersistentVolume. The volume can be used in a pod, but other users cannot use it until the claim is released.\"}\n",
      "Done for page number:120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"A PersistentVolume is created by specifying its capacity, access modes, and storage type. The administrator can then claim the PV with a PersistentVolumeClaim, which allows a container to read from or write to it. A PV is cluster-level resource like nodes and doesn't belong to any namespace. It's created with kubectl create command and shown as Available until claimed.\"}\n",
      "Done for page number:121\n",
      "{\"summary\": \"To use a PersistentVolume in a Kubernetes pod that requires persistent storage, you need to create a PersistentVolumeClaim (PVC) first. This is done by preparing a PVC manifest and posting it to the Kubernetes API through kubectl create. The PVC claims the PersistentVolume for exclusive use within a namespace, allowing the same PVC to stay available even if the pod is rescheduled.\"}\n",
      "Done for page number:122\n",
      "{\"summary\": \"A Kubernetes PersistentVolumeClaim is created with a requested 1Gi of storage and ReadWriteOnce access mode. The claim is bound to a matching PersistentVolume, which is shown as Bound in kubectl get pvc and pv commands. The PersistentVolume's capacity and access modes match the claim's requirements.\"}\n",
      "Done for page number:123\n",
      "{\"summary\": \"To use a PersistentVolume in a pod, reference the PersistentVolumeClaim by name inside the pod's volume. A Pod can claim and use the same PersistentVolume until it is released, allowing decoupling from underlying storage technology.\"}\n",
      "Done for page number:124\n",
      "{\"summary\": \"The chapter discusses the benefits of using PersistentVolumes (PVs) and claims to attach disk storage to containers, making it simpler for application developers by abstracting away infrastructure-specific details. A pod can use a GCE Persistent Disk either directly or through a PV and claim, allowing for greater flexibility and portability across different Kubernetes clusters.\"}\n",
      "Done for page number:125\n",
      "{\"summary\": \"When a PersistentVolumeClaim is deleted, its status becomes Pending and it's no longer bound to a PersistentVolume, which can be reused by other pods after being manually recycled or reclaimed automatically using Retain, Recycle, or Delete policies, allowing the reuse of volumes across different namespaces.\"}\n",
      "Done for page number:126\n",
      "{\"summary\": \"A PersistentVolume only supports Retain or Delete policies. The reclaim policy can be changed on an existing PersistentVolume. Kubernetes also performs dynamic provisioning of PersistentVolumes through persistent-volume provisioners and StorageClass objects, allowing users to choose the type of PersistentVolume they want.\"}\n",
      "Done for page number:127\n",
      "{\"summary\": \"Dynamic provisioning of PersistentVolumes allows administrators to define one or two StorageClasses, enabling the system to create new PersistentVolumes each time a PersistentVolumeClaim is requested. This eliminates the possibility of running out of PersistentVolumes. The StorageClass resource specifies the provisioner and parameters for provisioning, which can be specific to cloud providers like GCE. Users can refer to the storage class by name in their PersistentVolumeClaims, enabling dynamic provisioning of PersistentVolumes.\"}\n",
      "Done for page number:128\n",
      "{\"summary\": \"A PersistentVolumeClaim (PVC) can specify a custom storage class, such as 'fast', which is referenced by a provisioner to create a PersistentVolume. The provisioner is used even if an existing manually provisioned PV matches the PVC. If the storage class does not exist, provisioning will fail. The dynamically created PV has the requested capacity and access modes, with a reclaim policy of Delete, meaning it will be deleted when the PVC is deleted.\"}\n",
      "Done for page number:129\n",
      "{\"summary\": \"Dynamic provisioning of PersistentVolumes allows cluster admins to create multiple storage classes with different performance characteristics. Developers can then choose which one is most appropriate for each claim they create. This makes PVC definitions portable across different clusters as long as StorageClass names are the same, demonstrating flexibility and consistency in Kubernetes environments.\"}\n",
      "Done for page number:130\n",
      "{\"summary\": \"The default storage class in a GKE cluster is defined by an annotation, which makes it the default storage class. A PersistentVolumeClaim can be created without specifying a storage class and a GCE Persistent Disk of type pd-standard will be provisioned for you.\"}\n",
      "Done for page number:131\n",
      "{\"summary\": \"A PVC with an empty storageClassName will bind to a pre-provisioned PV instead of dynamically provisioning a new one. Setting storageClassName to \"\" explicitly ensures the PVC uses a manually provisioned PersistentVolume.\"}\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 161 (char 160)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_page_idx,end_page_index):\n\u001b[0;32m----> 3\u001b[0m         \u001b[43menrich_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone for page number:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(idx))\n",
      "Cell \u001b[0;32mIn[15], line 184\u001b[0m, in \u001b[0;36menrich_page\u001b[0;34m(page_idx)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(page_summary_txt)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m#print(\"Page Summary Json\")\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m#print(page_entity_lst_dict)\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m#page_highlights_lst_dict_json=json.loads(page_highlights_lst_dict)\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m document_dict_deserialized_stage2[page_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_summary_txt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 161 (char 160)"
     ]
    }
   ],
   "source": [
    "for idx in range(start_page_idx,end_page_index):\n",
    "   \n",
    "        enrich_page(idx)\n",
    "        print(\"Done for page number:\"+str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c44d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###The main contents of Kubernets In Action book is contained from page 87 to 565####\n",
    "####Thats why we will only take the sliced portion i.e. from page 87 to 565 of the pdf dictionary list#####\n",
    "import pickle\n",
    "\n",
    "# Store data (serialize in a pickle) upto page 102\n",
    "with open('./pdf_enriched_output/pdf_enriched_content_dict_phase4_pagewise_summary_131.pickle', 'wb') as handle:\n",
    "    pickle.dump(document_dict_deserialized_stage2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5054dc37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
